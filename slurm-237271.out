/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 07:08:06 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 07:08:06 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 07:08:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 07:08:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 07:08:06 visual_prompt]: Training with config:
[09/26 07:08:06 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 07:08:06 visual_prompt]: Loading training data...
2023-09-26 07:08:07.180410: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-26 07:08:07.228049: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-26 07:08:08.412496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[09/26 07:08:10 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 07:08:12 visual_prompt]: Number of images: 800
[09/26 07:08:12 visual_prompt]: Number of classes: 6 / 6
[09/26 07:08:12 visual_prompt]: Loading validation data...
[09/26 07:08:12 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 07:08:12 visual_prompt]: Number of images: 200
[09/26 07:08:12 visual_prompt]: Number of classes: 6 / 6
[09/26 07:08:12 visual_prompt]: Constructing models...
[09/26 07:08:15 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 07:08:15 visual_prompt]: tuned percent:0.540
[09/26 07:08:17 visual_prompt]: Device used for model: 0
[09/26 07:08:17 visual_prompt]: Setting up Evaluator...
[09/26 07:08:17 visual_prompt]: Setting up Trainer...
[09/26 07:08:17 visual_prompt]: 	Setting up the optimizer...
[09/26 07:08:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 07:08:28 visual_prompt]: Epoch 1 / 100: avg data time: 1.82e-01, avg batch time: 0.8080, average train loss: 2.9792
[09/26 07:08:29 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1676, average loss: 2.9268
[09/26 07:08:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:08:29 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 07:08:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 07:08:36 visual_prompt]: Epoch 2 / 100: avg data time: 4.91e-02, avg batch time: 0.4931, average train loss: 23.5363
[09/26 07:08:37 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1673, average loss: 29.6090
[09/26 07:08:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:08:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 07:08:44 visual_prompt]: Epoch 3 / 100: avg data time: 5.37e-02, avg batch time: 0.4973, average train loss: 40.6048
[09/26 07:08:45 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1669, average loss: 59.4204
[09/26 07:08:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 07:08:45 visual_prompt]: Best epoch 3: best metric: 0.205
[09/26 07:08:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 07:08:52 visual_prompt]: Epoch 4 / 100: avg data time: 4.80e-02, avg batch time: 0.4910, average train loss: 46.2531
[09/26 07:08:53 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1674, average loss: 48.5682
[09/26 07:08:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:08:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 07:09:00 visual_prompt]: Epoch 5 / 100: avg data time: 5.57e-02, avg batch time: 0.4995, average train loss: 59.4515
[09/26 07:09:01 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1675, average loss: 26.5960
[09/26 07:09:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:09:01 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 07:09:08 visual_prompt]: Epoch 6 / 100: avg data time: 4.10e-02, avg batch time: 0.4847, average train loss: 89.8182
[09/26 07:09:09 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1679, average loss: 109.7726
[09/26 07:09:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:09:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 07:09:16 visual_prompt]: Epoch 7 / 100: avg data time: 3.86e-02, avg batch time: 0.4834, average train loss: 97.7404
[09/26 07:09:17 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1681, average loss: 145.0070
[09/26 07:09:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:09:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 07:09:24 visual_prompt]: Epoch 8 / 100: avg data time: 5.40e-02, avg batch time: 0.4985, average train loss: 142.1008
[09/26 07:09:25 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1687, average loss: 139.4163
[09/26 07:09:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:09:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 07:09:32 visual_prompt]: Epoch 9 / 100: avg data time: 4.88e-02, avg batch time: 0.4940, average train loss: 166.4273
[09/26 07:09:33 visual_prompt]: Inference (val):avg data time: 1.88e-05, avg batch time: 0.1683, average loss: 154.8162
[09/26 07:09:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:09:33 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 07:09:40 visual_prompt]: Epoch 10 / 100: avg data time: 5.57e-02, avg batch time: 0.5016, average train loss: 197.1528
[09/26 07:09:41 visual_prompt]: Inference (val):avg data time: 1.87e-05, avg batch time: 0.1686, average loss: 261.9903
[09/26 07:09:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 07:09:41 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 07:09:48 visual_prompt]: Epoch 11 / 100: avg data time: 4.75e-02, avg batch time: 0.4947, average train loss: 207.5631
[09/26 07:09:49 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1688, average loss: 199.6588
[09/26 07:09:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.50	
[09/26 07:09:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 07:09:56 visual_prompt]: Epoch 12 / 100: avg data time: 5.48e-02, avg batch time: 0.4995, average train loss: 213.5150
[09/26 07:09:57 visual_prompt]: Inference (val):avg data time: 1.69e-05, avg batch time: 0.1687, average loss: 181.1307
[09/26 07:09:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 82.50	
[09/26 07:09:57 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 07:10:04 visual_prompt]: Epoch 13 / 100: avg data time: 4.88e-02, avg batch time: 0.4950, average train loss: 164.8717
[09/26 07:10:06 visual_prompt]: Inference (val):avg data time: 1.81e-05, avg batch time: 0.1686, average loss: 127.2543
[09/26 07:10:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 83.00	
[09/26 07:10:06 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 07:10:12 visual_prompt]: Epoch 14 / 100: avg data time: 5.34e-02, avg batch time: 0.5006, average train loss: 206.3848
[09/26 07:10:14 visual_prompt]: Inference (val):avg data time: 1.67e-05, avg batch time: 0.1690, average loss: 177.4199
[09/26 07:10:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 07:10:14 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 07:10:20 visual_prompt]: Epoch 15 / 100: avg data time: 4.34e-02, avg batch time: 0.4932, average train loss: 146.0318
[09/26 07:10:22 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1692, average loss: 224.6437
[09/26 07:10:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 07:10:22 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 07:10:28 visual_prompt]: Epoch 16 / 100: avg data time: 5.68e-02, avg batch time: 0.5033, average train loss: 234.9236
[09/26 07:10:30 visual_prompt]: Inference (val):avg data time: 1.73e-05, avg batch time: 0.1693, average loss: 299.4549
[09/26 07:10:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:10:30 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 07:10:36 visual_prompt]: Epoch 17 / 100: avg data time: 3.85e-02, avg batch time: 0.4881, average train loss: 280.2386
[09/26 07:10:38 visual_prompt]: Inference (val):avg data time: 1.84e-05, avg batch time: 0.1690, average loss: 212.0100
[09/26 07:10:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 07:10:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 07:10:44 visual_prompt]: Epoch 18 / 100: avg data time: 3.94e-02, avg batch time: 0.4877, average train loss: 296.3024
[09/26 07:10:46 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1692, average loss: 201.9834
[09/26 07:10:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:10:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 07:10:52 visual_prompt]: Epoch 19 / 100: avg data time: 3.83e-02, avg batch time: 0.4866, average train loss: 244.2814
[09/26 07:10:54 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1693, average loss: 264.4072
[09/26 07:10:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 07:10:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 07:11:00 visual_prompt]: Epoch 20 / 100: avg data time: 4.32e-02, avg batch time: 0.4936, average train loss: 215.2137
[09/26 07:11:02 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1695, average loss: 272.3279
[09/26 07:11:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:11:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 07:11:08 visual_prompt]: Epoch 21 / 100: avg data time: 4.92e-02, avg batch time: 0.4973, average train loss: 171.0925
[09/26 07:11:10 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1697, average loss: 277.7048
[09/26 07:11:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:11:10 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 07:11:16 visual_prompt]: Epoch 22 / 100: avg data time: 4.69e-02, avg batch time: 0.4980, average train loss: 212.9431
[09/26 07:11:18 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1694, average loss: 310.2446
[09/26 07:11:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:11:18 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 07:11:25 visual_prompt]: Epoch 23 / 100: avg data time: 4.28e-02, avg batch time: 0.4924, average train loss: 197.7394
[09/26 07:11:26 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1693, average loss: 218.2668
[09/26 07:11:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:11:26 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 07:11:33 visual_prompt]: Epoch 24 / 100: avg data time: 4.76e-02, avg batch time: 0.4952, average train loss: 130.7100
[09/26 07:11:34 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1700, average loss: 250.3922
[09/26 07:11:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:11:34 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 07:11:41 visual_prompt]: Epoch 25 / 100: avg data time: 4.60e-02, avg batch time: 0.4949, average train loss: 138.6392
[09/26 07:11:42 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1698, average loss: 120.5774
[09/26 07:11:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 83.50	
[09/26 07:11:42 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 07:11:49 visual_prompt]: Epoch 26 / 100: avg data time: 4.81e-02, avg batch time: 0.4964, average train loss: 171.1017
[09/26 07:11:50 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1696, average loss: 155.3933
[09/26 07:11:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 83.00	
[09/26 07:11:50 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 07:11:57 visual_prompt]: Epoch 27 / 100: avg data time: 5.86e-02, avg batch time: 0.5078, average train loss: 136.3712
[09/26 07:11:58 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1691, average loss: 113.7735
[09/26 07:11:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:11:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 07:12:05 visual_prompt]: Epoch 28 / 100: avg data time: 5.54e-02, avg batch time: 0.5039, average train loss: 179.2832
[09/26 07:12:07 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1694, average loss: 224.4411
[09/26 07:12:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:12:07 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 07:12:13 visual_prompt]: Epoch 29 / 100: avg data time: 4.74e-02, avg batch time: 0.4967, average train loss: 168.9937
[09/26 07:12:15 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1696, average loss: 147.6178
[09/26 07:12:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:12:15 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 07:12:21 visual_prompt]: Epoch 30 / 100: avg data time: 5.04e-02, avg batch time: 0.4980, average train loss: 208.7784
[09/26 07:12:23 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1700, average loss: 95.2344
[09/26 07:12:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 85.50	
[09/26 07:12:23 visual_prompt]: Best epoch 30: best metric: 0.215
[09/26 07:12:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 07:12:29 visual_prompt]: Epoch 31 / 100: avg data time: 4.88e-02, avg batch time: 0.4990, average train loss: 163.4982
[09/26 07:12:31 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1696, average loss: 154.0623
[09/26 07:12:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 07:12:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 07:12:37 visual_prompt]: Epoch 32 / 100: avg data time: 5.30e-02, avg batch time: 0.5017, average train loss: 159.3822
[09/26 07:12:39 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1696, average loss: 120.3696
[09/26 07:12:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.00	top5: 84.50	
[09/26 07:12:39 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 07:12:45 visual_prompt]: Epoch 33 / 100: avg data time: 4.68e-02, avg batch time: 0.4952, average train loss: 182.9663
[09/26 07:12:47 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1695, average loss: 117.3820
[09/26 07:12:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:12:47 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 07:12:53 visual_prompt]: Epoch 34 / 100: avg data time: 3.89e-02, avg batch time: 0.4873, average train loss: 228.7247
[09/26 07:12:55 visual_prompt]: Inference (val):avg data time: 1.87e-05, avg batch time: 0.1695, average loss: 126.7583
[09/26 07:12:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 07:12:55 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 07:13:01 visual_prompt]: Epoch 35 / 100: avg data time: 5.16e-02, avg batch time: 0.5003, average train loss: 214.8207
[09/26 07:13:03 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1697, average loss: 222.9436
[09/26 07:13:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 07:13:03 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 07:13:09 visual_prompt]: Epoch 36 / 100: avg data time: 4.59e-02, avg batch time: 0.4963, average train loss: 201.2073
[09/26 07:13:11 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1692, average loss: 107.5685
[09/26 07:13:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:13:11 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 07:13:18 visual_prompt]: Epoch 37 / 100: avg data time: 4.50e-02, avg batch time: 0.4959, average train loss: 189.9348
[09/26 07:13:19 visual_prompt]: Inference (val):avg data time: 1.50e-05, avg batch time: 0.1700, average loss: 207.4006
[09/26 07:13:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:13:19 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 07:13:25 visual_prompt]: Epoch 38 / 100: avg data time: 3.75e-02, avg batch time: 0.4879, average train loss: 169.1002
[09/26 07:13:27 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1699, average loss: 172.6866
[09/26 07:13:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:13:27 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 07:13:33 visual_prompt]: Epoch 39 / 100: avg data time: 4.28e-02, avg batch time: 0.4917, average train loss: 225.6362
[09/26 07:13:35 visual_prompt]: Inference (val):avg data time: 1.51e-05, avg batch time: 0.1700, average loss: 402.1622
[09/26 07:13:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 07:13:35 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 07:13:42 visual_prompt]: Epoch 40 / 100: avg data time: 5.06e-02, avg batch time: 0.4990, average train loss: 219.9525
[09/26 07:13:43 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1699, average loss: 124.1796
[09/26 07:13:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:13:43 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 07:13:50 visual_prompt]: Epoch 41 / 100: avg data time: 5.46e-02, avg batch time: 0.5033, average train loss: 172.1773
[09/26 07:13:51 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1697, average loss: 59.2299
[09/26 07:13:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:13:51 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 07:13:58 visual_prompt]: Epoch 42 / 100: avg data time: 5.33e-02, avg batch time: 0.5028, average train loss: 122.3804
[09/26 07:14:00 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1696, average loss: 103.8018
[09/26 07:14:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:14:00 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 07:14:06 visual_prompt]: Epoch 43 / 100: avg data time: 4.22e-02, avg batch time: 0.4919, average train loss: 135.2381
[09/26 07:14:08 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1696, average loss: 206.2713
[09/26 07:14:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:14:08 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 07:14:14 visual_prompt]: Epoch 44 / 100: avg data time: 4.55e-02, avg batch time: 0.4947, average train loss: 151.6706
[09/26 07:14:16 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1696, average loss: 121.8711
[09/26 07:14:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 84.00	
[09/26 07:14:16 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 07:14:22 visual_prompt]: Epoch 45 / 100: avg data time: 4.71e-02, avg batch time: 0.4964, average train loss: 126.1579
[09/26 07:14:24 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1699, average loss: 147.0016
[09/26 07:14:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 07:14:24 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 07:14:30 visual_prompt]: Epoch 46 / 100: avg data time: 5.43e-02, avg batch time: 0.5024, average train loss: 143.6475
[09/26 07:14:32 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1697, average loss: 165.4418
[09/26 07:14:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:14:32 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 07:14:38 visual_prompt]: Epoch 47 / 100: avg data time: 3.66e-02, avg batch time: 0.4891, average train loss: 122.1882
[09/26 07:14:40 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1697, average loss: 117.4420
[09/26 07:14:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 07:14:40 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 07:14:47 visual_prompt]: Epoch 48 / 100: avg data time: 4.29e-02, avg batch time: 0.4939, average train loss: 159.1968
[09/26 07:14:48 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1696, average loss: 150.2491
[09/26 07:14:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:14:48 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 07:14:55 visual_prompt]: Epoch 49 / 100: avg data time: 3.64e-02, avg batch time: 0.4853, average train loss: 116.2831
[09/26 07:14:56 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1697, average loss: 148.5823
[09/26 07:14:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 07:14:56 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 07:15:03 visual_prompt]: Epoch 50 / 100: avg data time: 5.10e-02, avg batch time: 0.4996, average train loss: 110.2527
[09/26 07:15:04 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1701, average loss: 68.0252
[09/26 07:15:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 07:15:04 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 07:15:11 visual_prompt]: Epoch 51 / 100: avg data time: 4.66e-02, avg batch time: 0.4958, average train loss: 109.7621
[09/26 07:15:12 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1703, average loss: 54.7464
[09/26 07:15:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:15:12 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 07:15:19 visual_prompt]: Epoch 52 / 100: avg data time: 5.60e-02, avg batch time: 0.5066, average train loss: 121.7261
[09/26 07:15:20 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.1697, average loss: 140.9615
[09/26 07:15:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:15:20 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 07:15:27 visual_prompt]: Epoch 53 / 100: avg data time: 5.32e-02, avg batch time: 0.5042, average train loss: 109.8219
[09/26 07:15:29 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1699, average loss: 284.0278
[09/26 07:15:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:15:29 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 07:15:35 visual_prompt]: Epoch 54 / 100: avg data time: 4.94e-02, avg batch time: 0.4986, average train loss: 142.2796
[09/26 07:15:37 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1704, average loss: 124.9539
[09/26 07:15:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:15:37 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 07:15:44 visual_prompt]: Epoch 55 / 100: avg data time: 5.31e-02, avg batch time: 0.5019, average train loss: 144.0543
[09/26 07:15:45 visual_prompt]: Inference (val):avg data time: 1.67e-05, avg batch time: 0.1697, average loss: 134.8330
[09/26 07:15:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 07:15:45 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 07:15:52 visual_prompt]: Epoch 56 / 100: avg data time: 5.13e-02, avg batch time: 0.4996, average train loss: 99.6698
[09/26 07:15:53 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1693, average loss: 97.5640
[09/26 07:15:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:15:53 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 07:16:00 visual_prompt]: Epoch 57 / 100: avg data time: 5.14e-02, avg batch time: 0.4999, average train loss: 108.1953
[09/26 07:16:01 visual_prompt]: Inference (val):avg data time: 1.73e-05, avg batch time: 0.1696, average loss: 136.5305
[09/26 07:16:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:16:01 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 07:16:08 visual_prompt]: Epoch 58 / 100: avg data time: 5.39e-02, avg batch time: 0.5041, average train loss: 103.4414
[09/26 07:16:09 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1697, average loss: 105.4503
[09/26 07:16:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:16:09 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 07:16:16 visual_prompt]: Epoch 59 / 100: avg data time: 5.07e-02, avg batch time: 0.4999, average train loss: 91.0013
[09/26 07:16:18 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1698, average loss: 50.4034
[09/26 07:16:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 84.50	
[09/26 07:16:18 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 07:16:24 visual_prompt]: Epoch 60 / 100: avg data time: 4.75e-02, avg batch time: 0.4970, average train loss: 94.4935
[09/26 07:16:26 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1697, average loss: 116.8914
[09/26 07:16:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:16:26 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 07:16:32 visual_prompt]: Epoch 61 / 100: avg data time: 4.83e-02, avg batch time: 0.4989, average train loss: 99.6112
[09/26 07:16:34 visual_prompt]: Inference (val):avg data time: 1.78e-05, avg batch time: 0.1700, average loss: 130.6357
[09/26 07:16:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:16:34 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 07:16:40 visual_prompt]: Epoch 62 / 100: avg data time: 4.21e-02, avg batch time: 0.4914, average train loss: 93.5735
[09/26 07:16:42 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1697, average loss: 68.7166
[09/26 07:16:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:16:42 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 07:16:48 visual_prompt]: Epoch 63 / 100: avg data time: 3.74e-02, avg batch time: 0.4888, average train loss: 72.5628
[09/26 07:16:50 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1696, average loss: 82.8866
[09/26 07:16:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:16:50 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 07:16:56 visual_prompt]: Epoch 64 / 100: avg data time: 5.55e-02, avg batch time: 0.5051, average train loss: 78.4476
[09/26 07:16:58 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1702, average loss: 31.2652
[09/26 07:16:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 07:16:58 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 07:17:05 visual_prompt]: Epoch 65 / 100: avg data time: 5.21e-02, avg batch time: 0.5022, average train loss: 74.5505
[09/26 07:17:06 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1695, average loss: 60.4899
[09/26 07:17:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 07:17:06 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 07:17:13 visual_prompt]: Epoch 66 / 100: avg data time: 3.84e-02, avg batch time: 0.4907, average train loss: 78.1921
[09/26 07:17:14 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1694, average loss: 76.3595
[09/26 07:17:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:17:14 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 07:17:21 visual_prompt]: Epoch 67 / 100: avg data time: 5.00e-02, avg batch time: 0.4989, average train loss: 73.0589
[09/26 07:17:22 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1695, average loss: 76.7135
[09/26 07:17:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:17:22 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 07:17:29 visual_prompt]: Epoch 68 / 100: avg data time: 5.60e-02, avg batch time: 0.5057, average train loss: 78.9286
[09/26 07:17:30 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1697, average loss: 70.3991
[09/26 07:17:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 07:17:30 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 07:17:37 visual_prompt]: Epoch 69 / 100: avg data time: 5.03e-02, avg batch time: 0.4991, average train loss: 55.7160
[09/26 07:17:38 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1693, average loss: 81.5883
[09/26 07:17:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:17:38 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 07:17:45 visual_prompt]: Epoch 70 / 100: avg data time: 3.96e-02, avg batch time: 0.4915, average train loss: 54.1869
[09/26 07:17:47 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1695, average loss: 86.1268
[09/26 07:17:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 07:17:47 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 07:17:53 visual_prompt]: Epoch 71 / 100: avg data time: 5.31e-02, avg batch time: 0.5021, average train loss: 45.2180
[09/26 07:17:55 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1697, average loss: 40.9121
[09/26 07:17:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:17:55 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 07:18:01 visual_prompt]: Epoch 72 / 100: avg data time: 5.52e-02, avg batch time: 0.5053, average train loss: 44.2814
[09/26 07:18:03 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1699, average loss: 48.0464
[09/26 07:18:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:18:03 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 07:18:09 visual_prompt]: Epoch 73 / 100: avg data time: 3.75e-02, avg batch time: 0.4910, average train loss: 46.9967
[09/26 07:18:11 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1701, average loss: 45.5295
[09/26 07:18:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 81.00	
[09/26 07:18:11 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 07:18:18 visual_prompt]: Epoch 74 / 100: avg data time: 5.53e-02, avg batch time: 0.5053, average train loss: 46.0539
[09/26 07:18:19 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1696, average loss: 46.1554
[09/26 07:18:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:18:19 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 07:18:26 visual_prompt]: Epoch 75 / 100: avg data time: 5.64e-02, avg batch time: 0.5063, average train loss: 40.3092
[09/26 07:18:27 visual_prompt]: Inference (val):avg data time: 4.57e-05, avg batch time: 0.1701, average loss: 37.8751
[09/26 07:18:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:18:27 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 07:18:34 visual_prompt]: Epoch 76 / 100: avg data time: 3.87e-02, avg batch time: 0.4898, average train loss: 43.8158
[09/26 07:18:35 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1696, average loss: 35.3721
[09/26 07:18:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 07:18:35 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 07:18:42 visual_prompt]: Epoch 77 / 100: avg data time: 4.85e-02, avg batch time: 0.4992, average train loss: 31.9771
[09/26 07:18:43 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1698, average loss: 28.4440
[09/26 07:18:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:18:43 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 07:18:50 visual_prompt]: Epoch 78 / 100: avg data time: 4.89e-02, avg batch time: 0.4986, average train loss: 30.7906
[09/26 07:18:52 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1699, average loss: 20.9913
[09/26 07:18:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:18:52 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 07:18:58 visual_prompt]: Epoch 79 / 100: avg data time: 5.52e-02, avg batch time: 0.5040, average train loss: 22.3882
[09/26 07:19:00 visual_prompt]: Inference (val):avg data time: 1.88e-05, avg batch time: 0.1699, average loss: 27.8736
[09/26 07:19:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 07:19:00 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 07:19:06 visual_prompt]: Epoch 80 / 100: avg data time: 4.66e-02, avg batch time: 0.4955, average train loss: 27.7142
[09/26 07:19:08 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1696, average loss: 11.8007
[09/26 07:19:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:19:08 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 07:19:15 visual_prompt]: Epoch 81 / 100: avg data time: 4.47e-02, avg batch time: 0.4942, average train loss: 18.3313
[09/26 07:19:16 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1696, average loss: 45.6756
[09/26 07:19:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:19:16 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 07:19:23 visual_prompt]: Epoch 82 / 100: avg data time: 4.90e-02, avg batch time: 0.4976, average train loss: 20.8993
[09/26 07:19:24 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1696, average loss: 16.2307
[09/26 07:19:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:19:24 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 07:19:31 visual_prompt]: Epoch 83 / 100: avg data time: 4.90e-02, avg batch time: 0.4987, average train loss: 23.2047
[09/26 07:19:32 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1700, average loss: 16.2298
[09/26 07:19:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:19:32 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 07:19:39 visual_prompt]: Epoch 84 / 100: avg data time: 4.20e-02, avg batch time: 0.4924, average train loss: 14.2061
[09/26 07:19:40 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1695, average loss: 15.5569
[09/26 07:19:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 12.50	top5: 78.00	
[09/26 07:19:40 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 07:19:47 visual_prompt]: Epoch 85 / 100: avg data time: 3.81e-02, avg batch time: 0.4886, average train loss: 15.9284
[09/26 07:19:48 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1697, average loss: 13.1328
[09/26 07:19:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:19:48 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 07:19:55 visual_prompt]: Epoch 86 / 100: avg data time: 4.09e-02, avg batch time: 0.4924, average train loss: 12.1732
[09/26 07:19:56 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1699, average loss: 8.0084
[09/26 07:19:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:19:56 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 07:20:03 visual_prompt]: Epoch 87 / 100: avg data time: 5.55e-02, avg batch time: 0.5051, average train loss: 7.2560
[09/26 07:20:04 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 9.7757
[09/26 07:20:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:20:04 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 07:20:11 visual_prompt]: Epoch 88 / 100: avg data time: 5.26e-02, avg batch time: 0.5018, average train loss: 6.4015
[09/26 07:20:13 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1697, average loss: 3.6279
[09/26 07:20:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:20:13 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 07:20:19 visual_prompt]: Epoch 89 / 100: avg data time: 4.86e-02, avg batch time: 0.4982, average train loss: 5.5795
[09/26 07:20:21 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1694, average loss: 4.7671
[09/26 07:20:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:20:21 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 07:20:27 visual_prompt]: Epoch 90 / 100: avg data time: 4.88e-02, avg batch time: 0.4981, average train loss: 4.3127
[09/26 07:20:29 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1701, average loss: 4.6708
[09/26 07:20:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 83.00	
[09/26 07:20:29 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 07:20:35 visual_prompt]: Epoch 91 / 100: avg data time: 4.94e-02, avg batch time: 0.5001, average train loss: 3.1853
[09/26 07:20:37 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1704, average loss: 2.4124
[09/26 07:20:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:20:37 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 07:20:43 visual_prompt]: Epoch 92 / 100: avg data time: 4.11e-02, avg batch time: 0.4900, average train loss: 1.9871
[09/26 07:20:45 visual_prompt]: Inference (val):avg data time: 1.70e-05, avg batch time: 0.1700, average loss: 2.8051
[09/26 07:20:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:20:45 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 07:20:52 visual_prompt]: Epoch 93 / 100: avg data time: 5.43e-02, avg batch time: 0.5050, average train loss: 1.9677
[09/26 07:20:53 visual_prompt]: Inference (val):avg data time: 1.70e-05, avg batch time: 0.1700, average loss: 1.9573
[09/26 07:20:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:20:53 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 07:20:59 visual_prompt]: Epoch 94 / 100: avg data time: 3.80e-02, avg batch time: 0.4880, average train loss: 1.8659
[09/26 07:21:01 visual_prompt]: Inference (val):avg data time: 1.84e-05, avg batch time: 0.1699, average loss: 1.8156
[09/26 07:21:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:21:01 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 07:21:08 visual_prompt]: Epoch 95 / 100: avg data time: 5.42e-02, avg batch time: 0.5035, average train loss: 1.8219
[09/26 07:21:09 visual_prompt]: Inference (val):avg data time: 1.89e-05, avg batch time: 0.1697, average loss: 1.7942
[09/26 07:21:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:21:09 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 07:21:16 visual_prompt]: Epoch 96 / 100: avg data time: 5.35e-02, avg batch time: 0.5025, average train loss: 1.7849
[09/26 07:21:17 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1693, average loss: 1.8663
[09/26 07:21:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:21:17 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 07:21:24 visual_prompt]: Epoch 97 / 100: avg data time: 3.91e-02, avg batch time: 0.4900, average train loss: 1.8245
[09/26 07:21:25 visual_prompt]: Inference (val):avg data time: 1.82e-05, avg batch time: 0.1695, average loss: 1.8268
[09/26 07:21:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:21:25 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 07:21:32 visual_prompt]: Epoch 98 / 100: avg data time: 4.95e-02, avg batch time: 0.5000, average train loss: 1.7798
[09/26 07:21:33 visual_prompt]: Inference (val):avg data time: 1.65e-05, avg batch time: 0.1697, average loss: 1.8027
[09/26 07:21:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 84.00	
[09/26 07:21:33 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 07:21:40 visual_prompt]: Epoch 99 / 100: avg data time: 6.33e-02, avg batch time: 0.5134, average train loss: 1.7664
[09/26 07:21:42 visual_prompt]: Inference (val):avg data time: 1.69e-05, avg batch time: 0.1697, average loss: 1.8147
[09/26 07:21:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:21:42 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 07:21:49 visual_prompt]: Epoch 100 / 100: avg data time: 4.98e-02, avg batch time: 0.5002, average train loss: 1.7688
[09/26 07:21:50 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1698, average loss: 1.8186
[09/26 07:21:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:21:50 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 07:21:50 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 07:21:50 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 07:21:50 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 07:21:50 visual_prompt]: Training with config:
[09/26 07:21:50 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 07:21:50 visual_prompt]: Loading training data...
[09/26 07:21:50 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 07:21:51 visual_prompt]: Number of images: 800
[09/26 07:21:51 visual_prompt]: Number of classes: 6 / 6
[09/26 07:21:51 visual_prompt]: Loading validation data...
[09/26 07:21:51 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 07:21:52 visual_prompt]: Number of images: 200
[09/26 07:21:52 visual_prompt]: Number of classes: 6 / 6
[09/26 07:21:52 visual_prompt]: Constructing models...
[09/26 07:21:54 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 07:21:54 visual_prompt]: tuned percent:0.540
[09/26 07:21:54 visual_prompt]: Device used for model: 0
[09/26 07:21:54 visual_prompt]: Setting up Evaluator...
[09/26 07:21:54 visual_prompt]: Setting up Trainer...
[09/26 07:21:54 visual_prompt]: 	Setting up the optimizer...
[09/26 07:21:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 07:22:01 visual_prompt]: Epoch 1 / 100: avg data time: 5.26e-02, avg batch time: 0.5018, average train loss: 2.9774
[09/26 07:22:03 visual_prompt]: Inference (val):avg data time: 1.75e-05, avg batch time: 0.1688, average loss: 2.9268
[09/26 07:22:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:22:03 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 07:22:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 07:22:09 visual_prompt]: Epoch 2 / 100: avg data time: 4.61e-02, avg batch time: 0.4954, average train loss: 25.8438
[09/26 07:22:11 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1689, average loss: 39.6832
[09/26 07:22:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:22:11 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 07:22:18 visual_prompt]: Epoch 3 / 100: avg data time: 4.90e-02, avg batch time: 0.4988, average train loss: 50.9943
[09/26 07:22:19 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1693, average loss: 106.4839
[09/26 07:22:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:22:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 07:22:26 visual_prompt]: Epoch 4 / 100: avg data time: 4.74e-02, avg batch time: 0.4973, average train loss: 69.7961
[09/26 07:22:27 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1686, average loss: 75.9277
[09/26 07:22:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:22:27 visual_prompt]: Best epoch 4: best metric: 0.205
[09/26 07:22:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 07:22:34 visual_prompt]: Epoch 5 / 100: avg data time: 4.22e-02, avg batch time: 0.4907, average train loss: 68.6105
[09/26 07:22:35 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1691, average loss: 38.0813
[09/26 07:22:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:22:35 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 07:22:42 visual_prompt]: Epoch 6 / 100: avg data time: 5.16e-02, avg batch time: 0.4994, average train loss: 84.2270
[09/26 07:22:43 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1695, average loss: 101.4008
[09/26 07:22:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:22:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 07:22:50 visual_prompt]: Epoch 7 / 100: avg data time: 5.60e-02, avg batch time: 0.5038, average train loss: 102.1986
[09/26 07:22:52 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1687, average loss: 219.9241
[09/26 07:22:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:22:52 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 07:22:58 visual_prompt]: Epoch 8 / 100: avg data time: 4.25e-02, avg batch time: 0.4908, average train loss: 143.1342
[09/26 07:23:00 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1699, average loss: 85.1686
[09/26 07:23:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:23:00 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 07:23:06 visual_prompt]: Epoch 9 / 100: avg data time: 5.09e-02, avg batch time: 0.4983, average train loss: 141.1002
[09/26 07:23:08 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1692, average loss: 84.5268
[09/26 07:23:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:23:08 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 07:23:15 visual_prompt]: Epoch 10 / 100: avg data time: 5.00e-02, avg batch time: 0.4972, average train loss: 222.8322
[09/26 07:23:16 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1694, average loss: 224.5888
[09/26 07:23:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:23:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 07:23:23 visual_prompt]: Epoch 11 / 100: avg data time: 5.37e-02, avg batch time: 0.5018, average train loss: 224.5907
[09/26 07:23:24 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1700, average loss: 288.9675
[09/26 07:23:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 07:23:24 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 07:23:31 visual_prompt]: Epoch 12 / 100: avg data time: 4.44e-02, avg batch time: 0.4962, average train loss: 184.5108
[09/26 07:23:32 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1692, average loss: 240.6577
[09/26 07:23:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:23:32 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 07:23:39 visual_prompt]: Epoch 13 / 100: avg data time: 4.75e-02, avg batch time: 0.4968, average train loss: 182.2367
[09/26 07:23:41 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1692, average loss: 474.6330
[09/26 07:23:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:23:41 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 07:23:47 visual_prompt]: Epoch 14 / 100: avg data time: 5.65e-02, avg batch time: 0.5052, average train loss: 226.5373
[09/26 07:23:49 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1694, average loss: 220.8515
[09/26 07:23:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:23:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 07:23:56 visual_prompt]: Epoch 15 / 100: avg data time: 4.86e-02, avg batch time: 0.4973, average train loss: 149.2712
[09/26 07:23:57 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1906, average loss: 240.4171
[09/26 07:23:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 07:23:57 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 07:24:04 visual_prompt]: Epoch 16 / 100: avg data time: 5.01e-02, avg batch time: 0.4982, average train loss: 326.2294
[09/26 07:24:05 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 472.7133
[09/26 07:24:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:24:05 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 07:24:12 visual_prompt]: Epoch 17 / 100: avg data time: 5.36e-02, avg batch time: 0.5013, average train loss: 363.7028
[09/26 07:24:14 visual_prompt]: Inference (val):avg data time: 4.47e-05, avg batch time: 0.1701, average loss: 231.9898
[09/26 07:24:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 84.00	
[09/26 07:24:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 07:24:21 visual_prompt]: Epoch 18 / 100: avg data time: 5.35e-02, avg batch time: 0.5009, average train loss: 274.6805
[09/26 07:24:22 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1695, average loss: 343.3980
[09/26 07:24:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:24:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 07:24:29 visual_prompt]: Epoch 19 / 100: avg data time: 4.89e-02, avg batch time: 0.4971, average train loss: 203.4740
[09/26 07:24:30 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1693, average loss: 232.2393
[09/26 07:24:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 07:24:30 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 07:24:37 visual_prompt]: Epoch 20 / 100: avg data time: 4.91e-02, avg batch time: 0.5011, average train loss: 167.6468
[09/26 07:24:39 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1695, average loss: 180.3175
[09/26 07:24:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:24:39 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 07:24:45 visual_prompt]: Epoch 21 / 100: avg data time: 4.50e-02, avg batch time: 0.4936, average train loss: 256.5118
[09/26 07:24:47 visual_prompt]: Inference (val):avg data time: 1.88e-05, avg batch time: 0.1693, average loss: 290.1081
[09/26 07:24:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:24:47 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 07:24:53 visual_prompt]: Epoch 22 / 100: avg data time: 5.54e-02, avg batch time: 0.5031, average train loss: 229.6561
[09/26 07:24:55 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1694, average loss: 160.7403
[09/26 07:24:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:24:55 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 07:25:02 visual_prompt]: Epoch 23 / 100: avg data time: 5.68e-02, avg batch time: 0.5045, average train loss: 185.6785
[09/26 07:25:03 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1699, average loss: 608.6264
[09/26 07:25:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 07:25:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 07:25:10 visual_prompt]: Epoch 24 / 100: avg data time: 4.63e-02, avg batch time: 0.4956, average train loss: 222.1652
[09/26 07:25:11 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 129.8675
[09/26 07:25:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:25:11 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 07:25:18 visual_prompt]: Epoch 25 / 100: avg data time: 5.50e-02, avg batch time: 0.5031, average train loss: 214.8749
[09/26 07:25:20 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1692, average loss: 227.5024
[09/26 07:25:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 07:25:20 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 07:25:26 visual_prompt]: Epoch 26 / 100: avg data time: 6.38e-02, avg batch time: 0.5115, average train loss: 206.4772
[09/26 07:25:28 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 325.5303
[09/26 07:25:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:25:28 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 07:25:35 visual_prompt]: Epoch 27 / 100: avg data time: 5.83e-02, avg batch time: 0.5065, average train loss: 249.9685
[09/26 07:25:36 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1695, average loss: 184.9481
[09/26 07:25:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 84.00	
[09/26 07:25:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 07:25:43 visual_prompt]: Epoch 28 / 100: avg data time: 5.40e-02, avg batch time: 0.5024, average train loss: 202.5229
[09/26 07:25:44 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1699, average loss: 174.6764
[09/26 07:25:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:25:44 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 07:25:51 visual_prompt]: Epoch 29 / 100: avg data time: 3.53e-02, avg batch time: 0.4848, average train loss: 266.3558
[09/26 07:25:53 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1694, average loss: 170.7467
[09/26 07:25:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:25:53 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 07:25:59 visual_prompt]: Epoch 30 / 100: avg data time: 5.29e-02, avg batch time: 0.5011, average train loss: 126.4657
[09/26 07:26:01 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1695, average loss: 315.1309
[09/26 07:26:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:26:01 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 07:26:07 visual_prompt]: Epoch 31 / 100: avg data time: 3.97e-02, avg batch time: 0.4882, average train loss: 240.0366
[09/26 07:26:09 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1697, average loss: 333.9966
[09/26 07:26:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 07:26:09 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 07:26:15 visual_prompt]: Epoch 32 / 100: avg data time: 5.04e-02, avg batch time: 0.4997, average train loss: 267.3056
[09/26 07:26:17 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 325.7371
[09/26 07:26:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 07:26:17 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 07:26:24 visual_prompt]: Epoch 33 / 100: avg data time: 5.59e-02, avg batch time: 0.5039, average train loss: 153.4386
[09/26 07:26:25 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1688, average loss: 204.3904
[09/26 07:26:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:26:25 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 07:26:32 visual_prompt]: Epoch 34 / 100: avg data time: 4.39e-02, avg batch time: 0.4915, average train loss: 164.2635
[09/26 07:26:33 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1694, average loss: 216.2260
[09/26 07:26:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 07:26:33 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 07:26:40 visual_prompt]: Epoch 35 / 100: avg data time: 5.11e-02, avg batch time: 0.4989, average train loss: 285.5659
[09/26 07:26:42 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1691, average loss: 238.4243
[09/26 07:26:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:26:42 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 07:26:48 visual_prompt]: Epoch 36 / 100: avg data time: 4.15e-02, avg batch time: 0.4943, average train loss: 259.0110
[09/26 07:26:50 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1694, average loss: 105.3924
[09/26 07:26:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 79.00	
[09/26 07:26:50 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 07:26:56 visual_prompt]: Epoch 37 / 100: avg data time: 4.22e-02, avg batch time: 0.4911, average train loss: 182.2152
[09/26 07:26:58 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1695, average loss: 208.4955
[09/26 07:26:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:26:58 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 07:27:05 visual_prompt]: Epoch 38 / 100: avg data time: 4.81e-02, avg batch time: 0.4963, average train loss: 160.9058
[09/26 07:27:06 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 197.8653
[09/26 07:27:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 85.50	
[09/26 07:27:06 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 07:27:13 visual_prompt]: Epoch 39 / 100: avg data time: 4.87e-02, avg batch time: 0.4984, average train loss: 166.9191
[09/26 07:27:14 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1697, average loss: 186.5115
[09/26 07:27:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:27:14 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 07:27:21 visual_prompt]: Epoch 40 / 100: avg data time: 6.17e-02, avg batch time: 0.5101, average train loss: 140.1202
[09/26 07:27:23 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1698, average loss: 120.7395
[09/26 07:27:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 07:27:23 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 07:27:30 visual_prompt]: Epoch 41 / 100: avg data time: 6.26e-02, avg batch time: 0.5108, average train loss: 164.3845
[09/26 07:27:31 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1691, average loss: 91.5424
[09/26 07:27:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 07:27:31 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 07:27:38 visual_prompt]: Epoch 42 / 100: avg data time: 5.46e-02, avg batch time: 0.5027, average train loss: 124.8859
[09/26 07:27:39 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1694, average loss: 55.3464
[09/26 07:27:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 07:27:39 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 07:27:46 visual_prompt]: Epoch 43 / 100: avg data time: 5.01e-02, avg batch time: 0.4990, average train loss: 163.3126
[09/26 07:27:48 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1696, average loss: 201.3174
[09/26 07:27:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:27:48 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 07:27:54 visual_prompt]: Epoch 44 / 100: avg data time: 4.11e-02, avg batch time: 0.4903, average train loss: 169.8542
[09/26 07:27:56 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1706, average loss: 85.6648
[09/26 07:27:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:27:56 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 07:28:03 visual_prompt]: Epoch 45 / 100: avg data time: 5.33e-02, avg batch time: 0.5006, average train loss: 214.4186
[09/26 07:28:04 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 131.4332
[09/26 07:28:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 07:28:04 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 07:28:11 visual_prompt]: Epoch 46 / 100: avg data time: 4.55e-02, avg batch time: 0.4958, average train loss: 150.5441
[09/26 07:28:12 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 160.8736
[09/26 07:28:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.00	top5: 85.50	
[09/26 07:28:12 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 07:28:19 visual_prompt]: Epoch 47 / 100: avg data time: 4.75e-02, avg batch time: 0.4962, average train loss: 144.1064
[09/26 07:28:20 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1704, average loss: 170.5159
[09/26 07:28:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:28:20 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 07:28:27 visual_prompt]: Epoch 48 / 100: avg data time: 4.92e-02, avg batch time: 0.4984, average train loss: 130.7168
[09/26 07:28:28 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1695, average loss: 129.9416
[09/26 07:28:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:28:28 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 07:28:35 visual_prompt]: Epoch 49 / 100: avg data time: 4.77e-02, avg batch time: 0.4952, average train loss: 168.9823
[09/26 07:28:37 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1696, average loss: 80.3736
[09/26 07:28:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 07:28:37 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 07:28:43 visual_prompt]: Epoch 50 / 100: avg data time: 5.86e-02, avg batch time: 0.5065, average train loss: 140.3280
[09/26 07:28:45 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 80.4228
[09/26 07:28:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 07:28:45 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 07:28:52 visual_prompt]: Epoch 51 / 100: avg data time: 5.19e-02, avg batch time: 0.5005, average train loss: 132.9864
[09/26 07:28:53 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1691, average loss: 160.2013
[09/26 07:28:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:28:53 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 07:29:00 visual_prompt]: Epoch 52 / 100: avg data time: 5.76e-02, avg batch time: 0.5069, average train loss: 126.5649
[09/26 07:29:02 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1696, average loss: 213.2109
[09/26 07:29:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:29:02 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 07:29:08 visual_prompt]: Epoch 53 / 100: avg data time: 4.33e-02, avg batch time: 0.4924, average train loss: 142.7728
[09/26 07:29:10 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1694, average loss: 289.5429
[09/26 07:29:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:29:10 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 07:29:16 visual_prompt]: Epoch 54 / 100: avg data time: 4.81e-02, avg batch time: 0.4978, average train loss: 152.3351
[09/26 07:29:18 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1697, average loss: 58.1756
[09/26 07:29:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:29:18 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 07:29:25 visual_prompt]: Epoch 55 / 100: avg data time: 5.01e-02, avg batch time: 0.5009, average train loss: 112.5625
[09/26 07:29:26 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1695, average loss: 106.0421
[09/26 07:29:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:29:26 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 07:29:33 visual_prompt]: Epoch 56 / 100: avg data time: 4.11e-02, avg batch time: 0.4902, average train loss: 200.2052
[09/26 07:29:34 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1698, average loss: 116.5988
[09/26 07:29:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 07:29:34 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 07:29:41 visual_prompt]: Epoch 57 / 100: avg data time: 4.58e-02, avg batch time: 0.4948, average train loss: 107.9456
[09/26 07:29:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 166.2494
[09/26 07:29:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:29:42 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 07:29:49 visual_prompt]: Epoch 58 / 100: avg data time: 5.13e-02, avg batch time: 0.5004, average train loss: 105.0725
[09/26 07:29:51 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1699, average loss: 157.4224
[09/26 07:29:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 07:29:51 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 07:29:57 visual_prompt]: Epoch 59 / 100: avg data time: 4.04e-02, avg batch time: 0.4923, average train loss: 117.6931
[09/26 07:29:59 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1695, average loss: 194.6600
[09/26 07:29:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:29:59 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 07:30:05 visual_prompt]: Epoch 60 / 100: avg data time: 5.64e-02, avg batch time: 0.5057, average train loss: 142.7187
[09/26 07:30:07 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 120.1990
[09/26 07:30:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:30:07 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 07:30:14 visual_prompt]: Epoch 61 / 100: avg data time: 4.36e-02, avg batch time: 0.4923, average train loss: 101.2486
[09/26 07:30:15 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1696, average loss: 63.8987
[09/26 07:30:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 07:30:15 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 07:30:22 visual_prompt]: Epoch 62 / 100: avg data time: 4.63e-02, avg batch time: 0.4956, average train loss: 141.8659
[09/26 07:30:23 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1695, average loss: 87.3578
[09/26 07:30:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:30:23 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 07:30:30 visual_prompt]: Epoch 63 / 100: avg data time: 5.52e-02, avg batch time: 0.5039, average train loss: 106.9199
[09/26 07:30:31 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1692, average loss: 55.7583
[09/26 07:30:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 07:30:31 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 07:30:38 visual_prompt]: Epoch 64 / 100: avg data time: 6.08e-02, avg batch time: 0.5094, average train loss: 69.6735
[09/26 07:30:40 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1695, average loss: 63.7554
[09/26 07:30:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:30:40 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 07:30:46 visual_prompt]: Epoch 65 / 100: avg data time: 4.70e-02, avg batch time: 0.4970, average train loss: 70.7043
[09/26 07:30:48 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1694, average loss: 71.5888
[09/26 07:30:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:30:48 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 07:30:54 visual_prompt]: Epoch 66 / 100: avg data time: 4.41e-02, avg batch time: 0.4938, average train loss: 76.6862
[09/26 07:30:56 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1695, average loss: 77.0121
[09/26 07:30:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:30:56 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 07:31:03 visual_prompt]: Epoch 67 / 100: avg data time: 4.99e-02, avg batch time: 0.4983, average train loss: 61.4387
[09/26 07:31:04 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1695, average loss: 22.7986
[09/26 07:31:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:31:04 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 07:31:11 visual_prompt]: Epoch 68 / 100: avg data time: 5.22e-02, avg batch time: 0.5006, average train loss: 48.3703
[09/26 07:31:12 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 51.1611
[09/26 07:31:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:31:12 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 07:31:19 visual_prompt]: Epoch 69 / 100: avg data time: 4.03e-02, avg batch time: 0.4904, average train loss: 56.2590
[09/26 07:31:20 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1698, average loss: 53.5000
[09/26 07:31:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 07:31:20 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 07:31:27 visual_prompt]: Epoch 70 / 100: avg data time: 4.29e-02, avg batch time: 0.4923, average train loss: 73.1907
[09/26 07:31:28 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 58.9076
[09/26 07:31:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:31:28 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 07:31:35 visual_prompt]: Epoch 71 / 100: avg data time: 4.82e-02, avg batch time: 0.4974, average train loss: 59.3937
[09/26 07:31:37 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1695, average loss: 40.2698
[09/26 07:31:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:31:37 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 07:31:43 visual_prompt]: Epoch 72 / 100: avg data time: 4.44e-02, avg batch time: 0.4927, average train loss: 37.0558
[09/26 07:31:45 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 50.3980
[09/26 07:31:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 07:31:45 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 07:31:51 visual_prompt]: Epoch 73 / 100: avg data time: 3.94e-02, avg batch time: 0.4900, average train loss: 41.6625
[09/26 07:31:53 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 47.7016
[09/26 07:31:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 07:31:53 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 07:31:59 visual_prompt]: Epoch 74 / 100: avg data time: 4.06e-02, avg batch time: 0.4907, average train loss: 69.4422
[09/26 07:32:01 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1692, average loss: 62.8521
[09/26 07:32:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:32:01 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 07:32:08 visual_prompt]: Epoch 75 / 100: avg data time: 4.73e-02, avg batch time: 0.4974, average train loss: 68.3368
[09/26 07:32:09 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 49.0474
[09/26 07:32:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 07:32:09 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 07:32:16 visual_prompt]: Epoch 76 / 100: avg data time: 5.57e-02, avg batch time: 0.5063, average train loss: 42.4335
[09/26 07:32:17 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1694, average loss: 37.4882
[09/26 07:32:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 07:32:17 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 07:32:24 visual_prompt]: Epoch 77 / 100: avg data time: 4.52e-02, avg batch time: 0.4953, average train loss: 24.7625
[09/26 07:32:26 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1691, average loss: 18.1137
[09/26 07:32:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:32:26 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 07:32:32 visual_prompt]: Epoch 78 / 100: avg data time: 4.80e-02, avg batch time: 0.4987, average train loss: 17.9555
[09/26 07:32:34 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1697, average loss: 33.1032
[09/26 07:32:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 07:32:34 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 07:32:41 visual_prompt]: Epoch 79 / 100: avg data time: 5.26e-02, avg batch time: 0.5024, average train loss: 28.5041
[09/26 07:32:42 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 19.4677
[09/26 07:32:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:32:42 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 07:32:49 visual_prompt]: Epoch 80 / 100: avg data time: 4.70e-02, avg batch time: 0.4959, average train loss: 20.1915
[09/26 07:32:50 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1695, average loss: 14.7533
[09/26 07:32:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 07:32:50 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 07:32:57 visual_prompt]: Epoch 81 / 100: avg data time: 5.39e-02, avg batch time: 0.5034, average train loss: 27.2755
[09/26 07:32:59 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1697, average loss: 26.7108
[09/26 07:32:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 87.50	
[09/26 07:32:59 visual_prompt]: Best epoch 81: best metric: 0.235
[09/26 07:32:59 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 07:33:05 visual_prompt]: Epoch 82 / 100: avg data time: 5.12e-02, avg batch time: 0.5005, average train loss: 33.2537
[09/26 07:33:07 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 23.4516
[09/26 07:33:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:33:07 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 07:33:13 visual_prompt]: Epoch 83 / 100: avg data time: 3.97e-02, avg batch time: 0.4902, average train loss: 29.0388
[09/26 07:33:15 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1692, average loss: 22.5147
[09/26 07:33:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:33:15 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 07:33:21 visual_prompt]: Epoch 84 / 100: avg data time: 3.89e-02, avg batch time: 0.4882, average train loss: 25.4577
[09/26 07:33:23 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1698, average loss: 36.8365
[09/26 07:33:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:33:23 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 07:33:30 visual_prompt]: Epoch 85 / 100: avg data time: 5.47e-02, avg batch time: 0.5032, average train loss: 13.0922
[09/26 07:33:31 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 7.7033
[09/26 07:33:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:33:31 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 07:33:38 visual_prompt]: Epoch 86 / 100: avg data time: 5.45e-02, avg batch time: 0.5038, average train loss: 8.3553
[09/26 07:33:39 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1691, average loss: 8.6557
[09/26 07:33:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 07:33:39 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 07:33:46 visual_prompt]: Epoch 87 / 100: avg data time: 4.00e-02, avg batch time: 0.4899, average train loss: 5.7775
[09/26 07:33:47 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1695, average loss: 2.9607
[09/26 07:33:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 07:33:47 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 07:33:54 visual_prompt]: Epoch 88 / 100: avg data time: 5.68e-02, avg batch time: 0.5051, average train loss: 3.2147
[09/26 07:33:56 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 2.6771
[09/26 07:33:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:33:56 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 07:34:03 visual_prompt]: Epoch 89 / 100: avg data time: 5.65e-02, avg batch time: 0.5041, average train loss: 2.2224
[09/26 07:34:04 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 2.2199
[09/26 07:34:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 07:34:04 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 07:34:11 visual_prompt]: Epoch 90 / 100: avg data time: 4.20e-02, avg batch time: 0.4915, average train loss: 2.1406
[09/26 07:34:12 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 2.0241
[09/26 07:34:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 07:34:12 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 07:34:19 visual_prompt]: Epoch 91 / 100: avg data time: 4.35e-02, avg batch time: 0.4944, average train loss: 1.9793
[09/26 07:34:20 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1689, average loss: 1.8966
[09/26 07:34:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:34:20 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 07:34:27 visual_prompt]: Epoch 92 / 100: avg data time: 5.39e-02, avg batch time: 0.5021, average train loss: 1.8984
[09/26 07:34:28 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1688, average loss: 2.0028
[09/26 07:34:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:34:28 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 07:34:35 visual_prompt]: Epoch 93 / 100: avg data time: 4.85e-02, avg batch time: 0.4972, average train loss: 1.8966
[09/26 07:34:36 visual_prompt]: Inference (val):avg data time: 1.76e-05, avg batch time: 0.1690, average loss: 2.0581
[09/26 07:34:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:34:37 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 07:34:43 visual_prompt]: Epoch 94 / 100: avg data time: 4.56e-02, avg batch time: 0.4945, average train loss: 1.8313
[09/26 07:34:45 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1690, average loss: 2.0306
[09/26 07:34:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:34:45 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 07:34:51 visual_prompt]: Epoch 95 / 100: avg data time: 4.42e-02, avg batch time: 0.4921, average train loss: 1.8183
[09/26 07:34:53 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 1.8369
[09/26 07:34:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:34:53 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 07:34:59 visual_prompt]: Epoch 96 / 100: avg data time: 4.21e-02, avg batch time: 0.4910, average train loss: 1.7811
[09/26 07:35:01 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 1.8603
[09/26 07:35:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:35:01 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 07:35:08 visual_prompt]: Epoch 97 / 100: avg data time: 4.83e-02, avg batch time: 0.4960, average train loss: 1.7785
[09/26 07:35:09 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1694, average loss: 1.8112
[09/26 07:35:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:35:09 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 07:35:16 visual_prompt]: Epoch 98 / 100: avg data time: 5.33e-02, avg batch time: 0.5018, average train loss: 1.7735
[09/26 07:35:17 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1693, average loss: 1.8707
[09/26 07:35:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:35:17 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 07:35:24 visual_prompt]: Epoch 99 / 100: avg data time: 4.20e-02, avg batch time: 0.4911, average train loss: 1.7733
[09/26 07:35:25 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1688, average loss: 1.8258
[09/26 07:35:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:35:25 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 07:35:32 visual_prompt]: Epoch 100 / 100: avg data time: 5.02e-02, avg batch time: 0.4990, average train loss: 1.7637
[09/26 07:35:33 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1692, average loss: 1.8238
[09/26 07:35:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:35:33 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 07:35:33 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 07:35:33 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 07:35:33 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 07:35:33 visual_prompt]: Training with config:
[09/26 07:35:33 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 07:35:33 visual_prompt]: Loading training data...
[09/26 07:35:33 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 07:35:35 visual_prompt]: Number of images: 800
[09/26 07:35:35 visual_prompt]: Number of classes: 6 / 6
[09/26 07:35:35 visual_prompt]: Loading validation data...
[09/26 07:35:35 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 07:35:35 visual_prompt]: Number of images: 200
[09/26 07:35:35 visual_prompt]: Number of classes: 6 / 6
[09/26 07:35:35 visual_prompt]: Constructing models...
[09/26 07:35:38 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 07:35:38 visual_prompt]: tuned percent:0.540
[09/26 07:35:38 visual_prompt]: Device used for model: 0
[09/26 07:35:38 visual_prompt]: Setting up Evaluator...
[09/26 07:35:38 visual_prompt]: Setting up Trainer...
[09/26 07:35:38 visual_prompt]: 	Setting up the optimizer...
[09/26 07:35:38 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 07:35:44 visual_prompt]: Epoch 1 / 100: avg data time: 4.55e-02, avg batch time: 0.4938, average train loss: 2.9917
[09/26 07:35:46 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1685, average loss: 2.9268
[09/26 07:35:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:35:46 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 07:35:46 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 07:35:53 visual_prompt]: Epoch 2 / 100: avg data time: 4.63e-02, avg batch time: 0.4951, average train loss: 24.6588
[09/26 07:35:54 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1688, average loss: 19.2762
[09/26 07:35:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:35:54 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 07:36:01 visual_prompt]: Epoch 3 / 100: avg data time: 4.75e-02, avg batch time: 0.4966, average train loss: 46.9680
[09/26 07:36:02 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 31.2304
[09/26 07:36:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:36:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 07:36:09 visual_prompt]: Epoch 4 / 100: avg data time: 5.20e-02, avg batch time: 0.5021, average train loss: 61.4512
[09/26 07:36:11 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.1702, average loss: 76.0230
[09/26 07:36:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:36:11 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 07:36:17 visual_prompt]: Epoch 5 / 100: avg data time: 4.41e-02, avg batch time: 0.4935, average train loss: 75.8965
[09/26 07:36:19 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 96.9812
[09/26 07:36:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:36:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 07:36:26 visual_prompt]: Epoch 6 / 100: avg data time: 5.16e-02, avg batch time: 0.4994, average train loss: 99.7226
[09/26 07:36:27 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1690, average loss: 161.5238
[09/26 07:36:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:36:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 07:36:34 visual_prompt]: Epoch 7 / 100: avg data time: 5.49e-02, avg batch time: 0.5023, average train loss: 102.0522
[09/26 07:36:35 visual_prompt]: Inference (val):avg data time: 4.92e-05, avg batch time: 0.1693, average loss: 107.5710
[09/26 07:36:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:36:35 visual_prompt]: Best epoch 7: best metric: 0.205
[09/26 07:36:35 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 07:36:42 visual_prompt]: Epoch 8 / 100: avg data time: 5.41e-02, avg batch time: 0.5019, average train loss: 126.4936
[09/26 07:36:43 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 280.5570
[09/26 07:36:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 86.00	
[09/26 07:36:43 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 07:36:50 visual_prompt]: Epoch 9 / 100: avg data time: 4.69e-02, avg batch time: 0.4949, average train loss: 142.9366
[09/26 07:36:52 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.1694, average loss: 177.7115
[09/26 07:36:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:36:52 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 07:36:58 visual_prompt]: Epoch 10 / 100: avg data time: 4.47e-02, avg batch time: 0.4937, average train loss: 137.4524
[09/26 07:37:00 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1693, average loss: 206.4678
[09/26 07:37:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:37:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 07:37:06 visual_prompt]: Epoch 11 / 100: avg data time: 4.29e-02, avg batch time: 0.4925, average train loss: 179.8729
[09/26 07:37:08 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1690, average loss: 170.8472
[09/26 07:37:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:37:08 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 07:37:15 visual_prompt]: Epoch 12 / 100: avg data time: 4.86e-02, avg batch time: 0.4980, average train loss: 119.6309
[09/26 07:37:16 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1697, average loss: 108.5695
[09/26 07:37:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 07:37:16 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 07:37:23 visual_prompt]: Epoch 13 / 100: avg data time: 5.29e-02, avg batch time: 0.5011, average train loss: 211.7003
[09/26 07:37:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1688, average loss: 158.0800
[09/26 07:37:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:37:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 07:37:31 visual_prompt]: Epoch 14 / 100: avg data time: 4.03e-02, avg batch time: 0.4906, average train loss: 277.6015
[09/26 07:37:32 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 464.9019
[09/26 07:37:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:37:32 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 07:37:39 visual_prompt]: Epoch 15 / 100: avg data time: 5.15e-02, avg batch time: 0.4983, average train loss: 205.1163
[09/26 07:37:41 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1692, average loss: 127.8370
[09/26 07:37:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:37:41 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 07:37:47 visual_prompt]: Epoch 16 / 100: avg data time: 5.38e-02, avg batch time: 0.5022, average train loss: 121.7992
[09/26 07:37:49 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 70.7061
[09/26 07:37:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.00	top5: 84.00	
[09/26 07:37:49 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 07:37:55 visual_prompt]: Epoch 17 / 100: avg data time: 4.22e-02, avg batch time: 0.4908, average train loss: 118.4428
[09/26 07:37:57 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 152.0302
[09/26 07:37:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 07:37:57 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 07:38:04 visual_prompt]: Epoch 18 / 100: avg data time: 5.33e-02, avg batch time: 0.5028, average train loss: 86.2715
[09/26 07:38:05 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1688, average loss: 48.3854
[09/26 07:38:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:38:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 07:38:12 visual_prompt]: Epoch 19 / 100: avg data time: 4.13e-02, avg batch time: 0.4908, average train loss: 114.1206
[09/26 07:38:13 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1693, average loss: 120.9248
[09/26 07:38:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 07:38:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 07:38:20 visual_prompt]: Epoch 20 / 100: avg data time: 4.13e-02, avg batch time: 0.4912, average train loss: 181.1449
[09/26 07:38:21 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1692, average loss: 189.1789
[09/26 07:38:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:38:21 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 07:38:28 visual_prompt]: Epoch 21 / 100: avg data time: 4.02e-02, avg batch time: 0.4913, average train loss: 201.5670
[09/26 07:38:29 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1698, average loss: 213.1733
[09/26 07:38:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 07:38:29 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 07:38:36 visual_prompt]: Epoch 22 / 100: avg data time: 4.53e-02, avg batch time: 0.4951, average train loss: 210.5607
[09/26 07:38:37 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1692, average loss: 322.1526
[09/26 07:38:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:38:37 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 07:38:44 visual_prompt]: Epoch 23 / 100: avg data time: 3.80e-02, avg batch time: 0.4859, average train loss: 213.2810
[09/26 07:38:45 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1697, average loss: 264.9918
[09/26 07:38:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:38:45 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 07:38:52 visual_prompt]: Epoch 24 / 100: avg data time: 4.46e-02, avg batch time: 0.4930, average train loss: 209.5330
[09/26 07:38:53 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 149.3116
[09/26 07:38:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:38:53 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 07:39:00 visual_prompt]: Epoch 25 / 100: avg data time: 4.99e-02, avg batch time: 0.4976, average train loss: 206.8730
[09/26 07:39:02 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1693, average loss: 133.8093
[09/26 07:39:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:39:02 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 07:39:08 visual_prompt]: Epoch 26 / 100: avg data time: 5.24e-02, avg batch time: 0.5018, average train loss: 174.2019
[09/26 07:39:10 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1696, average loss: 153.8193
[09/26 07:39:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:39:10 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 07:39:16 visual_prompt]: Epoch 27 / 100: avg data time: 4.39e-02, avg batch time: 0.4928, average train loss: 135.8202
[09/26 07:39:18 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1694, average loss: 139.1330
[09/26 07:39:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:39:18 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 07:39:24 visual_prompt]: Epoch 28 / 100: avg data time: 4.44e-02, avg batch time: 0.4926, average train loss: 126.8863
[09/26 07:39:26 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1696, average loss: 115.8993
[09/26 07:39:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 07:39:26 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 07:39:33 visual_prompt]: Epoch 29 / 100: avg data time: 4.28e-02, avg batch time: 0.4915, average train loss: 169.1670
[09/26 07:39:34 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1692, average loss: 295.1158
[09/26 07:39:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 07:39:34 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 07:39:41 visual_prompt]: Epoch 30 / 100: avg data time: 4.43e-02, avg batch time: 0.4931, average train loss: 203.4292
[09/26 07:39:42 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 303.3592
[09/26 07:39:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:39:42 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 07:39:49 visual_prompt]: Epoch 31 / 100: avg data time: 4.58e-02, avg batch time: 0.4955, average train loss: 150.6203
[09/26 07:39:50 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 198.1489
[09/26 07:39:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:39:50 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 07:39:57 visual_prompt]: Epoch 32 / 100: avg data time: 4.20e-02, avg batch time: 0.4926, average train loss: 175.6162
[09/26 07:39:58 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 95.7338
[09/26 07:39:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 07:39:58 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 07:40:05 visual_prompt]: Epoch 33 / 100: avg data time: 5.07e-02, avg batch time: 0.4994, average train loss: 96.1324
[09/26 07:40:07 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1693, average loss: 99.2576
[09/26 07:40:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 07:40:07 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 07:40:13 visual_prompt]: Epoch 34 / 100: avg data time: 5.30e-02, avg batch time: 0.5016, average train loss: 97.5417
[09/26 07:40:15 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1690, average loss: 86.6083
[09/26 07:40:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:40:15 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 07:40:21 visual_prompt]: Epoch 35 / 100: avg data time: 3.81e-02, avg batch time: 0.4898, average train loss: 119.6140
[09/26 07:40:23 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1694, average loss: 122.3349
[09/26 07:40:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:40:23 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 07:40:30 visual_prompt]: Epoch 36 / 100: avg data time: 4.63e-02, avg batch time: 0.4946, average train loss: 91.2745
[09/26 07:40:31 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1697, average loss: 85.2023
[09/26 07:40:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 07:40:31 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 07:40:38 visual_prompt]: Epoch 37 / 100: avg data time: 3.86e-02, avg batch time: 0.4901, average train loss: 118.7003
[09/26 07:40:39 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 146.6585
[09/26 07:40:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 07:40:39 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 07:40:46 visual_prompt]: Epoch 38 / 100: avg data time: 5.34e-02, avg batch time: 0.5025, average train loss: 124.4264
[09/26 07:40:47 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1690, average loss: 157.5647
[09/26 07:40:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:40:47 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 07:40:54 visual_prompt]: Epoch 39 / 100: avg data time: 4.21e-02, avg batch time: 0.4937, average train loss: 118.2786
[09/26 07:40:56 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 108.1432
[09/26 07:40:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 85.50	
[09/26 07:40:56 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 07:41:02 visual_prompt]: Epoch 40 / 100: avg data time: 4.42e-02, avg batch time: 0.4928, average train loss: 173.8890
[09/26 07:41:04 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1695, average loss: 142.3493
[09/26 07:41:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:41:04 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 07:41:10 visual_prompt]: Epoch 41 / 100: avg data time: 5.77e-02, avg batch time: 0.5060, average train loss: 132.7888
[09/26 07:41:12 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1698, average loss: 133.7492
[09/26 07:41:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:41:12 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 07:41:19 visual_prompt]: Epoch 42 / 100: avg data time: 5.42e-02, avg batch time: 0.5038, average train loss: 143.6702
[09/26 07:41:20 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 249.9032
[09/26 07:41:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:41:20 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 07:41:27 visual_prompt]: Epoch 43 / 100: avg data time: 4.18e-02, avg batch time: 0.4914, average train loss: 144.4488
[09/26 07:41:28 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1699, average loss: 101.8398
[09/26 07:41:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 07:41:28 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 07:41:35 visual_prompt]: Epoch 44 / 100: avg data time: 4.08e-02, avg batch time: 0.4906, average train loss: 182.9681
[09/26 07:41:36 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 195.1375
[09/26 07:41:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:41:36 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 07:41:43 visual_prompt]: Epoch 45 / 100: avg data time: 5.40e-02, avg batch time: 0.5018, average train loss: 218.1942
[09/26 07:41:45 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1691, average loss: 164.9835
[09/26 07:41:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:41:45 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 07:41:51 visual_prompt]: Epoch 46 / 100: avg data time: 6.01e-02, avg batch time: 0.5095, average train loss: 174.0476
[09/26 07:41:53 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 123.1171
[09/26 07:41:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:41:53 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 07:42:00 visual_prompt]: Epoch 47 / 100: avg data time: 4.95e-02, avg batch time: 0.4997, average train loss: 149.3025
[09/26 07:42:01 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 215.0848
[09/26 07:42:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:42:01 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 07:42:08 visual_prompt]: Epoch 48 / 100: avg data time: 5.44e-02, avg batch time: 0.5018, average train loss: 128.8332
[09/26 07:42:09 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1692, average loss: 67.0487
[09/26 07:42:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:42:09 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 07:42:16 visual_prompt]: Epoch 49 / 100: avg data time: 4.40e-02, avg batch time: 0.4927, average train loss: 108.7240
[09/26 07:42:17 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1692, average loss: 167.2549
[09/26 07:42:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:42:17 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 07:42:24 visual_prompt]: Epoch 50 / 100: avg data time: 4.52e-02, avg batch time: 0.4947, average train loss: 151.8037
[09/26 07:42:25 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1688, average loss: 149.1723
[09/26 07:42:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:42:25 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 07:42:32 visual_prompt]: Epoch 51 / 100: avg data time: 5.16e-02, avg batch time: 0.5005, average train loss: 96.2025
[09/26 07:42:34 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1687, average loss: 60.7903
[09/26 07:42:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:42:34 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 07:42:41 visual_prompt]: Epoch 52 / 100: avg data time: 6.26e-02, avg batch time: 0.5110, average train loss: 80.3142
[09/26 07:42:42 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1692, average loss: 142.0140
[09/26 07:42:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 07:42:42 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 07:42:49 visual_prompt]: Epoch 53 / 100: avg data time: 3.95e-02, avg batch time: 0.4913, average train loss: 101.6100
[09/26 07:42:50 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1688, average loss: 181.1197
[09/26 07:42:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:42:50 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 07:42:57 visual_prompt]: Epoch 54 / 100: avg data time: 4.33e-02, avg batch time: 0.4922, average train loss: 127.3834
[09/26 07:42:58 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1692, average loss: 180.0400
[09/26 07:42:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 07:42:58 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 07:43:05 visual_prompt]: Epoch 55 / 100: avg data time: 4.16e-02, avg batch time: 0.4906, average train loss: 109.3961
[09/26 07:43:06 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1693, average loss: 30.4406
[09/26 07:43:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:43:06 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 07:43:13 visual_prompt]: Epoch 56 / 100: avg data time: 4.56e-02, avg batch time: 0.4947, average train loss: 84.4332
[09/26 07:43:14 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 103.6592
[09/26 07:43:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 07:43:14 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 07:43:21 visual_prompt]: Epoch 57 / 100: avg data time: 4.64e-02, avg batch time: 0.4957, average train loss: 94.1611
[09/26 07:43:22 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1695, average loss: 92.3476
[09/26 07:43:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:43:22 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 07:43:29 visual_prompt]: Epoch 58 / 100: avg data time: 3.91e-02, avg batch time: 0.4893, average train loss: 73.2150
[09/26 07:43:30 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 46.4885
[09/26 07:43:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 85.50	
[09/26 07:43:30 visual_prompt]: Best epoch 58: best metric: 0.215
[09/26 07:43:30 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 07:43:37 visual_prompt]: Epoch 59 / 100: avg data time: 4.48e-02, avg batch time: 0.4946, average train loss: 71.7602
[09/26 07:43:38 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1694, average loss: 88.8236
[09/26 07:43:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:43:38 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 07:43:45 visual_prompt]: Epoch 60 / 100: avg data time: 5.03e-02, avg batch time: 0.5001, average train loss: 56.6181
[09/26 07:43:47 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1691, average loss: 116.7825
[09/26 07:43:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 07:43:47 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 07:43:53 visual_prompt]: Epoch 61 / 100: avg data time: 4.24e-02, avg batch time: 0.4934, average train loss: 94.1701
[09/26 07:43:55 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1696, average loss: 79.3247
[09/26 07:43:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 07:43:55 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 07:44:01 visual_prompt]: Epoch 62 / 100: avg data time: 4.52e-02, avg batch time: 0.4938, average train loss: 82.7837
[09/26 07:44:03 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1689, average loss: 115.7148
[09/26 07:44:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:44:03 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 07:44:09 visual_prompt]: Epoch 63 / 100: avg data time: 4.90e-02, avg batch time: 0.4973, average train loss: 52.4816
[09/26 07:44:11 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 68.3982
[09/26 07:44:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:44:11 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 07:44:18 visual_prompt]: Epoch 64 / 100: avg data time: 5.07e-02, avg batch time: 0.5013, average train loss: 64.5597
[09/26 07:44:19 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 62.2035
[09/26 07:44:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:44:19 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 07:44:26 visual_prompt]: Epoch 65 / 100: avg data time: 4.60e-02, avg batch time: 0.4964, average train loss: 51.3192
[09/26 07:44:27 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 38.7866
[09/26 07:44:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 07:44:27 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 07:44:34 visual_prompt]: Epoch 66 / 100: avg data time: 4.56e-02, avg batch time: 0.4963, average train loss: 46.2711
[09/26 07:44:35 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1695, average loss: 53.5580
[09/26 07:44:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:44:35 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 07:44:42 visual_prompt]: Epoch 67 / 100: avg data time: 5.11e-02, avg batch time: 0.4999, average train loss: 30.8718
[09/26 07:44:43 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1690, average loss: 7.5665
[09/26 07:44:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:44:43 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 07:44:50 visual_prompt]: Epoch 68 / 100: avg data time: 4.68e-02, avg batch time: 0.4961, average train loss: 27.9641
[09/26 07:44:52 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1691, average loss: 27.6811
[09/26 07:44:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 07:44:52 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 07:44:58 visual_prompt]: Epoch 69 / 100: avg data time: 4.59e-02, avg batch time: 0.4952, average train loss: 20.9030
[09/26 07:45:00 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 20.0156
[09/26 07:45:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 07:45:00 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 07:45:06 visual_prompt]: Epoch 70 / 100: avg data time: 4.16e-02, avg batch time: 0.4917, average train loss: 23.1540
[09/26 07:45:08 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1688, average loss: 19.4432
[09/26 07:45:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:45:08 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 07:45:15 visual_prompt]: Epoch 71 / 100: avg data time: 4.36e-02, avg batch time: 0.4962, average train loss: 20.9734
[09/26 07:45:16 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1691, average loss: 33.5317
[09/26 07:45:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:45:16 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 07:45:23 visual_prompt]: Epoch 72 / 100: avg data time: 4.11e-02, avg batch time: 0.4928, average train loss: 26.8904
[09/26 07:45:24 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 19.5962
[09/26 07:45:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:45:24 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 07:45:31 visual_prompt]: Epoch 73 / 100: avg data time: 3.95e-02, avg batch time: 0.4901, average train loss: 24.9406
[09/26 07:45:32 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1692, average loss: 18.9431
[09/26 07:45:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:45:32 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 07:45:39 visual_prompt]: Epoch 74 / 100: avg data time: 3.93e-02, avg batch time: 0.4885, average train loss: 23.3620
[09/26 07:45:40 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1690, average loss: 17.3077
[09/26 07:45:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:45:40 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 07:45:47 visual_prompt]: Epoch 75 / 100: avg data time: 4.28e-02, avg batch time: 0.4944, average train loss: 15.1342
[09/26 07:45:48 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 15.2393
[09/26 07:45:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:45:48 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 07:45:55 visual_prompt]: Epoch 76 / 100: avg data time: 4.12e-02, avg batch time: 0.4919, average train loss: 16.5366
[09/26 07:45:56 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1692, average loss: 24.0827
[09/26 07:45:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:45:56 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 07:46:03 visual_prompt]: Epoch 77 / 100: avg data time: 4.49e-02, avg batch time: 0.4946, average train loss: 22.6174
[09/26 07:46:05 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 26.2766
[09/26 07:46:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:46:05 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 07:46:11 visual_prompt]: Epoch 78 / 100: avg data time: 5.39e-02, avg batch time: 0.5031, average train loss: 19.0177
[09/26 07:46:13 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1696, average loss: 13.5109
[09/26 07:46:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:46:13 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 07:46:19 visual_prompt]: Epoch 79 / 100: avg data time: 3.67e-02, avg batch time: 0.4873, average train loss: 10.8689
[09/26 07:46:21 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1697, average loss: 16.3235
[09/26 07:46:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:46:21 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 07:46:28 visual_prompt]: Epoch 80 / 100: avg data time: 5.58e-02, avg batch time: 0.5049, average train loss: 10.8592
[09/26 07:46:29 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 5.9935
[09/26 07:46:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:46:29 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 07:46:36 visual_prompt]: Epoch 81 / 100: avg data time: 4.21e-02, avg batch time: 0.4920, average train loss: 7.2097
[09/26 07:46:37 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1692, average loss: 10.6460
[09/26 07:46:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:46:37 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 07:46:44 visual_prompt]: Epoch 82 / 100: avg data time: 5.68e-02, avg batch time: 0.5075, average train loss: 8.5856
[09/26 07:46:46 visual_prompt]: Inference (val):avg data time: 4.51e-05, avg batch time: 0.1693, average loss: 7.4421
[09/26 07:46:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:46:46 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 07:46:53 visual_prompt]: Epoch 83 / 100: avg data time: 5.43e-02, avg batch time: 0.5028, average train loss: 6.1008
[09/26 07:46:54 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 6.8680
[09/26 07:46:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:46:54 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 07:47:01 visual_prompt]: Epoch 84 / 100: avg data time: 5.75e-02, avg batch time: 0.5056, average train loss: 3.8766
[09/26 07:47:02 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 2.9885
[09/26 07:47:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:47:02 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 07:47:09 visual_prompt]: Epoch 85 / 100: avg data time: 5.53e-02, avg batch time: 0.5040, average train loss: 2.3566
[09/26 07:47:11 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1689, average loss: 2.1534
[09/26 07:47:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:47:11 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 07:47:17 visual_prompt]: Epoch 86 / 100: avg data time: 5.43e-02, avg batch time: 0.5026, average train loss: 1.9307
[09/26 07:47:19 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1688, average loss: 1.9123
[09/26 07:47:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:47:19 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 07:47:26 visual_prompt]: Epoch 87 / 100: avg data time: 5.39e-02, avg batch time: 0.5029, average train loss: 1.9875
[09/26 07:47:27 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1695, average loss: 2.0451
[09/26 07:47:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:47:27 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 07:47:34 visual_prompt]: Epoch 88 / 100: avg data time: 4.62e-02, avg batch time: 0.4984, average train loss: 2.0014
[09/26 07:47:35 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1692, average loss: 1.9148
[09/26 07:47:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 83.00	
[09/26 07:47:35 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 07:47:42 visual_prompt]: Epoch 89 / 100: avg data time: 4.77e-02, avg batch time: 0.4976, average train loss: 1.8947
[09/26 07:47:44 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1694, average loss: 2.2024
[09/26 07:47:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 07:47:44 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 07:47:50 visual_prompt]: Epoch 90 / 100: avg data time: 5.49e-02, avg batch time: 0.5030, average train loss: 1.9311
[09/26 07:47:52 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1689, average loss: 1.8824
[09/26 07:47:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:47:52 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 07:47:59 visual_prompt]: Epoch 91 / 100: avg data time: 5.14e-02, avg batch time: 0.5006, average train loss: 1.8743
[09/26 07:48:00 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1690, average loss: 1.8971
[09/26 07:48:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:48:00 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 07:48:07 visual_prompt]: Epoch 92 / 100: avg data time: 5.34e-02, avg batch time: 0.5036, average train loss: 1.8058
[09/26 07:48:08 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1691, average loss: 1.8230
[09/26 07:48:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:48:08 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 07:48:15 visual_prompt]: Epoch 93 / 100: avg data time: 5.15e-02, avg batch time: 0.4997, average train loss: 1.7968
[09/26 07:48:17 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1691, average loss: 1.8099
[09/26 07:48:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:48:17 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 07:48:23 visual_prompt]: Epoch 94 / 100: avg data time: 5.22e-02, avg batch time: 0.5021, average train loss: 1.7919
[09/26 07:48:25 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 1.8596
[09/26 07:48:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:48:25 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 07:48:32 visual_prompt]: Epoch 95 / 100: avg data time: 5.14e-02, avg batch time: 0.5006, average train loss: 1.8106
[09/26 07:48:33 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 1.8208
[09/26 07:48:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 82.00	
[09/26 07:48:33 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 07:48:40 visual_prompt]: Epoch 96 / 100: avg data time: 4.92e-02, avg batch time: 0.4980, average train loss: 1.7720
[09/26 07:48:41 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1694, average loss: 1.8167
[09/26 07:48:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:48:41 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 07:48:48 visual_prompt]: Epoch 97 / 100: avg data time: 4.08e-02, avg batch time: 0.4926, average train loss: 1.7611
[09/26 07:48:49 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1691, average loss: 1.8143
[09/26 07:48:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:48:49 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 07:48:56 visual_prompt]: Epoch 98 / 100: avg data time: 4.86e-02, avg batch time: 0.4991, average train loss: 1.7601
[09/26 07:48:57 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 1.8170
[09/26 07:48:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:48:57 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 07:49:04 visual_prompt]: Epoch 99 / 100: avg data time: 4.38e-02, avg batch time: 0.4940, average train loss: 1.7533
[09/26 07:49:06 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1690, average loss: 1.8150
[09/26 07:49:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 84.00	
[09/26 07:49:06 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 07:49:12 visual_prompt]: Epoch 100 / 100: avg data time: 5.18e-02, avg batch time: 0.5004, average train loss: 1.7514
[09/26 07:49:14 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1697, average loss: 1.8161
[09/26 07:49:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:49:14 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 07:49:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 07:49:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 07:49:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 07:49:14 visual_prompt]: Training with config:
[09/26 07:49:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 07:49:14 visual_prompt]: Loading training data...
[09/26 07:49:14 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 07:49:15 visual_prompt]: Number of images: 800
[09/26 07:49:15 visual_prompt]: Number of classes: 6 / 6
[09/26 07:49:15 visual_prompt]: Loading validation data...
[09/26 07:49:15 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 07:49:16 visual_prompt]: Number of images: 200
[09/26 07:49:16 visual_prompt]: Number of classes: 6 / 6
[09/26 07:49:16 visual_prompt]: Constructing models...
[09/26 07:49:18 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 07:49:18 visual_prompt]: tuned percent:0.540
[09/26 07:49:18 visual_prompt]: Device used for model: 0
[09/26 07:49:18 visual_prompt]: Setting up Evaluator...
[09/26 07:49:18 visual_prompt]: Setting up Trainer...
[09/26 07:49:18 visual_prompt]: 	Setting up the optimizer...
[09/26 07:49:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 07:49:25 visual_prompt]: Epoch 1 / 100: avg data time: 5.66e-02, avg batch time: 0.5056, average train loss: 2.9759
[09/26 07:49:26 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1688, average loss: 2.9268
[09/26 07:49:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:49:26 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 07:49:26 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 07:49:33 visual_prompt]: Epoch 2 / 100: avg data time: 4.94e-02, avg batch time: 0.4980, average train loss: 28.5797
[09/26 07:49:35 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1683, average loss: 22.1627
[09/26 07:49:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:49:35 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 07:49:42 visual_prompt]: Epoch 3 / 100: avg data time: 5.96e-02, avg batch time: 0.5069, average train loss: 49.8507
[09/26 07:49:43 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1688, average loss: 67.2560
[09/26 07:49:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:49:43 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 07:49:50 visual_prompt]: Epoch 4 / 100: avg data time: 4.16e-02, avg batch time: 0.4905, average train loss: 73.2715
[09/26 07:49:51 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1690, average loss: 102.3334
[09/26 07:49:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:49:51 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 07:49:58 visual_prompt]: Epoch 5 / 100: avg data time: 4.69e-02, avg batch time: 0.4967, average train loss: 113.3619
[09/26 07:49:59 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1696, average loss: 180.1156
[09/26 07:49:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:49:59 visual_prompt]: Best epoch 5: best metric: 0.205
[09/26 07:49:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 07:50:06 visual_prompt]: Epoch 6 / 100: avg data time: 5.54e-02, avg batch time: 0.5038, average train loss: 162.0630
[09/26 07:50:07 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1689, average loss: 172.3458
[09/26 07:50:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:50:07 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 07:50:14 visual_prompt]: Epoch 7 / 100: avg data time: 4.13e-02, avg batch time: 0.4899, average train loss: 88.8097
[09/26 07:50:15 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1691, average loss: 60.9517
[09/26 07:50:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:50:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 07:50:22 visual_prompt]: Epoch 8 / 100: avg data time: 4.80e-02, avg batch time: 0.4985, average train loss: 66.1976
[09/26 07:50:23 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1690, average loss: 93.6126
[09/26 07:50:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:50:23 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 07:50:30 visual_prompt]: Epoch 9 / 100: avg data time: 4.51e-02, avg batch time: 0.4939, average train loss: 98.4261
[09/26 07:50:32 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 121.6303
[09/26 07:50:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 07:50:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 07:50:38 visual_prompt]: Epoch 10 / 100: avg data time: 4.67e-02, avg batch time: 0.4948, average train loss: 133.7580
[09/26 07:50:40 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 160.5504
[09/26 07:50:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:50:40 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 07:50:47 visual_prompt]: Epoch 11 / 100: avg data time: 5.83e-02, avg batch time: 0.5061, average train loss: 190.6667
[09/26 07:50:48 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1689, average loss: 150.8781
[09/26 07:50:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:50:48 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 07:50:55 visual_prompt]: Epoch 12 / 100: avg data time: 5.45e-02, avg batch time: 0.5036, average train loss: 163.9060
[09/26 07:50:56 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 232.7713
[09/26 07:50:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:50:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 07:51:03 visual_prompt]: Epoch 13 / 100: avg data time: 4.36e-02, avg batch time: 0.4923, average train loss: 203.1249
[09/26 07:51:04 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1692, average loss: 178.2488
[09/26 07:51:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 07:51:04 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 07:51:11 visual_prompt]: Epoch 14 / 100: avg data time: 4.48e-02, avg batch time: 0.4961, average train loss: 245.8751
[09/26 07:51:13 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1693, average loss: 83.2418
[09/26 07:51:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 07:51:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 07:51:19 visual_prompt]: Epoch 15 / 100: avg data time: 5.28e-02, avg batch time: 0.5001, average train loss: 216.3464
[09/26 07:51:21 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1689, average loss: 156.6414
[09/26 07:51:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:51:21 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 07:51:27 visual_prompt]: Epoch 16 / 100: avg data time: 4.41e-02, avg batch time: 0.4937, average train loss: 243.4672
[09/26 07:51:29 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1691, average loss: 105.2711
[09/26 07:51:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:51:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 07:51:36 visual_prompt]: Epoch 17 / 100: avg data time: 4.03e-02, avg batch time: 0.4902, average train loss: 239.0325
[09/26 07:51:37 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1699, average loss: 532.2525
[09/26 07:51:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:51:37 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 07:51:44 visual_prompt]: Epoch 18 / 100: avg data time: 4.39e-02, avg batch time: 0.4918, average train loss: 309.6133
[09/26 07:51:45 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 278.7176
[09/26 07:51:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 07:51:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 07:51:52 visual_prompt]: Epoch 19 / 100: avg data time: 5.06e-02, avg batch time: 0.5039, average train loss: 235.5976
[09/26 07:51:53 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 213.1039
[09/26 07:51:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:51:53 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 07:52:00 visual_prompt]: Epoch 20 / 100: avg data time: 5.21e-02, avg batch time: 0.5009, average train loss: 125.0786
[09/26 07:52:02 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1689, average loss: 78.4871
[09/26 07:52:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 07:52:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 07:52:08 visual_prompt]: Epoch 21 / 100: avg data time: 4.91e-02, avg batch time: 0.4978, average train loss: 188.3003
[09/26 07:52:10 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 287.8286
[09/26 07:52:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:52:10 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 07:52:16 visual_prompt]: Epoch 22 / 100: avg data time: 4.00e-02, avg batch time: 0.4898, average train loss: 175.9017
[09/26 07:52:18 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1692, average loss: 149.8894
[09/26 07:52:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:52:18 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 07:52:25 visual_prompt]: Epoch 23 / 100: avg data time: 4.68e-02, avg batch time: 0.4946, average train loss: 116.4477
[09/26 07:52:26 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1691, average loss: 159.1251
[09/26 07:52:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:52:26 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 07:52:33 visual_prompt]: Epoch 24 / 100: avg data time: 4.11e-02, avg batch time: 0.4917, average train loss: 170.4932
[09/26 07:52:34 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1694, average loss: 108.3001
[09/26 07:52:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:52:34 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 07:52:41 visual_prompt]: Epoch 25 / 100: avg data time: 4.04e-02, avg batch time: 0.4909, average train loss: 114.3989
[09/26 07:52:42 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 159.0607
[09/26 07:52:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:52:42 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 07:52:49 visual_prompt]: Epoch 26 / 100: avg data time: 5.86e-02, avg batch time: 0.5067, average train loss: 121.5741
[09/26 07:52:51 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 125.5165
[09/26 07:52:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 07:52:51 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 07:52:57 visual_prompt]: Epoch 27 / 100: avg data time: 4.90e-02, avg batch time: 0.4980, average train loss: 126.3026
[09/26 07:52:59 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1690, average loss: 166.5497
[09/26 07:52:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:52:59 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 07:53:05 visual_prompt]: Epoch 28 / 100: avg data time: 4.12e-02, avg batch time: 0.4905, average train loss: 149.7242
[09/26 07:53:07 visual_prompt]: Inference (val):avg data time: 4.54e-05, avg batch time: 0.1692, average loss: 165.0049
[09/26 07:53:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 84.00	
[09/26 07:53:07 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 07:53:14 visual_prompt]: Epoch 29 / 100: avg data time: 5.65e-02, avg batch time: 0.5041, average train loss: 119.8216
[09/26 07:53:15 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1696, average loss: 197.6986
[09/26 07:53:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:53:15 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 07:53:22 visual_prompt]: Epoch 30 / 100: avg data time: 4.22e-02, avg batch time: 0.4907, average train loss: 140.5185
[09/26 07:53:23 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 100.0913
[09/26 07:53:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 07:53:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 07:53:30 visual_prompt]: Epoch 31 / 100: avg data time: 5.04e-02, avg batch time: 0.4990, average train loss: 169.1578
[09/26 07:53:32 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1693, average loss: 183.5150
[09/26 07:53:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 07:53:32 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 07:53:38 visual_prompt]: Epoch 32 / 100: avg data time: 5.28e-02, avg batch time: 0.5011, average train loss: 161.4927
[09/26 07:53:40 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1694, average loss: 210.6773
[09/26 07:53:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:53:40 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 07:53:47 visual_prompt]: Epoch 33 / 100: avg data time: 4.30e-02, avg batch time: 0.4913, average train loss: 169.9906
[09/26 07:53:48 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 144.1494
[09/26 07:53:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:53:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 07:53:55 visual_prompt]: Epoch 34 / 100: avg data time: 5.81e-02, avg batch time: 0.5061, average train loss: 100.5942
[09/26 07:53:56 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1694, average loss: 68.6307
[09/26 07:53:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:53:56 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 07:54:03 visual_prompt]: Epoch 35 / 100: avg data time: 5.32e-02, avg batch time: 0.5024, average train loss: 68.3808
[09/26 07:54:05 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 83.3827
[09/26 07:54:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:54:05 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 07:54:11 visual_prompt]: Epoch 36 / 100: avg data time: 4.70e-02, avg batch time: 0.4956, average train loss: 65.1520
[09/26 07:54:13 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1691, average loss: 108.6090
[09/26 07:54:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:54:13 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 07:54:20 visual_prompt]: Epoch 37 / 100: avg data time: 4.60e-02, avg batch time: 0.4953, average train loss: 129.5904
[09/26 07:54:21 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1689, average loss: 60.9335
[09/26 07:54:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:54:21 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 07:54:28 visual_prompt]: Epoch 38 / 100: avg data time: 5.69e-02, avg batch time: 0.5051, average train loss: 116.2049
[09/26 07:54:29 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 120.8441
[09/26 07:54:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 07:54:29 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 07:54:36 visual_prompt]: Epoch 39 / 100: avg data time: 5.77e-02, avg batch time: 0.5057, average train loss: 53.7718
[09/26 07:54:38 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1689, average loss: 90.0363
[09/26 07:54:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 07:54:38 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 07:54:44 visual_prompt]: Epoch 40 / 100: avg data time: 4.95e-02, avg batch time: 0.4971, average train loss: 86.2097
[09/26 07:54:46 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1688, average loss: 97.4189
[09/26 07:54:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 82.50	
[09/26 07:54:46 visual_prompt]: Best epoch 40: best metric: 0.235
[09/26 07:54:46 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 07:54:53 visual_prompt]: Epoch 41 / 100: avg data time: 5.62e-02, avg batch time: 0.5048, average train loss: 84.0717
[09/26 07:54:54 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1692, average loss: 90.4460
[09/26 07:54:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:54:54 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 07:55:01 visual_prompt]: Epoch 42 / 100: avg data time: 5.88e-02, avg batch time: 0.5067, average train loss: 91.0848
[09/26 07:55:02 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1689, average loss: 113.3194
[09/26 07:55:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:55:02 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 07:55:09 visual_prompt]: Epoch 43 / 100: avg data time: 5.62e-02, avg batch time: 0.5034, average train loss: 125.2160
[09/26 07:55:11 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1694, average loss: 109.6826
[09/26 07:55:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 07:55:11 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 07:55:17 visual_prompt]: Epoch 44 / 100: avg data time: 5.71e-02, avg batch time: 0.5056, average train loss: 125.6947
[09/26 07:55:19 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 84.6162
[09/26 07:55:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 07:55:19 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 07:55:26 visual_prompt]: Epoch 45 / 100: avg data time: 5.16e-02, avg batch time: 0.5009, average train loss: 117.9567
[09/26 07:55:27 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1690, average loss: 149.4685
[09/26 07:55:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 07:55:27 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 07:55:34 visual_prompt]: Epoch 46 / 100: avg data time: 5.33e-02, avg batch time: 0.5031, average train loss: 109.4072
[09/26 07:55:35 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 36.4123
[09/26 07:55:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 83.00	
[09/26 07:55:35 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 07:55:42 visual_prompt]: Epoch 47 / 100: avg data time: 5.34e-02, avg batch time: 0.5015, average train loss: 87.2275
[09/26 07:55:43 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 99.4184
[09/26 07:55:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 07:55:43 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 07:55:50 visual_prompt]: Epoch 48 / 100: avg data time: 5.06e-02, avg batch time: 0.4997, average train loss: 106.1010
[09/26 07:55:52 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1695, average loss: 113.6069
[09/26 07:55:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 07:55:52 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 07:55:58 visual_prompt]: Epoch 49 / 100: avg data time: 4.31e-02, avg batch time: 0.4929, average train loss: 112.0353
[09/26 07:56:00 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1689, average loss: 67.7590
[09/26 07:56:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 07:56:00 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 07:56:06 visual_prompt]: Epoch 50 / 100: avg data time: 4.08e-02, avg batch time: 0.4893, average train loss: 85.1827
[09/26 07:56:08 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1690, average loss: 64.7582
[09/26 07:56:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 07:56:08 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 07:56:14 visual_prompt]: Epoch 51 / 100: avg data time: 4.93e-02, avg batch time: 0.4983, average train loss: 93.8564
[09/26 07:56:16 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 114.1999
[09/26 07:56:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 07:56:16 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 07:56:23 visual_prompt]: Epoch 52 / 100: avg data time: 5.11e-02, avg batch time: 0.5009, average train loss: 70.6901
[09/26 07:56:24 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1691, average loss: 85.4839
[09/26 07:56:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 84.00	
[09/26 07:56:24 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 07:56:31 visual_prompt]: Epoch 53 / 100: avg data time: 4.55e-02, avg batch time: 0.4956, average train loss: 56.8687
[09/26 07:56:32 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 57.1269
[09/26 07:56:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 84.50	
[09/26 07:56:32 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 07:56:39 visual_prompt]: Epoch 54 / 100: avg data time: 5.93e-02, avg batch time: 0.5078, average train loss: 40.8287
[09/26 07:56:41 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1690, average loss: 37.5510
[09/26 07:56:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 87.00	
[09/26 07:56:41 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 07:56:48 visual_prompt]: Epoch 55 / 100: avg data time: 5.76e-02, avg batch time: 0.5064, average train loss: 29.5821
[09/26 07:56:49 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1692, average loss: 32.9045
[09/26 07:56:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 07:56:49 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 07:56:56 visual_prompt]: Epoch 56 / 100: avg data time: 5.42e-02, avg batch time: 0.5024, average train loss: 33.8099
[09/26 07:56:57 visual_prompt]: Inference (val):avg data time: 4.38e-05, avg batch time: 0.1690, average loss: 35.8094
[09/26 07:56:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 84.00	
[09/26 07:56:57 visual_prompt]: Best epoch 56: best metric: 0.250
[09/26 07:56:57 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 07:57:04 visual_prompt]: Epoch 57 / 100: avg data time: 5.35e-02, avg batch time: 0.5022, average train loss: 30.2300
[09/26 07:57:06 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1688, average loss: 29.3652
[09/26 07:57:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 84.00	
[09/26 07:57:06 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 07:57:12 visual_prompt]: Epoch 58 / 100: avg data time: 5.41e-02, avg batch time: 0.5041, average train loss: 21.6461
[09/26 07:57:14 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1688, average loss: 38.0461
[09/26 07:57:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 07:57:14 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 07:57:21 visual_prompt]: Epoch 59 / 100: avg data time: 5.01e-02, avg batch time: 0.4996, average train loss: 28.4301
[09/26 07:57:22 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1690, average loss: 20.4443
[09/26 07:57:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 84.00	
[09/26 07:57:22 visual_prompt]: Best epoch 59: best metric: 0.265
[09/26 07:57:22 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 07:57:29 visual_prompt]: Epoch 60 / 100: avg data time: 5.97e-02, avg batch time: 0.5103, average train loss: 17.3312
[09/26 07:57:31 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.1687, average loss: 13.7009
[09/26 07:57:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 86.50	
[09/26 07:57:31 visual_prompt]: Best epoch 60: best metric: 0.285
[09/26 07:57:31 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 07:57:37 visual_prompt]: Epoch 61 / 100: avg data time: 5.34e-02, avg batch time: 0.5027, average train loss: 13.4663
[09/26 07:57:39 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1690, average loss: 20.7450
[09/26 07:57:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 94.00	
[09/26 07:57:39 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 07:57:45 visual_prompt]: Epoch 62 / 100: avg data time: 4.97e-02, avg batch time: 0.4978, average train loss: 21.9404
[09/26 07:57:47 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1691, average loss: 23.4196
[09/26 07:57:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 79.50	
[09/26 07:57:47 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 07:57:54 visual_prompt]: Epoch 63 / 100: avg data time: 5.99e-02, avg batch time: 0.5087, average train loss: 23.7119
[09/26 07:57:55 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 23.4989
[09/26 07:57:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 84.00	
[09/26 07:57:55 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 07:58:02 visual_prompt]: Epoch 64 / 100: avg data time: 5.24e-02, avg batch time: 0.5018, average train loss: 19.6276
[09/26 07:58:04 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 26.9311
[09/26 07:58:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 84.00	
[09/26 07:58:04 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 07:58:10 visual_prompt]: Epoch 65 / 100: avg data time: 5.41e-02, avg batch time: 0.5031, average train loss: 18.5359
[09/26 07:58:12 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1691, average loss: 12.1222
[09/26 07:58:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 91.50	
[09/26 07:58:12 visual_prompt]: Best epoch 65: best metric: 0.295
[09/26 07:58:12 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 07:58:19 visual_prompt]: Epoch 66 / 100: avg data time: 4.83e-02, avg batch time: 0.4987, average train loss: 10.7217
[09/26 07:58:20 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1690, average loss: 22.0304
[09/26 07:58:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 93.50	
[09/26 07:58:20 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 07:58:27 visual_prompt]: Epoch 67 / 100: avg data time: 5.95e-02, avg batch time: 0.5074, average train loss: 14.9422
[09/26 07:58:28 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 11.8746
[09/26 07:58:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 85.50	
[09/26 07:58:28 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 07:58:35 visual_prompt]: Epoch 68 / 100: avg data time: 5.18e-02, avg batch time: 0.5010, average train loss: 9.6956
[09/26 07:58:37 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1689, average loss: 6.3914
[09/26 07:58:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 94.50	
[09/26 07:58:37 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 07:58:43 visual_prompt]: Epoch 69 / 100: avg data time: 5.46e-02, avg batch time: 0.5041, average train loss: 5.5894
[09/26 07:58:45 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 7.2578
[09/26 07:58:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 96.50	
[09/26 07:58:45 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 07:58:52 visual_prompt]: Epoch 70 / 100: avg data time: 5.35e-02, avg batch time: 0.5030, average train loss: 5.5466
[09/26 07:58:53 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 6.2099
[09/26 07:58:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 96.00	
[09/26 07:58:53 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 07:59:00 visual_prompt]: Epoch 71 / 100: avg data time: 6.16e-02, avg batch time: 0.5099, average train loss: 4.8371
[09/26 07:59:02 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 7.4461
[09/26 07:59:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 90.00	
[09/26 07:59:02 visual_prompt]: Best epoch 71: best metric: 0.300
[09/26 07:59:02 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 07:59:08 visual_prompt]: Epoch 72 / 100: avg data time: 4.76e-02, avg batch time: 0.4964, average train loss: 5.8829
[09/26 07:59:10 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1693, average loss: 6.4637
[09/26 07:59:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 87.50	
[09/26 07:59:10 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 07:59:16 visual_prompt]: Epoch 73 / 100: avg data time: 4.31e-02, avg batch time: 0.4931, average train loss: 5.1440
[09/26 07:59:18 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 8.5311
[09/26 07:59:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 95.00	
[09/26 07:59:18 visual_prompt]: Best epoch 73: best metric: 0.310
[09/26 07:59:18 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 07:59:25 visual_prompt]: Epoch 74 / 100: avg data time: 5.37e-02, avg batch time: 0.5034, average train loss: 4.4431
[09/26 07:59:26 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1693, average loss: 7.0010
[09/26 07:59:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 86.50	
[09/26 07:59:26 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 07:59:33 visual_prompt]: Epoch 75 / 100: avg data time: 5.87e-02, avg batch time: 0.5070, average train loss: 4.5493
[09/26 07:59:35 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1691, average loss: 3.8435
[09/26 07:59:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 89.50	
[09/26 07:59:35 visual_prompt]: Best epoch 75: best metric: 0.325
[09/26 07:59:35 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 07:59:41 visual_prompt]: Epoch 76 / 100: avg data time: 4.53e-02, avg batch time: 0.4954, average train loss: 4.4695
[09/26 07:59:43 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1693, average loss: 4.8228
[09/26 07:59:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 94.50	
[09/26 07:59:43 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 07:59:50 visual_prompt]: Epoch 77 / 100: avg data time: 5.55e-02, avg batch time: 0.5036, average train loss: 3.3983
[09/26 07:59:51 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1695, average loss: 3.8505
[09/26 07:59:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 96.50	
[09/26 07:59:51 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 07:59:58 visual_prompt]: Epoch 78 / 100: avg data time: 4.87e-02, avg batch time: 0.4983, average train loss: 3.3494
[09/26 07:59:59 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 3.9011
[09/26 07:59:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 93.00	
[09/26 07:59:59 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 08:00:06 visual_prompt]: Epoch 79 / 100: avg data time: 5.43e-02, avg batch time: 0.5025, average train loss: 3.0959
[09/26 08:00:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 2.6122
[09/26 08:00:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.00	
[09/26 08:00:08 visual_prompt]: Best epoch 79: best metric: 0.330
[09/26 08:00:08 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 08:00:14 visual_prompt]: Epoch 80 / 100: avg data time: 5.19e-02, avg batch time: 0.5000, average train loss: 2.7691
[09/26 08:00:16 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 2.5876
[09/26 08:00:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 95.00	
[09/26 08:00:16 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 08:00:23 visual_prompt]: Epoch 81 / 100: avg data time: 5.10e-02, avg batch time: 0.5004, average train loss: 2.7845
[09/26 08:00:24 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1691, average loss: 3.4936
[09/26 08:00:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 96.50	
[09/26 08:00:24 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 08:00:31 visual_prompt]: Epoch 82 / 100: avg data time: 4.73e-02, avg batch time: 0.4955, average train loss: 2.8694
[09/26 08:00:32 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1693, average loss: 3.5652
[09/26 08:00:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 90.50	
[09/26 08:00:32 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 08:00:39 visual_prompt]: Epoch 83 / 100: avg data time: 4.44e-02, avg batch time: 0.4944, average train loss: 2.5217
[09/26 08:00:40 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1688, average loss: 2.5289
[09/26 08:00:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.00	
[09/26 08:00:40 visual_prompt]: Best epoch 83: best metric: 0.365
[09/26 08:00:40 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 08:00:47 visual_prompt]: Epoch 84 / 100: avg data time: 4.50e-02, avg batch time: 0.4939, average train loss: 2.3318
[09/26 08:00:48 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1692, average loss: 2.3294
[09/26 08:00:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.00	
[09/26 08:00:48 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 08:00:55 visual_prompt]: Epoch 85 / 100: avg data time: 5.34e-02, avg batch time: 0.5034, average train loss: 2.1776
[09/26 08:00:57 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 2.5577
[09/26 08:00:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 96.50	
[09/26 08:00:57 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 08:01:04 visual_prompt]: Epoch 86 / 100: avg data time: 5.73e-02, avg batch time: 0.5068, average train loss: 2.1349
[09/26 08:01:05 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1692, average loss: 2.4118
[09/26 08:01:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 96.50	
[09/26 08:01:05 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 08:01:12 visual_prompt]: Epoch 87 / 100: avg data time: 4.09e-02, avg batch time: 0.4900, average train loss: 2.2865
[09/26 08:01:13 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 2.8101
[09/26 08:01:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 94.50	
[09/26 08:01:13 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 08:01:20 visual_prompt]: Epoch 88 / 100: avg data time: 5.55e-02, avg batch time: 0.5047, average train loss: 2.0948
[09/26 08:01:21 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1694, average loss: 2.0043
[09/26 08:01:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 97.50	
[09/26 08:01:21 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 08:01:28 visual_prompt]: Epoch 89 / 100: avg data time: 5.23e-02, avg batch time: 0.5009, average train loss: 1.9128
[09/26 08:01:29 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 2.4270
[09/26 08:01:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.00	
[09/26 08:01:29 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 08:01:36 visual_prompt]: Epoch 90 / 100: avg data time: 5.91e-02, avg batch time: 0.5074, average train loss: 1.9545
[09/26 08:01:38 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 1.9995
[09/26 08:01:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 96.50	
[09/26 08:01:38 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 08:01:45 visual_prompt]: Epoch 91 / 100: avg data time: 5.49e-02, avg batch time: 0.5033, average train loss: 1.8478
[09/26 08:01:46 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1696, average loss: 1.9769
[09/26 08:01:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.50	
[09/26 08:01:46 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 08:01:53 visual_prompt]: Epoch 92 / 100: avg data time: 5.56e-02, avg batch time: 0.5040, average train loss: 1.8137
[09/26 08:01:54 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 1.9639
[09/26 08:01:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 98.00	
[09/26 08:01:54 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 08:02:01 visual_prompt]: Epoch 93 / 100: avg data time: 5.84e-02, avg batch time: 0.5070, average train loss: 1.8163
[09/26 08:02:03 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 1.9339
[09/26 08:02:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 96.50	
[09/26 08:02:03 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 08:02:09 visual_prompt]: Epoch 94 / 100: avg data time: 5.65e-02, avg batch time: 0.5048, average train loss: 1.7366
[09/26 08:02:11 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1693, average loss: 2.0017
[09/26 08:02:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 96.50	
[09/26 08:02:11 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 08:02:18 visual_prompt]: Epoch 95 / 100: avg data time: 4.57e-02, avg batch time: 0.4952, average train loss: 1.7367
[09/26 08:02:19 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1694, average loss: 1.9458
[09/26 08:02:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 96.50	
[09/26 08:02:19 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 08:02:26 visual_prompt]: Epoch 96 / 100: avg data time: 4.05e-02, avg batch time: 0.4905, average train loss: 1.7161
[09/26 08:02:27 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1696, average loss: 1.8961
[09/26 08:02:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 97.00	
[09/26 08:02:27 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 08:02:34 visual_prompt]: Epoch 97 / 100: avg data time: 5.37e-02, avg batch time: 0.5032, average train loss: 1.7052
[09/26 08:02:35 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 1.9072
[09/26 08:02:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 96.50	
[09/26 08:02:35 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 08:02:42 visual_prompt]: Epoch 98 / 100: avg data time: 4.36e-02, avg batch time: 0.4942, average train loss: 1.7057
[09/26 08:02:44 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 1.8930
[09/26 08:02:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 96.50	
[09/26 08:02:44 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 08:02:50 visual_prompt]: Epoch 99 / 100: avg data time: 5.48e-02, avg batch time: 0.5046, average train loss: 1.7574
[09/26 08:02:52 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1689, average loss: 1.8897
[09/26 08:02:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 96.50	
[09/26 08:02:52 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 08:02:59 visual_prompt]: Epoch 100 / 100: avg data time: 5.12e-02, avg batch time: 0.5000, average train loss: 1.6771
[09/26 08:03:00 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1690, average loss: 1.8904
[09/26 08:03:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 96.50	
[09/26 08:03:00 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 08:03:00 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 08:03:00 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 08:03:00 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 08:03:00 visual_prompt]: Training with config:
[09/26 08:03:00 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 08:03:00 visual_prompt]: Loading training data...
[09/26 08:03:00 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 08:03:02 visual_prompt]: Number of images: 800
[09/26 08:03:02 visual_prompt]: Number of classes: 6 / 6
[09/26 08:03:02 visual_prompt]: Loading validation data...
[09/26 08:03:02 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 08:03:02 visual_prompt]: Number of images: 200
[09/26 08:03:02 visual_prompt]: Number of classes: 6 / 6
[09/26 08:03:02 visual_prompt]: Constructing models...
[09/26 08:03:04 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 08:03:04 visual_prompt]: tuned percent:0.540
[09/26 08:03:04 visual_prompt]: Device used for model: 0
[09/26 08:03:04 visual_prompt]: Setting up Evaluator...
[09/26 08:03:04 visual_prompt]: Setting up Trainer...
[09/26 08:03:04 visual_prompt]: 	Setting up the optimizer...
[09/26 08:03:04 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 08:03:11 visual_prompt]: Epoch 1 / 100: avg data time: 4.58e-02, avg batch time: 0.4931, average train loss: 2.9578
[09/26 08:03:13 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1686, average loss: 2.9268
[09/26 08:03:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:03:13 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 08:03:13 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 08:03:19 visual_prompt]: Epoch 2 / 100: avg data time: 5.46e-02, avg batch time: 0.5029, average train loss: 28.0349
[09/26 08:03:21 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1684, average loss: 17.8985
[09/26 08:03:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:03:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 08:03:27 visual_prompt]: Epoch 3 / 100: avg data time: 4.76e-02, avg batch time: 0.4960, average train loss: 16.6618
[09/26 08:03:29 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1685, average loss: 20.9982
[09/26 08:03:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:03:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 08:03:35 visual_prompt]: Epoch 4 / 100: avg data time: 3.98e-02, avg batch time: 0.4884, average train loss: 22.3324
[09/26 08:03:37 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.1691, average loss: 12.5356
[09/26 08:03:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:03:37 visual_prompt]: Best epoch 4: best metric: 0.205
[09/26 08:03:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 08:03:44 visual_prompt]: Epoch 5 / 100: avg data time: 4.35e-02, avg batch time: 0.4914, average train loss: 32.1868
[09/26 08:03:45 visual_prompt]: Inference (val):avg data time: 4.57e-05, avg batch time: 0.1694, average loss: 26.6585
[09/26 08:03:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.00	
[09/26 08:03:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 08:03:52 visual_prompt]: Epoch 6 / 100: avg data time: 4.07e-02, avg batch time: 0.4900, average train loss: 37.2444
[09/26 08:03:53 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1696, average loss: 56.7206
[09/26 08:03:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:03:53 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 08:04:00 visual_prompt]: Epoch 7 / 100: avg data time: 5.43e-02, avg batch time: 0.5036, average train loss: 48.8940
[09/26 08:04:01 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1695, average loss: 42.5260
[09/26 08:04:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:04:01 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 08:04:08 visual_prompt]: Epoch 8 / 100: avg data time: 5.15e-02, avg batch time: 0.4998, average train loss: 57.7338
[09/26 08:04:10 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 36.5575
[09/26 08:04:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:04:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 08:04:16 visual_prompt]: Epoch 9 / 100: avg data time: 4.47e-02, avg batch time: 0.4948, average train loss: 76.5885
[09/26 08:04:18 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 47.8504
[09/26 08:04:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 82.50	
[09/26 08:04:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 08:04:25 visual_prompt]: Epoch 10 / 100: avg data time: 5.78e-02, avg batch time: 0.5073, average train loss: 102.8898
[09/26 08:04:26 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1691, average loss: 55.6766
[09/26 08:04:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:04:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 08:04:33 visual_prompt]: Epoch 11 / 100: avg data time: 5.00e-02, avg batch time: 0.4985, average train loss: 98.2102
[09/26 08:04:34 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1695, average loss: 75.1874
[09/26 08:04:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 08:04:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 08:04:41 visual_prompt]: Epoch 12 / 100: avg data time: 5.11e-02, avg batch time: 0.4999, average train loss: 97.6640
[09/26 08:04:43 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1693, average loss: 85.4992
[09/26 08:04:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 08:04:43 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 08:04:49 visual_prompt]: Epoch 13 / 100: avg data time: 4.73e-02, avg batch time: 0.4970, average train loss: 105.9131
[09/26 08:04:51 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1695, average loss: 147.2520
[09/26 08:04:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:04:51 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 08:04:57 visual_prompt]: Epoch 14 / 100: avg data time: 5.49e-02, avg batch time: 0.5027, average train loss: 101.6967
[09/26 08:04:59 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 195.5435
[09/26 08:04:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:04:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 08:05:06 visual_prompt]: Epoch 15 / 100: avg data time: 6.37e-02, avg batch time: 0.5126, average train loss: 103.0545
[09/26 08:05:07 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1688, average loss: 121.6318
[09/26 08:05:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 08:05:07 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 08:05:14 visual_prompt]: Epoch 16 / 100: avg data time: 4.47e-02, avg batch time: 0.4948, average train loss: 108.7949
[09/26 08:05:15 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 113.3483
[09/26 08:05:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:05:15 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 08:05:22 visual_prompt]: Epoch 17 / 100: avg data time: 4.17e-02, avg batch time: 0.4930, average train loss: 119.9589
[09/26 08:05:24 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1689, average loss: 65.1124
[09/26 08:05:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.50	
[09/26 08:05:24 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 08:05:30 visual_prompt]: Epoch 18 / 100: avg data time: 5.08e-02, avg batch time: 0.4994, average train loss: 101.3029
[09/26 08:05:32 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1694, average loss: 64.8642
[09/26 08:05:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:05:32 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 08:05:39 visual_prompt]: Epoch 19 / 100: avg data time: 5.62e-02, avg batch time: 0.5045, average train loss: 106.2568
[09/26 08:05:40 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 75.0115
[09/26 08:05:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:05:40 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 08:05:47 visual_prompt]: Epoch 20 / 100: avg data time: 5.06e-02, avg batch time: 0.4993, average train loss: 120.4127
[09/26 08:05:48 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1698, average loss: 137.3060
[09/26 08:05:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 08:05:48 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 08:05:55 visual_prompt]: Epoch 21 / 100: avg data time: 4.32e-02, avg batch time: 0.4921, average train loss: 95.0293
[09/26 08:05:56 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1695, average loss: 72.4245
[09/26 08:05:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 08:05:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 08:06:03 visual_prompt]: Epoch 22 / 100: avg data time: 5.92e-02, avg batch time: 0.5101, average train loss: 105.2424
[09/26 08:06:05 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 216.6875
[09/26 08:06:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:06:05 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 08:06:11 visual_prompt]: Epoch 23 / 100: avg data time: 5.33e-02, avg batch time: 0.5010, average train loss: 125.7037
[09/26 08:06:13 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1691, average loss: 78.9842
[09/26 08:06:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:06:13 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 08:06:19 visual_prompt]: Epoch 24 / 100: avg data time: 4.22e-02, avg batch time: 0.4939, average train loss: 82.5088
[09/26 08:06:21 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1694, average loss: 100.2012
[09/26 08:06:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:06:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 08:06:28 visual_prompt]: Epoch 25 / 100: avg data time: 5.83e-02, avg batch time: 0.5087, average train loss: 86.7422
[09/26 08:06:29 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 192.9503
[09/26 08:06:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:06:29 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 08:06:36 visual_prompt]: Epoch 26 / 100: avg data time: 4.13e-02, avg batch time: 0.4895, average train loss: 130.6177
[09/26 08:06:37 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 162.6439
[09/26 08:06:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:06:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 08:06:44 visual_prompt]: Epoch 27 / 100: avg data time: 4.08e-02, avg batch time: 0.4940, average train loss: 110.2984
[09/26 08:06:46 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1693, average loss: 81.3523
[09/26 08:06:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:06:46 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 08:06:52 visual_prompt]: Epoch 28 / 100: avg data time: 4.38e-02, avg batch time: 0.4943, average train loss: 80.4920
[09/26 08:06:54 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1692, average loss: 78.9266
[09/26 08:06:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:06:54 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 08:07:00 visual_prompt]: Epoch 29 / 100: avg data time: 4.07e-02, avg batch time: 0.4901, average train loss: 90.8088
[09/26 08:07:02 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1695, average loss: 78.8225
[09/26 08:07:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:07:02 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 08:07:09 visual_prompt]: Epoch 30 / 100: avg data time: 5.07e-02, avg batch time: 0.4993, average train loss: 107.2095
[09/26 08:07:10 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.1698, average loss: 110.2759
[09/26 08:07:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 13.50	top5: 82.00	
[09/26 08:07:10 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 08:07:17 visual_prompt]: Epoch 31 / 100: avg data time: 5.38e-02, avg batch time: 0.5029, average train loss: 93.0005
[09/26 08:07:18 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1691, average loss: 129.0794
[09/26 08:07:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:07:18 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 08:07:25 visual_prompt]: Epoch 32 / 100: avg data time: 4.21e-02, avg batch time: 0.4925, average train loss: 96.0793
[09/26 08:07:26 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1696, average loss: 66.7803
[09/26 08:07:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:07:26 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 08:07:33 visual_prompt]: Epoch 33 / 100: avg data time: 5.82e-02, avg batch time: 0.5057, average train loss: 155.4606
[09/26 08:07:35 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1692, average loss: 108.6673
[09/26 08:07:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:07:35 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 08:07:41 visual_prompt]: Epoch 34 / 100: avg data time: 4.20e-02, avg batch time: 0.4935, average train loss: 104.2113
[09/26 08:07:43 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1699, average loss: 106.3202
[09/26 08:07:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:07:43 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 08:07:50 visual_prompt]: Epoch 35 / 100: avg data time: 4.00e-02, avg batch time: 0.4887, average train loss: 111.4714
[09/26 08:07:51 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1695, average loss: 24.5828
[09/26 08:07:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:07:51 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 08:07:58 visual_prompt]: Epoch 36 / 100: avg data time: 4.94e-02, avg batch time: 0.4978, average train loss: 88.5368
[09/26 08:07:59 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1698, average loss: 47.8743
[09/26 08:07:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:07:59 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 08:08:06 visual_prompt]: Epoch 37 / 100: avg data time: 5.58e-02, avg batch time: 0.5046, average train loss: 92.9381
[09/26 08:08:08 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1699, average loss: 18.3730
[09/26 08:08:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 08:08:08 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 08:08:14 visual_prompt]: Epoch 38 / 100: avg data time: 4.24e-02, avg batch time: 0.4938, average train loss: 75.7031
[09/26 08:08:16 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1696, average loss: 160.8088
[09/26 08:08:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:08:16 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 08:08:22 visual_prompt]: Epoch 39 / 100: avg data time: 4.42e-02, avg batch time: 0.4933, average train loss: 63.6444
[09/26 08:08:24 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 151.4189
[09/26 08:08:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 08:08:24 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 08:08:31 visual_prompt]: Epoch 40 / 100: avg data time: 4.53e-02, avg batch time: 0.4969, average train loss: 70.5431
[09/26 08:08:32 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1696, average loss: 56.0186
[09/26 08:08:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:08:32 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 08:08:39 visual_prompt]: Epoch 41 / 100: avg data time: 5.73e-02, avg batch time: 0.5060, average train loss: 67.8992
[09/26 08:08:40 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 70.9322
[09/26 08:08:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 08:08:40 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 08:08:47 visual_prompt]: Epoch 42 / 100: avg data time: 5.68e-02, avg batch time: 0.5051, average train loss: 92.2882
[09/26 08:08:49 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1688, average loss: 69.9531
[09/26 08:08:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 08:08:49 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 08:08:55 visual_prompt]: Epoch 43 / 100: avg data time: 4.50e-02, avg batch time: 0.4949, average train loss: 62.7707
[09/26 08:08:57 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1696, average loss: 89.8582
[09/26 08:08:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:08:57 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 08:09:04 visual_prompt]: Epoch 44 / 100: avg data time: 5.26e-02, avg batch time: 0.5038, average train loss: 84.2145
[09/26 08:09:05 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 56.9426
[09/26 08:09:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 83.00	
[09/26 08:09:05 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 08:09:12 visual_prompt]: Epoch 45 / 100: avg data time: 6.28e-02, avg batch time: 0.5114, average train loss: 85.2286
[09/26 08:09:14 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1694, average loss: 109.3129
[09/26 08:09:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 87.00	
[09/26 08:09:14 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 08:09:20 visual_prompt]: Epoch 46 / 100: avg data time: 4.40e-02, avg batch time: 0.4960, average train loss: 75.8179
[09/26 08:09:22 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 61.2538
[09/26 08:09:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 08:09:22 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 08:09:29 visual_prompt]: Epoch 47 / 100: avg data time: 4.53e-02, avg batch time: 0.4966, average train loss: 61.3783
[09/26 08:09:30 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1696, average loss: 80.5683
[09/26 08:09:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:09:30 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 08:09:37 visual_prompt]: Epoch 48 / 100: avg data time: 5.05e-02, avg batch time: 0.5009, average train loss: 75.3173
[09/26 08:09:38 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 92.5546
[09/26 08:09:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:09:38 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 08:09:45 visual_prompt]: Epoch 49 / 100: avg data time: 5.30e-02, avg batch time: 0.5024, average train loss: 70.1527
[09/26 08:09:47 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 54.5263
[09/26 08:09:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:09:47 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 08:09:53 visual_prompt]: Epoch 50 / 100: avg data time: 5.10e-02, avg batch time: 0.4998, average train loss: 54.1991
[09/26 08:09:55 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1693, average loss: 68.6031
[09/26 08:09:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:09:55 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 08:10:02 visual_prompt]: Epoch 51 / 100: avg data time: 5.25e-02, avg batch time: 0.5021, average train loss: 48.8173
[09/26 08:10:03 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 42.0122
[09/26 08:10:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:10:03 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 08:10:10 visual_prompt]: Epoch 52 / 100: avg data time: 4.63e-02, avg batch time: 0.4950, average train loss: 59.2165
[09/26 08:10:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 31.9234
[09/26 08:10:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 80.00	
[09/26 08:10:11 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 08:10:18 visual_prompt]: Epoch 53 / 100: avg data time: 5.50e-02, avg batch time: 0.5035, average train loss: 60.2913
[09/26 08:10:20 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 30.4414
[09/26 08:10:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:10:20 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 08:10:26 visual_prompt]: Epoch 54 / 100: avg data time: 5.79e-02, avg batch time: 0.5076, average train loss: 47.1828
[09/26 08:10:28 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1698, average loss: 24.8621
[09/26 08:10:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 08:10:28 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 08:10:35 visual_prompt]: Epoch 55 / 100: avg data time: 5.18e-02, avg batch time: 0.5005, average train loss: 67.5270
[09/26 08:10:36 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1695, average loss: 50.4973
[09/26 08:10:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 08:10:36 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 08:10:43 visual_prompt]: Epoch 56 / 100: avg data time: 5.35e-02, avg batch time: 0.5030, average train loss: 48.5782
[09/26 08:10:44 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1694, average loss: 73.0931
[09/26 08:10:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:10:44 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 08:10:51 visual_prompt]: Epoch 57 / 100: avg data time: 5.57e-02, avg batch time: 0.5042, average train loss: 66.9549
[09/26 08:10:53 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 81.5318
[09/26 08:10:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 08:10:53 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 08:10:59 visual_prompt]: Epoch 58 / 100: avg data time: 4.76e-02, avg batch time: 0.4970, average train loss: 43.5110
[09/26 08:11:01 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 70.2258
[09/26 08:11:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:11:01 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 08:11:08 visual_prompt]: Epoch 59 / 100: avg data time: 4.82e-02, avg batch time: 0.4981, average train loss: 42.8581
[09/26 08:11:09 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1696, average loss: 40.5433
[09/26 08:11:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 08:11:09 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 08:11:16 visual_prompt]: Epoch 60 / 100: avg data time: 5.52e-02, avg batch time: 0.5036, average train loss: 58.6048
[09/26 08:11:17 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1692, average loss: 112.1980
[09/26 08:11:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 08:11:17 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 08:11:24 visual_prompt]: Epoch 61 / 100: avg data time: 5.49e-02, avg batch time: 0.5041, average train loss: 47.3818
[09/26 08:11:26 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 24.3654
[09/26 08:11:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:11:26 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 08:11:32 visual_prompt]: Epoch 62 / 100: avg data time: 5.67e-02, avg batch time: 0.5063, average train loss: 36.3849
[09/26 08:11:34 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 51.1436
[09/26 08:11:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:11:34 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 08:11:41 visual_prompt]: Epoch 63 / 100: avg data time: 5.79e-02, avg batch time: 0.5065, average train loss: 34.2581
[09/26 08:11:42 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1689, average loss: 61.0500
[09/26 08:11:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:11:42 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 08:11:49 visual_prompt]: Epoch 64 / 100: avg data time: 4.89e-02, avg batch time: 0.5001, average train loss: 65.6404
[09/26 08:11:51 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1694, average loss: 22.6550
[09/26 08:11:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 08:11:51 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 08:11:57 visual_prompt]: Epoch 65 / 100: avg data time: 4.21e-02, avg batch time: 0.4917, average train loss: 43.2469
[09/26 08:11:59 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1693, average loss: 28.3363
[09/26 08:11:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:11:59 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 08:12:05 visual_prompt]: Epoch 66 / 100: avg data time: 4.15e-02, avg batch time: 0.4924, average train loss: 32.1512
[09/26 08:12:07 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1690, average loss: 43.8469
[09/26 08:12:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:12:07 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 08:12:14 visual_prompt]: Epoch 67 / 100: avg data time: 5.73e-02, avg batch time: 0.5063, average train loss: 26.0533
[09/26 08:12:15 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 37.1955
[09/26 08:12:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:12:15 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 08:12:22 visual_prompt]: Epoch 68 / 100: avg data time: 4.66e-02, avg batch time: 0.4970, average train loss: 34.0829
[09/26 08:12:23 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1691, average loss: 27.1602
[09/26 08:12:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:12:23 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 08:12:30 visual_prompt]: Epoch 69 / 100: avg data time: 5.87e-02, avg batch time: 0.5071, average train loss: 27.2931
[09/26 08:12:31 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 26.1273
[09/26 08:12:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 08:12:31 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 08:12:38 visual_prompt]: Epoch 70 / 100: avg data time: 4.76e-02, avg batch time: 0.4996, average train loss: 20.0153
[09/26 08:12:40 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 20.1646
[09/26 08:12:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 08:12:40 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 08:12:46 visual_prompt]: Epoch 71 / 100: avg data time: 5.38e-02, avg batch time: 0.5041, average train loss: 20.9424
[09/26 08:12:48 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1693, average loss: 32.8958
[09/26 08:12:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:12:48 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 08:12:55 visual_prompt]: Epoch 72 / 100: avg data time: 5.14e-02, avg batch time: 0.5008, average train loss: 25.4888
[09/26 08:12:56 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1691, average loss: 24.6004
[09/26 08:12:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:12:56 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 08:13:03 visual_prompt]: Epoch 73 / 100: avg data time: 6.10e-02, avg batch time: 0.5095, average train loss: 24.7018
[09/26 08:13:04 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1694, average loss: 24.3021
[09/26 08:13:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:13:04 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 08:13:11 visual_prompt]: Epoch 74 / 100: avg data time: 5.08e-02, avg batch time: 0.5004, average train loss: 25.6942
[09/26 08:13:13 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1695, average loss: 15.2487
[09/26 08:13:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:13:13 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 08:13:19 visual_prompt]: Epoch 75 / 100: avg data time: 4.94e-02, avg batch time: 0.5001, average train loss: 19.1161
[09/26 08:13:21 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1693, average loss: 20.0245
[09/26 08:13:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 08:13:21 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 08:13:28 visual_prompt]: Epoch 76 / 100: avg data time: 5.19e-02, avg batch time: 0.5012, average train loss: 16.8225
[09/26 08:13:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 18.0149
[09/26 08:13:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:13:29 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 08:13:36 visual_prompt]: Epoch 77 / 100: avg data time: 4.83e-02, avg batch time: 0.4986, average train loss: 14.6322
[09/26 08:13:37 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 16.7423
[09/26 08:13:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:13:37 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 08:13:44 visual_prompt]: Epoch 78 / 100: avg data time: 6.17e-02, avg batch time: 0.5113, average train loss: 12.9199
[09/26 08:13:46 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 12.7940
[09/26 08:13:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:13:46 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 08:13:52 visual_prompt]: Epoch 79 / 100: avg data time: 5.61e-02, avg batch time: 0.5073, average train loss: 11.0438
[09/26 08:13:54 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1694, average loss: 20.2365
[09/26 08:13:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:13:54 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 08:14:01 visual_prompt]: Epoch 80 / 100: avg data time: 4.03e-02, avg batch time: 0.4923, average train loss: 14.1826
[09/26 08:14:02 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 13.8900
[09/26 08:14:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.00	top5: 80.50	
[09/26 08:14:02 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 08:14:09 visual_prompt]: Epoch 81 / 100: avg data time: 5.14e-02, avg batch time: 0.4997, average train loss: 11.3194
[09/26 08:14:10 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1696, average loss: 18.2130
[09/26 08:14:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 08:14:10 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 08:14:17 visual_prompt]: Epoch 82 / 100: avg data time: 4.76e-02, avg batch time: 0.4987, average train loss: 14.6608
[09/26 08:14:18 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1693, average loss: 16.0195
[09/26 08:14:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:14:18 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 08:14:25 visual_prompt]: Epoch 83 / 100: avg data time: 4.32e-02, avg batch time: 0.4933, average train loss: 13.6878
[09/26 08:14:27 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 6.0203
[09/26 08:14:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:14:27 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 08:14:33 visual_prompt]: Epoch 84 / 100: avg data time: 4.22e-02, avg batch time: 0.4921, average train loss: 4.4306
[09/26 08:14:35 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1693, average loss: 3.4902
[09/26 08:14:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 08:14:35 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 08:14:41 visual_prompt]: Epoch 85 / 100: avg data time: 4.54e-02, avg batch time: 0.4957, average train loss: 4.7399
[09/26 08:14:43 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 6.3686
[09/26 08:14:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:14:43 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 08:14:50 visual_prompt]: Epoch 86 / 100: avg data time: 5.33e-02, avg batch time: 0.5020, average train loss: 4.8678
[09/26 08:14:51 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1697, average loss: 4.5222
[09/26 08:14:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:14:51 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 08:14:58 visual_prompt]: Epoch 87 / 100: avg data time: 5.48e-02, avg batch time: 0.5037, average train loss: 3.4892
[09/26 08:14:59 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1688, average loss: 4.1646
[09/26 08:14:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 08:14:59 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 08:15:06 visual_prompt]: Epoch 88 / 100: avg data time: 4.65e-02, avg batch time: 0.4948, average train loss: 3.1480
[09/26 08:15:08 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 2.3134
[09/26 08:15:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:15:08 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 08:15:14 visual_prompt]: Epoch 89 / 100: avg data time: 4.60e-02, avg batch time: 0.4962, average train loss: 2.3144
[09/26 08:15:16 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1697, average loss: 2.3874
[09/26 08:15:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:15:16 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 08:15:23 visual_prompt]: Epoch 90 / 100: avg data time: 5.71e-02, avg batch time: 0.5063, average train loss: 2.5697
[09/26 08:15:24 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1696, average loss: 1.9880
[09/26 08:15:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:15:24 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 08:15:31 visual_prompt]: Epoch 91 / 100: avg data time: 5.79e-02, avg batch time: 0.5064, average train loss: 1.8937
[09/26 08:15:32 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1688, average loss: 1.9934
[09/26 08:15:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:15:32 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 08:15:39 visual_prompt]: Epoch 92 / 100: avg data time: 4.74e-02, avg batch time: 0.4959, average train loss: 1.8633
[09/26 08:15:41 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1693, average loss: 2.0325
[09/26 08:15:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:15:41 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 08:15:48 visual_prompt]: Epoch 93 / 100: avg data time: 5.61e-02, avg batch time: 0.5046, average train loss: 1.8880
[09/26 08:15:49 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1699, average loss: 1.9973
[09/26 08:15:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:15:49 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 08:15:56 visual_prompt]: Epoch 94 / 100: avg data time: 5.14e-02, avg batch time: 0.5020, average train loss: 1.8386
[09/26 08:15:57 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1695, average loss: 1.8529
[09/26 08:15:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:15:57 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 08:16:04 visual_prompt]: Epoch 95 / 100: avg data time: 5.27e-02, avg batch time: 0.5014, average train loss: 1.8052
[09/26 08:16:05 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1691, average loss: 1.8275
[09/26 08:16:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:16:05 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 08:16:12 visual_prompt]: Epoch 96 / 100: avg data time: 3.99e-02, avg batch time: 0.4903, average train loss: 1.8053
[09/26 08:16:14 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 1.8390
[09/26 08:16:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 84.00	
[09/26 08:16:14 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 08:16:20 visual_prompt]: Epoch 97 / 100: avg data time: 4.43e-02, avg batch time: 0.4960, average train loss: 1.7888
[09/26 08:16:22 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1688, average loss: 1.8112
[09/26 08:16:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 84.00	
[09/26 08:16:22 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 08:16:28 visual_prompt]: Epoch 98 / 100: avg data time: 4.89e-02, avg batch time: 0.4990, average train loss: 1.7756
[09/26 08:16:30 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 1.8298
[09/26 08:16:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:16:30 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 08:16:37 visual_prompt]: Epoch 99 / 100: avg data time: 5.25e-02, avg batch time: 0.5023, average train loss: 1.7689
[09/26 08:16:38 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1693, average loss: 1.8161
[09/26 08:16:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:16:38 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 08:16:45 visual_prompt]: Epoch 100 / 100: avg data time: 5.42e-02, avg batch time: 0.5035, average train loss: 1.7639
[09/26 08:16:46 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1695, average loss: 1.8117
[09/26 08:16:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:16:46 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 08:16:46 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 08:16:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 08:16:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 08:16:46 visual_prompt]: Training with config:
[09/26 08:16:46 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 08:16:46 visual_prompt]: Loading training data...
[09/26 08:16:46 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 08:16:48 visual_prompt]: Number of images: 800
[09/26 08:16:48 visual_prompt]: Number of classes: 6 / 6
[09/26 08:16:48 visual_prompt]: Loading validation data...
[09/26 08:16:48 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 08:16:48 visual_prompt]: Number of images: 200
[09/26 08:16:48 visual_prompt]: Number of classes: 6 / 6
[09/26 08:16:48 visual_prompt]: Constructing models...
[09/26 08:16:51 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 08:16:51 visual_prompt]: tuned percent:0.540
[09/26 08:16:51 visual_prompt]: Device used for model: 0
[09/26 08:16:51 visual_prompt]: Setting up Evaluator...
[09/26 08:16:51 visual_prompt]: Setting up Trainer...
[09/26 08:16:51 visual_prompt]: 	Setting up the optimizer...
[09/26 08:16:51 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 08:16:57 visual_prompt]: Epoch 1 / 100: avg data time: 5.33e-02, avg batch time: 0.5033, average train loss: 2.9771
[09/26 08:16:59 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1687, average loss: 2.9268
[09/26 08:16:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:16:59 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 08:16:59 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 08:17:06 visual_prompt]: Epoch 2 / 100: avg data time: 5.05e-02, avg batch time: 0.4981, average train loss: 15.1746
[09/26 08:17:07 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 14.7337
[09/26 08:17:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 08:17:07 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 08:17:14 visual_prompt]: Epoch 3 / 100: avg data time: 4.94e-02, avg batch time: 0.4976, average train loss: 16.0237
[09/26 08:17:15 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1692, average loss: 22.8422
[09/26 08:17:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:17:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 08:17:22 visual_prompt]: Epoch 4 / 100: avg data time: 4.17e-02, avg batch time: 0.4915, average train loss: 33.0077
[09/26 08:17:24 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1693, average loss: 35.0211
[09/26 08:17:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:17:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 08:17:31 visual_prompt]: Epoch 5 / 100: avg data time: 6.14e-02, avg batch time: 0.5114, average train loss: 38.8797
[09/26 08:17:32 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1692, average loss: 43.7459
[09/26 08:17:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.00	
[09/26 08:17:32 visual_prompt]: Best epoch 5: best metric: 0.205
[09/26 08:17:32 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 08:17:39 visual_prompt]: Epoch 6 / 100: avg data time: 5.09e-02, avg batch time: 0.5000, average train loss: 46.5881
[09/26 08:17:40 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 43.5127
[09/26 08:17:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 08:17:40 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 08:17:47 visual_prompt]: Epoch 7 / 100: avg data time: 5.44e-02, avg batch time: 0.5037, average train loss: 56.4849
[09/26 08:17:49 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1691, average loss: 38.6175
[09/26 08:17:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:17:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 08:17:55 visual_prompt]: Epoch 8 / 100: avg data time: 4.87e-02, avg batch time: 0.4985, average train loss: 67.4690
[09/26 08:17:57 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1696, average loss: 58.2842
[09/26 08:17:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:17:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 08:18:04 visual_prompt]: Epoch 9 / 100: avg data time: 5.67e-02, avg batch time: 0.5049, average train loss: 72.4576
[09/26 08:18:05 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1693, average loss: 101.6209
[09/26 08:18:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 83.50	
[09/26 08:18:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 08:18:12 visual_prompt]: Epoch 10 / 100: avg data time: 5.18e-02, avg batch time: 0.5010, average train loss: 63.6951
[09/26 08:18:14 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 54.4342
[09/26 08:18:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:18:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 08:18:20 visual_prompt]: Epoch 11 / 100: avg data time: 5.54e-02, avg batch time: 0.5044, average train loss: 72.3105
[09/26 08:18:22 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1688, average loss: 77.2843
[09/26 08:18:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:18:22 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 08:18:29 visual_prompt]: Epoch 12 / 100: avg data time: 4.80e-02, avg batch time: 0.4988, average train loss: 100.4060
[09/26 08:18:30 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 163.0832
[09/26 08:18:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:18:30 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 08:18:37 visual_prompt]: Epoch 13 / 100: avg data time: 4.37e-02, avg batch time: 0.4918, average train loss: 106.7238
[09/26 08:18:38 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1694, average loss: 84.6972
[09/26 08:18:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 08:18:38 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 08:18:45 visual_prompt]: Epoch 14 / 100: avg data time: 5.32e-02, avg batch time: 0.5020, average train loss: 124.5658
[09/26 08:18:46 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1697, average loss: 138.4004
[09/26 08:18:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:18:46 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 08:18:53 visual_prompt]: Epoch 15 / 100: avg data time: 5.05e-02, avg batch time: 0.5006, average train loss: 74.1559
[09/26 08:18:55 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 116.3217
[09/26 08:18:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:18:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 08:19:01 visual_prompt]: Epoch 16 / 100: avg data time: 6.05e-02, avg batch time: 0.5086, average train loss: 105.6533
[09/26 08:19:03 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1691, average loss: 130.2488
[09/26 08:19:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 08:19:03 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 08:19:10 visual_prompt]: Epoch 17 / 100: avg data time: 5.49e-02, avg batch time: 0.5039, average train loss: 137.0644
[09/26 08:19:11 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 118.1659
[09/26 08:19:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:19:11 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 08:19:18 visual_prompt]: Epoch 18 / 100: avg data time: 4.71e-02, avg batch time: 0.4968, average train loss: 89.3721
[09/26 08:19:19 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 68.1513
[09/26 08:19:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:19:19 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 08:19:26 visual_prompt]: Epoch 19 / 100: avg data time: 4.63e-02, avg batch time: 0.4954, average train loss: 89.3738
[09/26 08:19:28 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 84.0335
[09/26 08:19:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 08:19:28 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 08:19:34 visual_prompt]: Epoch 20 / 100: avg data time: 4.84e-02, avg batch time: 0.4986, average train loss: 79.4864
[09/26 08:19:36 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 42.0138
[09/26 08:19:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 08:19:36 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 08:19:43 visual_prompt]: Epoch 21 / 100: avg data time: 5.05e-02, avg batch time: 0.4990, average train loss: 83.0714
[09/26 08:19:44 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1694, average loss: 114.3331
[09/26 08:19:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 08:19:44 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 08:19:51 visual_prompt]: Epoch 22 / 100: avg data time: 3.87e-02, avg batch time: 0.4901, average train loss: 123.2248
[09/26 08:19:52 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1695, average loss: 219.0575
[09/26 08:19:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:19:52 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 08:19:59 visual_prompt]: Epoch 23 / 100: avg data time: 4.28e-02, avg batch time: 0.4931, average train loss: 131.2051
[09/26 08:20:00 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1691, average loss: 147.8675
[09/26 08:20:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 08:20:00 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 08:20:07 visual_prompt]: Epoch 24 / 100: avg data time: 4.90e-02, avg batch time: 0.4992, average train loss: 104.4504
[09/26 08:20:09 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1694, average loss: 110.3455
[09/26 08:20:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:20:09 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 08:20:15 visual_prompt]: Epoch 25 / 100: avg data time: 5.30e-02, avg batch time: 0.5010, average train loss: 98.5096
[09/26 08:20:17 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 151.4805
[09/26 08:20:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:20:17 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 08:20:24 visual_prompt]: Epoch 26 / 100: avg data time: 5.22e-02, avg batch time: 0.5011, average train loss: 95.9512
[09/26 08:20:25 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 70.2781
[09/26 08:20:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:20:25 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 08:20:32 visual_prompt]: Epoch 27 / 100: avg data time: 5.23e-02, avg batch time: 0.5033, average train loss: 101.2035
[09/26 08:20:33 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1695, average loss: 142.4185
[09/26 08:20:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:20:33 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 08:20:40 visual_prompt]: Epoch 28 / 100: avg data time: 5.14e-02, avg batch time: 0.5004, average train loss: 103.9311
[09/26 08:20:41 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1694, average loss: 117.1242
[09/26 08:20:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:20:41 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 08:20:48 visual_prompt]: Epoch 29 / 100: avg data time: 5.61e-02, avg batch time: 0.5059, average train loss: 84.4383
[09/26 08:20:50 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1696, average loss: 114.2117
[09/26 08:20:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 08:20:50 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 08:20:57 visual_prompt]: Epoch 30 / 100: avg data time: 5.24e-02, avg batch time: 0.5025, average train loss: 92.7951
[09/26 08:20:58 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1693, average loss: 55.6310
[09/26 08:20:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 08:20:58 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 08:21:05 visual_prompt]: Epoch 31 / 100: avg data time: 5.50e-02, avg batch time: 0.5043, average train loss: 86.4492
[09/26 08:21:06 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 72.4042
[09/26 08:21:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 08:21:06 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 08:21:13 visual_prompt]: Epoch 32 / 100: avg data time: 5.77e-02, avg batch time: 0.5065, average train loss: 91.2831
[09/26 08:21:15 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1693, average loss: 65.8803
[09/26 08:21:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:21:15 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 08:21:21 visual_prompt]: Epoch 33 / 100: avg data time: 5.76e-02, avg batch time: 0.5056, average train loss: 90.5661
[09/26 08:21:23 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1693, average loss: 151.5162
[09/26 08:21:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:21:23 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 08:21:30 visual_prompt]: Epoch 34 / 100: avg data time: 4.16e-02, avg batch time: 0.4926, average train loss: 86.8780
[09/26 08:21:31 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1697, average loss: 64.9350
[09/26 08:21:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 08:21:31 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 08:21:38 visual_prompt]: Epoch 35 / 100: avg data time: 4.48e-02, avg batch time: 0.4935, average train loss: 81.5200
[09/26 08:21:39 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1700, average loss: 72.3395
[09/26 08:21:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:21:39 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 08:21:46 visual_prompt]: Epoch 36 / 100: avg data time: 5.62e-02, avg batch time: 0.5038, average train loss: 113.8621
[09/26 08:21:47 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 106.6028
[09/26 08:21:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:21:47 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 08:21:54 visual_prompt]: Epoch 37 / 100: avg data time: 5.87e-02, avg batch time: 0.5072, average train loss: 105.5376
[09/26 08:21:56 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 58.0662
[09/26 08:21:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 79.50	
[09/26 08:21:56 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 08:22:03 visual_prompt]: Epoch 38 / 100: avg data time: 6.20e-02, avg batch time: 0.5105, average train loss: 83.5090
[09/26 08:22:04 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1690, average loss: 98.4225
[09/26 08:22:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 08:22:04 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 08:22:11 visual_prompt]: Epoch 39 / 100: avg data time: 6.10e-02, avg batch time: 0.5097, average train loss: 71.7604
[09/26 08:22:13 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1697, average loss: 100.0468
[09/26 08:22:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:22:13 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 08:22:19 visual_prompt]: Epoch 40 / 100: avg data time: 4.34e-02, avg batch time: 0.4932, average train loss: 85.2652
[09/26 08:22:21 visual_prompt]: Inference (val):avg data time: 4.96e-05, avg batch time: 0.1693, average loss: 54.6806
[09/26 08:22:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:22:21 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 08:22:28 visual_prompt]: Epoch 41 / 100: avg data time: 4.80e-02, avg batch time: 0.4965, average train loss: 75.3954
[09/26 08:22:29 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1694, average loss: 73.4397
[09/26 08:22:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:22:29 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 08:22:36 visual_prompt]: Epoch 42 / 100: avg data time: 6.01e-02, avg batch time: 0.5087, average train loss: 84.6407
[09/26 08:22:37 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1691, average loss: 75.1506
[09/26 08:22:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:22:37 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 08:22:44 visual_prompt]: Epoch 43 / 100: avg data time: 5.87e-02, avg batch time: 0.5071, average train loss: 78.5791
[09/26 08:22:46 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 57.0363
[09/26 08:22:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:22:46 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 08:22:52 visual_prompt]: Epoch 44 / 100: avg data time: 6.09e-02, avg batch time: 0.5093, average train loss: 57.4456
[09/26 08:22:54 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1691, average loss: 24.0437
[09/26 08:22:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:22:54 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 08:23:01 visual_prompt]: Epoch 45 / 100: avg data time: 5.55e-02, avg batch time: 0.5046, average train loss: 42.3659
[09/26 08:23:02 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 39.0024
[09/26 08:23:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:23:02 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 08:23:09 visual_prompt]: Epoch 46 / 100: avg data time: 4.21e-02, avg batch time: 0.4919, average train loss: 58.1361
[09/26 08:23:10 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 53.3071
[09/26 08:23:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:23:10 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 08:23:17 visual_prompt]: Epoch 47 / 100: avg data time: 5.04e-02, avg batch time: 0.4997, average train loss: 76.0073
[09/26 08:23:19 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1691, average loss: 43.7765
[09/26 08:23:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:23:19 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 08:23:25 visual_prompt]: Epoch 48 / 100: avg data time: 5.62e-02, avg batch time: 0.5065, average train loss: 70.3383
[09/26 08:23:27 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1691, average loss: 59.3557
[09/26 08:23:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:23:27 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 08:23:34 visual_prompt]: Epoch 49 / 100: avg data time: 5.10e-02, avg batch time: 0.5002, average train loss: 55.5064
[09/26 08:23:35 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1689, average loss: 92.2278
[09/26 08:23:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:23:35 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 08:23:42 visual_prompt]: Epoch 50 / 100: avg data time: 4.65e-02, avg batch time: 0.4957, average train loss: 131.2685
[09/26 08:23:44 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1691, average loss: 149.5592
[09/26 08:23:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 84.00	
[09/26 08:23:44 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 08:23:50 visual_prompt]: Epoch 51 / 100: avg data time: 5.48e-02, avg batch time: 0.5036, average train loss: 86.3333
[09/26 08:23:52 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1689, average loss: 30.6882
[09/26 08:23:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.00	
[09/26 08:23:52 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 08:23:59 visual_prompt]: Epoch 52 / 100: avg data time: 5.60e-02, avg batch time: 0.5055, average train loss: 63.8270
[09/26 08:24:00 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 67.5338
[09/26 08:24:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:24:00 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 08:24:07 visual_prompt]: Epoch 53 / 100: avg data time: 5.82e-02, avg batch time: 0.5072, average train loss: 59.3125
[09/26 08:24:08 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1694, average loss: 30.2525
[09/26 08:24:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:24:08 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 08:24:15 visual_prompt]: Epoch 54 / 100: avg data time: 5.54e-02, avg batch time: 0.5041, average train loss: 48.2242
[09/26 08:24:17 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1691, average loss: 32.4720
[09/26 08:24:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:24:17 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 08:24:24 visual_prompt]: Epoch 55 / 100: avg data time: 5.70e-02, avg batch time: 0.5060, average train loss: 42.5629
[09/26 08:24:25 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1690, average loss: 91.9340
[09/26 08:24:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:24:25 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 08:24:32 visual_prompt]: Epoch 56 / 100: avg data time: 5.23e-02, avg batch time: 0.5012, average train loss: 82.7231
[09/26 08:24:33 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1693, average loss: 77.5448
[09/26 08:24:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:24:33 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 08:24:40 visual_prompt]: Epoch 57 / 100: avg data time: 5.54e-02, avg batch time: 0.5051, average train loss: 60.5901
[09/26 08:24:41 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1690, average loss: 62.0968
[09/26 08:24:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:24:41 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 08:24:48 visual_prompt]: Epoch 58 / 100: avg data time: 4.21e-02, avg batch time: 0.4929, average train loss: 44.3101
[09/26 08:24:50 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1689, average loss: 45.1969
[09/26 08:24:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:24:50 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 08:24:56 visual_prompt]: Epoch 59 / 100: avg data time: 4.17e-02, avg batch time: 0.4917, average train loss: 52.9261
[09/26 08:24:58 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1699, average loss: 37.8936
[09/26 08:24:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:24:58 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 08:25:04 visual_prompt]: Epoch 60 / 100: avg data time: 4.83e-02, avg batch time: 0.4979, average train loss: 51.0294
[09/26 08:25:06 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1691, average loss: 54.8502
[09/26 08:25:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:25:06 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 08:25:13 visual_prompt]: Epoch 61 / 100: avg data time: 5.60e-02, avg batch time: 0.5046, average train loss: 41.4483
[09/26 08:25:14 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 32.2982
[09/26 08:25:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:25:14 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 08:25:21 visual_prompt]: Epoch 62 / 100: avg data time: 5.77e-02, avg batch time: 0.5088, average train loss: 46.3265
[09/26 08:25:23 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1692, average loss: 40.5256
[09/26 08:25:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 08:25:23 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 08:25:30 visual_prompt]: Epoch 63 / 100: avg data time: 5.10e-02, avg batch time: 0.4998, average train loss: 55.6197
[09/26 08:25:31 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1692, average loss: 76.1639
[09/26 08:25:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:25:31 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 08:25:38 visual_prompt]: Epoch 64 / 100: avg data time: 5.71e-02, avg batch time: 0.5060, average train loss: 79.2637
[09/26 08:25:39 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1695, average loss: 74.4177
[09/26 08:25:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:25:39 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 08:25:46 visual_prompt]: Epoch 65 / 100: avg data time: 5.29e-02, avg batch time: 0.5012, average train loss: 40.1888
[09/26 08:25:48 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 33.1186
[09/26 08:25:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:25:48 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 08:25:54 visual_prompt]: Epoch 66 / 100: avg data time: 4.64e-02, avg batch time: 0.4971, average train loss: 33.8336
[09/26 08:25:56 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1690, average loss: 60.9210
[09/26 08:25:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 08:25:56 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 08:26:03 visual_prompt]: Epoch 67 / 100: avg data time: 5.60e-02, avg batch time: 0.5051, average train loss: 34.8101
[09/26 08:26:04 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1696, average loss: 28.3891
[09/26 08:26:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:26:04 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 08:26:11 visual_prompt]: Epoch 68 / 100: avg data time: 4.48e-02, avg batch time: 0.4956, average train loss: 22.3507
[09/26 08:26:12 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1696, average loss: 16.4729
[09/26 08:26:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 08:26:12 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 08:26:19 visual_prompt]: Epoch 69 / 100: avg data time: 5.87e-02, avg batch time: 0.5075, average train loss: 18.6142
[09/26 08:26:21 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1693, average loss: 15.8478
[09/26 08:26:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 08:26:21 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 08:26:28 visual_prompt]: Epoch 70 / 100: avg data time: 5.93e-02, avg batch time: 0.5085, average train loss: 21.1474
[09/26 08:26:29 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 27.8809
[09/26 08:26:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:26:29 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 08:26:36 visual_prompt]: Epoch 71 / 100: avg data time: 5.29e-02, avg batch time: 0.5012, average train loss: 18.3346
[09/26 08:26:37 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 20.9948
[09/26 08:26:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:26:37 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 08:26:44 visual_prompt]: Epoch 72 / 100: avg data time: 6.13e-02, avg batch time: 0.5112, average train loss: 26.4847
[09/26 08:26:46 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 27.5135
[09/26 08:26:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:26:46 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 08:26:52 visual_prompt]: Epoch 73 / 100: avg data time: 4.45e-02, avg batch time: 0.4944, average train loss: 18.3485
[09/26 08:26:54 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 13.3145
[09/26 08:26:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:26:54 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 08:27:00 visual_prompt]: Epoch 74 / 100: avg data time: 4.14e-02, avg batch time: 0.4931, average train loss: 15.1696
[09/26 08:27:02 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 6.6760
[09/26 08:27:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 08:27:02 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 08:27:09 visual_prompt]: Epoch 75 / 100: avg data time: 4.63e-02, avg batch time: 0.4971, average train loss: 10.6779
[09/26 08:27:10 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1695, average loss: 4.1297
[09/26 08:27:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:27:10 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 08:27:17 visual_prompt]: Epoch 76 / 100: avg data time: 4.08e-02, avg batch time: 0.4913, average train loss: 8.7284
[09/26 08:27:18 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1692, average loss: 11.2407
[09/26 08:27:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:27:18 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 08:27:25 visual_prompt]: Epoch 77 / 100: avg data time: 4.67e-02, avg batch time: 0.4982, average train loss: 6.8225
[09/26 08:27:26 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.1696, average loss: 2.7117
[09/26 08:27:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:27:26 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 08:27:33 visual_prompt]: Epoch 78 / 100: avg data time: 4.44e-02, avg batch time: 0.4945, average train loss: 3.4460
[09/26 08:27:34 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 4.2435
[09/26 08:27:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:27:34 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 08:27:41 visual_prompt]: Epoch 79 / 100: avg data time: 4.11e-02, avg batch time: 0.4916, average train loss: 2.9749
[09/26 08:27:42 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 2.2776
[09/26 08:27:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:27:42 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 08:27:49 visual_prompt]: Epoch 80 / 100: avg data time: 4.16e-02, avg batch time: 0.4920, average train loss: 2.5147
[09/26 08:27:51 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1694, average loss: 2.4175
[09/26 08:27:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:27:51 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 08:27:57 visual_prompt]: Epoch 81 / 100: avg data time: 5.28e-02, avg batch time: 0.5017, average train loss: 2.1009
[09/26 08:27:59 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 2.1015
[09/26 08:27:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:27:59 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 08:28:06 visual_prompt]: Epoch 82 / 100: avg data time: 5.07e-02, avg batch time: 0.4993, average train loss: 2.0053
[09/26 08:28:07 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1692, average loss: 2.3238
[09/26 08:28:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:28:07 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 08:28:14 visual_prompt]: Epoch 83 / 100: avg data time: 4.29e-02, avg batch time: 0.4935, average train loss: 2.0156
[09/26 08:28:15 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1695, average loss: 2.1884
[09/26 08:28:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:28:15 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 08:28:22 visual_prompt]: Epoch 84 / 100: avg data time: 4.03e-02, avg batch time: 0.4918, average train loss: 2.0027
[09/26 08:28:23 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 1.8999
[09/26 08:28:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 83.50	
[09/26 08:28:23 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 08:28:30 visual_prompt]: Epoch 85 / 100: avg data time: 5.53e-02, avg batch time: 0.5052, average train loss: 1.9517
[09/26 08:28:32 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1691, average loss: 1.9895
[09/26 08:28:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:28:32 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 08:28:38 visual_prompt]: Epoch 86 / 100: avg data time: 5.65e-02, avg batch time: 0.5061, average train loss: 1.8565
[09/26 08:28:40 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1690, average loss: 1.9548
[09/26 08:28:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 88.50	
[09/26 08:28:40 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 08:28:47 visual_prompt]: Epoch 87 / 100: avg data time: 4.32e-02, avg batch time: 0.4930, average train loss: 1.7897
[09/26 08:28:48 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1689, average loss: 1.8248
[09/26 08:28:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:28:48 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 08:28:55 visual_prompt]: Epoch 88 / 100: avg data time: 3.92e-02, avg batch time: 0.4890, average train loss: 1.8382
[09/26 08:28:56 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 1.9145
[09/26 08:28:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:28:56 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 08:29:03 visual_prompt]: Epoch 89 / 100: avg data time: 4.31e-02, avg batch time: 0.4943, average train loss: 1.7943
[09/26 08:29:04 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 2.1299
[09/26 08:29:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:29:04 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 08:29:11 visual_prompt]: Epoch 90 / 100: avg data time: 5.89e-02, avg batch time: 0.5083, average train loss: 1.8118
[09/26 08:29:13 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1697, average loss: 1.8126
[09/26 08:29:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 90.50	
[09/26 08:29:13 visual_prompt]: Best epoch 90: best metric: 0.265
[09/26 08:29:13 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 08:29:20 visual_prompt]: Epoch 91 / 100: avg data time: 5.85e-02, avg batch time: 0.5071, average train loss: 1.6088
[09/26 08:29:21 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1695, average loss: 1.6635
[09/26 08:29:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 95.00	
[09/26 08:29:21 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 08:29:28 visual_prompt]: Epoch 92 / 100: avg data time: 5.22e-02, avg batch time: 0.5036, average train loss: 1.4387
[09/26 08:29:29 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1699, average loss: 2.0913
[09/26 08:29:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 88.50	
[09/26 08:29:29 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 08:29:36 visual_prompt]: Epoch 93 / 100: avg data time: 4.30e-02, avg batch time: 0.4928, average train loss: 1.4765
[09/26 08:29:37 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 1.4805
[09/26 08:29:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 96.00	
[09/26 08:29:37 visual_prompt]: Best epoch 93: best metric: 0.315
[09/26 08:29:37 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 08:29:44 visual_prompt]: Epoch 94 / 100: avg data time: 4.40e-02, avg batch time: 0.4940, average train loss: 1.3647
[09/26 08:29:45 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1699, average loss: 1.4124
[09/26 08:29:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 98.00	
[09/26 08:29:45 visual_prompt]: Best epoch 94: best metric: 0.330
[09/26 08:29:45 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 08:29:52 visual_prompt]: Epoch 95 / 100: avg data time: 5.23e-02, avg batch time: 0.5021, average train loss: 1.3035
[09/26 08:29:54 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1694, average loss: 1.4379
[09/26 08:29:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 08:29:54 visual_prompt]: Best epoch 95: best metric: 0.345
[09/26 08:29:54 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 08:30:01 visual_prompt]: Epoch 96 / 100: avg data time: 5.54e-02, avg batch time: 0.5053, average train loss: 1.2717
[09/26 08:30:02 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1691, average loss: 1.4625
[09/26 08:30:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 98.50	
[09/26 08:30:02 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 08:30:09 visual_prompt]: Epoch 97 / 100: avg data time: 5.94e-02, avg batch time: 0.5098, average train loss: 1.2753
[09/26 08:30:10 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1694, average loss: 1.3694
[09/26 08:30:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 98.00	
[09/26 08:30:10 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 08:30:17 visual_prompt]: Epoch 98 / 100: avg data time: 5.25e-02, avg batch time: 0.5019, average train loss: 1.1844
[09/26 08:30:20 visual_prompt]: Inference (val):avg data time: 3.29e-04, avg batch time: 0.3440, average loss: 1.3332
[09/26 08:30:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.50	
[09/26 08:30:20 visual_prompt]: Best epoch 98: best metric: 0.390
[09/26 08:30:20 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 08:30:26 visual_prompt]: Epoch 99 / 100: avg data time: 5.04e-02, avg batch time: 0.4987, average train loss: 1.1449
[09/26 08:30:28 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1689, average loss: 1.3460
[09/26 08:30:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 08:30:28 visual_prompt]: Best epoch 99: best metric: 0.395
[09/26 08:30:28 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 08:30:35 visual_prompt]: Epoch 100 / 100: avg data time: 5.85e-02, avg batch time: 0.5068, average train loss: 1.1437
[09/26 08:30:36 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1691, average loss: 1.3297
[09/26 08:30:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 08:30:36 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 08:30:36 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 08:30:36 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 08:30:36 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 08:30:36 visual_prompt]: Training with config:
[09/26 08:30:36 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 08:30:36 visual_prompt]: Loading training data...
[09/26 08:30:36 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 08:30:38 visual_prompt]: Number of images: 800
[09/26 08:30:38 visual_prompt]: Number of classes: 6 / 6
[09/26 08:30:38 visual_prompt]: Loading validation data...
[09/26 08:30:38 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 08:30:38 visual_prompt]: Number of images: 200
[09/26 08:30:38 visual_prompt]: Number of classes: 6 / 6
[09/26 08:30:38 visual_prompt]: Constructing models...
[09/26 08:30:41 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 08:30:41 visual_prompt]: tuned percent:0.540
[09/26 08:30:41 visual_prompt]: Device used for model: 0
[09/26 08:30:41 visual_prompt]: Setting up Evaluator...
[09/26 08:30:41 visual_prompt]: Setting up Trainer...
[09/26 08:30:41 visual_prompt]: 	Setting up the optimizer...
[09/26 08:30:41 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 08:30:48 visual_prompt]: Epoch 1 / 100: avg data time: 5.39e-02, avg batch time: 0.5003, average train loss: 2.9668
[09/26 08:30:49 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1691, average loss: 2.9268
[09/26 08:30:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:30:49 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 08:30:49 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 08:30:56 visual_prompt]: Epoch 2 / 100: avg data time: 5.48e-02, avg batch time: 0.5012, average train loss: 13.1925
[09/26 08:30:57 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1686, average loss: 13.7083
[09/26 08:30:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 08:30:57 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 08:31:04 visual_prompt]: Epoch 3 / 100: avg data time: 4.40e-02, avg batch time: 0.4919, average train loss: 39.6792
[09/26 08:31:06 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1684, average loss: 60.7204
[09/26 08:31:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:31:06 visual_prompt]: Best epoch 3: best metric: 0.205
[09/26 08:31:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 08:31:12 visual_prompt]: Epoch 4 / 100: avg data time: 4.99e-02, avg batch time: 0.4981, average train loss: 45.6448
[09/26 08:31:14 visual_prompt]: Inference (val):avg data time: 3.23e-04, avg batch time: 0.2430, average loss: 25.1168
[09/26 08:31:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:31:14 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 08:31:21 visual_prompt]: Epoch 5 / 100: avg data time: 4.68e-02, avg batch time: 0.4957, average train loss: 61.3774
[09/26 08:31:22 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1686, average loss: 35.1039
[09/26 08:31:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 82.50	
[09/26 08:31:22 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 08:31:29 visual_prompt]: Epoch 6 / 100: avg data time: 5.72e-02, avg batch time: 0.5056, average train loss: 66.0375
[09/26 08:31:31 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.1685, average loss: 86.9809
[09/26 08:31:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:31:31 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 08:31:38 visual_prompt]: Epoch 7 / 100: avg data time: 5.87e-02, avg batch time: 0.5072, average train loss: 69.4873
[09/26 08:31:39 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 103.4392
[09/26 08:31:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:31:39 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 08:31:46 visual_prompt]: Epoch 8 / 100: avg data time: 5.32e-02, avg batch time: 0.5021, average train loss: 68.2616
[09/26 08:31:47 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1691, average loss: 88.3327
[09/26 08:31:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 08:31:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 08:31:54 visual_prompt]: Epoch 9 / 100: avg data time: 5.42e-02, avg batch time: 0.5017, average train loss: 100.8462
[09/26 08:31:56 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1690, average loss: 66.0130
[09/26 08:31:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:31:56 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 08:32:03 visual_prompt]: Epoch 10 / 100: avg data time: 5.90e-02, avg batch time: 0.5071, average train loss: 61.3527
[09/26 08:32:04 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1687, average loss: 80.2086
[09/26 08:32:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 08:32:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 08:32:11 visual_prompt]: Epoch 11 / 100: avg data time: 5.51e-02, avg batch time: 0.5038, average train loss: 99.9636
[09/26 08:32:12 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1693, average loss: 54.4314
[09/26 08:32:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 85.50	
[09/26 08:32:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 08:32:19 visual_prompt]: Epoch 12 / 100: avg data time: 4.84e-02, avg batch time: 0.4978, average train loss: 62.7576
[09/26 08:32:21 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1690, average loss: 47.3711
[09/26 08:32:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 08:32:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 08:32:27 visual_prompt]: Epoch 13 / 100: avg data time: 4.58e-02, avg batch time: 0.4939, average train loss: 76.7073
[09/26 08:32:29 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1687, average loss: 134.0858
[09/26 08:32:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:32:29 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 08:32:35 visual_prompt]: Epoch 14 / 100: avg data time: 4.23e-02, avg batch time: 0.4913, average train loss: 108.4439
[09/26 08:32:37 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1692, average loss: 121.5944
[09/26 08:32:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 08:32:37 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 08:32:44 visual_prompt]: Epoch 15 / 100: avg data time: 4.17e-02, avg batch time: 0.4944, average train loss: 109.6831
[09/26 08:32:45 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1693, average loss: 72.0125
[09/26 08:32:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:32:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 08:32:52 visual_prompt]: Epoch 16 / 100: avg data time: 4.44e-02, avg batch time: 0.4933, average train loss: 95.2734
[09/26 08:32:53 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 65.5991
[09/26 08:32:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:32:53 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 08:33:00 visual_prompt]: Epoch 17 / 100: avg data time: 4.26e-02, avg batch time: 0.4927, average train loss: 103.4904
[09/26 08:33:01 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1695, average loss: 198.9729
[09/26 08:33:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:33:01 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 08:33:08 visual_prompt]: Epoch 18 / 100: avg data time: 5.48e-02, avg batch time: 0.5025, average train loss: 167.2487
[09/26 08:33:10 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1692, average loss: 117.2143
[09/26 08:33:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 08:33:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 08:33:16 visual_prompt]: Epoch 19 / 100: avg data time: 4.35e-02, avg batch time: 0.4921, average train loss: 89.4768
[09/26 08:33:18 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1691, average loss: 43.7321
[09/26 08:33:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:33:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 08:33:25 visual_prompt]: Epoch 20 / 100: avg data time: 4.64e-02, avg batch time: 0.4960, average train loss: 77.7828
[09/26 08:33:26 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1689, average loss: 86.9218
[09/26 08:33:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:33:26 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 08:33:33 visual_prompt]: Epoch 21 / 100: avg data time: 5.12e-02, avg batch time: 0.5000, average train loss: 73.9001
[09/26 08:33:34 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1689, average loss: 91.1412
[09/26 08:33:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 84.00	
[09/26 08:33:34 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 08:33:41 visual_prompt]: Epoch 22 / 100: avg data time: 5.41e-02, avg batch time: 0.5035, average train loss: 95.9363
[09/26 08:33:43 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1695, average loss: 60.8483
[09/26 08:33:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:33:43 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 08:33:49 visual_prompt]: Epoch 23 / 100: avg data time: 5.39e-02, avg batch time: 0.5022, average train loss: 126.6321
[09/26 08:33:51 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 41.0385
[09/26 08:33:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:33:51 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 08:33:58 visual_prompt]: Epoch 24 / 100: avg data time: 4.89e-02, avg batch time: 0.4972, average train loss: 95.1588
[09/26 08:33:59 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1689, average loss: 147.3298
[09/26 08:33:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 08:33:59 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 08:34:06 visual_prompt]: Epoch 25 / 100: avg data time: 4.72e-02, avg batch time: 0.4961, average train loss: 124.4054
[09/26 08:34:07 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1691, average loss: 62.7378
[09/26 08:34:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:34:07 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 08:34:14 visual_prompt]: Epoch 26 / 100: avg data time: 4.75e-02, avg batch time: 0.4976, average train loss: 83.9740
[09/26 08:34:16 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1691, average loss: 68.4762
[09/26 08:34:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:34:16 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 08:34:22 visual_prompt]: Epoch 27 / 100: avg data time: 5.27e-02, avg batch time: 0.5014, average train loss: 72.2638
[09/26 08:34:24 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1697, average loss: 79.0486
[09/26 08:34:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 08:34:24 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 08:34:31 visual_prompt]: Epoch 28 / 100: avg data time: 4.92e-02, avg batch time: 0.4978, average train loss: 73.5715
[09/26 08:34:32 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1697, average loss: 46.7899
[09/26 08:34:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 08:34:32 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 08:34:39 visual_prompt]: Epoch 29 / 100: avg data time: 5.33e-02, avg batch time: 0.5025, average train loss: 52.5352
[09/26 08:34:40 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1695, average loss: 40.9180
[09/26 08:34:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:34:40 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 08:34:47 visual_prompt]: Epoch 30 / 100: avg data time: 5.26e-02, avg batch time: 0.5012, average train loss: 53.6250
[09/26 08:34:49 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 71.2945
[09/26 08:34:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 08:34:49 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 08:34:55 visual_prompt]: Epoch 31 / 100: avg data time: 4.37e-02, avg batch time: 0.4931, average train loss: 54.6185
[09/26 08:34:57 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 52.8911
[09/26 08:34:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:34:57 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 08:35:03 visual_prompt]: Epoch 32 / 100: avg data time: 5.43e-02, avg batch time: 0.5027, average train loss: 46.5072
[09/26 08:35:05 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 70.8708
[09/26 08:35:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:35:05 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 08:35:12 visual_prompt]: Epoch 33 / 100: avg data time: 5.05e-02, avg batch time: 0.5001, average train loss: 76.0561
[09/26 08:35:13 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1688, average loss: 165.5277
[09/26 08:35:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:35:13 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 08:35:20 visual_prompt]: Epoch 34 / 100: avg data time: 4.20e-02, avg batch time: 0.4912, average train loss: 127.8427
[09/26 08:35:21 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1689, average loss: 130.2779
[09/26 08:35:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:35:21 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 08:35:28 visual_prompt]: Epoch 35 / 100: avg data time: 6.05e-02, avg batch time: 0.5081, average train loss: 110.4757
[09/26 08:35:30 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1691, average loss: 59.6571
[09/26 08:35:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 08:35:30 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 08:35:37 visual_prompt]: Epoch 36 / 100: avg data time: 5.13e-02, avg batch time: 0.5009, average train loss: 82.3485
[09/26 08:35:38 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1695, average loss: 120.5731
[09/26 08:35:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 08:35:38 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 08:35:45 visual_prompt]: Epoch 37 / 100: avg data time: 4.45e-02, avg batch time: 0.4947, average train loss: 74.0762
[09/26 08:35:46 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1695, average loss: 63.5908
[09/26 08:35:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:35:46 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 08:35:53 visual_prompt]: Epoch 38 / 100: avg data time: 5.17e-02, avg batch time: 0.5017, average train loss: 54.6318
[09/26 08:35:54 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1695, average loss: 63.5748
[09/26 08:35:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 08:35:54 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 08:36:01 visual_prompt]: Epoch 39 / 100: avg data time: 4.76e-02, avg batch time: 0.4965, average train loss: 51.8777
[09/26 08:36:03 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1689, average loss: 39.6458
[09/26 08:36:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 08:36:03 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 08:36:10 visual_prompt]: Epoch 40 / 100: avg data time: 5.89e-02, avg batch time: 0.5068, average train loss: 71.0178
[09/26 08:36:11 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 57.8968
[09/26 08:36:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 08:36:11 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 08:36:18 visual_prompt]: Epoch 41 / 100: avg data time: 5.35e-02, avg batch time: 0.5017, average train loss: 86.7954
[09/26 08:36:19 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1688, average loss: 44.0959
[09/26 08:36:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:36:19 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 08:36:26 visual_prompt]: Epoch 42 / 100: avg data time: 5.45e-02, avg batch time: 0.5026, average train loss: 135.4946
[09/26 08:36:28 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1693, average loss: 43.2944
[09/26 08:36:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 08:36:28 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 08:36:34 visual_prompt]: Epoch 43 / 100: avg data time: 4.23e-02, avg batch time: 0.4934, average train loss: 106.7129
[09/26 08:36:36 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 78.9314
[09/26 08:36:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:36:36 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 08:36:42 visual_prompt]: Epoch 44 / 100: avg data time: 5.50e-02, avg batch time: 0.5039, average train loss: 85.9750
[09/26 08:36:44 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1694, average loss: 75.5173
[09/26 08:36:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:36:44 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 08:36:51 visual_prompt]: Epoch 45 / 100: avg data time: 4.72e-02, avg batch time: 0.4970, average train loss: 76.6433
[09/26 08:36:52 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1690, average loss: 52.9881
[09/26 08:36:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:36:52 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 08:36:59 visual_prompt]: Epoch 46 / 100: avg data time: 4.07e-02, avg batch time: 0.4909, average train loss: 46.5770
[09/26 08:37:00 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1696, average loss: 39.7565
[09/26 08:37:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:37:00 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 08:37:07 visual_prompt]: Epoch 47 / 100: avg data time: 5.10e-02, avg batch time: 0.5001, average train loss: 42.9881
[09/26 08:37:08 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 47.3237
[09/26 08:37:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:37:08 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 08:37:15 visual_prompt]: Epoch 48 / 100: avg data time: 4.86e-02, avg batch time: 0.4975, average train loss: 39.5825
[09/26 08:37:16 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 20.7239
[09/26 08:37:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 08:37:16 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 08:37:23 visual_prompt]: Epoch 49 / 100: avg data time: 4.24e-02, avg batch time: 0.4938, average train loss: 28.6156
[09/26 08:37:25 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1690, average loss: 23.0113
[09/26 08:37:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:37:25 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 08:37:31 visual_prompt]: Epoch 50 / 100: avg data time: 4.35e-02, avg batch time: 0.4919, average train loss: 25.8799
[09/26 08:37:33 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1690, average loss: 13.6005
[09/26 08:37:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 08:37:33 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 08:37:40 visual_prompt]: Epoch 51 / 100: avg data time: 4.35e-02, avg batch time: 0.4933, average train loss: 27.4744
[09/26 08:37:41 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1695, average loss: 47.0576
[09/26 08:37:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 08:37:41 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 08:37:48 visual_prompt]: Epoch 52 / 100: avg data time: 5.18e-02, avg batch time: 0.5006, average train loss: 38.0080
[09/26 08:37:49 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1692, average loss: 29.4934
[09/26 08:37:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:37:49 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 08:37:56 visual_prompt]: Epoch 53 / 100: avg data time: 4.37e-02, avg batch time: 0.4921, average train loss: 43.7740
[09/26 08:37:58 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1696, average loss: 58.2367
[09/26 08:37:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:37:58 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 08:38:04 visual_prompt]: Epoch 54 / 100: avg data time: 4.37e-02, avg batch time: 0.4939, average train loss: 45.7892
[09/26 08:38:06 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1695, average loss: 32.0812
[09/26 08:38:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:38:06 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 08:38:12 visual_prompt]: Epoch 55 / 100: avg data time: 4.11e-02, avg batch time: 0.4911, average train loss: 36.4205
[09/26 08:38:14 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1694, average loss: 33.5727
[09/26 08:38:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:38:14 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 08:38:21 visual_prompt]: Epoch 56 / 100: avg data time: 4.99e-02, avg batch time: 0.4990, average train loss: 37.9130
[09/26 08:38:22 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1690, average loss: 23.7469
[09/26 08:38:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:38:22 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 08:38:29 visual_prompt]: Epoch 57 / 100: avg data time: 5.80e-02, avg batch time: 0.5070, average train loss: 29.5700
[09/26 08:38:30 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.1695, average loss: 20.5635
[09/26 08:38:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 08:38:30 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 08:38:37 visual_prompt]: Epoch 58 / 100: avg data time: 4.38e-02, avg batch time: 0.4928, average train loss: 19.1394
[09/26 08:38:39 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1691, average loss: 14.0172
[09/26 08:38:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 08:38:39 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 08:38:45 visual_prompt]: Epoch 59 / 100: avg data time: 5.45e-02, avg batch time: 0.5020, average train loss: 18.4832
[09/26 08:38:47 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 22.3224
[09/26 08:38:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:38:47 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 08:38:54 visual_prompt]: Epoch 60 / 100: avg data time: 5.48e-02, avg batch time: 0.5042, average train loss: 14.8515
[09/26 08:38:55 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 17.4182
[09/26 08:38:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:38:55 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 08:39:02 visual_prompt]: Epoch 61 / 100: avg data time: 4.18e-02, avg batch time: 0.4951, average train loss: 11.1343
[09/26 08:39:03 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 11.0007
[09/26 08:39:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:39:03 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 08:39:10 visual_prompt]: Epoch 62 / 100: avg data time: 4.50e-02, avg batch time: 0.4940, average train loss: 12.1110
[09/26 08:39:11 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 9.5263
[09/26 08:39:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:39:11 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 08:39:18 visual_prompt]: Epoch 63 / 100: avg data time: 4.74e-02, avg batch time: 0.4986, average train loss: 11.9572
[09/26 08:39:20 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1690, average loss: 16.6534
[09/26 08:39:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:39:20 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 08:39:27 visual_prompt]: Epoch 64 / 100: avg data time: 5.40e-02, avg batch time: 0.5022, average train loss: 13.2851
[09/26 08:39:28 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 17.0622
[09/26 08:39:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:39:28 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 08:39:35 visual_prompt]: Epoch 65 / 100: avg data time: 5.22e-02, avg batch time: 0.5029, average train loss: 11.0302
[09/26 08:39:36 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 10.7408
[09/26 08:39:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:39:36 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 08:39:43 visual_prompt]: Epoch 66 / 100: avg data time: 5.54e-02, avg batch time: 0.5045, average train loss: 7.4540
[09/26 08:39:44 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1689, average loss: 3.6874
[09/26 08:39:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:39:44 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 08:39:51 visual_prompt]: Epoch 67 / 100: avg data time: 5.39e-02, avg batch time: 0.5015, average train loss: 4.2929
[09/26 08:39:53 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1693, average loss: 3.3512
[09/26 08:39:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:39:53 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 08:39:59 visual_prompt]: Epoch 68 / 100: avg data time: 4.40e-02, avg batch time: 0.4925, average train loss: 5.9248
[09/26 08:40:01 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 5.0299
[09/26 08:40:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:40:01 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 08:40:08 visual_prompt]: Epoch 69 / 100: avg data time: 5.24e-02, avg batch time: 0.5013, average train loss: 4.6316
[09/26 08:40:09 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1691, average loss: 5.8287
[09/26 08:40:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:40:09 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 08:40:16 visual_prompt]: Epoch 70 / 100: avg data time: 4.46e-02, avg batch time: 0.4953, average train loss: 4.4103
[09/26 08:40:17 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1693, average loss: 4.4894
[09/26 08:40:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:40:17 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 08:40:24 visual_prompt]: Epoch 71 / 100: avg data time: 5.21e-02, avg batch time: 0.5004, average train loss: 3.3600
[09/26 08:40:26 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1690, average loss: 2.4123
[09/26 08:40:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:40:26 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 08:40:32 visual_prompt]: Epoch 72 / 100: avg data time: 4.13e-02, avg batch time: 0.4911, average train loss: 3.2124
[09/26 08:40:34 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1690, average loss: 2.7829
[09/26 08:40:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:40:34 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 08:40:40 visual_prompt]: Epoch 73 / 100: avg data time: 4.67e-02, avg batch time: 0.4949, average train loss: 3.1706
[09/26 08:40:42 visual_prompt]: Inference (val):avg data time: 4.82e-05, avg batch time: 0.1688, average loss: 3.3899
[09/26 08:40:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 08:40:42 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 08:40:49 visual_prompt]: Epoch 74 / 100: avg data time: 5.42e-02, avg batch time: 0.5015, average train loss: 2.5685
[09/26 08:40:50 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1692, average loss: 2.8255
[09/26 08:40:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:40:50 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 08:40:57 visual_prompt]: Epoch 75 / 100: avg data time: 5.45e-02, avg batch time: 0.5032, average train loss: 2.3577
[09/26 08:40:58 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1689, average loss: 1.8836
[09/26 08:40:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:40:58 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 08:41:05 visual_prompt]: Epoch 76 / 100: avg data time: 5.35e-02, avg batch time: 0.5032, average train loss: 2.1777
[09/26 08:41:07 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1693, average loss: 2.3878
[09/26 08:41:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 82.50	
[09/26 08:41:07 visual_prompt]: Best epoch 76: best metric: 0.210
[09/26 08:41:07 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 08:41:14 visual_prompt]: Epoch 77 / 100: avg data time: 5.67e-02, avg batch time: 0.5041, average train loss: 2.0097
[09/26 08:41:15 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1686, average loss: 1.9091
[09/26 08:41:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:41:15 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 08:41:22 visual_prompt]: Epoch 78 / 100: avg data time: 5.86e-02, avg batch time: 0.5057, average train loss: 1.9558
[09/26 08:41:23 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 2.0241
[09/26 08:41:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:41:23 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 08:41:30 visual_prompt]: Epoch 79 / 100: avg data time: 4.63e-02, avg batch time: 0.4951, average train loss: 1.9741
[09/26 08:41:32 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1690, average loss: 1.9717
[09/26 08:41:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:41:32 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 08:41:38 visual_prompt]: Epoch 80 / 100: avg data time: 5.62e-02, avg batch time: 0.5046, average train loss: 1.9203
[09/26 08:41:40 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1694, average loss: 2.2284
[09/26 08:41:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:41:40 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 08:41:47 visual_prompt]: Epoch 81 / 100: avg data time: 5.44e-02, avg batch time: 0.5024, average train loss: 1.9081
[09/26 08:41:48 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1688, average loss: 2.0018
[09/26 08:41:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 08:41:48 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 08:41:55 visual_prompt]: Epoch 82 / 100: avg data time: 5.85e-02, avg batch time: 0.5058, average train loss: 2.0539
[09/26 08:41:56 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1688, average loss: 2.0908
[09/26 08:41:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:41:56 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 08:42:03 visual_prompt]: Epoch 83 / 100: avg data time: 5.16e-02, avg batch time: 0.4998, average train loss: 1.9891
[09/26 08:42:05 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 2.0572
[09/26 08:42:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 84.00	
[09/26 08:42:05 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 08:42:12 visual_prompt]: Epoch 84 / 100: avg data time: 5.58e-02, avg batch time: 0.5036, average train loss: 1.9212
[09/26 08:42:13 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 1.8591
[09/26 08:42:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:42:13 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 08:42:20 visual_prompt]: Epoch 85 / 100: avg data time: 5.41e-02, avg batch time: 0.5028, average train loss: 1.8060
[09/26 08:42:21 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1688, average loss: 1.7933
[09/26 08:42:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 90.00	
[09/26 08:42:21 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 08:42:28 visual_prompt]: Epoch 86 / 100: avg data time: 4.77e-02, avg batch time: 0.4958, average train loss: 1.7715
[09/26 08:42:29 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1693, average loss: 1.8001
[09/26 08:42:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 87.00	
[09/26 08:42:29 visual_prompt]: Best epoch 86: best metric: 0.245
[09/26 08:42:29 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 08:42:36 visual_prompt]: Epoch 87 / 100: avg data time: 6.02e-02, avg batch time: 0.5099, average train loss: 1.7371
[09/26 08:42:38 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1694, average loss: 1.9306
[09/26 08:42:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 84.00	
[09/26 08:42:38 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 08:42:45 visual_prompt]: Epoch 88 / 100: avg data time: 5.89e-02, avg batch time: 0.5081, average train loss: 1.7660
[09/26 08:42:46 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1692, average loss: 1.8768
[09/26 08:42:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 86.00	
[09/26 08:42:46 visual_prompt]: Best epoch 88: best metric: 0.250
[09/26 08:42:46 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 08:42:53 visual_prompt]: Epoch 89 / 100: avg data time: 4.92e-02, avg batch time: 0.4986, average train loss: 1.7095
[09/26 08:42:54 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 1.8428
[09/26 08:42:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 83.50	
[09/26 08:42:54 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 08:43:01 visual_prompt]: Epoch 90 / 100: avg data time: 4.63e-02, avg batch time: 0.4970, average train loss: 1.8504
[09/26 08:43:03 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.1693, average loss: 1.7380
[09/26 08:43:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 86.50	
[09/26 08:43:03 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 08:43:09 visual_prompt]: Epoch 91 / 100: avg data time: 4.44e-02, avg batch time: 0.4941, average train loss: 1.7210
[09/26 08:43:11 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1692, average loss: 1.8284
[09/26 08:43:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:43:11 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 08:43:18 visual_prompt]: Epoch 92 / 100: avg data time: 5.07e-02, avg batch time: 0.5003, average train loss: 1.7037
[09/26 08:43:19 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1696, average loss: 1.7526
[09/26 08:43:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 87.00	
[09/26 08:43:19 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 08:43:26 visual_prompt]: Epoch 93 / 100: avg data time: 4.45e-02, avg batch time: 0.4949, average train loss: 1.6273
[09/26 08:43:27 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1694, average loss: 1.6888
[09/26 08:43:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 90.00	
[09/26 08:43:27 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 08:43:34 visual_prompt]: Epoch 94 / 100: avg data time: 5.65e-02, avg batch time: 0.5064, average train loss: 1.5984
[09/26 08:43:36 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1692, average loss: 1.6421
[09/26 08:43:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 90.50	
[09/26 08:43:36 visual_prompt]: Best epoch 94: best metric: 0.270
[09/26 08:43:36 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 08:43:42 visual_prompt]: Epoch 95 / 100: avg data time: 4.55e-02, avg batch time: 0.4979, average train loss: 1.5362
[09/26 08:43:44 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1695, average loss: 1.6443
[09/26 08:43:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 91.50	
[09/26 08:43:44 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 08:43:51 visual_prompt]: Epoch 96 / 100: avg data time: 5.51e-02, avg batch time: 0.5044, average train loss: 1.5163
[09/26 08:43:52 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1697, average loss: 1.6087
[09/26 08:43:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 91.00	
[09/26 08:43:52 visual_prompt]: Best epoch 96: best metric: 0.275
[09/26 08:43:52 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 08:43:59 visual_prompt]: Epoch 97 / 100: avg data time: 4.25e-02, avg batch time: 0.4913, average train loss: 1.4914
[09/26 08:44:00 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 1.6114
[09/26 08:44:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 94.50	
[09/26 08:44:00 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 08:44:07 visual_prompt]: Epoch 98 / 100: avg data time: 5.37e-02, avg batch time: 0.5031, average train loss: 1.4816
[09/26 08:44:08 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1697, average loss: 1.5834
[09/26 08:44:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 94.00	
[09/26 08:44:08 visual_prompt]: Best epoch 98: best metric: 0.290
[09/26 08:44:08 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 08:44:15 visual_prompt]: Epoch 99 / 100: avg data time: 5.59e-02, avg batch time: 0.5063, average train loss: 1.4571
[09/26 08:44:17 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 1.6010
[09/26 08:44:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 94.00	
[09/26 08:44:17 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 08:44:23 visual_prompt]: Epoch 100 / 100: avg data time: 4.36e-02, avg batch time: 0.4933, average train loss: 1.4571
[09/26 08:44:25 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 1.5863
[09/26 08:44:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 94.00	
[09/26 08:44:25 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 08:44:25 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 08:44:25 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 08:44:25 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 08:44:25 visual_prompt]: Training with config:
[09/26 08:44:25 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 08:44:25 visual_prompt]: Loading training data...
[09/26 08:44:25 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 08:44:26 visual_prompt]: Number of images: 800
[09/26 08:44:26 visual_prompt]: Number of classes: 6 / 6
[09/26 08:44:26 visual_prompt]: Loading validation data...
[09/26 08:44:26 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 08:44:27 visual_prompt]: Number of images: 200
[09/26 08:44:27 visual_prompt]: Number of classes: 6 / 6
[09/26 08:44:27 visual_prompt]: Constructing models...
[09/26 08:44:30 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 08:44:30 visual_prompt]: tuned percent:0.540
[09/26 08:44:30 visual_prompt]: Device used for model: 0
[09/26 08:44:30 visual_prompt]: Setting up Evaluator...
[09/26 08:44:30 visual_prompt]: Setting up Trainer...
[09/26 08:44:30 visual_prompt]: 	Setting up the optimizer...
[09/26 08:44:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 08:44:37 visual_prompt]: Epoch 1 / 100: avg data time: 5.73e-02, avg batch time: 0.5066, average train loss: 2.9695
[09/26 08:44:38 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1686, average loss: 2.9268
[09/26 08:44:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:44:38 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 08:44:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 08:44:45 visual_prompt]: Epoch 2 / 100: avg data time: 4.20e-02, avg batch time: 0.4930, average train loss: 39.9411
[09/26 08:44:46 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1684, average loss: 43.2381
[09/26 08:44:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 08:44:46 visual_prompt]: Best epoch 2: best metric: 0.205
[09/26 08:44:46 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 08:44:53 visual_prompt]: Epoch 3 / 100: avg data time: 4.96e-02, avg batch time: 0.4970, average train loss: 31.0351
[09/26 08:44:54 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1692, average loss: 42.7995
[09/26 08:44:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:44:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 08:45:01 visual_prompt]: Epoch 4 / 100: avg data time: 5.73e-02, avg batch time: 0.5060, average train loss: 31.3570
[09/26 08:45:03 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1685, average loss: 42.4822
[09/26 08:45:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:45:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 08:45:10 visual_prompt]: Epoch 5 / 100: avg data time: 6.08e-02, avg batch time: 0.5087, average train loss: 45.9952
[09/26 08:45:11 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1688, average loss: 56.5885
[09/26 08:45:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:45:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 08:45:18 visual_prompt]: Epoch 6 / 100: avg data time: 5.77e-02, avg batch time: 0.5077, average train loss: 49.3282
[09/26 08:45:19 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1688, average loss: 61.3055
[09/26 08:45:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:45:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 08:45:26 visual_prompt]: Epoch 7 / 100: avg data time: 4.79e-02, avg batch time: 0.4966, average train loss: 60.7443
[09/26 08:45:28 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1693, average loss: 63.0623
[09/26 08:45:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 08:45:28 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 08:45:34 visual_prompt]: Epoch 8 / 100: avg data time: 4.53e-02, avg batch time: 0.4957, average train loss: 50.4830
[09/26 08:45:36 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1689, average loss: 49.9271
[09/26 08:45:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:45:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 08:45:43 visual_prompt]: Epoch 9 / 100: avg data time: 5.79e-02, avg batch time: 0.5059, average train loss: 49.0354
[09/26 08:45:44 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1691, average loss: 65.6088
[09/26 08:45:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:45:44 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 08:45:51 visual_prompt]: Epoch 10 / 100: avg data time: 5.42e-02, avg batch time: 0.5028, average train loss: 57.4508
[09/26 08:45:52 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1691, average loss: 68.0242
[09/26 08:45:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 08:45:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 08:45:59 visual_prompt]: Epoch 11 / 100: avg data time: 4.83e-02, avg batch time: 0.4980, average train loss: 44.6793
[09/26 08:46:00 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1689, average loss: 41.0715
[09/26 08:46:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:46:00 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 08:46:07 visual_prompt]: Epoch 12 / 100: avg data time: 4.40e-02, avg batch time: 0.4948, average train loss: 59.0423
[09/26 08:46:09 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 119.2710
[09/26 08:46:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:46:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 08:46:15 visual_prompt]: Epoch 13 / 100: avg data time: 4.98e-02, avg batch time: 0.4984, average train loss: 64.0164
[09/26 08:46:17 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1687, average loss: 70.5088
[09/26 08:46:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 08:46:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 08:46:24 visual_prompt]: Epoch 14 / 100: avg data time: 4.86e-02, avg batch time: 0.4996, average train loss: 56.0862
[09/26 08:46:25 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1695, average loss: 80.1728
[09/26 08:46:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 08:46:25 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 08:46:32 visual_prompt]: Epoch 15 / 100: avg data time: 4.23e-02, avg batch time: 0.4958, average train loss: 68.0387
[09/26 08:46:33 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 65.9676
[09/26 08:46:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:46:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 08:46:40 visual_prompt]: Epoch 16 / 100: avg data time: 4.84e-02, avg batch time: 0.4991, average train loss: 53.9199
[09/26 08:46:41 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 54.1804
[09/26 08:46:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:46:41 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 08:46:48 visual_prompt]: Epoch 17 / 100: avg data time: 5.04e-02, avg batch time: 0.5002, average train loss: 59.6899
[09/26 08:46:50 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1691, average loss: 69.0194
[09/26 08:46:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 08:46:50 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 08:46:56 visual_prompt]: Epoch 18 / 100: avg data time: 4.22e-02, avg batch time: 0.4927, average train loss: 53.8383
[09/26 08:46:58 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1693, average loss: 86.3045
[09/26 08:46:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:46:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 08:47:05 visual_prompt]: Epoch 19 / 100: avg data time: 4.91e-02, avg batch time: 0.4980, average train loss: 69.3008
[09/26 08:47:06 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1691, average loss: 32.7466
[09/26 08:47:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 08:47:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 08:47:13 visual_prompt]: Epoch 20 / 100: avg data time: 4.24e-02, avg batch time: 0.4927, average train loss: 67.2087
[09/26 08:47:14 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1692, average loss: 41.7476
[09/26 08:47:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:47:14 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 08:47:21 visual_prompt]: Epoch 21 / 100: avg data time: 5.61e-02, avg batch time: 0.5049, average train loss: 59.4788
[09/26 08:47:23 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 51.2075
[09/26 08:47:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:47:23 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 08:47:29 visual_prompt]: Epoch 22 / 100: avg data time: 4.28e-02, avg batch time: 0.4937, average train loss: 49.6311
[09/26 08:47:31 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 56.8747
[09/26 08:47:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:47:31 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 08:47:38 visual_prompt]: Epoch 23 / 100: avg data time: 4.80e-02, avg batch time: 0.4973, average train loss: 44.3851
[09/26 08:47:39 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1694, average loss: 51.7023
[09/26 08:47:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 08:47:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 08:47:46 visual_prompt]: Epoch 24 / 100: avg data time: 5.63e-02, avg batch time: 0.5068, average train loss: 47.7570
[09/26 08:47:47 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 64.9162
[09/26 08:47:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 08:47:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 08:47:54 visual_prompt]: Epoch 25 / 100: avg data time: 5.33e-02, avg batch time: 0.5029, average train loss: 44.6268
[09/26 08:47:56 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1696, average loss: 45.6773
[09/26 08:47:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 08:47:56 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 08:48:02 visual_prompt]: Epoch 26 / 100: avg data time: 5.17e-02, avg batch time: 0.5005, average train loss: 47.7996
[09/26 08:48:04 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1697, average loss: 53.2429
[09/26 08:48:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 08:48:04 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 08:48:11 visual_prompt]: Epoch 27 / 100: avg data time: 5.94e-02, avg batch time: 0.5082, average train loss: 19.5694
[09/26 08:48:12 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1696, average loss: 26.7525
[09/26 08:48:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 08:48:12 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 08:48:19 visual_prompt]: Epoch 28 / 100: avg data time: 4.11e-02, avg batch time: 0.4907, average train loss: 22.1949
[09/26 08:48:20 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1692, average loss: 15.7342
[09/26 08:48:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 84.50	
[09/26 08:48:20 visual_prompt]: Best epoch 28: best metric: 0.225
[09/26 08:48:20 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 08:48:27 visual_prompt]: Epoch 29 / 100: avg data time: 5.73e-02, avg batch time: 0.5068, average train loss: 20.3336
[09/26 08:48:29 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1692, average loss: 35.0475
[09/26 08:48:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 08:48:29 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 08:48:35 visual_prompt]: Epoch 30 / 100: avg data time: 5.14e-02, avg batch time: 0.5010, average train loss: 28.2920
[09/26 08:48:37 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1690, average loss: 17.4526
[09/26 08:48:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:48:37 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 08:48:44 visual_prompt]: Epoch 31 / 100: avg data time: 4.76e-02, avg batch time: 0.4972, average train loss: 15.7413
[09/26 08:48:45 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1693, average loss: 12.0124
[09/26 08:48:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 84.00	
[09/26 08:48:45 visual_prompt]: Best epoch 31: best metric: 0.240
[09/26 08:48:45 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 08:48:52 visual_prompt]: Epoch 32 / 100: avg data time: 4.57e-02, avg batch time: 0.4962, average train loss: 14.3147
[09/26 08:48:53 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1691, average loss: 21.6874
[09/26 08:48:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 84.00	
[09/26 08:48:53 visual_prompt]: Best epoch 32: best metric: 0.250
[09/26 08:48:53 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 08:49:00 visual_prompt]: Epoch 33 / 100: avg data time: 5.11e-02, avg batch time: 0.5003, average train loss: 28.5025
[09/26 08:49:02 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 29.1307
[09/26 08:49:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 84.00	
[09/26 08:49:02 visual_prompt]: Best epoch 33: best metric: 0.280
[09/26 08:49:02 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 08:49:08 visual_prompt]: Epoch 34 / 100: avg data time: 4.30e-02, avg batch time: 0.4933, average train loss: 18.1660
[09/26 08:49:10 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 19.8709
[09/26 08:49:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 84.00	
[09/26 08:49:10 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 08:49:16 visual_prompt]: Epoch 35 / 100: avg data time: 4.16e-02, avg batch time: 0.4932, average train loss: 14.3866
[09/26 08:49:18 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1697, average loss: 15.7441
[09/26 08:49:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 84.00	
[09/26 08:49:18 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 08:49:25 visual_prompt]: Epoch 36 / 100: avg data time: 5.11e-02, avg batch time: 0.5013, average train loss: 12.8306
[09/26 08:49:26 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 13.7262
[09/26 08:49:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 89.50	
[09/26 08:49:26 visual_prompt]: Best epoch 36: best metric: 0.305
[09/26 08:49:26 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 08:49:33 visual_prompt]: Epoch 37 / 100: avg data time: 4.66e-02, avg batch time: 0.4958, average train loss: 6.8900
[09/26 08:49:35 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 6.1338
[09/26 08:49:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 96.00	
[09/26 08:49:35 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 08:49:41 visual_prompt]: Epoch 38 / 100: avg data time: 5.14e-02, avg batch time: 0.5024, average train loss: 8.2510
[09/26 08:49:43 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1696, average loss: 11.5787
[09/26 08:49:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 94.00	
[09/26 08:49:43 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 08:49:50 visual_prompt]: Epoch 39 / 100: avg data time: 5.87e-02, avg batch time: 0.5080, average train loss: 10.6861
[09/26 08:49:51 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 14.1031
[09/26 08:49:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 92.50	
[09/26 08:49:51 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 08:49:58 visual_prompt]: Epoch 40 / 100: avg data time: 4.30e-02, avg batch time: 0.4925, average train loss: 12.0685
[09/26 08:49:59 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 11.6297
[09/26 08:49:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 94.50	
[09/26 08:49:59 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 08:50:06 visual_prompt]: Epoch 41 / 100: avg data time: 5.46e-02, avg batch time: 0.5037, average train loss: 7.2649
[09/26 08:50:08 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1695, average loss: 8.3992
[09/26 08:50:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.00	
[09/26 08:50:08 visual_prompt]: Best epoch 41: best metric: 0.360
[09/26 08:50:08 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 08:50:15 visual_prompt]: Epoch 42 / 100: avg data time: 5.59e-02, avg batch time: 0.5056, average train loss: 6.0170
[09/26 08:50:16 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1694, average loss: 7.8683
[09/26 08:50:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 93.50	
[09/26 08:50:16 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 08:50:23 visual_prompt]: Epoch 43 / 100: avg data time: 5.69e-02, avg batch time: 0.5056, average train loss: 5.9938
[09/26 08:50:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1692, average loss: 6.3446
[09/26 08:50:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 97.00	
[09/26 08:50:24 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 08:50:31 visual_prompt]: Epoch 44 / 100: avg data time: 5.92e-02, avg batch time: 0.5097, average train loss: 6.6130
[09/26 08:50:33 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 6.3531
[09/26 08:50:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 95.50	
[09/26 08:50:33 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 08:50:40 visual_prompt]: Epoch 45 / 100: avg data time: 5.76e-02, avg batch time: 0.5070, average train loss: 4.4058
[09/26 08:50:41 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1692, average loss: 6.1098
[09/26 08:50:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 97.50	
[09/26 08:50:41 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 08:50:48 visual_prompt]: Epoch 46 / 100: avg data time: 5.92e-02, avg batch time: 0.5088, average train loss: 4.7768
[09/26 08:50:49 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1694, average loss: 5.4503
[09/26 08:50:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 96.50	
[09/26 08:50:49 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 08:50:56 visual_prompt]: Epoch 47 / 100: avg data time: 4.17e-02, avg batch time: 0.4917, average train loss: 4.2435
[09/26 08:50:57 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1694, average loss: 7.2232
[09/26 08:50:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 97.00	
[09/26 08:50:57 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 08:51:04 visual_prompt]: Epoch 48 / 100: avg data time: 5.85e-02, avg batch time: 0.5081, average train loss: 3.0502
[09/26 08:51:06 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1694, average loss: 2.2215
[09/26 08:51:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 08:51:06 visual_prompt]: Best epoch 48: best metric: 0.410
[09/26 08:51:06 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 08:51:13 visual_prompt]: Epoch 49 / 100: avg data time: 5.57e-02, avg batch time: 0.5044, average train loss: 1.8923
[09/26 08:51:14 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1690, average loss: 2.8807
[09/26 08:51:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.50	
[09/26 08:51:14 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 08:51:21 visual_prompt]: Epoch 50 / 100: avg data time: 5.48e-02, avg batch time: 0.5047, average train loss: 2.1698
[09/26 08:51:23 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1695, average loss: 2.7495
[09/26 08:51:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 08:51:23 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 08:51:29 visual_prompt]: Epoch 51 / 100: avg data time: 5.24e-02, avg batch time: 0.5031, average train loss: 3.5250
[09/26 08:51:31 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1689, average loss: 4.2256
[09/26 08:51:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 97.00	
[09/26 08:51:31 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 08:51:38 visual_prompt]: Epoch 52 / 100: avg data time: 4.47e-02, avg batch time: 0.4945, average train loss: 4.1016
[09/26 08:51:39 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1691, average loss: 4.0134
[09/26 08:51:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 08:51:39 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 08:51:46 visual_prompt]: Epoch 53 / 100: avg data time: 4.50e-02, avg batch time: 0.4956, average train loss: 3.4584
[09/26 08:51:47 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1694, average loss: 5.0684
[09/26 08:51:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 90.50	
[09/26 08:51:47 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 08:51:54 visual_prompt]: Epoch 54 / 100: avg data time: 4.62e-02, avg batch time: 0.4966, average train loss: 3.0231
[09/26 08:51:56 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1692, average loss: 4.1137
[09/26 08:51:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.50	
[09/26 08:51:56 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 08:52:02 visual_prompt]: Epoch 55 / 100: avg data time: 4.59e-02, avg batch time: 0.4968, average train loss: 2.1412
[09/26 08:52:04 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1694, average loss: 2.8981
[09/26 08:52:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 08:52:04 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 08:52:10 visual_prompt]: Epoch 56 / 100: avg data time: 4.85e-02, avg batch time: 0.4978, average train loss: 2.2746
[09/26 08:52:12 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1692, average loss: 3.5508
[09/26 08:52:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.50	
[09/26 08:52:12 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 08:52:19 visual_prompt]: Epoch 57 / 100: avg data time: 4.81e-02, avg batch time: 0.4988, average train loss: 3.0084
[09/26 08:52:20 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1693, average loss: 2.4750
[09/26 08:52:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 08:52:20 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 08:52:27 visual_prompt]: Epoch 58 / 100: avg data time: 4.74e-02, avg batch time: 0.4979, average train loss: 1.8688
[09/26 08:52:28 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 3.2112
[09/26 08:52:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 08:52:28 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 08:52:35 visual_prompt]: Epoch 59 / 100: avg data time: 4.14e-02, avg batch time: 0.4929, average train loss: 2.2067
[09/26 08:52:36 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1697, average loss: 3.9130
[09/26 08:52:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 93.50	
[09/26 08:52:36 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 08:52:43 visual_prompt]: Epoch 60 / 100: avg data time: 5.63e-02, avg batch time: 0.5062, average train loss: 2.1048
[09/26 08:52:45 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1692, average loss: 3.8124
[09/26 08:52:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 08:52:45 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 08:52:52 visual_prompt]: Epoch 61 / 100: avg data time: 6.05e-02, avg batch time: 0.5114, average train loss: 1.6547
[09/26 08:52:53 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1694, average loss: 3.2805
[09/26 08:52:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 95.50	
[09/26 08:52:53 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 08:53:00 visual_prompt]: Epoch 62 / 100: avg data time: 5.14e-02, avg batch time: 0.5011, average train loss: 1.6430
[09/26 08:53:01 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 2.7246
[09/26 08:53:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 08:53:01 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 08:53:08 visual_prompt]: Epoch 63 / 100: avg data time: 5.38e-02, avg batch time: 0.5068, average train loss: 1.5620
[09/26 08:53:10 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1693, average loss: 2.4052
[09/26 08:53:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.50	
[09/26 08:53:10 visual_prompt]: Best epoch 63: best metric: 0.415
[09/26 08:53:10 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 08:53:17 visual_prompt]: Epoch 64 / 100: avg data time: 4.42e-02, avg batch time: 0.4941, average train loss: 1.4788
[09/26 08:53:18 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1696, average loss: 2.3185
[09/26 08:53:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 98.00	
[09/26 08:53:18 visual_prompt]: Best epoch 64: best metric: 0.440
[09/26 08:53:18 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 08:53:25 visual_prompt]: Epoch 65 / 100: avg data time: 4.60e-02, avg batch time: 0.4980, average train loss: 1.3064
[09/26 08:53:26 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 2.2183
[09/26 08:53:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 08:53:26 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 08:53:33 visual_prompt]: Epoch 66 / 100: avg data time: 5.64e-02, avg batch time: 0.5054, average train loss: 1.2796
[09/26 08:53:35 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1694, average loss: 2.6418
[09/26 08:53:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 08:53:35 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 08:53:41 visual_prompt]: Epoch 67 / 100: avg data time: 4.28e-02, avg batch time: 0.4946, average train loss: 1.2162
[09/26 08:53:43 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 2.6675
[09/26 08:53:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 08:53:43 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 08:53:50 visual_prompt]: Epoch 68 / 100: avg data time: 4.91e-02, avg batch time: 0.4992, average train loss: 0.9914
[09/26 08:53:51 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 2.2402
[09/26 08:53:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 98.00	
[09/26 08:53:51 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 08:53:58 visual_prompt]: Epoch 69 / 100: avg data time: 4.24e-02, avg batch time: 0.4940, average train loss: 0.9847
[09/26 08:53:59 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1697, average loss: 2.0746
[09/26 08:53:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 98.00	
[09/26 08:53:59 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 08:54:06 visual_prompt]: Epoch 70 / 100: avg data time: 5.03e-02, avg batch time: 0.4999, average train loss: 0.9856
[09/26 08:54:08 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1694, average loss: 2.5544
[09/26 08:54:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 08:54:08 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 08:54:14 visual_prompt]: Epoch 71 / 100: avg data time: 5.30e-02, avg batch time: 0.5028, average train loss: 1.1762
[09/26 08:54:16 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1696, average loss: 2.3366
[09/26 08:54:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 98.00	
[09/26 08:54:16 visual_prompt]: Best epoch 71: best metric: 0.460
[09/26 08:54:16 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 08:54:23 visual_prompt]: Epoch 72 / 100: avg data time: 4.37e-02, avg batch time: 0.4940, average train loss: 1.0439
[09/26 08:54:24 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 2.6935
[09/26 08:54:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 08:54:24 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 08:54:31 visual_prompt]: Epoch 73 / 100: avg data time: 5.70e-02, avg batch time: 0.5065, average train loss: 1.0048
[09/26 08:54:32 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1692, average loss: 2.5501
[09/26 08:54:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 08:54:32 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 08:54:39 visual_prompt]: Epoch 74 / 100: avg data time: 5.81e-02, avg batch time: 0.5078, average train loss: 0.9899
[09/26 08:54:41 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1691, average loss: 2.1038
[09/26 08:54:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.00	
[09/26 08:54:41 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 08:54:47 visual_prompt]: Epoch 75 / 100: avg data time: 4.39e-02, avg batch time: 0.4942, average train loss: 0.7799
[09/26 08:54:49 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1696, average loss: 2.2926
[09/26 08:54:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 97.50	
[09/26 08:54:49 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 08:54:56 visual_prompt]: Epoch 76 / 100: avg data time: 5.42e-02, avg batch time: 0.5034, average train loss: 0.7706
[09/26 08:54:57 visual_prompt]: Inference (val):avg data time: 1.89e-05, avg batch time: 0.1696, average loss: 2.0854
[09/26 08:54:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 98.00	
[09/26 08:54:57 visual_prompt]: Best epoch 76: best metric: 0.465
[09/26 08:54:57 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 08:55:04 visual_prompt]: Epoch 77 / 100: avg data time: 4.63e-02, avg batch time: 0.4961, average train loss: 0.7056
[09/26 08:55:05 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1692, average loss: 2.2376
[09/26 08:55:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 98.00	
[09/26 08:55:05 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 08:55:12 visual_prompt]: Epoch 78 / 100: avg data time: 5.46e-02, avg batch time: 0.5061, average train loss: 0.8043
[09/26 08:55:14 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1696, average loss: 2.1212
[09/26 08:55:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.50	
[09/26 08:55:14 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 08:55:21 visual_prompt]: Epoch 79 / 100: avg data time: 5.51e-02, avg batch time: 0.5051, average train loss: 0.7886
[09/26 08:55:22 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 2.2650
[09/26 08:55:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 08:55:22 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 08:55:29 visual_prompt]: Epoch 80 / 100: avg data time: 4.57e-02, avg batch time: 0.4956, average train loss: 0.7208
[09/26 08:55:30 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1691, average loss: 2.1417
[09/26 08:55:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.00	top5: 98.00	
[09/26 08:55:30 visual_prompt]: Best epoch 80: best metric: 0.470
[09/26 08:55:30 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 08:55:37 visual_prompt]: Epoch 81 / 100: avg data time: 4.59e-02, avg batch time: 0.4967, average train loss: 0.6879
[09/26 08:55:38 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1697, average loss: 2.1127
[09/26 08:55:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.00	top5: 97.00	
[09/26 08:55:38 visual_prompt]: Best epoch 81: best metric: 0.480
[09/26 08:55:38 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 08:55:45 visual_prompt]: Epoch 82 / 100: avg data time: 5.91e-02, avg batch time: 0.5079, average train loss: 0.6959
[09/26 08:55:47 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1693, average loss: 2.2652
[09/26 08:55:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 08:55:47 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 08:55:54 visual_prompt]: Epoch 83 / 100: avg data time: 5.08e-02, avg batch time: 0.5011, average train loss: 0.6705
[09/26 08:55:55 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 2.2695
[09/26 08:55:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 97.50	
[09/26 08:55:55 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 08:56:02 visual_prompt]: Epoch 84 / 100: avg data time: 5.60e-02, avg batch time: 0.5051, average train loss: 0.6869
[09/26 08:56:03 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 2.1201
[09/26 08:56:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.50	top5: 98.00	
[09/26 08:56:03 visual_prompt]: Best epoch 84: best metric: 0.485
[09/26 08:56:03 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 08:56:10 visual_prompt]: Epoch 85 / 100: avg data time: 4.43e-02, avg batch time: 0.4932, average train loss: 0.6558
[09/26 08:56:12 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1698, average loss: 2.1795
[09/26 08:56:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 08:56:12 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 08:56:18 visual_prompt]: Epoch 86 / 100: avg data time: 4.41e-02, avg batch time: 0.4940, average train loss: 0.6585
[09/26 08:56:20 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1697, average loss: 2.1626
[09/26 08:56:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 97.50	
[09/26 08:56:20 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 08:56:27 visual_prompt]: Epoch 87 / 100: avg data time: 5.84e-02, avg batch time: 0.5075, average train loss: 0.6605
[09/26 08:56:28 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 2.1154
[09/26 08:56:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 98.00	
[09/26 08:56:28 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 08:56:35 visual_prompt]: Epoch 88 / 100: avg data time: 5.37e-02, avg batch time: 0.5035, average train loss: 0.6264
[09/26 08:56:36 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 2.1908
[09/26 08:56:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 08:56:36 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 08:56:43 visual_prompt]: Epoch 89 / 100: avg data time: 4.82e-02, avg batch time: 0.4990, average train loss: 0.6091
[09/26 08:56:45 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 2.2111
[09/26 08:56:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.50	
[09/26 08:56:45 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 08:56:52 visual_prompt]: Epoch 90 / 100: avg data time: 5.71e-02, avg batch time: 0.5063, average train loss: 0.6482
[09/26 08:56:53 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1695, average loss: 2.1223
[09/26 08:56:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 97.50	
[09/26 08:56:53 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 08:57:00 visual_prompt]: Epoch 91 / 100: avg data time: 5.55e-02, avg batch time: 0.5045, average train loss: 0.5913
[09/26 08:57:01 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1695, average loss: 2.1340
[09/26 08:57:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 98.00	
[09/26 08:57:01 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 08:57:08 visual_prompt]: Epoch 92 / 100: avg data time: 5.12e-02, avg batch time: 0.5052, average train loss: 0.5991
[09/26 08:57:10 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 2.1961
[09/26 08:57:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.00	
[09/26 08:57:10 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 08:57:16 visual_prompt]: Epoch 93 / 100: avg data time: 5.32e-02, avg batch time: 0.5050, average train loss: 0.6191
[09/26 08:57:18 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 2.1133
[09/26 08:57:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 98.00	
[09/26 08:57:18 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 08:57:25 visual_prompt]: Epoch 94 / 100: avg data time: 4.25e-02, avg batch time: 0.4926, average train loss: 0.6017
[09/26 08:57:26 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 2.1392
[09/26 08:57:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 98.00	
[09/26 08:57:26 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 08:57:33 visual_prompt]: Epoch 95 / 100: avg data time: 5.86e-02, avg batch time: 0.5075, average train loss: 0.6389
[09/26 08:57:34 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1697, average loss: 2.1064
[09/26 08:57:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 98.00	
[09/26 08:57:34 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 08:57:41 visual_prompt]: Epoch 96 / 100: avg data time: 4.69e-02, avg batch time: 0.4967, average train loss: 0.5762
[09/26 08:57:43 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1692, average loss: 2.1170
[09/26 08:57:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 98.00	
[09/26 08:57:43 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 08:57:50 visual_prompt]: Epoch 97 / 100: avg data time: 5.54e-02, avg batch time: 0.5052, average train loss: 0.5974
[09/26 08:57:51 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1692, average loss: 2.1234
[09/26 08:57:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 98.00	
[09/26 08:57:51 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 08:57:58 visual_prompt]: Epoch 98 / 100: avg data time: 5.60e-02, avg batch time: 0.5053, average train loss: 0.5914
[09/26 08:57:59 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1695, average loss: 2.1221
[09/26 08:57:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 98.00	
[09/26 08:57:59 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 08:58:06 visual_prompt]: Epoch 99 / 100: avg data time: 5.57e-02, avg batch time: 0.5056, average train loss: 0.5874
[09/26 08:58:08 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1698, average loss: 2.1183
[09/26 08:58:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 98.00	
[09/26 08:58:08 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 08:58:14 visual_prompt]: Epoch 100 / 100: avg data time: 4.23e-02, avg batch time: 0.4939, average train loss: 0.6024
[09/26 08:58:16 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1696, average loss: 2.1177
[09/26 08:58:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 98.00	
[09/26 08:58:16 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 08:58:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 08:58:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 08:58:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 08:58:16 visual_prompt]: Training with config:
[09/26 08:58:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 08:58:16 visual_prompt]: Loading training data...
[09/26 08:58:16 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 08:58:17 visual_prompt]: Number of images: 800
[09/26 08:58:17 visual_prompt]: Number of classes: 6 / 6
[09/26 08:58:17 visual_prompt]: Loading validation data...
[09/26 08:58:17 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 08:58:18 visual_prompt]: Number of images: 200
[09/26 08:58:18 visual_prompt]: Number of classes: 6 / 6
[09/26 08:58:18 visual_prompt]: Constructing models...
[09/26 08:58:20 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 08:58:20 visual_prompt]: tuned percent:0.540
[09/26 08:58:20 visual_prompt]: Device used for model: 0
[09/26 08:58:20 visual_prompt]: Setting up Evaluator...
[09/26 08:58:20 visual_prompt]: Setting up Trainer...
[09/26 08:58:20 visual_prompt]: 	Setting up the optimizer...
[09/26 08:58:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 08:58:27 visual_prompt]: Epoch 1 / 100: avg data time: 5.43e-02, avg batch time: 0.5025, average train loss: 2.9759
[09/26 08:58:28 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1684, average loss: 2.9268
[09/26 08:58:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 08:58:28 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 08:58:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 08:58:35 visual_prompt]: Epoch 2 / 100: avg data time: 4.95e-02, avg batch time: 0.4973, average train loss: 10.5539
[09/26 08:58:37 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1685, average loss: 4.9587
[09/26 08:58:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:58:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 08:58:43 visual_prompt]: Epoch 3 / 100: avg data time: 4.47e-02, avg batch time: 0.4936, average train loss: 7.2070
[09/26 08:58:45 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1689, average loss: 4.1512
[09/26 08:58:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 08:58:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 08:58:52 visual_prompt]: Epoch 4 / 100: avg data time: 5.87e-02, avg batch time: 0.5071, average train loss: 7.6072
[09/26 08:58:53 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1683, average loss: 9.6205
[09/26 08:58:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 08:58:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 08:59:00 visual_prompt]: Epoch 5 / 100: avg data time: 5.43e-02, avg batch time: 0.5015, average train loss: 13.6308
[09/26 08:59:02 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1688, average loss: 8.9122
[09/26 08:59:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 08:59:02 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 08:59:09 visual_prompt]: Epoch 6 / 100: avg data time: 5.87e-02, avg batch time: 0.5066, average train loss: 16.9618
[09/26 08:59:10 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1688, average loss: 16.1120
[09/26 08:59:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 08:59:10 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 08:59:17 visual_prompt]: Epoch 7 / 100: avg data time: 5.40e-02, avg batch time: 0.5018, average train loss: 19.9025
[09/26 08:59:18 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1693, average loss: 25.3672
[09/26 08:59:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 08:59:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 08:59:25 visual_prompt]: Epoch 8 / 100: avg data time: 5.83e-02, avg batch time: 0.5067, average train loss: 23.7339
[09/26 08:59:27 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1694, average loss: 23.3896
[09/26 08:59:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:59:27 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 08:59:33 visual_prompt]: Epoch 9 / 100: avg data time: 4.31e-02, avg batch time: 0.4920, average train loss: 22.1460
[09/26 08:59:35 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1689, average loss: 40.4368
[09/26 08:59:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 08:59:35 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 08:59:42 visual_prompt]: Epoch 10 / 100: avg data time: 5.17e-02, avg batch time: 0.5005, average train loss: 36.8071
[09/26 08:59:43 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1690, average loss: 36.0865
[09/26 08:59:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 08:59:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 08:59:50 visual_prompt]: Epoch 11 / 100: avg data time: 4.23e-02, avg batch time: 0.4917, average train loss: 31.7104
[09/26 08:59:51 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1690, average loss: 31.1491
[09/26 08:59:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 08:59:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 08:59:58 visual_prompt]: Epoch 12 / 100: avg data time: 5.44e-02, avg batch time: 0.5024, average train loss: 32.0703
[09/26 09:00:00 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 61.0928
[09/26 09:00:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 84.50	
[09/26 09:00:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 09:00:06 visual_prompt]: Epoch 13 / 100: avg data time: 4.65e-02, avg batch time: 0.4965, average train loss: 38.0847
[09/26 09:00:08 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1692, average loss: 28.3066
[09/26 09:00:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 84.50	
[09/26 09:00:08 visual_prompt]: Best epoch 13: best metric: 0.210
[09/26 09:00:08 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 09:00:14 visual_prompt]: Epoch 14 / 100: avg data time: 4.33e-02, avg batch time: 0.4924, average train loss: 42.0062
[09/26 09:00:16 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1690, average loss: 59.5847
[09/26 09:00:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:00:16 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 09:00:22 visual_prompt]: Epoch 15 / 100: avg data time: 4.10e-02, avg batch time: 0.4900, average train loss: 43.9677
[09/26 09:00:24 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1691, average loss: 55.8027
[09/26 09:00:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 09:00:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 09:00:31 visual_prompt]: Epoch 16 / 100: avg data time: 4.10e-02, avg batch time: 0.4917, average train loss: 40.6498
[09/26 09:00:32 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.1688, average loss: 53.5640
[09/26 09:00:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:00:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 09:00:39 visual_prompt]: Epoch 17 / 100: avg data time: 4.02e-02, avg batch time: 0.4900, average train loss: 57.1784
[09/26 09:00:40 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 35.7977
[09/26 09:00:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 80.50	
[09/26 09:00:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 09:00:47 visual_prompt]: Epoch 18 / 100: avg data time: 4.12e-02, avg batch time: 0.4949, average train loss: 46.7122
[09/26 09:00:48 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1690, average loss: 31.4437
[09/26 09:00:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:00:48 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 09:00:55 visual_prompt]: Epoch 19 / 100: avg data time: 4.34e-02, avg batch time: 0.4950, average train loss: 42.7427
[09/26 09:00:57 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 20.6163
[09/26 09:00:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:00:57 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 09:01:03 visual_prompt]: Epoch 20 / 100: avg data time: 4.85e-02, avg batch time: 0.4981, average train loss: 33.3401
[09/26 09:01:05 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1698, average loss: 34.7618
[09/26 09:01:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 09:01:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 09:01:11 visual_prompt]: Epoch 21 / 100: avg data time: 4.56e-02, avg batch time: 0.4947, average train loss: 33.5875
[09/26 09:01:13 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1694, average loss: 47.8430
[09/26 09:01:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:01:13 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 09:01:20 visual_prompt]: Epoch 22 / 100: avg data time: 4.72e-02, avg batch time: 0.4962, average train loss: 28.6930
[09/26 09:01:21 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1688, average loss: 52.5899
[09/26 09:01:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:01:21 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 09:01:28 visual_prompt]: Epoch 23 / 100: avg data time: 5.90e-02, avg batch time: 0.5087, average train loss: 35.9374
[09/26 09:01:30 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1695, average loss: 22.6025
[09/26 09:01:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:01:30 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 09:01:36 visual_prompt]: Epoch 24 / 100: avg data time: 4.24e-02, avg batch time: 0.4923, average train loss: 30.2978
[09/26 09:01:38 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 15.6728
[09/26 09:01:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:01:38 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 09:01:44 visual_prompt]: Epoch 25 / 100: avg data time: 4.19e-02, avg batch time: 0.4903, average train loss: 29.4699
[09/26 09:01:46 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1690, average loss: 30.6050
[09/26 09:01:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:01:46 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 09:01:52 visual_prompt]: Epoch 26 / 100: avg data time: 4.30e-02, avg batch time: 0.4945, average train loss: 41.6832
[09/26 09:01:54 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1693, average loss: 32.7772
[09/26 09:01:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:01:54 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 09:02:01 visual_prompt]: Epoch 27 / 100: avg data time: 4.35e-02, avg batch time: 0.4926, average train loss: 49.7943
[09/26 09:02:02 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 69.6298
[09/26 09:02:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 09:02:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 09:02:09 visual_prompt]: Epoch 28 / 100: avg data time: 4.89e-02, avg batch time: 0.4984, average train loss: 41.7229
[09/26 09:02:10 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1692, average loss: 28.2463
[09/26 09:02:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:02:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 09:02:17 visual_prompt]: Epoch 29 / 100: avg data time: 4.66e-02, avg batch time: 0.4978, average train loss: 42.3312
[09/26 09:02:18 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1694, average loss: 64.7008
[09/26 09:02:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:02:18 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 09:02:25 visual_prompt]: Epoch 30 / 100: avg data time: 4.08e-02, avg batch time: 0.4922, average train loss: 37.5980
[09/26 09:02:27 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 47.1423
[09/26 09:02:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:02:27 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 09:02:33 visual_prompt]: Epoch 31 / 100: avg data time: 4.46e-02, avg batch time: 0.4931, average train loss: 29.8836
[09/26 09:02:35 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1689, average loss: 32.6935
[09/26 09:02:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 09:02:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 09:02:41 visual_prompt]: Epoch 32 / 100: avg data time: 4.63e-02, avg batch time: 0.4968, average train loss: 47.8820
[09/26 09:02:43 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 32.8298
[09/26 09:02:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 09:02:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 09:02:50 visual_prompt]: Epoch 33 / 100: avg data time: 5.08e-02, avg batch time: 0.5008, average train loss: 32.9377
[09/26 09:02:51 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1690, average loss: 15.3935
[09/26 09:02:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 09:02:51 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 09:02:58 visual_prompt]: Epoch 34 / 100: avg data time: 4.51e-02, avg batch time: 0.4946, average train loss: 27.9517
[09/26 09:02:59 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1692, average loss: 26.5087
[09/26 09:02:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:02:59 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 09:03:06 visual_prompt]: Epoch 35 / 100: avg data time: 4.63e-02, avg batch time: 0.4958, average train loss: 39.6470
[09/26 09:03:07 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 30.4624
[09/26 09:03:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 09:03:07 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 09:03:14 visual_prompt]: Epoch 36 / 100: avg data time: 5.65e-02, avg batch time: 0.5052, average train loss: 32.7928
[09/26 09:03:16 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 26.5805
[09/26 09:03:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:03:16 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 09:03:23 visual_prompt]: Epoch 37 / 100: avg data time: 6.25e-02, avg batch time: 0.5112, average train loss: 36.3865
[09/26 09:03:24 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1689, average loss: 37.8226
[09/26 09:03:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:03:24 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 09:03:31 visual_prompt]: Epoch 38 / 100: avg data time: 5.66e-02, avg batch time: 0.5050, average train loss: 30.0256
[09/26 09:03:32 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1691, average loss: 17.3160
[09/26 09:03:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:03:32 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 09:03:39 visual_prompt]: Epoch 39 / 100: avg data time: 4.50e-02, avg batch time: 0.4944, average train loss: 31.7043
[09/26 09:03:41 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1693, average loss: 24.3168
[09/26 09:03:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 09:03:41 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 09:03:48 visual_prompt]: Epoch 40 / 100: avg data time: 5.30e-02, avg batch time: 0.5018, average train loss: 42.2152
[09/26 09:03:49 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1696, average loss: 12.9146
[09/26 09:03:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.00	top5: 84.00	
[09/26 09:03:49 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 09:03:56 visual_prompt]: Epoch 41 / 100: avg data time: 5.64e-02, avg batch time: 0.5050, average train loss: 25.9217
[09/26 09:03:57 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1692, average loss: 24.6506
[09/26 09:03:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:03:57 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 09:04:04 visual_prompt]: Epoch 42 / 100: avg data time: 5.46e-02, avg batch time: 0.5043, average train loss: 29.8355
[09/26 09:04:06 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 29.0934
[09/26 09:04:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 09:04:06 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 09:04:12 visual_prompt]: Epoch 43 / 100: avg data time: 5.59e-02, avg batch time: 0.5053, average train loss: 26.9950
[09/26 09:04:14 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 31.2666
[09/26 09:04:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:04:14 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 09:04:21 visual_prompt]: Epoch 44 / 100: avg data time: 5.47e-02, avg batch time: 0.5029, average train loss: 26.3696
[09/26 09:04:22 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1691, average loss: 15.1627
[09/26 09:04:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:04:22 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 09:04:29 visual_prompt]: Epoch 45 / 100: avg data time: 4.97e-02, avg batch time: 0.4994, average train loss: 17.8657
[09/26 09:04:30 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 34.1741
[09/26 09:04:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:04:30 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 09:04:37 visual_prompt]: Epoch 46 / 100: avg data time: 5.55e-02, avg batch time: 0.5060, average train loss: 23.9320
[09/26 09:04:39 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1693, average loss: 22.0023
[09/26 09:04:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:04:39 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 09:04:46 visual_prompt]: Epoch 47 / 100: avg data time: 5.56e-02, avg batch time: 0.5046, average train loss: 30.3561
[09/26 09:04:47 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1694, average loss: 18.6784
[09/26 09:04:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:04:47 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 09:04:54 visual_prompt]: Epoch 48 / 100: avg data time: 5.73e-02, avg batch time: 0.5066, average train loss: 23.7620
[09/26 09:04:55 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1693, average loss: 20.3047
[09/26 09:04:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 09:04:55 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 09:05:02 visual_prompt]: Epoch 49 / 100: avg data time: 5.60e-02, avg batch time: 0.5048, average train loss: 29.1998
[09/26 09:05:04 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1691, average loss: 34.9155
[09/26 09:05:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:05:04 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 09:05:10 visual_prompt]: Epoch 50 / 100: avg data time: 4.44e-02, avg batch time: 0.4939, average train loss: 23.9833
[09/26 09:05:12 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 13.9861
[09/26 09:05:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:05:12 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 09:05:18 visual_prompt]: Epoch 51 / 100: avg data time: 5.61e-02, avg batch time: 0.5046, average train loss: 31.5150
[09/26 09:05:20 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 27.9478
[09/26 09:05:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:05:20 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 09:05:27 visual_prompt]: Epoch 52 / 100: avg data time: 5.84e-02, avg batch time: 0.5088, average train loss: 21.3481
[09/26 09:05:28 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 30.2272
[09/26 09:05:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:05:28 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 09:05:35 visual_prompt]: Epoch 53 / 100: avg data time: 5.63e-02, avg batch time: 0.5057, average train loss: 23.2526
[09/26 09:05:36 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1694, average loss: 25.5663
[09/26 09:05:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 09:05:36 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 09:05:43 visual_prompt]: Epoch 54 / 100: avg data time: 5.47e-02, avg batch time: 0.5048, average train loss: 19.6740
[09/26 09:05:45 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1688, average loss: 32.3569
[09/26 09:05:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:05:45 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 09:05:51 visual_prompt]: Epoch 55 / 100: avg data time: 3.93e-02, avg batch time: 0.4906, average train loss: 18.1705
[09/26 09:05:53 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1691, average loss: 21.6555
[09/26 09:05:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:05:53 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 09:06:00 visual_prompt]: Epoch 56 / 100: avg data time: 4.51e-02, avg batch time: 0.4933, average train loss: 16.3977
[09/26 09:06:01 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 55.6718
[09/26 09:06:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.00	top5: 82.50	
[09/26 09:06:01 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 09:06:08 visual_prompt]: Epoch 57 / 100: avg data time: 4.13e-02, avg batch time: 0.4917, average train loss: 27.8687
[09/26 09:06:09 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1695, average loss: 21.6914
[09/26 09:06:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 09:06:09 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 09:06:16 visual_prompt]: Epoch 58 / 100: avg data time: 4.63e-02, avg batch time: 0.4959, average train loss: 20.1445
[09/26 09:06:17 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 22.8093
[09/26 09:06:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 09:06:17 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 09:06:24 visual_prompt]: Epoch 59 / 100: avg data time: 3.89e-02, avg batch time: 0.4885, average train loss: 26.9676
[09/26 09:06:26 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1691, average loss: 22.5746
[09/26 09:06:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:06:26 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 09:06:33 visual_prompt]: Epoch 60 / 100: avg data time: 5.61e-02, avg batch time: 0.5044, average train loss: 14.5294
[09/26 09:06:34 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1690, average loss: 21.7651
[09/26 09:06:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 09:06:34 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 09:06:41 visual_prompt]: Epoch 61 / 100: avg data time: 5.48e-02, avg batch time: 0.5038, average train loss: 17.9574
[09/26 09:06:42 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 13.4215
[09/26 09:06:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:06:42 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 09:06:49 visual_prompt]: Epoch 62 / 100: avg data time: 4.83e-02, avg batch time: 0.4980, average train loss: 15.1183
[09/26 09:06:51 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 8.8114
[09/26 09:06:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:06:51 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 09:06:57 visual_prompt]: Epoch 63 / 100: avg data time: 5.64e-02, avg batch time: 0.5053, average train loss: 13.2150
[09/26 09:06:59 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1695, average loss: 14.5169
[09/26 09:06:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 09:06:59 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 09:07:06 visual_prompt]: Epoch 64 / 100: avg data time: 4.21e-02, avg batch time: 0.4928, average train loss: 12.0273
[09/26 09:07:07 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1688, average loss: 16.4535
[09/26 09:07:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 09:07:07 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 09:07:14 visual_prompt]: Epoch 65 / 100: avg data time: 5.29e-02, avg batch time: 0.5008, average train loss: 10.4162
[09/26 09:07:15 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1690, average loss: 7.7506
[09/26 09:07:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 09:07:15 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 09:07:22 visual_prompt]: Epoch 66 / 100: avg data time: 4.41e-02, avg batch time: 0.4951, average train loss: 17.3770
[09/26 09:07:24 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1692, average loss: 10.4324
[09/26 09:07:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:07:24 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 09:07:30 visual_prompt]: Epoch 67 / 100: avg data time: 4.59e-02, avg batch time: 0.4962, average train loss: 9.9179
[09/26 09:07:32 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1694, average loss: 7.1938
[09/26 09:07:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 09:07:32 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 09:07:38 visual_prompt]: Epoch 68 / 100: avg data time: 4.97e-02, avg batch time: 0.4991, average train loss: 11.5697
[09/26 09:07:40 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1693, average loss: 15.8721
[09/26 09:07:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:07:40 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 09:07:47 visual_prompt]: Epoch 69 / 100: avg data time: 4.22e-02, avg batch time: 0.4935, average train loss: 14.0053
[09/26 09:07:48 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1693, average loss: 3.7684
[09/26 09:07:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 09:07:48 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 09:07:55 visual_prompt]: Epoch 70 / 100: avg data time: 4.92e-02, avg batch time: 0.4999, average train loss: 5.7507
[09/26 09:07:56 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 5.5742
[09/26 09:07:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:07:56 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 09:08:03 visual_prompt]: Epoch 71 / 100: avg data time: 4.39e-02, avg batch time: 0.4929, average train loss: 6.9167
[09/26 09:08:04 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1694, average loss: 3.0517
[09/26 09:08:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:08:04 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 09:08:11 visual_prompt]: Epoch 72 / 100: avg data time: 6.02e-02, avg batch time: 0.5087, average train loss: 5.9698
[09/26 09:08:13 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 5.4857
[09/26 09:08:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:08:13 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 09:08:20 visual_prompt]: Epoch 73 / 100: avg data time: 5.53e-02, avg batch time: 0.5036, average train loss: 5.8793
[09/26 09:08:21 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1694, average loss: 4.5437
[09/26 09:08:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:08:21 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 09:08:28 visual_prompt]: Epoch 74 / 100: avg data time: 5.95e-02, avg batch time: 0.5087, average train loss: 4.1663
[09/26 09:08:29 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1690, average loss: 2.5918
[09/26 09:08:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 09:08:29 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 09:08:36 visual_prompt]: Epoch 75 / 100: avg data time: 4.39e-02, avg batch time: 0.4932, average train loss: 3.1080
[09/26 09:08:38 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1693, average loss: 2.2905
[09/26 09:08:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:08:38 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 09:08:45 visual_prompt]: Epoch 76 / 100: avg data time: 5.90e-02, avg batch time: 0.5083, average train loss: 3.7227
[09/26 09:08:46 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1696, average loss: 4.9407
[09/26 09:08:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:08:46 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 09:08:53 visual_prompt]: Epoch 77 / 100: avg data time: 4.54e-02, avg batch time: 0.4962, average train loss: 3.9256
[09/26 09:08:54 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1689, average loss: 2.8083
[09/26 09:08:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 09:08:54 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 09:09:01 visual_prompt]: Epoch 78 / 100: avg data time: 4.40e-02, avg batch time: 0.4946, average train loss: 2.9827
[09/26 09:09:03 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1688, average loss: 1.8447
[09/26 09:09:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:09:03 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 09:09:09 visual_prompt]: Epoch 79 / 100: avg data time: 4.67e-02, avg batch time: 0.4952, average train loss: 2.9171
[09/26 09:09:11 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 3.2347
[09/26 09:09:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 09:09:11 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 09:09:18 visual_prompt]: Epoch 80 / 100: avg data time: 5.74e-02, avg batch time: 0.5072, average train loss: 3.2845
[09/26 09:09:19 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1692, average loss: 2.5539
[09/26 09:09:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 09:09:19 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 09:09:26 visual_prompt]: Epoch 81 / 100: avg data time: 5.70e-02, avg batch time: 0.5052, average train loss: 2.3952
[09/26 09:09:28 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1694, average loss: 2.0216
[09/26 09:09:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 09:09:28 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 09:09:34 visual_prompt]: Epoch 82 / 100: avg data time: 5.33e-02, avg batch time: 0.5022, average train loss: 2.0475
[09/26 09:09:36 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1694, average loss: 2.0279
[09/26 09:09:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 84.50	
[09/26 09:09:36 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 09:09:43 visual_prompt]: Epoch 83 / 100: avg data time: 4.25e-02, avg batch time: 0.4939, average train loss: 1.8537
[09/26 09:09:44 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1691, average loss: 2.0404
[09/26 09:09:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:09:44 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 09:09:51 visual_prompt]: Epoch 84 / 100: avg data time: 5.48e-02, avg batch time: 0.5036, average train loss: 1.9071
[09/26 09:09:52 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1694, average loss: 1.9525
[09/26 09:09:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 09:09:52 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 09:09:59 visual_prompt]: Epoch 85 / 100: avg data time: 5.75e-02, avg batch time: 0.5077, average train loss: 1.8127
[09/26 09:10:01 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 1.8874
[09/26 09:10:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:10:01 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 09:10:08 visual_prompt]: Epoch 86 / 100: avg data time: 4.71e-02, avg batch time: 0.4965, average train loss: 1.7940
[09/26 09:10:09 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1692, average loss: 1.8679
[09/26 09:10:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:10:09 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 09:10:16 visual_prompt]: Epoch 87 / 100: avg data time: 4.19e-02, avg batch time: 0.4911, average train loss: 1.8204
[09/26 09:10:17 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1686, average loss: 1.8134
[09/26 09:10:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:10:17 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 09:10:24 visual_prompt]: Epoch 88 / 100: avg data time: 4.57e-02, avg batch time: 0.4941, average train loss: 1.8178
[09/26 09:10:25 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1687, average loss: 1.8651
[09/26 09:10:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:10:25 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 09:10:32 visual_prompt]: Epoch 89 / 100: avg data time: 4.11e-02, avg batch time: 0.4898, average train loss: 1.8390
[09/26 09:10:33 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1694, average loss: 1.9647
[09/26 09:10:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:10:33 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 09:10:40 visual_prompt]: Epoch 90 / 100: avg data time: 4.11e-02, avg batch time: 0.4919, average train loss: 1.7997
[09/26 09:10:42 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1690, average loss: 1.8982
[09/26 09:10:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:10:42 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 09:10:48 visual_prompt]: Epoch 91 / 100: avg data time: 5.31e-02, avg batch time: 0.5021, average train loss: 1.7909
[09/26 09:10:50 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 1.8136
[09/26 09:10:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 82.50	
[09/26 09:10:50 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 09:10:57 visual_prompt]: Epoch 92 / 100: avg data time: 4.31e-02, avg batch time: 0.4929, average train loss: 1.7851
[09/26 09:10:58 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1689, average loss: 1.8298
[09/26 09:10:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:10:58 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 09:11:05 visual_prompt]: Epoch 93 / 100: avg data time: 5.89e-02, avg batch time: 0.5086, average train loss: 1.7773
[09/26 09:11:06 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1694, average loss: 1.8175
[09/26 09:11:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:11:06 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 09:11:13 visual_prompt]: Epoch 94 / 100: avg data time: 4.32e-02, avg batch time: 0.4929, average train loss: 1.7745
[09/26 09:11:15 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1695, average loss: 1.8069
[09/26 09:11:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:11:15 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 09:11:21 visual_prompt]: Epoch 95 / 100: avg data time: 4.60e-02, avg batch time: 0.4953, average train loss: 1.7763
[09/26 09:11:23 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1688, average loss: 1.8285
[09/26 09:11:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:11:23 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 09:11:29 visual_prompt]: Epoch 96 / 100: avg data time: 4.76e-02, avg batch time: 0.4974, average train loss: 1.7724
[09/26 09:11:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1689, average loss: 1.8120
[09/26 09:11:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:11:31 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 09:11:38 visual_prompt]: Epoch 97 / 100: avg data time: 5.22e-02, avg batch time: 0.5012, average train loss: 1.7636
[09/26 09:11:39 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1688, average loss: 1.8054
[09/26 09:11:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:11:39 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 09:11:46 visual_prompt]: Epoch 98 / 100: avg data time: 5.27e-02, avg batch time: 0.5013, average train loss: 1.7588
[09/26 09:11:48 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1687, average loss: 1.8076
[09/26 09:11:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:11:48 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 09:11:54 visual_prompt]: Epoch 99 / 100: avg data time: 5.25e-02, avg batch time: 0.5011, average train loss: 1.7427
[09/26 09:11:56 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 1.7981
[09/26 09:11:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 84.00	
[09/26 09:11:56 visual_prompt]: Best epoch 99: best metric: 0.250
[09/26 09:11:56 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 09:12:03 visual_prompt]: Epoch 100 / 100: avg data time: 5.53e-02, avg batch time: 0.5049, average train loss: 1.7313
[09/26 09:12:04 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1692, average loss: 1.7567
[09/26 09:12:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 84.00	
[09/26 09:12:04 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 09:12:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 09:12:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 09:12:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 09:12:04 visual_prompt]: Training with config:
[09/26 09:12:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 09:12:04 visual_prompt]: Loading training data...
[09/26 09:12:04 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 09:12:06 visual_prompt]: Number of images: 800
[09/26 09:12:06 visual_prompt]: Number of classes: 6 / 6
[09/26 09:12:06 visual_prompt]: Loading validation data...
[09/26 09:12:06 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 09:12:06 visual_prompt]: Number of images: 200
[09/26 09:12:06 visual_prompt]: Number of classes: 6 / 6
[09/26 09:12:06 visual_prompt]: Constructing models...
[09/26 09:12:08 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 09:12:08 visual_prompt]: tuned percent:0.540
[09/26 09:12:09 visual_prompt]: Device used for model: 0
[09/26 09:12:09 visual_prompt]: Setting up Evaluator...
[09/26 09:12:09 visual_prompt]: Setting up Trainer...
[09/26 09:12:09 visual_prompt]: 	Setting up the optimizer...
[09/26 09:12:09 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 09:12:15 visual_prompt]: Epoch 1 / 100: avg data time: 4.85e-02, avg batch time: 0.4975, average train loss: 2.9605
[09/26 09:12:17 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1685, average loss: 2.9268
[09/26 09:12:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 09:12:17 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 09:12:17 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 09:12:23 visual_prompt]: Epoch 2 / 100: avg data time: 4.99e-02, avg batch time: 0.4986, average train loss: 10.4100
[09/26 09:12:25 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 8.4025
[09/26 09:12:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:12:25 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 09:12:32 visual_prompt]: Epoch 3 / 100: avg data time: 5.71e-02, avg batch time: 0.5048, average train loss: 10.6071
[09/26 09:12:33 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1686, average loss: 10.0464
[09/26 09:12:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:12:33 visual_prompt]: Best epoch 3: best metric: 0.205
[09/26 09:12:33 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 09:12:40 visual_prompt]: Epoch 4 / 100: avg data time: 5.53e-02, avg batch time: 0.5037, average train loss: 12.4870
[09/26 09:12:41 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1688, average loss: 6.2414
[09/26 09:12:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:12:41 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 09:12:48 visual_prompt]: Epoch 5 / 100: avg data time: 4.60e-02, avg batch time: 0.4952, average train loss: 12.8939
[09/26 09:12:50 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1689, average loss: 11.0874
[09/26 09:12:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 09:12:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 09:12:56 visual_prompt]: Epoch 6 / 100: avg data time: 5.40e-02, avg batch time: 0.5035, average train loss: 9.0566
[09/26 09:12:58 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 10.4742
[09/26 09:12:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:12:58 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 09:13:05 visual_prompt]: Epoch 7 / 100: avg data time: 4.86e-02, avg batch time: 0.4978, average train loss: 13.3577
[09/26 09:13:06 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 17.4961
[09/26 09:13:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:13:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 09:13:13 visual_prompt]: Epoch 8 / 100: avg data time: 4.71e-02, avg batch time: 0.4996, average train loss: 27.8517
[09/26 09:13:14 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1688, average loss: 29.3450
[09/26 09:13:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 09:13:14 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 09:13:21 visual_prompt]: Epoch 9 / 100: avg data time: 5.74e-02, avg batch time: 0.5058, average train loss: 25.9405
[09/26 09:13:23 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1690, average loss: 40.7152
[09/26 09:13:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 09:13:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 09:13:29 visual_prompt]: Epoch 10 / 100: avg data time: 4.73e-02, avg batch time: 0.4973, average train loss: 37.7400
[09/26 09:13:31 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1689, average loss: 17.2034
[09/26 09:13:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 85.50	
[09/26 09:13:31 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 09:13:38 visual_prompt]: Epoch 11 / 100: avg data time: 5.74e-02, avg batch time: 0.5057, average train loss: 32.2046
[09/26 09:13:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1690, average loss: 38.6729
[09/26 09:13:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 09:13:39 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 09:13:46 visual_prompt]: Epoch 12 / 100: avg data time: 4.71e-02, avg batch time: 0.4961, average train loss: 42.6003
[09/26 09:13:47 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1693, average loss: 26.8539
[09/26 09:13:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:13:47 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 09:13:54 visual_prompt]: Epoch 13 / 100: avg data time: 5.56e-02, avg batch time: 0.5039, average train loss: 30.1671
[09/26 09:13:56 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 28.2040
[09/26 09:13:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:13:56 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 09:14:02 visual_prompt]: Epoch 14 / 100: avg data time: 5.23e-02, avg batch time: 0.5015, average train loss: 39.7694
[09/26 09:14:04 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1692, average loss: 34.4752
[09/26 09:14:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:14:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 09:14:11 visual_prompt]: Epoch 15 / 100: avg data time: 5.90e-02, avg batch time: 0.5079, average train loss: 44.2770
[09/26 09:14:12 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1690, average loss: 44.1157
[09/26 09:14:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:14:12 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 09:14:19 visual_prompt]: Epoch 16 / 100: avg data time: 4.86e-02, avg batch time: 0.4973, average train loss: 41.8455
[09/26 09:14:21 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1695, average loss: 61.2270
[09/26 09:14:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:14:21 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 09:14:27 visual_prompt]: Epoch 17 / 100: avg data time: 5.35e-02, avg batch time: 0.5015, average train loss: 73.5676
[09/26 09:14:29 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1694, average loss: 53.5807
[09/26 09:14:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:14:29 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 09:14:36 visual_prompt]: Epoch 18 / 100: avg data time: 5.78e-02, avg batch time: 0.5066, average train loss: 36.1215
[09/26 09:14:37 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1695, average loss: 54.3309
[09/26 09:14:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:14:37 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 09:14:44 visual_prompt]: Epoch 19 / 100: avg data time: 5.97e-02, avg batch time: 0.5082, average train loss: 42.4558
[09/26 09:14:46 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1695, average loss: 26.6047
[09/26 09:14:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:14:46 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 09:14:52 visual_prompt]: Epoch 20 / 100: avg data time: 5.63e-02, avg batch time: 0.5043, average train loss: 23.6461
[09/26 09:14:54 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1698, average loss: 17.1901
[09/26 09:14:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:14:54 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 09:15:01 visual_prompt]: Epoch 21 / 100: avg data time: 5.60e-02, avg batch time: 0.5053, average train loss: 31.2574
[09/26 09:15:02 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1696, average loss: 44.4244
[09/26 09:15:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 09:15:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 09:15:09 visual_prompt]: Epoch 22 / 100: avg data time: 6.03e-02, avg batch time: 0.5092, average train loss: 31.9050
[09/26 09:15:11 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1689, average loss: 39.9213
[09/26 09:15:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:15:11 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 09:15:17 visual_prompt]: Epoch 23 / 100: avg data time: 4.32e-02, avg batch time: 0.4932, average train loss: 30.0129
[09/26 09:15:19 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 39.5455
[09/26 09:15:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 09:15:19 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 09:15:26 visual_prompt]: Epoch 24 / 100: avg data time: 5.91e-02, avg batch time: 0.5093, average train loss: 26.9290
[09/26 09:15:27 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1694, average loss: 37.5608
[09/26 09:15:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 81.50	
[09/26 09:15:27 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 09:15:34 visual_prompt]: Epoch 25 / 100: avg data time: 5.77e-02, avg batch time: 0.5069, average train loss: 35.5385
[09/26 09:15:36 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1696, average loss: 24.6528
[09/26 09:15:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:15:36 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 09:15:42 visual_prompt]: Epoch 26 / 100: avg data time: 4.78e-02, avg batch time: 0.4958, average train loss: 26.1624
[09/26 09:15:44 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1694, average loss: 28.0022
[09/26 09:15:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 09:15:44 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 09:15:51 visual_prompt]: Epoch 27 / 100: avg data time: 5.43e-02, avg batch time: 0.5024, average train loss: 35.0056
[09/26 09:15:52 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1699, average loss: 41.7392
[09/26 09:15:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 83.00	
[09/26 09:15:52 visual_prompt]: Best epoch 27: best metric: 0.215
[09/26 09:15:52 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 09:15:59 visual_prompt]: Epoch 28 / 100: avg data time: 5.55e-02, avg batch time: 0.5035, average train loss: 21.0126
[09/26 09:16:00 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1694, average loss: 29.2087
[09/26 09:16:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:16:00 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 09:16:07 visual_prompt]: Epoch 29 / 100: avg data time: 5.65e-02, avg batch time: 0.5053, average train loss: 35.1585
[09/26 09:16:09 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1689, average loss: 28.8247
[09/26 09:16:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 09:16:09 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 09:16:16 visual_prompt]: Epoch 30 / 100: avg data time: 5.97e-02, avg batch time: 0.5098, average train loss: 21.6519
[09/26 09:16:17 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 33.3357
[09/26 09:16:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 83.00	
[09/26 09:16:17 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 09:16:24 visual_prompt]: Epoch 31 / 100: avg data time: 5.24e-02, avg batch time: 0.5008, average train loss: 36.6504
[09/26 09:16:25 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1692, average loss: 28.1537
[09/26 09:16:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:16:25 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 09:16:32 visual_prompt]: Epoch 32 / 100: avg data time: 5.38e-02, avg batch time: 0.5039, average train loss: 26.6149
[09/26 09:16:34 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1692, average loss: 21.3556
[09/26 09:16:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:16:34 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 09:16:40 visual_prompt]: Epoch 33 / 100: avg data time: 5.17e-02, avg batch time: 0.5011, average train loss: 24.9888
[09/26 09:16:42 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1688, average loss: 17.1947
[09/26 09:16:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 13.50	top5: 83.00	
[09/26 09:16:42 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 09:16:49 visual_prompt]: Epoch 34 / 100: avg data time: 5.62e-02, avg batch time: 0.5044, average train loss: 21.2893
[09/26 09:16:50 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1692, average loss: 22.5767
[09/26 09:16:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 09:16:50 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 09:16:57 visual_prompt]: Epoch 35 / 100: avg data time: 5.68e-02, avg batch time: 0.5053, average train loss: 25.0877
[09/26 09:16:59 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 27.9940
[09/26 09:16:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 09:16:59 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 09:17:05 visual_prompt]: Epoch 36 / 100: avg data time: 5.46e-02, avg batch time: 0.5047, average train loss: 24.5720
[09/26 09:17:07 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1698, average loss: 26.8712
[09/26 09:17:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 09:17:07 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 09:17:14 visual_prompt]: Epoch 37 / 100: avg data time: 5.30e-02, avg batch time: 0.5030, average train loss: 29.3313
[09/26 09:17:15 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1691, average loss: 22.2975
[09/26 09:17:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:17:15 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 09:17:22 visual_prompt]: Epoch 38 / 100: avg data time: 5.49e-02, avg batch time: 0.5040, average train loss: 21.1784
[09/26 09:17:24 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1693, average loss: 25.4726
[09/26 09:17:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 09:17:24 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 09:17:30 visual_prompt]: Epoch 39 / 100: avg data time: 4.20e-02, avg batch time: 0.4919, average train loss: 21.5379
[09/26 09:17:32 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 11.9301
[09/26 09:17:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:17:32 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 09:17:38 visual_prompt]: Epoch 40 / 100: avg data time: 4.22e-02, avg batch time: 0.4928, average train loss: 18.6020
[09/26 09:17:40 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 20.6967
[09/26 09:17:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:17:40 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 09:17:47 visual_prompt]: Epoch 41 / 100: avg data time: 4.88e-02, avg batch time: 0.4978, average train loss: 20.1362
[09/26 09:17:48 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1696, average loss: 30.0873
[09/26 09:17:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:17:48 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 09:17:55 visual_prompt]: Epoch 42 / 100: avg data time: 4.98e-02, avg batch time: 0.5000, average train loss: 19.9524
[09/26 09:17:57 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1694, average loss: 42.1103
[09/26 09:17:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:17:57 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 09:18:03 visual_prompt]: Epoch 43 / 100: avg data time: 5.08e-02, avg batch time: 0.5007, average train loss: 23.3612
[09/26 09:18:05 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1698, average loss: 28.0067
[09/26 09:18:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:18:05 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 09:18:12 visual_prompt]: Epoch 44 / 100: avg data time: 5.75e-02, avg batch time: 0.5072, average train loss: 25.0336
[09/26 09:18:13 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1696, average loss: 17.5491
[09/26 09:18:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:18:13 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 09:18:20 visual_prompt]: Epoch 45 / 100: avg data time: 6.11e-02, avg batch time: 0.5090, average train loss: 19.8708
[09/26 09:18:21 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1696, average loss: 27.0698
[09/26 09:18:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:18:21 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 09:18:28 visual_prompt]: Epoch 46 / 100: avg data time: 5.64e-02, avg batch time: 0.5061, average train loss: 30.9002
[09/26 09:18:30 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1689, average loss: 12.7433
[09/26 09:18:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 85.50	
[09/26 09:18:30 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 09:18:37 visual_prompt]: Epoch 47 / 100: avg data time: 6.12e-02, avg batch time: 0.5092, average train loss: 21.5763
[09/26 09:18:38 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 20.6254
[09/26 09:18:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:18:38 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 09:18:45 visual_prompt]: Epoch 48 / 100: avg data time: 5.44e-02, avg batch time: 0.5033, average train loss: 19.4233
[09/26 09:18:46 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1693, average loss: 13.8757
[09/26 09:18:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:18:46 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 09:18:53 visual_prompt]: Epoch 49 / 100: avg data time: 4.68e-02, avg batch time: 0.4963, average train loss: 11.5852
[09/26 09:18:55 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1695, average loss: 32.7200
[09/26 09:18:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:18:55 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 09:19:01 visual_prompt]: Epoch 50 / 100: avg data time: 5.54e-02, avg batch time: 0.5037, average train loss: 22.5350
[09/26 09:19:03 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1697, average loss: 30.8973
[09/26 09:19:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:19:03 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 09:19:10 visual_prompt]: Epoch 51 / 100: avg data time: 5.01e-02, avg batch time: 0.4995, average train loss: 23.0221
[09/26 09:19:11 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.1693, average loss: 23.6784
[09/26 09:19:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:19:11 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 09:19:19 visual_prompt]: Epoch 52 / 100: avg data time: 5.40e-02, avg batch time: 0.5526, average train loss: 17.8083
[09/26 09:19:20 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 19.7389
[09/26 09:19:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:19:20 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 09:19:27 visual_prompt]: Epoch 53 / 100: avg data time: 4.89e-02, avg batch time: 0.4965, average train loss: 12.5461
[09/26 09:19:28 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1691, average loss: 13.2866
[09/26 09:19:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:19:28 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 09:19:35 visual_prompt]: Epoch 54 / 100: avg data time: 5.84e-02, avg batch time: 0.5074, average train loss: 12.6269
[09/26 09:19:37 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 20.5675
[09/26 09:19:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:19:37 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 09:19:44 visual_prompt]: Epoch 55 / 100: avg data time: 5.52e-02, avg batch time: 0.5036, average train loss: 10.8861
[09/26 09:19:45 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 7.0052
[09/26 09:19:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:19:45 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 09:19:52 visual_prompt]: Epoch 56 / 100: avg data time: 5.10e-02, avg batch time: 0.5001, average train loss: 10.0917
[09/26 09:19:53 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1695, average loss: 12.0701
[09/26 09:19:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:19:53 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 09:20:00 visual_prompt]: Epoch 57 / 100: avg data time: 4.41e-02, avg batch time: 0.4942, average train loss: 9.6370
[09/26 09:20:01 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1696, average loss: 9.4489
[09/26 09:20:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:20:01 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 09:20:08 visual_prompt]: Epoch 58 / 100: avg data time: 4.76e-02, avg batch time: 0.4978, average train loss: 11.0998
[09/26 09:20:09 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1690, average loss: 12.4365
[09/26 09:20:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:20:09 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 09:20:16 visual_prompt]: Epoch 59 / 100: avg data time: 5.05e-02, avg batch time: 0.4999, average train loss: 12.5191
[09/26 09:20:18 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1695, average loss: 10.6520
[09/26 09:20:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 09:20:18 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 09:20:24 visual_prompt]: Epoch 60 / 100: avg data time: 4.60e-02, avg batch time: 0.4962, average train loss: 11.5007
[09/26 09:20:26 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1693, average loss: 10.3811
[09/26 09:20:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 09:20:26 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 09:20:33 visual_prompt]: Epoch 61 / 100: avg data time: 5.02e-02, avg batch time: 0.5002, average train loss: 11.7894
[09/26 09:20:34 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 17.9837
[09/26 09:20:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 09:20:34 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 09:20:41 visual_prompt]: Epoch 62 / 100: avg data time: 5.10e-02, avg batch time: 0.5000, average train loss: 14.2014
[09/26 09:20:42 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 7.6444
[09/26 09:20:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 09:20:42 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 09:20:49 visual_prompt]: Epoch 63 / 100: avg data time: 5.48e-02, avg batch time: 0.5036, average train loss: 6.8814
[09/26 09:20:51 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1690, average loss: 6.2998
[09/26 09:20:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 09:20:51 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 09:20:57 visual_prompt]: Epoch 64 / 100: avg data time: 4.27e-02, avg batch time: 0.4929, average train loss: 4.2868
[09/26 09:20:59 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1696, average loss: 3.1352
[09/26 09:20:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 09:20:59 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 09:21:06 visual_prompt]: Epoch 65 / 100: avg data time: 4.07e-02, avg batch time: 0.4919, average train loss: 4.3433
[09/26 09:21:07 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 3.6316
[09/26 09:21:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 09:21:07 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 09:21:14 visual_prompt]: Epoch 66 / 100: avg data time: 5.37e-02, avg batch time: 0.5028, average train loss: 3.2104
[09/26 09:21:15 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1697, average loss: 2.9346
[09/26 09:21:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:21:15 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 09:21:22 visual_prompt]: Epoch 67 / 100: avg data time: 5.14e-02, avg batch time: 0.5005, average train loss: 2.6839
[09/26 09:21:24 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1696, average loss: 2.2183
[09/26 09:21:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:21:24 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 09:21:31 visual_prompt]: Epoch 68 / 100: avg data time: 5.82e-02, avg batch time: 0.5071, average train loss: 2.0247
[09/26 09:21:32 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1697, average loss: 1.9106
[09/26 09:21:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 09:21:32 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 09:21:39 visual_prompt]: Epoch 69 / 100: avg data time: 5.68e-02, avg batch time: 0.5052, average train loss: 1.9080
[09/26 09:21:40 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1695, average loss: 1.9597
[09/26 09:21:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:21:40 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 09:21:47 visual_prompt]: Epoch 70 / 100: avg data time: 5.53e-02, avg batch time: 0.5047, average train loss: 1.9251
[09/26 09:21:49 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1697, average loss: 2.0288
[09/26 09:21:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 09:21:49 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 09:21:55 visual_prompt]: Epoch 71 / 100: avg data time: 5.13e-02, avg batch time: 0.4998, average train loss: 1.8350
[09/26 09:21:57 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1691, average loss: 1.9883
[09/26 09:21:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 85.50	
[09/26 09:21:57 visual_prompt]: Best epoch 71: best metric: 0.265
[09/26 09:21:57 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 09:22:04 visual_prompt]: Epoch 72 / 100: avg data time: 4.93e-02, avg batch time: 0.4989, average train loss: 2.2002
[09/26 09:22:05 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1698, average loss: 2.7869
[09/26 09:22:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:22:05 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 09:22:12 visual_prompt]: Epoch 73 / 100: avg data time: 4.98e-02, avg batch time: 0.4988, average train loss: 2.9157
[09/26 09:22:13 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1697, average loss: 2.3945
[09/26 09:22:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:22:13 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 09:22:20 visual_prompt]: Epoch 74 / 100: avg data time: 5.26e-02, avg batch time: 0.5035, average train loss: 2.1005
[09/26 09:22:21 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1696, average loss: 1.8521
[09/26 09:22:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:22:21 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 09:22:28 visual_prompt]: Epoch 75 / 100: avg data time: 4.28e-02, avg batch time: 0.4941, average train loss: 1.8438
[09/26 09:22:30 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 1.9481
[09/26 09:22:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:22:30 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 09:22:36 visual_prompt]: Epoch 76 / 100: avg data time: 4.50e-02, avg batch time: 0.4950, average train loss: 1.8656
[09/26 09:22:38 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 2.0691
[09/26 09:22:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:22:38 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 09:22:45 visual_prompt]: Epoch 77 / 100: avg data time: 5.60e-02, avg batch time: 0.5069, average train loss: 2.0072
[09/26 09:22:46 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1692, average loss: 1.9421
[09/26 09:22:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 87.50	
[09/26 09:22:46 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 09:22:53 visual_prompt]: Epoch 78 / 100: avg data time: 5.74e-02, avg batch time: 0.5068, average train loss: 1.8172
[09/26 09:22:54 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 1.7768
[09/26 09:22:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 92.50	
[09/26 09:22:54 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 09:23:01 visual_prompt]: Epoch 79 / 100: avg data time: 4.79e-02, avg batch time: 0.4975, average train loss: 1.8501
[09/26 09:23:03 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1699, average loss: 1.8698
[09/26 09:23:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:23:03 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 09:23:09 visual_prompt]: Epoch 80 / 100: avg data time: 4.43e-02, avg batch time: 0.4956, average train loss: 1.8957
[09/26 09:23:11 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 2.0450
[09/26 09:23:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 09:23:11 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 09:23:18 visual_prompt]: Epoch 81 / 100: avg data time: 5.22e-02, avg batch time: 0.5010, average train loss: 1.8837
[09/26 09:23:19 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1690, average loss: 2.0752
[09/26 09:23:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 84.50	
[09/26 09:23:19 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 09:23:26 visual_prompt]: Epoch 82 / 100: avg data time: 5.02e-02, avg batch time: 0.4999, average train loss: 1.8214
[09/26 09:23:27 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1697, average loss: 1.8199
[09/26 09:23:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 79.50	
[09/26 09:23:27 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 09:23:34 visual_prompt]: Epoch 83 / 100: avg data time: 5.63e-02, avg batch time: 0.5048, average train loss: 1.7086
[09/26 09:23:36 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 1.7199
[09/26 09:23:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 95.50	
[09/26 09:23:36 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 09:23:42 visual_prompt]: Epoch 84 / 100: avg data time: 4.88e-02, avg batch time: 0.4990, average train loss: 1.6124
[09/26 09:23:44 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1698, average loss: 1.5667
[09/26 09:23:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 94.00	
[09/26 09:23:44 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 09:23:51 visual_prompt]: Epoch 85 / 100: avg data time: 5.29e-02, avg batch time: 0.5021, average train loss: 1.5273
[09/26 09:23:52 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1697, average loss: 1.6584
[09/26 09:23:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 92.50	
[09/26 09:23:52 visual_prompt]: Best epoch 85: best metric: 0.300
[09/26 09:23:52 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 09:23:59 visual_prompt]: Epoch 86 / 100: avg data time: 5.54e-02, avg batch time: 0.5052, average train loss: 1.5577
[09/26 09:24:00 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 1.5255
[09/26 09:24:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 96.50	
[09/26 09:24:00 visual_prompt]: Best epoch 86: best metric: 0.305
[09/26 09:24:00 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 09:24:07 visual_prompt]: Epoch 87 / 100: avg data time: 4.89e-02, avg batch time: 0.4981, average train loss: 1.4136
[09/26 09:24:09 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1693, average loss: 1.4341
[09/26 09:24:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 09:24:09 visual_prompt]: Best epoch 87: best metric: 0.350
[09/26 09:24:09 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 09:24:16 visual_prompt]: Epoch 88 / 100: avg data time: 5.77e-02, avg batch time: 0.5078, average train loss: 1.3323
[09/26 09:24:17 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1696, average loss: 1.4002
[09/26 09:24:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 99.00	
[09/26 09:24:17 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 09:24:24 visual_prompt]: Epoch 89 / 100: avg data time: 5.53e-02, avg batch time: 0.5046, average train loss: 1.3043
[09/26 09:24:25 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1697, average loss: 1.5274
[09/26 09:24:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 95.00	
[09/26 09:24:25 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 09:24:32 visual_prompt]: Epoch 90 / 100: avg data time: 5.38e-02, avg batch time: 0.5023, average train loss: 1.3281
[09/26 09:24:34 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1695, average loss: 1.4279
[09/26 09:24:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 09:24:34 visual_prompt]: Best epoch 90: best metric: 0.380
[09/26 09:24:34 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 09:24:41 visual_prompt]: Epoch 91 / 100: avg data time: 5.59e-02, avg batch time: 0.5046, average train loss: 1.2771
[09/26 09:24:42 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1697, average loss: 1.5588
[09/26 09:24:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 95.50	
[09/26 09:24:42 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 09:24:49 visual_prompt]: Epoch 92 / 100: avg data time: 4.06e-02, avg batch time: 0.4924, average train loss: 1.3124
[09/26 09:24:50 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1695, average loss: 1.3688
[09/26 09:24:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 99.00	
[09/26 09:24:50 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 09:24:57 visual_prompt]: Epoch 93 / 100: avg data time: 4.86e-02, avg batch time: 0.4983, average train loss: 1.1961
[09/26 09:24:59 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1698, average loss: 1.3880
[09/26 09:24:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.00	
[09/26 09:24:59 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 09:25:05 visual_prompt]: Epoch 94 / 100: avg data time: 5.54e-02, avg batch time: 0.5054, average train loss: 1.1444
[09/26 09:25:07 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1693, average loss: 1.2942
[09/26 09:25:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 99.00	
[09/26 09:25:07 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 09:25:14 visual_prompt]: Epoch 95 / 100: avg data time: 4.87e-02, avg batch time: 0.4976, average train loss: 1.1344
[09/26 09:25:15 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 1.3347
[09/26 09:25:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/26 09:25:15 visual_prompt]: Best epoch 95: best metric: 0.410
[09/26 09:25:15 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 09:25:22 visual_prompt]: Epoch 96 / 100: avg data time: 4.36e-02, avg batch time: 0.4922, average train loss: 1.0757
[09/26 09:25:23 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1694, average loss: 1.3155
[09/26 09:25:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.50	
[09/26 09:25:23 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 09:25:30 visual_prompt]: Epoch 97 / 100: avg data time: 4.63e-02, avg batch time: 0.4961, average train loss: 1.0289
[09/26 09:25:31 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1694, average loss: 1.2911
[09/26 09:25:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 09:25:31 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 09:25:38 visual_prompt]: Epoch 98 / 100: avg data time: 5.33e-02, avg batch time: 0.5026, average train loss: 0.9972
[09/26 09:25:40 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 1.3003
[09/26 09:25:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/26 09:25:40 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 09:25:46 visual_prompt]: Epoch 99 / 100: avg data time: 4.99e-02, avg batch time: 0.4992, average train loss: 0.9733
[09/26 09:25:48 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 1.3142
[09/26 09:25:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.00	
[09/26 09:25:48 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 09:25:55 visual_prompt]: Epoch 100 / 100: avg data time: 4.36e-02, avg batch time: 0.4930, average train loss: 0.9645
[09/26 09:25:56 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1691, average loss: 1.3067
[09/26 09:25:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.00	
[09/26 09:25:56 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 09:25:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 09:25:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 09:25:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 09:25:56 visual_prompt]: Training with config:
[09/26 09:25:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 09:25:56 visual_prompt]: Loading training data...
[09/26 09:25:56 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 09:25:57 visual_prompt]: Number of images: 800
[09/26 09:25:57 visual_prompt]: Number of classes: 6 / 6
[09/26 09:25:57 visual_prompt]: Loading validation data...
[09/26 09:25:57 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 09:25:58 visual_prompt]: Number of images: 200
[09/26 09:25:58 visual_prompt]: Number of classes: 6 / 6
[09/26 09:25:58 visual_prompt]: Constructing models...
[09/26 09:26:00 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 09:26:00 visual_prompt]: tuned percent:0.540
[09/26 09:26:00 visual_prompt]: Device used for model: 0
[09/26 09:26:00 visual_prompt]: Setting up Evaluator...
[09/26 09:26:00 visual_prompt]: Setting up Trainer...
[09/26 09:26:00 visual_prompt]: 	Setting up the optimizer...
[09/26 09:26:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 09:26:07 visual_prompt]: Epoch 1 / 100: avg data time: 5.48e-02, avg batch time: 0.5023, average train loss: 2.9722
[09/26 09:26:09 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1685, average loss: 2.9268
[09/26 09:26:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 09:26:09 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 09:26:09 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 09:26:16 visual_prompt]: Epoch 2 / 100: avg data time: 5.47e-02, avg batch time: 0.5023, average train loss: 16.1047
[09/26 09:26:17 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1685, average loss: 12.3951
[09/26 09:26:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 09:26:17 visual_prompt]: Best epoch 2: best metric: 0.205
[09/26 09:26:17 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 09:26:24 visual_prompt]: Epoch 3 / 100: avg data time: 5.33e-02, avg batch time: 0.5018, average train loss: 8.9979
[09/26 09:26:25 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1686, average loss: 9.9523
[09/26 09:26:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:26:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 09:26:32 visual_prompt]: Epoch 4 / 100: avg data time: 5.46e-02, avg batch time: 0.5028, average train loss: 10.6626
[09/26 09:26:33 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 4.2171
[09/26 09:26:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 09:26:33 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 09:26:40 visual_prompt]: Epoch 5 / 100: avg data time: 4.11e-02, avg batch time: 0.4921, average train loss: 9.5963
[09/26 09:26:42 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1695, average loss: 21.5602
[09/26 09:26:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 09:26:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 09:26:48 visual_prompt]: Epoch 6 / 100: avg data time: 5.16e-02, avg batch time: 0.5017, average train loss: 17.3566
[09/26 09:26:50 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1690, average loss: 21.4074
[09/26 09:26:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:26:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 09:26:57 visual_prompt]: Epoch 7 / 100: avg data time: 5.50e-02, avg batch time: 0.5032, average train loss: 20.4923
[09/26 09:26:58 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1696, average loss: 28.5469
[09/26 09:26:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:26:58 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 09:27:05 visual_prompt]: Epoch 8 / 100: avg data time: 5.79e-02, avg batch time: 0.5068, average train loss: 23.2785
[09/26 09:27:07 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 20.7137
[09/26 09:27:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:27:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 09:27:13 visual_prompt]: Epoch 9 / 100: avg data time: 6.08e-02, avg batch time: 0.5086, average train loss: 21.6921
[09/26 09:27:15 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1687, average loss: 12.1265
[09/26 09:27:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:27:15 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 09:27:22 visual_prompt]: Epoch 10 / 100: avg data time: 4.51e-02, avg batch time: 0.4943, average train loss: 23.6135
[09/26 09:27:23 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1696, average loss: 35.8194
[09/26 09:27:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 09:27:23 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 09:27:30 visual_prompt]: Epoch 11 / 100: avg data time: 5.51e-02, avg batch time: 0.5040, average train loss: 34.8720
[09/26 09:27:31 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 39.9834
[09/26 09:27:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 09:27:31 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 09:27:38 visual_prompt]: Epoch 12 / 100: avg data time: 5.32e-02, avg batch time: 0.5017, average train loss: 43.2565
[09/26 09:27:39 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1691, average loss: 50.4423
[09/26 09:27:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:27:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 09:27:46 visual_prompt]: Epoch 13 / 100: avg data time: 5.34e-02, avg batch time: 0.5031, average train loss: 23.7559
[09/26 09:27:48 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1691, average loss: 19.4855
[09/26 09:27:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:27:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 09:27:55 visual_prompt]: Epoch 14 / 100: avg data time: 4.36e-02, avg batch time: 0.4949, average train loss: 24.0853
[09/26 09:27:56 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 28.0796
[09/26 09:27:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:27:56 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 09:28:03 visual_prompt]: Epoch 15 / 100: avg data time: 5.35e-02, avg batch time: 0.5025, average train loss: 27.0304
[09/26 09:28:04 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1693, average loss: 10.8752
[09/26 09:28:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 09:28:04 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 09:28:11 visual_prompt]: Epoch 16 / 100: avg data time: 4.39e-02, avg batch time: 0.4924, average train loss: 15.2171
[09/26 09:28:12 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 12.8866
[09/26 09:28:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:28:12 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 09:28:19 visual_prompt]: Epoch 17 / 100: avg data time: 4.44e-02, avg batch time: 0.4940, average train loss: 16.3238
[09/26 09:28:20 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1690, average loss: 18.5161
[09/26 09:28:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 09:28:20 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 09:28:27 visual_prompt]: Epoch 18 / 100: avg data time: 4.59e-02, avg batch time: 0.4966, average train loss: 21.0883
[09/26 09:28:29 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1694, average loss: 13.8758
[09/26 09:28:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:28:29 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 09:28:35 visual_prompt]: Epoch 19 / 100: avg data time: 4.68e-02, avg batch time: 0.4973, average train loss: 18.7696
[09/26 09:28:37 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1691, average loss: 15.7662
[09/26 09:28:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:28:37 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 09:28:43 visual_prompt]: Epoch 20 / 100: avg data time: 4.39e-02, avg batch time: 0.4934, average train loss: 9.7905
[09/26 09:28:45 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1694, average loss: 20.1573
[09/26 09:28:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:28:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 09:28:52 visual_prompt]: Epoch 21 / 100: avg data time: 4.32e-02, avg batch time: 0.4946, average train loss: 17.9040
[09/26 09:28:53 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1690, average loss: 10.5340
[09/26 09:28:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:28:53 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 09:29:00 visual_prompt]: Epoch 22 / 100: avg data time: 5.79e-02, avg batch time: 0.5063, average train loss: 13.8123
[09/26 09:29:02 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.1690, average loss: 7.9833
[09/26 09:29:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:29:02 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 09:29:08 visual_prompt]: Epoch 23 / 100: avg data time: 4.53e-02, avg batch time: 0.4944, average train loss: 12.7778
[09/26 09:29:10 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1695, average loss: 20.5523
[09/26 09:29:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:29:10 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 09:29:16 visual_prompt]: Epoch 24 / 100: avg data time: 5.20e-02, avg batch time: 0.5015, average train loss: 18.0090
[09/26 09:29:18 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1695, average loss: 16.8336
[09/26 09:29:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:29:18 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 09:29:25 visual_prompt]: Epoch 25 / 100: avg data time: 5.20e-02, avg batch time: 0.5018, average train loss: 21.2287
[09/26 09:29:26 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1694, average loss: 20.3912
[09/26 09:29:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 09:29:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 09:29:33 visual_prompt]: Epoch 26 / 100: avg data time: 4.20e-02, avg batch time: 0.4908, average train loss: 21.6529
[09/26 09:29:34 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1697, average loss: 40.7240
[09/26 09:29:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:29:34 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 09:29:41 visual_prompt]: Epoch 27 / 100: avg data time: 5.43e-02, avg batch time: 0.5036, average train loss: 25.4073
[09/26 09:29:43 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1697, average loss: 9.9545
[09/26 09:29:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 09:29:43 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 09:29:49 visual_prompt]: Epoch 28 / 100: avg data time: 5.46e-02, avg batch time: 0.5033, average train loss: 21.7649
[09/26 09:29:51 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1692, average loss: 19.1609
[09/26 09:29:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:29:51 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 09:29:58 visual_prompt]: Epoch 29 / 100: avg data time: 4.54e-02, avg batch time: 0.4952, average train loss: 31.7962
[09/26 09:29:59 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1689, average loss: 29.1219
[09/26 09:29:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 09:29:59 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 09:30:06 visual_prompt]: Epoch 30 / 100: avg data time: 4.70e-02, avg batch time: 0.4975, average train loss: 15.8438
[09/26 09:30:07 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1691, average loss: 24.6357
[09/26 09:30:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:30:07 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 09:30:14 visual_prompt]: Epoch 31 / 100: avg data time: 5.75e-02, avg batch time: 0.5067, average train loss: 18.8060
[09/26 09:30:15 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1690, average loss: 13.7821
[09/26 09:30:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:30:15 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 09:30:22 visual_prompt]: Epoch 32 / 100: avg data time: 4.68e-02, avg batch time: 0.4990, average train loss: 14.5860
[09/26 09:30:24 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 19.2227
[09/26 09:30:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:30:24 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 09:30:30 visual_prompt]: Epoch 33 / 100: avg data time: 4.72e-02, avg batch time: 0.4980, average train loss: 15.6044
[09/26 09:30:32 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 13.2560
[09/26 09:30:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:30:32 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 09:30:39 visual_prompt]: Epoch 34 / 100: avg data time: 4.72e-02, avg batch time: 0.4958, average train loss: 12.2449
[09/26 09:30:40 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 16.9911
[09/26 09:30:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:30:40 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 09:30:47 visual_prompt]: Epoch 35 / 100: avg data time: 5.69e-02, avg batch time: 0.5060, average train loss: 15.9749
[09/26 09:30:48 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1688, average loss: 18.2693
[09/26 09:30:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:30:48 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 09:30:55 visual_prompt]: Epoch 36 / 100: avg data time: 6.21e-02, avg batch time: 0.5111, average train loss: 14.2969
[09/26 09:30:57 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 17.6779
[09/26 09:30:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:30:57 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 09:31:04 visual_prompt]: Epoch 37 / 100: avg data time: 4.58e-02, avg batch time: 0.4945, average train loss: 17.7826
[09/26 09:31:05 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1689, average loss: 21.1301
[09/26 09:31:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:31:05 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 09:31:12 visual_prompt]: Epoch 38 / 100: avg data time: 5.80e-02, avg batch time: 0.5070, average train loss: 22.1631
[09/26 09:31:13 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1690, average loss: 14.3503
[09/26 09:31:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 09:31:13 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 09:31:20 visual_prompt]: Epoch 39 / 100: avg data time: 4.34e-02, avg batch time: 0.4946, average train loss: 19.1795
[09/26 09:31:22 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1689, average loss: 12.4400
[09/26 09:31:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:31:22 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 09:31:29 visual_prompt]: Epoch 40 / 100: avg data time: 5.94e-02, avg batch time: 0.5081, average train loss: 15.1488
[09/26 09:31:30 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1693, average loss: 23.6515
[09/26 09:31:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:31:30 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 09:31:37 visual_prompt]: Epoch 41 / 100: avg data time: 4.49e-02, avg batch time: 0.4936, average train loss: 21.5450
[09/26 09:31:38 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1697, average loss: 18.8408
[09/26 09:31:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 09:31:38 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 09:31:45 visual_prompt]: Epoch 42 / 100: avg data time: 4.56e-02, avg batch time: 0.4964, average train loss: 17.5800
[09/26 09:31:47 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1691, average loss: 20.5252
[09/26 09:31:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 09:31:47 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 09:31:53 visual_prompt]: Epoch 43 / 100: avg data time: 5.81e-02, avg batch time: 0.5061, average train loss: 13.6761
[09/26 09:31:55 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1692, average loss: 9.3117
[09/26 09:31:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:31:55 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 09:32:02 visual_prompt]: Epoch 44 / 100: avg data time: 5.69e-02, avg batch time: 0.5056, average train loss: 10.0484
[09/26 09:32:03 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.1684, average loss: 5.7729
[09/26 09:32:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 09:32:03 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 09:32:10 visual_prompt]: Epoch 45 / 100: avg data time: 5.63e-02, avg batch time: 0.5054, average train loss: 7.6395
[09/26 09:32:12 visual_prompt]: Inference (val):avg data time: 4.51e-05, avg batch time: 0.1684, average loss: 7.8773
[09/26 09:32:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:32:12 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 09:32:19 visual_prompt]: Epoch 46 / 100: avg data time: 5.14e-02, avg batch time: 0.5002, average train loss: 6.7307
[09/26 09:32:20 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1691, average loss: 7.5568
[09/26 09:32:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 09:32:20 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 09:32:27 visual_prompt]: Epoch 47 / 100: avg data time: 4.72e-02, avg batch time: 0.4963, average train loss: 5.1803
[09/26 09:32:28 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1690, average loss: 4.4718
[09/26 09:32:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:32:28 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 09:32:35 visual_prompt]: Epoch 48 / 100: avg data time: 4.72e-02, avg batch time: 0.4966, average train loss: 3.9306
[09/26 09:32:36 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1687, average loss: 3.5781
[09/26 09:32:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:32:36 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 09:32:43 visual_prompt]: Epoch 49 / 100: avg data time: 4.95e-02, avg batch time: 0.4983, average train loss: 4.3262
[09/26 09:32:45 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 3.4026
[09/26 09:32:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 84.00	
[09/26 09:32:45 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 09:32:51 visual_prompt]: Epoch 50 / 100: avg data time: 6.04e-02, avg batch time: 0.5093, average train loss: 2.8279
[09/26 09:32:53 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1692, average loss: 2.7349
[09/26 09:32:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:32:53 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 09:33:00 visual_prompt]: Epoch 51 / 100: avg data time: 5.54e-02, avg batch time: 0.5044, average train loss: 3.0064
[09/26 09:33:01 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 2.4766
[09/26 09:33:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 82.50	
[09/26 09:33:01 visual_prompt]: Best epoch 51: best metric: 0.215
[09/26 09:33:01 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 09:33:08 visual_prompt]: Epoch 52 / 100: avg data time: 4.81e-02, avg batch time: 0.4977, average train loss: 2.1509
[09/26 09:33:09 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 2.5874
[09/26 09:33:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 84.00	
[09/26 09:33:09 visual_prompt]: Best epoch 52: best metric: 0.220
[09/26 09:33:09 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 09:33:16 visual_prompt]: Epoch 53 / 100: avg data time: 5.01e-02, avg batch time: 0.4987, average train loss: 2.1000
[09/26 09:33:18 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1694, average loss: 2.1047
[09/26 09:33:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 90.00	
[09/26 09:33:18 visual_prompt]: Best epoch 53: best metric: 0.255
[09/26 09:33:18 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 09:33:24 visual_prompt]: Epoch 54 / 100: avg data time: 5.07e-02, avg batch time: 0.5007, average train loss: 2.0973
[09/26 09:33:26 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 2.4576
[09/26 09:33:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 91.50	
[09/26 09:33:26 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 09:33:33 visual_prompt]: Epoch 55 / 100: avg data time: 5.75e-02, avg batch time: 0.5059, average train loss: 2.4118
[09/26 09:33:34 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1693, average loss: 2.0215
[09/26 09:33:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 84.00	
[09/26 09:33:34 visual_prompt]: Best epoch 55: best metric: 0.280
[09/26 09:33:34 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 09:33:41 visual_prompt]: Epoch 56 / 100: avg data time: 5.55e-02, avg batch time: 0.5046, average train loss: 1.8787
[09/26 09:33:43 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 3.3710
[09/26 09:33:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 79.50	
[09/26 09:33:43 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 09:33:49 visual_prompt]: Epoch 57 / 100: avg data time: 4.77e-02, avg batch time: 0.4971, average train loss: 2.3053
[09/26 09:33:51 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 2.0371
[09/26 09:33:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 94.50	
[09/26 09:33:51 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 09:33:58 visual_prompt]: Epoch 58 / 100: avg data time: 4.77e-02, avg batch time: 0.4976, average train loss: 1.7888
[09/26 09:33:59 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1694, average loss: 1.9346
[09/26 09:33:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 91.00	
[09/26 09:33:59 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 09:34:06 visual_prompt]: Epoch 59 / 100: avg data time: 4.25e-02, avg batch time: 0.4923, average train loss: 1.7470
[09/26 09:34:07 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 1.7243
[09/26 09:34:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 93.00	
[09/26 09:34:07 visual_prompt]: Best epoch 59: best metric: 0.295
[09/26 09:34:07 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 09:34:14 visual_prompt]: Epoch 60 / 100: avg data time: 5.47e-02, avg batch time: 0.5037, average train loss: 1.5926
[09/26 09:34:16 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1696, average loss: 1.6168
[09/26 09:34:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 09:34:16 visual_prompt]: Best epoch 60: best metric: 0.380
[09/26 09:34:16 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 09:34:22 visual_prompt]: Epoch 61 / 100: avg data time: 4.33e-02, avg batch time: 0.4940, average train loss: 1.8437
[09/26 09:34:24 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 2.0938
[09/26 09:34:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 96.00	
[09/26 09:34:24 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 09:34:31 visual_prompt]: Epoch 62 / 100: avg data time: 4.73e-02, avg batch time: 0.4978, average train loss: 1.6505
[09/26 09:34:32 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1696, average loss: 1.8263
[09/26 09:34:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 96.50	
[09/26 09:34:32 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 09:34:39 visual_prompt]: Epoch 63 / 100: avg data time: 4.61e-02, avg batch time: 0.4967, average train loss: 1.6142
[09/26 09:34:40 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1697, average loss: 1.6283
[09/26 09:34:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 95.00	
[09/26 09:34:40 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 09:34:47 visual_prompt]: Epoch 64 / 100: avg data time: 5.55e-02, avg batch time: 0.5046, average train loss: 1.4432
[09/26 09:34:49 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1696, average loss: 1.4377
[09/26 09:34:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.00	
[09/26 09:34:49 visual_prompt]: Best epoch 64: best metric: 0.430
[09/26 09:34:49 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 09:34:55 visual_prompt]: Epoch 65 / 100: avg data time: 4.85e-02, avg batch time: 0.4978, average train loss: 1.4416
[09/26 09:34:57 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 1.7995
[09/26 09:34:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.00	
[09/26 09:34:57 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 09:35:04 visual_prompt]: Epoch 66 / 100: avg data time: 4.69e-02, avg batch time: 0.4963, average train loss: 1.3158
[09/26 09:35:05 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1695, average loss: 1.4948
[09/26 09:35:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 99.00	
[09/26 09:35:05 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 09:35:12 visual_prompt]: Epoch 67 / 100: avg data time: 6.24e-02, avg batch time: 0.5114, average train loss: 1.2954
[09/26 09:35:13 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 2.0466
[09/26 09:35:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 93.50	
[09/26 09:35:13 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 09:35:20 visual_prompt]: Epoch 68 / 100: avg data time: 5.61e-02, avg batch time: 0.5069, average train loss: 1.2911
[09/26 09:35:22 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1691, average loss: 1.3293
[09/26 09:35:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 99.00	
[09/26 09:35:22 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 09:35:28 visual_prompt]: Epoch 69 / 100: avg data time: 4.06e-02, avg batch time: 0.4916, average train loss: 1.1515
[09/26 09:35:30 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 1.6231
[09/26 09:35:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.50	
[09/26 09:35:30 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 09:35:37 visual_prompt]: Epoch 70 / 100: avg data time: 4.85e-02, avg batch time: 0.4986, average train loss: 1.2124
[09/26 09:35:38 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1695, average loss: 1.6545
[09/26 09:35:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 99.00	
[09/26 09:35:38 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 09:35:45 visual_prompt]: Epoch 71 / 100: avg data time: 5.22e-02, avg batch time: 0.5013, average train loss: 1.1759
[09/26 09:35:46 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1698, average loss: 1.4134
[09/26 09:35:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.50	
[09/26 09:35:46 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 09:35:53 visual_prompt]: Epoch 72 / 100: avg data time: 4.97e-02, avg batch time: 0.4997, average train loss: 1.1336
[09/26 09:35:54 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1698, average loss: 1.8259
[09/26 09:35:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.50	
[09/26 09:35:54 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 09:36:02 visual_prompt]: Epoch 73 / 100: avg data time: 6.95e-02, avg batch time: 0.5188, average train loss: 1.1904
[09/26 09:36:03 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1696, average loss: 1.4846
[09/26 09:36:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/26 09:36:03 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 09:36:10 visual_prompt]: Epoch 74 / 100: avg data time: 5.37e-02, avg batch time: 0.5027, average train loss: 1.1662
[09/26 09:36:11 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 1.5387
[09/26 09:36:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 09:36:11 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 09:36:18 visual_prompt]: Epoch 75 / 100: avg data time: 5.33e-02, avg batch time: 0.5041, average train loss: 1.0417
[09/26 09:36:20 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.5106
[09/26 09:36:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 09:36:20 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 09:36:26 visual_prompt]: Epoch 76 / 100: avg data time: 5.08e-02, avg batch time: 0.5013, average train loss: 1.0660
[09/26 09:36:28 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 1.4194
[09/26 09:36:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 99.00	
[09/26 09:36:28 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 09:36:35 visual_prompt]: Epoch 77 / 100: avg data time: 4.65e-02, avg batch time: 0.4970, average train loss: 0.9160
[09/26 09:36:36 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 1.6829
[09/26 09:36:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.50	
[09/26 09:36:36 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 09:36:43 visual_prompt]: Epoch 78 / 100: avg data time: 4.61e-02, avg batch time: 0.4961, average train loss: 0.8933
[09/26 09:36:44 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1694, average loss: 1.5591
[09/26 09:36:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 99.50	
[09/26 09:36:44 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 09:36:51 visual_prompt]: Epoch 79 / 100: avg data time: 5.62e-02, avg batch time: 0.5061, average train loss: 0.8598
[09/26 09:36:53 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1695, average loss: 1.5123
[09/26 09:36:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 09:36:53 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 09:37:00 visual_prompt]: Epoch 80 / 100: avg data time: 5.82e-02, avg batch time: 0.5075, average train loss: 0.8537
[09/26 09:37:01 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1694, average loss: 1.6787
[09/26 09:37:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/26 09:37:01 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 09:37:08 visual_prompt]: Epoch 81 / 100: avg data time: 4.44e-02, avg batch time: 0.4936, average train loss: 0.8993
[09/26 09:37:09 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1701, average loss: 1.5048
[09/26 09:37:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.50	
[09/26 09:37:09 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 09:37:16 visual_prompt]: Epoch 82 / 100: avg data time: 5.70e-02, avg batch time: 0.5062, average train loss: 0.8047
[09/26 09:37:18 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1694, average loss: 1.4451
[09/26 09:37:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 99.00	
[09/26 09:37:18 visual_prompt]: Best epoch 82: best metric: 0.455
[09/26 09:37:18 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 09:37:24 visual_prompt]: Epoch 83 / 100: avg data time: 5.31e-02, avg batch time: 0.5025, average train loss: 0.7136
[09/26 09:37:26 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1693, average loss: 1.8192
[09/26 09:37:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.00	
[09/26 09:37:26 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 09:37:33 visual_prompt]: Epoch 84 / 100: avg data time: 4.20e-02, avg batch time: 0.4930, average train loss: 0.7269
[09/26 09:37:34 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 1.5580
[09/26 09:37:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 98.50	
[09/26 09:37:34 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 09:37:41 visual_prompt]: Epoch 85 / 100: avg data time: 6.07e-02, avg batch time: 0.5096, average train loss: 0.7594
[09/26 09:37:42 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1693, average loss: 1.7012
[09/26 09:37:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/26 09:37:42 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 09:37:49 visual_prompt]: Epoch 86 / 100: avg data time: 5.48e-02, avg batch time: 0.5042, average train loss: 0.6751
[09/26 09:37:51 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1695, average loss: 1.8382
[09/26 09:37:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 09:37:51 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 09:37:58 visual_prompt]: Epoch 87 / 100: avg data time: 5.50e-02, avg batch time: 0.5049, average train loss: 0.6348
[09/26 09:37:59 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 1.8754
[09/26 09:37:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 99.00	
[09/26 09:37:59 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 09:38:06 visual_prompt]: Epoch 88 / 100: avg data time: 5.31e-02, avg batch time: 0.5027, average train loss: 0.6233
[09/26 09:38:07 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 1.7740
[09/26 09:38:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 09:38:07 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 09:38:14 visual_prompt]: Epoch 89 / 100: avg data time: 4.80e-02, avg batch time: 0.4983, average train loss: 0.5536
[09/26 09:38:16 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 1.8234
[09/26 09:38:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.00	
[09/26 09:38:16 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 09:38:22 visual_prompt]: Epoch 90 / 100: avg data time: 4.55e-02, avg batch time: 0.4969, average train loss: 0.5159
[09/26 09:38:24 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1695, average loss: 2.2065
[09/26 09:38:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 09:38:24 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 09:38:31 visual_prompt]: Epoch 91 / 100: avg data time: 5.44e-02, avg batch time: 0.5036, average train loss: 0.5031
[09/26 09:38:32 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 1.9618
[09/26 09:38:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 99.00	
[09/26 09:38:32 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 09:38:39 visual_prompt]: Epoch 92 / 100: avg data time: 4.20e-02, avg batch time: 0.4918, average train loss: 0.4516
[09/26 09:38:40 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 2.1651
[09/26 09:38:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 09:38:40 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 09:38:47 visual_prompt]: Epoch 93 / 100: avg data time: 4.06e-02, avg batch time: 0.4919, average train loss: 0.4273
[09/26 09:38:49 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1691, average loss: 2.1644
[09/26 09:38:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 98.50	
[09/26 09:38:49 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 09:38:55 visual_prompt]: Epoch 94 / 100: avg data time: 4.39e-02, avg batch time: 0.4954, average train loss: 0.3863
[09/26 09:38:57 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1697, average loss: 2.4396
[09/26 09:38:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 09:38:57 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 09:39:03 visual_prompt]: Epoch 95 / 100: avg data time: 4.35e-02, avg batch time: 0.4948, average train loss: 0.3750
[09/26 09:39:05 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 2.4571
[09/26 09:39:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.50	
[09/26 09:39:05 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 09:39:12 visual_prompt]: Epoch 96 / 100: avg data time: 6.08e-02, avg batch time: 0.5101, average train loss: 0.3652
[09/26 09:39:13 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 2.4534
[09/26 09:39:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/26 09:39:13 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 09:39:20 visual_prompt]: Epoch 97 / 100: avg data time: 5.62e-02, avg batch time: 0.5068, average train loss: 0.3467
[09/26 09:39:22 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1695, average loss: 2.4986
[09/26 09:39:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/26 09:39:22 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 09:39:29 visual_prompt]: Epoch 98 / 100: avg data time: 5.87e-02, avg batch time: 0.5072, average train loss: 0.3268
[09/26 09:39:30 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1690, average loss: 2.5133
[09/26 09:39:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.00	
[09/26 09:39:30 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 09:39:37 visual_prompt]: Epoch 99 / 100: avg data time: 5.05e-02, avg batch time: 0.5001, average train loss: 0.3285
[09/26 09:39:38 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1691, average loss: 2.5339
[09/26 09:39:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.00	
[09/26 09:39:38 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 09:39:45 visual_prompt]: Epoch 100 / 100: avg data time: 5.22e-02, avg batch time: 0.5029, average train loss: 0.3228
[09/26 09:39:47 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1695, average loss: 2.5354
[09/26 09:39:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.00	
[09/26 09:39:47 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 09:39:47 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 09:39:47 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 09:39:47 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 09:39:47 visual_prompt]: Training with config:
[09/26 09:39:47 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 09:39:47 visual_prompt]: Loading training data...
[09/26 09:39:47 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 09:39:48 visual_prompt]: Number of images: 800
[09/26 09:39:48 visual_prompt]: Number of classes: 6 / 6
[09/26 09:39:48 visual_prompt]: Loading validation data...
[09/26 09:39:48 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 09:39:49 visual_prompt]: Number of images: 200
[09/26 09:39:49 visual_prompt]: Number of classes: 6 / 6
[09/26 09:39:49 visual_prompt]: Constructing models...
[09/26 09:39:51 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 09:39:51 visual_prompt]: tuned percent:0.540
[09/26 09:39:51 visual_prompt]: Device used for model: 0
[09/26 09:39:51 visual_prompt]: Setting up Evaluator...
[09/26 09:39:51 visual_prompt]: Setting up Trainer...
[09/26 09:39:51 visual_prompt]: 	Setting up the optimizer...
[09/26 09:39:51 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 09:39:58 visual_prompt]: Epoch 1 / 100: avg data time: 5.84e-02, avg batch time: 0.5062, average train loss: 2.9634
[09/26 09:39:59 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1686, average loss: 2.9268
[09/26 09:39:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 09:39:59 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 09:39:59 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 09:40:06 visual_prompt]: Epoch 2 / 100: avg data time: 5.44e-02, avg batch time: 0.5023, average train loss: 11.6936
[09/26 09:40:08 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1684, average loss: 10.8038
[09/26 09:40:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 09:40:08 visual_prompt]: Best epoch 2: best metric: 0.205
[09/26 09:40:08 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 09:40:15 visual_prompt]: Epoch 3 / 100: avg data time: 5.52e-02, avg batch time: 0.5036, average train loss: 8.1901
[09/26 09:40:16 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1690, average loss: 4.0733
[09/26 09:40:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 84.00	
[09/26 09:40:16 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 09:40:23 visual_prompt]: Epoch 4 / 100: avg data time: 5.50e-02, avg batch time: 0.5042, average train loss: 12.6022
[09/26 09:40:24 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1687, average loss: 20.0403
[09/26 09:40:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:40:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 09:40:31 visual_prompt]: Epoch 5 / 100: avg data time: 5.27e-02, avg batch time: 0.5006, average train loss: 17.8786
[09/26 09:40:33 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1689, average loss: 22.5853
[09/26 09:40:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 09:40:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 09:40:39 visual_prompt]: Epoch 6 / 100: avg data time: 4.49e-02, avg batch time: 0.4929, average train loss: 13.5502
[09/26 09:40:41 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1690, average loss: 16.0034
[09/26 09:40:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:40:41 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 09:40:48 visual_prompt]: Epoch 7 / 100: avg data time: 4.91e-02, avg batch time: 0.4977, average train loss: 17.7345
[09/26 09:40:49 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 11.2291
[09/26 09:40:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 09:40:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 09:40:56 visual_prompt]: Epoch 8 / 100: avg data time: 4.59e-02, avg batch time: 0.4950, average train loss: 19.3219
[09/26 09:40:57 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1690, average loss: 18.8953
[09/26 09:40:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:40:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 09:41:04 visual_prompt]: Epoch 9 / 100: avg data time: 5.54e-02, avg batch time: 0.5045, average train loss: 26.6241
[09/26 09:41:06 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1697, average loss: 16.8656
[09/26 09:41:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 09:41:06 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 09:41:13 visual_prompt]: Epoch 10 / 100: avg data time: 6.10e-02, avg batch time: 0.5117, average train loss: 19.7362
[09/26 09:41:14 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1688, average loss: 8.4254
[09/26 09:41:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:41:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 09:41:21 visual_prompt]: Epoch 11 / 100: avg data time: 5.00e-02, avg batch time: 0.4999, average train loss: 19.8512
[09/26 09:41:23 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 13.5621
[09/26 09:41:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:41:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 09:41:29 visual_prompt]: Epoch 12 / 100: avg data time: 6.16e-02, avg batch time: 0.5101, average train loss: 17.9625
[09/26 09:41:31 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 24.1833
[09/26 09:41:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:41:31 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 09:41:38 visual_prompt]: Epoch 13 / 100: avg data time: 5.74e-02, avg batch time: 0.5070, average train loss: 19.7584
[09/26 09:41:39 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 7.0452
[09/26 09:41:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 09:41:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 09:41:46 visual_prompt]: Epoch 14 / 100: avg data time: 4.59e-02, avg batch time: 0.4945, average train loss: 11.8760
[09/26 09:41:48 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1690, average loss: 22.3592
[09/26 09:41:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:41:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 09:41:54 visual_prompt]: Epoch 15 / 100: avg data time: 4.30e-02, avg batch time: 0.4928, average train loss: 19.5745
[09/26 09:41:56 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1692, average loss: 19.9778
[09/26 09:41:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:41:56 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 09:42:03 visual_prompt]: Epoch 16 / 100: avg data time: 5.00e-02, avg batch time: 0.4993, average train loss: 20.2066
[09/26 09:42:04 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1692, average loss: 20.1884
[09/26 09:42:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 09:42:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 09:42:11 visual_prompt]: Epoch 17 / 100: avg data time: 5.76e-02, avg batch time: 0.5065, average train loss: 26.9623
[09/26 09:42:12 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1693, average loss: 13.9099
[09/26 09:42:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 09:42:12 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 09:42:19 visual_prompt]: Epoch 18 / 100: avg data time: 5.48e-02, avg batch time: 0.5048, average train loss: 12.3750
[09/26 09:42:21 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 20.3661
[09/26 09:42:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:42:21 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 09:42:28 visual_prompt]: Epoch 19 / 100: avg data time: 5.99e-02, avg batch time: 0.5086, average train loss: 15.2892
[09/26 09:42:29 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1691, average loss: 32.2778
[09/26 09:42:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:42:29 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 09:42:36 visual_prompt]: Epoch 20 / 100: avg data time: 5.11e-02, avg batch time: 0.5013, average train loss: 18.4388
[09/26 09:42:37 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1686, average loss: 20.7418
[09/26 09:42:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 09:42:37 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 09:42:44 visual_prompt]: Epoch 21 / 100: avg data time: 5.80e-02, avg batch time: 0.5088, average train loss: 19.4553
[09/26 09:42:46 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1694, average loss: 21.4970
[09/26 09:42:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 09:42:46 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 09:42:53 visual_prompt]: Epoch 22 / 100: avg data time: 5.64e-02, avg batch time: 0.5060, average train loss: 13.5810
[09/26 09:42:54 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 10.0071
[09/26 09:42:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:42:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 09:43:01 visual_prompt]: Epoch 23 / 100: avg data time: 5.09e-02, avg batch time: 0.5009, average train loss: 8.2694
[09/26 09:43:02 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1695, average loss: 6.6924
[09/26 09:43:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 09:43:02 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 09:43:09 visual_prompt]: Epoch 24 / 100: avg data time: 5.88e-02, avg batch time: 0.5073, average train loss: 9.2853
[09/26 09:43:11 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 13.1138
[09/26 09:43:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 09:43:11 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 09:43:17 visual_prompt]: Epoch 25 / 100: avg data time: 4.26e-02, avg batch time: 0.4926, average train loss: 10.0043
[09/26 09:43:19 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1696, average loss: 10.2772
[09/26 09:43:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 09:43:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 09:43:26 visual_prompt]: Epoch 26 / 100: avg data time: 4.83e-02, avg batch time: 0.4993, average train loss: 9.5570
[09/26 09:43:27 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1695, average loss: 6.4441
[09/26 09:43:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 90.00	
[09/26 09:43:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 09:43:34 visual_prompt]: Epoch 27 / 100: avg data time: 4.95e-02, avg batch time: 0.4988, average train loss: 8.1191
[09/26 09:43:35 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1691, average loss: 10.0717
[09/26 09:43:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 85.50	
[09/26 09:43:35 visual_prompt]: Best epoch 27: best metric: 0.250
[09/26 09:43:35 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 09:43:42 visual_prompt]: Epoch 28 / 100: avg data time: 5.37e-02, avg batch time: 0.5037, average train loss: 8.4388
[09/26 09:43:44 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1697, average loss: 5.1739
[09/26 09:43:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 84.00	
[09/26 09:43:44 visual_prompt]: Best epoch 28: best metric: 0.260
[09/26 09:43:44 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 09:43:51 visual_prompt]: Epoch 29 / 100: avg data time: 4.94e-02, avg batch time: 0.4983, average train loss: 7.2523
[09/26 09:43:52 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1695, average loss: 8.3172
[09/26 09:43:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 85.50	
[09/26 09:43:52 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 09:43:59 visual_prompt]: Epoch 30 / 100: avg data time: 4.84e-02, avg batch time: 0.4994, average train loss: 5.2938
[09/26 09:44:00 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 4.7015
[09/26 09:44:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 83.00	
[09/26 09:44:00 visual_prompt]: Best epoch 30: best metric: 0.270
[09/26 09:44:00 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 09:44:07 visual_prompt]: Epoch 31 / 100: avg data time: 5.50e-02, avg batch time: 0.5046, average train loss: 4.8817
[09/26 09:44:09 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 5.1556
[09/26 09:44:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 94.00	
[09/26 09:44:09 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 09:44:15 visual_prompt]: Epoch 32 / 100: avg data time: 5.35e-02, avg batch time: 0.5025, average train loss: 3.5132
[09/26 09:44:17 visual_prompt]: Inference (val):avg data time: 6.01e-05, avg batch time: 0.1697, average loss: 2.7623
[09/26 09:44:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 85.50	
[09/26 09:44:17 visual_prompt]: Best epoch 32: best metric: 0.275
[09/26 09:44:17 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 09:44:24 visual_prompt]: Epoch 33 / 100: avg data time: 4.97e-02, avg batch time: 0.4992, average train loss: 1.8682
[09/26 09:44:25 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1695, average loss: 1.8428
[09/26 09:44:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 09:44:25 visual_prompt]: Best epoch 33: best metric: 0.365
[09/26 09:44:25 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 09:44:32 visual_prompt]: Epoch 34 / 100: avg data time: 4.82e-02, avg batch time: 0.4985, average train loss: 1.5793
[09/26 09:44:33 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1692, average loss: 1.7835
[09/26 09:44:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 09:44:33 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 09:44:40 visual_prompt]: Epoch 35 / 100: avg data time: 4.74e-02, avg batch time: 0.4977, average train loss: 1.6359
[09/26 09:44:42 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 2.0013
[09/26 09:44:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 88.00	
[09/26 09:44:42 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 09:44:49 visual_prompt]: Epoch 36 / 100: avg data time: 6.05e-02, avg batch time: 0.5111, average train loss: 2.2200
[09/26 09:44:50 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 2.2098
[09/26 09:44:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 09:44:50 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 09:44:57 visual_prompt]: Epoch 37 / 100: avg data time: 7.07e-02, avg batch time: 0.5193, average train loss: 2.1337
[09/26 09:44:59 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1692, average loss: 1.8565
[09/26 09:44:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 09:44:59 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 09:45:05 visual_prompt]: Epoch 38 / 100: avg data time: 5.76e-02, avg batch time: 0.5065, average train loss: 1.7446
[09/26 09:45:07 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1690, average loss: 1.9317
[09/26 09:45:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 09:45:07 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 09:45:14 visual_prompt]: Epoch 39 / 100: avg data time: 5.09e-02, avg batch time: 0.5014, average train loss: 1.6753
[09/26 09:45:15 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1690, average loss: 2.0351
[09/26 09:45:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.50	
[09/26 09:45:15 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 09:45:22 visual_prompt]: Epoch 40 / 100: avg data time: 4.74e-02, avg batch time: 0.4970, average train loss: 1.4489
[09/26 09:45:23 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1694, average loss: 2.0939
[09/26 09:45:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 09:45:23 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 09:45:30 visual_prompt]: Epoch 41 / 100: avg data time: 5.39e-02, avg batch time: 0.5036, average train loss: 1.4830
[09/26 09:45:32 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1692, average loss: 2.6234
[09/26 09:45:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 96.50	
[09/26 09:45:32 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 09:45:39 visual_prompt]: Epoch 42 / 100: avg data time: 4.89e-02, avg batch time: 0.4989, average train loss: 1.3558
[09/26 09:45:40 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 1.7482
[09/26 09:45:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 09:45:40 visual_prompt]: Best epoch 42: best metric: 0.400
[09/26 09:45:40 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 09:45:47 visual_prompt]: Epoch 43 / 100: avg data time: 5.23e-02, avg batch time: 0.5018, average train loss: 1.2159
[09/26 09:45:48 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 1.6328
[09/26 09:45:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 09:45:48 visual_prompt]: Best epoch 43: best metric: 0.415
[09/26 09:45:48 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 09:45:55 visual_prompt]: Epoch 44 / 100: avg data time: 4.84e-02, avg batch time: 0.4995, average train loss: 1.0325
[09/26 09:45:56 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 1.6646
[09/26 09:45:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.50	
[09/26 09:45:56 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 09:46:03 visual_prompt]: Epoch 45 / 100: avg data time: 4.73e-02, avg batch time: 0.4986, average train loss: 1.1777
[09/26 09:46:05 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1694, average loss: 2.4029
[09/26 09:46:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 98.50	
[09/26 09:46:05 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 09:46:12 visual_prompt]: Epoch 46 / 100: avg data time: 5.48e-02, avg batch time: 0.5038, average train loss: 1.6253
[09/26 09:46:13 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1696, average loss: 1.7682
[09/26 09:46:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 09:46:13 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 09:46:20 visual_prompt]: Epoch 47 / 100: avg data time: 5.44e-02, avg batch time: 0.5052, average train loss: 1.1411
[09/26 09:46:21 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1698, average loss: 2.2161
[09/26 09:46:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/26 09:46:21 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 09:46:28 visual_prompt]: Epoch 48 / 100: avg data time: 5.51e-02, avg batch time: 0.5046, average train loss: 1.0141
[09/26 09:46:30 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1698, average loss: 1.6943
[09/26 09:46:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/26 09:46:30 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 09:46:36 visual_prompt]: Epoch 49 / 100: avg data time: 5.39e-02, avg batch time: 0.5035, average train loss: 0.9366
[09/26 09:46:38 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1689, average loss: 2.6964
[09/26 09:46:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 09:46:38 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 09:46:45 visual_prompt]: Epoch 50 / 100: avg data time: 5.57e-02, avg batch time: 0.5062, average train loss: 0.9556
[09/26 09:46:46 visual_prompt]: Inference (val):avg data time: 4.99e-05, avg batch time: 0.1691, average loss: 2.2134
[09/26 09:46:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 09:46:46 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 09:46:53 visual_prompt]: Epoch 51 / 100: avg data time: 5.77e-02, avg batch time: 0.5081, average train loss: 0.9726
[09/26 09:46:55 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1694, average loss: 2.1249
[09/26 09:46:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 09:46:55 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 09:47:02 visual_prompt]: Epoch 52 / 100: avg data time: 5.16e-02, avg batch time: 0.5017, average train loss: 0.8049
[09/26 09:47:03 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1695, average loss: 1.9612
[09/26 09:47:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 98.00	
[09/26 09:47:03 visual_prompt]: Best epoch 52: best metric: 0.425
[09/26 09:47:03 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 09:47:10 visual_prompt]: Epoch 53 / 100: avg data time: 4.76e-02, avg batch time: 0.4973, average train loss: 0.8461
[09/26 09:47:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 2.7846
[09/26 09:47:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 98.50	
[09/26 09:47:11 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 09:47:18 visual_prompt]: Epoch 54 / 100: avg data time: 5.61e-02, avg batch time: 0.5055, average train loss: 0.7277
[09/26 09:47:20 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1694, average loss: 1.9312
[09/26 09:47:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 09:47:20 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 09:47:26 visual_prompt]: Epoch 55 / 100: avg data time: 4.40e-02, avg batch time: 0.4928, average train loss: 0.6921
[09/26 09:47:28 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 2.5282
[09/26 09:47:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.50	
[09/26 09:47:28 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 09:47:35 visual_prompt]: Epoch 56 / 100: avg data time: 5.09e-02, avg batch time: 0.5008, average train loss: 0.6306
[09/26 09:47:36 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1686, average loss: 2.4786
[09/26 09:47:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 09:47:36 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 09:47:43 visual_prompt]: Epoch 57 / 100: avg data time: 7.75e-02, avg batch time: 0.5258, average train loss: 0.6164
[09/26 09:47:45 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1687, average loss: 2.4004
[09/26 09:47:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 09:47:45 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 09:47:52 visual_prompt]: Epoch 58 / 100: avg data time: 5.99e-02, avg batch time: 0.5087, average train loss: 0.6494
[09/26 09:47:53 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1691, average loss: 2.1667
[09/26 09:47:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/26 09:47:53 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 09:48:00 visual_prompt]: Epoch 59 / 100: avg data time: 4.31e-02, avg batch time: 0.4930, average train loss: 0.5293
[09/26 09:48:01 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1694, average loss: 2.4235
[09/26 09:48:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.50	
[09/26 09:48:01 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 09:48:08 visual_prompt]: Epoch 60 / 100: avg data time: 5.78e-02, avg batch time: 0.5067, average train loss: 0.4464
[09/26 09:48:10 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 2.7008
[09/26 09:48:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 09:48:10 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 09:48:17 visual_prompt]: Epoch 61 / 100: avg data time: 6.08e-02, avg batch time: 0.5098, average train loss: 0.4011
[09/26 09:48:18 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1696, average loss: 2.7625
[09/26 09:48:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.00	
[09/26 09:48:18 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 09:48:25 visual_prompt]: Epoch 62 / 100: avg data time: 5.67e-02, avg batch time: 0.5057, average train loss: 0.3459
[09/26 09:48:26 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1696, average loss: 2.7996
[09/26 09:48:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 09:48:26 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 09:48:33 visual_prompt]: Epoch 63 / 100: avg data time: 4.86e-02, avg batch time: 0.5001, average train loss: 0.3561
[09/26 09:48:35 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1693, average loss: 2.7354
[09/26 09:48:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 09:48:35 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 09:48:41 visual_prompt]: Epoch 64 / 100: avg data time: 4.32e-02, avg batch time: 0.4949, average train loss: 0.3531
[09/26 09:48:43 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1698, average loss: 2.7828
[09/26 09:48:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.50	
[09/26 09:48:43 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 09:48:50 visual_prompt]: Epoch 65 / 100: avg data time: 5.48e-02, avg batch time: 0.5038, average train loss: 0.3941
[09/26 09:48:51 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 2.8428
[09/26 09:48:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 09:48:51 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 09:48:58 visual_prompt]: Epoch 66 / 100: avg data time: 5.19e-02, avg batch time: 0.5008, average train loss: 0.2874
[09/26 09:48:59 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 2.8996
[09/26 09:49:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 09:49:00 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 09:49:06 visual_prompt]: Epoch 67 / 100: avg data time: 4.83e-02, avg batch time: 0.4972, average train loss: 0.2804
[09/26 09:49:08 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1692, average loss: 2.6154
[09/26 09:49:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.50	
[09/26 09:49:08 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 09:49:14 visual_prompt]: Epoch 68 / 100: avg data time: 4.79e-02, avg batch time: 0.4979, average train loss: 0.2613
[09/26 09:49:16 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 3.0265
[09/26 09:49:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 09:49:16 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 09:49:23 visual_prompt]: Epoch 69 / 100: avg data time: 4.63e-02, avg batch time: 0.4968, average train loss: 0.2574
[09/26 09:49:24 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 3.0319
[09/26 09:49:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 09:49:24 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 09:49:31 visual_prompt]: Epoch 70 / 100: avg data time: 4.24e-02, avg batch time: 0.4940, average train loss: 0.2829
[09/26 09:49:33 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 3.1648
[09/26 09:49:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.00	
[09/26 09:49:33 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 09:49:39 visual_prompt]: Epoch 71 / 100: avg data time: 4.83e-02, avg batch time: 0.4988, average train loss: 0.2367
[09/26 09:49:41 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1697, average loss: 3.2089
[09/26 09:49:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 09:49:41 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 09:49:48 visual_prompt]: Epoch 72 / 100: avg data time: 4.62e-02, avg batch time: 0.4975, average train loss: 0.1790
[09/26 09:49:49 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1695, average loss: 3.2391
[09/26 09:49:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 09:49:49 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 09:49:56 visual_prompt]: Epoch 73 / 100: avg data time: 5.61e-02, avg batch time: 0.5055, average train loss: 0.1870
[09/26 09:49:57 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1687, average loss: 3.1957
[09/26 09:49:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.00	
[09/26 09:49:57 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 09:50:04 visual_prompt]: Epoch 74 / 100: avg data time: 5.79e-02, avg batch time: 0.5076, average train loss: 0.1474
[09/26 09:50:06 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1695, average loss: 3.3501
[09/26 09:50:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.50	
[09/26 09:50:06 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 09:50:13 visual_prompt]: Epoch 75 / 100: avg data time: 5.73e-02, avg batch time: 0.5066, average train loss: 0.1449
[09/26 09:50:14 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1695, average loss: 3.4539
[09/26 09:50:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.50	
[09/26 09:50:14 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 09:50:21 visual_prompt]: Epoch 76 / 100: avg data time: 5.79e-02, avg batch time: 0.5080, average train loss: 0.1332
[09/26 09:50:23 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1695, average loss: 3.4012
[09/26 09:50:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.00	
[09/26 09:50:23 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 09:50:29 visual_prompt]: Epoch 77 / 100: avg data time: 5.62e-02, avg batch time: 0.5069, average train loss: 0.1301
[09/26 09:50:31 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1694, average loss: 3.2232
[09/26 09:50:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 09:50:31 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 09:50:38 visual_prompt]: Epoch 78 / 100: avg data time: 5.64e-02, avg batch time: 0.5049, average train loss: 0.1263
[09/26 09:50:39 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1696, average loss: 3.4072
[09/26 09:50:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 98.00	
[09/26 09:50:39 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 09:50:46 visual_prompt]: Epoch 79 / 100: avg data time: 5.33e-02, avg batch time: 0.5041, average train loss: 0.1047
[09/26 09:50:48 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 3.4972
[09/26 09:50:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 97.50	
[09/26 09:50:48 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 09:50:54 visual_prompt]: Epoch 80 / 100: avg data time: 5.43e-02, avg batch time: 0.5044, average train loss: 0.1014
[09/26 09:50:56 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1696, average loss: 3.5596
[09/26 09:50:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 09:50:56 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 09:51:03 visual_prompt]: Epoch 81 / 100: avg data time: 5.89e-02, avg batch time: 0.5090, average train loss: 0.0866
[09/26 09:51:04 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1696, average loss: 3.5827
[09/26 09:51:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 09:51:04 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 09:51:11 visual_prompt]: Epoch 82 / 100: avg data time: 5.79e-02, avg batch time: 0.5078, average train loss: 0.0961
[09/26 09:51:13 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1694, average loss: 3.5935
[09/26 09:51:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 09:51:13 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 09:51:19 visual_prompt]: Epoch 83 / 100: avg data time: 4.90e-02, avg batch time: 0.4998, average train loss: 0.0965
[09/26 09:51:21 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 3.5883
[09/26 09:51:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 09:51:21 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 09:51:28 visual_prompt]: Epoch 84 / 100: avg data time: 4.81e-02, avg batch time: 0.4988, average train loss: 0.0870
[09/26 09:51:29 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1695, average loss: 3.5808
[09/26 09:51:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 09:51:29 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 09:51:36 visual_prompt]: Epoch 85 / 100: avg data time: 4.15e-02, avg batch time: 0.4905, average train loss: 0.0749
[09/26 09:51:37 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1696, average loss: 3.9134
[09/26 09:51:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 09:51:37 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 09:51:44 visual_prompt]: Epoch 86 / 100: avg data time: 5.63e-02, avg batch time: 0.5053, average train loss: 0.0713
[09/26 09:51:46 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.1691, average loss: 3.8644
[09/26 09:51:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 09:51:46 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 09:51:53 visual_prompt]: Epoch 87 / 100: avg data time: 5.40e-02, avg batch time: 0.5026, average train loss: 0.0642
[09/26 09:51:54 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1695, average loss: 3.8676
[09/26 09:51:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 09:51:54 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 09:52:01 visual_prompt]: Epoch 88 / 100: avg data time: 4.11e-02, avg batch time: 0.4924, average train loss: 0.0575
[09/26 09:52:02 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1696, average loss: 4.0644
[09/26 09:52:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 09:52:02 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 09:52:09 visual_prompt]: Epoch 89 / 100: avg data time: 5.54e-02, avg batch time: 0.5043, average train loss: 0.0567
[09/26 09:52:11 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1697, average loss: 4.1933
[09/26 09:52:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.00	
[09/26 09:52:11 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 09:52:17 visual_prompt]: Epoch 90 / 100: avg data time: 4.93e-02, avg batch time: 0.4992, average train loss: 0.0485
[09/26 09:52:19 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 4.1596
[09/26 09:52:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 09:52:19 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 09:52:26 visual_prompt]: Epoch 91 / 100: avg data time: 5.78e-02, avg batch time: 0.5068, average train loss: 0.0557
[09/26 09:52:27 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1691, average loss: 4.2657
[09/26 09:52:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.00	
[09/26 09:52:27 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 09:52:34 visual_prompt]: Epoch 92 / 100: avg data time: 4.17e-02, avg batch time: 0.4913, average train loss: 0.0616
[09/26 09:52:36 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1696, average loss: 4.3140
[09/26 09:52:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 09:52:36 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 09:52:42 visual_prompt]: Epoch 93 / 100: avg data time: 5.51e-02, avg batch time: 0.5039, average train loss: 0.0631
[09/26 09:52:44 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 4.3013
[09/26 09:52:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 09:52:44 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 09:52:51 visual_prompt]: Epoch 94 / 100: avg data time: 5.66e-02, avg batch time: 0.5066, average train loss: 0.0448
[09/26 09:52:52 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 4.3905
[09/26 09:52:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 09:52:52 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 09:52:59 visual_prompt]: Epoch 95 / 100: avg data time: 5.54e-02, avg batch time: 0.5054, average train loss: 0.0500
[09/26 09:53:01 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1690, average loss: 4.4281
[09/26 09:53:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 09:53:01 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 09:53:08 visual_prompt]: Epoch 96 / 100: avg data time: 6.02e-02, avg batch time: 0.5102, average train loss: 0.0407
[09/26 09:53:09 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 4.3980
[09/26 09:53:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 09:53:09 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 09:53:16 visual_prompt]: Epoch 97 / 100: avg data time: 4.75e-02, avg batch time: 0.4964, average train loss: 0.0441
[09/26 09:53:17 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 4.4065
[09/26 09:53:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 09:53:17 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 09:53:24 visual_prompt]: Epoch 98 / 100: avg data time: 5.27e-02, avg batch time: 0.5021, average train loss: 0.0386
[09/26 09:53:25 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 4.4106
[09/26 09:53:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 09:53:25 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 09:53:32 visual_prompt]: Epoch 99 / 100: avg data time: 5.49e-02, avg batch time: 0.5039, average train loss: 0.0393
[09/26 09:53:34 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1697, average loss: 4.4105
[09/26 09:53:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 09:53:34 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 09:53:40 visual_prompt]: Epoch 100 / 100: avg data time: 5.77e-02, avg batch time: 0.5071, average train loss: 0.0379
[09/26 09:53:42 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 4.4112
[09/26 09:53:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 09:53:42 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 09:53:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 09:53:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 09:53:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 09:53:42 visual_prompt]: Training with config:
[09/26 09:53:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 09:53:42 visual_prompt]: Loading training data...
[09/26 09:53:42 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 09:53:43 visual_prompt]: Number of images: 800
[09/26 09:53:43 visual_prompt]: Number of classes: 6 / 6
[09/26 09:53:43 visual_prompt]: Loading validation data...
[09/26 09:53:43 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 09:53:44 visual_prompt]: Number of images: 200
[09/26 09:53:44 visual_prompt]: Number of classes: 6 / 6
[09/26 09:53:44 visual_prompt]: Constructing models...
[09/26 09:53:46 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 09:53:46 visual_prompt]: tuned percent:0.540
[09/26 09:53:46 visual_prompt]: Device used for model: 0
[09/26 09:53:46 visual_prompt]: Setting up Evaluator...
[09/26 09:53:46 visual_prompt]: Setting up Trainer...
[09/26 09:53:46 visual_prompt]: 	Setting up the optimizer...
[09/26 09:53:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 09:53:53 visual_prompt]: Epoch 1 / 100: avg data time: 5.26e-02, avg batch time: 0.5009, average train loss: 2.9708
[09/26 09:53:55 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1685, average loss: 2.9268
[09/26 09:53:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 09:53:55 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 09:53:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 09:54:01 visual_prompt]: Epoch 2 / 100: avg data time: 4.62e-02, avg batch time: 0.4947, average train loss: 6.9322
[09/26 09:54:03 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1687, average loss: 4.5853
[09/26 09:54:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 09:54:03 visual_prompt]: Best epoch 2: best metric: 0.205
[09/26 09:54:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 09:54:09 visual_prompt]: Epoch 3 / 100: avg data time: 5.19e-02, avg batch time: 0.5012, average train loss: 3.1126
[09/26 09:54:11 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1684, average loss: 1.8235
[09/26 09:54:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 13.00	top5: 84.00	
[09/26 09:54:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 09:54:18 visual_prompt]: Epoch 4 / 100: avg data time: 4.88e-02, avg batch time: 0.4971, average train loss: 2.3123
[09/26 09:54:19 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1687, average loss: 3.2544
[09/26 09:54:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 09:54:19 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 09:54:26 visual_prompt]: Epoch 5 / 100: avg data time: 4.35e-02, avg batch time: 0.4946, average train loss: 3.4192
[09/26 09:54:27 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 4.8885
[09/26 09:54:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:54:27 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 09:54:34 visual_prompt]: Epoch 6 / 100: avg data time: 4.27e-02, avg batch time: 0.4913, average train loss: 4.0102
[09/26 09:54:36 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1686, average loss: 3.2143
[09/26 09:54:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:54:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 09:54:42 visual_prompt]: Epoch 7 / 100: avg data time: 4.59e-02, avg batch time: 0.4962, average train loss: 6.8390
[09/26 09:54:44 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1688, average loss: 4.8468
[09/26 09:54:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 09:54:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 09:54:51 visual_prompt]: Epoch 8 / 100: avg data time: 4.44e-02, avg batch time: 0.4943, average train loss: 7.9527
[09/26 09:54:52 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1697, average loss: 9.5470
[09/26 09:54:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:54:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 09:54:59 visual_prompt]: Epoch 9 / 100: avg data time: 4.47e-02, avg batch time: 0.4936, average train loss: 14.7998
[09/26 09:55:00 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1692, average loss: 12.7825
[09/26 09:55:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:55:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 09:55:07 visual_prompt]: Epoch 10 / 100: avg data time: 5.31e-02, avg batch time: 0.5017, average train loss: 16.0599
[09/26 09:55:09 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 19.1418
[09/26 09:55:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 09:55:09 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 09:55:16 visual_prompt]: Epoch 11 / 100: avg data time: 4.49e-02, avg batch time: 0.4934, average train loss: 11.8286
[09/26 09:55:17 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 5.8738
[09/26 09:55:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 09:55:17 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 09:55:24 visual_prompt]: Epoch 12 / 100: avg data time: 5.83e-02, avg batch time: 0.5084, average train loss: 14.8976
[09/26 09:55:25 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1694, average loss: 18.4265
[09/26 09:55:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 09:55:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 09:55:32 visual_prompt]: Epoch 13 / 100: avg data time: 5.29e-02, avg batch time: 0.5010, average train loss: 16.8218
[09/26 09:55:34 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1691, average loss: 14.6929
[09/26 09:55:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 09:55:34 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 09:55:41 visual_prompt]: Epoch 14 / 100: avg data time: 5.41e-02, avg batch time: 0.5035, average train loss: 18.0128
[09/26 09:55:42 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 23.2781
[09/26 09:55:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:55:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 09:55:49 visual_prompt]: Epoch 15 / 100: avg data time: 5.57e-02, avg batch time: 0.5036, average train loss: 16.2345
[09/26 09:55:50 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1688, average loss: 13.1234
[09/26 09:55:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 09:55:50 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 09:55:57 visual_prompt]: Epoch 16 / 100: avg data time: 5.21e-02, avg batch time: 0.5013, average train loss: 13.1998
[09/26 09:55:59 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1687, average loss: 18.6911
[09/26 09:55:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:55:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 09:56:05 visual_prompt]: Epoch 17 / 100: avg data time: 5.41e-02, avg batch time: 0.5034, average train loss: 24.6275
[09/26 09:56:07 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1687, average loss: 10.1398
[09/26 09:56:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:56:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 09:56:14 visual_prompt]: Epoch 18 / 100: avg data time: 5.54e-02, avg batch time: 0.5043, average train loss: 20.3330
[09/26 09:56:15 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1695, average loss: 17.2343
[09/26 09:56:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 09:56:15 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 09:56:22 visual_prompt]: Epoch 19 / 100: avg data time: 6.19e-02, avg batch time: 0.5115, average train loss: 24.4602
[09/26 09:56:24 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 14.2239
[09/26 09:56:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 84.00	
[09/26 09:56:24 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 09:56:30 visual_prompt]: Epoch 20 / 100: avg data time: 4.23e-02, avg batch time: 0.4907, average train loss: 13.2748
[09/26 09:56:32 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 19.1766
[09/26 09:56:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 83.00	
[09/26 09:56:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 09:56:39 visual_prompt]: Epoch 21 / 100: avg data time: 5.09e-02, avg batch time: 0.5012, average train loss: 14.0719
[09/26 09:56:40 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1697, average loss: 10.8706
[09/26 09:56:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 09:56:40 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 09:56:47 visual_prompt]: Epoch 22 / 100: avg data time: 4.24e-02, avg batch time: 0.4916, average train loss: 17.7203
[09/26 09:56:48 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1690, average loss: 27.9638
[09/26 09:56:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 09:56:48 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 09:56:55 visual_prompt]: Epoch 23 / 100: avg data time: 5.35e-02, avg batch time: 0.5036, average train loss: 23.1518
[09/26 09:56:57 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 17.4754
[09/26 09:56:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:56:57 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 09:57:03 visual_prompt]: Epoch 24 / 100: avg data time: 4.53e-02, avg batch time: 0.4944, average train loss: 14.1061
[09/26 09:57:05 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1691, average loss: 13.3035
[09/26 09:57:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 09:57:05 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 09:57:12 visual_prompt]: Epoch 25 / 100: avg data time: 5.22e-02, avg batch time: 0.5007, average train loss: 23.2693
[09/26 09:57:13 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1691, average loss: 19.3213
[09/26 09:57:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 09:57:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 09:57:20 visual_prompt]: Epoch 26 / 100: avg data time: 5.98e-02, avg batch time: 0.5090, average train loss: 15.4694
[09/26 09:57:21 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1690, average loss: 36.1781
[09/26 09:57:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:57:21 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 09:57:28 visual_prompt]: Epoch 27 / 100: avg data time: 5.23e-02, avg batch time: 0.5018, average train loss: 18.9768
[09/26 09:57:30 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1688, average loss: 16.4944
[09/26 09:57:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:57:30 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 09:57:37 visual_prompt]: Epoch 28 / 100: avg data time: 5.42e-02, avg batch time: 0.5044, average train loss: 12.4775
[09/26 09:57:38 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1686, average loss: 20.4531
[09/26 09:57:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 09:57:38 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 09:57:45 visual_prompt]: Epoch 29 / 100: avg data time: 5.58e-02, avg batch time: 0.5040, average train loss: 13.2704
[09/26 09:57:47 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 26.4596
[09/26 09:57:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 09:57:47 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 09:57:53 visual_prompt]: Epoch 30 / 100: avg data time: 5.43e-02, avg batch time: 0.5027, average train loss: 21.0581
[09/26 09:57:55 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 21.8498
[09/26 09:57:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:57:55 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 09:58:01 visual_prompt]: Epoch 31 / 100: avg data time: 4.20e-02, avg batch time: 0.4921, average train loss: 16.5211
[09/26 09:58:03 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1691, average loss: 19.8884
[09/26 09:58:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:58:03 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 09:58:10 visual_prompt]: Epoch 32 / 100: avg data time: 4.90e-02, avg batch time: 0.4988, average train loss: 13.1660
[09/26 09:58:11 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 5.1442
[09/26 09:58:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:58:11 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 09:58:18 visual_prompt]: Epoch 33 / 100: avg data time: 4.45e-02, avg batch time: 0.4935, average train loss: 12.3430
[09/26 09:58:19 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 15.9044
[09/26 09:58:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 09:58:19 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 09:58:26 visual_prompt]: Epoch 34 / 100: avg data time: 4.98e-02, avg batch time: 0.5003, average train loss: 16.1034
[09/26 09:58:28 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1690, average loss: 18.2896
[09/26 09:58:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 09:58:28 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 09:58:35 visual_prompt]: Epoch 35 / 100: avg data time: 5.48e-02, avg batch time: 0.5042, average train loss: 19.0060
[09/26 09:58:36 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 10.6850
[09/26 09:58:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 09:58:36 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 09:58:43 visual_prompt]: Epoch 36 / 100: avg data time: 5.33e-02, avg batch time: 0.5030, average train loss: 14.9747
[09/26 09:58:44 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 16.2851
[09/26 09:58:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 09:58:44 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 09:58:51 visual_prompt]: Epoch 37 / 100: avg data time: 5.51e-02, avg batch time: 0.5037, average train loss: 10.2918
[09/26 09:58:53 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 8.9826
[09/26 09:58:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 09:58:53 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 09:58:59 visual_prompt]: Epoch 38 / 100: avg data time: 4.40e-02, avg batch time: 0.4948, average train loss: 22.5015
[09/26 09:59:01 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 14.1358
[09/26 09:59:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:59:01 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 09:59:08 visual_prompt]: Epoch 39 / 100: avg data time: 5.62e-02, avg batch time: 0.5046, average train loss: 13.5224
[09/26 09:59:09 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1695, average loss: 14.7018
[09/26 09:59:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 85.50	
[09/26 09:59:09 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 09:59:16 visual_prompt]: Epoch 40 / 100: avg data time: 5.37e-02, avg batch time: 0.5023, average train loss: 20.4995
[09/26 09:59:18 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1695, average loss: 18.4612
[09/26 09:59:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:59:18 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 09:59:24 visual_prompt]: Epoch 41 / 100: avg data time: 5.43e-02, avg batch time: 0.5023, average train loss: 14.0038
[09/26 09:59:26 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 16.8079
[09/26 09:59:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 09:59:26 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 09:59:33 visual_prompt]: Epoch 42 / 100: avg data time: 5.14e-02, avg batch time: 0.5019, average train loss: 10.5239
[09/26 09:59:34 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 12.2863
[09/26 09:59:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 09:59:34 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 09:59:41 visual_prompt]: Epoch 43 / 100: avg data time: 4.38e-02, avg batch time: 0.4925, average train loss: 14.5906
[09/26 09:59:42 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 16.9008
[09/26 09:59:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 09:59:42 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 09:59:49 visual_prompt]: Epoch 44 / 100: avg data time: 5.60e-02, avg batch time: 0.5038, average train loss: 12.1795
[09/26 09:59:51 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 13.5116
[09/26 09:59:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 09:59:51 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 09:59:57 visual_prompt]: Epoch 45 / 100: avg data time: 5.03e-02, avg batch time: 0.4989, average train loss: 14.6685
[09/26 09:59:59 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1688, average loss: 12.8613
[09/26 09:59:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 09:59:59 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 10:00:06 visual_prompt]: Epoch 46 / 100: avg data time: 5.71e-02, avg batch time: 0.5063, average train loss: 8.9028
[09/26 10:00:07 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 13.2162
[09/26 10:00:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:00:07 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 10:00:14 visual_prompt]: Epoch 47 / 100: avg data time: 5.38e-02, avg batch time: 0.5028, average train loss: 8.4412
[09/26 10:00:16 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1692, average loss: 8.5946
[09/26 10:00:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 10:00:16 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 10:00:23 visual_prompt]: Epoch 48 / 100: avg data time: 5.81e-02, avg batch time: 0.5065, average train loss: 8.3006
[09/26 10:00:24 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 6.8587
[09/26 10:00:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 10:00:24 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 10:00:31 visual_prompt]: Epoch 49 / 100: avg data time: 5.06e-02, avg batch time: 0.5002, average train loss: 9.2862
[09/26 10:00:32 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 8.6177
[09/26 10:00:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:00:32 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 10:00:39 visual_prompt]: Epoch 50 / 100: avg data time: 3.93e-02, avg batch time: 0.4901, average train loss: 8.0551
[09/26 10:00:40 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1697, average loss: 5.2082
[09/26 10:00:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:00:40 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 10:00:47 visual_prompt]: Epoch 51 / 100: avg data time: 5.29e-02, avg batch time: 0.5025, average train loss: 6.4974
[09/26 10:00:49 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1691, average loss: 10.1214
[09/26 10:00:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 84.00	
[09/26 10:00:49 visual_prompt]: Best epoch 51: best metric: 0.210
[09/26 10:00:49 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 10:00:55 visual_prompt]: Epoch 52 / 100: avg data time: 5.76e-02, avg batch time: 0.5072, average train loss: 9.5741
[09/26 10:00:57 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1697, average loss: 3.1669
[09/26 10:00:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:00:57 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 10:01:04 visual_prompt]: Epoch 53 / 100: avg data time: 4.88e-02, avg batch time: 0.4988, average train loss: 8.2893
[09/26 10:01:05 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1690, average loss: 28.5671
[09/26 10:01:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:01:05 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 10:01:12 visual_prompt]: Epoch 54 / 100: avg data time: 4.42e-02, avg batch time: 0.4951, average train loss: 10.0041
[09/26 10:01:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 3.1182
[09/26 10:01:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:01:13 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 10:01:20 visual_prompt]: Epoch 55 / 100: avg data time: 4.48e-02, avg batch time: 0.4954, average train loss: 7.4597
[09/26 10:01:22 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1690, average loss: 12.2308
[09/26 10:01:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:01:22 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 10:01:28 visual_prompt]: Epoch 56 / 100: avg data time: 4.29e-02, avg batch time: 0.4922, average train loss: 9.5482
[09/26 10:01:30 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1687, average loss: 17.2651
[09/26 10:01:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:01:30 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 10:01:37 visual_prompt]: Epoch 57 / 100: avg data time: 5.32e-02, avg batch time: 0.5023, average train loss: 16.4471
[09/26 10:01:38 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1692, average loss: 9.1332
[09/26 10:01:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 10:01:38 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 10:01:45 visual_prompt]: Epoch 58 / 100: avg data time: 5.27e-02, avg batch time: 0.5014, average train loss: 8.9799
[09/26 10:01:47 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 5.7480
[09/26 10:01:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:01:47 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 10:01:53 visual_prompt]: Epoch 59 / 100: avg data time: 4.60e-02, avg batch time: 0.4968, average train loss: 5.9032
[09/26 10:01:55 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1696, average loss: 5.0480
[09/26 10:01:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 10:01:55 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 10:02:02 visual_prompt]: Epoch 60 / 100: avg data time: 5.80e-02, avg batch time: 0.5057, average train loss: 7.0299
[09/26 10:02:03 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1687, average loss: 3.2253
[09/26 10:02:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 10:02:03 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 10:02:10 visual_prompt]: Epoch 61 / 100: avg data time: 4.14e-02, avg batch time: 0.4927, average train loss: 4.1861
[09/26 10:02:12 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1689, average loss: 2.7317
[09/26 10:02:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:02:12 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 10:02:18 visual_prompt]: Epoch 62 / 100: avg data time: 4.58e-02, avg batch time: 0.4948, average train loss: 3.0637
[09/26 10:02:20 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 2.3017
[09/26 10:02:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:02:20 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 10:02:26 visual_prompt]: Epoch 63 / 100: avg data time: 4.34e-02, avg batch time: 0.4928, average train loss: 2.9304
[09/26 10:02:28 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1697, average loss: 6.2117
[09/26 10:02:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 80.00	
[09/26 10:02:28 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 10:02:35 visual_prompt]: Epoch 64 / 100: avg data time: 4.50e-02, avg batch time: 0.4962, average train loss: 7.0823
[09/26 10:02:36 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 7.5281
[09/26 10:02:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 10:02:36 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 10:02:43 visual_prompt]: Epoch 65 / 100: avg data time: 4.39e-02, avg batch time: 0.4939, average train loss: 9.4136
[09/26 10:02:44 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 3.8371
[09/26 10:02:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 85.50	
[09/26 10:02:44 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 10:02:51 visual_prompt]: Epoch 66 / 100: avg data time: 5.00e-02, avg batch time: 0.4999, average train loss: 7.4692
[09/26 10:02:52 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 11.2693
[09/26 10:02:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 10:02:52 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 10:02:59 visual_prompt]: Epoch 67 / 100: avg data time: 4.39e-02, avg batch time: 0.4972, average train loss: 5.4719
[09/26 10:03:01 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1695, average loss: 3.3165
[09/26 10:03:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:03:01 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 10:03:07 visual_prompt]: Epoch 68 / 100: avg data time: 5.03e-02, avg batch time: 0.5003, average train loss: 2.4787
[09/26 10:03:09 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1690, average loss: 2.3282
[09/26 10:03:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:03:09 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 10:03:16 visual_prompt]: Epoch 69 / 100: avg data time: 4.84e-02, avg batch time: 0.4982, average train loss: 2.4419
[09/26 10:03:17 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1693, average loss: 2.9899
[09/26 10:03:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:03:17 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 10:03:24 visual_prompt]: Epoch 70 / 100: avg data time: 5.49e-02, avg batch time: 0.5039, average train loss: 2.1821
[09/26 10:03:25 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1693, average loss: 2.4014
[09/26 10:03:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:03:25 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 10:03:32 visual_prompt]: Epoch 71 / 100: avg data time: 4.42e-02, avg batch time: 0.4940, average train loss: 2.6241
[09/26 10:03:34 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1693, average loss: 4.9643
[09/26 10:03:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:03:34 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 10:03:40 visual_prompt]: Epoch 72 / 100: avg data time: 5.66e-02, avg batch time: 0.5045, average train loss: 2.3426
[09/26 10:03:42 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1689, average loss: 1.9035
[09/26 10:03:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 10:03:42 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 10:03:49 visual_prompt]: Epoch 73 / 100: avg data time: 5.84e-02, avg batch time: 0.5075, average train loss: 1.9514
[09/26 10:03:50 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 1.9874
[09/26 10:03:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 10:03:50 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 10:03:57 visual_prompt]: Epoch 74 / 100: avg data time: 4.77e-02, avg batch time: 0.4981, average train loss: 1.9241
[09/26 10:03:59 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1691, average loss: 1.8330
[09/26 10:03:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 10:03:59 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 10:04:05 visual_prompt]: Epoch 75 / 100: avg data time: 4.21e-02, avg batch time: 0.4934, average train loss: 1.8484
[09/26 10:04:07 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1692, average loss: 1.9510
[09/26 10:04:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:04:07 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 10:04:14 visual_prompt]: Epoch 76 / 100: avg data time: 4.85e-02, avg batch time: 0.4985, average train loss: 1.8926
[09/26 10:04:15 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1689, average loss: 1.8457
[09/26 10:04:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:04:15 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 10:04:22 visual_prompt]: Epoch 77 / 100: avg data time: 4.26e-02, avg batch time: 0.4924, average train loss: 1.8087
[09/26 10:04:23 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 1.9482
[09/26 10:04:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:04:23 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 10:04:30 visual_prompt]: Epoch 78 / 100: avg data time: 5.96e-02, avg batch time: 0.5074, average train loss: 1.8165
[09/26 10:04:32 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1689, average loss: 2.1922
[09/26 10:04:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:04:32 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 10:04:38 visual_prompt]: Epoch 79 / 100: avg data time: 5.60e-02, avg batch time: 0.5045, average train loss: 1.8918
[09/26 10:04:40 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1691, average loss: 1.8409
[09/26 10:04:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:04:40 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 10:04:47 visual_prompt]: Epoch 80 / 100: avg data time: 4.71e-02, avg batch time: 0.4970, average train loss: 1.9268
[09/26 10:04:48 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 1.9350
[09/26 10:04:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:04:48 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 10:04:55 visual_prompt]: Epoch 81 / 100: avg data time: 4.48e-02, avg batch time: 0.4947, average train loss: 1.8852
[09/26 10:04:57 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1690, average loss: 1.9313
[09/26 10:04:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:04:57 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 10:05:03 visual_prompt]: Epoch 82 / 100: avg data time: 5.17e-02, avg batch time: 0.5009, average train loss: 1.8098
[09/26 10:05:05 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 1.8182
[09/26 10:05:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 10:05:05 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 10:05:12 visual_prompt]: Epoch 83 / 100: avg data time: 5.10e-02, avg batch time: 0.4994, average train loss: 1.8232
[09/26 10:05:13 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1693, average loss: 1.8464
[09/26 10:05:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:05:13 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 10:05:20 visual_prompt]: Epoch 84 / 100: avg data time: 5.65e-02, avg batch time: 0.5054, average train loss: 1.8032
[09/26 10:05:21 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1692, average loss: 1.8218
[09/26 10:05:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:05:21 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 10:05:28 visual_prompt]: Epoch 85 / 100: avg data time: 4.29e-02, avg batch time: 0.4944, average train loss: 1.7771
[09/26 10:05:30 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1690, average loss: 1.8063
[09/26 10:05:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:05:30 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 10:05:36 visual_prompt]: Epoch 86 / 100: avg data time: 5.00e-02, avg batch time: 0.4986, average train loss: 1.8057
[09/26 10:05:38 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1691, average loss: 1.8508
[09/26 10:05:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:05:38 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 10:05:45 visual_prompt]: Epoch 87 / 100: avg data time: 6.05e-02, avg batch time: 0.5089, average train loss: 1.8187
[09/26 10:05:46 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1697, average loss: 1.8961
[09/26 10:05:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 10:05:46 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 10:05:53 visual_prompt]: Epoch 88 / 100: avg data time: 5.77e-02, avg batch time: 0.5070, average train loss: 1.8029
[09/26 10:05:55 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1694, average loss: 1.8909
[09/26 10:05:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:05:55 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 10:06:01 visual_prompt]: Epoch 89 / 100: avg data time: 4.71e-02, avg batch time: 0.4964, average train loss: 1.7889
[09/26 10:06:03 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1686, average loss: 1.8252
[09/26 10:06:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:06:03 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 10:06:10 visual_prompt]: Epoch 90 / 100: avg data time: 5.55e-02, avg batch time: 0.5040, average train loss: 1.7842
[09/26 10:06:11 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1694, average loss: 1.9576
[09/26 10:06:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:06:11 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 10:06:18 visual_prompt]: Epoch 91 / 100: avg data time: 5.39e-02, avg batch time: 0.5032, average train loss: 1.7965
[09/26 10:06:20 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1691, average loss: 1.8011
[09/26 10:06:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:06:20 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 10:06:26 visual_prompt]: Epoch 92 / 100: avg data time: 5.60e-02, avg batch time: 0.5050, average train loss: 1.7777
[09/26 10:06:28 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1687, average loss: 1.7951
[09/26 10:06:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:06:28 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 10:06:35 visual_prompt]: Epoch 93 / 100: avg data time: 5.13e-02, avg batch time: 0.4998, average train loss: 1.7752
[09/26 10:06:36 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1697, average loss: 1.8172
[09/26 10:06:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 84.00	
[09/26 10:06:36 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 10:06:43 visual_prompt]: Epoch 94 / 100: avg data time: 5.54e-02, avg batch time: 0.5048, average train loss: 1.7659
[09/26 10:06:45 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 1.8196
[09/26 10:06:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:06:45 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 10:06:51 visual_prompt]: Epoch 95 / 100: avg data time: 5.54e-02, avg batch time: 0.5057, average train loss: 1.7606
[09/26 10:06:53 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1692, average loss: 1.7935
[09/26 10:06:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 84.00	
[09/26 10:06:53 visual_prompt]: Best epoch 95: best metric: 0.245
[09/26 10:06:53 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 10:07:00 visual_prompt]: Epoch 96 / 100: avg data time: 5.00e-02, avg batch time: 0.4998, average train loss: 1.7399
[09/26 10:07:01 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 1.7866
[09/26 10:07:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 84.00	
[09/26 10:07:01 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 10:07:08 visual_prompt]: Epoch 97 / 100: avg data time: 5.45e-02, avg batch time: 0.5032, average train loss: 1.7487
[09/26 10:07:09 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 1.7857
[09/26 10:07:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 84.00	
[09/26 10:07:09 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 10:07:16 visual_prompt]: Epoch 98 / 100: avg data time: 4.27e-02, avg batch time: 0.4933, average train loss: 1.7142
[09/26 10:07:18 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1695, average loss: 1.7670
[09/26 10:07:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 85.00	
[09/26 10:07:18 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 10:07:24 visual_prompt]: Epoch 99 / 100: avg data time: 5.03e-02, avg batch time: 0.5005, average train loss: 1.7089
[09/26 10:07:26 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1692, average loss: 1.7743
[09/26 10:07:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 84.00	
[09/26 10:07:26 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 10:07:33 visual_prompt]: Epoch 100 / 100: avg data time: 5.56e-02, avg batch time: 0.5050, average train loss: 1.7024
[09/26 10:07:34 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1692, average loss: 1.7380
[09/26 10:07:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 84.50	
[09/26 10:07:34 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:07:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:07:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:07:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:07:34 visual_prompt]: Training with config:
[09/26 10:07:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:07:34 visual_prompt]: Loading training data...
[09/26 10:07:34 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 10:07:36 visual_prompt]: Number of images: 800
[09/26 10:07:36 visual_prompt]: Number of classes: 6 / 6
[09/26 10:07:36 visual_prompt]: Loading validation data...
[09/26 10:07:36 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 10:07:36 visual_prompt]: Number of images: 200
[09/26 10:07:36 visual_prompt]: Number of classes: 6 / 6
[09/26 10:07:36 visual_prompt]: Constructing models...
[09/26 10:07:38 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 10:07:38 visual_prompt]: tuned percent:0.540
[09/26 10:07:39 visual_prompt]: Device used for model: 0
[09/26 10:07:39 visual_prompt]: Setting up Evaluator...
[09/26 10:07:39 visual_prompt]: Setting up Trainer...
[09/26 10:07:39 visual_prompt]: 	Setting up the optimizer...
[09/26 10:07:39 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:07:45 visual_prompt]: Epoch 1 / 100: avg data time: 5.58e-02, avg batch time: 0.5056, average train loss: 2.9740
[09/26 10:07:47 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1680, average loss: 2.9268
[09/26 10:07:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 10:07:47 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 10:07:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 10:07:54 visual_prompt]: Epoch 2 / 100: avg data time: 5.58e-02, avg batch time: 0.5028, average train loss: 7.7363
[09/26 10:07:55 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1687, average loss: 5.9079
[09/26 10:07:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 10:07:55 visual_prompt]: Best epoch 2: best metric: 0.205
[09/26 10:07:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 10:08:02 visual_prompt]: Epoch 3 / 100: avg data time: 5.68e-02, avg batch time: 0.5042, average train loss: 4.2067
[09/26 10:08:04 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1688, average loss: 2.7351
[09/26 10:08:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:08:04 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 10:08:11 visual_prompt]: Epoch 4 / 100: avg data time: 6.44e-02, avg batch time: 0.5121, average train loss: 2.3546
[09/26 10:08:12 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1685, average loss: 2.5192
[09/26 10:08:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 10:08:12 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 10:08:19 visual_prompt]: Epoch 5 / 100: avg data time: 6.06e-02, avg batch time: 0.5087, average train loss: 2.1563
[09/26 10:08:21 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1695, average loss: 2.7821
[09/26 10:08:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 10:08:21 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 10:08:27 visual_prompt]: Epoch 6 / 100: avg data time: 5.02e-02, avg batch time: 0.4997, average train loss: 2.2375
[09/26 10:08:29 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1691, average loss: 1.8636
[09/26 10:08:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:08:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 10:08:36 visual_prompt]: Epoch 7 / 100: avg data time: 6.12e-02, avg batch time: 0.5090, average train loss: 2.1220
[09/26 10:08:37 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 2.9606
[09/26 10:08:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 84.00	
[09/26 10:08:37 visual_prompt]: Best epoch 7: best metric: 0.260
[09/26 10:08:37 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 10:08:44 visual_prompt]: Epoch 8 / 100: avg data time: 5.75e-02, avg batch time: 0.5062, average train loss: 3.1159
[09/26 10:08:46 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1689, average loss: 3.5002
[09/26 10:08:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:08:46 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 10:08:52 visual_prompt]: Epoch 9 / 100: avg data time: 4.41e-02, avg batch time: 0.4923, average train loss: 6.2514
[09/26 10:08:54 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1691, average loss: 13.7426
[09/26 10:08:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 10:08:54 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 10:09:00 visual_prompt]: Epoch 10 / 100: avg data time: 4.49e-02, avg batch time: 0.4950, average train loss: 18.7292
[09/26 10:09:02 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 17.1854
[09/26 10:09:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 10:09:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 10:09:09 visual_prompt]: Epoch 11 / 100: avg data time: 4.99e-02, avg batch time: 0.4996, average train loss: 17.0410
[09/26 10:09:10 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1690, average loss: 3.8155
[09/26 10:09:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:09:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 10:09:17 visual_prompt]: Epoch 12 / 100: avg data time: 5.15e-02, avg batch time: 0.5004, average train loss: 12.9975
[09/26 10:09:19 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1693, average loss: 13.3833
[09/26 10:09:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 10:09:19 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 10:09:25 visual_prompt]: Epoch 13 / 100: avg data time: 5.96e-02, avg batch time: 0.5077, average train loss: 10.0486
[09/26 10:09:27 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 16.5530
[09/26 10:09:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 10:09:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 10:09:34 visual_prompt]: Epoch 14 / 100: avg data time: 5.00e-02, avg batch time: 0.5007, average train loss: 11.4889
[09/26 10:09:35 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1693, average loss: 8.2196
[09/26 10:09:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:09:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 10:09:42 visual_prompt]: Epoch 15 / 100: avg data time: 5.45e-02, avg batch time: 0.5023, average train loss: 21.0492
[09/26 10:09:44 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1690, average loss: 17.6122
[09/26 10:09:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 10:09:44 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 10:09:50 visual_prompt]: Epoch 16 / 100: avg data time: 5.61e-02, avg batch time: 0.5060, average train loss: 23.0401
[09/26 10:09:52 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1692, average loss: 23.3373
[09/26 10:09:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 84.00	
[09/26 10:09:52 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 10:09:59 visual_prompt]: Epoch 17 / 100: avg data time: 5.70e-02, avg batch time: 0.5053, average train loss: 12.3977
[09/26 10:10:00 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1693, average loss: 13.6715
[09/26 10:10:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 10:10:00 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 10:10:07 visual_prompt]: Epoch 18 / 100: avg data time: 5.58e-02, avg batch time: 0.5051, average train loss: 17.8175
[09/26 10:10:09 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1691, average loss: 17.9471
[09/26 10:10:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:10:09 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 10:10:15 visual_prompt]: Epoch 19 / 100: avg data time: 5.80e-02, avg batch time: 0.5067, average train loss: 11.1118
[09/26 10:10:17 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1690, average loss: 6.3249
[09/26 10:10:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:10:17 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 10:10:24 visual_prompt]: Epoch 20 / 100: avg data time: 4.65e-02, avg batch time: 0.4957, average train loss: 9.8838
[09/26 10:10:25 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1689, average loss: 9.4808
[09/26 10:10:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:10:25 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 10:10:32 visual_prompt]: Epoch 21 / 100: avg data time: 6.03e-02, avg batch time: 0.5086, average train loss: 12.3044
[09/26 10:10:34 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 5.2289
[09/26 10:10:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:10:34 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 10:10:40 visual_prompt]: Epoch 22 / 100: avg data time: 5.88e-02, avg batch time: 0.5079, average train loss: 10.5956
[09/26 10:10:42 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1688, average loss: 13.3455
[09/26 10:10:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:10:42 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 10:10:49 visual_prompt]: Epoch 23 / 100: avg data time: 5.46e-02, avg batch time: 0.5054, average train loss: 17.5652
[09/26 10:10:50 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 24.5853
[09/26 10:10:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 10:10:50 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 10:10:57 visual_prompt]: Epoch 24 / 100: avg data time: 4.66e-02, avg batch time: 0.4961, average train loss: 19.2847
[09/26 10:10:58 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 13.5123
[09/26 10:10:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 10:10:58 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 10:11:05 visual_prompt]: Epoch 25 / 100: avg data time: 5.45e-02, avg batch time: 0.5040, average train loss: 9.6034
[09/26 10:11:07 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1691, average loss: 4.0317
[09/26 10:11:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 10:11:07 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 10:11:13 visual_prompt]: Epoch 26 / 100: avg data time: 5.30e-02, avg batch time: 0.5031, average train loss: 14.4014
[09/26 10:11:15 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1697, average loss: 20.2718
[09/26 10:11:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 10:11:15 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 10:11:22 visual_prompt]: Epoch 27 / 100: avg data time: 5.45e-02, avg batch time: 0.5032, average train loss: 15.1413
[09/26 10:11:23 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1690, average loss: 11.6597
[09/26 10:11:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 10:11:23 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 10:11:30 visual_prompt]: Epoch 28 / 100: avg data time: 5.53e-02, avg batch time: 0.5041, average train loss: 10.2106
[09/26 10:11:32 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1697, average loss: 11.3365
[09/26 10:11:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:11:32 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 10:11:38 visual_prompt]: Epoch 29 / 100: avg data time: 5.46e-02, avg batch time: 0.5047, average train loss: 7.4226
[09/26 10:11:40 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1699, average loss: 5.2366
[09/26 10:11:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:11:40 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 10:11:47 visual_prompt]: Epoch 30 / 100: avg data time: 5.35e-02, avg batch time: 0.5046, average train loss: 8.0777
[09/26 10:11:48 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 11.2256
[09/26 10:11:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 10:11:48 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 10:11:55 visual_prompt]: Epoch 31 / 100: avg data time: 4.66e-02, avg batch time: 0.4969, average train loss: 11.6366
[09/26 10:11:56 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 18.4706
[09/26 10:11:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 85.00	
[09/26 10:11:56 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 10:12:03 visual_prompt]: Epoch 32 / 100: avg data time: 4.25e-02, avg batch time: 0.4909, average train loss: 10.7597
[09/26 10:12:05 visual_prompt]: Inference (val):avg data time: 4.62e-05, avg batch time: 0.1696, average loss: 11.4043
[09/26 10:12:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 10:12:05 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 10:12:11 visual_prompt]: Epoch 33 / 100: avg data time: 4.50e-02, avg batch time: 0.4960, average train loss: 9.8029
[09/26 10:12:13 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1688, average loss: 2.2985
[09/26 10:12:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 10:12:13 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 10:12:19 visual_prompt]: Epoch 34 / 100: avg data time: 4.51e-02, avg batch time: 0.4929, average train loss: 4.8451
[09/26 10:12:21 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1689, average loss: 11.2049
[09/26 10:12:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 10:12:21 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 10:12:28 visual_prompt]: Epoch 35 / 100: avg data time: 5.49e-02, avg batch time: 0.5043, average train loss: 6.0018
[09/26 10:12:29 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1693, average loss: 6.4674
[09/26 10:12:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 10:12:29 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 10:12:36 visual_prompt]: Epoch 36 / 100: avg data time: 6.04e-02, avg batch time: 0.5092, average train loss: 7.1247
[09/26 10:12:38 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 4.9202
[09/26 10:12:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:12:38 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 10:12:44 visual_prompt]: Epoch 37 / 100: avg data time: 4.22e-02, avg batch time: 0.4911, average train loss: 7.6157
[09/26 10:12:46 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1692, average loss: 5.4760
[09/26 10:12:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 10:12:46 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 10:12:53 visual_prompt]: Epoch 38 / 100: avg data time: 5.64e-02, avg batch time: 0.5046, average train loss: 8.1761
[09/26 10:12:54 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 8.7498
[09/26 10:12:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:12:54 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 10:13:01 visual_prompt]: Epoch 39 / 100: avg data time: 6.08e-02, avg batch time: 0.5090, average train loss: 6.7041
[09/26 10:13:03 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1689, average loss: 4.9265
[09/26 10:13:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 10:13:03 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 10:13:10 visual_prompt]: Epoch 40 / 100: avg data time: 5.43e-02, avg batch time: 0.5028, average train loss: 5.1574
[09/26 10:13:11 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1688, average loss: 2.8028
[09/26 10:13:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:13:11 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 10:13:18 visual_prompt]: Epoch 41 / 100: avg data time: 4.80e-02, avg batch time: 0.4972, average train loss: 3.6550
[09/26 10:13:19 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1690, average loss: 4.4521
[09/26 10:13:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:13:19 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 10:13:26 visual_prompt]: Epoch 42 / 100: avg data time: 5.46e-02, avg batch time: 0.5030, average train loss: 3.9645
[09/26 10:13:28 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 2.8325
[09/26 10:13:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 10:13:28 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 10:13:35 visual_prompt]: Epoch 43 / 100: avg data time: 5.35e-02, avg batch time: 0.5027, average train loss: 2.7351
[09/26 10:13:36 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1688, average loss: 2.1886
[09/26 10:13:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 10:13:36 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 10:13:43 visual_prompt]: Epoch 44 / 100: avg data time: 5.96e-02, avg batch time: 0.5068, average train loss: 1.9721
[09/26 10:13:45 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1683, average loss: 2.0444
[09/26 10:13:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 10:13:45 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 10:13:51 visual_prompt]: Epoch 45 / 100: avg data time: 6.15e-02, avg batch time: 0.5090, average train loss: 2.0755
[09/26 10:13:53 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1690, average loss: 2.0441
[09/26 10:13:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 10:13:53 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 10:14:00 visual_prompt]: Epoch 46 / 100: avg data time: 4.86e-02, avg batch time: 0.4980, average train loss: 2.0126
[09/26 10:14:01 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1696, average loss: 2.7611
[09/26 10:14:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:14:01 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 10:14:08 visual_prompt]: Epoch 47 / 100: avg data time: 5.79e-02, avg batch time: 0.5066, average train loss: 3.0955
[09/26 10:14:10 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 2.4879
[09/26 10:14:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 10:14:10 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 10:14:17 visual_prompt]: Epoch 48 / 100: avg data time: 5.68e-02, avg batch time: 0.5060, average train loss: 5.8344
[09/26 10:14:18 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1688, average loss: 6.8870
[09/26 10:14:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:14:18 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 10:14:25 visual_prompt]: Epoch 49 / 100: avg data time: 5.81e-02, avg batch time: 0.5060, average train loss: 5.2578
[09/26 10:14:26 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1689, average loss: 5.2891
[09/26 10:14:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 10:14:26 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 10:14:33 visual_prompt]: Epoch 50 / 100: avg data time: 5.60e-02, avg batch time: 0.5046, average train loss: 2.8862
[09/26 10:14:35 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1693, average loss: 2.5282
[09/26 10:14:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 10:14:35 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 10:14:42 visual_prompt]: Epoch 51 / 100: avg data time: 5.65e-02, avg batch time: 0.5065, average train loss: 2.3019
[09/26 10:14:43 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 2.4444
[09/26 10:14:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:14:43 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 10:14:50 visual_prompt]: Epoch 52 / 100: avg data time: 5.91e-02, avg batch time: 0.5081, average train loss: 2.3288
[09/26 10:14:51 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1694, average loss: 2.1955
[09/26 10:14:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:14:51 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 10:14:58 visual_prompt]: Epoch 53 / 100: avg data time: 4.56e-02, avg batch time: 0.4938, average train loss: 2.2703
[09/26 10:15:00 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1694, average loss: 1.9059
[09/26 10:15:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 10:15:00 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 10:15:06 visual_prompt]: Epoch 54 / 100: avg data time: 5.58e-02, avg batch time: 0.5038, average train loss: 2.0073
[09/26 10:15:08 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1684, average loss: 2.5123
[09/26 10:15:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 10:15:08 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 10:15:15 visual_prompt]: Epoch 55 / 100: avg data time: 4.80e-02, avg batch time: 0.4972, average train loss: 2.0619
[09/26 10:15:16 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1691, average loss: 1.9963
[09/26 10:15:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 10:15:16 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 10:15:23 visual_prompt]: Epoch 56 / 100: avg data time: 4.72e-02, avg batch time: 0.4963, average train loss: 1.9599
[09/26 10:15:24 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1685, average loss: 1.8996
[09/26 10:15:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:15:24 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 10:15:31 visual_prompt]: Epoch 57 / 100: avg data time: 5.59e-02, avg batch time: 0.5045, average train loss: 1.8990
[09/26 10:15:33 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1681, average loss: 1.9239
[09/26 10:15:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 10:15:33 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 10:15:40 visual_prompt]: Epoch 58 / 100: avg data time: 5.27e-02, avg batch time: 0.5013, average train loss: 1.9656
[09/26 10:15:41 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1689, average loss: 1.8304
[09/26 10:15:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 10:15:41 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 10:15:48 visual_prompt]: Epoch 59 / 100: avg data time: 5.34e-02, avg batch time: 0.5008, average train loss: 1.8684
[09/26 10:15:50 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1692, average loss: 2.2025
[09/26 10:15:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:15:50 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 10:15:56 visual_prompt]: Epoch 60 / 100: avg data time: 5.61e-02, avg batch time: 0.5035, average train loss: 1.8938
[09/26 10:15:58 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 1.9305
[09/26 10:15:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:15:58 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 10:16:05 visual_prompt]: Epoch 61 / 100: avg data time: 6.07e-02, avg batch time: 0.5091, average train loss: 1.8712
[09/26 10:16:06 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 1.8809
[09/26 10:16:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 10:16:06 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 10:16:13 visual_prompt]: Epoch 62 / 100: avg data time: 6.10e-02, avg batch time: 0.5086, average train loss: 1.8286
[09/26 10:16:15 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 1.9001
[09/26 10:16:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:16:15 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 10:16:22 visual_prompt]: Epoch 63 / 100: avg data time: 5.42e-02, avg batch time: 0.5030, average train loss: 1.8368
[09/26 10:16:23 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1689, average loss: 1.8832
[09/26 10:16:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:16:23 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 10:16:30 visual_prompt]: Epoch 64 / 100: avg data time: 4.33e-02, avg batch time: 0.4930, average train loss: 1.8664
[09/26 10:16:31 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 1.8191
[09/26 10:16:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.00	
[09/26 10:16:31 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 10:16:38 visual_prompt]: Epoch 65 / 100: avg data time: 6.45e-02, avg batch time: 0.5127, average train loss: 1.8899
[09/26 10:16:40 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1689, average loss: 1.8437
[09/26 10:16:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:16:40 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 10:16:47 visual_prompt]: Epoch 66 / 100: avg data time: 5.77e-02, avg batch time: 0.5052, average train loss: 1.8067
[09/26 10:16:48 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 1.9544
[09/26 10:16:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:16:48 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 10:16:55 visual_prompt]: Epoch 67 / 100: avg data time: 5.05e-02, avg batch time: 0.5007, average train loss: 1.8675
[09/26 10:16:56 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1688, average loss: 1.8697
[09/26 10:16:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:16:56 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 10:17:03 visual_prompt]: Epoch 68 / 100: avg data time: 4.45e-02, avg batch time: 0.4938, average train loss: 1.8579
[09/26 10:17:04 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1691, average loss: 1.9316
[09/26 10:17:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:17:04 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 10:17:11 visual_prompt]: Epoch 69 / 100: avg data time: 4.79e-02, avg batch time: 0.4989, average train loss: 1.8569
[09/26 10:17:13 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 1.9280
[09/26 10:17:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:17:13 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 10:17:20 visual_prompt]: Epoch 70 / 100: avg data time: 5.43e-02, avg batch time: 0.5039, average train loss: 1.8257
[09/26 10:17:21 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1691, average loss: 1.8405
[09/26 10:17:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 84.00	
[09/26 10:17:21 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 10:17:28 visual_prompt]: Epoch 71 / 100: avg data time: 4.29e-02, avg batch time: 0.4945, average train loss: 1.8299
[09/26 10:17:29 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1695, average loss: 1.9536
[09/26 10:17:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:17:29 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 10:17:36 visual_prompt]: Epoch 72 / 100: avg data time: 5.93e-02, avg batch time: 0.5077, average train loss: 1.8247
[09/26 10:17:38 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1692, average loss: 1.9568
[09/26 10:17:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 10:17:38 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 10:17:44 visual_prompt]: Epoch 73 / 100: avg data time: 5.00e-02, avg batch time: 0.4984, average train loss: 1.8070
[09/26 10:17:46 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 1.8435
[09/26 10:17:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 87.50	
[09/26 10:17:46 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 10:17:53 visual_prompt]: Epoch 74 / 100: avg data time: 5.19e-02, avg batch time: 0.5044, average train loss: 1.8303
[09/26 10:17:54 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1693, average loss: 2.1707
[09/26 10:17:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:17:54 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 10:18:01 visual_prompt]: Epoch 75 / 100: avg data time: 4.13e-02, avg batch time: 0.4920, average train loss: 1.8676
[09/26 10:18:02 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1694, average loss: 1.8594
[09/26 10:18:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:18:02 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 10:18:09 visual_prompt]: Epoch 76 / 100: avg data time: 4.22e-02, avg batch time: 0.4939, average train loss: 1.7652
[09/26 10:18:11 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1687, average loss: 1.7003
[09/26 10:18:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 90.50	
[09/26 10:18:11 visual_prompt]: Best epoch 76: best metric: 0.275
[09/26 10:18:11 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 10:18:17 visual_prompt]: Epoch 77 / 100: avg data time: 4.76e-02, avg batch time: 0.4981, average train loss: 1.7251
[09/26 10:18:19 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1695, average loss: 2.0020
[09/26 10:18:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 85.00	
[09/26 10:18:19 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 10:18:26 visual_prompt]: Epoch 78 / 100: avg data time: 5.97e-02, avg batch time: 0.5092, average train loss: 1.6739
[09/26 10:18:28 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1692, average loss: 1.7098
[09/26 10:18:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 92.50	
[09/26 10:18:28 visual_prompt]: Best epoch 78: best metric: 0.305
[09/26 10:18:28 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 10:18:35 visual_prompt]: Epoch 79 / 100: avg data time: 6.14e-02, avg batch time: 0.5104, average train loss: 1.6370
[09/26 10:18:36 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1692, average loss: 1.6712
[09/26 10:18:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 86.00	
[09/26 10:18:36 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 10:18:43 visual_prompt]: Epoch 80 / 100: avg data time: 5.75e-02, avg batch time: 0.5065, average train loss: 1.5255
[09/26 10:18:44 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1698, average loss: 1.8827
[09/26 10:18:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 89.00	
[09/26 10:18:44 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 10:18:51 visual_prompt]: Epoch 81 / 100: avg data time: 4.89e-02, avg batch time: 0.4984, average train loss: 1.5119
[09/26 10:18:53 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1693, average loss: 1.4197
[09/26 10:18:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 99.50	
[09/26 10:18:53 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 10:18:59 visual_prompt]: Epoch 82 / 100: avg data time: 5.41e-02, avg batch time: 0.5027, average train loss: 1.3930
[09/26 10:19:01 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1691, average loss: 1.4780
[09/26 10:19:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 99.00	
[09/26 10:19:01 visual_prompt]: Best epoch 82: best metric: 0.315
[09/26 10:19:01 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 10:19:08 visual_prompt]: Epoch 83 / 100: avg data time: 5.49e-02, avg batch time: 0.5039, average train loss: 1.3455
[09/26 10:19:09 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1693, average loss: 1.5124
[09/26 10:19:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.00	
[09/26 10:19:09 visual_prompt]: Best epoch 83: best metric: 0.350
[09/26 10:19:09 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 10:19:16 visual_prompt]: Epoch 84 / 100: avg data time: 5.80e-02, avg batch time: 0.5070, average train loss: 1.3885
[09/26 10:19:18 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1698, average loss: 1.5740
[09/26 10:19:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 95.00	
[09/26 10:19:18 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 10:19:24 visual_prompt]: Epoch 85 / 100: avg data time: 5.30e-02, avg batch time: 0.5025, average train loss: 1.3683
[09/26 10:19:26 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1692, average loss: 1.5533
[09/26 10:19:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 95.50	
[09/26 10:19:26 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 10:19:33 visual_prompt]: Epoch 86 / 100: avg data time: 5.07e-02, avg batch time: 0.5004, average train loss: 1.3705
[09/26 10:19:34 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 1.4001
[09/26 10:19:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 10:19:34 visual_prompt]: Best epoch 86: best metric: 0.360
[09/26 10:19:34 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 10:19:41 visual_prompt]: Epoch 87 / 100: avg data time: 6.42e-02, avg batch time: 0.5138, average train loss: 1.3205
[09/26 10:19:43 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1689, average loss: 1.3718
[09/26 10:19:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 10:19:43 visual_prompt]: Best epoch 87: best metric: 0.365
[09/26 10:19:43 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 10:19:50 visual_prompt]: Epoch 88 / 100: avg data time: 4.97e-02, avg batch time: 0.4983, average train loss: 1.2136
[09/26 10:19:51 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 1.4630
[09/26 10:19:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 10:19:51 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 10:19:58 visual_prompt]: Epoch 89 / 100: avg data time: 5.52e-02, avg batch time: 0.5056, average train loss: 1.1695
[09/26 10:19:59 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1689, average loss: 1.4027
[09/26 10:19:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 10:19:59 visual_prompt]: Best epoch 89: best metric: 0.385
[09/26 10:19:59 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 10:20:06 visual_prompt]: Epoch 90 / 100: avg data time: 5.80e-02, avg batch time: 0.5092, average train loss: 1.1375
[09/26 10:20:08 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1694, average loss: 1.4038
[09/26 10:20:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 10:20:08 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 10:20:14 visual_prompt]: Epoch 91 / 100: avg data time: 5.47e-02, avg batch time: 0.5034, average train loss: 1.1217
[09/26 10:20:16 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1703, average loss: 1.5115
[09/26 10:20:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.50	
[09/26 10:20:16 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 10:20:23 visual_prompt]: Epoch 92 / 100: avg data time: 5.53e-02, avg batch time: 0.5049, average train loss: 1.1458
[09/26 10:20:24 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1694, average loss: 1.3800
[09/26 10:20:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 10:20:24 visual_prompt]: Best epoch 92: best metric: 0.400
[09/26 10:20:24 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 10:20:31 visual_prompt]: Epoch 93 / 100: avg data time: 5.09e-02, avg batch time: 0.5004, average train loss: 1.0423
[09/26 10:20:32 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1693, average loss: 1.3747
[09/26 10:20:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 10:20:32 visual_prompt]: Best epoch 93: best metric: 0.405
[09/26 10:20:32 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 10:20:39 visual_prompt]: Epoch 94 / 100: avg data time: 5.12e-02, avg batch time: 0.5013, average train loss: 0.9622
[09/26 10:20:41 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1694, average loss: 1.4925
[09/26 10:20:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.50	
[09/26 10:20:41 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 10:20:48 visual_prompt]: Epoch 95 / 100: avg data time: 6.36e-02, avg batch time: 0.5128, average train loss: 0.9762
[09/26 10:20:49 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1692, average loss: 1.6040
[09/26 10:20:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 10:20:49 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 10:20:56 visual_prompt]: Epoch 96 / 100: avg data time: 5.34e-02, avg batch time: 0.5029, average train loss: 0.9390
[09/26 10:20:58 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1694, average loss: 1.5483
[09/26 10:20:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 10:20:58 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 10:21:05 visual_prompt]: Epoch 97 / 100: avg data time: 5.90e-02, avg batch time: 0.5099, average train loss: 0.9235
[09/26 10:21:06 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1686, average loss: 1.5804
[09/26 10:21:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/26 10:21:06 visual_prompt]: Best epoch 97: best metric: 0.410
[09/26 10:21:06 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 10:21:13 visual_prompt]: Epoch 98 / 100: avg data time: 5.74e-02, avg batch time: 0.5059, average train loss: 0.8740
[09/26 10:21:15 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 1.5393
[09/26 10:21:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.50	
[09/26 10:21:15 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 10:21:22 visual_prompt]: Epoch 99 / 100: avg data time: 5.89e-02, avg batch time: 0.5092, average train loss: 0.8541
[09/26 10:21:23 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1696, average loss: 1.5574
[09/26 10:21:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.50	
[09/26 10:21:23 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 10:21:30 visual_prompt]: Epoch 100 / 100: avg data time: 5.14e-02, avg batch time: 0.5008, average train loss: 0.8388
[09/26 10:21:31 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 1.5605
[09/26 10:21:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.50	
[09/26 10:21:31 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:21:31 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:21:31 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:21:31 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:21:31 visual_prompt]: Training with config:
[09/26 10:21:31 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:21:31 visual_prompt]: Loading training data...
[09/26 10:21:31 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 10:21:33 visual_prompt]: Number of images: 800
[09/26 10:21:33 visual_prompt]: Number of classes: 6 / 6
[09/26 10:21:33 visual_prompt]: Loading validation data...
[09/26 10:21:33 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 10:21:33 visual_prompt]: Number of images: 200
[09/26 10:21:33 visual_prompt]: Number of classes: 6 / 6
[09/26 10:21:33 visual_prompt]: Constructing models...
[09/26 10:21:36 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 10:21:36 visual_prompt]: tuned percent:0.540
[09/26 10:21:36 visual_prompt]: Device used for model: 0
[09/26 10:21:36 visual_prompt]: Setting up Evaluator...
[09/26 10:21:36 visual_prompt]: Setting up Trainer...
[09/26 10:21:36 visual_prompt]: 	Setting up the optimizer...
[09/26 10:21:36 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:21:43 visual_prompt]: Epoch 1 / 100: avg data time: 5.90e-02, avg batch time: 0.5088, average train loss: 2.9778
[09/26 10:21:44 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1682, average loss: 2.9268
[09/26 10:21:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 10:21:44 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 10:21:44 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 10:21:51 visual_prompt]: Epoch 2 / 100: avg data time: 5.47e-02, avg batch time: 0.5023, average train loss: 7.8504
[09/26 10:21:52 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1687, average loss: 5.9131
[09/26 10:21:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 10:21:52 visual_prompt]: Best epoch 2: best metric: 0.205
[09/26 10:21:52 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 10:21:59 visual_prompt]: Epoch 3 / 100: avg data time: 6.23e-02, avg batch time: 0.5096, average train loss: 3.5998
[09/26 10:22:01 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1689, average loss: 2.2189
[09/26 10:22:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:22:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 10:22:08 visual_prompt]: Epoch 4 / 100: avg data time: 4.20e-02, avg batch time: 0.4924, average train loss: 2.5684
[09/26 10:22:09 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.1686, average loss: 2.0066
[09/26 10:22:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 10:22:09 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 10:22:16 visual_prompt]: Epoch 5 / 100: avg data time: 7.24e-02, avg batch time: 0.5197, average train loss: 2.2893
[09/26 10:22:18 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 2.3343
[09/26 10:22:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 10:22:18 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 10:22:24 visual_prompt]: Epoch 6 / 100: avg data time: 5.21e-02, avg batch time: 0.5006, average train loss: 3.0478
[09/26 10:22:26 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1689, average loss: 3.6320
[09/26 10:22:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 10:22:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 10:22:33 visual_prompt]: Epoch 7 / 100: avg data time: 5.02e-02, avg batch time: 0.5003, average train loss: 4.1480
[09/26 10:22:34 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1692, average loss: 4.0704
[09/26 10:22:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:22:34 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 10:22:41 visual_prompt]: Epoch 8 / 100: avg data time: 6.50e-02, avg batch time: 0.5138, average train loss: 7.4107
[09/26 10:22:43 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.1694, average loss: 10.6887
[09/26 10:22:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 10:22:43 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 10:22:50 visual_prompt]: Epoch 9 / 100: avg data time: 6.69e-02, avg batch time: 0.5155, average train loss: 10.8926
[09/26 10:22:51 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1692, average loss: 12.7680
[09/26 10:22:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:22:51 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 10:22:58 visual_prompt]: Epoch 10 / 100: avg data time: 6.73e-02, avg batch time: 0.5150, average train loss: 13.7557
[09/26 10:23:00 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1692, average loss: 8.2256
[09/26 10:23:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:23:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 10:23:07 visual_prompt]: Epoch 11 / 100: avg data time: 5.27e-02, avg batch time: 0.5015, average train loss: 12.4331
[09/26 10:23:08 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1688, average loss: 13.1893
[09/26 10:23:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 10:23:08 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 10:23:15 visual_prompt]: Epoch 12 / 100: avg data time: 6.70e-02, avg batch time: 0.5152, average train loss: 12.2444
[09/26 10:23:17 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 8.2669
[09/26 10:23:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 10:23:17 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 10:23:24 visual_prompt]: Epoch 13 / 100: avg data time: 6.71e-02, avg batch time: 0.5150, average train loss: 10.3266
[09/26 10:23:25 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 7.7836
[09/26 10:23:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 10:23:25 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 10:23:32 visual_prompt]: Epoch 14 / 100: avg data time: 5.10e-02, avg batch time: 0.4999, average train loss: 12.4888
[09/26 10:23:33 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1693, average loss: 12.4827
[09/26 10:23:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 86.00	
[09/26 10:23:33 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 10:23:40 visual_prompt]: Epoch 15 / 100: avg data time: 4.62e-02, avg batch time: 0.4959, average train loss: 10.2916
[09/26 10:23:42 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1693, average loss: 8.4065
[09/26 10:23:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:23:42 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 10:23:48 visual_prompt]: Epoch 16 / 100: avg data time: 4.76e-02, avg batch time: 0.4973, average train loss: 6.9620
[09/26 10:23:50 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1690, average loss: 2.3934
[09/26 10:23:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 10:23:50 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 10:23:57 visual_prompt]: Epoch 17 / 100: avg data time: 4.31e-02, avg batch time: 0.4927, average train loss: 5.2492
[09/26 10:23:58 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1692, average loss: 4.4774
[09/26 10:23:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:23:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 10:24:05 visual_prompt]: Epoch 18 / 100: avg data time: 6.23e-02, avg batch time: 0.5103, average train loss: 5.1275
[09/26 10:24:07 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 4.1032
[09/26 10:24:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:24:07 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 10:24:13 visual_prompt]: Epoch 19 / 100: avg data time: 5.62e-02, avg batch time: 0.5044, average train loss: 4.6094
[09/26 10:24:15 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1695, average loss: 3.3785
[09/26 10:24:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 85.50	
[09/26 10:24:15 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 10:24:22 visual_prompt]: Epoch 20 / 100: avg data time: 5.93e-02, avg batch time: 0.5075, average train loss: 2.4518
[09/26 10:24:23 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1692, average loss: 2.4331
[09/26 10:24:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 90.00	
[09/26 10:24:23 visual_prompt]: Best epoch 20: best metric: 0.225
[09/26 10:24:23 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 10:24:30 visual_prompt]: Epoch 21 / 100: avg data time: 5.59e-02, avg batch time: 0.5061, average train loss: 1.9274
[09/26 10:24:32 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1692, average loss: 2.1497
[09/26 10:24:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 86.50	
[09/26 10:24:32 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 10:24:38 visual_prompt]: Epoch 22 / 100: avg data time: 4.14e-02, avg batch time: 0.4923, average train loss: 2.0632
[09/26 10:24:40 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1691, average loss: 2.2129
[09/26 10:24:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 84.00	
[09/26 10:24:40 visual_prompt]: Best epoch 22: best metric: 0.275
[09/26 10:24:40 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 10:24:47 visual_prompt]: Epoch 23 / 100: avg data time: 6.89e-02, avg batch time: 0.5175, average train loss: 1.8410
[09/26 10:24:49 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1693, average loss: 2.3299
[09/26 10:24:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 94.50	
[09/26 10:24:49 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 10:24:55 visual_prompt]: Epoch 24 / 100: avg data time: 5.22e-02, avg batch time: 0.5004, average train loss: 1.5141
[09/26 10:24:57 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 1.6414
[09/26 10:24:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 96.50	
[09/26 10:24:57 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 10:25:04 visual_prompt]: Epoch 25 / 100: avg data time: 5.04e-02, avg batch time: 0.4995, average train loss: 1.4042
[09/26 10:25:05 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1690, average loss: 1.9072
[09/26 10:25:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 97.50	
[09/26 10:25:05 visual_prompt]: Best epoch 25: best metric: 0.305
[09/26 10:25:05 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 10:25:12 visual_prompt]: Epoch 26 / 100: avg data time: 6.00e-02, avg batch time: 0.5082, average train loss: 1.7348
[09/26 10:25:14 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 1.4388
[09/26 10:25:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.50	
[09/26 10:25:14 visual_prompt]: Best epoch 26: best metric: 0.360
[09/26 10:25:14 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 10:25:20 visual_prompt]: Epoch 27 / 100: avg data time: 5.19e-02, avg batch time: 0.5027, average train loss: 1.3788
[09/26 10:25:22 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1689, average loss: 1.8531
[09/26 10:25:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 95.00	
[09/26 10:25:22 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 10:25:29 visual_prompt]: Epoch 28 / 100: avg data time: 4.43e-02, avg batch time: 0.4949, average train loss: 1.3386
[09/26 10:25:30 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.5593
[09/26 10:25:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 99.00	
[09/26 10:25:30 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 10:25:37 visual_prompt]: Epoch 29 / 100: avg data time: 6.57e-02, avg batch time: 0.5140, average train loss: 1.6372
[09/26 10:25:39 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1690, average loss: 2.1423
[09/26 10:25:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 10:25:39 visual_prompt]: Best epoch 29: best metric: 0.370
[09/26 10:25:39 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 10:25:46 visual_prompt]: Epoch 30 / 100: avg data time: 6.42e-02, avg batch time: 0.5131, average train loss: 1.6941
[09/26 10:25:47 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 2.0019
[09/26 10:25:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 84.00	
[09/26 10:25:47 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 10:25:54 visual_prompt]: Epoch 31 / 100: avg data time: 5.42e-02, avg batch time: 0.5034, average train loss: 1.5731
[09/26 10:25:55 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 2.0099
[09/26 10:25:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 10:25:55 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 10:26:02 visual_prompt]: Epoch 32 / 100: avg data time: 4.68e-02, avg batch time: 0.4969, average train loss: 1.3991
[09/26 10:26:04 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1694, average loss: 2.2480
[09/26 10:26:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 95.50	
[09/26 10:26:04 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 10:26:10 visual_prompt]: Epoch 33 / 100: avg data time: 5.11e-02, avg batch time: 0.5008, average train loss: 1.3011
[09/26 10:26:12 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1693, average loss: 1.4034
[09/26 10:26:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 10:26:12 visual_prompt]: Best epoch 33: best metric: 0.400
[09/26 10:26:12 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 10:26:19 visual_prompt]: Epoch 34 / 100: avg data time: 5.21e-02, avg batch time: 0.5016, average train loss: 1.1883
[09/26 10:26:20 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.1692, average loss: 1.8217
[09/26 10:26:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 10:26:20 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 10:26:27 visual_prompt]: Epoch 35 / 100: avg data time: 6.16e-02, avg batch time: 0.5109, average train loss: 1.2612
[09/26 10:26:29 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 1.9023
[09/26 10:26:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.50	
[09/26 10:26:29 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 10:26:36 visual_prompt]: Epoch 36 / 100: avg data time: 7.14e-02, avg batch time: 0.5196, average train loss: 1.2305
[09/26 10:26:37 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.1702, average loss: 1.9337
[09/26 10:26:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.00	
[09/26 10:26:37 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 10:26:44 visual_prompt]: Epoch 37 / 100: avg data time: 6.85e-02, avg batch time: 0.5167, average train loss: 1.1687
[09/26 10:26:46 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1688, average loss: 1.7682
[09/26 10:26:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 98.50	
[09/26 10:26:46 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 10:26:53 visual_prompt]: Epoch 38 / 100: avg data time: 5.03e-02, avg batch time: 0.5000, average train loss: 1.0349
[09/26 10:26:54 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 1.5365
[09/26 10:26:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 97.50	
[09/26 10:26:54 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 10:27:01 visual_prompt]: Epoch 39 / 100: avg data time: 6.13e-02, avg batch time: 0.5108, average train loss: 1.1691
[09/26 10:27:02 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 1.7478
[09/26 10:27:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 10:27:02 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 10:27:09 visual_prompt]: Epoch 40 / 100: avg data time: 5.34e-02, avg batch time: 0.5030, average train loss: 1.2603
[09/26 10:27:11 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 1.7538
[09/26 10:27:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 10:27:11 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 10:27:18 visual_prompt]: Epoch 41 / 100: avg data time: 6.98e-02, avg batch time: 0.5182, average train loss: 1.2994
[09/26 10:27:19 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1691, average loss: 1.8907
[09/26 10:27:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.00	
[09/26 10:27:19 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 10:27:26 visual_prompt]: Epoch 42 / 100: avg data time: 5.67e-02, avg batch time: 0.5063, average train loss: 1.1622
[09/26 10:27:28 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 2.2313
[09/26 10:27:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 96.00	
[09/26 10:27:28 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 10:27:34 visual_prompt]: Epoch 43 / 100: avg data time: 4.42e-02, avg batch time: 0.4958, average train loss: 1.2029
[09/26 10:27:36 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1692, average loss: 2.0720
[09/26 10:27:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 98.00	
[09/26 10:27:36 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 10:27:43 visual_prompt]: Epoch 44 / 100: avg data time: 5.47e-02, avg batch time: 0.5052, average train loss: 1.2023
[09/26 10:27:44 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 1.7398
[09/26 10:27:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 10:27:44 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 10:27:51 visual_prompt]: Epoch 45 / 100: avg data time: 4.92e-02, avg batch time: 0.4993, average train loss: 1.0040
[09/26 10:27:52 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1690, average loss: 1.8994
[09/26 10:27:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 10:27:52 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 10:27:59 visual_prompt]: Epoch 46 / 100: avg data time: 5.64e-02, avg batch time: 0.5050, average train loss: 1.0312
[09/26 10:28:01 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.1695, average loss: 1.9597
[09/26 10:28:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 96.50	
[09/26 10:28:01 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 10:28:08 visual_prompt]: Epoch 47 / 100: avg data time: 5.51e-02, avg batch time: 0.5057, average train loss: 1.0487
[09/26 10:28:09 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1692, average loss: 2.1533
[09/26 10:28:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.00	
[09/26 10:28:09 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 10:28:16 visual_prompt]: Epoch 48 / 100: avg data time: 6.66e-02, avg batch time: 0.5154, average train loss: 0.8865
[09/26 10:28:18 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1692, average loss: 1.5841
[09/26 10:28:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.50	
[09/26 10:28:18 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 10:28:25 visual_prompt]: Epoch 49 / 100: avg data time: 5.87e-02, avg batch time: 0.5084, average train loss: 0.8229
[09/26 10:28:26 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 1.8136
[09/26 10:28:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 98.50	
[09/26 10:28:26 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 10:28:33 visual_prompt]: Epoch 50 / 100: avg data time: 5.30e-02, avg batch time: 0.5024, average train loss: 0.8003
[09/26 10:28:34 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 1.9968
[09/26 10:28:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 98.50	
[09/26 10:28:34 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 10:28:41 visual_prompt]: Epoch 51 / 100: avg data time: 5.42e-02, avg batch time: 0.5035, average train loss: 0.9100
[09/26 10:28:43 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1689, average loss: 2.0372
[09/26 10:28:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 99.00	
[09/26 10:28:43 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 10:28:50 visual_prompt]: Epoch 52 / 100: avg data time: 5.21e-02, avg batch time: 0.5006, average train loss: 0.8940
[09/26 10:28:51 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 2.1233
[09/26 10:28:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 10:28:51 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 10:28:58 visual_prompt]: Epoch 53 / 100: avg data time: 5.53e-02, avg batch time: 0.5046, average train loss: 0.8596
[09/26 10:29:00 visual_prompt]: Inference (val):avg data time: 4.75e-05, avg batch time: 0.1693, average loss: 1.8950
[09/26 10:29:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 10:29:00 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 10:29:06 visual_prompt]: Epoch 54 / 100: avg data time: 5.52e-02, avg batch time: 0.5042, average train loss: 0.7229
[09/26 10:29:08 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1693, average loss: 1.8661
[09/26 10:29:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.00	
[09/26 10:29:08 visual_prompt]: Best epoch 54: best metric: 0.435
[09/26 10:29:08 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 10:29:15 visual_prompt]: Epoch 55 / 100: avg data time: 5.22e-02, avg batch time: 0.5016, average train loss: 0.6511
[09/26 10:29:16 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1693, average loss: 2.1280
[09/26 10:29:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 10:29:16 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 10:29:23 visual_prompt]: Epoch 56 / 100: avg data time: 6.00e-02, avg batch time: 0.5084, average train loss: 0.6348
[09/26 10:29:25 visual_prompt]: Inference (val):avg data time: 4.81e-05, avg batch time: 0.1694, average loss: 2.5305
[09/26 10:29:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 10:29:25 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 10:29:32 visual_prompt]: Epoch 57 / 100: avg data time: 5.64e-02, avg batch time: 0.5063, average train loss: 0.6070
[09/26 10:29:33 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1695, average loss: 2.2086
[09/26 10:29:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 10:29:33 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 10:29:40 visual_prompt]: Epoch 58 / 100: avg data time: 4.29e-02, avg batch time: 0.4936, average train loss: 0.5847
[09/26 10:29:42 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1696, average loss: 2.3124
[09/26 10:29:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 99.00	
[09/26 10:29:42 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 10:29:49 visual_prompt]: Epoch 59 / 100: avg data time: 6.07e-02, avg batch time: 0.5094, average train loss: 0.5754
[09/26 10:29:50 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1693, average loss: 2.4791
[09/26 10:29:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 99.50	
[09/26 10:29:50 visual_prompt]: Best epoch 59: best metric: 0.440
[09/26 10:29:50 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 10:29:57 visual_prompt]: Epoch 60 / 100: avg data time: 5.38e-02, avg batch time: 0.5022, average train loss: 0.5257
[09/26 10:29:59 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1694, average loss: 2.3617
[09/26 10:29:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 10:29:59 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 10:30:05 visual_prompt]: Epoch 61 / 100: avg data time: 4.20e-02, avg batch time: 0.4941, average train loss: 0.4381
[09/26 10:30:07 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1698, average loss: 2.8367
[09/26 10:30:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 10:30:07 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 10:30:14 visual_prompt]: Epoch 62 / 100: avg data time: 4.42e-02, avg batch time: 0.4938, average train loss: 0.4150
[09/26 10:30:15 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 3.3277
[09/26 10:30:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 10:30:15 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 10:30:22 visual_prompt]: Epoch 63 / 100: avg data time: 5.95e-02, avg batch time: 0.5099, average train loss: 0.3949
[09/26 10:30:24 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1695, average loss: 2.8184
[09/26 10:30:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 10:30:24 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 10:30:30 visual_prompt]: Epoch 64 / 100: avg data time: 5.11e-02, avg batch time: 0.5009, average train loss: 0.3380
[09/26 10:30:32 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 2.6473
[09/26 10:30:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 10:30:32 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 10:30:39 visual_prompt]: Epoch 65 / 100: avg data time: 4.34e-02, avg batch time: 0.4954, average train loss: 0.3453
[09/26 10:30:40 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 2.6607
[09/26 10:30:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 100.00	
[09/26 10:30:40 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 10:30:47 visual_prompt]: Epoch 66 / 100: avg data time: 5.42e-02, avg batch time: 0.5039, average train loss: 0.2975
[09/26 10:30:48 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1693, average loss: 2.9822
[09/26 10:30:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.00	
[09/26 10:30:48 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 10:30:55 visual_prompt]: Epoch 67 / 100: avg data time: 6.03e-02, avg batch time: 0.5094, average train loss: 0.2856
[09/26 10:30:57 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1696, average loss: 3.4323
[09/26 10:30:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 10:30:57 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 10:31:03 visual_prompt]: Epoch 68 / 100: avg data time: 4.58e-02, avg batch time: 0.4959, average train loss: 0.4382
[09/26 10:31:05 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1697, average loss: 2.3345
[09/26 10:31:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 10:31:05 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 10:31:12 visual_prompt]: Epoch 69 / 100: avg data time: 5.39e-02, avg batch time: 0.5035, average train loss: 0.3599
[09/26 10:31:13 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 2.4570
[09/26 10:31:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 100.00	
[09/26 10:31:13 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 10:31:20 visual_prompt]: Epoch 70 / 100: avg data time: 4.69e-02, avg batch time: 0.4959, average train loss: 0.2528
[09/26 10:31:22 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1691, average loss: 2.6484
[09/26 10:31:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/26 10:31:22 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 10:31:28 visual_prompt]: Epoch 71 / 100: avg data time: 4.57e-02, avg batch time: 0.4951, average train loss: 0.2168
[09/26 10:31:30 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1698, average loss: 3.0280
[09/26 10:31:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 100.00	
[09/26 10:31:30 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 10:31:37 visual_prompt]: Epoch 72 / 100: avg data time: 5.07e-02, avg batch time: 0.4999, average train loss: 0.1741
[09/26 10:31:38 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1691, average loss: 3.2743
[09/26 10:31:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.50	
[09/26 10:31:38 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 10:31:45 visual_prompt]: Epoch 73 / 100: avg data time: 4.29e-02, avg batch time: 0.4936, average train loss: 0.1717
[09/26 10:31:46 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1692, average loss: 3.8478
[09/26 10:31:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.50	
[09/26 10:31:46 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 10:31:53 visual_prompt]: Epoch 74 / 100: avg data time: 4.18e-02, avg batch time: 0.4916, average train loss: 0.2833
[09/26 10:31:54 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1695, average loss: 2.9666
[09/26 10:31:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 10:31:54 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 10:32:01 visual_prompt]: Epoch 75 / 100: avg data time: 5.48e-02, avg batch time: 0.5037, average train loss: 0.1934
[09/26 10:32:03 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1693, average loss: 2.9923
[09/26 10:32:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.50	
[09/26 10:32:03 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 10:32:10 visual_prompt]: Epoch 76 / 100: avg data time: 5.68e-02, avg batch time: 0.5064, average train loss: 0.1611
[09/26 10:32:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 3.4421
[09/26 10:32:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 10:32:11 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 10:32:18 visual_prompt]: Epoch 77 / 100: avg data time: 5.87e-02, avg batch time: 0.5079, average train loss: 0.1022
[09/26 10:32:20 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1696, average loss: 3.6521
[09/26 10:32:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 100.00	
[09/26 10:32:20 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 10:32:26 visual_prompt]: Epoch 78 / 100: avg data time: 5.78e-02, avg batch time: 0.5074, average train loss: 0.1054
[09/26 10:32:28 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 4.3621
[09/26 10:32:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 100.00	
[09/26 10:32:28 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 10:32:35 visual_prompt]: Epoch 79 / 100: avg data time: 6.78e-02, avg batch time: 0.5183, average train loss: 0.0659
[09/26 10:32:37 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1688, average loss: 4.7318
[09/26 10:32:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 100.00	
[09/26 10:32:37 visual_prompt]: Best epoch 79: best metric: 0.445
[09/26 10:32:37 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 10:32:44 visual_prompt]: Epoch 80 / 100: avg data time: 7.46e-02, avg batch time: 0.5232, average train loss: 0.0414
[09/26 10:32:45 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1698, average loss: 4.5899
[09/26 10:32:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 100.00	
[09/26 10:32:45 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 10:32:52 visual_prompt]: Epoch 81 / 100: avg data time: 5.15e-02, avg batch time: 0.5011, average train loss: 0.0420
[09/26 10:32:54 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1698, average loss: 5.6185
[09/26 10:32:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 100.00	
[09/26 10:32:54 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 10:33:01 visual_prompt]: Epoch 82 / 100: avg data time: 5.80e-02, avg batch time: 0.5079, average train loss: 0.0387
[09/26 10:33:02 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1696, average loss: 5.4981
[09/26 10:33:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 100.00	
[09/26 10:33:02 visual_prompt]: Best epoch 82: best metric: 0.460
[09/26 10:33:02 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 10:33:09 visual_prompt]: Epoch 83 / 100: avg data time: 6.31e-02, avg batch time: 0.5120, average train loss: 0.0357
[09/26 10:33:10 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1692, average loss: 5.3073
[09/26 10:33:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.50	
[09/26 10:33:10 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 10:33:18 visual_prompt]: Epoch 84 / 100: avg data time: 7.30e-02, avg batch time: 0.5215, average train loss: 0.0475
[09/26 10:33:19 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1693, average loss: 5.6166
[09/26 10:33:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 99.50	
[09/26 10:33:19 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 10:33:26 visual_prompt]: Epoch 85 / 100: avg data time: 5.53e-02, avg batch time: 0.5059, average train loss: 0.0491
[09/26 10:33:27 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 5.3329
[09/26 10:33:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.50	
[09/26 10:33:27 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 10:33:34 visual_prompt]: Epoch 86 / 100: avg data time: 5.43e-02, avg batch time: 0.5037, average train loss: 0.0391
[09/26 10:33:36 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 5.3492
[09/26 10:33:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/26 10:33:36 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 10:33:42 visual_prompt]: Epoch 87 / 100: avg data time: 5.54e-02, avg batch time: 0.5045, average train loss: 0.0313
[09/26 10:33:44 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1695, average loss: 5.0441
[09/26 10:33:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.50	
[09/26 10:33:44 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 10:33:51 visual_prompt]: Epoch 88 / 100: avg data time: 5.63e-02, avg batch time: 0.5057, average train loss: 0.0134
[09/26 10:33:52 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1694, average loss: 5.1637
[09/26 10:33:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.50	
[09/26 10:33:52 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 10:33:59 visual_prompt]: Epoch 89 / 100: avg data time: 6.07e-02, avg batch time: 0.5099, average train loss: 0.0084
[09/26 10:34:01 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1691, average loss: 5.4789
[09/26 10:34:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.50	
[09/26 10:34:01 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 10:34:07 visual_prompt]: Epoch 90 / 100: avg data time: 5.61e-02, avg batch time: 0.5051, average train loss: 0.0034
[09/26 10:34:09 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1696, average loss: 5.7274
[09/26 10:34:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 99.50	
[09/26 10:34:09 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 10:34:16 visual_prompt]: Epoch 91 / 100: avg data time: 5.87e-02, avg batch time: 0.5077, average train loss: 0.0034
[09/26 10:34:17 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1696, average loss: 5.8267
[09/26 10:34:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 99.50	
[09/26 10:34:17 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 10:34:24 visual_prompt]: Epoch 92 / 100: avg data time: 5.62e-02, avg batch time: 0.5052, average train loss: 0.0017
[09/26 10:34:26 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1689, average loss: 5.9142
[09/26 10:34:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 99.50	
[09/26 10:34:26 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 10:34:32 visual_prompt]: Epoch 93 / 100: avg data time: 4.36e-02, avg batch time: 0.4941, average train loss: 0.0024
[09/26 10:34:34 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1698, average loss: 5.9526
[09/26 10:34:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 99.50	
[09/26 10:34:34 visual_prompt]: Best epoch 93: best metric: 0.465
[09/26 10:34:34 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 10:34:41 visual_prompt]: Epoch 94 / 100: avg data time: 5.72e-02, avg batch time: 0.5062, average train loss: 0.0018
[09/26 10:34:42 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.1690, average loss: 5.9854
[09/26 10:34:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 99.50	
[09/26 10:34:42 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 10:34:49 visual_prompt]: Epoch 95 / 100: avg data time: 5.02e-02, avg batch time: 0.5009, average train loss: 0.0014
[09/26 10:34:51 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1695, average loss: 6.0083
[09/26 10:34:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 99.50	
[09/26 10:34:51 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 10:34:57 visual_prompt]: Epoch 96 / 100: avg data time: 4.64e-02, avg batch time: 0.4970, average train loss: 0.0015
[09/26 10:34:59 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 6.0188
[09/26 10:34:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.00	top5: 99.50	
[09/26 10:34:59 visual_prompt]: Best epoch 96: best metric: 0.470
[09/26 10:34:59 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 10:35:06 visual_prompt]: Epoch 97 / 100: avg data time: 5.02e-02, avg batch time: 0.4997, average train loss: 0.0017
[09/26 10:35:07 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1694, average loss: 6.0324
[09/26 10:35:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.50	
[09/26 10:35:07 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 10:35:14 visual_prompt]: Epoch 98 / 100: avg data time: 4.24e-02, avg batch time: 0.4942, average train loss: 0.0016
[09/26 10:35:15 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1695, average loss: 6.0396
[09/26 10:35:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.50	
[09/26 10:35:15 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 10:35:22 visual_prompt]: Epoch 99 / 100: avg data time: 5.46e-02, avg batch time: 0.5046, average train loss: 0.0010
[09/26 10:35:24 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 6.0421
[09/26 10:35:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.50	
[09/26 10:35:24 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 10:35:31 visual_prompt]: Epoch 100 / 100: avg data time: 5.40e-02, avg batch time: 0.5037, average train loss: 0.0012
[09/26 10:35:32 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1696, average loss: 6.0427
[09/26 10:35:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.50	
[09/26 10:35:32 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:35:32 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:35:32 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:35:32 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:35:32 visual_prompt]: Training with config:
[09/26 10:35:32 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:35:32 visual_prompt]: Loading training data...
[09/26 10:35:32 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 10:35:33 visual_prompt]: Number of images: 800
[09/26 10:35:33 visual_prompt]: Number of classes: 6 / 6
[09/26 10:35:33 visual_prompt]: Loading validation data...
[09/26 10:35:33 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 10:35:34 visual_prompt]: Number of images: 200
[09/26 10:35:34 visual_prompt]: Number of classes: 6 / 6
[09/26 10:35:34 visual_prompt]: Constructing models...
[09/26 10:35:36 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 10:35:36 visual_prompt]: tuned percent:0.540
[09/26 10:35:36 visual_prompt]: Device used for model: 0
[09/26 10:35:36 visual_prompt]: Setting up Evaluator...
[09/26 10:35:36 visual_prompt]: Setting up Trainer...
[09/26 10:35:36 visual_prompt]: 	Setting up the optimizer...
[09/26 10:35:36 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:35:43 visual_prompt]: Epoch 1 / 100: avg data time: 5.75e-02, avg batch time: 0.5049, average train loss: 2.9686
[09/26 10:35:45 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1684, average loss: 2.9268
[09/26 10:35:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 10:35:45 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 10:35:45 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 10:35:51 visual_prompt]: Epoch 2 / 100: avg data time: 4.49e-02, avg batch time: 0.4934, average train loss: 8.0271
[09/26 10:35:53 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1685, average loss: 6.2647
[09/26 10:35:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 10:35:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 10:36:00 visual_prompt]: Epoch 3 / 100: avg data time: 5.28e-02, avg batch time: 0.4996, average train loss: 5.2831
[09/26 10:36:01 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1686, average loss: 3.4547
[09/26 10:36:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 10:36:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 10:36:08 visual_prompt]: Epoch 4 / 100: avg data time: 5.46e-02, avg batch time: 0.5021, average train loss: 3.2530
[09/26 10:36:10 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1682, average loss: 2.3781
[09/26 10:36:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:36:10 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 10:36:17 visual_prompt]: Epoch 5 / 100: avg data time: 5.36e-02, avg batch time: 0.5013, average train loss: 2.3352
[09/26 10:36:18 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1694, average loss: 2.2903
[09/26 10:36:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 10:36:18 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 10:36:25 visual_prompt]: Epoch 6 / 100: avg data time: 6.31e-02, avg batch time: 0.5112, average train loss: 2.4163
[09/26 10:36:27 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1686, average loss: 2.6972
[09/26 10:36:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:36:27 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 10:36:34 visual_prompt]: Epoch 7 / 100: avg data time: 5.06e-02, avg batch time: 0.5003, average train loss: 4.0520
[09/26 10:36:35 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1690, average loss: 5.4491
[09/26 10:36:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 10:36:35 visual_prompt]: Best epoch 7: best metric: 0.205
[09/26 10:36:35 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 10:36:42 visual_prompt]: Epoch 8 / 100: avg data time: 5.97e-02, avg batch time: 0.5082, average train loss: 6.6782
[09/26 10:36:44 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 6.0525
[09/26 10:36:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:36:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 10:36:51 visual_prompt]: Epoch 9 / 100: avg data time: 5.42e-02, avg batch time: 0.5016, average train loss: 11.2929
[09/26 10:36:52 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1689, average loss: 9.6330
[09/26 10:36:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:36:52 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 10:36:59 visual_prompt]: Epoch 10 / 100: avg data time: 5.66e-02, avg batch time: 0.5048, average train loss: 11.3954
[09/26 10:37:01 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1687, average loss: 7.1714
[09/26 10:37:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:37:01 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 10:37:08 visual_prompt]: Epoch 11 / 100: avg data time: 6.90e-02, avg batch time: 0.5166, average train loss: 11.0850
[09/26 10:37:10 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1683, average loss: 8.0568
[09/26 10:37:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 10:37:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 10:37:17 visual_prompt]: Epoch 12 / 100: avg data time: 6.38e-02, avg batch time: 0.5121, average train loss: 11.4054
[09/26 10:37:18 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 14.4315
[09/26 10:37:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:37:18 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 10:37:25 visual_prompt]: Epoch 13 / 100: avg data time: 7.81e-02, avg batch time: 0.5256, average train loss: 9.7163
[09/26 10:37:27 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 9.1523
[09/26 10:37:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:37:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 10:37:34 visual_prompt]: Epoch 14 / 100: avg data time: 6.05e-02, avg batch time: 0.5081, average train loss: 9.8879
[09/26 10:37:35 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1686, average loss: 15.3638
[09/26 10:37:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:37:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 10:37:42 visual_prompt]: Epoch 15 / 100: avg data time: 5.73e-02, avg batch time: 0.5051, average train loss: 11.9249
[09/26 10:37:44 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1688, average loss: 11.8846
[09/26 10:37:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:37:44 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 10:37:51 visual_prompt]: Epoch 16 / 100: avg data time: 5.58e-02, avg batch time: 0.5043, average train loss: 7.8463
[09/26 10:37:52 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1691, average loss: 4.9364
[09/26 10:37:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:37:52 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 10:37:59 visual_prompt]: Epoch 17 / 100: avg data time: 6.54e-02, avg batch time: 0.5135, average train loss: 4.2827
[09/26 10:38:01 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 5.1451
[09/26 10:38:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:38:01 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 10:38:08 visual_prompt]: Epoch 18 / 100: avg data time: 6.34e-02, avg batch time: 0.5112, average train loss: 4.5023
[09/26 10:38:09 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1692, average loss: 4.5786
[09/26 10:38:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:38:09 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 10:38:16 visual_prompt]: Epoch 19 / 100: avg data time: 6.97e-02, avg batch time: 0.5175, average train loss: 3.7682
[09/26 10:38:18 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1688, average loss: 2.8508
[09/26 10:38:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 10:38:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 10:38:24 visual_prompt]: Epoch 20 / 100: avg data time: 5.56e-02, avg batch time: 0.5033, average train loss: 3.7906
[09/26 10:38:26 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1691, average loss: 5.5718
[09/26 10:38:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 91.00	
[09/26 10:38:26 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 10:38:33 visual_prompt]: Epoch 21 / 100: avg data time: 4.65e-02, avg batch time: 0.4974, average train loss: 3.6035
[09/26 10:38:34 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1688, average loss: 4.5582
[09/26 10:38:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:38:34 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 10:38:41 visual_prompt]: Epoch 22 / 100: avg data time: 4.55e-02, avg batch time: 0.4958, average train loss: 2.7579
[09/26 10:38:43 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1687, average loss: 2.2537
[09/26 10:38:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 84.00	
[09/26 10:38:43 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 10:38:49 visual_prompt]: Epoch 23 / 100: avg data time: 4.66e-02, avg batch time: 0.4969, average train loss: 2.3048
[09/26 10:38:51 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1689, average loss: 1.6308
[09/26 10:38:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 92.00	
[09/26 10:38:51 visual_prompt]: Best epoch 23: best metric: 0.275
[09/26 10:38:51 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 10:38:58 visual_prompt]: Epoch 24 / 100: avg data time: 5.47e-02, avg batch time: 0.5044, average train loss: 2.1046
[09/26 10:38:59 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1687, average loss: 1.8703
[09/26 10:38:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 94.00	
[09/26 10:38:59 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 10:39:06 visual_prompt]: Epoch 25 / 100: avg data time: 6.28e-02, avg batch time: 0.5118, average train loss: 1.6699
[09/26 10:39:08 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1690, average loss: 1.5475
[09/26 10:39:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 10:39:08 visual_prompt]: Best epoch 25: best metric: 0.345
[09/26 10:39:08 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 10:39:14 visual_prompt]: Epoch 26 / 100: avg data time: 4.94e-02, avg batch time: 0.5002, average train loss: 1.6424
[09/26 10:39:16 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1692, average loss: 1.9140
[09/26 10:39:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 97.50	
[09/26 10:39:16 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 10:39:23 visual_prompt]: Epoch 27 / 100: avg data time: 5.21e-02, avg batch time: 0.5013, average train loss: 1.5653
[09/26 10:39:24 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1690, average loss: 1.8248
[09/26 10:39:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 98.50	
[09/26 10:39:24 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 10:39:31 visual_prompt]: Epoch 28 / 100: avg data time: 6.18e-02, avg batch time: 0.5104, average train loss: 1.6074
[09/26 10:39:33 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1689, average loss: 1.9615
[09/26 10:39:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 88.00	
[09/26 10:39:33 visual_prompt]: Best epoch 28: best metric: 0.355
[09/26 10:39:33 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 10:39:40 visual_prompt]: Epoch 29 / 100: avg data time: 6.17e-02, avg batch time: 0.5123, average train loss: 1.4218
[09/26 10:39:41 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 1.8286
[09/26 10:39:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 93.00	
[09/26 10:39:41 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 10:39:48 visual_prompt]: Epoch 30 / 100: avg data time: 5.28e-02, avg batch time: 0.5030, average train loss: 1.4326
[09/26 10:39:50 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1691, average loss: 1.7034
[09/26 10:39:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 98.50	
[09/26 10:39:50 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 10:39:56 visual_prompt]: Epoch 31 / 100: avg data time: 4.70e-02, avg batch time: 0.4960, average train loss: 1.2834
[09/26 10:39:58 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1687, average loss: 1.7242
[09/26 10:39:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.50	
[09/26 10:39:58 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 10:40:05 visual_prompt]: Epoch 32 / 100: avg data time: 5.80e-02, avg batch time: 0.5062, average train loss: 1.2961
[09/26 10:40:06 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1689, average loss: 1.8346
[09/26 10:40:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 95.50	
[09/26 10:40:06 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 10:40:13 visual_prompt]: Epoch 33 / 100: avg data time: 5.84e-02, avg batch time: 0.5071, average train loss: 1.4865
[09/26 10:40:15 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1690, average loss: 1.8540
[09/26 10:40:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 95.50	
[09/26 10:40:15 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 10:40:22 visual_prompt]: Epoch 34 / 100: avg data time: 5.65e-02, avg batch time: 0.5063, average train loss: 1.3229
[09/26 10:40:23 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1690, average loss: 1.5805
[09/26 10:40:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 99.00	
[09/26 10:40:23 visual_prompt]: Best epoch 34: best metric: 0.385
[09/26 10:40:23 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 10:40:30 visual_prompt]: Epoch 35 / 100: avg data time: 6.00e-02, avg batch time: 0.5088, average train loss: 1.2759
[09/26 10:40:32 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1690, average loss: 1.5878
[09/26 10:40:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 10:40:32 visual_prompt]: Best epoch 35: best metric: 0.400
[09/26 10:40:32 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 10:40:38 visual_prompt]: Epoch 36 / 100: avg data time: 5.18e-02, avg batch time: 0.5016, average train loss: 1.2578
[09/26 10:40:40 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 2.1220
[09/26 10:40:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 98.00	
[09/26 10:40:40 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 10:40:47 visual_prompt]: Epoch 37 / 100: avg data time: 6.27e-02, avg batch time: 0.5115, average train loss: 1.3957
[09/26 10:40:49 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1692, average loss: 1.6407
[09/26 10:40:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 96.50	
[09/26 10:40:49 visual_prompt]: Best epoch 37: best metric: 0.425
[09/26 10:40:49 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 10:40:56 visual_prompt]: Epoch 38 / 100: avg data time: 5.57e-02, avg batch time: 0.5042, average train loss: 1.0638
[09/26 10:40:57 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1689, average loss: 1.4938
[09/26 10:40:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/26 10:40:57 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 10:41:04 visual_prompt]: Epoch 39 / 100: avg data time: 5.43e-02, avg batch time: 0.5048, average train loss: 0.9566
[09/26 10:41:05 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1691, average loss: 1.8132
[09/26 10:41:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.50	
[09/26 10:41:05 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 10:41:12 visual_prompt]: Epoch 40 / 100: avg data time: 5.74e-02, avg batch time: 0.5054, average train loss: 0.9334
[09/26 10:41:14 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 2.1109
[09/26 10:41:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.50	
[09/26 10:41:14 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 10:41:21 visual_prompt]: Epoch 41 / 100: avg data time: 5.89e-02, avg batch time: 0.5087, average train loss: 0.8979
[09/26 10:41:22 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1691, average loss: 1.7216
[09/26 10:41:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 10:41:22 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 10:41:29 visual_prompt]: Epoch 42 / 100: avg data time: 6.18e-02, avg batch time: 0.5110, average train loss: 0.8481
[09/26 10:41:31 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1688, average loss: 2.1328
[09/26 10:41:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 10:41:31 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 10:41:38 visual_prompt]: Epoch 43 / 100: avg data time: 6.16e-02, avg batch time: 0.5104, average train loss: 1.0552
[09/26 10:41:40 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1691, average loss: 2.4286
[09/26 10:41:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 99.00	
[09/26 10:41:40 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 10:41:46 visual_prompt]: Epoch 44 / 100: avg data time: 4.68e-02, avg batch time: 0.4966, average train loss: 1.4116
[09/26 10:41:48 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1690, average loss: 1.8085
[09/26 10:41:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 10:41:48 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 10:41:55 visual_prompt]: Epoch 45 / 100: avg data time: 6.22e-02, avg batch time: 0.5102, average train loss: 1.0357
[09/26 10:41:56 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1687, average loss: 1.8845
[09/26 10:41:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 10:41:56 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 10:42:03 visual_prompt]: Epoch 46 / 100: avg data time: 7.38e-02, avg batch time: 0.5218, average train loss: 0.8529
[09/26 10:42:05 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1687, average loss: 2.0420
[09/26 10:42:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 10:42:05 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 10:42:12 visual_prompt]: Epoch 47 / 100: avg data time: 5.43e-02, avg batch time: 0.5039, average train loss: 0.8298
[09/26 10:42:14 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1682, average loss: 2.0323
[09/26 10:42:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.00	
[09/26 10:42:14 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 10:42:20 visual_prompt]: Epoch 48 / 100: avg data time: 4.64e-02, avg batch time: 0.4970, average train loss: 0.7545
[09/26 10:42:22 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1690, average loss: 2.2498
[09/26 10:42:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 10:42:22 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 10:42:29 visual_prompt]: Epoch 49 / 100: avg data time: 5.95e-02, avg batch time: 0.5081, average train loss: 0.7172
[09/26 10:42:31 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1685, average loss: 2.0349
[09/26 10:42:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 10:42:31 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 10:42:38 visual_prompt]: Epoch 50 / 100: avg data time: 6.24e-02, avg batch time: 0.5118, average train loss: 0.5247
[09/26 10:42:39 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1691, average loss: 2.0239
[09/26 10:42:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.00	top5: 98.50	
[09/26 10:42:39 visual_prompt]: Best epoch 50: best metric: 0.470
[09/26 10:42:39 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 10:42:46 visual_prompt]: Epoch 51 / 100: avg data time: 7.28e-02, avg batch time: 0.5209, average train loss: 0.5029
[09/26 10:42:48 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1690, average loss: 2.3553
[09/26 10:42:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 10:42:48 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 10:42:55 visual_prompt]: Epoch 52 / 100: avg data time: 6.07e-02, avg batch time: 0.5089, average train loss: 0.5395
[09/26 10:42:56 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1688, average loss: 2.1316
[09/26 10:42:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 10:42:56 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 10:43:03 visual_prompt]: Epoch 53 / 100: avg data time: 6.25e-02, avg batch time: 0.5108, average train loss: 0.4175
[09/26 10:43:04 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1688, average loss: 2.2469
[09/26 10:43:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 98.00	
[09/26 10:43:04 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 10:43:11 visual_prompt]: Epoch 54 / 100: avg data time: 4.96e-02, avg batch time: 0.5003, average train loss: 0.4088
[09/26 10:43:13 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1688, average loss: 2.6533
[09/26 10:43:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 10:43:13 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 10:43:20 visual_prompt]: Epoch 55 / 100: avg data time: 6.44e-02, avg batch time: 0.5127, average train loss: 0.4460
[09/26 10:43:21 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1690, average loss: 2.6251
[09/26 10:43:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 10:43:21 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 10:43:28 visual_prompt]: Epoch 56 / 100: avg data time: 5.73e-02, avg batch time: 0.5060, average train loss: 0.4221
[09/26 10:43:30 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1689, average loss: 2.5982
[09/26 10:43:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.00	
[09/26 10:43:30 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 10:43:37 visual_prompt]: Epoch 57 / 100: avg data time: 5.63e-02, avg batch time: 0.5046, average train loss: 0.3555
[09/26 10:43:38 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1689, average loss: 2.8311
[09/26 10:43:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.00	
[09/26 10:43:38 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 10:43:45 visual_prompt]: Epoch 58 / 100: avg data time: 6.19e-02, avg batch time: 0.5103, average train loss: 0.4655
[09/26 10:43:46 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 3.0218
[09/26 10:43:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 10:43:46 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 10:43:53 visual_prompt]: Epoch 59 / 100: avg data time: 4.85e-02, avg batch time: 0.4999, average train loss: 0.3344
[09/26 10:43:55 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1688, average loss: 2.5501
[09/26 10:43:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 98.00	
[09/26 10:43:55 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 10:44:02 visual_prompt]: Epoch 60 / 100: avg data time: 5.52e-02, avg batch time: 0.5051, average train loss: 0.2578
[09/26 10:44:03 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1689, average loss: 3.1062
[09/26 10:44:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 10:44:03 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 10:44:10 visual_prompt]: Epoch 61 / 100: avg data time: 6.00e-02, avg batch time: 0.5092, average train loss: 0.2807
[09/26 10:44:12 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1690, average loss: 2.9842
[09/26 10:44:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 10:44:12 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 10:44:19 visual_prompt]: Epoch 62 / 100: avg data time: 7.52e-02, avg batch time: 0.5234, average train loss: 0.2361
[09/26 10:44:20 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 3.2869
[09/26 10:44:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 10:44:20 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 10:44:27 visual_prompt]: Epoch 63 / 100: avg data time: 6.23e-02, avg batch time: 0.5117, average train loss: 0.2572
[09/26 10:44:29 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1690, average loss: 3.4632
[09/26 10:44:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 10:44:29 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 10:44:36 visual_prompt]: Epoch 64 / 100: avg data time: 5.66e-02, avg batch time: 0.5048, average train loss: 0.2005
[09/26 10:44:38 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1690, average loss: 3.4017
[09/26 10:44:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 10:44:38 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 10:44:44 visual_prompt]: Epoch 65 / 100: avg data time: 5.95e-02, avg batch time: 0.5079, average train loss: 0.1738
[09/26 10:44:46 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1693, average loss: 3.6042
[09/26 10:44:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 10:44:46 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 10:44:53 visual_prompt]: Epoch 66 / 100: avg data time: 4.70e-02, avg batch time: 0.4963, average train loss: 0.0845
[09/26 10:44:54 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1688, average loss: 3.6725
[09/26 10:44:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 10:44:54 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 10:45:01 visual_prompt]: Epoch 67 / 100: avg data time: 7.17e-02, avg batch time: 0.5198, average train loss: 0.0835
[09/26 10:45:03 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1695, average loss: 4.2050
[09/26 10:45:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 98.00	
[09/26 10:45:03 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 10:45:10 visual_prompt]: Epoch 68 / 100: avg data time: 5.98e-02, avg batch time: 0.5091, average train loss: 0.0734
[09/26 10:45:11 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1690, average loss: 4.0999
[09/26 10:45:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 10:45:11 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 10:45:18 visual_prompt]: Epoch 69 / 100: avg data time: 6.44e-02, avg batch time: 0.5127, average train loss: 0.0860
[09/26 10:45:20 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1687, average loss: 4.6352
[09/26 10:45:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.50	
[09/26 10:45:20 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 10:45:27 visual_prompt]: Epoch 70 / 100: avg data time: 7.82e-02, avg batch time: 0.5263, average train loss: 0.0849
[09/26 10:45:29 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.1692, average loss: 4.8300
[09/26 10:45:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.50	
[09/26 10:45:29 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 10:45:36 visual_prompt]: Epoch 71 / 100: avg data time: 6.33e-02, avg batch time: 0.5117, average train loss: 0.0451
[09/26 10:45:37 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1688, average loss: 5.2571
[09/26 10:45:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.00	
[09/26 10:45:37 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 10:45:44 visual_prompt]: Epoch 72 / 100: avg data time: 4.92e-02, avg batch time: 0.4992, average train loss: 0.0697
[09/26 10:45:46 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1691, average loss: 4.5865
[09/26 10:45:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 10:45:46 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 10:45:52 visual_prompt]: Epoch 73 / 100: avg data time: 5.29e-02, avg batch time: 0.5013, average train loss: 0.0701
[09/26 10:45:54 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1694, average loss: 5.1125
[09/26 10:45:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.50	
[09/26 10:45:54 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 10:46:01 visual_prompt]: Epoch 74 / 100: avg data time: 5.53e-02, avg batch time: 0.5047, average train loss: 0.0995
[09/26 10:46:02 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1688, average loss: 5.1787
[09/26 10:46:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 10:46:02 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 10:46:09 visual_prompt]: Epoch 75 / 100: avg data time: 5.70e-02, avg batch time: 0.5054, average train loss: 0.1060
[09/26 10:46:11 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 3.9288
[09/26 10:46:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.50	
[09/26 10:46:11 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 10:46:17 visual_prompt]: Epoch 76 / 100: avg data time: 4.80e-02, avg batch time: 0.4989, average train loss: 0.0723
[09/26 10:46:19 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1688, average loss: 4.0552
[09/26 10:46:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 10:46:19 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 10:46:26 visual_prompt]: Epoch 77 / 100: avg data time: 4.56e-02, avg batch time: 0.4958, average train loss: 0.0526
[09/26 10:46:27 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1689, average loss: 4.8242
[09/26 10:46:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.50	
[09/26 10:46:27 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 10:46:34 visual_prompt]: Epoch 78 / 100: avg data time: 6.23e-02, avg batch time: 0.5111, average train loss: 0.0350
[09/26 10:46:36 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1689, average loss: 5.0392
[09/26 10:46:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 10:46:36 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 10:46:42 visual_prompt]: Epoch 79 / 100: avg data time: 4.24e-02, avg batch time: 0.4928, average train loss: 0.0191
[09/26 10:46:44 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 5.4077
[09/26 10:46:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 97.50	
[09/26 10:46:44 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 10:46:51 visual_prompt]: Epoch 80 / 100: avg data time: 5.02e-02, avg batch time: 0.4994, average train loss: 0.0142
[09/26 10:46:53 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1690, average loss: 5.9568
[09/26 10:46:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 10:46:53 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 10:46:59 visual_prompt]: Epoch 81 / 100: avg data time: 5.01e-02, avg batch time: 0.4992, average train loss: 0.0119
[09/26 10:47:01 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1689, average loss: 5.8654
[09/26 10:47:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 10:47:01 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 10:47:08 visual_prompt]: Epoch 82 / 100: avg data time: 6.53e-02, avg batch time: 0.5141, average train loss: 0.0127
[09/26 10:47:09 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1692, average loss: 5.8667
[09/26 10:47:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 10:47:09 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 10:47:16 visual_prompt]: Epoch 83 / 100: avg data time: 6.60e-02, avg batch time: 0.5142, average train loss: 0.0096
[09/26 10:47:18 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1688, average loss: 6.2753
[09/26 10:47:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 10:47:18 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 10:47:25 visual_prompt]: Epoch 84 / 100: avg data time: 6.60e-02, avg batch time: 0.5142, average train loss: 0.0119
[09/26 10:47:27 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1692, average loss: 6.2286
[09/26 10:47:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 10:47:27 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 10:47:33 visual_prompt]: Epoch 85 / 100: avg data time: 5.56e-02, avg batch time: 0.5056, average train loss: 0.0094
[09/26 10:47:35 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1693, average loss: 6.3300
[09/26 10:47:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 10:47:35 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 10:47:42 visual_prompt]: Epoch 86 / 100: avg data time: 5.18e-02, avg batch time: 0.5014, average train loss: 0.0094
[09/26 10:47:43 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1689, average loss: 6.3553
[09/26 10:47:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 10:47:43 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 10:47:51 visual_prompt]: Epoch 87 / 100: avg data time: 6.92e-02, avg batch time: 0.5176, average train loss: 0.0052
[09/26 10:47:52 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1687, average loss: 6.4684
[09/26 10:47:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 10:47:52 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 10:47:59 visual_prompt]: Epoch 88 / 100: avg data time: 7.17e-02, avg batch time: 0.5198, average train loss: 0.0138
[09/26 10:48:01 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1688, average loss: 6.5086
[09/26 10:48:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 10:48:01 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 10:48:08 visual_prompt]: Epoch 89 / 100: avg data time: 7.45e-02, avg batch time: 0.5226, average train loss: 0.0071
[09/26 10:48:10 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1691, average loss: 6.5217
[09/26 10:48:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 10:48:10 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 10:48:17 visual_prompt]: Epoch 90 / 100: avg data time: 6.20e-02, avg batch time: 0.5120, average train loss: 0.0076
[09/26 10:48:18 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1690, average loss: 6.4508
[09/26 10:48:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 10:48:18 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 10:48:25 visual_prompt]: Epoch 91 / 100: avg data time: 5.33e-02, avg batch time: 0.5023, average train loss: 0.0065
[09/26 10:48:26 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1690, average loss: 6.5091
[09/26 10:48:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 10:48:26 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 10:48:33 visual_prompt]: Epoch 92 / 100: avg data time: 5.99e-02, avg batch time: 0.5082, average train loss: 0.0042
[09/26 10:48:35 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1693, average loss: 6.5795
[09/26 10:48:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 10:48:35 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 10:48:42 visual_prompt]: Epoch 93 / 100: avg data time: 5.82e-02, avg batch time: 0.5066, average train loss: 0.0114
[09/26 10:48:43 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1689, average loss: 6.5129
[09/26 10:48:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 10:48:43 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 10:48:50 visual_prompt]: Epoch 94 / 100: avg data time: 5.88e-02, avg batch time: 0.5069, average train loss: 0.0052
[09/26 10:48:52 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1689, average loss: 6.4626
[09/26 10:48:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 10:48:52 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 10:48:58 visual_prompt]: Epoch 95 / 100: avg data time: 4.74e-02, avg batch time: 0.4968, average train loss: 0.0059
[09/26 10:49:00 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1687, average loss: 6.4868
[09/26 10:49:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 10:49:00 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 10:49:07 visual_prompt]: Epoch 96 / 100: avg data time: 5.06e-02, avg batch time: 0.4998, average train loss: 0.0079
[09/26 10:49:08 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1690, average loss: 6.4996
[09/26 10:49:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 10:49:08 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 10:49:15 visual_prompt]: Epoch 97 / 100: avg data time: 5.21e-02, avg batch time: 0.5012, average train loss: 0.0052
[09/26 10:49:17 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1687, average loss: 6.5018
[09/26 10:49:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 10:49:17 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 10:49:23 visual_prompt]: Epoch 98 / 100: avg data time: 5.32e-02, avg batch time: 0.5014, average train loss: 0.0027
[09/26 10:49:25 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1693, average loss: 6.5049
[09/26 10:49:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 10:49:25 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 10:49:32 visual_prompt]: Epoch 99 / 100: avg data time: 5.94e-02, avg batch time: 0.5087, average train loss: 0.0062
[09/26 10:49:34 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1688, average loss: 6.5058
[09/26 10:49:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 10:49:34 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 10:49:41 visual_prompt]: Epoch 100 / 100: avg data time: 6.02e-02, avg batch time: 0.5090, average train loss: 0.0062
[09/26 10:49:42 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1689, average loss: 6.5062
[09/26 10:49:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 10:49:42 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:49:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:49:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:49:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:49:42 visual_prompt]: Training with config:
[09/26 10:49:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:49:42 visual_prompt]: Loading training data...
[09/26 10:49:42 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 10:49:44 visual_prompt]: Number of images: 800
[09/26 10:49:44 visual_prompt]: Number of classes: 6 / 6
[09/26 10:49:44 visual_prompt]: Loading validation data...
[09/26 10:49:44 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 10:49:44 visual_prompt]: Number of images: 200
[09/26 10:49:44 visual_prompt]: Number of classes: 6 / 6
[09/26 10:49:44 visual_prompt]: Constructing models...
[09/26 10:49:46 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 10:49:46 visual_prompt]: tuned percent:0.540
[09/26 10:49:47 visual_prompt]: Device used for model: 0
[09/26 10:49:47 visual_prompt]: Setting up Evaluator...
[09/26 10:49:47 visual_prompt]: Setting up Trainer...
[09/26 10:49:47 visual_prompt]: 	Setting up the optimizer...
[09/26 10:49:47 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:49:53 visual_prompt]: Epoch 1 / 100: avg data time: 6.21e-02, avg batch time: 0.5104, average train loss: 2.9795
[09/26 10:49:55 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1679, average loss: 2.9268
[09/26 10:49:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 10:49:55 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 10:49:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 10:50:02 visual_prompt]: Epoch 2 / 100: avg data time: 5.58e-02, avg batch time: 0.5031, average train loss: 4.8259
[09/26 10:50:03 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1687, average loss: 4.1197
[09/26 10:50:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 10:50:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 10:50:10 visual_prompt]: Epoch 3 / 100: avg data time: 6.11e-02, avg batch time: 0.5082, average train loss: 2.5760
[09/26 10:50:12 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1682, average loss: 1.9023
[09/26 10:50:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:50:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 10:50:19 visual_prompt]: Epoch 4 / 100: avg data time: 4.70e-02, avg batch time: 0.4943, average train loss: 1.8826
[09/26 10:50:20 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1686, average loss: 1.8720
[09/26 10:50:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 10:50:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 10:50:27 visual_prompt]: Epoch 5 / 100: avg data time: 5.62e-02, avg batch time: 0.5054, average train loss: 1.8653
[09/26 10:50:28 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1690, average loss: 1.9096
[09/26 10:50:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:50:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 10:50:35 visual_prompt]: Epoch 6 / 100: avg data time: 6.11e-02, avg batch time: 0.5087, average train loss: 1.9165
[09/26 10:50:37 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1686, average loss: 1.8993
[09/26 10:50:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 10:50:37 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 10:50:44 visual_prompt]: Epoch 7 / 100: avg data time: 5.74e-02, avg batch time: 0.5055, average train loss: 1.9382
[09/26 10:50:45 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.1689, average loss: 3.7144
[09/26 10:50:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 10:50:45 visual_prompt]: Best epoch 7: best metric: 0.205
[09/26 10:50:45 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 10:50:52 visual_prompt]: Epoch 8 / 100: avg data time: 6.19e-02, avg batch time: 0.5102, average train loss: 2.6684
[09/26 10:50:54 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1686, average loss: 2.9213
[09/26 10:50:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 10:50:54 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 10:51:01 visual_prompt]: Epoch 9 / 100: avg data time: 4.67e-02, avg batch time: 0.4944, average train loss: 2.6646
[09/26 10:51:02 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1685, average loss: 3.6709
[09/26 10:51:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 10:51:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 10:51:09 visual_prompt]: Epoch 10 / 100: avg data time: 6.89e-02, avg batch time: 0.5157, average train loss: 4.9572
[09/26 10:51:11 visual_prompt]: Inference (val):avg data time: 5.76e-05, avg batch time: 0.1691, average loss: 6.1785
[09/26 10:51:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:51:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 10:51:18 visual_prompt]: Epoch 11 / 100: avg data time: 5.66e-02, avg batch time: 0.5040, average train loss: 4.3583
[09/26 10:51:19 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1685, average loss: 2.8378
[09/26 10:51:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 10:51:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 10:51:26 visual_prompt]: Epoch 12 / 100: avg data time: 5.23e-02, avg batch time: 0.4998, average train loss: 3.5377
[09/26 10:51:28 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 2.7324
[09/26 10:51:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:51:28 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 10:51:35 visual_prompt]: Epoch 13 / 100: avg data time: 5.46e-02, avg batch time: 0.5031, average train loss: 5.0661
[09/26 10:51:36 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1688, average loss: 5.7268
[09/26 10:51:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 10:51:36 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 10:51:43 visual_prompt]: Epoch 14 / 100: avg data time: 6.08e-02, avg batch time: 0.5093, average train loss: 6.1030
[09/26 10:51:45 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1689, average loss: 8.2079
[09/26 10:51:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:51:45 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 10:51:51 visual_prompt]: Epoch 15 / 100: avg data time: 5.18e-02, avg batch time: 0.5005, average train loss: 5.8181
[09/26 10:51:53 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1689, average loss: 7.1775
[09/26 10:51:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:51:53 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 10:52:00 visual_prompt]: Epoch 16 / 100: avg data time: 6.24e-02, avg batch time: 0.5114, average train loss: 4.9257
[09/26 10:52:01 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1690, average loss: 5.4211
[09/26 10:52:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 10:52:01 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 10:52:08 visual_prompt]: Epoch 17 / 100: avg data time: 4.62e-02, avg batch time: 0.4942, average train loss: 6.1082
[09/26 10:52:10 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1687, average loss: 10.2551
[09/26 10:52:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 10:52:10 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 10:52:16 visual_prompt]: Epoch 18 / 100: avg data time: 5.84e-02, avg batch time: 0.5064, average train loss: 9.4515
[09/26 10:52:18 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1687, average loss: 8.3834
[09/26 10:52:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:52:18 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 10:52:25 visual_prompt]: Epoch 19 / 100: avg data time: 6.33e-02, avg batch time: 0.5120, average train loss: 7.3585
[09/26 10:52:26 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1688, average loss: 6.1199
[09/26 10:52:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:52:26 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 10:52:33 visual_prompt]: Epoch 20 / 100: avg data time: 4.75e-02, avg batch time: 0.4955, average train loss: 6.7652
[09/26 10:52:35 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1687, average loss: 7.7480
[09/26 10:52:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 10:52:35 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 10:52:42 visual_prompt]: Epoch 21 / 100: avg data time: 4.90e-02, avg batch time: 0.4970, average train loss: 6.1791
[09/26 10:52:43 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1690, average loss: 5.6684
[09/26 10:52:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:52:43 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 10:52:50 visual_prompt]: Epoch 22 / 100: avg data time: 6.65e-02, avg batch time: 0.5160, average train loss: 8.3161
[09/26 10:52:52 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1690, average loss: 5.5131
[09/26 10:52:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:52:52 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 10:52:59 visual_prompt]: Epoch 23 / 100: avg data time: 6.38e-02, avg batch time: 0.5118, average train loss: 5.8227
[09/26 10:53:00 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1688, average loss: 5.0465
[09/26 10:53:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:53:00 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 10:53:07 visual_prompt]: Epoch 24 / 100: avg data time: 5.23e-02, avg batch time: 0.4998, average train loss: 4.3850
[09/26 10:53:09 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1686, average loss: 3.4687
[09/26 10:53:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 10:53:09 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 10:53:16 visual_prompt]: Epoch 25 / 100: avg data time: 5.79e-02, avg batch time: 0.5060, average train loss: 3.3371
[09/26 10:53:17 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1684, average loss: 3.8827
[09/26 10:53:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:53:17 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 10:53:24 visual_prompt]: Epoch 26 / 100: avg data time: 6.13e-02, avg batch time: 0.5094, average train loss: 8.1514
[09/26 10:53:26 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1688, average loss: 7.2825
[09/26 10:53:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 10:53:26 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 10:53:33 visual_prompt]: Epoch 27 / 100: avg data time: 5.02e-02, avg batch time: 0.4983, average train loss: 5.8283
[09/26 10:53:34 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1689, average loss: 4.0252
[09/26 10:53:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:53:34 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 10:53:41 visual_prompt]: Epoch 28 / 100: avg data time: 6.12e-02, avg batch time: 0.5101, average train loss: 5.3386
[09/26 10:53:43 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1683, average loss: 7.0837
[09/26 10:53:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:53:43 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 10:53:50 visual_prompt]: Epoch 29 / 100: avg data time: 5.60e-02, avg batch time: 0.5045, average train loss: 3.6060
[09/26 10:53:51 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1688, average loss: 3.6061
[09/26 10:53:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:53:51 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 10:53:58 visual_prompt]: Epoch 30 / 100: avg data time: 6.29e-02, avg batch time: 0.5103, average train loss: 7.1280
[09/26 10:54:00 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1685, average loss: 8.0293
[09/26 10:54:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 10:54:00 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 10:54:07 visual_prompt]: Epoch 31 / 100: avg data time: 6.71e-02, avg batch time: 0.5138, average train loss: 8.0171
[09/26 10:54:08 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1689, average loss: 5.1949
[09/26 10:54:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:54:08 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 10:54:15 visual_prompt]: Epoch 32 / 100: avg data time: 5.41e-02, avg batch time: 0.5027, average train loss: 7.1884
[09/26 10:54:17 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1690, average loss: 5.8142
[09/26 10:54:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 10:54:17 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 10:54:24 visual_prompt]: Epoch 33 / 100: avg data time: 6.14e-02, avg batch time: 0.5085, average train loss: 5.1099
[09/26 10:54:25 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 5.3124
[09/26 10:54:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 10:54:25 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 10:54:32 visual_prompt]: Epoch 34 / 100: avg data time: 6.15e-02, avg batch time: 0.5093, average train loss: 5.0525
[09/26 10:54:34 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1692, average loss: 5.2821
[09/26 10:54:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 10:54:34 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 10:54:41 visual_prompt]: Epoch 35 / 100: avg data time: 7.57e-02, avg batch time: 0.5235, average train loss: 4.1471
[09/26 10:54:42 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1692, average loss: 2.9059
[09/26 10:54:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 10:54:42 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 10:54:49 visual_prompt]: Epoch 36 / 100: avg data time: 4.74e-02, avg batch time: 0.4966, average train loss: 3.6319
[09/26 10:54:51 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1687, average loss: 2.4165
[09/26 10:54:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 10:54:51 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 10:54:58 visual_prompt]: Epoch 37 / 100: avg data time: 5.20e-02, avg batch time: 0.5011, average train loss: 2.6890
[09/26 10:54:59 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1687, average loss: 2.0305
[09/26 10:54:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 10:54:59 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 10:55:06 visual_prompt]: Epoch 38 / 100: avg data time: 5.64e-02, avg batch time: 0.5056, average train loss: 2.3811
[09/26 10:55:07 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1686, average loss: 2.8958
[09/26 10:55:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:55:07 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 10:55:14 visual_prompt]: Epoch 39 / 100: avg data time: 5.47e-02, avg batch time: 0.5029, average train loss: 3.2769
[09/26 10:55:16 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1688, average loss: 3.1889
[09/26 10:55:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 10:55:16 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 10:55:23 visual_prompt]: Epoch 40 / 100: avg data time: 5.88e-02, avg batch time: 0.5080, average train loss: 3.4971
[09/26 10:55:24 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 4.0734
[09/26 10:55:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 10:55:24 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 10:55:31 visual_prompt]: Epoch 41 / 100: avg data time: 4.84e-02, avg batch time: 0.4989, average train loss: 3.6134
[09/26 10:55:32 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 2.5800
[09/26 10:55:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 10:55:32 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 10:55:39 visual_prompt]: Epoch 42 / 100: avg data time: 4.54e-02, avg batch time: 0.4942, average train loss: 2.4195
[09/26 10:55:41 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1685, average loss: 2.4193
[09/26 10:55:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:55:41 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 10:55:48 visual_prompt]: Epoch 43 / 100: avg data time: 6.81e-02, avg batch time: 0.5155, average train loss: 4.2618
[09/26 10:55:49 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1689, average loss: 4.7431
[09/26 10:55:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:55:49 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 10:55:56 visual_prompt]: Epoch 44 / 100: avg data time: 5.93e-02, avg batch time: 0.5085, average train loss: 3.2203
[09/26 10:55:58 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1687, average loss: 3.7994
[09/26 10:55:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:55:58 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 10:56:05 visual_prompt]: Epoch 45 / 100: avg data time: 6.17e-02, avg batch time: 0.5092, average train loss: 4.3704
[09/26 10:56:06 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1692, average loss: 2.7529
[09/26 10:56:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 10:56:06 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 10:56:13 visual_prompt]: Epoch 46 / 100: avg data time: 5.90e-02, avg batch time: 0.5068, average train loss: 2.4320
[09/26 10:56:14 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1689, average loss: 2.2240
[09/26 10:56:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:56:14 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 10:56:21 visual_prompt]: Epoch 47 / 100: avg data time: 5.46e-02, avg batch time: 0.5023, average train loss: 2.0490
[09/26 10:56:23 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1687, average loss: 2.4557
[09/26 10:56:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 10:56:23 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 10:56:30 visual_prompt]: Epoch 48 / 100: avg data time: 5.23e-02, avg batch time: 0.5005, average train loss: 2.1535
[09/26 10:56:31 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1688, average loss: 2.2645
[09/26 10:56:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 84.00	
[09/26 10:56:31 visual_prompt]: Best epoch 48: best metric: 0.220
[09/26 10:56:31 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 10:56:38 visual_prompt]: Epoch 49 / 100: avg data time: 5.32e-02, avg batch time: 0.5023, average train loss: 4.7926
[09/26 10:56:40 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1689, average loss: 2.9865
[09/26 10:56:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:56:40 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 10:56:47 visual_prompt]: Epoch 50 / 100: avg data time: 6.03e-02, avg batch time: 0.5077, average train loss: 2.2112
[09/26 10:56:48 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1685, average loss: 1.9574
[09/26 10:56:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 84.00	
[09/26 10:56:48 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 10:56:55 visual_prompt]: Epoch 51 / 100: avg data time: 5.03e-02, avg batch time: 0.4979, average train loss: 2.4650
[09/26 10:56:56 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1690, average loss: 3.6109
[09/26 10:56:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 10:56:56 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 10:57:03 visual_prompt]: Epoch 52 / 100: avg data time: 4.41e-02, avg batch time: 0.4927, average train loss: 2.8335
[09/26 10:57:05 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1689, average loss: 2.4710
[09/26 10:57:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 10:57:05 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 10:57:12 visual_prompt]: Epoch 53 / 100: avg data time: 5.62e-02, avg batch time: 0.5042, average train loss: 2.2040
[09/26 10:57:13 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1690, average loss: 2.7373
[09/26 10:57:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 10:57:13 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 10:57:20 visual_prompt]: Epoch 54 / 100: avg data time: 6.14e-02, avg batch time: 0.5089, average train loss: 1.9793
[09/26 10:57:22 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1691, average loss: 1.8437
[09/26 10:57:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 84.00	
[09/26 10:57:22 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 10:57:29 visual_prompt]: Epoch 55 / 100: avg data time: 6.55e-02, avg batch time: 0.5131, average train loss: 1.9139
[09/26 10:57:30 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1690, average loss: 1.8506
[09/26 10:57:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:57:30 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 10:57:37 visual_prompt]: Epoch 56 / 100: avg data time: 4.74e-02, avg batch time: 0.4974, average train loss: 1.9267
[09/26 10:57:39 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1690, average loss: 2.1591
[09/26 10:57:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 83.50	
[09/26 10:57:39 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 10:57:45 visual_prompt]: Epoch 57 / 100: avg data time: 5.89e-02, avg batch time: 0.5073, average train loss: 2.3485
[09/26 10:57:47 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1689, average loss: 2.1188
[09/26 10:57:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:57:47 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 10:57:54 visual_prompt]: Epoch 58 / 100: avg data time: 6.01e-02, avg batch time: 0.5086, average train loss: 1.9390
[09/26 10:57:55 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1689, average loss: 1.9881
[09/26 10:57:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:57:55 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 10:58:02 visual_prompt]: Epoch 59 / 100: avg data time: 5.52e-02, avg batch time: 0.5034, average train loss: 1.9019
[09/26 10:58:04 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1692, average loss: 1.8888
[09/26 10:58:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 10:58:04 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 10:58:11 visual_prompt]: Epoch 60 / 100: avg data time: 5.17e-02, avg batch time: 0.5000, average train loss: 1.8936
[09/26 10:58:12 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1688, average loss: 1.9271
[09/26 10:58:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:58:12 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 10:58:19 visual_prompt]: Epoch 61 / 100: avg data time: 4.17e-02, avg batch time: 0.4913, average train loss: 2.0714
[09/26 10:58:20 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1690, average loss: 1.8617
[09/26 10:58:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 10:58:20 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 10:58:27 visual_prompt]: Epoch 62 / 100: avg data time: 6.10e-02, avg batch time: 0.5092, average train loss: 1.8810
[09/26 10:58:29 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1688, average loss: 2.0850
[09/26 10:58:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 10:58:29 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 10:58:36 visual_prompt]: Epoch 63 / 100: avg data time: 5.64e-02, avg batch time: 0.5040, average train loss: 1.8332
[09/26 10:58:37 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1690, average loss: 1.9203
[09/26 10:58:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 10:58:37 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 10:58:44 visual_prompt]: Epoch 64 / 100: avg data time: 4.73e-02, avg batch time: 0.4970, average train loss: 1.8888
[09/26 10:58:46 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1687, average loss: 1.8972
[09/26 10:58:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 10:58:46 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 10:58:53 visual_prompt]: Epoch 65 / 100: avg data time: 6.32e-02, avg batch time: 0.5104, average train loss: 1.8250
[09/26 10:58:54 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1686, average loss: 1.8802
[09/26 10:58:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:58:54 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 10:59:01 visual_prompt]: Epoch 66 / 100: avg data time: 5.40e-02, avg batch time: 0.5026, average train loss: 1.8127
[09/26 10:59:03 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.1693, average loss: 1.8790
[09/26 10:59:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 10:59:03 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 10:59:09 visual_prompt]: Epoch 67 / 100: avg data time: 5.10e-02, avg batch time: 0.4986, average train loss: 1.8045
[09/26 10:59:11 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1689, average loss: 1.9579
[09/26 10:59:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 10:59:11 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 10:59:18 visual_prompt]: Epoch 68 / 100: avg data time: 6.17e-02, avg batch time: 0.5129, average train loss: 1.8499
[09/26 10:59:20 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1687, average loss: 1.8596
[09/26 10:59:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 10:59:20 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 10:59:27 visual_prompt]: Epoch 69 / 100: avg data time: 6.49e-02, avg batch time: 0.5140, average train loss: 1.8274
[09/26 10:59:28 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1690, average loss: 1.8050
[09/26 10:59:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 10:59:28 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 10:59:35 visual_prompt]: Epoch 70 / 100: avg data time: 6.66e-02, avg batch time: 0.5156, average train loss: 1.7976
[09/26 10:59:37 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1689, average loss: 1.9589
[09/26 10:59:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 10:59:37 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 10:59:43 visual_prompt]: Epoch 71 / 100: avg data time: 6.11e-02, avg batch time: 0.5100, average train loss: 1.8182
[09/26 10:59:45 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1692, average loss: 1.8402
[09/26 10:59:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 81.50	
[09/26 10:59:45 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 10:59:52 visual_prompt]: Epoch 72 / 100: avg data time: 6.62e-02, avg batch time: 0.5142, average train loss: 1.8072
[09/26 10:59:54 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1695, average loss: 1.8155
[09/26 10:59:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 10:59:54 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 11:00:01 visual_prompt]: Epoch 73 / 100: avg data time: 7.62e-02, avg batch time: 0.5243, average train loss: 1.7840
[09/26 11:00:02 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1690, average loss: 2.0787
[09/26 11:00:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:00:02 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 11:00:09 visual_prompt]: Epoch 74 / 100: avg data time: 5.12e-02, avg batch time: 0.4995, average train loss: 1.8221
[09/26 11:00:11 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1688, average loss: 1.8647
[09/26 11:00:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 11:00:11 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 11:00:17 visual_prompt]: Epoch 75 / 100: avg data time: 5.65e-02, avg batch time: 0.5059, average train loss: 1.7869
[09/26 11:00:19 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1689, average loss: 1.8539
[09/26 11:00:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:00:19 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 11:00:26 visual_prompt]: Epoch 76 / 100: avg data time: 4.73e-02, avg batch time: 0.4965, average train loss: 1.8185
[09/26 11:00:27 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1687, average loss: 1.8503
[09/26 11:00:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:00:27 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 11:00:34 visual_prompt]: Epoch 77 / 100: avg data time: 5.36e-02, avg batch time: 0.5025, average train loss: 1.8189
[09/26 11:00:36 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1687, average loss: 1.8426
[09/26 11:00:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:00:36 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 11:00:42 visual_prompt]: Epoch 78 / 100: avg data time: 5.31e-02, avg batch time: 0.5021, average train loss: 1.7935
[09/26 11:00:44 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1691, average loss: 1.8408
[09/26 11:00:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:00:44 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 11:00:51 visual_prompt]: Epoch 79 / 100: avg data time: 4.79e-02, avg batch time: 0.4966, average train loss: 1.7877
[09/26 11:00:52 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1690, average loss: 1.8114
[09/26 11:00:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:00:52 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 11:00:59 visual_prompt]: Epoch 80 / 100: avg data time: 6.91e-02, avg batch time: 0.5171, average train loss: 1.7940
[09/26 11:01:01 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1687, average loss: 1.8350
[09/26 11:01:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:01:01 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 11:01:08 visual_prompt]: Epoch 81 / 100: avg data time: 7.28e-02, avg batch time: 0.5207, average train loss: 1.7873
[09/26 11:01:10 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1692, average loss: 1.8240
[09/26 11:01:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:01:10 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 11:01:17 visual_prompt]: Epoch 82 / 100: avg data time: 5.24e-02, avg batch time: 0.5027, average train loss: 1.7893
[09/26 11:01:18 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1687, average loss: 1.8126
[09/26 11:01:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:01:18 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 11:01:25 visual_prompt]: Epoch 83 / 100: avg data time: 7.22e-02, avg batch time: 0.5201, average train loss: 1.7783
[09/26 11:01:27 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1687, average loss: 1.8955
[09/26 11:01:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:01:27 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 11:01:34 visual_prompt]: Epoch 84 / 100: avg data time: 5.41e-02, avg batch time: 0.5025, average train loss: 1.7943
[09/26 11:01:35 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1691, average loss: 1.8489
[09/26 11:01:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:01:35 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 11:01:42 visual_prompt]: Epoch 85 / 100: avg data time: 6.19e-02, avg batch time: 0.5105, average train loss: 1.7756
[09/26 11:01:44 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1692, average loss: 1.8287
[09/26 11:01:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:01:44 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 11:01:51 visual_prompt]: Epoch 86 / 100: avg data time: 7.00e-02, avg batch time: 0.5189, average train loss: 1.7760
[09/26 11:01:53 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1690, average loss: 1.8124
[09/26 11:01:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 11:01:53 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 11:01:59 visual_prompt]: Epoch 87 / 100: avg data time: 5.81e-02, avg batch time: 0.5063, average train loss: 1.7839
[09/26 11:02:01 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1693, average loss: 1.8156
[09/26 11:02:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:02:01 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 11:02:08 visual_prompt]: Epoch 88 / 100: avg data time: 6.05e-02, avg batch time: 0.5101, average train loss: 1.7689
[09/26 11:02:10 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1693, average loss: 1.8135
[09/26 11:02:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:02:10 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 11:02:17 visual_prompt]: Epoch 89 / 100: avg data time: 6.84e-02, avg batch time: 0.5166, average train loss: 1.7673
[09/26 11:02:18 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1689, average loss: 1.8081
[09/26 11:02:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:02:18 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 11:02:25 visual_prompt]: Epoch 90 / 100: avg data time: 5.96e-02, avg batch time: 0.5075, average train loss: 1.7591
[09/26 11:02:27 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1690, average loss: 1.8019
[09/26 11:02:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:02:27 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 11:02:33 visual_prompt]: Epoch 91 / 100: avg data time: 5.31e-02, avg batch time: 0.5022, average train loss: 1.7683
[09/26 11:02:35 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1687, average loss: 1.8114
[09/26 11:02:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:02:35 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 11:02:42 visual_prompt]: Epoch 92 / 100: avg data time: 5.74e-02, avg batch time: 0.5065, average train loss: 1.7682
[09/26 11:02:44 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1687, average loss: 1.8112
[09/26 11:02:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:02:44 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 11:02:51 visual_prompt]: Epoch 93 / 100: avg data time: 5.23e-02, avg batch time: 0.5019, average train loss: 1.7396
[09/26 11:02:52 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1686, average loss: 1.7639
[09/26 11:02:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 91.00	
[09/26 11:02:52 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 11:02:59 visual_prompt]: Epoch 94 / 100: avg data time: 6.13e-02, avg batch time: 0.5095, average train loss: 1.7887
[09/26 11:03:01 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.1691, average loss: 1.8103
[09/26 11:03:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:03:01 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 11:03:07 visual_prompt]: Epoch 95 / 100: avg data time: 5.73e-02, avg batch time: 0.5082, average train loss: 1.7650
[09/26 11:03:09 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1692, average loss: 1.8044
[09/26 11:03:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:03:09 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 11:03:16 visual_prompt]: Epoch 96 / 100: avg data time: 5.40e-02, avg batch time: 0.5019, average train loss: 1.7519
[09/26 11:03:17 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1692, average loss: 1.8195
[09/26 11:03:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:03:17 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 11:03:24 visual_prompt]: Epoch 97 / 100: avg data time: 5.89e-02, avg batch time: 0.5082, average train loss: 1.7565
[09/26 11:03:26 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1688, average loss: 1.7853
[09/26 11:03:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 84.00	
[09/26 11:03:26 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 11:03:33 visual_prompt]: Epoch 98 / 100: avg data time: 5.62e-02, avg batch time: 0.5042, average train loss: 1.7207
[09/26 11:03:34 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1692, average loss: 1.7703
[09/26 11:03:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 86.50	
[09/26 11:03:34 visual_prompt]: Best epoch 98: best metric: 0.235
[09/26 11:03:34 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 11:03:41 visual_prompt]: Epoch 99 / 100: avg data time: 5.60e-02, avg batch time: 0.5039, average train loss: 1.6559
[09/26 11:03:43 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1692, average loss: 1.6971
[09/26 11:03:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 91.00	
[09/26 11:03:43 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 11:03:50 visual_prompt]: Epoch 100 / 100: avg data time: 6.23e-02, avg batch time: 0.5121, average train loss: 1.5818
[09/26 11:03:51 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 1.6998
[09/26 11:03:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 90.00	
[09/26 11:03:51 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:03:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:03:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:03:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:03:51 visual_prompt]: Training with config:
[09/26 11:03:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:03:51 visual_prompt]: Loading training data...
[09/26 11:03:51 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 11:03:53 visual_prompt]: Number of images: 800
[09/26 11:03:53 visual_prompt]: Number of classes: 6 / 6
[09/26 11:03:53 visual_prompt]: Loading validation data...
[09/26 11:03:53 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 11:03:53 visual_prompt]: Number of images: 200
[09/26 11:03:53 visual_prompt]: Number of classes: 6 / 6
[09/26 11:03:53 visual_prompt]: Constructing models...
[09/26 11:03:56 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 11:03:56 visual_prompt]: tuned percent:0.540
[09/26 11:03:56 visual_prompt]: Device used for model: 0
[09/26 11:03:56 visual_prompt]: Setting up Evaluator...
[09/26 11:03:56 visual_prompt]: Setting up Trainer...
[09/26 11:03:56 visual_prompt]: 	Setting up the optimizer...
[09/26 11:03:56 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:04:03 visual_prompt]: Epoch 1 / 100: avg data time: 6.19e-02, avg batch time: 0.5103, average train loss: 2.9823
[09/26 11:04:04 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1682, average loss: 2.9268
[09/26 11:04:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 11:04:04 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 11:04:04 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 11:04:11 visual_prompt]: Epoch 2 / 100: avg data time: 6.05e-02, avg batch time: 0.5072, average train loss: 4.9852
[09/26 11:04:13 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1682, average loss: 4.2401
[09/26 11:04:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 11:04:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 11:04:19 visual_prompt]: Epoch 3 / 100: avg data time: 4.80e-02, avg batch time: 0.4970, average train loss: 2.7006
[09/26 11:04:21 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1685, average loss: 1.9698
[09/26 11:04:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:04:21 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 11:04:28 visual_prompt]: Epoch 4 / 100: avg data time: 5.82e-02, avg batch time: 0.5053, average train loss: 1.9396
[09/26 11:04:29 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1684, average loss: 1.8243
[09/26 11:04:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 84.00	
[09/26 11:04:29 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 11:04:36 visual_prompt]: Epoch 5 / 100: avg data time: 5.59e-02, avg batch time: 0.5037, average train loss: 1.9150
[09/26 11:04:38 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1687, average loss: 1.8236
[09/26 11:04:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 11:04:38 visual_prompt]: Best epoch 5: best metric: 0.205
[09/26 11:04:38 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 11:04:44 visual_prompt]: Epoch 6 / 100: avg data time: 5.46e-02, avg batch time: 0.5034, average train loss: 1.9668
[09/26 11:04:46 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1686, average loss: 2.0432
[09/26 11:04:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 11:04:46 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 11:04:53 visual_prompt]: Epoch 7 / 100: avg data time: 7.42e-02, avg batch time: 0.5215, average train loss: 1.9111
[09/26 11:04:54 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1689, average loss: 1.9169
[09/26 11:04:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 85.50	
[09/26 11:04:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 11:05:01 visual_prompt]: Epoch 8 / 100: avg data time: 5.63e-02, avg batch time: 0.5038, average train loss: 1.9891
[09/26 11:05:03 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1689, average loss: 2.3097
[09/26 11:05:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 89.50	
[09/26 11:05:03 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 11:05:10 visual_prompt]: Epoch 9 / 100: avg data time: 5.96e-02, avg batch time: 0.5079, average train loss: 2.0641
[09/26 11:05:11 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 2.4173
[09/26 11:05:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:05:11 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 11:05:18 visual_prompt]: Epoch 10 / 100: avg data time: 5.88e-02, avg batch time: 0.5074, average train loss: 2.3655
[09/26 11:05:20 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1687, average loss: 2.4978
[09/26 11:05:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.50	
[09/26 11:05:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 11:05:27 visual_prompt]: Epoch 11 / 100: avg data time: 5.94e-02, avg batch time: 0.5076, average train loss: 3.4986
[09/26 11:05:28 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1685, average loss: 5.1094
[09/26 11:05:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:05:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 11:05:35 visual_prompt]: Epoch 12 / 100: avg data time: 5.55e-02, avg batch time: 0.5049, average train loss: 3.3790
[09/26 11:05:37 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1690, average loss: 2.1932
[09/26 11:05:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 11:05:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 11:05:43 visual_prompt]: Epoch 13 / 100: avg data time: 5.53e-02, avg batch time: 0.5024, average train loss: 2.2197
[09/26 11:05:45 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1684, average loss: 2.0065
[09/26 11:05:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.00	top5: 82.50	
[09/26 11:05:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 11:05:52 visual_prompt]: Epoch 14 / 100: avg data time: 6.02e-02, avg batch time: 0.5086, average train loss: 1.9231
[09/26 11:05:53 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1681, average loss: 2.0281
[09/26 11:05:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:05:53 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 11:06:00 visual_prompt]: Epoch 15 / 100: avg data time: 5.53e-02, avg batch time: 0.5036, average train loss: 2.0217
[09/26 11:06:02 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1684, average loss: 1.9854
[09/26 11:06:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 11:06:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 11:06:09 visual_prompt]: Epoch 16 / 100: avg data time: 5.31e-02, avg batch time: 0.5027, average train loss: 2.0384
[09/26 11:06:10 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1687, average loss: 1.8747
[09/26 11:06:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 82.00	
[09/26 11:06:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 11:06:17 visual_prompt]: Epoch 17 / 100: avg data time: 4.93e-02, avg batch time: 0.4976, average train loss: 1.8807
[09/26 11:06:19 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1687, average loss: 1.8461
[09/26 11:06:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 85.00	
[09/26 11:06:19 visual_prompt]: Best epoch 17: best metric: 0.210
[09/26 11:06:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 11:06:25 visual_prompt]: Epoch 18 / 100: avg data time: 4.55e-02, avg batch time: 0.4927, average train loss: 1.9254
[09/26 11:06:27 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1687, average loss: 2.0039
[09/26 11:06:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 79.50	
[09/26 11:06:27 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 11:06:34 visual_prompt]: Epoch 19 / 100: avg data time: 5.84e-02, avg batch time: 0.5065, average train loss: 1.9400
[09/26 11:06:35 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1685, average loss: 1.8215
[09/26 11:06:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 85.50	
[09/26 11:06:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 11:06:42 visual_prompt]: Epoch 20 / 100: avg data time: 5.94e-02, avg batch time: 0.5067, average train loss: 2.0635
[09/26 11:06:44 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1686, average loss: 2.0594
[09/26 11:06:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 11:06:44 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 11:06:51 visual_prompt]: Epoch 21 / 100: avg data time: 5.82e-02, avg batch time: 0.5053, average train loss: 2.1181
[09/26 11:06:52 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1687, average loss: 2.1840
[09/26 11:06:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 84.00	
[09/26 11:06:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 11:06:59 visual_prompt]: Epoch 22 / 100: avg data time: 4.60e-02, avg batch time: 0.4970, average train loss: 1.9268
[09/26 11:07:00 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1684, average loss: 2.0127
[09/26 11:07:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 79.50	
[09/26 11:07:00 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 11:07:07 visual_prompt]: Epoch 23 / 100: avg data time: 7.75e-02, avg batch time: 0.5243, average train loss: 1.8615
[09/26 11:07:09 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1685, average loss: 1.9321
[09/26 11:07:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 83.50	
[09/26 11:07:09 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 11:07:16 visual_prompt]: Epoch 24 / 100: avg data time: 4.41e-02, avg batch time: 0.4930, average train loss: 1.8629
[09/26 11:07:17 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1683, average loss: 1.9068
[09/26 11:07:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:07:17 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 11:07:24 visual_prompt]: Epoch 25 / 100: avg data time: 4.13e-02, avg batch time: 0.4908, average train loss: 1.8711
[09/26 11:07:25 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1687, average loss: 1.9270
[09/26 11:07:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:07:25 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 11:07:32 visual_prompt]: Epoch 26 / 100: avg data time: 4.99e-02, avg batch time: 0.4983, average train loss: 1.8609
[09/26 11:07:34 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1685, average loss: 1.8300
[09/26 11:07:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:07:34 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 11:07:40 visual_prompt]: Epoch 27 / 100: avg data time: 4.67e-02, avg batch time: 0.4954, average train loss: 1.8668
[09/26 11:07:42 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1687, average loss: 1.9258
[09/26 11:07:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:07:42 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 11:07:49 visual_prompt]: Epoch 28 / 100: avg data time: 5.79e-02, avg batch time: 0.5052, average train loss: 1.8784
[09/26 11:07:50 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1692, average loss: 1.9354
[09/26 11:07:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 85.50	
[09/26 11:07:50 visual_prompt]: Best epoch 28: best metric: 0.230
[09/26 11:07:50 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 11:07:57 visual_prompt]: Epoch 29 / 100: avg data time: 5.41e-02, avg batch time: 0.5028, average train loss: 2.4019
[09/26 11:07:59 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1683, average loss: 2.2785
[09/26 11:07:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 11:07:59 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 11:08:06 visual_prompt]: Epoch 30 / 100: avg data time: 6.32e-02, avg batch time: 0.5107, average train loss: 3.3079
[09/26 11:08:07 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1690, average loss: 4.0219
[09/26 11:08:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:08:07 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 11:08:14 visual_prompt]: Epoch 31 / 100: avg data time: 5.29e-02, avg batch time: 0.5015, average train loss: 3.2381
[09/26 11:08:16 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1688, average loss: 2.4722
[09/26 11:08:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 11:08:16 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 11:08:23 visual_prompt]: Epoch 32 / 100: avg data time: 5.63e-02, avg batch time: 0.5034, average train loss: 2.1037
[09/26 11:08:24 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1687, average loss: 2.2761
[09/26 11:08:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:08:24 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 11:08:31 visual_prompt]: Epoch 33 / 100: avg data time: 5.64e-02, avg batch time: 0.5059, average train loss: 1.9696
[09/26 11:08:33 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1686, average loss: 1.8996
[09/26 11:08:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 11:08:33 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 11:08:39 visual_prompt]: Epoch 34 / 100: avg data time: 6.17e-02, avg batch time: 0.5090, average train loss: 1.9135
[09/26 11:08:41 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 1.8403
[09/26 11:08:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 89.50	
[09/26 11:08:41 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 11:08:48 visual_prompt]: Epoch 35 / 100: avg data time: 5.84e-02, avg batch time: 0.5060, average train loss: 1.8824
[09/26 11:08:49 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1687, average loss: 1.8547
[09/26 11:08:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:08:49 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 11:08:56 visual_prompt]: Epoch 36 / 100: avg data time: 4.93e-02, avg batch time: 0.4979, average train loss: 1.8633
[09/26 11:08:58 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1689, average loss: 1.9809
[09/26 11:08:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:08:58 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 11:09:04 visual_prompt]: Epoch 37 / 100: avg data time: 4.45e-02, avg batch time: 0.4952, average train loss: 1.9686
[09/26 11:09:06 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1686, average loss: 2.0749
[09/26 11:09:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 11:09:06 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 11:09:13 visual_prompt]: Epoch 38 / 100: avg data time: 4.79e-02, avg batch time: 0.4983, average train loss: 2.1653
[09/26 11:09:14 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 3.3469
[09/26 11:09:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:09:14 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 11:09:21 visual_prompt]: Epoch 39 / 100: avg data time: 5.28e-02, avg batch time: 0.5011, average train loss: 3.0475
[09/26 11:09:23 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 3.4728
[09/26 11:09:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:09:23 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 11:09:29 visual_prompt]: Epoch 40 / 100: avg data time: 4.36e-02, avg batch time: 0.4933, average train loss: 2.5447
[09/26 11:09:31 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1686, average loss: 2.0493
[09/26 11:09:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 11:09:31 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 11:09:38 visual_prompt]: Epoch 41 / 100: avg data time: 5.32e-02, avg batch time: 0.5029, average train loss: 1.9445
[09/26 11:09:39 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1689, average loss: 1.8373
[09/26 11:09:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 85.50	
[09/26 11:09:39 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 11:09:46 visual_prompt]: Epoch 42 / 100: avg data time: 4.84e-02, avg batch time: 0.4966, average train loss: 1.9043
[09/26 11:09:48 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1689, average loss: 1.8603
[09/26 11:09:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 11:09:48 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 11:09:54 visual_prompt]: Epoch 43 / 100: avg data time: 4.86e-02, avg batch time: 0.4995, average train loss: 1.8537
[09/26 11:09:56 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1688, average loss: 2.1327
[09/26 11:09:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:09:56 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 11:10:03 visual_prompt]: Epoch 44 / 100: avg data time: 5.96e-02, avg batch time: 0.5090, average train loss: 1.9195
[09/26 11:10:04 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1691, average loss: 2.0078
[09/26 11:10:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 11:10:04 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 11:10:11 visual_prompt]: Epoch 45 / 100: avg data time: 5.12e-02, avg batch time: 0.4999, average train loss: 1.9071
[09/26 11:10:13 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1692, average loss: 2.0904
[09/26 11:10:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 11:10:13 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 11:10:20 visual_prompt]: Epoch 46 / 100: avg data time: 5.00e-02, avg batch time: 0.5001, average train loss: 1.8736
[09/26 11:10:21 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.1692, average loss: 1.9040
[09/26 11:10:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 11:10:21 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 11:10:28 visual_prompt]: Epoch 47 / 100: avg data time: 4.52e-02, avg batch time: 0.4929, average train loss: 1.9756
[09/26 11:10:29 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1690, average loss: 1.9428
[09/26 11:10:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:10:29 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 11:10:36 visual_prompt]: Epoch 48 / 100: avg data time: 5.19e-02, avg batch time: 0.5011, average train loss: 1.8475
[09/26 11:10:38 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1691, average loss: 1.8188
[09/26 11:10:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:10:38 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 11:10:44 visual_prompt]: Epoch 49 / 100: avg data time: 5.02e-02, avg batch time: 0.4996, average train loss: 1.8084
[09/26 11:10:46 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1688, average loss: 1.8862
[09/26 11:10:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 11:10:46 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 11:10:53 visual_prompt]: Epoch 50 / 100: avg data time: 4.36e-02, avg batch time: 0.4939, average train loss: 1.8064
[09/26 11:10:54 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1692, average loss: 1.8975
[09/26 11:10:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:10:54 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 11:11:01 visual_prompt]: Epoch 51 / 100: avg data time: 5.06e-02, avg batch time: 0.4999, average train loss: 1.8086
[09/26 11:11:02 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1694, average loss: 1.8576
[09/26 11:11:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 84.00	
[09/26 11:11:02 visual_prompt]: Best epoch 51: best metric: 0.270
[09/26 11:11:02 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 11:11:09 visual_prompt]: Epoch 52 / 100: avg data time: 4.69e-02, avg batch time: 0.4986, average train loss: 1.8489
[09/26 11:11:11 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1689, average loss: 1.9043
[09/26 11:11:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:11:11 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 11:11:17 visual_prompt]: Epoch 53 / 100: avg data time: 5.43e-02, avg batch time: 0.5032, average train loss: 1.8364
[09/26 11:11:19 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1686, average loss: 1.9760
[09/26 11:11:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:11:19 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 11:11:26 visual_prompt]: Epoch 54 / 100: avg data time: 5.29e-02, avg batch time: 0.5011, average train loss: 1.8718
[09/26 11:11:27 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 1.8529
[09/26 11:11:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:11:27 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 11:11:34 visual_prompt]: Epoch 55 / 100: avg data time: 5.82e-02, avg batch time: 0.5058, average train loss: 1.8857
[09/26 11:11:36 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1688, average loss: 1.8404
[09/26 11:11:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 11:11:36 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 11:11:42 visual_prompt]: Epoch 56 / 100: avg data time: 4.68e-02, avg batch time: 0.4988, average train loss: 1.7895
[09/26 11:11:44 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 1.9837
[09/26 11:11:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.50	
[09/26 11:11:44 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 11:11:51 visual_prompt]: Epoch 57 / 100: avg data time: 5.05e-02, avg batch time: 0.4990, average train loss: 1.8566
[09/26 11:11:52 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1692, average loss: 1.7953
[09/26 11:11:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 85.50	
[09/26 11:11:52 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 11:11:59 visual_prompt]: Epoch 58 / 100: avg data time: 5.08e-02, avg batch time: 0.4983, average train loss: 1.8378
[09/26 11:12:00 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1685, average loss: 1.9231
[09/26 11:12:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:12:00 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 11:12:07 visual_prompt]: Epoch 59 / 100: avg data time: 6.03e-02, avg batch time: 0.5078, average train loss: 1.8812
[09/26 11:12:09 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1689, average loss: 1.9056
[09/26 11:12:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 11:12:09 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 11:12:16 visual_prompt]: Epoch 60 / 100: avg data time: 4.49e-02, avg batch time: 0.4929, average train loss: 1.8633
[09/26 11:12:17 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1687, average loss: 1.8651
[09/26 11:12:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:12:17 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 11:12:24 visual_prompt]: Epoch 61 / 100: avg data time: 5.89e-02, avg batch time: 0.5066, average train loss: 1.8377
[09/26 11:12:26 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1688, average loss: 1.8667
[09/26 11:12:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:12:26 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 11:12:33 visual_prompt]: Epoch 62 / 100: avg data time: 5.30e-02, avg batch time: 0.5014, average train loss: 1.8078
[09/26 11:12:34 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1688, average loss: 1.8611
[09/26 11:12:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:12:34 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 11:12:41 visual_prompt]: Epoch 63 / 100: avg data time: 6.34e-02, avg batch time: 0.5109, average train loss: 1.7990
[09/26 11:12:43 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1686, average loss: 1.8448
[09/26 11:12:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:12:43 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 11:12:49 visual_prompt]: Epoch 64 / 100: avg data time: 4.72e-02, avg batch time: 0.4963, average train loss: 1.7946
[09/26 11:12:51 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 1.8365
[09/26 11:12:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.50	
[09/26 11:12:51 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 11:12:58 visual_prompt]: Epoch 65 / 100: avg data time: 4.76e-02, avg batch time: 0.4958, average train loss: 1.7688
[09/26 11:12:59 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1687, average loss: 1.8144
[09/26 11:12:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.00	top5: 84.00	
[09/26 11:12:59 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 11:13:06 visual_prompt]: Epoch 66 / 100: avg data time: 7.15e-02, avg batch time: 0.5188, average train loss: 1.7974
[09/26 11:13:08 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1692, average loss: 2.0127
[09/26 11:13:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:13:08 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 11:13:15 visual_prompt]: Epoch 67 / 100: avg data time: 7.21e-02, avg batch time: 0.5195, average train loss: 1.8209
[09/26 11:13:17 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 1.8504
[09/26 11:13:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:13:17 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 11:13:23 visual_prompt]: Epoch 68 / 100: avg data time: 4.38e-02, avg batch time: 0.4954, average train loss: 1.7762
[09/26 11:13:25 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1692, average loss: 1.8036
[09/26 11:13:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:13:25 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 11:13:32 visual_prompt]: Epoch 69 / 100: avg data time: 5.79e-02, avg batch time: 0.5068, average train loss: 1.7978
[09/26 11:13:33 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1692, average loss: 1.8681
[09/26 11:13:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 12.50	top5: 84.00	
[09/26 11:13:33 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 11:13:40 visual_prompt]: Epoch 70 / 100: avg data time: 5.48e-02, avg batch time: 0.5040, average train loss: 1.7767
[09/26 11:13:42 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1686, average loss: 1.7817
[09/26 11:13:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 11:13:42 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 11:13:49 visual_prompt]: Epoch 71 / 100: avg data time: 6.65e-02, avg batch time: 0.5143, average train loss: 1.6917
[09/26 11:13:50 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1690, average loss: 1.6527
[09/26 11:13:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 93.00	
[09/26 11:13:50 visual_prompt]: Best epoch 71: best metric: 0.285
[09/26 11:13:50 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 11:13:57 visual_prompt]: Epoch 72 / 100: avg data time: 5.76e-02, avg batch time: 0.5059, average train loss: 1.7909
[09/26 11:13:59 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 1.8358
[09/26 11:13:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:13:59 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 11:14:05 visual_prompt]: Epoch 73 / 100: avg data time: 5.58e-02, avg batch time: 0.5039, average train loss: 1.7192
[09/26 11:14:07 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1691, average loss: 1.7411
[09/26 11:14:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 87.00	
[09/26 11:14:07 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 11:14:14 visual_prompt]: Epoch 74 / 100: avg data time: 5.27e-02, avg batch time: 0.5020, average train loss: 1.8209
[09/26 11:14:15 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1692, average loss: 1.8922
[09/26 11:14:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 11:14:15 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 11:14:22 visual_prompt]: Epoch 75 / 100: avg data time: 5.63e-02, avg batch time: 0.5051, average train loss: 1.8688
[09/26 11:14:24 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1691, average loss: 1.9167
[09/26 11:14:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 11:14:24 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 11:14:31 visual_prompt]: Epoch 76 / 100: avg data time: 5.98e-02, avg batch time: 0.5073, average train loss: 1.8142
[09/26 11:14:33 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1693, average loss: 1.9201
[09/26 11:14:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:14:33 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 11:14:39 visual_prompt]: Epoch 77 / 100: avg data time: 5.32e-02, avg batch time: 0.5023, average train loss: 1.7807
[09/26 11:14:41 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1688, average loss: 1.7826
[09/26 11:14:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 84.00	
[09/26 11:14:41 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 11:14:48 visual_prompt]: Epoch 78 / 100: avg data time: 6.63e-02, avg batch time: 0.5138, average train loss: 1.7483
[09/26 11:14:49 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1688, average loss: 1.7544
[09/26 11:14:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 88.00	
[09/26 11:14:49 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 11:14:56 visual_prompt]: Epoch 79 / 100: avg data time: 5.70e-02, avg batch time: 0.5047, average train loss: 1.7538
[09/26 11:14:58 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 1.7887
[09/26 11:14:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 84.00	
[09/26 11:14:58 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 11:15:05 visual_prompt]: Epoch 80 / 100: avg data time: 5.95e-02, avg batch time: 0.5082, average train loss: 1.6707
[09/26 11:15:06 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1687, average loss: 1.9532
[09/26 11:15:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 93.50	
[09/26 11:15:06 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 11:15:13 visual_prompt]: Epoch 81 / 100: avg data time: 6.59e-02, avg batch time: 0.5137, average train loss: 1.6640
[09/26 11:15:15 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1688, average loss: 1.6369
[09/26 11:15:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 96.50	
[09/26 11:15:15 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 11:15:22 visual_prompt]: Epoch 82 / 100: avg data time: 6.17e-02, avg batch time: 0.5114, average train loss: 1.5716
[09/26 11:15:23 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1687, average loss: 1.6625
[09/26 11:15:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 95.00	
[09/26 11:15:23 visual_prompt]: Best epoch 82: best metric: 0.310
[09/26 11:15:23 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 11:15:30 visual_prompt]: Epoch 83 / 100: avg data time: 5.11e-02, avg batch time: 0.4988, average train loss: 1.4768
[09/26 11:15:32 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1689, average loss: 1.4850
[09/26 11:15:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 96.00	
[09/26 11:15:32 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 11:15:38 visual_prompt]: Epoch 84 / 100: avg data time: 4.86e-02, avg batch time: 0.4971, average train loss: 1.4287
[09/26 11:15:40 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1684, average loss: 1.4900
[09/26 11:15:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 95.00	
[09/26 11:15:40 visual_prompt]: Best epoch 84: best metric: 0.315
[09/26 11:15:40 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 11:15:47 visual_prompt]: Epoch 85 / 100: avg data time: 6.11e-02, avg batch time: 0.5100, average train loss: 1.3554
[09/26 11:15:49 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1692, average loss: 1.5949
[09/26 11:15:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 95.00	
[09/26 11:15:49 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 11:15:55 visual_prompt]: Epoch 86 / 100: avg data time: 4.93e-02, avg batch time: 0.4976, average train loss: 1.4205
[09/26 11:15:57 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1688, average loss: 1.4586
[09/26 11:15:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.50	
[09/26 11:15:57 visual_prompt]: Best epoch 86: best metric: 0.345
[09/26 11:15:57 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 11:16:04 visual_prompt]: Epoch 87 / 100: avg data time: 6.40e-02, avg batch time: 0.5117, average train loss: 1.3580
[09/26 11:16:05 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1687, average loss: 1.4412
[09/26 11:16:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 97.50	
[09/26 11:16:05 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 11:16:12 visual_prompt]: Epoch 88 / 100: avg data time: 5.26e-02, avg batch time: 0.5008, average train loss: 1.3068
[09/26 11:16:14 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1688, average loss: 1.5713
[09/26 11:16:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 97.00	
[09/26 11:16:14 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 11:16:20 visual_prompt]: Epoch 89 / 100: avg data time: 5.39e-02, avg batch time: 0.5029, average train loss: 1.2826
[09/26 11:16:22 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1696, average loss: 1.4074
[09/26 11:16:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 96.00	
[09/26 11:16:22 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 11:16:29 visual_prompt]: Epoch 90 / 100: avg data time: 4.15e-02, avg batch time: 0.4902, average train loss: 1.2812
[09/26 11:16:30 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1693, average loss: 1.6078
[09/26 11:16:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 99.00	
[09/26 11:16:30 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 11:16:37 visual_prompt]: Epoch 91 / 100: avg data time: 6.59e-02, avg batch time: 0.5151, average train loss: 1.2299
[09/26 11:16:39 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1686, average loss: 1.4449
[09/26 11:16:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.00	
[09/26 11:16:39 visual_prompt]: Best epoch 91: best metric: 0.355
[09/26 11:16:39 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 11:16:46 visual_prompt]: Epoch 92 / 100: avg data time: 6.69e-02, avg batch time: 0.5146, average train loss: 1.1804
[09/26 11:16:48 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1688, average loss: 1.4071
[09/26 11:16:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 99.00	
[09/26 11:16:48 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 11:16:55 visual_prompt]: Epoch 93 / 100: avg data time: 6.99e-02, avg batch time: 0.5174, average train loss: 1.1461
[09/26 11:16:56 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1686, average loss: 1.4545
[09/26 11:16:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 99.00	
[09/26 11:16:56 visual_prompt]: Best epoch 93: best metric: 0.390
[09/26 11:16:56 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 11:17:03 visual_prompt]: Epoch 94 / 100: avg data time: 4.72e-02, avg batch time: 0.4959, average train loss: 1.1224
[09/26 11:17:04 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1688, average loss: 1.4415
[09/26 11:17:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 11:17:04 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 11:17:11 visual_prompt]: Epoch 95 / 100: avg data time: 4.67e-02, avg batch time: 0.4959, average train loss: 1.0908
[09/26 11:17:13 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1687, average loss: 1.5070
[09/26 11:17:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 99.00	
[09/26 11:17:13 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 11:17:20 visual_prompt]: Epoch 96 / 100: avg data time: 7.45e-02, avg batch time: 0.5221, average train loss: 1.0812
[09/26 11:17:21 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1690, average loss: 1.4243
[09/26 11:17:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 99.00	
[09/26 11:17:21 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 11:17:28 visual_prompt]: Epoch 97 / 100: avg data time: 5.75e-02, avg batch time: 0.5066, average train loss: 1.0576
[09/26 11:17:30 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1687, average loss: 1.4457
[09/26 11:17:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 99.00	
[09/26 11:17:30 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 11:17:37 visual_prompt]: Epoch 98 / 100: avg data time: 5.91e-02, avg batch time: 0.5073, average train loss: 1.0319
[09/26 11:17:38 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1689, average loss: 1.4677
[09/26 11:17:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 99.00	
[09/26 11:17:38 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 11:17:45 visual_prompt]: Epoch 99 / 100: avg data time: 5.61e-02, avg batch time: 0.5037, average train loss: 1.0149
[09/26 11:17:47 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1691, average loss: 1.4692
[09/26 11:17:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 99.00	
[09/26 11:17:47 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 11:17:54 visual_prompt]: Epoch 100 / 100: avg data time: 6.22e-02, avg batch time: 0.5109, average train loss: 1.0192
[09/26 11:17:55 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1688, average loss: 1.4779
[09/26 11:17:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 99.00	
[09/26 11:17:55 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:17:55 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:17:55 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:17:55 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:17:55 visual_prompt]: Training with config:
[09/26 11:17:55 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:17:55 visual_prompt]: Loading training data...
[09/26 11:17:55 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 11:17:56 visual_prompt]: Number of images: 800
[09/26 11:17:56 visual_prompt]: Number of classes: 6 / 6
[09/26 11:17:56 visual_prompt]: Loading validation data...
[09/26 11:17:56 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 11:17:57 visual_prompt]: Number of images: 200
[09/26 11:17:57 visual_prompt]: Number of classes: 6 / 6
[09/26 11:17:57 visual_prompt]: Constructing models...
[09/26 11:17:59 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 11:17:59 visual_prompt]: tuned percent:0.540
[09/26 11:17:59 visual_prompt]: Device used for model: 0
[09/26 11:17:59 visual_prompt]: Setting up Evaluator...
[09/26 11:17:59 visual_prompt]: Setting up Trainer...
[09/26 11:17:59 visual_prompt]: 	Setting up the optimizer...
[09/26 11:17:59 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:18:07 visual_prompt]: Epoch 1 / 100: avg data time: 7.50e-02, avg batch time: 0.5235, average train loss: 2.9860
[09/26 11:18:08 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1686, average loss: 2.9268
[09/26 11:18:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 11:18:08 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 11:18:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 11:18:15 visual_prompt]: Epoch 2 / 100: avg data time: 6.09e-02, avg batch time: 0.5088, average train loss: 5.3029
[09/26 11:18:16 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1682, average loss: 4.7544
[09/26 11:18:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 11:18:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 11:18:23 visual_prompt]: Epoch 3 / 100: avg data time: 6.26e-02, avg batch time: 0.5096, average train loss: 3.0149
[09/26 11:18:25 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1685, average loss: 2.1857
[09/26 11:18:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 11:18:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 11:18:32 visual_prompt]: Epoch 4 / 100: avg data time: 6.82e-02, avg batch time: 0.5152, average train loss: 2.0751
[09/26 11:18:33 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1682, average loss: 1.8940
[09/26 11:18:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 84.00	
[09/26 11:18:33 visual_prompt]: Best epoch 4: best metric: 0.190
[09/26 11:18:33 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 11:18:40 visual_prompt]: Epoch 5 / 100: avg data time: 5.38e-02, avg batch time: 0.5019, average train loss: 2.0473
[09/26 11:18:42 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 2.0895
[09/26 11:18:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:18:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 11:18:48 visual_prompt]: Epoch 6 / 100: avg data time: 5.33e-02, avg batch time: 0.5016, average train loss: 2.2681
[09/26 11:18:50 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1691, average loss: 2.6542
[09/26 11:18:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 11:18:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 11:18:57 visual_prompt]: Epoch 7 / 100: avg data time: 5.69e-02, avg batch time: 0.5056, average train loss: 2.3406
[09/26 11:18:58 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1690, average loss: 2.8172
[09/26 11:18:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 11:18:58 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 11:19:05 visual_prompt]: Epoch 8 / 100: avg data time: 6.00e-02, avg batch time: 0.5078, average train loss: 2.6122
[09/26 11:19:07 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1685, average loss: 1.9775
[09/26 11:19:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 89.50	
[09/26 11:19:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 11:19:14 visual_prompt]: Epoch 9 / 100: avg data time: 5.80e-02, avg batch time: 0.5062, average train loss: 2.5584
[09/26 11:19:15 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1692, average loss: 2.5766
[09/26 11:19:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 85.50	
[09/26 11:19:15 visual_prompt]: Best epoch 9: best metric: 0.200
[09/26 11:19:15 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 11:19:22 visual_prompt]: Epoch 10 / 100: avg data time: 6.50e-02, avg batch time: 0.5125, average train loss: 2.1483
[09/26 11:19:23 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1691, average loss: 2.2673
[09/26 11:19:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 84.00	
[09/26 11:19:24 visual_prompt]: Best epoch 10: best metric: 0.210
[09/26 11:19:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 11:19:30 visual_prompt]: Epoch 11 / 100: avg data time: 5.63e-02, avg batch time: 0.5042, average train loss: 1.8972
[09/26 11:19:32 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1689, average loss: 1.8215
[09/26 11:19:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 94.00	
[09/26 11:19:32 visual_prompt]: Best epoch 11: best metric: 0.235
[09/26 11:19:32 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 11:19:39 visual_prompt]: Epoch 12 / 100: avg data time: 4.54e-02, avg batch time: 0.4948, average train loss: 1.7898
[09/26 11:19:40 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1688, average loss: 2.4229
[09/26 11:19:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 90.50	
[09/26 11:19:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 11:19:47 visual_prompt]: Epoch 13 / 100: avg data time: 6.69e-02, avg batch time: 0.5148, average train loss: 3.2504
[09/26 11:19:49 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1685, average loss: 6.2397
[09/26 11:19:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 79.50	
[09/26 11:19:49 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 11:19:56 visual_prompt]: Epoch 14 / 100: avg data time: 5.71e-02, avg batch time: 0.5048, average train loss: 7.7027
[09/26 11:19:57 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1690, average loss: 8.8007
[09/26 11:19:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:19:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 11:20:04 visual_prompt]: Epoch 15 / 100: avg data time: 4.99e-02, avg batch time: 0.4990, average train loss: 6.0170
[09/26 11:20:06 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1691, average loss: 3.8727
[09/26 11:20:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:20:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 11:20:12 visual_prompt]: Epoch 16 / 100: avg data time: 6.03e-02, avg batch time: 0.5079, average train loss: 3.1343
[09/26 11:20:14 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1686, average loss: 2.9046
[09/26 11:20:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 84.00	
[09/26 11:20:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 11:20:21 visual_prompt]: Epoch 17 / 100: avg data time: 5.53e-02, avg batch time: 0.5033, average train loss: 2.7395
[09/26 11:20:22 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1693, average loss: 3.9365
[09/26 11:20:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 85.50	
[09/26 11:20:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 11:20:29 visual_prompt]: Epoch 18 / 100: avg data time: 5.82e-02, avg batch time: 0.5063, average train loss: 2.7694
[09/26 11:20:31 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1690, average loss: 2.6853
[09/26 11:20:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 84.00	
[09/26 11:20:31 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 11:20:38 visual_prompt]: Epoch 19 / 100: avg data time: 5.42e-02, avg batch time: 0.5035, average train loss: 2.1170
[09/26 11:20:39 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1689, average loss: 2.2207
[09/26 11:20:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 91.00	
[09/26 11:20:39 visual_prompt]: Best epoch 19: best metric: 0.240
[09/26 11:20:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 11:20:46 visual_prompt]: Epoch 20 / 100: avg data time: 5.70e-02, avg batch time: 0.5066, average train loss: 1.8826
[09/26 11:20:47 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1688, average loss: 1.9287
[09/26 11:20:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 91.50	
[09/26 11:20:47 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 11:20:54 visual_prompt]: Epoch 21 / 100: avg data time: 4.93e-02, avg batch time: 0.5010, average train loss: 1.8079
[09/26 11:20:56 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1687, average loss: 2.0922
[09/26 11:20:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 91.00	
[09/26 11:20:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 11:21:03 visual_prompt]: Epoch 22 / 100: avg data time: 5.96e-02, avg batch time: 0.5085, average train loss: 1.6161
[09/26 11:21:05 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1685, average loss: 2.7890
[09/26 11:21:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 91.50	
[09/26 11:21:05 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 11:21:11 visual_prompt]: Epoch 23 / 100: avg data time: 5.87e-02, avg batch time: 0.5082, average train loss: 1.8704
[09/26 11:21:13 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1688, average loss: 1.8950
[09/26 11:21:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 90.00	
[09/26 11:21:13 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 11:21:20 visual_prompt]: Epoch 24 / 100: avg data time: 4.49e-02, avg batch time: 0.4938, average train loss: 1.6798
[09/26 11:21:21 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 1.8322
[09/26 11:21:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 98.00	
[09/26 11:21:21 visual_prompt]: Best epoch 24: best metric: 0.290
[09/26 11:21:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 11:21:28 visual_prompt]: Epoch 25 / 100: avg data time: 6.00e-02, avg batch time: 0.5087, average train loss: 1.7317
[09/26 11:21:30 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1692, average loss: 2.0545
[09/26 11:21:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 97.50	
[09/26 11:21:30 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 11:21:37 visual_prompt]: Epoch 26 / 100: avg data time: 5.67e-02, avg batch time: 0.5045, average train loss: 1.6460
[09/26 11:21:38 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1687, average loss: 1.4443
[09/26 11:21:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 11:21:38 visual_prompt]: Best epoch 26: best metric: 0.375
[09/26 11:21:38 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 11:21:45 visual_prompt]: Epoch 27 / 100: avg data time: 6.46e-02, avg batch time: 0.5123, average train loss: 1.4194
[09/26 11:21:47 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1688, average loss: 1.6636
[09/26 11:21:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 99.50	
[09/26 11:21:47 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 11:21:54 visual_prompt]: Epoch 28 / 100: avg data time: 7.23e-02, avg batch time: 0.5200, average train loss: 1.5257
[09/26 11:21:56 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1691, average loss: 2.2285
[09/26 11:21:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 94.50	
[09/26 11:21:56 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 11:22:02 visual_prompt]: Epoch 29 / 100: avg data time: 4.02e-02, avg batch time: 0.4903, average train loss: 1.4308
[09/26 11:22:04 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1689, average loss: 1.9400
[09/26 11:22:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 95.50	
[09/26 11:22:04 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 11:22:11 visual_prompt]: Epoch 30 / 100: avg data time: 4.82e-02, avg batch time: 0.4979, average train loss: 1.3051
[09/26 11:22:12 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1689, average loss: 1.5053
[09/26 11:22:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.00	
[09/26 11:22:12 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 11:22:19 visual_prompt]: Epoch 31 / 100: avg data time: 7.83e-02, avg batch time: 0.5259, average train loss: 1.3998
[09/26 11:22:21 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1689, average loss: 1.5627
[09/26 11:22:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 99.50	
[09/26 11:22:21 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 11:22:28 visual_prompt]: Epoch 32 / 100: avg data time: 5.69e-02, avg batch time: 0.5051, average train loss: 1.4852
[09/26 11:22:29 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1690, average loss: 1.6037
[09/26 11:22:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 99.00	
[09/26 11:22:29 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 11:22:36 visual_prompt]: Epoch 33 / 100: avg data time: 5.40e-02, avg batch time: 0.5033, average train loss: 1.2723
[09/26 11:22:38 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1688, average loss: 1.4659
[09/26 11:22:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 11:22:38 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 11:22:44 visual_prompt]: Epoch 34 / 100: avg data time: 4.85e-02, avg batch time: 0.4985, average train loss: 1.2608
[09/26 11:22:46 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1693, average loss: 1.8301
[09/26 11:22:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 11:22:46 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 11:22:53 visual_prompt]: Epoch 35 / 100: avg data time: 5.14e-02, avg batch time: 0.5005, average train loss: 1.4380
[09/26 11:22:54 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 1.9819
[09/26 11:22:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.00	
[09/26 11:22:54 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 11:23:01 visual_prompt]: Epoch 36 / 100: avg data time: 4.99e-02, avg batch time: 0.4980, average train loss: 1.2492
[09/26 11:23:02 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1691, average loss: 1.5102
[09/26 11:23:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 11:23:02 visual_prompt]: Best epoch 36: best metric: 0.395
[09/26 11:23:02 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 11:23:09 visual_prompt]: Epoch 37 / 100: avg data time: 5.56e-02, avg batch time: 0.5039, average train loss: 1.4382
[09/26 11:23:11 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1688, average loss: 1.9572
[09/26 11:23:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 98.00	
[09/26 11:23:11 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 11:23:18 visual_prompt]: Epoch 38 / 100: avg data time: 5.12e-02, avg batch time: 0.5002, average train loss: 1.4138
[09/26 11:23:19 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1690, average loss: 1.8595
[09/26 11:23:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 88.00	
[09/26 11:23:19 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 11:23:26 visual_prompt]: Epoch 39 / 100: avg data time: 5.65e-02, avg batch time: 0.5046, average train loss: 1.1614
[09/26 11:23:28 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1688, average loss: 1.6458
[09/26 11:23:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 11:23:28 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 11:23:34 visual_prompt]: Epoch 40 / 100: avg data time: 5.50e-02, avg batch time: 0.5038, average train loss: 1.1094
[09/26 11:23:36 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1692, average loss: 1.6481
[09/26 11:23:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.00	
[09/26 11:23:36 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 11:23:43 visual_prompt]: Epoch 41 / 100: avg data time: 5.68e-02, avg batch time: 0.5059, average train loss: 1.0493
[09/26 11:23:44 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 1.7177
[09/26 11:23:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 97.50	
[09/26 11:23:44 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 11:23:51 visual_prompt]: Epoch 42 / 100: avg data time: 4.78e-02, avg batch time: 0.4961, average train loss: 1.0304
[09/26 11:23:53 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1690, average loss: 2.1240
[09/26 11:23:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.50	
[09/26 11:23:53 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 11:24:00 visual_prompt]: Epoch 43 / 100: avg data time: 5.81e-02, avg batch time: 0.5069, average train loss: 1.2724
[09/26 11:24:01 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1687, average loss: 1.5707
[09/26 11:24:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 99.00	
[09/26 11:24:01 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 11:24:08 visual_prompt]: Epoch 44 / 100: avg data time: 5.21e-02, avg batch time: 0.5009, average train loss: 1.1197
[09/26 11:24:09 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 1.4603
[09/26 11:24:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 99.00	
[09/26 11:24:09 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 11:24:16 visual_prompt]: Epoch 45 / 100: avg data time: 5.02e-02, avg batch time: 0.4987, average train loss: 1.1217
[09/26 11:24:18 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1693, average loss: 1.9669
[09/26 11:24:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 11:24:18 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 11:24:25 visual_prompt]: Epoch 46 / 100: avg data time: 5.72e-02, avg batch time: 0.5072, average train loss: 1.3570
[09/26 11:24:26 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1691, average loss: 1.9922
[09/26 11:24:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 95.00	
[09/26 11:24:26 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 11:24:33 visual_prompt]: Epoch 47 / 100: avg data time: 4.85e-02, avg batch time: 0.4990, average train loss: 1.0754
[09/26 11:24:35 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1689, average loss: 1.5081
[09/26 11:24:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 11:24:35 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 11:24:42 visual_prompt]: Epoch 48 / 100: avg data time: 5.36e-02, avg batch time: 0.5033, average train loss: 0.9803
[09/26 11:24:43 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1692, average loss: 2.0002
[09/26 11:24:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 11:24:43 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 11:24:50 visual_prompt]: Epoch 49 / 100: avg data time: 4.49e-02, avg batch time: 0.4938, average train loss: 0.9903
[09/26 11:24:52 visual_prompt]: Inference (val):avg data time: 4.08e-04, avg batch time: 0.3200, average loss: 1.7717
[09/26 11:24:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 11:24:52 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 11:24:59 visual_prompt]: Epoch 50 / 100: avg data time: 4.60e-02, avg batch time: 0.4966, average train loss: 0.9774
[09/26 11:25:01 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1686, average loss: 1.6484
[09/26 11:25:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.50	
[09/26 11:25:01 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 11:25:08 visual_prompt]: Epoch 51 / 100: avg data time: 6.07e-02, avg batch time: 0.5088, average train loss: 0.9039
[09/26 11:25:09 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 1.5543
[09/26 11:25:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 99.50	
[09/26 11:25:09 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 11:25:16 visual_prompt]: Epoch 52 / 100: avg data time: 5.33e-02, avg batch time: 0.5032, average train loss: 0.8396
[09/26 11:25:18 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1688, average loss: 1.7980
[09/26 11:25:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 11:25:18 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 11:25:25 visual_prompt]: Epoch 53 / 100: avg data time: 6.44e-02, avg batch time: 0.5120, average train loss: 0.7553
[09/26 11:25:26 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 1.6405
[09/26 11:25:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.00	
[09/26 11:25:26 visual_prompt]: Best epoch 53: best metric: 0.435
[09/26 11:25:26 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 11:25:33 visual_prompt]: Epoch 54 / 100: avg data time: 6.42e-02, avg batch time: 0.5122, average train loss: 0.7816
[09/26 11:25:35 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1686, average loss: 2.0835
[09/26 11:25:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.00	
[09/26 11:25:35 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 11:25:42 visual_prompt]: Epoch 55 / 100: avg data time: 5.64e-02, avg batch time: 0.5046, average train loss: 0.8320
[09/26 11:25:43 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1688, average loss: 2.0209
[09/26 11:25:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 11:25:43 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 11:25:50 visual_prompt]: Epoch 56 / 100: avg data time: 4.60e-02, avg batch time: 0.4951, average train loss: 0.7564
[09/26 11:25:51 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1693, average loss: 1.9097
[09/26 11:25:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.00	
[09/26 11:25:51 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 11:25:58 visual_prompt]: Epoch 57 / 100: avg data time: 7.72e-02, avg batch time: 0.5253, average train loss: 0.7215
[09/26 11:26:00 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 1.7792
[09/26 11:26:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 11:26:00 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 11:26:07 visual_prompt]: Epoch 58 / 100: avg data time: 6.45e-02, avg batch time: 0.5128, average train loss: 0.6592
[09/26 11:26:08 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 2.0235
[09/26 11:26:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.00	
[09/26 11:26:08 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 11:26:15 visual_prompt]: Epoch 59 / 100: avg data time: 5.79e-02, avg batch time: 0.5064, average train loss: 0.7035
[09/26 11:26:17 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1687, average loss: 2.1016
[09/26 11:26:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.00	
[09/26 11:26:17 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 11:26:24 visual_prompt]: Epoch 60 / 100: avg data time: 4.79e-02, avg batch time: 0.4962, average train loss: 0.6095
[09/26 11:26:25 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1688, average loss: 2.2522
[09/26 11:26:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 11:26:25 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 11:26:32 visual_prompt]: Epoch 61 / 100: avg data time: 5.47e-02, avg batch time: 0.5031, average train loss: 0.5604
[09/26 11:26:33 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1690, average loss: 1.9612
[09/26 11:26:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 11:26:33 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 11:26:40 visual_prompt]: Epoch 62 / 100: avg data time: 5.51e-02, avg batch time: 0.5047, average train loss: 0.6304
[09/26 11:26:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1690, average loss: 2.0934
[09/26 11:26:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 11:26:42 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 11:26:48 visual_prompt]: Epoch 63 / 100: avg data time: 4.91e-02, avg batch time: 0.4978, average train loss: 0.6071
[09/26 11:26:50 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 2.5150
[09/26 11:26:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.00	
[09/26 11:26:50 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 11:26:57 visual_prompt]: Epoch 64 / 100: avg data time: 5.95e-02, avg batch time: 0.5080, average train loss: 0.4993
[09/26 11:26:58 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1690, average loss: 2.1602
[09/26 11:26:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.00	
[09/26 11:26:58 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 11:27:05 visual_prompt]: Epoch 65 / 100: avg data time: 6.47e-02, avg batch time: 0.5130, average train loss: 0.6577
[09/26 11:27:07 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1687, average loss: 2.0003
[09/26 11:27:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 11:27:07 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 11:27:14 visual_prompt]: Epoch 66 / 100: avg data time: 5.28e-02, avg batch time: 0.5033, average train loss: 0.5119
[09/26 11:27:15 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 2.4170
[09/26 11:27:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/26 11:27:15 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 11:27:22 visual_prompt]: Epoch 67 / 100: avg data time: 5.56e-02, avg batch time: 0.5040, average train loss: 0.3877
[09/26 11:27:24 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1691, average loss: 2.8127
[09/26 11:27:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 11:27:24 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 11:27:30 visual_prompt]: Epoch 68 / 100: avg data time: 5.48e-02, avg batch time: 0.5033, average train loss: 0.4038
[09/26 11:27:32 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 2.8553
[09/26 11:27:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.50	
[09/26 11:27:32 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 11:27:39 visual_prompt]: Epoch 69 / 100: avg data time: 4.65e-02, avg batch time: 0.4972, average train loss: 0.5032
[09/26 11:27:40 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1690, average loss: 2.5654
[09/26 11:27:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.00	
[09/26 11:27:40 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 11:27:47 visual_prompt]: Epoch 70 / 100: avg data time: 4.58e-02, avg batch time: 0.4959, average train loss: 0.3284
[09/26 11:27:49 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1693, average loss: 2.6739
[09/26 11:27:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 11:27:49 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 11:27:55 visual_prompt]: Epoch 71 / 100: avg data time: 4.70e-02, avg batch time: 0.4986, average train loss: 0.3507
[09/26 11:27:57 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1691, average loss: 3.0726
[09/26 11:27:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 11:27:57 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 11:28:04 visual_prompt]: Epoch 72 / 100: avg data time: 5.84e-02, avg batch time: 0.5067, average train loss: 0.2566
[09/26 11:28:05 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1698, average loss: 3.2376
[09/26 11:28:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 11:28:05 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 11:28:12 visual_prompt]: Epoch 73 / 100: avg data time: 5.18e-02, avg batch time: 0.5005, average train loss: 0.3168
[09/26 11:28:14 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.1693, average loss: 3.2513
[09/26 11:28:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.50	
[09/26 11:28:14 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 11:28:21 visual_prompt]: Epoch 74 / 100: avg data time: 5.08e-02, avg batch time: 0.5030, average train loss: 0.2357
[09/26 11:28:22 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1692, average loss: 3.4112
[09/26 11:28:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 99.50	
[09/26 11:28:22 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 11:28:29 visual_prompt]: Epoch 75 / 100: avg data time: 4.96e-02, avg batch time: 0.4981, average train loss: 0.2365
[09/26 11:28:31 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1688, average loss: 3.2764
[09/26 11:28:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 97.50	
[09/26 11:28:31 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 11:28:37 visual_prompt]: Epoch 76 / 100: avg data time: 5.72e-02, avg batch time: 0.5054, average train loss: 0.2588
[09/26 11:28:39 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1691, average loss: 4.0289
[09/26 11:28:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 99.50	
[09/26 11:28:39 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 11:28:46 visual_prompt]: Epoch 77 / 100: avg data time: 4.50e-02, avg batch time: 0.4972, average train loss: 0.3649
[09/26 11:28:47 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1689, average loss: 2.9135
[09/26 11:28:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 11:28:47 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 11:28:54 visual_prompt]: Epoch 78 / 100: avg data time: 5.88e-02, avg batch time: 0.5080, average train loss: 0.2012
[09/26 11:28:56 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 3.0325
[09/26 11:28:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 11:28:56 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 11:29:02 visual_prompt]: Epoch 79 / 100: avg data time: 4.35e-02, avg batch time: 0.4940, average train loss: 0.1525
[09/26 11:29:04 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1689, average loss: 3.4461
[09/26 11:29:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.50	
[09/26 11:29:04 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 11:29:11 visual_prompt]: Epoch 80 / 100: avg data time: 5.11e-02, avg batch time: 0.5010, average train loss: 0.1335
[09/26 11:29:12 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1691, average loss: 4.0167
[09/26 11:29:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 99.50	
[09/26 11:29:12 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 11:29:19 visual_prompt]: Epoch 81 / 100: avg data time: 4.49e-02, avg batch time: 0.4943, average train loss: 0.1261
[09/26 11:29:20 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1688, average loss: 4.3115
[09/26 11:29:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 11:29:20 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 11:29:27 visual_prompt]: Epoch 82 / 100: avg data time: 5.42e-02, avg batch time: 0.5040, average train loss: 0.1623
[09/26 11:29:29 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1690, average loss: 3.9123
[09/26 11:29:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 11:29:29 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 11:29:36 visual_prompt]: Epoch 83 / 100: avg data time: 4.66e-02, avg batch time: 0.4971, average train loss: 0.0994
[09/26 11:29:37 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1688, average loss: 4.4231
[09/26 11:29:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 11:29:37 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 11:29:44 visual_prompt]: Epoch 84 / 100: avg data time: 5.20e-02, avg batch time: 0.5022, average train loss: 0.0829
[09/26 11:29:45 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1690, average loss: 4.6030
[09/26 11:29:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 99.00	
[09/26 11:29:45 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 11:29:52 visual_prompt]: Epoch 85 / 100: avg data time: 4.92e-02, avg batch time: 0.4980, average train loss: 0.0679
[09/26 11:29:54 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 4.9356
[09/26 11:29:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 11:29:54 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 11:30:01 visual_prompt]: Epoch 86 / 100: avg data time: 4.40e-02, avg batch time: 0.4939, average train loss: 0.0429
[09/26 11:30:02 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 5.6544
[09/26 11:30:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 99.00	
[09/26 11:30:02 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 11:30:09 visual_prompt]: Epoch 87 / 100: avg data time: 5.26e-02, avg batch time: 0.5041, average train loss: 0.0447
[09/26 11:30:11 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 5.5642
[09/26 11:30:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 11:30:11 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 11:30:17 visual_prompt]: Epoch 88 / 100: avg data time: 4.74e-02, avg batch time: 0.4968, average train loss: 0.0457
[09/26 11:30:19 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 5.6665
[09/26 11:30:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 11:30:19 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 11:30:26 visual_prompt]: Epoch 89 / 100: avg data time: 5.55e-02, avg batch time: 0.5047, average train loss: 0.0412
[09/26 11:30:27 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 5.7503
[09/26 11:30:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.50	
[09/26 11:30:27 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 11:30:34 visual_prompt]: Epoch 90 / 100: avg data time: 5.93e-02, avg batch time: 0.5086, average train loss: 0.0241
[09/26 11:30:36 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1691, average loss: 5.9349
[09/26 11:30:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 99.00	
[09/26 11:30:36 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 11:30:43 visual_prompt]: Epoch 91 / 100: avg data time: 6.85e-02, avg batch time: 0.5170, average train loss: 0.0218
[09/26 11:30:45 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1691, average loss: 6.2067
[09/26 11:30:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 11:30:45 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 11:30:52 visual_prompt]: Epoch 92 / 100: avg data time: 7.52e-02, avg batch time: 0.5233, average train loss: 0.0277
[09/26 11:30:53 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1686, average loss: 6.2787
[09/26 11:30:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 11:30:53 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 11:31:00 visual_prompt]: Epoch 93 / 100: avg data time: 5.99e-02, avg batch time: 0.5086, average train loss: 0.0169
[09/26 11:31:02 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1690, average loss: 6.2092
[09/26 11:31:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 11:31:02 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 11:31:09 visual_prompt]: Epoch 94 / 100: avg data time: 5.89e-02, avg batch time: 0.5074, average train loss: 0.0206
[09/26 11:31:10 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1691, average loss: 6.2431
[09/26 11:31:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 11:31:10 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 11:31:17 visual_prompt]: Epoch 95 / 100: avg data time: 6.93e-02, avg batch time: 0.5175, average train loss: 0.0139
[09/26 11:31:19 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1685, average loss: 6.3290
[09/26 11:31:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 11:31:19 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 11:31:26 visual_prompt]: Epoch 96 / 100: avg data time: 8.10e-02, avg batch time: 0.5290, average train loss: 0.0201
[09/26 11:31:28 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1690, average loss: 6.3723
[09/26 11:31:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 11:31:28 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 11:31:34 visual_prompt]: Epoch 97 / 100: avg data time: 5.96e-02, avg batch time: 0.5080, average train loss: 0.0149
[09/26 11:31:36 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 6.3963
[09/26 11:31:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 11:31:36 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 11:31:43 visual_prompt]: Epoch 98 / 100: avg data time: 5.30e-02, avg batch time: 0.5031, average train loss: 0.0159
[09/26 11:31:44 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1689, average loss: 6.4214
[09/26 11:31:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 11:31:44 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 11:31:51 visual_prompt]: Epoch 99 / 100: avg data time: 6.10e-02, avg batch time: 0.5094, average train loss: 0.0137
[09/26 11:31:53 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 6.4256
[09/26 11:31:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 11:31:53 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 11:32:00 visual_prompt]: Epoch 100 / 100: avg data time: 5.47e-02, avg batch time: 0.5028, average train loss: 0.0122
[09/26 11:32:01 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1690, average loss: 6.4256
[09/26 11:32:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 11:32:01 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:32:01 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:32:01 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:32:01 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:32:01 visual_prompt]: Training with config:
[09/26 11:32:01 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:32:01 visual_prompt]: Loading training data...
[09/26 11:32:01 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 11:32:02 visual_prompt]: Number of images: 800
[09/26 11:32:02 visual_prompt]: Number of classes: 6 / 6
[09/26 11:32:02 visual_prompt]: Loading validation data...
[09/26 11:32:02 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 11:32:03 visual_prompt]: Number of images: 200
[09/26 11:32:03 visual_prompt]: Number of classes: 6 / 6
[09/26 11:32:03 visual_prompt]: Constructing models...
[09/26 11:32:05 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 11:32:05 visual_prompt]: tuned percent:0.540
[09/26 11:32:05 visual_prompt]: Device used for model: 0
[09/26 11:32:05 visual_prompt]: Setting up Evaluator...
[09/26 11:32:05 visual_prompt]: Setting up Trainer...
[09/26 11:32:05 visual_prompt]: 	Setting up the optimizer...
[09/26 11:32:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:32:12 visual_prompt]: Epoch 1 / 100: avg data time: 5.97e-02, avg batch time: 0.5090, average train loss: 2.9666
[09/26 11:32:14 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1684, average loss: 2.9268
[09/26 11:32:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 11:32:14 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 11:32:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 11:32:21 visual_prompt]: Epoch 2 / 100: avg data time: 5.54e-02, avg batch time: 0.5020, average train loss: 5.0191
[09/26 11:32:22 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1686, average loss: 4.0984
[09/26 11:32:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 11:32:22 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 11:32:29 visual_prompt]: Epoch 3 / 100: avg data time: 4.89e-02, avg batch time: 0.4971, average train loss: 2.6880
[09/26 11:32:31 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1686, average loss: 1.8083
[09/26 11:32:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 11:32:31 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 11:32:37 visual_prompt]: Epoch 4 / 100: avg data time: 5.01e-02, avg batch time: 0.4998, average train loss: 1.9537
[09/26 11:32:39 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1690, average loss: 2.0924
[09/26 11:32:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 84.00	
[09/26 11:32:39 visual_prompt]: Best epoch 4: best metric: 0.195
[09/26 11:32:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 11:32:46 visual_prompt]: Epoch 5 / 100: avg data time: 4.51e-02, avg batch time: 0.4933, average train loss: 2.0037
[09/26 11:32:47 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1685, average loss: 2.0743
[09/26 11:32:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:32:47 visual_prompt]: Best epoch 5: best metric: 0.205
[09/26 11:32:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 11:32:54 visual_prompt]: Epoch 6 / 100: avg data time: 4.80e-02, avg batch time: 0.4994, average train loss: 2.0696
[09/26 11:32:55 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1689, average loss: 2.0009
[09/26 11:32:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:32:55 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 11:33:02 visual_prompt]: Epoch 7 / 100: avg data time: 5.31e-02, avg batch time: 0.5022, average train loss: 2.1373
[09/26 11:33:04 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 2.5009
[09/26 11:33:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:33:04 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 11:33:11 visual_prompt]: Epoch 8 / 100: avg data time: 5.68e-02, avg batch time: 0.5050, average train loss: 2.3794
[09/26 11:33:12 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1688, average loss: 2.3950
[09/26 11:33:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 11:33:12 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 11:33:19 visual_prompt]: Epoch 9 / 100: avg data time: 5.76e-02, avg batch time: 0.5063, average train loss: 2.7840
[09/26 11:33:20 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1693, average loss: 2.2587
[09/26 11:33:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 91.50	
[09/26 11:33:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 11:33:27 visual_prompt]: Epoch 10 / 100: avg data time: 5.77e-02, avg batch time: 0.5064, average train loss: 2.0108
[09/26 11:33:29 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 1.9904
[09/26 11:33:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 84.00	
[09/26 11:33:29 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 11:33:36 visual_prompt]: Epoch 11 / 100: avg data time: 5.39e-02, avg batch time: 0.5021, average train loss: 2.4388
[09/26 11:33:37 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1687, average loss: 2.5824
[09/26 11:33:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 84.50	
[09/26 11:33:37 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 11:33:44 visual_prompt]: Epoch 12 / 100: avg data time: 5.26e-02, avg batch time: 0.5024, average train loss: 2.3132
[09/26 11:33:45 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1692, average loss: 2.2653
[09/26 11:33:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 94.50	
[09/26 11:33:45 visual_prompt]: Best epoch 12: best metric: 0.240
[09/26 11:33:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 11:33:52 visual_prompt]: Epoch 13 / 100: avg data time: 4.43e-02, avg batch time: 0.4923, average train loss: 2.3319
[09/26 11:33:54 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1693, average loss: 1.8984
[09/26 11:33:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 95.50	
[09/26 11:33:54 visual_prompt]: Best epoch 13: best metric: 0.280
[09/26 11:33:54 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 11:34:00 visual_prompt]: Epoch 14 / 100: avg data time: 6.24e-02, avg batch time: 0.5117, average train loss: 1.9350
[09/26 11:34:02 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1690, average loss: 1.5856
[09/26 11:34:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 95.00	
[09/26 11:34:02 visual_prompt]: Best epoch 14: best metric: 0.370
[09/26 11:34:02 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 11:34:09 visual_prompt]: Epoch 15 / 100: avg data time: 6.08e-02, avg batch time: 0.5085, average train loss: 1.6527
[09/26 11:34:11 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 1.5511
[09/26 11:34:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.00	
[09/26 11:34:11 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 11:34:17 visual_prompt]: Epoch 16 / 100: avg data time: 5.07e-02, avg batch time: 0.5000, average train loss: 1.4574
[09/26 11:34:19 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1689, average loss: 1.8110
[09/26 11:34:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 97.00	
[09/26 11:34:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 11:34:26 visual_prompt]: Epoch 17 / 100: avg data time: 5.15e-02, avg batch time: 0.5012, average train loss: 1.6393
[09/26 11:34:27 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1690, average loss: 1.7490
[09/26 11:34:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 97.00	
[09/26 11:34:27 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 11:34:34 visual_prompt]: Epoch 18 / 100: avg data time: 4.58e-02, avg batch time: 0.4947, average train loss: 1.4381
[09/26 11:34:35 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 1.5037
[09/26 11:34:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.50	
[09/26 11:34:35 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 11:34:42 visual_prompt]: Epoch 19 / 100: avg data time: 4.59e-02, avg batch time: 0.4963, average train loss: 1.3035
[09/26 11:34:43 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1689, average loss: 1.6743
[09/26 11:34:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 96.50	
[09/26 11:34:43 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 11:34:50 visual_prompt]: Epoch 20 / 100: avg data time: 4.05e-02, avg batch time: 0.4934, average train loss: 1.3516
[09/26 11:34:52 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1689, average loss: 2.0651
[09/26 11:34:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 95.50	
[09/26 11:34:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 11:34:59 visual_prompt]: Epoch 21 / 100: avg data time: 5.43e-02, avg batch time: 0.5042, average train loss: 1.3816
[09/26 11:35:00 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1692, average loss: 1.9978
[09/26 11:35:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 95.50	
[09/26 11:35:00 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 11:35:07 visual_prompt]: Epoch 22 / 100: avg data time: 4.48e-02, avg batch time: 0.4944, average train loss: 1.3565
[09/26 11:35:08 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 1.7617
[09/26 11:35:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.50	
[09/26 11:35:08 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 11:35:15 visual_prompt]: Epoch 23 / 100: avg data time: 5.69e-02, avg batch time: 0.5051, average train loss: 1.4413
[09/26 11:35:17 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1688, average loss: 2.0682
[09/26 11:35:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 97.00	
[09/26 11:35:17 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 11:35:24 visual_prompt]: Epoch 24 / 100: avg data time: 4.90e-02, avg batch time: 0.4987, average train loss: 1.3131
[09/26 11:35:25 visual_prompt]: Inference (val):avg data time: 4.45e-05, avg batch time: 0.1690, average loss: 1.6990
[09/26 11:35:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 11:35:25 visual_prompt]: Best epoch 24: best metric: 0.385
[09/26 11:35:25 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 11:35:32 visual_prompt]: Epoch 25 / 100: avg data time: 5.09e-02, avg batch time: 0.5001, average train loss: 1.1702
[09/26 11:35:34 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1693, average loss: 1.9792
[09/26 11:35:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 91.50	
[09/26 11:35:34 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 11:35:40 visual_prompt]: Epoch 26 / 100: avg data time: 5.24e-02, avg batch time: 0.5019, average train loss: 1.2071
[09/26 11:35:42 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 1.7627
[09/26 11:35:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.00	
[09/26 11:35:42 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 11:35:49 visual_prompt]: Epoch 27 / 100: avg data time: 5.29e-02, avg batch time: 0.5013, average train loss: 1.2561
[09/26 11:35:50 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1690, average loss: 1.9550
[09/26 11:35:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 94.00	
[09/26 11:35:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 11:35:57 visual_prompt]: Epoch 28 / 100: avg data time: 4.99e-02, avg batch time: 0.4991, average train loss: 1.3227
[09/26 11:35:58 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1689, average loss: 1.9772
[09/26 11:35:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 11:35:58 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 11:36:05 visual_prompt]: Epoch 29 / 100: avg data time: 4.98e-02, avg batch time: 0.5000, average train loss: 1.0252
[09/26 11:36:07 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 2.0250
[09/26 11:36:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 11:36:07 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 11:36:14 visual_prompt]: Epoch 30 / 100: avg data time: 6.21e-02, avg batch time: 0.5113, average train loss: 0.9015
[09/26 11:36:15 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1689, average loss: 2.0674
[09/26 11:36:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.00	
[09/26 11:36:15 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 11:36:22 visual_prompt]: Epoch 31 / 100: avg data time: 6.14e-02, avg batch time: 0.5094, average train loss: 0.9313
[09/26 11:36:24 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1692, average loss: 2.2906
[09/26 11:36:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.00	
[09/26 11:36:24 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 11:36:31 visual_prompt]: Epoch 32 / 100: avg data time: 6.29e-02, avg batch time: 0.5374, average train loss: 1.0311
[09/26 11:36:32 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 2.0511
[09/26 11:36:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 11:36:32 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 11:36:39 visual_prompt]: Epoch 33 / 100: avg data time: 5.24e-02, avg batch time: 0.5036, average train loss: 0.8611
[09/26 11:36:41 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1690, average loss: 1.8909
[09/26 11:36:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 11:36:41 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 11:36:48 visual_prompt]: Epoch 34 / 100: avg data time: 5.35e-02, avg batch time: 0.5015, average train loss: 0.7101
[09/26 11:36:49 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 2.2857
[09/26 11:36:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.00	
[09/26 11:36:49 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 11:36:56 visual_prompt]: Epoch 35 / 100: avg data time: 5.31e-02, avg batch time: 0.5021, average train loss: 0.6806
[09/26 11:36:57 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1687, average loss: 1.9973
[09/26 11:36:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 11:36:57 visual_prompt]: Best epoch 35: best metric: 0.410
[09/26 11:36:57 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 11:37:04 visual_prompt]: Epoch 36 / 100: avg data time: 5.26e-02, avg batch time: 0.5020, average train loss: 0.5825
[09/26 11:37:06 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 2.2812
[09/26 11:37:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 11:37:06 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 11:37:12 visual_prompt]: Epoch 37 / 100: avg data time: 4.70e-02, avg batch time: 0.4958, average train loss: 0.6410
[09/26 11:37:14 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1689, average loss: 2.2642
[09/26 11:37:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 99.00	
[09/26 11:37:14 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 11:37:21 visual_prompt]: Epoch 38 / 100: avg data time: 4.76e-02, avg batch time: 0.4984, average train loss: 0.7090
[09/26 11:37:22 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1694, average loss: 2.4053
[09/26 11:37:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 11:37:22 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 11:37:29 visual_prompt]: Epoch 39 / 100: avg data time: 5.91e-02, avg batch time: 0.5081, average train loss: 0.6943
[09/26 11:37:30 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1692, average loss: 2.2260
[09/26 11:37:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.00	
[09/26 11:37:30 visual_prompt]: Best epoch 39: best metric: 0.435
[09/26 11:37:30 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 11:37:37 visual_prompt]: Epoch 40 / 100: avg data time: 5.50e-02, avg batch time: 0.5036, average train loss: 0.5822
[09/26 11:37:39 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1692, average loss: 2.4080
[09/26 11:37:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.50	
[09/26 11:37:39 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 11:37:46 visual_prompt]: Epoch 41 / 100: avg data time: 4.60e-02, avg batch time: 0.4968, average train loss: 0.6701
[09/26 11:37:47 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.1691, average loss: 2.4464
[09/26 11:37:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 94.00	
[09/26 11:37:47 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 11:37:54 visual_prompt]: Epoch 42 / 100: avg data time: 4.40e-02, avg batch time: 0.4929, average train loss: 0.5727
[09/26 11:37:55 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1691, average loss: 2.3576
[09/26 11:37:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.50	
[09/26 11:37:55 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 11:38:02 visual_prompt]: Epoch 43 / 100: avg data time: 5.79e-02, avg batch time: 0.5059, average train loss: 0.4196
[09/26 11:38:04 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 2.2886
[09/26 11:38:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.50	
[09/26 11:38:04 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 11:38:10 visual_prompt]: Epoch 44 / 100: avg data time: 4.36e-02, avg batch time: 0.4949, average train loss: 0.3008
[09/26 11:38:12 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1689, average loss: 2.5766
[09/26 11:38:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 11:38:12 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 11:38:19 visual_prompt]: Epoch 45 / 100: avg data time: 4.28e-02, avg batch time: 0.4949, average train loss: 0.2580
[09/26 11:38:20 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1688, average loss: 3.1815
[09/26 11:38:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 11:38:20 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 11:38:27 visual_prompt]: Epoch 46 / 100: avg data time: 5.42e-02, avg batch time: 0.5031, average train loss: 0.2799
[09/26 11:38:28 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1695, average loss: 3.1533
[09/26 11:38:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/26 11:38:28 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 11:38:35 visual_prompt]: Epoch 47 / 100: avg data time: 5.16e-02, avg batch time: 0.5017, average train loss: 0.3476
[09/26 11:38:36 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1692, average loss: 2.7355
[09/26 11:38:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 11:38:36 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 11:38:43 visual_prompt]: Epoch 48 / 100: avg data time: 5.14e-02, avg batch time: 0.5003, average train loss: 0.2942
[09/26 11:38:45 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1690, average loss: 2.9419
[09/26 11:38:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.00	
[09/26 11:38:45 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 11:38:52 visual_prompt]: Epoch 49 / 100: avg data time: 5.18e-02, avg batch time: 0.5005, average train loss: 0.2452
[09/26 11:38:53 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1690, average loss: 2.8156
[09/26 11:38:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.50	
[09/26 11:38:53 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 11:39:00 visual_prompt]: Epoch 50 / 100: avg data time: 5.53e-02, avg batch time: 0.5040, average train loss: 0.1819
[09/26 11:39:02 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1694, average loss: 3.2149
[09/26 11:39:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 98.50	
[09/26 11:39:02 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 11:39:08 visual_prompt]: Epoch 51 / 100: avg data time: 4.47e-02, avg batch time: 0.4933, average train loss: 0.1510
[09/26 11:39:10 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 3.3395
[09/26 11:39:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.50	
[09/26 11:39:10 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 11:39:17 visual_prompt]: Epoch 52 / 100: avg data time: 5.30e-02, avg batch time: 0.5020, average train loss: 0.1289
[09/26 11:39:18 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1690, average loss: 3.4514
[09/26 11:39:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 98.00	
[09/26 11:39:18 visual_prompt]: Best epoch 52: best metric: 0.445
[09/26 11:39:18 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 11:39:25 visual_prompt]: Epoch 53 / 100: avg data time: 5.72e-02, avg batch time: 0.5057, average train loss: 0.1004
[09/26 11:39:27 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 3.8853
[09/26 11:39:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 11:39:27 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 11:39:33 visual_prompt]: Epoch 54 / 100: avg data time: 5.36e-02, avg batch time: 0.5027, average train loss: 0.1299
[09/26 11:39:35 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1690, average loss: 4.2142
[09/26 11:39:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 98.50	
[09/26 11:39:35 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 11:39:42 visual_prompt]: Epoch 55 / 100: avg data time: 4.63e-02, avg batch time: 0.4961, average train loss: 0.1414
[09/26 11:39:43 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 3.8739
[09/26 11:39:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 99.00	
[09/26 11:39:43 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 11:39:50 visual_prompt]: Epoch 56 / 100: avg data time: 5.74e-02, avg batch time: 0.5069, average train loss: 0.1223
[09/26 11:39:52 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 3.7395
[09/26 11:39:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 11:39:52 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 11:39:59 visual_prompt]: Epoch 57 / 100: avg data time: 7.12e-02, avg batch time: 0.5194, average train loss: 0.1079
[09/26 11:40:00 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1693, average loss: 3.6511
[09/26 11:40:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 11:40:00 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 11:40:07 visual_prompt]: Epoch 58 / 100: avg data time: 5.34e-02, avg batch time: 0.5032, average train loss: 0.0808
[09/26 11:40:08 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1692, average loss: 3.8542
[09/26 11:40:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.50	
[09/26 11:40:08 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 11:40:15 visual_prompt]: Epoch 59 / 100: avg data time: 4.60e-02, avg batch time: 0.4957, average train loss: 0.1381
[09/26 11:40:17 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 4.0897
[09/26 11:40:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.00	
[09/26 11:40:17 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 11:40:23 visual_prompt]: Epoch 60 / 100: avg data time: 4.99e-02, avg batch time: 0.4984, average train loss: 0.1235
[09/26 11:40:25 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1691, average loss: 3.7479
[09/26 11:40:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 99.00	
[09/26 11:40:25 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 11:40:32 visual_prompt]: Epoch 61 / 100: avg data time: 4.74e-02, avg batch time: 0.4983, average train loss: 0.1018
[09/26 11:40:33 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1690, average loss: 4.1001
[09/26 11:40:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/26 11:40:33 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 11:40:40 visual_prompt]: Epoch 62 / 100: avg data time: 4.26e-02, avg batch time: 0.4940, average train loss: 0.0843
[09/26 11:40:41 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1692, average loss: 3.9799
[09/26 11:40:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.00	
[09/26 11:40:41 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 11:40:48 visual_prompt]: Epoch 63 / 100: avg data time: 4.95e-02, avg batch time: 0.4986, average train loss: 0.0513
[09/26 11:40:50 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1695, average loss: 4.3820
[09/26 11:40:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.00	
[09/26 11:40:50 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 11:40:56 visual_prompt]: Epoch 64 / 100: avg data time: 4.43e-02, avg batch time: 0.4940, average train loss: 0.0536
[09/26 11:40:58 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1691, average loss: 4.2834
[09/26 11:40:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.50	
[09/26 11:40:58 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 11:41:05 visual_prompt]: Epoch 65 / 100: avg data time: 4.84e-02, avg batch time: 0.4984, average train loss: 0.0367
[09/26 11:41:06 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1690, average loss: 4.9212
[09/26 11:41:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.00	
[09/26 11:41:06 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 11:41:13 visual_prompt]: Epoch 66 / 100: avg data time: 4.56e-02, avg batch time: 0.4964, average train loss: 0.0484
[09/26 11:41:14 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1689, average loss: 4.9198
[09/26 11:41:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.50	
[09/26 11:41:14 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 11:41:21 visual_prompt]: Epoch 67 / 100: avg data time: 4.24e-02, avg batch time: 0.4929, average train loss: 0.0271
[09/26 11:41:23 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1691, average loss: 4.7671
[09/26 11:41:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.00	
[09/26 11:41:23 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 11:41:30 visual_prompt]: Epoch 68 / 100: avg data time: 5.75e-02, avg batch time: 0.5065, average train loss: 0.0256
[09/26 11:41:31 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1693, average loss: 4.9775
[09/26 11:41:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/26 11:41:31 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 11:41:38 visual_prompt]: Epoch 69 / 100: avg data time: 4.66e-02, avg batch time: 0.4963, average train loss: 0.0092
[09/26 11:41:39 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 5.0347
[09/26 11:41:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 100.00	
[09/26 11:41:39 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 11:41:46 visual_prompt]: Epoch 70 / 100: avg data time: 4.86e-02, avg batch time: 0.4997, average train loss: 0.0135
[09/26 11:41:48 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1693, average loss: 5.1432
[09/26 11:41:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.00	
[09/26 11:41:48 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 11:41:55 visual_prompt]: Epoch 71 / 100: avg data time: 5.99e-02, avg batch time: 0.5095, average train loss: 0.0169
[09/26 11:41:56 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 5.3727
[09/26 11:41:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.00	
[09/26 11:41:56 visual_prompt]: Best epoch 71: best metric: 0.460
[09/26 11:41:56 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 11:42:03 visual_prompt]: Epoch 72 / 100: avg data time: 4.74e-02, avg batch time: 0.4976, average train loss: 0.0077
[09/26 11:42:04 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 5.1814
[09/26 11:42:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.00	
[09/26 11:42:04 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 11:42:11 visual_prompt]: Epoch 73 / 100: avg data time: 4.18e-02, avg batch time: 0.4926, average train loss: 0.0065
[09/26 11:42:12 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1696, average loss: 5.3303
[09/26 11:42:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.00	
[09/26 11:42:12 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 11:42:19 visual_prompt]: Epoch 74 / 100: avg data time: 5.91e-02, avg batch time: 0.5086, average train loss: 0.0300
[09/26 11:42:21 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1690, average loss: 4.9913
[09/26 11:42:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.00	
[09/26 11:42:21 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 11:42:28 visual_prompt]: Epoch 75 / 100: avg data time: 4.75e-02, avg batch time: 0.4966, average train loss: 0.0087
[09/26 11:42:29 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1693, average loss: 4.9500
[09/26 11:42:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.50	
[09/26 11:42:29 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 11:42:36 visual_prompt]: Epoch 76 / 100: avg data time: 5.51e-02, avg batch time: 0.5046, average train loss: 0.0100
[09/26 11:42:38 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 4.9965
[09/26 11:42:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 99.50	
[09/26 11:42:38 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 11:42:44 visual_prompt]: Epoch 77 / 100: avg data time: 5.15e-02, avg batch time: 0.5006, average train loss: 0.0103
[09/26 11:42:46 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 5.0236
[09/26 11:42:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.00	
[09/26 11:42:46 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 11:42:53 visual_prompt]: Epoch 78 / 100: avg data time: 5.87e-02, avg batch time: 0.5084, average train loss: 0.0101
[09/26 11:42:54 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1689, average loss: 5.0412
[09/26 11:42:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 99.50	
[09/26 11:42:54 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 11:43:01 visual_prompt]: Epoch 79 / 100: avg data time: 5.86e-02, avg batch time: 0.5072, average train loss: 0.0032
[09/26 11:43:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 5.1541
[09/26 11:43:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.50	
[09/26 11:43:03 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 11:43:10 visual_prompt]: Epoch 80 / 100: avg data time: 5.46e-02, avg batch time: 0.5038, average train loss: 0.0027
[09/26 11:43:11 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 5.1661
[09/26 11:43:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.50	
[09/26 11:43:11 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 11:43:18 visual_prompt]: Epoch 81 / 100: avg data time: 5.82e-02, avg batch time: 0.5067, average train loss: 0.0048
[09/26 11:43:19 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1691, average loss: 5.2278
[09/26 11:43:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 99.00	
[09/26 11:43:19 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 11:43:26 visual_prompt]: Epoch 82 / 100: avg data time: 5.16e-02, avg batch time: 0.5001, average train loss: 0.0031
[09/26 11:43:28 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1694, average loss: 5.2725
[09/26 11:43:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 99.00	
[09/26 11:43:28 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 11:43:35 visual_prompt]: Epoch 83 / 100: avg data time: 5.07e-02, avg batch time: 0.4994, average train loss: 0.0062
[09/26 11:43:36 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1695, average loss: 5.2616
[09/26 11:43:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.50	
[09/26 11:43:36 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 11:43:43 visual_prompt]: Epoch 84 / 100: avg data time: 4.31e-02, avg batch time: 0.4946, average train loss: 0.0022
[09/26 11:43:44 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1694, average loss: 5.3016
[09/26 11:43:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.50	
[09/26 11:43:44 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 11:43:51 visual_prompt]: Epoch 85 / 100: avg data time: 5.08e-02, avg batch time: 0.4997, average train loss: 0.0018
[09/26 11:43:53 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1694, average loss: 5.3532
[09/26 11:43:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 99.00	
[09/26 11:43:53 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 11:43:59 visual_prompt]: Epoch 86 / 100: avg data time: 5.44e-02, avg batch time: 0.5030, average train loss: 0.0015
[09/26 11:44:01 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.1689, average loss: 5.3840
[09/26 11:44:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 99.00	
[09/26 11:44:01 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 11:44:08 visual_prompt]: Epoch 87 / 100: avg data time: 5.94e-02, avg batch time: 0.5087, average train loss: 0.0018
[09/26 11:44:09 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1691, average loss: 5.4001
[09/26 11:44:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 99.00	
[09/26 11:44:09 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 11:44:16 visual_prompt]: Epoch 88 / 100: avg data time: 5.31e-02, avg batch time: 0.5022, average train loss: 0.0017
[09/26 11:44:18 visual_prompt]: Inference (val):avg data time: 4.51e-05, avg batch time: 0.1690, average loss: 5.4101
[09/26 11:44:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:44:18 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 11:44:25 visual_prompt]: Epoch 89 / 100: avg data time: 6.27e-02, avg batch time: 0.5124, average train loss: 0.0022
[09/26 11:44:26 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1704, average loss: 5.4365
[09/26 11:44:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:44:26 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 11:44:33 visual_prompt]: Epoch 90 / 100: avg data time: 5.84e-02, avg batch time: 0.5073, average train loss: 0.0020
[09/26 11:44:35 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1692, average loss: 5.4325
[09/26 11:44:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:44:35 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 11:44:41 visual_prompt]: Epoch 91 / 100: avg data time: 4.40e-02, avg batch time: 0.4939, average train loss: 0.0022
[09/26 11:44:43 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1689, average loss: 5.4235
[09/26 11:44:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:44:43 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 11:44:49 visual_prompt]: Epoch 92 / 100: avg data time: 4.70e-02, avg batch time: 0.4976, average train loss: 0.0013
[09/26 11:44:51 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1688, average loss: 5.4255
[09/26 11:44:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:44:51 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 11:44:58 visual_prompt]: Epoch 93 / 100: avg data time: 5.79e-02, avg batch time: 0.5070, average train loss: 0.0010
[09/26 11:44:59 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1696, average loss: 5.4291
[09/26 11:44:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:44:59 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 11:45:06 visual_prompt]: Epoch 94 / 100: avg data time: 4.77e-02, avg batch time: 0.4980, average train loss: 0.0015
[09/26 11:45:08 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 5.4318
[09/26 11:45:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:45:08 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 11:45:14 visual_prompt]: Epoch 95 / 100: avg data time: 5.92e-02, avg batch time: 0.5077, average train loss: 0.0016
[09/26 11:45:16 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 5.4344
[09/26 11:45:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:45:16 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 11:45:23 visual_prompt]: Epoch 96 / 100: avg data time: 6.09e-02, avg batch time: 0.5101, average train loss: 0.0015
[09/26 11:45:24 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1690, average loss: 5.4356
[09/26 11:45:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:45:24 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 11:45:31 visual_prompt]: Epoch 97 / 100: avg data time: 5.27e-02, avg batch time: 0.5019, average train loss: 0.0015
[09/26 11:45:33 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1690, average loss: 5.4363
[09/26 11:45:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:45:33 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 11:45:40 visual_prompt]: Epoch 98 / 100: avg data time: 4.86e-02, avg batch time: 0.4991, average train loss: 0.0011
[09/26 11:45:41 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1693, average loss: 5.4368
[09/26 11:45:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:45:41 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 11:45:48 visual_prompt]: Epoch 99 / 100: avg data time: 6.33e-02, avg batch time: 0.5117, average train loss: 0.0022
[09/26 11:45:50 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1696, average loss: 5.4371
[09/26 11:45:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:45:50 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 11:45:56 visual_prompt]: Epoch 100 / 100: avg data time: 4.72e-02, avg batch time: 0.4971, average train loss: 0.0016
[09/26 11:45:58 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1690, average loss: 5.4372
[09/26 11:45:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 11:45:58 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:45:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:45:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:45:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:45:58 visual_prompt]: Training with config:
[09/26 11:45:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:45:58 visual_prompt]: Loading training data...
[09/26 11:45:58 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 11:45:59 visual_prompt]: Number of images: 800
[09/26 11:45:59 visual_prompt]: Number of classes: 6 / 6
[09/26 11:45:59 visual_prompt]: Loading validation data...
[09/26 11:45:59 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 11:46:00 visual_prompt]: Number of images: 200
[09/26 11:46:00 visual_prompt]: Number of classes: 6 / 6
[09/26 11:46:00 visual_prompt]: Constructing models...
[09/26 11:46:02 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 11:46:02 visual_prompt]: tuned percent:0.540
[09/26 11:46:02 visual_prompt]: Device used for model: 0
[09/26 11:46:02 visual_prompt]: Setting up Evaluator...
[09/26 11:46:02 visual_prompt]: Setting up Trainer...
[09/26 11:46:02 visual_prompt]: 	Setting up the optimizer...
[09/26 11:46:02 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:46:09 visual_prompt]: Epoch 1 / 100: avg data time: 6.31e-02, avg batch time: 0.5109, average train loss: 2.9851
[09/26 11:46:11 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1687, average loss: 2.9268
[09/26 11:46:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 11:46:11 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 11:46:11 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 11:46:17 visual_prompt]: Epoch 2 / 100: avg data time: 5.20e-02, avg batch time: 0.5007, average train loss: 2.8327
[09/26 11:46:19 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1682, average loss: 2.1678
[09/26 11:46:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 11:46:19 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 11:46:26 visual_prompt]: Epoch 3 / 100: avg data time: 5.48e-02, avg batch time: 0.5020, average train loss: 1.8983
[09/26 11:46:27 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1684, average loss: 1.8437
[09/26 11:46:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:46:27 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 11:46:34 visual_prompt]: Epoch 4 / 100: avg data time: 5.06e-02, avg batch time: 0.4995, average train loss: 1.8702
[09/26 11:46:35 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1689, average loss: 1.8580
[09/26 11:46:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 11:46:35 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 11:46:42 visual_prompt]: Epoch 5 / 100: avg data time: 5.38e-02, avg batch time: 0.5012, average train loss: 1.8282
[09/26 11:46:44 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1687, average loss: 1.8955
[09/26 11:46:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:46:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 11:46:51 visual_prompt]: Epoch 6 / 100: avg data time: 5.90e-02, avg batch time: 0.5063, average train loss: 1.8233
[09/26 11:46:52 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1687, average loss: 1.8775
[09/26 11:46:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 11:46:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 11:46:59 visual_prompt]: Epoch 7 / 100: avg data time: 4.82e-02, avg batch time: 0.4972, average train loss: 1.8075
[09/26 11:47:00 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1691, average loss: 1.8518
[09/26 11:47:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 11:47:00 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 11:47:07 visual_prompt]: Epoch 8 / 100: avg data time: 6.36e-02, avg batch time: 0.5131, average train loss: 1.7996
[09/26 11:47:09 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 1.9157
[09/26 11:47:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:47:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 11:47:16 visual_prompt]: Epoch 9 / 100: avg data time: 4.82e-02, avg batch time: 0.4966, average train loss: 1.8111
[09/26 11:47:17 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1690, average loss: 1.8119
[09/26 11:47:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 11:47:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 11:47:24 visual_prompt]: Epoch 10 / 100: avg data time: 4.16e-02, avg batch time: 0.4918, average train loss: 1.8102
[09/26 11:47:25 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 1.8478
[09/26 11:47:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:47:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 11:47:32 visual_prompt]: Epoch 11 / 100: avg data time: 4.65e-02, avg batch time: 0.4957, average train loss: 2.3151
[09/26 11:47:34 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1688, average loss: 2.5187
[09/26 11:47:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 11:47:34 visual_prompt]: Best epoch 11: best metric: 0.205
[09/26 11:47:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 11:47:40 visual_prompt]: Epoch 12 / 100: avg data time: 5.66e-02, avg batch time: 0.5047, average train loss: 2.2360
[09/26 11:47:42 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1688, average loss: 2.2763
[09/26 11:47:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:47:42 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 11:47:49 visual_prompt]: Epoch 13 / 100: avg data time: 4.69e-02, avg batch time: 0.4969, average train loss: 1.9536
[09/26 11:47:50 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1691, average loss: 1.8421
[09/26 11:47:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 11:47:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 11:47:57 visual_prompt]: Epoch 14 / 100: avg data time: 5.99e-02, avg batch time: 0.5090, average train loss: 1.9384
[09/26 11:47:59 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1688, average loss: 2.1964
[09/26 11:47:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 11:47:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 11:48:05 visual_prompt]: Epoch 15 / 100: avg data time: 5.26e-02, avg batch time: 0.5002, average train loss: 1.9882
[09/26 11:48:07 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1686, average loss: 1.9023
[09/26 11:48:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 11:48:07 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 11:48:14 visual_prompt]: Epoch 16 / 100: avg data time: 5.18e-02, avg batch time: 0.5013, average train loss: 1.8824
[09/26 11:48:15 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1688, average loss: 2.0811
[09/26 11:48:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 85.50	
[09/26 11:48:15 visual_prompt]: Best epoch 16: best metric: 0.210
[09/26 11:48:15 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 11:48:22 visual_prompt]: Epoch 17 / 100: avg data time: 5.55e-02, avg batch time: 0.5035, average train loss: 1.8937
[09/26 11:48:24 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 1.8567
[09/26 11:48:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 11:48:24 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 11:48:31 visual_prompt]: Epoch 18 / 100: avg data time: 6.34e-02, avg batch time: 0.5112, average train loss: 1.8341
[09/26 11:48:32 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1686, average loss: 1.8435
[09/26 11:48:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 82.50	
[09/26 11:48:32 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 11:48:39 visual_prompt]: Epoch 19 / 100: avg data time: 5.99e-02, avg batch time: 0.5088, average train loss: 1.8018
[09/26 11:48:41 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1692, average loss: 3.0860
[09/26 11:48:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 11:48:41 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 11:48:47 visual_prompt]: Epoch 20 / 100: avg data time: 5.84e-02, avg batch time: 0.5073, average train loss: 2.0353
[09/26 11:48:49 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1687, average loss: 1.9697
[09/26 11:48:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 11:48:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 11:48:56 visual_prompt]: Epoch 21 / 100: avg data time: 6.11e-02, avg batch time: 0.5106, average train loss: 1.9831
[09/26 11:48:57 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1691, average loss: 1.9307
[09/26 11:48:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 82.50	
[09/26 11:48:57 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 11:49:04 visual_prompt]: Epoch 22 / 100: avg data time: 5.85e-02, avg batch time: 0.5092, average train loss: 2.0119
[09/26 11:49:06 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1686, average loss: 2.1617
[09/26 11:49:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:49:06 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 11:49:13 visual_prompt]: Epoch 23 / 100: avg data time: 5.71e-02, avg batch time: 0.5049, average train loss: 2.4277
[09/26 11:49:14 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1685, average loss: 2.0548
[09/26 11:49:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:49:14 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 11:49:21 visual_prompt]: Epoch 24 / 100: avg data time: 5.77e-02, avg batch time: 0.5054, average train loss: 1.9452
[09/26 11:49:23 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1687, average loss: 2.1442
[09/26 11:49:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 11:49:23 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 11:49:29 visual_prompt]: Epoch 25 / 100: avg data time: 4.27e-02, avg batch time: 0.4908, average train loss: 3.0422
[09/26 11:49:31 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 7.5669
[09/26 11:49:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 79.50	
[09/26 11:49:31 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 11:49:37 visual_prompt]: Epoch 26 / 100: avg data time: 4.45e-02, avg batch time: 0.4942, average train loss: 2.9383
[09/26 11:49:39 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1692, average loss: 2.0635
[09/26 11:49:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 86.00	
[09/26 11:49:39 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 11:49:46 visual_prompt]: Epoch 27 / 100: avg data time: 5.89e-02, avg batch time: 0.5069, average train loss: 2.1332
[09/26 11:49:47 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 2.5467
[09/26 11:49:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 11:49:47 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 11:49:54 visual_prompt]: Epoch 28 / 100: avg data time: 5.63e-02, avg batch time: 0.5045, average train loss: 2.1551
[09/26 11:49:56 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1687, average loss: 1.9079
[09/26 11:49:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 11:49:56 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 11:50:03 visual_prompt]: Epoch 29 / 100: avg data time: 5.71e-02, avg batch time: 0.5045, average train loss: 1.8396
[09/26 11:50:04 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 1.9268
[09/26 11:50:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:50:04 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 11:50:11 visual_prompt]: Epoch 30 / 100: avg data time: 5.70e-02, avg batch time: 0.5050, average train loss: 1.9461
[09/26 11:50:12 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1695, average loss: 2.1027
[09/26 11:50:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 11:50:12 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 11:50:19 visual_prompt]: Epoch 31 / 100: avg data time: 6.00e-02, avg batch time: 0.5088, average train loss: 1.8997
[09/26 11:50:21 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1692, average loss: 2.5993
[09/26 11:50:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 11:50:21 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 11:50:27 visual_prompt]: Epoch 32 / 100: avg data time: 4.67e-02, avg batch time: 0.4950, average train loss: 1.9836
[09/26 11:50:29 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1692, average loss: 1.9955
[09/26 11:50:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 11:50:29 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 11:50:36 visual_prompt]: Epoch 33 / 100: avg data time: 5.84e-02, avg batch time: 0.5061, average train loss: 2.0017
[09/26 11:50:37 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1689, average loss: 2.4067
[09/26 11:50:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:50:37 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 11:50:44 visual_prompt]: Epoch 34 / 100: avg data time: 5.41e-02, avg batch time: 0.5024, average train loss: 2.0060
[09/26 11:50:46 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 1.8395
[09/26 11:50:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:50:46 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 11:50:53 visual_prompt]: Epoch 35 / 100: avg data time: 6.18e-02, avg batch time: 0.5101, average train loss: 1.8469
[09/26 11:50:54 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1688, average loss: 2.0283
[09/26 11:50:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 11:50:54 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 11:51:01 visual_prompt]: Epoch 36 / 100: avg data time: 5.54e-02, avg batch time: 0.5043, average train loss: 1.8806
[09/26 11:51:02 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1688, average loss: 1.8803
[09/26 11:51:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:51:02 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 11:51:09 visual_prompt]: Epoch 37 / 100: avg data time: 4.32e-02, avg batch time: 0.4935, average train loss: 1.8031
[09/26 11:51:11 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1689, average loss: 1.9574
[09/26 11:51:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:51:11 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 11:51:17 visual_prompt]: Epoch 38 / 100: avg data time: 4.38e-02, avg batch time: 0.4930, average train loss: 1.8459
[09/26 11:51:19 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1693, average loss: 1.9363
[09/26 11:51:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 11:51:19 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 11:51:25 visual_prompt]: Epoch 39 / 100: avg data time: 4.26e-02, avg batch time: 0.4930, average train loss: 1.8535
[09/26 11:51:27 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1688, average loss: 2.0701
[09/26 11:51:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:51:27 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 11:51:34 visual_prompt]: Epoch 40 / 100: avg data time: 5.07e-02, avg batch time: 0.4997, average train loss: 1.8489
[09/26 11:51:35 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1689, average loss: 1.9061
[09/26 11:51:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:51:35 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 11:51:42 visual_prompt]: Epoch 41 / 100: avg data time: 5.06e-02, avg batch time: 0.4992, average train loss: 1.8146
[09/26 11:51:44 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1692, average loss: 1.8415
[09/26 11:51:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:51:44 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 11:51:51 visual_prompt]: Epoch 42 / 100: avg data time: 6.12e-02, avg batch time: 0.5092, average train loss: 1.8521
[09/26 11:51:52 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1694, average loss: 1.8540
[09/26 11:51:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:51:52 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 11:51:59 visual_prompt]: Epoch 43 / 100: avg data time: 5.38e-02, avg batch time: 0.5031, average train loss: 1.8449
[09/26 11:52:00 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1686, average loss: 1.9497
[09/26 11:52:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 11:52:00 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 11:52:07 visual_prompt]: Epoch 44 / 100: avg data time: 5.60e-02, avg batch time: 0.5044, average train loss: 1.8126
[09/26 11:52:09 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1690, average loss: 1.8669
[09/26 11:52:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 11:52:09 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 11:52:16 visual_prompt]: Epoch 45 / 100: avg data time: 5.39e-02, avg batch time: 0.5043, average train loss: 1.7977
[09/26 11:52:17 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1687, average loss: 1.8303
[09/26 11:52:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:52:17 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 11:52:24 visual_prompt]: Epoch 46 / 100: avg data time: 5.61e-02, avg batch time: 0.5049, average train loss: 1.8290
[09/26 11:52:26 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1694, average loss: 1.8215
[09/26 11:52:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 11:52:26 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 11:52:32 visual_prompt]: Epoch 47 / 100: avg data time: 4.58e-02, avg batch time: 0.4965, average train loss: 1.8293
[09/26 11:52:34 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1687, average loss: 1.8873
[09/26 11:52:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 11:52:34 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 11:52:41 visual_prompt]: Epoch 48 / 100: avg data time: 5.59e-02, avg batch time: 0.5062, average train loss: 1.8444
[09/26 11:52:42 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1685, average loss: 1.8283
[09/26 11:52:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:52:42 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 11:52:49 visual_prompt]: Epoch 49 / 100: avg data time: 5.68e-02, avg batch time: 0.5065, average train loss: 1.8007
[09/26 11:52:51 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1686, average loss: 1.8714
[09/26 11:52:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.50	
[09/26 11:52:51 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 11:52:58 visual_prompt]: Epoch 50 / 100: avg data time: 4.87e-02, avg batch time: 0.4966, average train loss: 1.8371
[09/26 11:52:59 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1685, average loss: 1.8530
[09/26 11:52:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:52:59 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 11:53:06 visual_prompt]: Epoch 51 / 100: avg data time: 4.64e-02, avg batch time: 0.4955, average train loss: 1.8404
[09/26 11:53:07 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 1.9188
[09/26 11:53:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 11:53:07 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 11:53:14 visual_prompt]: Epoch 52 / 100: avg data time: 4.61e-02, avg batch time: 0.4941, average train loss: 1.8082
[09/26 11:53:16 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1689, average loss: 1.9505
[09/26 11:53:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:53:16 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 11:53:22 visual_prompt]: Epoch 53 / 100: avg data time: 5.40e-02, avg batch time: 0.5040, average train loss: 1.8331
[09/26 11:53:24 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1691, average loss: 1.8262
[09/26 11:53:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 84.00	
[09/26 11:53:24 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 11:53:31 visual_prompt]: Epoch 54 / 100: avg data time: 5.70e-02, avg batch time: 0.5051, average train loss: 1.8164
[09/26 11:53:32 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1689, average loss: 1.8590
[09/26 11:53:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:53:32 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 11:53:39 visual_prompt]: Epoch 55 / 100: avg data time: 5.72e-02, avg batch time: 0.5055, average train loss: 1.8154
[09/26 11:53:41 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1689, average loss: 1.9304
[09/26 11:53:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 11:53:41 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 11:53:48 visual_prompt]: Epoch 56 / 100: avg data time: 5.73e-02, avg batch time: 0.5052, average train loss: 1.8179
[09/26 11:53:49 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1687, average loss: 1.8743
[09/26 11:53:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 11:53:49 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 11:53:56 visual_prompt]: Epoch 57 / 100: avg data time: 5.21e-02, avg batch time: 0.5019, average train loss: 1.8089
[09/26 11:53:58 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1688, average loss: 1.8580
[09/26 11:53:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:53:58 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 11:54:04 visual_prompt]: Epoch 58 / 100: avg data time: 5.89e-02, avg batch time: 0.5071, average train loss: 1.8466
[09/26 11:54:06 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1689, average loss: 1.8813
[09/26 11:54:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 11:54:06 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 11:54:13 visual_prompt]: Epoch 59 / 100: avg data time: 4.81e-02, avg batch time: 0.4977, average train loss: 1.8349
[09/26 11:54:14 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1689, average loss: 1.9210
[09/26 11:54:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:54:14 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 11:54:21 visual_prompt]: Epoch 60 / 100: avg data time: 6.13e-02, avg batch time: 0.5101, average train loss: 1.8201
[09/26 11:54:23 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 1.8918
[09/26 11:54:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:54:23 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 11:54:29 visual_prompt]: Epoch 61 / 100: avg data time: 4.38e-02, avg batch time: 0.4942, average train loss: 1.8115
[09/26 11:54:31 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1687, average loss: 1.8545
[09/26 11:54:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:54:31 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 11:54:38 visual_prompt]: Epoch 62 / 100: avg data time: 5.88e-02, avg batch time: 0.5076, average train loss: 1.7796
[09/26 11:54:39 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1685, average loss: 1.8369
[09/26 11:54:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:54:39 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 11:54:46 visual_prompt]: Epoch 63 / 100: avg data time: 4.27e-02, avg batch time: 0.4911, average train loss: 1.7864
[09/26 11:54:47 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 1.8186
[09/26 11:54:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 11:54:47 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 11:54:54 visual_prompt]: Epoch 64 / 100: avg data time: 4.27e-02, avg batch time: 0.4947, average train loss: 1.7989
[09/26 11:54:56 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1691, average loss: 1.8556
[09/26 11:54:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:54:56 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 11:55:02 visual_prompt]: Epoch 65 / 100: avg data time: 5.39e-02, avg batch time: 0.5029, average train loss: 1.8174
[09/26 11:55:04 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1688, average loss: 1.8511
[09/26 11:55:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:55:04 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 11:55:11 visual_prompt]: Epoch 66 / 100: avg data time: 5.41e-02, avg batch time: 0.5024, average train loss: 1.8250
[09/26 11:55:12 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1694, average loss: 1.8300
[09/26 11:55:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 11:55:12 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 11:55:19 visual_prompt]: Epoch 67 / 100: avg data time: 5.50e-02, avg batch time: 0.5034, average train loss: 1.7966
[09/26 11:55:21 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1689, average loss: 1.8381
[09/26 11:55:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:55:21 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 11:55:27 visual_prompt]: Epoch 68 / 100: avg data time: 5.57e-02, avg batch time: 0.5047, average train loss: 1.7960
[09/26 11:55:29 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1692, average loss: 1.8472
[09/26 11:55:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:55:29 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 11:55:36 visual_prompt]: Epoch 69 / 100: avg data time: 4.34e-02, avg batch time: 0.4931, average train loss: 1.7904
[09/26 11:55:37 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1695, average loss: 1.8450
[09/26 11:55:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:55:37 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 11:55:44 visual_prompt]: Epoch 70 / 100: avg data time: 5.69e-02, avg batch time: 0.5056, average train loss: 1.7984
[09/26 11:55:46 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 1.8229
[09/26 11:55:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:55:46 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 11:55:52 visual_prompt]: Epoch 71 / 100: avg data time: 4.65e-02, avg batch time: 0.4961, average train loss: 1.7916
[09/26 11:55:54 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1689, average loss: 1.8426
[09/26 11:55:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:55:54 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 11:56:01 visual_prompt]: Epoch 72 / 100: avg data time: 4.81e-02, avg batch time: 0.4973, average train loss: 1.7724
[09/26 11:56:02 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 1.8571
[09/26 11:56:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:56:02 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 11:56:09 visual_prompt]: Epoch 73 / 100: avg data time: 4.47e-02, avg batch time: 0.4956, average train loss: 1.8055
[09/26 11:56:10 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1690, average loss: 1.8628
[09/26 11:56:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:56:10 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 11:56:17 visual_prompt]: Epoch 74 / 100: avg data time: 4.67e-02, avg batch time: 0.4971, average train loss: 1.8042
[09/26 11:56:18 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 1.8332
[09/26 11:56:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 11:56:18 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 11:56:25 visual_prompt]: Epoch 75 / 100: avg data time: 5.89e-02, avg batch time: 0.5064, average train loss: 1.7823
[09/26 11:56:27 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1691, average loss: 1.8846
[09/26 11:56:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 11:56:27 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 11:56:34 visual_prompt]: Epoch 76 / 100: avg data time: 6.21e-02, avg batch time: 0.5117, average train loss: 1.8067
[09/26 11:56:35 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1692, average loss: 1.8418
[09/26 11:56:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 84.00	
[09/26 11:56:35 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 11:56:42 visual_prompt]: Epoch 77 / 100: avg data time: 5.34e-02, avg batch time: 0.5024, average train loss: 1.7900
[09/26 11:56:44 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1692, average loss: 1.8037
[09/26 11:56:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:56:44 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 11:56:50 visual_prompt]: Epoch 78 / 100: avg data time: 5.76e-02, avg batch time: 0.5079, average train loss: 1.7691
[09/26 11:56:52 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1687, average loss: 1.8065
[09/26 11:56:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 11:56:52 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 11:56:59 visual_prompt]: Epoch 79 / 100: avg data time: 4.81e-02, avg batch time: 0.4970, average train loss: 1.7582
[09/26 11:57:00 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1691, average loss: 1.7828
[09/26 11:57:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 86.00	
[09/26 11:57:00 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 11:57:07 visual_prompt]: Epoch 80 / 100: avg data time: 5.59e-02, avg batch time: 0.5050, average train loss: 1.7146
[09/26 11:57:09 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1688, average loss: 1.8480
[09/26 11:57:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 11:57:09 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 11:57:16 visual_prompt]: Epoch 81 / 100: avg data time: 6.11e-02, avg batch time: 0.5100, average train loss: 1.7740
[09/26 11:57:17 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1689, average loss: 1.8346
[09/26 11:57:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:57:17 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 11:57:24 visual_prompt]: Epoch 82 / 100: avg data time: 5.86e-02, avg batch time: 0.5081, average train loss: 1.7587
[09/26 11:57:25 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1687, average loss: 1.7981
[09/26 11:57:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 84.00	
[09/26 11:57:25 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 11:57:32 visual_prompt]: Epoch 83 / 100: avg data time: 4.86e-02, avg batch time: 0.4985, average train loss: 1.7627
[09/26 11:57:34 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1689, average loss: 1.7841
[09/26 11:57:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 84.00	
[09/26 11:57:34 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 11:57:41 visual_prompt]: Epoch 84 / 100: avg data time: 6.06e-02, avg batch time: 0.5091, average train loss: 1.7576
[09/26 11:57:42 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1690, average loss: 1.7875
[09/26 11:57:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 84.00	
[09/26 11:57:42 visual_prompt]: Best epoch 84: best metric: 0.265
[09/26 11:57:42 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 11:57:49 visual_prompt]: Epoch 85 / 100: avg data time: 4.77e-02, avg batch time: 0.4974, average train loss: 1.7577
[09/26 11:57:51 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1688, average loss: 1.8017
[09/26 11:57:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:57:51 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 11:57:57 visual_prompt]: Epoch 86 / 100: avg data time: 5.95e-02, avg batch time: 0.5082, average train loss: 1.7197
[09/26 11:57:59 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1689, average loss: 1.6894
[09/26 11:57:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 84.00	
[09/26 11:57:59 visual_prompt]: Best epoch 86: best metric: 0.270
[09/26 11:57:59 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 11:58:06 visual_prompt]: Epoch 87 / 100: avg data time: 4.53e-02, avg batch time: 0.4948, average train loss: 1.7234
[09/26 11:58:07 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 1.7956
[09/26 11:58:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:58:07 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 11:58:14 visual_prompt]: Epoch 88 / 100: avg data time: 5.62e-02, avg batch time: 0.5048, average train loss: 1.7705
[09/26 11:58:16 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1688, average loss: 1.7972
[09/26 11:58:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:58:16 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 11:58:22 visual_prompt]: Epoch 89 / 100: avg data time: 5.02e-02, avg batch time: 0.4988, average train loss: 1.7725
[09/26 11:58:24 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1688, average loss: 1.8174
[09/26 11:58:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:58:24 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 11:58:31 visual_prompt]: Epoch 90 / 100: avg data time: 6.51e-02, avg batch time: 0.5126, average train loss: 1.7662
[09/26 11:58:32 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1690, average loss: 1.8065
[09/26 11:58:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 11:58:32 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 11:58:39 visual_prompt]: Epoch 91 / 100: avg data time: 5.62e-02, avg batch time: 0.5045, average train loss: 1.7588
[09/26 11:58:41 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1687, average loss: 1.8007
[09/26 11:58:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 84.00	
[09/26 11:58:41 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 11:58:48 visual_prompt]: Epoch 92 / 100: avg data time: 5.75e-02, avg batch time: 0.5068, average train loss: 1.7244
[09/26 11:58:49 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 1.7457
[09/26 11:58:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 11:58:49 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 11:58:56 visual_prompt]: Epoch 93 / 100: avg data time: 4.82e-02, avg batch time: 0.4989, average train loss: 1.6084
[09/26 11:58:57 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 1.6129
[09/26 11:58:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 94.50	
[09/26 11:58:57 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 11:59:05 visual_prompt]: Epoch 94 / 100: avg data time: 5.98e-02, avg batch time: 0.5079, average train loss: 1.6164
[09/26 11:59:06 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1691, average loss: 1.7852
[09/26 11:59:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 88.50	
[09/26 11:59:06 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 11:59:13 visual_prompt]: Epoch 95 / 100: avg data time: 5.07e-02, avg batch time: 0.4996, average train loss: 1.6255
[09/26 11:59:14 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1689, average loss: 1.6254
[09/26 11:59:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 95.00	
[09/26 11:59:14 visual_prompt]: Best epoch 95: best metric: 0.275
[09/26 11:59:14 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 11:59:21 visual_prompt]: Epoch 96 / 100: avg data time: 4.72e-02, avg batch time: 0.4971, average train loss: 1.5173
[09/26 11:59:23 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 1.5233
[09/26 11:59:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 97.00	
[09/26 11:59:23 visual_prompt]: Best epoch 96: best metric: 0.295
[09/26 11:59:23 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 11:59:30 visual_prompt]: Epoch 97 / 100: avg data time: 5.20e-02, avg batch time: 0.4999, average train loss: 1.4154
[09/26 11:59:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 1.4814
[09/26 11:59:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 97.00	
[09/26 11:59:31 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 11:59:38 visual_prompt]: Epoch 98 / 100: avg data time: 5.55e-02, avg batch time: 0.5043, average train loss: 1.3702
[09/26 11:59:39 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1691, average loss: 1.4632
[09/26 11:59:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 96.50	
[09/26 11:59:39 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 11:59:46 visual_prompt]: Epoch 99 / 100: avg data time: 6.10e-02, avg batch time: 0.5100, average train loss: 1.3465
[09/26 11:59:48 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1694, average loss: 1.4398
[09/26 11:59:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 97.00	
[09/26 11:59:48 visual_prompt]: Best epoch 99: best metric: 0.300
[09/26 11:59:48 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 11:59:55 visual_prompt]: Epoch 100 / 100: avg data time: 4.59e-02, avg batch time: 0.4959, average train loss: 1.3342
[09/26 11:59:56 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1692, average loss: 1.4401
[09/26 11:59:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 96.00	
[09/26 11:59:56 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:59:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:59:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:59:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:59:56 visual_prompt]: Training with config:
[09/26 11:59:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:59:56 visual_prompt]: Loading training data...
[09/26 11:59:56 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 11:59:58 visual_prompt]: Number of images: 800
[09/26 11:59:58 visual_prompt]: Number of classes: 6 / 6
[09/26 11:59:58 visual_prompt]: Loading validation data...
[09/26 11:59:58 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 11:59:58 visual_prompt]: Number of images: 200
[09/26 11:59:58 visual_prompt]: Number of classes: 6 / 6
[09/26 11:59:58 visual_prompt]: Constructing models...
[09/26 12:00:00 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 12:00:00 visual_prompt]: tuned percent:0.540
[09/26 12:00:00 visual_prompt]: Device used for model: 0
[09/26 12:00:00 visual_prompt]: Setting up Evaluator...
[09/26 12:00:00 visual_prompt]: Setting up Trainer...
[09/26 12:00:00 visual_prompt]: 	Setting up the optimizer...
[09/26 12:00:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:00:07 visual_prompt]: Epoch 1 / 100: avg data time: 4.94e-02, avg batch time: 0.4978, average train loss: 2.9617
[09/26 12:00:09 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1679, average loss: 2.9268
[09/26 12:00:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 12:00:09 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 12:00:09 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 12:00:16 visual_prompt]: Epoch 2 / 100: avg data time: 5.43e-02, avg batch time: 0.5011, average train loss: 3.1366
[09/26 12:00:17 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1686, average loss: 2.4709
[09/26 12:00:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 12:00:17 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 12:00:24 visual_prompt]: Epoch 3 / 100: avg data time: 4.82e-02, avg batch time: 0.4964, average train loss: 2.0460
[09/26 12:00:25 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1687, average loss: 1.9356
[09/26 12:00:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 86.50	
[09/26 12:00:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 12:00:32 visual_prompt]: Epoch 4 / 100: avg data time: 5.60e-02, avg batch time: 0.5029, average train loss: 1.8389
[09/26 12:00:34 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1687, average loss: 1.8891
[09/26 12:00:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 12:00:34 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 12:00:40 visual_prompt]: Epoch 5 / 100: avg data time: 4.50e-02, avg batch time: 0.4947, average train loss: 1.8169
[09/26 12:00:42 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1686, average loss: 2.0854
[09/26 12:00:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:00:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 12:00:49 visual_prompt]: Epoch 6 / 100: avg data time: 5.98e-02, avg batch time: 0.5080, average train loss: 1.8602
[09/26 12:00:50 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1688, average loss: 1.9042
[09/26 12:00:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 84.00	
[09/26 12:00:50 visual_prompt]: Best epoch 6: best metric: 0.220
[09/26 12:00:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 12:00:57 visual_prompt]: Epoch 7 / 100: avg data time: 5.67e-02, avg batch time: 0.5053, average train loss: 1.8169
[09/26 12:00:59 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1688, average loss: 1.7707
[09/26 12:00:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 91.00	
[09/26 12:00:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 12:01:06 visual_prompt]: Epoch 8 / 100: avg data time: 6.16e-02, avg batch time: 0.5090, average train loss: 1.7815
[09/26 12:01:07 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1690, average loss: 1.7646
[09/26 12:01:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 84.00	
[09/26 12:01:07 visual_prompt]: Best epoch 8: best metric: 0.240
[09/26 12:01:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 12:01:14 visual_prompt]: Epoch 9 / 100: avg data time: 5.24e-02, avg batch time: 0.5008, average train loss: 1.7709
[09/26 12:01:16 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1690, average loss: 1.9736
[09/26 12:01:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 82.00	
[09/26 12:01:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 12:01:22 visual_prompt]: Epoch 10 / 100: avg data time: 5.89e-02, avg batch time: 0.5072, average train loss: 1.7792
[09/26 12:01:24 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1683, average loss: 2.1063
[09/26 12:01:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 83.50	
[09/26 12:01:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 12:01:31 visual_prompt]: Epoch 11 / 100: avg data time: 5.21e-02, avg batch time: 0.5007, average train loss: 1.8372
[09/26 12:01:32 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1689, average loss: 1.7366
[09/26 12:01:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 93.00	
[09/26 12:01:32 visual_prompt]: Best epoch 11: best metric: 0.250
[09/26 12:01:32 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 12:01:39 visual_prompt]: Epoch 12 / 100: avg data time: 5.91e-02, avg batch time: 0.5083, average train loss: 1.5919
[09/26 12:01:41 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1688, average loss: 2.3310
[09/26 12:01:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 90.50	
[09/26 12:01:41 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 12:01:48 visual_prompt]: Epoch 13 / 100: avg data time: 5.59e-02, avg batch time: 0.5050, average train loss: 1.6584
[09/26 12:01:49 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1685, average loss: 1.5850
[09/26 12:01:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 97.50	
[09/26 12:01:49 visual_prompt]: Best epoch 13: best metric: 0.275
[09/26 12:01:49 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 12:01:56 visual_prompt]: Epoch 14 / 100: avg data time: 5.82e-02, avg batch time: 0.5076, average train loss: 1.4904
[09/26 12:01:57 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1689, average loss: 1.5054
[09/26 12:01:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 97.50	
[09/26 12:01:57 visual_prompt]: Best epoch 14: best metric: 0.300
[09/26 12:01:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 12:02:04 visual_prompt]: Epoch 15 / 100: avg data time: 6.12e-02, avg batch time: 0.5092, average train loss: 1.3841
[09/26 12:02:06 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1688, average loss: 1.8777
[09/26 12:02:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 95.50	
[09/26 12:02:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 12:02:13 visual_prompt]: Epoch 16 / 100: avg data time: 5.01e-02, avg batch time: 0.4987, average train loss: 1.3448
[09/26 12:02:14 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1686, average loss: 1.5011
[09/26 12:02:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.50	
[09/26 12:02:14 visual_prompt]: Best epoch 16: best metric: 0.370
[09/26 12:02:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 12:02:21 visual_prompt]: Epoch 17 / 100: avg data time: 4.47e-02, avg batch time: 0.4938, average train loss: 1.3985
[09/26 12:02:22 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1689, average loss: 1.6634
[09/26 12:02:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 98.00	
[09/26 12:02:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 12:02:29 visual_prompt]: Epoch 18 / 100: avg data time: 5.69e-02, avg batch time: 0.5047, average train loss: 1.3991
[09/26 12:02:31 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1688, average loss: 1.4780
[09/26 12:02:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.50	
[09/26 12:02:31 visual_prompt]: Best epoch 18: best metric: 0.375
[09/26 12:02:31 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 12:02:38 visual_prompt]: Epoch 19 / 100: avg data time: 5.67e-02, avg batch time: 0.5053, average train loss: 1.3217
[09/26 12:02:39 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1686, average loss: 1.4787
[09/26 12:02:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 97.00	
[09/26 12:02:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 12:02:46 visual_prompt]: Epoch 20 / 100: avg data time: 5.84e-02, avg batch time: 0.5067, average train loss: 1.5859
[09/26 12:02:48 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1691, average loss: 2.0724
[09/26 12:02:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:02:48 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 12:02:54 visual_prompt]: Epoch 21 / 100: avg data time: 5.76e-02, avg batch time: 0.5053, average train loss: 1.6288
[09/26 12:02:56 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1688, average loss: 1.8991
[09/26 12:02:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 97.00	
[09/26 12:02:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 12:03:03 visual_prompt]: Epoch 22 / 100: avg data time: 4.18e-02, avg batch time: 0.4923, average train loss: 1.5589
[09/26 12:03:04 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 1.6633
[09/26 12:03:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 86.00	
[09/26 12:03:04 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 12:03:11 visual_prompt]: Epoch 23 / 100: avg data time: 5.42e-02, avg batch time: 0.5024, average train loss: 1.4213
[09/26 12:03:12 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1687, average loss: 1.7685
[09/26 12:03:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 96.00	
[09/26 12:03:12 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 12:03:19 visual_prompt]: Epoch 24 / 100: avg data time: 5.51e-02, avg batch time: 0.5039, average train loss: 1.4573
[09/26 12:03:21 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1690, average loss: 1.5161
[09/26 12:03:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 95.50	
[09/26 12:03:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 12:03:27 visual_prompt]: Epoch 25 / 100: avg data time: 4.29e-02, avg batch time: 0.4928, average train loss: 1.3132
[09/26 12:03:29 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1687, average loss: 1.6430
[09/26 12:03:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 96.00	
[09/26 12:03:29 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 12:03:36 visual_prompt]: Epoch 26 / 100: avg data time: 4.29e-02, avg batch time: 0.4926, average train loss: 1.3475
[09/26 12:03:37 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1688, average loss: 1.4603
[09/26 12:03:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.00	
[09/26 12:03:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 12:03:44 visual_prompt]: Epoch 27 / 100: avg data time: 5.23e-02, avg batch time: 0.5014, average train loss: 1.2806
[09/26 12:03:45 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1689, average loss: 1.3386
[09/26 12:03:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.00	
[09/26 12:03:45 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 12:03:52 visual_prompt]: Epoch 28 / 100: avg data time: 4.65e-02, avg batch time: 0.4958, average train loss: 1.3016
[09/26 12:03:54 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1690, average loss: 1.4696
[09/26 12:03:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.00	
[09/26 12:03:54 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 12:04:00 visual_prompt]: Epoch 29 / 100: avg data time: 4.27e-02, avg batch time: 0.4915, average train loss: 1.3022
[09/26 12:04:02 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1689, average loss: 1.6825
[09/26 12:04:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 98.50	
[09/26 12:04:02 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 12:04:09 visual_prompt]: Epoch 30 / 100: avg data time: 5.82e-02, avg batch time: 0.5068, average train loss: 1.2923
[09/26 12:04:10 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 1.4480
[09/26 12:04:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.00	
[09/26 12:04:10 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 12:04:17 visual_prompt]: Epoch 31 / 100: avg data time: 4.77e-02, avg batch time: 0.4957, average train loss: 1.3422
[09/26 12:04:19 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1688, average loss: 1.6028
[09/26 12:04:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 97.50	
[09/26 12:04:19 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 12:04:25 visual_prompt]: Epoch 32 / 100: avg data time: 4.96e-02, avg batch time: 0.4983, average train loss: 1.3908
[09/26 12:04:27 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1690, average loss: 1.5531
[09/26 12:04:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 12:04:27 visual_prompt]: Best epoch 32: best metric: 0.380
[09/26 12:04:27 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 12:04:34 visual_prompt]: Epoch 33 / 100: avg data time: 4.76e-02, avg batch time: 0.4961, average train loss: 1.3167
[09/26 12:04:35 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1687, average loss: 1.3949
[09/26 12:04:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.00	
[09/26 12:04:35 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 12:04:42 visual_prompt]: Epoch 34 / 100: avg data time: 4.36e-02, avg batch time: 0.4927, average train loss: 1.2549
[09/26 12:04:43 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1685, average loss: 1.5530
[09/26 12:04:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.50	
[09/26 12:04:43 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 12:04:50 visual_prompt]: Epoch 35 / 100: avg data time: 4.12e-02, avg batch time: 0.4913, average train loss: 1.3205
[09/26 12:04:52 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1687, average loss: 1.6058
[09/26 12:04:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 94.50	
[09/26 12:04:52 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 12:04:58 visual_prompt]: Epoch 36 / 100: avg data time: 5.22e-02, avg batch time: 0.5005, average train loss: 1.4271
[09/26 12:05:00 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1690, average loss: 1.7381
[09/26 12:05:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 92.50	
[09/26 12:05:00 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 12:05:07 visual_prompt]: Epoch 37 / 100: avg data time: 4.73e-02, avg batch time: 0.4964, average train loss: 1.3794
[09/26 12:05:08 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1688, average loss: 1.5001
[09/26 12:05:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 95.50	
[09/26 12:05:08 visual_prompt]: Best epoch 37: best metric: 0.400
[09/26 12:05:08 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 12:05:15 visual_prompt]: Epoch 38 / 100: avg data time: 5.17e-02, avg batch time: 0.4999, average train loss: 1.2533
[09/26 12:05:16 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1688, average loss: 1.4524
[09/26 12:05:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 12:05:16 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 12:05:23 visual_prompt]: Epoch 39 / 100: avg data time: 5.41e-02, avg batch time: 0.5029, average train loss: 1.2505
[09/26 12:05:25 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1691, average loss: 1.4901
[09/26 12:05:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.00	
[09/26 12:05:25 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 12:05:32 visual_prompt]: Epoch 40 / 100: avg data time: 5.70e-02, avg batch time: 0.5061, average train loss: 1.2172
[09/26 12:05:33 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1688, average loss: 1.4616
[09/26 12:05:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 12:05:33 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 12:05:40 visual_prompt]: Epoch 41 / 100: avg data time: 5.70e-02, avg batch time: 0.5049, average train loss: 1.2712
[09/26 12:05:42 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1690, average loss: 1.3730
[09/26 12:05:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 12:05:42 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 12:05:48 visual_prompt]: Epoch 42 / 100: avg data time: 5.80e-02, avg batch time: 0.5063, average train loss: 1.2281
[09/26 12:05:50 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 1.6160
[09/26 12:05:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.00	
[09/26 12:05:50 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 12:05:57 visual_prompt]: Epoch 43 / 100: avg data time: 5.43e-02, avg batch time: 0.5030, average train loss: 1.2040
[09/26 12:05:58 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1689, average loss: 1.2974
[09/26 12:05:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.50	
[09/26 12:05:58 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 12:06:05 visual_prompt]: Epoch 44 / 100: avg data time: 5.14e-02, avg batch time: 0.5010, average train loss: 1.1796
[09/26 12:06:07 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1691, average loss: 1.4808
[09/26 12:06:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.50	
[09/26 12:06:07 visual_prompt]: Best epoch 44: best metric: 0.405
[09/26 12:06:07 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 12:06:13 visual_prompt]: Epoch 45 / 100: avg data time: 4.93e-02, avg batch time: 0.4975, average train loss: 1.1362
[09/26 12:06:15 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1687, average loss: 1.5380
[09/26 12:06:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 12:06:15 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 12:06:22 visual_prompt]: Epoch 46 / 100: avg data time: 4.77e-02, avg batch time: 0.4982, average train loss: 1.2687
[09/26 12:06:23 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1690, average loss: 1.5793
[09/26 12:06:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 98.00	
[09/26 12:06:23 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 12:06:30 visual_prompt]: Epoch 47 / 100: avg data time: 5.80e-02, avg batch time: 0.5075, average train loss: 1.1915
[09/26 12:06:31 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1688, average loss: 1.3941
[09/26 12:06:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.50	
[09/26 12:06:31 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 12:06:38 visual_prompt]: Epoch 48 / 100: avg data time: 4.27e-02, avg batch time: 0.4918, average train loss: 1.2049
[09/26 12:06:40 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1686, average loss: 1.5430
[09/26 12:06:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.50	
[09/26 12:06:40 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 12:06:47 visual_prompt]: Epoch 49 / 100: avg data time: 4.99e-02, avg batch time: 0.4989, average train loss: 1.1886
[09/26 12:06:48 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1687, average loss: 1.6076
[09/26 12:06:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 95.00	
[09/26 12:06:48 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 12:06:55 visual_prompt]: Epoch 50 / 100: avg data time: 4.77e-02, avg batch time: 0.4968, average train loss: 1.1703
[09/26 12:06:56 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1685, average loss: 1.2965
[09/26 12:06:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 12:06:56 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 12:07:03 visual_prompt]: Epoch 51 / 100: avg data time: 5.20e-02, avg batch time: 0.5009, average train loss: 1.0859
[09/26 12:07:05 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1685, average loss: 1.4713
[09/26 12:07:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 12:07:05 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 12:07:12 visual_prompt]: Epoch 52 / 100: avg data time: 4.67e-02, avg batch time: 0.4959, average train loss: 1.1650
[09/26 12:07:13 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1687, average loss: 1.3161
[09/26 12:07:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.00	
[09/26 12:07:13 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 12:07:20 visual_prompt]: Epoch 53 / 100: avg data time: 6.07e-02, avg batch time: 0.5100, average train loss: 1.0714
[09/26 12:07:21 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1687, average loss: 1.5963
[09/26 12:07:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 12:07:21 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 12:07:28 visual_prompt]: Epoch 54 / 100: avg data time: 6.55e-02, avg batch time: 0.5147, average train loss: 1.1669
[09/26 12:07:30 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1688, average loss: 1.2741
[09/26 12:07:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.00	
[09/26 12:07:30 visual_prompt]: Best epoch 54: best metric: 0.425
[09/26 12:07:30 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 12:07:37 visual_prompt]: Epoch 55 / 100: avg data time: 5.61e-02, avg batch time: 0.5044, average train loss: 1.0806
[09/26 12:07:38 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1690, average loss: 1.5500
[09/26 12:07:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 12:07:38 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 12:07:45 visual_prompt]: Epoch 56 / 100: avg data time: 5.26e-02, avg batch time: 0.5014, average train loss: 0.9849
[09/26 12:07:47 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 1.3462
[09/26 12:07:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 99.50	
[09/26 12:07:47 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 12:07:53 visual_prompt]: Epoch 57 / 100: avg data time: 5.89e-02, avg batch time: 0.5081, average train loss: 0.9550
[09/26 12:07:55 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1688, average loss: 1.4579
[09/26 12:07:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.00	
[09/26 12:07:55 visual_prompt]: Best epoch 57: best metric: 0.435
[09/26 12:07:55 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 12:08:02 visual_prompt]: Epoch 58 / 100: avg data time: 4.89e-02, avg batch time: 0.4993, average train loss: 0.9615
[09/26 12:08:03 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1684, average loss: 1.3572
[09/26 12:08:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.50	
[09/26 12:08:03 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 12:08:10 visual_prompt]: Epoch 59 / 100: avg data time: 5.56e-02, avg batch time: 0.5045, average train loss: 0.9915
[09/26 12:08:12 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1691, average loss: 1.6898
[09/26 12:08:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 96.50	
[09/26 12:08:12 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 12:08:18 visual_prompt]: Epoch 60 / 100: avg data time: 4.38e-02, avg batch time: 0.4946, average train loss: 0.9831
[09/26 12:08:20 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1688, average loss: 1.5441
[09/26 12:08:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 12:08:20 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 12:08:27 visual_prompt]: Epoch 61 / 100: avg data time: 5.84e-02, avg batch time: 0.5067, average train loss: 0.9922
[09/26 12:08:28 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1686, average loss: 1.4335
[09/26 12:08:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 12:08:28 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 12:08:35 visual_prompt]: Epoch 62 / 100: avg data time: 5.03e-02, avg batch time: 0.4989, average train loss: 1.0043
[09/26 12:08:36 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1688, average loss: 1.5492
[09/26 12:08:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 12:08:37 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 12:08:43 visual_prompt]: Epoch 63 / 100: avg data time: 5.74e-02, avg batch time: 0.5054, average train loss: 0.9404
[09/26 12:08:45 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1691, average loss: 1.4210
[09/26 12:08:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.00	
[09/26 12:08:45 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 12:08:52 visual_prompt]: Epoch 64 / 100: avg data time: 4.03e-02, avg batch time: 0.4891, average train loss: 0.8943
[09/26 12:08:53 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1692, average loss: 1.4099
[09/26 12:08:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.00	
[09/26 12:08:53 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 12:09:00 visual_prompt]: Epoch 65 / 100: avg data time: 5.25e-02, avg batch time: 0.5017, average train loss: 0.8764
[09/26 12:09:01 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1689, average loss: 1.5001
[09/26 12:09:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.50	
[09/26 12:09:01 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 12:09:08 visual_prompt]: Epoch 66 / 100: avg data time: 5.68e-02, avg batch time: 0.5051, average train loss: 0.7797
[09/26 12:09:10 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1684, average loss: 1.6229
[09/26 12:09:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.50	
[09/26 12:09:10 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 12:09:17 visual_prompt]: Epoch 67 / 100: avg data time: 5.96e-02, avg batch time: 0.5077, average train loss: 0.7505
[09/26 12:09:18 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1692, average loss: 1.6559
[09/26 12:09:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.00	
[09/26 12:09:18 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 12:09:25 visual_prompt]: Epoch 68 / 100: avg data time: 4.61e-02, avg batch time: 0.4954, average train loss: 0.7684
[09/26 12:09:26 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 1.4839
[09/26 12:09:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 99.00	
[09/26 12:09:26 visual_prompt]: Best epoch 68: best metric: 0.440
[09/26 12:09:26 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 12:09:33 visual_prompt]: Epoch 69 / 100: avg data time: 5.33e-02, avg batch time: 0.5029, average train loss: 0.8384
[09/26 12:09:35 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1686, average loss: 1.4694
[09/26 12:09:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 12:09:35 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 12:09:41 visual_prompt]: Epoch 70 / 100: avg data time: 5.19e-02, avg batch time: 0.5020, average train loss: 0.7090
[09/26 12:09:43 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1689, average loss: 1.6938
[09/26 12:09:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 97.50	
[09/26 12:09:43 visual_prompt]: Best epoch 70: best metric: 0.455
[09/26 12:09:43 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 12:09:50 visual_prompt]: Epoch 71 / 100: avg data time: 5.05e-02, avg batch time: 0.4994, average train loss: 0.6427
[09/26 12:09:51 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 1.7268
[09/26 12:09:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 98.50	
[09/26 12:09:51 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 12:09:58 visual_prompt]: Epoch 72 / 100: avg data time: 4.97e-02, avg batch time: 0.4982, average train loss: 0.5904
[09/26 12:09:59 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1690, average loss: 1.5689
[09/26 12:09:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 98.00	
[09/26 12:09:59 visual_prompt]: Best epoch 72: best metric: 0.460
[09/26 12:09:59 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 12:10:06 visual_prompt]: Epoch 73 / 100: avg data time: 5.00e-02, avg batch time: 0.5000, average train loss: 0.6030
[09/26 12:10:08 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1690, average loss: 1.7845
[09/26 12:10:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 99.00	
[09/26 12:10:08 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 12:10:14 visual_prompt]: Epoch 74 / 100: avg data time: 4.28e-02, avg batch time: 0.4936, average train loss: 0.5013
[09/26 12:10:16 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1690, average loss: 1.8487
[09/26 12:10:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.50	
[09/26 12:10:16 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 12:10:23 visual_prompt]: Epoch 75 / 100: avg data time: 5.07e-02, avg batch time: 0.4995, average train loss: 0.5108
[09/26 12:10:24 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1687, average loss: 1.8866
[09/26 12:10:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 98.50	
[09/26 12:10:24 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 12:10:31 visual_prompt]: Epoch 76 / 100: avg data time: 6.05e-02, avg batch time: 0.5095, average train loss: 0.5104
[09/26 12:10:33 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1689, average loss: 1.9590
[09/26 12:10:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 99.00	
[09/26 12:10:33 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 12:10:39 visual_prompt]: Epoch 77 / 100: avg data time: 4.59e-02, avg batch time: 0.4945, average train loss: 0.4444
[09/26 12:10:41 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1687, average loss: 1.8808
[09/26 12:10:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.50	top5: 99.00	
[09/26 12:10:41 visual_prompt]: Best epoch 77: best metric: 0.475
[09/26 12:10:41 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 12:10:48 visual_prompt]: Epoch 78 / 100: avg data time: 5.94e-02, avg batch time: 0.5073, average train loss: 0.3693
[09/26 12:10:49 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1692, average loss: 1.9628
[09/26 12:10:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 49.50	top5: 99.00	
[09/26 12:10:49 visual_prompt]: Best epoch 78: best metric: 0.495
[09/26 12:10:49 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 12:10:56 visual_prompt]: Epoch 79 / 100: avg data time: 5.79e-02, avg batch time: 0.5059, average train loss: 0.4139
[09/26 12:10:58 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1686, average loss: 2.1349
[09/26 12:10:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 98.50	
[09/26 12:10:58 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 12:11:05 visual_prompt]: Epoch 80 / 100: avg data time: 6.71e-02, avg batch time: 0.5153, average train loss: 0.4239
[09/26 12:11:06 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1689, average loss: 1.8471
[09/26 12:11:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 99.00	
[09/26 12:11:06 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 12:11:13 visual_prompt]: Epoch 81 / 100: avg data time: 4.98e-02, avg batch time: 0.4985, average train loss: 0.3543
[09/26 12:11:14 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 2.3048
[09/26 12:11:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 98.00	
[09/26 12:11:14 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 12:11:21 visual_prompt]: Epoch 82 / 100: avg data time: 4.76e-02, avg batch time: 0.4974, average train loss: 0.2807
[09/26 12:11:23 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1686, average loss: 2.4249
[09/26 12:11:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 98.00	
[09/26 12:11:23 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 12:11:29 visual_prompt]: Epoch 83 / 100: avg data time: 4.30e-02, avg batch time: 0.4925, average train loss: 0.2402
[09/26 12:11:31 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1690, average loss: 2.4271
[09/26 12:11:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.00	top5: 98.50	
[09/26 12:11:31 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 12:11:38 visual_prompt]: Epoch 84 / 100: avg data time: 6.09e-02, avg batch time: 0.5092, average train loss: 0.2359
[09/26 12:11:39 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1686, average loss: 2.4224
[09/26 12:11:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 98.50	
[09/26 12:11:39 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 12:11:46 visual_prompt]: Epoch 85 / 100: avg data time: 4.76e-02, avg batch time: 0.4981, average train loss: 0.2251
[09/26 12:11:48 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1693, average loss: 2.4608
[09/26 12:11:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.50	top5: 99.00	
[09/26 12:11:48 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 12:11:54 visual_prompt]: Epoch 86 / 100: avg data time: 4.50e-02, avg batch time: 0.4952, average train loss: 0.1935
[09/26 12:11:56 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1688, average loss: 2.5808
[09/26 12:11:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.50	top5: 98.00	
[09/26 12:11:56 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 12:12:03 visual_prompt]: Epoch 87 / 100: avg data time: 5.24e-02, avg batch time: 0.5010, average train loss: 0.2402
[09/26 12:12:04 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1690, average loss: 2.2584
[09/26 12:12:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 49.00	top5: 99.00	
[09/26 12:12:04 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 12:12:11 visual_prompt]: Epoch 88 / 100: avg data time: 4.52e-02, avg batch time: 0.4963, average train loss: 0.2069
[09/26 12:12:12 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1688, average loss: 2.4799
[09/26 12:12:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.50	top5: 98.50	
[09/26 12:12:12 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 12:12:19 visual_prompt]: Epoch 89 / 100: avg data time: 6.11e-02, avg batch time: 0.5095, average train loss: 0.1612
[09/26 12:12:21 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1691, average loss: 2.5272
[09/26 12:12:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.00	top5: 97.50	
[09/26 12:12:21 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 12:12:28 visual_prompt]: Epoch 90 / 100: avg data time: 5.78e-02, avg batch time: 0.5075, average train loss: 0.1300
[09/26 12:12:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1687, average loss: 2.5490
[09/26 12:12:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 98.00	
[09/26 12:12:29 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 12:12:36 visual_prompt]: Epoch 91 / 100: avg data time: 5.87e-02, avg batch time: 0.5075, average train loss: 0.1119
[09/26 12:12:38 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1687, average loss: 2.6178
[09/26 12:12:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 98.50	
[09/26 12:12:38 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 12:12:45 visual_prompt]: Epoch 92 / 100: avg data time: 5.58e-02, avg batch time: 0.5059, average train loss: 0.1054
[09/26 12:12:46 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1689, average loss: 2.5889
[09/26 12:12:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 98.00	
[09/26 12:12:46 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 12:12:53 visual_prompt]: Epoch 93 / 100: avg data time: 5.80e-02, avg batch time: 0.5074, average train loss: 0.0882
[09/26 12:12:54 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1692, average loss: 2.7155
[09/26 12:12:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 49.00	top5: 98.00	
[09/26 12:12:54 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 12:13:01 visual_prompt]: Epoch 94 / 100: avg data time: 5.53e-02, avg batch time: 0.5036, average train loss: 0.0716
[09/26 12:13:03 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.1690, average loss: 2.6511
[09/26 12:13:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.50	top5: 98.50	
[09/26 12:13:03 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 12:13:10 visual_prompt]: Epoch 95 / 100: avg data time: 4.70e-02, avg batch time: 0.4963, average train loss: 0.0651
[09/26 12:13:11 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 2.6530
[09/26 12:13:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.00	top5: 98.50	
[09/26 12:13:11 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 12:13:18 visual_prompt]: Epoch 96 / 100: avg data time: 5.64e-02, avg batch time: 0.5074, average train loss: 0.0669
[09/26 12:13:19 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1692, average loss: 2.7030
[09/26 12:13:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.50	top5: 98.00	
[09/26 12:13:19 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 12:13:26 visual_prompt]: Epoch 97 / 100: avg data time: 5.47e-02, avg batch time: 0.5039, average train loss: 0.0623
[09/26 12:13:28 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1688, average loss: 2.7139
[09/26 12:13:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.00	top5: 98.00	
[09/26 12:13:28 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 12:13:35 visual_prompt]: Epoch 98 / 100: avg data time: 5.61e-02, avg batch time: 0.5053, average train loss: 0.0564
[09/26 12:13:36 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 2.7279
[09/26 12:13:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.00	top5: 98.00	
[09/26 12:13:36 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 12:13:43 visual_prompt]: Epoch 99 / 100: avg data time: 5.40e-02, avg batch time: 0.5021, average train loss: 0.0550
[09/26 12:13:44 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1689, average loss: 2.7317
[09/26 12:13:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 98.00	
[09/26 12:13:44 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 12:13:51 visual_prompt]: Epoch 100 / 100: avg data time: 5.44e-02, avg batch time: 0.5028, average train loss: 0.0500
[09/26 12:13:53 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 2.7322
[09/26 12:13:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 98.00	
[09/26 12:13:53 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:13:53 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:13:53 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:13:53 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:13:53 visual_prompt]: Training with config:
[09/26 12:13:53 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:13:53 visual_prompt]: Loading training data...
[09/26 12:13:53 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 12:13:54 visual_prompt]: Number of images: 800
[09/26 12:13:54 visual_prompt]: Number of classes: 6 / 6
[09/26 12:13:54 visual_prompt]: Loading validation data...
[09/26 12:13:54 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 12:13:54 visual_prompt]: Number of images: 200
[09/26 12:13:54 visual_prompt]: Number of classes: 6 / 6
[09/26 12:13:54 visual_prompt]: Constructing models...
[09/26 12:13:57 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 12:13:57 visual_prompt]: tuned percent:0.540
[09/26 12:13:57 visual_prompt]: Device used for model: 0
[09/26 12:13:57 visual_prompt]: Setting up Evaluator...
[09/26 12:13:57 visual_prompt]: Setting up Trainer...
[09/26 12:13:57 visual_prompt]: 	Setting up the optimizer...
[09/26 12:13:57 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:14:04 visual_prompt]: Epoch 1 / 100: avg data time: 4.71e-02, avg batch time: 0.4961, average train loss: 2.9810
[09/26 12:14:05 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1680, average loss: 2.9268
[09/26 12:14:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 12:14:05 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 12:14:05 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 12:14:12 visual_prompt]: Epoch 2 / 100: avg data time: 6.05e-02, avg batch time: 0.5079, average train loss: 3.0879
[09/26 12:14:14 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1681, average loss: 2.3474
[09/26 12:14:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.00	top5: 82.50	
[09/26 12:14:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 12:14:21 visual_prompt]: Epoch 3 / 100: avg data time: 6.51e-02, avg batch time: 0.5126, average train loss: 1.9846
[09/26 12:14:22 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1685, average loss: 1.9119
[09/26 12:14:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 85.00	
[09/26 12:14:22 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 12:14:29 visual_prompt]: Epoch 4 / 100: avg data time: 5.77e-02, avg batch time: 0.5058, average train loss: 1.8962
[09/26 12:14:30 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1685, average loss: 1.8015
[09/26 12:14:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 12:14:30 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 12:14:37 visual_prompt]: Epoch 5 / 100: avg data time: 6.02e-02, avg batch time: 0.5074, average train loss: 1.8316
[09/26 12:14:39 visual_prompt]: Inference (val):avg data time: 5.60e-05, avg batch time: 0.1683, average loss: 1.8786
[09/26 12:14:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 88.00	
[09/26 12:14:39 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 12:14:46 visual_prompt]: Epoch 6 / 100: avg data time: 5.79e-02, avg batch time: 0.5058, average train loss: 1.8193
[09/26 12:14:47 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1685, average loss: 1.7982
[09/26 12:14:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 84.00	
[09/26 12:14:47 visual_prompt]: Best epoch 6: best metric: 0.225
[09/26 12:14:47 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 12:14:54 visual_prompt]: Epoch 7 / 100: avg data time: 5.64e-02, avg batch time: 0.5042, average train loss: 1.7732
[09/26 12:14:56 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1689, average loss: 1.8305
[09/26 12:14:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 84.00	
[09/26 12:14:56 visual_prompt]: Best epoch 7: best metric: 0.240
[09/26 12:14:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 12:15:02 visual_prompt]: Epoch 8 / 100: avg data time: 4.50e-02, avg batch time: 0.4933, average train loss: 1.7579
[09/26 12:15:04 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1685, average loss: 1.8420
[09/26 12:15:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 84.50	
[09/26 12:15:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 12:15:11 visual_prompt]: Epoch 9 / 100: avg data time: 5.81e-02, avg batch time: 0.5067, average train loss: 1.7506
[09/26 12:15:12 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1689, average loss: 1.8634
[09/26 12:15:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 91.50	
[09/26 12:15:12 visual_prompt]: Best epoch 9: best metric: 0.260
[09/26 12:15:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 12:15:19 visual_prompt]: Epoch 10 / 100: avg data time: 5.56e-02, avg batch time: 0.5029, average train loss: 1.7034
[09/26 12:15:21 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1691, average loss: 1.6782
[09/26 12:15:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 94.50	
[09/26 12:15:21 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 12:15:27 visual_prompt]: Epoch 11 / 100: avg data time: 5.39e-02, avg batch time: 0.5024, average train loss: 1.5702
[09/26 12:15:29 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 2.1452
[09/26 12:15:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 97.00	
[09/26 12:15:29 visual_prompt]: Best epoch 11: best metric: 0.265
[09/26 12:15:29 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 12:15:36 visual_prompt]: Epoch 12 / 100: avg data time: 5.75e-02, avg batch time: 0.5052, average train loss: 1.5377
[09/26 12:15:37 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1689, average loss: 1.5203
[09/26 12:15:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 98.00	
[09/26 12:15:37 visual_prompt]: Best epoch 12: best metric: 0.315
[09/26 12:15:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 12:15:44 visual_prompt]: Epoch 13 / 100: avg data time: 4.55e-02, avg batch time: 0.4952, average train loss: 1.4151
[09/26 12:15:46 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1685, average loss: 1.5144
[09/26 12:15:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 99.00	
[09/26 12:15:46 visual_prompt]: Best epoch 13: best metric: 0.345
[09/26 12:15:46 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 12:15:52 visual_prompt]: Epoch 14 / 100: avg data time: 6.48e-02, avg batch time: 0.5136, average train loss: 1.4063
[09/26 12:15:54 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1689, average loss: 1.8973
[09/26 12:15:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 94.50	
[09/26 12:15:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 12:16:01 visual_prompt]: Epoch 15 / 100: avg data time: 4.45e-02, avg batch time: 0.4948, average train loss: 1.3430
[09/26 12:16:02 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1688, average loss: 1.8576
[09/26 12:16:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 94.50	
[09/26 12:16:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 12:16:09 visual_prompt]: Epoch 16 / 100: avg data time: 5.54e-02, avg batch time: 0.5042, average train loss: 1.3270
[09/26 12:16:11 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1687, average loss: 1.8820
[09/26 12:16:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 94.50	
[09/26 12:16:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 12:16:17 visual_prompt]: Epoch 17 / 100: avg data time: 4.70e-02, avg batch time: 0.4971, average train loss: 1.3372
[09/26 12:16:19 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1690, average loss: 1.5328
[09/26 12:16:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 98.00	
[09/26 12:16:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 12:16:26 visual_prompt]: Epoch 18 / 100: avg data time: 5.19e-02, avg batch time: 0.5005, average train loss: 1.2283
[09/26 12:16:27 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1691, average loss: 1.5618
[09/26 12:16:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.00	
[09/26 12:16:27 visual_prompt]: Best epoch 18: best metric: 0.355
[09/26 12:16:27 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 12:16:34 visual_prompt]: Epoch 19 / 100: avg data time: 5.67e-02, avg batch time: 0.5050, average train loss: 1.2063
[09/26 12:16:36 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1686, average loss: 1.5624
[09/26 12:16:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 12:16:36 visual_prompt]: Best epoch 19: best metric: 0.395
[09/26 12:16:36 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 12:16:42 visual_prompt]: Epoch 20 / 100: avg data time: 5.18e-02, avg batch time: 0.5025, average train loss: 1.0940
[09/26 12:16:44 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1689, average loss: 1.4837
[09/26 12:16:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/26 12:16:44 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 12:16:51 visual_prompt]: Epoch 21 / 100: avg data time: 5.84e-02, avg batch time: 0.5061, average train loss: 1.0670
[09/26 12:16:52 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1688, average loss: 1.8047
[09/26 12:16:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.50	
[09/26 12:16:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 12:16:59 visual_prompt]: Epoch 22 / 100: avg data time: 5.29e-02, avg batch time: 0.5010, average train loss: 1.1538
[09/26 12:17:01 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1688, average loss: 1.3858
[09/26 12:17:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.50	
[09/26 12:17:01 visual_prompt]: Best epoch 22: best metric: 0.430
[09/26 12:17:01 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 12:17:07 visual_prompt]: Epoch 23 / 100: avg data time: 4.34e-02, avg batch time: 0.4946, average train loss: 1.1476
[09/26 12:17:09 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1688, average loss: 1.5267
[09/26 12:17:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.50	
[09/26 12:17:09 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 12:17:16 visual_prompt]: Epoch 24 / 100: avg data time: 5.81e-02, avg batch time: 0.5063, average train loss: 1.0025
[09/26 12:17:17 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1691, average loss: 1.4973
[09/26 12:17:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 12:17:17 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 12:17:24 visual_prompt]: Epoch 25 / 100: avg data time: 4.98e-02, avg batch time: 0.4989, average train loss: 0.9610
[09/26 12:17:26 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1689, average loss: 1.7405
[09/26 12:17:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.50	
[09/26 12:17:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 12:17:33 visual_prompt]: Epoch 26 / 100: avg data time: 6.53e-02, avg batch time: 0.5131, average train loss: 0.8663
[09/26 12:17:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1684, average loss: 1.6548
[09/26 12:17:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 12:17:34 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 12:17:41 visual_prompt]: Epoch 27 / 100: avg data time: 6.51e-02, avg batch time: 0.5139, average train loss: 0.9165
[09/26 12:17:43 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1686, average loss: 1.7137
[09/26 12:17:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.50	
[09/26 12:17:43 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 12:17:50 visual_prompt]: Epoch 28 / 100: avg data time: 5.90e-02, avg batch time: 0.5087, average train loss: 0.8455
[09/26 12:17:51 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1691, average loss: 1.7228
[09/26 12:17:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 12:17:51 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 12:17:58 visual_prompt]: Epoch 29 / 100: avg data time: 5.52e-02, avg batch time: 0.5044, average train loss: 0.9204
[09/26 12:17:59 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1690, average loss: 1.8516
[09/26 12:17:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.50	
[09/26 12:17:59 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 12:18:06 visual_prompt]: Epoch 30 / 100: avg data time: 4.61e-02, avg batch time: 0.4956, average train loss: 0.7749
[09/26 12:18:08 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 2.1128
[09/26 12:18:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 12:18:08 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 12:18:15 visual_prompt]: Epoch 31 / 100: avg data time: 5.51e-02, avg batch time: 0.5049, average train loss: 0.8437
[09/26 12:18:16 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1684, average loss: 2.1636
[09/26 12:18:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 93.50	
[09/26 12:18:16 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 12:18:23 visual_prompt]: Epoch 32 / 100: avg data time: 5.60e-02, avg batch time: 0.5050, average train loss: 1.0117
[09/26 12:18:24 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1686, average loss: 1.8367
[09/26 12:18:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 98.00	
[09/26 12:18:24 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 12:18:31 visual_prompt]: Epoch 33 / 100: avg data time: 5.82e-02, avg batch time: 0.5075, average train loss: 0.8454
[09/26 12:18:33 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1688, average loss: 1.9293
[09/26 12:18:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.00	
[09/26 12:18:33 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 12:18:40 visual_prompt]: Epoch 34 / 100: avg data time: 4.93e-02, avg batch time: 0.4986, average train loss: 0.7698
[09/26 12:18:41 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1689, average loss: 1.9266
[09/26 12:18:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 100.00	
[09/26 12:18:41 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 12:18:48 visual_prompt]: Epoch 35 / 100: avg data time: 5.61e-02, avg batch time: 0.5044, average train loss: 0.6761
[09/26 12:18:49 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1687, average loss: 1.8759
[09/26 12:18:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 99.00	
[09/26 12:18:49 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 12:18:56 visual_prompt]: Epoch 36 / 100: avg data time: 4.58e-02, avg batch time: 0.4955, average train loss: 0.6835
[09/26 12:18:58 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1687, average loss: 1.9674
[09/26 12:18:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.00	
[09/26 12:18:58 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 12:19:05 visual_prompt]: Epoch 37 / 100: avg data time: 5.57e-02, avg batch time: 0.5039, average train loss: 0.5842
[09/26 12:19:06 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1687, average loss: 2.0604
[09/26 12:19:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 99.00	
[09/26 12:19:06 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 12:19:13 visual_prompt]: Epoch 38 / 100: avg data time: 6.20e-02, avg batch time: 0.5102, average train loss: 0.5477
[09/26 12:19:14 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1688, average loss: 1.9692
[09/26 12:19:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 12:19:14 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 12:19:21 visual_prompt]: Epoch 39 / 100: avg data time: 4.68e-02, avg batch time: 0.4978, average train loss: 0.4854
[09/26 12:19:23 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1686, average loss: 2.1651
[09/26 12:19:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 98.00	
[09/26 12:19:23 visual_prompt]: Best epoch 39: best metric: 0.450
[09/26 12:19:23 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 12:19:30 visual_prompt]: Epoch 40 / 100: avg data time: 5.83e-02, avg batch time: 0.5074, average train loss: 0.4695
[09/26 12:19:31 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1689, average loss: 2.2926
[09/26 12:19:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 95.50	
[09/26 12:19:31 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 12:19:38 visual_prompt]: Epoch 41 / 100: avg data time: 4.64e-02, avg batch time: 0.4964, average train loss: 0.6391
[09/26 12:19:39 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 2.2438
[09/26 12:19:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 12:19:39 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 12:19:46 visual_prompt]: Epoch 42 / 100: avg data time: 4.58e-02, avg batch time: 0.4952, average train loss: 0.5265
[09/26 12:19:48 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1689, average loss: 2.0066
[09/26 12:19:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.50	
[09/26 12:19:48 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 12:19:54 visual_prompt]: Epoch 43 / 100: avg data time: 4.29e-02, avg batch time: 0.4939, average train loss: 0.3256
[09/26 12:19:56 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1688, average loss: 2.6171
[09/26 12:19:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 12:19:56 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 12:20:03 visual_prompt]: Epoch 44 / 100: avg data time: 4.78e-02, avg batch time: 0.4979, average train loss: 0.2319
[09/26 12:20:04 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1687, average loss: 3.0102
[09/26 12:20:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 12:20:04 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 12:20:11 visual_prompt]: Epoch 45 / 100: avg data time: 5.15e-02, avg batch time: 0.5017, average train loss: 0.2951
[09/26 12:20:12 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1693, average loss: 3.5509
[09/26 12:20:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 96.00	
[09/26 12:20:12 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 12:20:19 visual_prompt]: Epoch 46 / 100: avg data time: 5.58e-02, avg batch time: 0.5045, average train loss: 0.5304
[09/26 12:20:21 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1693, average loss: 2.1939
[09/26 12:20:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.50	
[09/26 12:20:21 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 12:20:28 visual_prompt]: Epoch 47 / 100: avg data time: 5.90e-02, avg batch time: 0.5085, average train loss: 0.3743
[09/26 12:20:29 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1692, average loss: 2.3958
[09/26 12:20:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 12:20:29 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 12:20:36 visual_prompt]: Epoch 48 / 100: avg data time: 5.70e-02, avg batch time: 0.5062, average train loss: 0.2974
[09/26 12:20:38 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1690, average loss: 2.6386
[09/26 12:20:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 98.50	
[09/26 12:20:38 visual_prompt]: Best epoch 48: best metric: 0.460
[09/26 12:20:38 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 12:20:44 visual_prompt]: Epoch 49 / 100: avg data time: 5.94e-02, avg batch time: 0.5075, average train loss: 0.2538
[09/26 12:20:46 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1687, average loss: 3.1373
[09/26 12:20:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 12:20:46 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 12:20:53 visual_prompt]: Epoch 50 / 100: avg data time: 5.48e-02, avg batch time: 0.5031, average train loss: 0.3603
[09/26 12:20:54 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1688, average loss: 3.5595
[09/26 12:20:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 98.00	
[09/26 12:20:54 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 12:21:01 visual_prompt]: Epoch 51 / 100: avg data time: 4.37e-02, avg batch time: 0.4953, average train loss: 0.4187
[09/26 12:21:02 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 2.5227
[09/26 12:21:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 98.00	
[09/26 12:21:02 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 12:21:09 visual_prompt]: Epoch 52 / 100: avg data time: 4.20e-02, avg batch time: 0.4917, average train loss: 0.2330
[09/26 12:21:11 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1690, average loss: 2.8816
[09/26 12:21:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 98.00	
[09/26 12:21:11 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 12:21:17 visual_prompt]: Epoch 53 / 100: avg data time: 4.67e-02, avg batch time: 0.4986, average train loss: 0.1957
[09/26 12:21:19 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1687, average loss: 3.2223
[09/26 12:21:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 12:21:19 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 12:21:26 visual_prompt]: Epoch 54 / 100: avg data time: 4.27e-02, avg batch time: 0.4936, average train loss: 0.1427
[09/26 12:21:27 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1686, average loss: 3.4397
[09/26 12:21:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 12:21:27 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 12:21:34 visual_prompt]: Epoch 55 / 100: avg data time: 4.45e-02, avg batch time: 0.4939, average train loss: 0.0906
[09/26 12:21:35 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1688, average loss: 3.4991
[09/26 12:21:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.50	
[09/26 12:21:35 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 12:21:42 visual_prompt]: Epoch 56 / 100: avg data time: 4.77e-02, avg batch time: 0.4964, average train loss: 0.1191
[09/26 12:21:44 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1694, average loss: 3.8973
[09/26 12:21:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.00	
[09/26 12:21:44 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 12:21:50 visual_prompt]: Epoch 57 / 100: avg data time: 5.02e-02, avg batch time: 0.4998, average train loss: 0.0997
[09/26 12:21:52 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1689, average loss: 3.4435
[09/26 12:21:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.50	
[09/26 12:21:52 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 12:21:59 visual_prompt]: Epoch 58 / 100: avg data time: 5.04e-02, avg batch time: 0.4998, average train loss: 0.0765
[09/26 12:22:00 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1690, average loss: 3.6471
[09/26 12:22:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 12:22:00 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 12:22:07 visual_prompt]: Epoch 59 / 100: avg data time: 5.37e-02, avg batch time: 0.5027, average train loss: 0.0536
[09/26 12:22:09 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1690, average loss: 3.9189
[09/26 12:22:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.00	
[09/26 12:22:09 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 12:22:16 visual_prompt]: Epoch 60 / 100: avg data time: 5.53e-02, avg batch time: 0.5049, average train loss: 0.0422
[09/26 12:22:17 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1688, average loss: 4.1393
[09/26 12:22:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 97.50	
[09/26 12:22:17 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 12:22:24 visual_prompt]: Epoch 61 / 100: avg data time: 4.79e-02, avg batch time: 0.4982, average train loss: 0.0242
[09/26 12:22:25 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1690, average loss: 4.2871
[09/26 12:22:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 97.00	
[09/26 12:22:25 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 12:22:32 visual_prompt]: Epoch 62 / 100: avg data time: 5.30e-02, avg batch time: 0.5018, average train loss: 0.0254
[09/26 12:22:34 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1688, average loss: 4.2802
[09/26 12:22:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 98.00	
[09/26 12:22:34 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 12:22:40 visual_prompt]: Epoch 63 / 100: avg data time: 5.39e-02, avg batch time: 0.5028, average train loss: 0.0421
[09/26 12:22:42 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1691, average loss: 4.5778
[09/26 12:22:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 96.00	
[09/26 12:22:42 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 12:22:49 visual_prompt]: Epoch 64 / 100: avg data time: 5.87e-02, avg batch time: 0.5071, average train loss: 0.0631
[09/26 12:22:50 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1689, average loss: 4.1639
[09/26 12:22:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.50	
[09/26 12:22:50 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 12:22:57 visual_prompt]: Epoch 65 / 100: avg data time: 5.92e-02, avg batch time: 0.5071, average train loss: 0.0458
[09/26 12:22:59 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1685, average loss: 4.0997
[09/26 12:22:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 12:22:59 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 12:23:06 visual_prompt]: Epoch 66 / 100: avg data time: 5.13e-02, avg batch time: 0.4999, average train loss: 0.0497
[09/26 12:23:07 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1687, average loss: 3.8849
[09/26 12:23:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 12:23:07 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 12:23:14 visual_prompt]: Epoch 67 / 100: avg data time: 4.35e-02, avg batch time: 0.4937, average train loss: 0.0399
[09/26 12:23:15 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1687, average loss: 3.8110
[09/26 12:23:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.00	
[09/26 12:23:15 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 12:23:22 visual_prompt]: Epoch 68 / 100: avg data time: 4.56e-02, avg batch time: 0.4948, average train loss: 0.0257
[09/26 12:23:24 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1691, average loss: 4.1323
[09/26 12:23:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 12:23:24 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 12:23:30 visual_prompt]: Epoch 69 / 100: avg data time: 4.89e-02, avg batch time: 0.4984, average train loss: 0.0148
[09/26 12:23:32 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.1690, average loss: 3.9655
[09/26 12:23:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 98.00	
[09/26 12:23:32 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 12:23:39 visual_prompt]: Epoch 70 / 100: avg data time: 4.98e-02, avg batch time: 0.4990, average train loss: 0.0199
[09/26 12:23:40 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 4.1687
[09/26 12:23:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 97.50	
[09/26 12:23:40 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 12:23:47 visual_prompt]: Epoch 71 / 100: avg data time: 4.95e-02, avg batch time: 0.4980, average train loss: 0.0244
[09/26 12:23:48 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1688, average loss: 4.0712
[09/26 12:23:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 97.50	
[09/26 12:23:48 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 12:23:55 visual_prompt]: Epoch 72 / 100: avg data time: 5.09e-02, avg batch time: 0.4991, average train loss: 0.0145
[09/26 12:23:57 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1689, average loss: 4.3690
[09/26 12:23:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.00	
[09/26 12:23:57 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 12:24:04 visual_prompt]: Epoch 73 / 100: avg data time: 4.86e-02, avg batch time: 0.4993, average train loss: 0.0138
[09/26 12:24:05 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1687, average loss: 4.1054
[09/26 12:24:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:24:05 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 12:24:12 visual_prompt]: Epoch 74 / 100: avg data time: 4.78e-02, avg batch time: 0.4984, average train loss: 0.0077
[09/26 12:24:14 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1686, average loss: 3.9643
[09/26 12:24:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 98.00	
[09/26 12:24:14 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 12:24:20 visual_prompt]: Epoch 75 / 100: avg data time: 5.76e-02, avg batch time: 0.5067, average train loss: 0.0069
[09/26 12:24:22 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1689, average loss: 4.0199
[09/26 12:24:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 97.50	
[09/26 12:24:22 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 12:24:29 visual_prompt]: Epoch 76 / 100: avg data time: 6.17e-02, avg batch time: 0.5099, average train loss: 0.0045
[09/26 12:24:30 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1690, average loss: 4.0832
[09/26 12:24:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 97.50	
[09/26 12:24:30 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 12:24:37 visual_prompt]: Epoch 77 / 100: avg data time: 5.57e-02, avg batch time: 0.5039, average train loss: 0.0033
[09/26 12:24:39 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1687, average loss: 4.1758
[09/26 12:24:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 97.50	
[09/26 12:24:39 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 12:24:46 visual_prompt]: Epoch 78 / 100: avg data time: 5.73e-02, avg batch time: 0.5057, average train loss: 0.0027
[09/26 12:24:47 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 4.2179
[09/26 12:24:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 97.50	
[09/26 12:24:47 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 12:24:54 visual_prompt]: Epoch 79 / 100: avg data time: 5.19e-02, avg batch time: 0.5031, average train loss: 0.0025
[09/26 12:24:56 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1687, average loss: 4.2633
[09/26 12:24:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 97.50	
[09/26 12:24:56 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 12:25:02 visual_prompt]: Epoch 80 / 100: avg data time: 5.48e-02, avg batch time: 0.5040, average train loss: 0.0044
[09/26 12:25:04 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1687, average loss: 4.2461
[09/26 12:25:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 97.50	
[09/26 12:25:04 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 12:25:10 visual_prompt]: Epoch 81 / 100: avg data time: 4.61e-02, avg batch time: 0.4951, average train loss: 0.0078
[09/26 12:25:12 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1684, average loss: 4.4651
[09/26 12:25:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:25:12 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 12:25:19 visual_prompt]: Epoch 82 / 100: avg data time: 5.73e-02, avg batch time: 0.5071, average train loss: 0.0037
[09/26 12:25:20 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 4.5974
[09/26 12:25:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.50	
[09/26 12:25:20 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 12:25:27 visual_prompt]: Epoch 83 / 100: avg data time: 5.89e-02, avg batch time: 0.5089, average train loss: 0.0026
[09/26 12:25:29 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1690, average loss: 4.6238
[09/26 12:25:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.50	
[09/26 12:25:29 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 12:25:36 visual_prompt]: Epoch 84 / 100: avg data time: 4.42e-02, avg batch time: 0.4941, average train loss: 0.0041
[09/26 12:25:37 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1689, average loss: 4.6032
[09/26 12:25:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 12:25:37 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 12:25:44 visual_prompt]: Epoch 85 / 100: avg data time: 4.26e-02, avg batch time: 0.4945, average train loss: 0.0025
[09/26 12:25:45 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1687, average loss: 4.4601
[09/26 12:25:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 97.50	
[09/26 12:25:45 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 12:25:52 visual_prompt]: Epoch 86 / 100: avg data time: 5.10e-02, avg batch time: 0.5007, average train loss: 0.0022
[09/26 12:25:54 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1690, average loss: 4.5017
[09/26 12:25:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:25:54 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 12:26:00 visual_prompt]: Epoch 87 / 100: avg data time: 5.03e-02, avg batch time: 0.4993, average train loss: 0.0064
[09/26 12:26:02 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1692, average loss: 4.6078
[09/26 12:26:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.00	
[09/26 12:26:02 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 12:26:09 visual_prompt]: Epoch 88 / 100: avg data time: 4.60e-02, avg batch time: 0.4961, average train loss: 0.0030
[09/26 12:26:10 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1688, average loss: 4.6072
[09/26 12:26:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.00	
[09/26 12:26:10 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 12:26:17 visual_prompt]: Epoch 89 / 100: avg data time: 5.80e-02, avg batch time: 0.5063, average train loss: 0.0027
[09/26 12:26:18 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1687, average loss: 4.5995
[09/26 12:26:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 98.00	
[09/26 12:26:18 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 12:26:25 visual_prompt]: Epoch 90 / 100: avg data time: 4.94e-02, avg batch time: 0.4978, average train loss: 0.0056
[09/26 12:26:27 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.1685, average loss: 4.5969
[09/26 12:26:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 98.00	
[09/26 12:26:27 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 12:26:33 visual_prompt]: Epoch 91 / 100: avg data time: 4.36e-02, avg batch time: 0.4920, average train loss: 0.0028
[09/26 12:26:35 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 4.5856
[09/26 12:26:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.00	
[09/26 12:26:35 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 12:26:42 visual_prompt]: Epoch 92 / 100: avg data time: 4.46e-02, avg batch time: 0.4950, average train loss: 0.0021
[09/26 12:26:43 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1689, average loss: 4.5589
[09/26 12:26:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:26:43 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 12:26:50 visual_prompt]: Epoch 93 / 100: avg data time: 4.83e-02, avg batch time: 0.4973, average train loss: 0.0022
[09/26 12:26:51 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1689, average loss: 4.5517
[09/26 12:26:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:26:51 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 12:26:58 visual_prompt]: Epoch 94 / 100: avg data time: 4.93e-02, avg batch time: 0.4998, average train loss: 0.0020
[09/26 12:27:00 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 4.5561
[09/26 12:27:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:27:00 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 12:27:06 visual_prompt]: Epoch 95 / 100: avg data time: 5.28e-02, avg batch time: 0.5024, average train loss: 0.0020
[09/26 12:27:08 visual_prompt]: Inference (val):avg data time: 4.66e-05, avg batch time: 0.1693, average loss: 4.5597
[09/26 12:27:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:27:08 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 12:27:15 visual_prompt]: Epoch 96 / 100: avg data time: 5.44e-02, avg batch time: 0.5045, average train loss: 0.0019
[09/26 12:27:16 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1689, average loss: 4.5610
[09/26 12:27:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:27:16 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 12:27:23 visual_prompt]: Epoch 97 / 100: avg data time: 5.60e-02, avg batch time: 0.5055, average train loss: 0.0016
[09/26 12:27:25 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 4.5613
[09/26 12:27:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:27:25 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 12:27:31 visual_prompt]: Epoch 98 / 100: avg data time: 4.55e-02, avg batch time: 0.4947, average train loss: 0.0060
[09/26 12:27:33 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1692, average loss: 4.5673
[09/26 12:27:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:27:33 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 12:27:40 visual_prompt]: Epoch 99 / 100: avg data time: 4.62e-02, avg batch time: 0.4957, average train loss: 0.0017
[09/26 12:27:41 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1689, average loss: 4.5691
[09/26 12:27:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:27:41 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 12:27:48 visual_prompt]: Epoch 100 / 100: avg data time: 5.87e-02, avg batch time: 0.5065, average train loss: 0.0017
[09/26 12:27:50 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 4.5691
[09/26 12:27:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:27:50 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:27:50 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:27:50 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:27:50 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:27:50 visual_prompt]: Training with config:
[09/26 12:27:50 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:27:50 visual_prompt]: Loading training data...
[09/26 12:27:50 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 12:27:51 visual_prompt]: Number of images: 800
[09/26 12:27:51 visual_prompt]: Number of classes: 6 / 6
[09/26 12:27:51 visual_prompt]: Loading validation data...
[09/26 12:27:51 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 12:27:51 visual_prompt]: Number of images: 200
[09/26 12:27:51 visual_prompt]: Number of classes: 6 / 6
[09/26 12:27:51 visual_prompt]: Constructing models...
[09/26 12:27:54 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 12:27:54 visual_prompt]: tuned percent:0.540
[09/26 12:27:54 visual_prompt]: Device used for model: 0
[09/26 12:27:54 visual_prompt]: Setting up Evaluator...
[09/26 12:27:54 visual_prompt]: Setting up Trainer...
[09/26 12:27:54 visual_prompt]: 	Setting up the optimizer...
[09/26 12:27:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:28:01 visual_prompt]: Epoch 1 / 100: avg data time: 4.62e-02, avg batch time: 0.4943, average train loss: 2.9755
[09/26 12:28:02 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1681, average loss: 2.9268
[09/26 12:28:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 12:28:02 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 12:28:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 12:28:09 visual_prompt]: Epoch 2 / 100: avg data time: 5.24e-02, avg batch time: 0.5003, average train loss: 2.8800
[09/26 12:28:10 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1681, average loss: 2.4030
[09/26 12:28:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 12:28:10 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 12:28:17 visual_prompt]: Epoch 3 / 100: avg data time: 5.73e-02, avg batch time: 0.5047, average train loss: 1.9645
[09/26 12:28:19 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1682, average loss: 1.9056
[09/26 12:28:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 12:28:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 12:28:26 visual_prompt]: Epoch 4 / 100: avg data time: 5.75e-02, avg batch time: 0.5059, average train loss: 1.8303
[09/26 12:28:27 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1682, average loss: 1.8153
[09/26 12:28:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 12:28:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 12:28:34 visual_prompt]: Epoch 5 / 100: avg data time: 6.33e-02, avg batch time: 0.5104, average train loss: 1.8038
[09/26 12:28:36 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1682, average loss: 1.8066
[09/26 12:28:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 84.00	
[09/26 12:28:36 visual_prompt]: Best epoch 5: best metric: 0.200
[09/26 12:28:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 12:28:42 visual_prompt]: Epoch 6 / 100: avg data time: 4.85e-02, avg batch time: 0.4966, average train loss: 1.7844
[09/26 12:28:44 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1685, average loss: 1.9748
[09/26 12:28:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 84.00	
[09/26 12:28:44 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 12:28:51 visual_prompt]: Epoch 7 / 100: avg data time: 5.28e-02, avg batch time: 0.5011, average train loss: 1.7839
[09/26 12:28:52 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1686, average loss: 2.2206
[09/26 12:28:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 87.00	
[09/26 12:28:52 visual_prompt]: Best epoch 7: best metric: 0.230
[09/26 12:28:52 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 12:28:59 visual_prompt]: Epoch 8 / 100: avg data time: 5.41e-02, avg batch time: 0.5024, average train loss: 1.7557
[09/26 12:29:01 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1686, average loss: 1.7283
[09/26 12:29:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 89.50	
[09/26 12:29:01 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 12:29:07 visual_prompt]: Epoch 9 / 100: avg data time: 5.29e-02, avg batch time: 0.5005, average train loss: 1.6097
[09/26 12:29:09 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1686, average loss: 1.5520
[09/26 12:29:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 94.50	
[09/26 12:29:09 visual_prompt]: Best epoch 9: best metric: 0.265
[09/26 12:29:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 12:29:16 visual_prompt]: Epoch 10 / 100: avg data time: 5.56e-02, avg batch time: 0.5056, average train loss: 1.5721
[09/26 12:29:17 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1692, average loss: 1.7132
[09/26 12:29:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 94.50	
[09/26 12:29:17 visual_prompt]: Best epoch 10: best metric: 0.295
[09/26 12:29:17 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 12:29:24 visual_prompt]: Epoch 11 / 100: avg data time: 4.34e-02, avg batch time: 0.4925, average train loss: 1.5054
[09/26 12:29:25 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1689, average loss: 1.5966
[09/26 12:29:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 98.50	
[09/26 12:29:25 visual_prompt]: Best epoch 11: best metric: 0.315
[09/26 12:29:25 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 12:29:32 visual_prompt]: Epoch 12 / 100: avg data time: 5.63e-02, avg batch time: 0.5040, average train loss: 1.4223
[09/26 12:29:34 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1687, average loss: 1.5279
[09/26 12:29:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 96.00	
[09/26 12:29:34 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 12:29:41 visual_prompt]: Epoch 13 / 100: avg data time: 5.87e-02, avg batch time: 0.5078, average train loss: 1.4700
[09/26 12:29:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1687, average loss: 1.5841
[09/26 12:29:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 98.50	
[09/26 12:29:42 visual_prompt]: Best epoch 13: best metric: 0.335
[09/26 12:29:42 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 12:29:49 visual_prompt]: Epoch 14 / 100: avg data time: 5.54e-02, avg batch time: 0.5034, average train loss: 1.4311
[09/26 12:29:51 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1688, average loss: 1.4407
[09/26 12:29:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 12:29:51 visual_prompt]: Best epoch 14: best metric: 0.365
[09/26 12:29:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 12:29:57 visual_prompt]: Epoch 15 / 100: avg data time: 5.96e-02, avg batch time: 0.5073, average train loss: 1.3822
[09/26 12:29:59 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1690, average loss: 1.3900
[09/26 12:29:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.00	
[09/26 12:29:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 12:30:06 visual_prompt]: Epoch 16 / 100: avg data time: 5.23e-02, avg batch time: 0.5010, average train loss: 1.2788
[09/26 12:30:07 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1685, average loss: 1.5269
[09/26 12:30:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 99.00	
[09/26 12:30:07 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 12:30:14 visual_prompt]: Epoch 17 / 100: avg data time: 5.44e-02, avg batch time: 0.5043, average train loss: 1.3036
[09/26 12:30:16 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1686, average loss: 2.0172
[09/26 12:30:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 91.00	
[09/26 12:30:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 12:30:23 visual_prompt]: Epoch 18 / 100: avg data time: 5.86e-02, avg batch time: 0.5066, average train loss: 1.1764
[09/26 12:30:24 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1688, average loss: 1.6758
[09/26 12:30:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 97.00	
[09/26 12:30:24 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 12:30:31 visual_prompt]: Epoch 19 / 100: avg data time: 5.39e-02, avg batch time: 0.5018, average train loss: 1.1937
[09/26 12:30:32 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 1.7769
[09/26 12:30:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 98.50	
[09/26 12:30:32 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 12:30:39 visual_prompt]: Epoch 20 / 100: avg data time: 5.47e-02, avg batch time: 0.5056, average train loss: 1.1028
[09/26 12:30:41 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 1.4957
[09/26 12:30:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 98.00	
[09/26 12:30:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 12:30:48 visual_prompt]: Epoch 21 / 100: avg data time: 6.04e-02, avg batch time: 0.5088, average train loss: 0.9740
[09/26 12:30:49 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1690, average loss: 1.5219
[09/26 12:30:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.50	
[09/26 12:30:49 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 12:30:56 visual_prompt]: Epoch 22 / 100: avg data time: 4.13e-02, avg batch time: 0.4915, average train loss: 0.9280
[09/26 12:30:57 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1686, average loss: 1.5622
[09/26 12:30:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.00	
[09/26 12:30:57 visual_prompt]: Best epoch 22: best metric: 0.410
[09/26 12:30:57 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 12:31:04 visual_prompt]: Epoch 23 / 100: avg data time: 5.35e-02, avg batch time: 0.5029, average train loss: 0.9662
[09/26 12:31:06 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1691, average loss: 1.5750
[09/26 12:31:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 12:31:06 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 12:31:12 visual_prompt]: Epoch 24 / 100: avg data time: 4.20e-02, avg batch time: 0.4928, average train loss: 0.8984
[09/26 12:31:14 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1690, average loss: 1.6977
[09/26 12:31:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 12:31:14 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 12:31:21 visual_prompt]: Epoch 25 / 100: avg data time: 5.45e-02, avg batch time: 0.5034, average train loss: 0.9286
[09/26 12:31:22 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1685, average loss: 1.7423
[09/26 12:31:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 12:31:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 12:31:29 visual_prompt]: Epoch 26 / 100: avg data time: 4.55e-02, avg batch time: 0.4946, average train loss: 1.0149
[09/26 12:31:30 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1687, average loss: 1.9800
[09/26 12:31:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 12:31:30 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 12:31:37 visual_prompt]: Epoch 27 / 100: avg data time: 6.09e-02, avg batch time: 0.5095, average train loss: 1.0567
[09/26 12:31:39 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1688, average loss: 1.6936
[09/26 12:31:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.00	
[09/26 12:31:39 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 12:31:46 visual_prompt]: Epoch 28 / 100: avg data time: 4.89e-02, avg batch time: 0.4986, average train loss: 0.8075
[09/26 12:31:47 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1685, average loss: 1.9037
[09/26 12:31:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 12:31:47 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 12:31:54 visual_prompt]: Epoch 29 / 100: avg data time: 5.62e-02, avg batch time: 0.5044, average train loss: 0.6977
[09/26 12:31:55 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1690, average loss: 1.9114
[09/26 12:31:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 12:31:55 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 12:32:02 visual_prompt]: Epoch 30 / 100: avg data time: 5.23e-02, avg batch time: 0.5012, average train loss: 0.6884
[09/26 12:32:04 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1688, average loss: 2.3254
[09/26 12:32:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.00	
[09/26 12:32:04 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 12:32:11 visual_prompt]: Epoch 31 / 100: avg data time: 4.38e-02, avg batch time: 0.4952, average train loss: 0.7692
[09/26 12:32:12 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1689, average loss: 2.4066
[09/26 12:32:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 12:32:12 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 12:32:19 visual_prompt]: Epoch 32 / 100: avg data time: 4.18e-02, avg batch time: 0.4924, average train loss: 0.8390
[09/26 12:32:20 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 2.0972
[09/26 12:32:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.00	
[09/26 12:32:20 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 12:32:27 visual_prompt]: Epoch 33 / 100: avg data time: 4.66e-02, avg batch time: 0.4949, average train loss: 0.5972
[09/26 12:32:28 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1689, average loss: 1.8339
[09/26 12:32:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 98.00	
[09/26 12:32:28 visual_prompt]: Best epoch 33: best metric: 0.455
[09/26 12:32:28 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 12:32:35 visual_prompt]: Epoch 34 / 100: avg data time: 5.52e-02, avg batch time: 0.5036, average train loss: 0.5190
[09/26 12:32:37 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1691, average loss: 2.2574
[09/26 12:32:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.00	
[09/26 12:32:37 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 12:32:43 visual_prompt]: Epoch 35 / 100: avg data time: 4.58e-02, avg batch time: 0.4946, average train loss: 0.5416
[09/26 12:32:45 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1689, average loss: 1.9057
[09/26 12:32:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 98.50	
[09/26 12:32:45 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 12:32:52 visual_prompt]: Epoch 36 / 100: avg data time: 5.78e-02, avg batch time: 0.5074, average train loss: 0.4636
[09/26 12:32:53 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 2.8344
[09/26 12:32:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 12:32:53 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 12:33:00 visual_prompt]: Epoch 37 / 100: avg data time: 5.03e-02, avg batch time: 0.5005, average train loss: 0.4913
[09/26 12:33:02 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1692, average loss: 2.6954
[09/26 12:33:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 12:33:02 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 12:33:08 visual_prompt]: Epoch 38 / 100: avg data time: 4.87e-02, avg batch time: 0.4987, average train loss: 0.4868
[09/26 12:33:10 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1689, average loss: 2.3692
[09/26 12:33:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/26 12:33:10 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 12:33:17 visual_prompt]: Epoch 39 / 100: avg data time: 4.98e-02, avg batch time: 0.4984, average train loss: 0.3363
[09/26 12:33:18 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1689, average loss: 2.6672
[09/26 12:33:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 12:33:18 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 12:33:25 visual_prompt]: Epoch 40 / 100: avg data time: 5.27e-02, avg batch time: 0.5032, average train loss: 0.2820
[09/26 12:33:26 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 3.0309
[09/26 12:33:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.50	
[09/26 12:33:26 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 12:33:33 visual_prompt]: Epoch 41 / 100: avg data time: 4.77e-02, avg batch time: 0.4981, average train loss: 0.2208
[09/26 12:33:35 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1687, average loss: 3.4922
[09/26 12:33:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.50	
[09/26 12:33:35 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 12:33:42 visual_prompt]: Epoch 42 / 100: avg data time: 6.38e-02, avg batch time: 0.5133, average train loss: 0.2086
[09/26 12:33:43 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1687, average loss: 2.9694
[09/26 12:33:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 97.00	
[09/26 12:33:43 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 12:33:50 visual_prompt]: Epoch 43 / 100: avg data time: 5.36e-02, avg batch time: 0.5023, average train loss: 0.2128
[09/26 12:33:51 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1687, average loss: 3.1409
[09/26 12:33:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.00	
[09/26 12:33:51 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 12:33:58 visual_prompt]: Epoch 44 / 100: avg data time: 5.28e-02, avg batch time: 0.5010, average train loss: 0.2030
[09/26 12:34:00 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1689, average loss: 3.3092
[09/26 12:34:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.00	
[09/26 12:34:00 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 12:34:07 visual_prompt]: Epoch 45 / 100: avg data time: 5.58e-02, avg batch time: 0.5040, average train loss: 0.1671
[09/26 12:34:08 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1688, average loss: 3.4075
[09/26 12:34:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 12:34:08 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 12:34:15 visual_prompt]: Epoch 46 / 100: avg data time: 5.92e-02, avg batch time: 0.5083, average train loss: 0.1464
[09/26 12:34:17 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1690, average loss: 3.1529
[09/26 12:34:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 12:34:17 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 12:34:23 visual_prompt]: Epoch 47 / 100: avg data time: 4.81e-02, avg batch time: 0.4969, average train loss: 0.1759
[09/26 12:34:25 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1686, average loss: 3.0273
[09/26 12:34:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 97.50	
[09/26 12:34:25 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 12:34:32 visual_prompt]: Epoch 48 / 100: avg data time: 5.93e-02, avg batch time: 0.5072, average train loss: 0.1768
[09/26 12:34:33 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1684, average loss: 3.3473
[09/26 12:34:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 12:34:33 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 12:34:40 visual_prompt]: Epoch 49 / 100: avg data time: 4.59e-02, avg batch time: 0.4962, average train loss: 0.1018
[09/26 12:34:41 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1688, average loss: 3.5193
[09/26 12:34:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 12:34:41 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 12:34:48 visual_prompt]: Epoch 50 / 100: avg data time: 5.95e-02, avg batch time: 0.5082, average train loss: 0.0917
[09/26 12:34:50 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1687, average loss: 3.7614
[09/26 12:34:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 12:34:50 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 12:34:57 visual_prompt]: Epoch 51 / 100: avg data time: 5.72e-02, avg batch time: 0.5062, average train loss: 0.1240
[09/26 12:34:58 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1689, average loss: 4.2713
[09/26 12:34:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 12:34:58 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 12:35:05 visual_prompt]: Epoch 52 / 100: avg data time: 6.15e-02, avg batch time: 0.5098, average train loss: 0.1834
[09/26 12:35:06 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 3.4566
[09/26 12:35:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 12:35:07 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 12:35:13 visual_prompt]: Epoch 53 / 100: avg data time: 5.48e-02, avg batch time: 0.5040, average train loss: 0.1748
[09/26 12:35:15 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 3.4598
[09/26 12:35:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.00	
[09/26 12:35:15 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 12:35:22 visual_prompt]: Epoch 54 / 100: avg data time: 5.81e-02, avg batch time: 0.5072, average train loss: 0.1123
[09/26 12:35:23 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1686, average loss: 3.2353
[09/26 12:35:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 12:35:23 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 12:35:30 visual_prompt]: Epoch 55 / 100: avg data time: 5.31e-02, avg batch time: 0.5025, average train loss: 0.0946
[09/26 12:35:32 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1692, average loss: 3.5910
[09/26 12:35:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 12:35:32 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 12:35:38 visual_prompt]: Epoch 56 / 100: avg data time: 5.39e-02, avg batch time: 0.5029, average train loss: 0.0437
[09/26 12:35:40 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1690, average loss: 4.3435
[09/26 12:35:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 12:35:40 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 12:35:47 visual_prompt]: Epoch 57 / 100: avg data time: 5.34e-02, avg batch time: 0.5021, average train loss: 0.0677
[09/26 12:35:48 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1690, average loss: 3.9441
[09/26 12:35:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 97.00	
[09/26 12:35:48 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 12:35:55 visual_prompt]: Epoch 58 / 100: avg data time: 5.09e-02, avg batch time: 0.4992, average train loss: 0.0449
[09/26 12:35:57 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1689, average loss: 3.8796
[09/26 12:35:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.00	
[09/26 12:35:57 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 12:36:04 visual_prompt]: Epoch 59 / 100: avg data time: 6.53e-02, avg batch time: 0.5146, average train loss: 0.0262
[09/26 12:36:05 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1689, average loss: 4.3384
[09/26 12:36:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.50	
[09/26 12:36:05 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 12:36:12 visual_prompt]: Epoch 60 / 100: avg data time: 5.47e-02, avg batch time: 0.5035, average train loss: 0.0239
[09/26 12:36:13 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 4.3632
[09/26 12:36:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 96.50	
[09/26 12:36:13 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 12:36:20 visual_prompt]: Epoch 61 / 100: avg data time: 4.79e-02, avg batch time: 0.4976, average train loss: 0.0236
[09/26 12:36:22 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 4.3012
[09/26 12:36:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 96.50	
[09/26 12:36:22 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 12:36:29 visual_prompt]: Epoch 62 / 100: avg data time: 5.73e-02, avg batch time: 0.5062, average train loss: 0.0210
[09/26 12:36:30 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 4.4164
[09/26 12:36:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.00	
[09/26 12:36:30 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 12:36:37 visual_prompt]: Epoch 63 / 100: avg data time: 5.93e-02, avg batch time: 0.5089, average train loss: 0.0131
[09/26 12:36:39 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1690, average loss: 4.5773
[09/26 12:36:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.00	
[09/26 12:36:39 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 12:36:45 visual_prompt]: Epoch 64 / 100: avg data time: 4.79e-02, avg batch time: 0.4972, average train loss: 0.0265
[09/26 12:36:47 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1691, average loss: 4.9157
[09/26 12:36:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.00	
[09/26 12:36:47 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 12:36:54 visual_prompt]: Epoch 65 / 100: avg data time: 5.12e-02, avg batch time: 0.5005, average train loss: 0.0296
[09/26 12:36:55 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1687, average loss: 4.3426
[09/26 12:36:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.00	
[09/26 12:36:55 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 12:37:02 visual_prompt]: Epoch 66 / 100: avg data time: 5.32e-02, avg batch time: 0.5024, average train loss: 0.0310
[09/26 12:37:03 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1689, average loss: 4.6165
[09/26 12:37:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.00	
[09/26 12:37:03 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 12:37:10 visual_prompt]: Epoch 67 / 100: avg data time: 4.42e-02, avg batch time: 0.4948, average train loss: 0.0189
[09/26 12:37:12 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1689, average loss: 4.4065
[09/26 12:37:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 12:37:12 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 12:37:19 visual_prompt]: Epoch 68 / 100: avg data time: 5.62e-02, avg batch time: 0.5055, average train loss: 0.0155
[09/26 12:37:20 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1689, average loss: 4.3347
[09/26 12:37:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 96.50	
[09/26 12:37:20 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 12:37:27 visual_prompt]: Epoch 69 / 100: avg data time: 4.56e-02, avg batch time: 0.4948, average train loss: 0.0054
[09/26 12:37:28 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1689, average loss: 4.4685
[09/26 12:37:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 96.50	
[09/26 12:37:28 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 12:37:35 visual_prompt]: Epoch 70 / 100: avg data time: 5.14e-02, avg batch time: 0.5003, average train loss: 0.0066
[09/26 12:37:37 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1689, average loss: 4.6274
[09/26 12:37:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 96.50	
[09/26 12:37:37 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 12:37:43 visual_prompt]: Epoch 71 / 100: avg data time: 5.17e-02, avg batch time: 0.5012, average train loss: 0.0061
[09/26 12:37:45 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1688, average loss: 4.6503
[09/26 12:37:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 96.50	
[09/26 12:37:45 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 12:37:52 visual_prompt]: Epoch 72 / 100: avg data time: 6.24e-02, avg batch time: 0.5105, average train loss: 0.0046
[09/26 12:37:53 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1690, average loss: 4.6625
[09/26 12:37:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.00	
[09/26 12:37:53 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 12:38:00 visual_prompt]: Epoch 73 / 100: avg data time: 6.16e-02, avg batch time: 0.5107, average train loss: 0.0035
[09/26 12:38:02 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1689, average loss: 4.8394
[09/26 12:38:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 96.50	
[09/26 12:38:02 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 12:38:09 visual_prompt]: Epoch 74 / 100: avg data time: 6.61e-02, avg batch time: 0.5155, average train loss: 0.0026
[09/26 12:38:10 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1688, average loss: 4.9285
[09/26 12:38:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 96.50	
[09/26 12:38:10 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 12:38:17 visual_prompt]: Epoch 75 / 100: avg data time: 5.61e-02, avg batch time: 0.5040, average train loss: 0.0026
[09/26 12:38:19 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1686, average loss: 4.9604
[09/26 12:38:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 96.50	
[09/26 12:38:19 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 12:38:26 visual_prompt]: Epoch 76 / 100: avg data time: 5.58e-02, avg batch time: 0.5045, average train loss: 0.0026
[09/26 12:38:27 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 5.0111
[09/26 12:38:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 96.50	
[09/26 12:38:27 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 12:38:34 visual_prompt]: Epoch 77 / 100: avg data time: 5.52e-02, avg batch time: 0.5044, average train loss: 0.0045
[09/26 12:38:35 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1687, average loss: 5.0357
[09/26 12:38:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 12:38:35 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 12:38:42 visual_prompt]: Epoch 78 / 100: avg data time: 6.00e-02, avg batch time: 0.5092, average train loss: 0.0031
[09/26 12:38:44 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 5.0679
[09/26 12:38:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.50	
[09/26 12:38:44 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 12:38:51 visual_prompt]: Epoch 79 / 100: avg data time: 4.54e-02, avg batch time: 0.4960, average train loss: 0.0069
[09/26 12:38:52 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 5.0475
[09/26 12:38:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.00	
[09/26 12:38:52 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 12:38:59 visual_prompt]: Epoch 80 / 100: avg data time: 5.05e-02, avg batch time: 0.4999, average train loss: 0.0036
[09/26 12:39:00 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1690, average loss: 5.0417
[09/26 12:39:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 96.50	
[09/26 12:39:00 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 12:39:07 visual_prompt]: Epoch 81 / 100: avg data time: 6.10e-02, avg batch time: 0.5094, average train loss: 0.0025
[09/26 12:39:09 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1685, average loss: 5.0448
[09/26 12:39:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 96.50	
[09/26 12:39:09 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 12:39:16 visual_prompt]: Epoch 82 / 100: avg data time: 5.32e-02, avg batch time: 0.5032, average train loss: 0.0024
[09/26 12:39:17 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1689, average loss: 5.0573
[09/26 12:39:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 96.50	
[09/26 12:39:17 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 12:39:24 visual_prompt]: Epoch 83 / 100: avg data time: 4.56e-02, avg batch time: 0.4954, average train loss: 0.0029
[09/26 12:39:25 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1693, average loss: 5.0518
[09/26 12:39:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 96.50	
[09/26 12:39:25 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 12:39:32 visual_prompt]: Epoch 84 / 100: avg data time: 5.72e-02, avg batch time: 0.5055, average train loss: 0.0025
[09/26 12:39:34 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1694, average loss: 5.0605
[09/26 12:39:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:39:34 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 12:39:41 visual_prompt]: Epoch 85 / 100: avg data time: 6.63e-02, avg batch time: 0.5142, average train loss: 0.0024
[09/26 12:39:42 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1687, average loss: 5.0732
[09/26 12:39:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 12:39:42 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 12:39:49 visual_prompt]: Epoch 86 / 100: avg data time: 4.96e-02, avg batch time: 0.4992, average train loss: 0.0025
[09/26 12:39:51 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1690, average loss: 5.0915
[09/26 12:39:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.50	
[09/26 12:39:51 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 12:39:57 visual_prompt]: Epoch 87 / 100: avg data time: 5.53e-02, avg batch time: 0.5040, average train loss: 0.0021
[09/26 12:39:59 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1690, average loss: 5.1003
[09/26 12:39:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 12:39:59 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 12:40:06 visual_prompt]: Epoch 88 / 100: avg data time: 6.12e-02, avg batch time: 0.5096, average train loss: 0.0023
[09/26 12:40:07 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1688, average loss: 5.1046
[09/26 12:40:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:40:07 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 12:40:14 visual_prompt]: Epoch 89 / 100: avg data time: 6.40e-02, avg batch time: 0.5124, average train loss: 0.0024
[09/26 12:40:16 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1689, average loss: 5.1154
[09/26 12:40:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:40:16 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 12:40:23 visual_prompt]: Epoch 90 / 100: avg data time: 4.74e-02, avg batch time: 0.4961, average train loss: 0.0020
[09/26 12:40:24 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1687, average loss: 5.1262
[09/26 12:40:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:40:24 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 12:40:31 visual_prompt]: Epoch 91 / 100: avg data time: 5.90e-02, avg batch time: 0.5098, average train loss: 0.0021
[09/26 12:40:32 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1687, average loss: 5.1282
[09/26 12:40:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:40:32 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 12:40:39 visual_prompt]: Epoch 92 / 100: avg data time: 5.50e-02, avg batch time: 0.5040, average train loss: 0.0020
[09/26 12:40:41 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1691, average loss: 5.1309
[09/26 12:40:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:40:41 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 12:40:48 visual_prompt]: Epoch 93 / 100: avg data time: 4.81e-02, avg batch time: 0.4982, average train loss: 0.0019
[09/26 12:40:49 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1689, average loss: 5.1334
[09/26 12:40:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:40:49 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 12:40:56 visual_prompt]: Epoch 94 / 100: avg data time: 4.75e-02, avg batch time: 0.4976, average train loss: 0.0021
[09/26 12:40:57 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1689, average loss: 5.1345
[09/26 12:40:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:40:57 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 12:41:04 visual_prompt]: Epoch 95 / 100: avg data time: 4.84e-02, avg batch time: 0.4990, average train loss: 0.0022
[09/26 12:41:06 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1691, average loss: 5.1357
[09/26 12:41:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:41:06 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 12:41:12 visual_prompt]: Epoch 96 / 100: avg data time: 5.37e-02, avg batch time: 0.5026, average train loss: 0.0022
[09/26 12:41:14 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1689, average loss: 5.1365
[09/26 12:41:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:41:14 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 12:41:21 visual_prompt]: Epoch 97 / 100: avg data time: 5.07e-02, avg batch time: 0.5001, average train loss: 0.0028
[09/26 12:41:22 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1688, average loss: 5.1368
[09/26 12:41:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:41:22 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 12:41:29 visual_prompt]: Epoch 98 / 100: avg data time: 5.41e-02, avg batch time: 0.5031, average train loss: 0.0022
[09/26 12:41:31 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1691, average loss: 5.1362
[09/26 12:41:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:41:31 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 12:41:38 visual_prompt]: Epoch 99 / 100: avg data time: 5.99e-02, avg batch time: 0.5092, average train loss: 0.0022
[09/26 12:41:39 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 5.1363
[09/26 12:41:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:41:39 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 12:41:46 visual_prompt]: Epoch 100 / 100: avg data time: 5.41e-02, avg batch time: 0.5019, average train loss: 0.0020
[09/26 12:41:48 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1689, average loss: 5.1363
[09/26 12:41:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 12:41:48 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:41:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:41:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:41:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:41:48 visual_prompt]: Training with config:
[09/26 12:41:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:41:48 visual_prompt]: Loading training data...
[09/26 12:41:48 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 12:41:49 visual_prompt]: Number of images: 800
[09/26 12:41:49 visual_prompt]: Number of classes: 6 / 6
[09/26 12:41:49 visual_prompt]: Loading validation data...
[09/26 12:41:49 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 12:41:49 visual_prompt]: Number of images: 200
[09/26 12:41:49 visual_prompt]: Number of classes: 6 / 6
[09/26 12:41:49 visual_prompt]: Constructing models...
[09/26 12:41:52 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 12:41:52 visual_prompt]: tuned percent:0.540
[09/26 12:41:52 visual_prompt]: Device used for model: 0
[09/26 12:41:52 visual_prompt]: Setting up Evaluator...
[09/26 12:41:52 visual_prompt]: Setting up Trainer...
[09/26 12:41:52 visual_prompt]: 	Setting up the optimizer...
[09/26 12:41:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:41:59 visual_prompt]: Epoch 1 / 100: avg data time: 4.72e-02, avg batch time: 0.4975, average train loss: 2.9859
[09/26 12:42:00 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1683, average loss: 2.9268
[09/26 12:42:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 12:42:00 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 12:42:00 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 12:42:07 visual_prompt]: Epoch 2 / 100: avg data time: 5.30e-02, avg batch time: 0.5004, average train loss: 2.7407
[09/26 12:42:09 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1683, average loss: 1.8949
[09/26 12:42:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 12:42:09 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 12:42:15 visual_prompt]: Epoch 3 / 100: avg data time: 4.58e-02, avg batch time: 0.4951, average train loss: 1.8134
[09/26 12:42:17 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1685, average loss: 1.8097
[09/26 12:42:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 82.50	
[09/26 12:42:17 visual_prompt]: Best epoch 3: best metric: 0.225
[09/26 12:42:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 12:42:24 visual_prompt]: Epoch 4 / 100: avg data time: 4.76e-02, avg batch time: 0.4959, average train loss: 1.8026
[09/26 12:42:25 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1685, average loss: 1.8162
[09/26 12:42:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 79.50	
[09/26 12:42:25 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 12:42:32 visual_prompt]: Epoch 5 / 100: avg data time: 5.74e-02, avg batch time: 0.5045, average train loss: 1.7641
[09/26 12:42:34 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1680, average loss: 1.8246
[09/26 12:42:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 84.00	
[09/26 12:42:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 12:42:40 visual_prompt]: Epoch 6 / 100: avg data time: 4.69e-02, avg batch time: 0.4960, average train loss: 1.7548
[09/26 12:42:42 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1683, average loss: 1.7336
[09/26 12:42:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 86.50	
[09/26 12:42:42 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 12:42:49 visual_prompt]: Epoch 7 / 100: avg data time: 5.05e-02, avg batch time: 0.4995, average train loss: 1.7431
[09/26 12:42:50 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1685, average loss: 1.8121
[09/26 12:42:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 84.00	
[09/26 12:42:50 visual_prompt]: Best epoch 7: best metric: 0.230
[09/26 12:42:50 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 12:42:57 visual_prompt]: Epoch 8 / 100: avg data time: 4.31e-02, avg batch time: 0.4925, average train loss: 1.7746
[09/26 12:42:58 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1687, average loss: 1.8189
[09/26 12:42:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 12:42:58 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 12:43:05 visual_prompt]: Epoch 9 / 100: avg data time: 5.75e-02, avg batch time: 0.5055, average train loss: 1.8048
[09/26 12:43:07 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 1.8523
[09/26 12:43:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 12:43:07 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 12:43:14 visual_prompt]: Epoch 10 / 100: avg data time: 4.89e-02, avg batch time: 0.4971, average train loss: 1.8187
[09/26 12:43:15 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.1690, average loss: 1.8516
[09/26 12:43:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:43:15 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 12:43:22 visual_prompt]: Epoch 11 / 100: avg data time: 4.48e-02, avg batch time: 0.4937, average train loss: 1.7918
[09/26 12:43:23 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1688, average loss: 1.8197
[09/26 12:43:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 12:43:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 12:43:30 visual_prompt]: Epoch 12 / 100: avg data time: 4.50e-02, avg batch time: 0.4953, average train loss: 1.8225
[09/26 12:43:32 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1686, average loss: 1.8318
[09/26 12:43:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 12:43:32 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 12:43:38 visual_prompt]: Epoch 13 / 100: avg data time: 5.12e-02, avg batch time: 0.4984, average train loss: 1.8655
[09/26 12:43:40 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1686, average loss: 1.7910
[09/26 12:43:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/26 12:43:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 12:43:47 visual_prompt]: Epoch 14 / 100: avg data time: 4.77e-02, avg batch time: 0.4962, average train loss: 1.8289
[09/26 12:43:48 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1686, average loss: 1.9279
[09/26 12:43:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 12:43:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 12:43:55 visual_prompt]: Epoch 15 / 100: avg data time: 5.10e-02, avg batch time: 0.4999, average train loss: 1.8278
[09/26 12:43:56 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1685, average loss: 1.8152
[09/26 12:43:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 12:43:56 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 12:44:03 visual_prompt]: Epoch 16 / 100: avg data time: 5.63e-02, avg batch time: 0.5032, average train loss: 1.8075
[09/26 12:44:05 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1687, average loss: 1.8638
[09/26 12:44:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 12:44:05 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 12:44:12 visual_prompt]: Epoch 17 / 100: avg data time: 4.37e-02, avg batch time: 0.4926, average train loss: 1.8117
[09/26 12:44:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1688, average loss: 1.8258
[09/26 12:44:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 12:44:13 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 12:44:20 visual_prompt]: Epoch 18 / 100: avg data time: 5.86e-02, avg batch time: 0.5061, average train loss: 1.8127
[09/26 12:44:22 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1686, average loss: 1.8647
[09/26 12:44:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:44:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 12:44:28 visual_prompt]: Epoch 19 / 100: avg data time: 5.29e-02, avg batch time: 0.5016, average train loss: 1.7956
[09/26 12:44:30 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1686, average loss: 1.8839
[09/26 12:44:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:44:30 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 12:44:37 visual_prompt]: Epoch 20 / 100: avg data time: 5.65e-02, avg batch time: 0.5044, average train loss: 1.8234
[09/26 12:44:38 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1686, average loss: 1.8440
[09/26 12:44:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 12:44:38 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 12:44:45 visual_prompt]: Epoch 21 / 100: avg data time: 5.79e-02, avg batch time: 0.5054, average train loss: 1.8151
[09/26 12:44:47 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1685, average loss: 1.8507
[09/26 12:44:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:44:47 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 12:44:53 visual_prompt]: Epoch 22 / 100: avg data time: 5.70e-02, avg batch time: 0.5060, average train loss: 1.7933
[09/26 12:44:55 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1684, average loss: 1.9678
[09/26 12:44:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 12:44:55 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 12:45:02 visual_prompt]: Epoch 23 / 100: avg data time: 5.08e-02, avg batch time: 0.4988, average train loss: 1.8432
[09/26 12:45:03 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1692, average loss: 1.8318
[09/26 12:45:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:45:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 12:45:10 visual_prompt]: Epoch 24 / 100: avg data time: 6.00e-02, avg batch time: 0.5082, average train loss: 1.8030
[09/26 12:45:12 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1685, average loss: 1.8050
[09/26 12:45:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:45:12 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 12:45:18 visual_prompt]: Epoch 25 / 100: avg data time: 4.34e-02, avg batch time: 0.4925, average train loss: 1.7976
[09/26 12:45:20 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1685, average loss: 1.8165
[09/26 12:45:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:45:20 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 12:45:27 visual_prompt]: Epoch 26 / 100: avg data time: 4.80e-02, avg batch time: 0.4958, average train loss: 1.7987
[09/26 12:45:28 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1682, average loss: 1.8467
[09/26 12:45:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 12:45:28 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 12:45:35 visual_prompt]: Epoch 27 / 100: avg data time: 4.16e-02, avg batch time: 0.4886, average train loss: 1.8034
[09/26 12:45:36 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1683, average loss: 1.8841
[09/26 12:45:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 12:45:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 12:45:43 visual_prompt]: Epoch 28 / 100: avg data time: 4.94e-02, avg batch time: 0.4974, average train loss: 1.8327
[09/26 12:45:45 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1681, average loss: 1.8127
[09/26 12:45:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 12:45:45 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 12:45:52 visual_prompt]: Epoch 29 / 100: avg data time: 5.74e-02, avg batch time: 0.5054, average train loss: 1.8364
[09/26 12:45:53 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1684, average loss: 1.9057
[09/26 12:45:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:45:53 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 12:46:00 visual_prompt]: Epoch 30 / 100: avg data time: 5.51e-02, avg batch time: 0.5023, average train loss: 1.8321
[09/26 12:46:01 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1685, average loss: 1.9011
[09/26 12:46:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:46:01 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 12:46:08 visual_prompt]: Epoch 31 / 100: avg data time: 6.41e-02, avg batch time: 0.5110, average train loss: 1.7969
[09/26 12:46:10 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1686, average loss: 1.8061
[09/26 12:46:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:46:10 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 12:46:17 visual_prompt]: Epoch 32 / 100: avg data time: 6.14e-02, avg batch time: 0.5090, average train loss: 1.7952
[09/26 12:46:18 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1685, average loss: 1.8673
[09/26 12:46:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 12:46:18 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 12:46:25 visual_prompt]: Epoch 33 / 100: avg data time: 5.31e-02, avg batch time: 0.5019, average train loss: 1.7921
[09/26 12:46:27 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1684, average loss: 1.8278
[09/26 12:46:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:46:27 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 12:46:34 visual_prompt]: Epoch 34 / 100: avg data time: 6.27e-02, avg batch time: 0.5095, average train loss: 1.8485
[09/26 12:46:35 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1685, average loss: 1.8796
[09/26 12:46:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:46:35 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 12:46:42 visual_prompt]: Epoch 35 / 100: avg data time: 6.03e-02, avg batch time: 0.5086, average train loss: 1.8133
[09/26 12:46:44 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1688, average loss: 1.7998
[09/26 12:46:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 12:46:44 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 12:46:50 visual_prompt]: Epoch 36 / 100: avg data time: 5.85e-02, avg batch time: 0.5068, average train loss: 1.8329
[09/26 12:46:52 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1685, average loss: 1.8823
[09/26 12:46:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:46:52 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 12:46:59 visual_prompt]: Epoch 37 / 100: avg data time: 5.07e-02, avg batch time: 0.4992, average train loss: 1.8105
[09/26 12:47:00 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1680, average loss: 1.8446
[09/26 12:47:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 12:47:00 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 12:47:07 visual_prompt]: Epoch 38 / 100: avg data time: 5.64e-02, avg batch time: 0.5039, average train loss: 1.7859
[09/26 12:47:09 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1687, average loss: 1.8127
[09/26 12:47:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 12:47:09 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 12:47:15 visual_prompt]: Epoch 39 / 100: avg data time: 5.37e-02, avg batch time: 0.5007, average train loss: 1.7972
[09/26 12:47:17 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1687, average loss: 1.8730
[09/26 12:47:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 12:47:17 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 12:47:24 visual_prompt]: Epoch 40 / 100: avg data time: 4.69e-02, avg batch time: 0.4955, average train loss: 1.8370
[09/26 12:47:25 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1685, average loss: 1.8495
[09/26 12:47:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:47:25 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 12:47:32 visual_prompt]: Epoch 41 / 100: avg data time: 4.43e-02, avg batch time: 0.4921, average train loss: 1.7906
[09/26 12:47:33 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1686, average loss: 1.8170
[09/26 12:47:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:47:33 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 12:47:40 visual_prompt]: Epoch 42 / 100: avg data time: 4.76e-02, avg batch time: 0.4964, average train loss: 1.7836
[09/26 12:47:42 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1687, average loss: 1.9133
[09/26 12:47:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 12:47:42 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 12:47:48 visual_prompt]: Epoch 43 / 100: avg data time: 4.46e-02, avg batch time: 0.4951, average train loss: 1.8205
[09/26 12:47:50 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1689, average loss: 1.8435
[09/26 12:47:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:47:50 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 12:47:57 visual_prompt]: Epoch 44 / 100: avg data time: 5.64e-02, avg batch time: 0.5041, average train loss: 1.7821
[09/26 12:47:58 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1687, average loss: 1.8510
[09/26 12:47:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 12:47:58 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 12:48:05 visual_prompt]: Epoch 45 / 100: avg data time: 5.16e-02, avg batch time: 0.5001, average train loss: 1.7688
[09/26 12:48:06 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1687, average loss: 1.7984
[09/26 12:48:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 83.50	
[09/26 12:48:06 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 12:48:13 visual_prompt]: Epoch 46 / 100: avg data time: 4.44e-02, avg batch time: 0.4922, average train loss: 1.7807
[09/26 12:48:15 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1684, average loss: 1.9118
[09/26 12:48:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:48:15 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 12:48:21 visual_prompt]: Epoch 47 / 100: avg data time: 5.12e-02, avg batch time: 0.5002, average train loss: 1.7930
[09/26 12:48:23 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1686, average loss: 1.8414
[09/26 12:48:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 12:48:23 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 12:48:30 visual_prompt]: Epoch 48 / 100: avg data time: 4.45e-02, avg batch time: 0.4938, average train loss: 1.7713
[09/26 12:48:31 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1687, average loss: 1.8011
[09/26 12:48:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 86.50	
[09/26 12:48:31 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 12:48:38 visual_prompt]: Epoch 49 / 100: avg data time: 5.45e-02, avg batch time: 0.5024, average train loss: 1.7850
[09/26 12:48:40 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1686, average loss: 1.8038
[09/26 12:48:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 84.00	
[09/26 12:48:40 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 12:48:46 visual_prompt]: Epoch 50 / 100: avg data time: 4.67e-02, avg batch time: 0.4961, average train loss: 1.7574
[09/26 12:48:48 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1685, average loss: 1.7958
[09/26 12:48:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 84.00	
[09/26 12:48:48 visual_prompt]: Best epoch 50: best metric: 0.235
[09/26 12:48:48 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 12:48:55 visual_prompt]: Epoch 51 / 100: avg data time: 5.45e-02, avg batch time: 0.5022, average train loss: 1.7766
[09/26 12:48:56 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1689, average loss: 1.8382
[09/26 12:48:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:48:56 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 12:49:03 visual_prompt]: Epoch 52 / 100: avg data time: 4.68e-02, avg batch time: 0.4965, average train loss: 1.7701
[09/26 12:49:04 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1687, average loss: 1.8158
[09/26 12:49:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 84.00	
[09/26 12:49:04 visual_prompt]: Best epoch 52: best metric: 0.260
[09/26 12:49:04 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 12:49:11 visual_prompt]: Epoch 53 / 100: avg data time: 4.58e-02, avg batch time: 0.4981, average train loss: 1.7926
[09/26 12:49:13 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 2.1839
[09/26 12:49:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 84.00	
[09/26 12:49:13 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 12:49:19 visual_prompt]: Epoch 54 / 100: avg data time: 5.13e-02, avg batch time: 0.5000, average train loss: 1.8388
[09/26 12:49:21 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1687, average loss: 1.8340
[09/26 12:49:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:49:21 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 12:49:28 visual_prompt]: Epoch 55 / 100: avg data time: 5.27e-02, avg batch time: 0.5010, average train loss: 1.7803
[09/26 12:49:29 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1690, average loss: 1.8733
[09/26 12:49:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 12:49:29 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 12:49:36 visual_prompt]: Epoch 56 / 100: avg data time: 4.88e-02, avg batch time: 0.4972, average train loss: 1.7950
[09/26 12:49:37 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1686, average loss: 1.8225
[09/26 12:49:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 12.00	top5: 79.50	
[09/26 12:49:37 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 12:49:44 visual_prompt]: Epoch 57 / 100: avg data time: 4.88e-02, avg batch time: 0.4972, average train loss: 1.7781
[09/26 12:49:46 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1690, average loss: 1.8508
[09/26 12:49:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:49:46 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 12:49:52 visual_prompt]: Epoch 58 / 100: avg data time: 4.37e-02, avg batch time: 0.4918, average train loss: 1.7556
[09/26 12:49:54 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 2.1202
[09/26 12:49:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 85.50	
[09/26 12:49:54 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 12:50:01 visual_prompt]: Epoch 59 / 100: avg data time: 5.33e-02, avg batch time: 0.5023, average train loss: 1.8013
[09/26 12:50:02 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1684, average loss: 1.8071
[09/26 12:50:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:50:02 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 12:50:09 visual_prompt]: Epoch 60 / 100: avg data time: 4.42e-02, avg batch time: 0.4932, average train loss: 1.7800
[09/26 12:50:11 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1687, average loss: 1.8749
[09/26 12:50:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:50:11 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 12:50:17 visual_prompt]: Epoch 61 / 100: avg data time: 5.38e-02, avg batch time: 0.5031, average train loss: 1.8160
[09/26 12:50:19 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1685, average loss: 1.8918
[09/26 12:50:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 12:50:19 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 12:50:26 visual_prompt]: Epoch 62 / 100: avg data time: 5.52e-02, avg batch time: 0.5051, average train loss: 1.7879
[09/26 12:50:27 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1684, average loss: 1.8055
[09/26 12:50:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:50:27 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 12:50:34 visual_prompt]: Epoch 63 / 100: avg data time: 4.08e-02, avg batch time: 0.4909, average train loss: 1.8069
[09/26 12:50:35 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1685, average loss: 1.8828
[09/26 12:50:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:50:35 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 12:50:42 visual_prompt]: Epoch 64 / 100: avg data time: 4.34e-02, avg batch time: 0.4913, average train loss: 1.8002
[09/26 12:50:44 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1688, average loss: 1.8120
[09/26 12:50:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 12:50:44 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 12:50:50 visual_prompt]: Epoch 65 / 100: avg data time: 5.43e-02, avg batch time: 0.5020, average train loss: 1.7865
[09/26 12:50:52 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1685, average loss: 1.8485
[09/26 12:50:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 83.50	
[09/26 12:50:52 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 12:50:59 visual_prompt]: Epoch 66 / 100: avg data time: 5.67e-02, avg batch time: 0.5046, average train loss: 1.7970
[09/26 12:51:00 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1687, average loss: 1.7968
[09/26 12:51:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 12:51:00 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 12:51:07 visual_prompt]: Epoch 67 / 100: avg data time: 5.73e-02, avg batch time: 0.5056, average train loss: 1.8037
[09/26 12:51:09 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1686, average loss: 1.7974
[09/26 12:51:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:51:09 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 12:51:16 visual_prompt]: Epoch 68 / 100: avg data time: 5.91e-02, avg batch time: 0.5069, average train loss: 1.7918
[09/26 12:51:17 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1685, average loss: 1.8348
[09/26 12:51:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 12:51:17 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 12:51:24 visual_prompt]: Epoch 69 / 100: avg data time: 4.86e-02, avg batch time: 0.4992, average train loss: 1.7908
[09/26 12:51:26 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1688, average loss: 1.8038
[09/26 12:51:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:51:26 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 12:51:32 visual_prompt]: Epoch 70 / 100: avg data time: 5.91e-02, avg batch time: 0.5088, average train loss: 1.7791
[09/26 12:51:34 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1686, average loss: 1.8415
[09/26 12:51:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:51:34 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 12:51:41 visual_prompt]: Epoch 71 / 100: avg data time: 6.21e-02, avg batch time: 0.5108, average train loss: 1.7823
[09/26 12:51:43 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1681, average loss: 1.8058
[09/26 12:51:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.50	
[09/26 12:51:43 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 12:51:49 visual_prompt]: Epoch 72 / 100: avg data time: 5.38e-02, avg batch time: 0.5031, average train loss: 1.7484
[09/26 12:51:51 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1685, average loss: 1.8211
[09/26 12:51:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 12:51:51 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 12:51:58 visual_prompt]: Epoch 73 / 100: avg data time: 5.68e-02, avg batch time: 0.5041, average train loss: 1.7775
[09/26 12:51:59 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1689, average loss: 1.8332
[09/26 12:51:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 12:51:59 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 12:52:06 visual_prompt]: Epoch 74 / 100: avg data time: 4.29e-02, avg batch time: 0.4923, average train loss: 1.7681
[09/26 12:52:07 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1683, average loss: 1.7920
[09/26 12:52:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 84.00	
[09/26 12:52:07 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 12:52:14 visual_prompt]: Epoch 75 / 100: avg data time: 4.36e-02, avg batch time: 0.4941, average train loss: 1.7486
[09/26 12:52:16 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1685, average loss: 1.7743
[09/26 12:52:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:52:16 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 12:52:22 visual_prompt]: Epoch 76 / 100: avg data time: 5.72e-02, avg batch time: 0.5051, average train loss: 1.7256
[09/26 12:52:24 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1686, average loss: 1.7486
[09/26 12:52:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 88.50	
[09/26 12:52:24 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 12:52:31 visual_prompt]: Epoch 77 / 100: avg data time: 5.13e-02, avg batch time: 0.4998, average train loss: 1.6831
[09/26 12:52:32 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1689, average loss: 1.7308
[09/26 12:52:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 90.00	
[09/26 12:52:32 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 12:52:39 visual_prompt]: Epoch 78 / 100: avg data time: 5.38e-02, avg batch time: 0.5025, average train loss: 1.6675
[09/26 12:52:41 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1691, average loss: 1.7853
[09/26 12:52:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 85.50	
[09/26 12:52:41 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 12:52:48 visual_prompt]: Epoch 79 / 100: avg data time: 5.52e-02, avg batch time: 0.5042, average train loss: 1.7408
[09/26 12:52:49 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1684, average loss: 1.7684
[09/26 12:52:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 86.50	
[09/26 12:52:49 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 12:52:56 visual_prompt]: Epoch 80 / 100: avg data time: 5.96e-02, avg batch time: 0.5097, average train loss: 1.6040
[09/26 12:52:57 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 1.6435
[09/26 12:52:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 92.50	
[09/26 12:52:57 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 12:53:04 visual_prompt]: Epoch 81 / 100: avg data time: 5.91e-02, avg batch time: 0.5075, average train loss: 1.5130
[09/26 12:53:06 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1685, average loss: 1.5698
[09/26 12:53:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 96.00	
[09/26 12:53:06 visual_prompt]: Best epoch 81: best metric: 0.280
[09/26 12:53:06 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 12:53:13 visual_prompt]: Epoch 82 / 100: avg data time: 5.06e-02, avg batch time: 0.4994, average train loss: 1.4429
[09/26 12:53:14 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1688, average loss: 1.4442
[09/26 12:53:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 97.00	
[09/26 12:53:14 visual_prompt]: Best epoch 82: best metric: 0.300
[09/26 12:53:14 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 12:53:21 visual_prompt]: Epoch 83 / 100: avg data time: 6.28e-02, avg batch time: 0.5107, average train loss: 1.3996
[09/26 12:53:23 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1686, average loss: 1.4602
[09/26 12:53:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 95.50	
[09/26 12:53:23 visual_prompt]: Best epoch 83: best metric: 0.350
[09/26 12:53:23 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 12:53:29 visual_prompt]: Epoch 84 / 100: avg data time: 4.34e-02, avg batch time: 0.4956, average train loss: 1.3957
[09/26 12:53:31 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1688, average loss: 1.7556
[09/26 12:53:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 89.50	
[09/26 12:53:31 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 12:53:38 visual_prompt]: Epoch 85 / 100: avg data time: 5.43e-02, avg batch time: 0.5024, average train loss: 1.3937
[09/26 12:53:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1688, average loss: 1.4780
[09/26 12:53:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 97.00	
[09/26 12:53:39 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 12:53:46 visual_prompt]: Epoch 86 / 100: avg data time: 5.30e-02, avg batch time: 0.5017, average train loss: 1.3431
[09/26 12:53:47 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1692, average loss: 1.3967
[09/26 12:53:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.50	
[09/26 12:53:47 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 12:53:54 visual_prompt]: Epoch 87 / 100: avg data time: 6.39e-02, avg batch time: 0.5120, average train loss: 1.3161
[09/26 12:53:56 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1687, average loss: 1.4449
[09/26 12:53:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 95.50	
[09/26 12:53:56 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 12:54:03 visual_prompt]: Epoch 88 / 100: avg data time: 4.21e-02, avg batch time: 0.4927, average train loss: 1.3561
[09/26 12:54:04 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1688, average loss: 1.4172
[09/26 12:54:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 97.00	
[09/26 12:54:04 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 12:54:11 visual_prompt]: Epoch 89 / 100: avg data time: 5.60e-02, avg batch time: 0.5040, average train loss: 1.3165
[09/26 12:54:13 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1686, average loss: 1.5220
[09/26 12:54:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 95.00	
[09/26 12:54:13 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 12:54:20 visual_prompt]: Epoch 90 / 100: avg data time: 6.49e-02, avg batch time: 0.5138, average train loss: 1.2989
[09/26 12:54:21 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.1689, average loss: 1.3821
[09/26 12:54:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 12:54:21 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 12:54:28 visual_prompt]: Epoch 91 / 100: avg data time: 5.89e-02, avg batch time: 0.5072, average train loss: 1.2186
[09/26 12:54:30 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1689, average loss: 1.4166
[09/26 12:54:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 95.50	
[09/26 12:54:30 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 12:54:36 visual_prompt]: Epoch 92 / 100: avg data time: 5.46e-02, avg batch time: 0.5025, average train loss: 1.1997
[09/26 12:54:38 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1691, average loss: 1.3581
[09/26 12:54:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 99.00	
[09/26 12:54:38 visual_prompt]: Best epoch 92: best metric: 0.370
[09/26 12:54:38 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 12:54:45 visual_prompt]: Epoch 93 / 100: avg data time: 5.88e-02, avg batch time: 0.5069, average train loss: 1.1689
[09/26 12:54:46 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1685, average loss: 1.4294
[09/26 12:54:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.00	
[09/26 12:54:46 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 12:54:53 visual_prompt]: Epoch 94 / 100: avg data time: 5.98e-02, avg batch time: 0.5094, average train loss: 1.2059
[09/26 12:54:55 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1689, average loss: 1.4377
[09/26 12:54:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 12:54:55 visual_prompt]: Best epoch 94: best metric: 0.385
[09/26 12:54:55 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 12:55:02 visual_prompt]: Epoch 95 / 100: avg data time: 4.89e-02, avg batch time: 0.4979, average train loss: 1.1532
[09/26 12:55:03 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1690, average loss: 1.3492
[09/26 12:55:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.50	
[09/26 12:55:03 visual_prompt]: Best epoch 95: best metric: 0.390
[09/26 12:55:03 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 12:55:10 visual_prompt]: Epoch 96 / 100: avg data time: 6.43e-02, avg batch time: 0.5135, average train loss: 1.1102
[09/26 12:55:12 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1685, average loss: 1.3647
[09/26 12:55:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 12:55:12 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 12:55:18 visual_prompt]: Epoch 97 / 100: avg data time: 5.58e-02, avg batch time: 0.5038, average train loss: 1.0906
[09/26 12:55:20 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1686, average loss: 1.3748
[09/26 12:55:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.00	
[09/26 12:55:20 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 12:55:27 visual_prompt]: Epoch 98 / 100: avg data time: 4.84e-02, avg batch time: 0.4981, average train loss: 1.0541
[09/26 12:55:28 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1688, average loss: 1.3586
[09/26 12:55:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 12:55:28 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 12:55:35 visual_prompt]: Epoch 99 / 100: avg data time: 5.59e-02, avg batch time: 0.5049, average train loss: 1.0513
[09/26 12:55:37 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1685, average loss: 1.3638
[09/26 12:55:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 12:55:37 visual_prompt]: Best epoch 99: best metric: 0.395
[09/26 12:55:37 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 12:55:43 visual_prompt]: Epoch 100 / 100: avg data time: 5.94e-02, avg batch time: 0.5079, average train loss: 1.0481
[09/26 12:55:45 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1684, average loss: 1.3638
[09/26 12:55:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 12:55:45 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:55:45 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:55:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:55:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:55:45 visual_prompt]: Training with config:
[09/26 12:55:45 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:55:45 visual_prompt]: Loading training data...
[09/26 12:55:45 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 12:55:46 visual_prompt]: Number of images: 800
[09/26 12:55:46 visual_prompt]: Number of classes: 6 / 6
[09/26 12:55:46 visual_prompt]: Loading validation data...
[09/26 12:55:46 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 12:55:47 visual_prompt]: Number of images: 200
[09/26 12:55:47 visual_prompt]: Number of classes: 6 / 6
[09/26 12:55:47 visual_prompt]: Constructing models...
[09/26 12:55:50 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 12:55:50 visual_prompt]: tuned percent:0.540
[09/26 12:55:50 visual_prompt]: Device used for model: 0
[09/26 12:55:50 visual_prompt]: Setting up Evaluator...
[09/26 12:55:50 visual_prompt]: Setting up Trainer...
[09/26 12:55:50 visual_prompt]: 	Setting up the optimizer...
[09/26 12:55:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:55:57 visual_prompt]: Epoch 1 / 100: avg data time: 6.37e-02, avg batch time: 0.5132, average train loss: 2.9797
[09/26 12:55:58 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1688, average loss: 2.9268
[09/26 12:55:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 12:55:58 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 12:55:58 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 12:56:05 visual_prompt]: Epoch 2 / 100: avg data time: 5.64e-02, avg batch time: 0.5040, average train loss: 2.5360
[09/26 12:56:06 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1686, average loss: 1.8798
[09/26 12:56:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 12:56:06 visual_prompt]: Best epoch 2: best metric: 0.205
[09/26 12:56:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 12:56:13 visual_prompt]: Epoch 3 / 100: avg data time: 5.03e-02, avg batch time: 0.4990, average train loss: 1.8253
[09/26 12:56:15 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1690, average loss: 1.8214
[09/26 12:56:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 84.00	
[09/26 12:56:15 visual_prompt]: Best epoch 3: best metric: 0.210
[09/26 12:56:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 12:56:21 visual_prompt]: Epoch 4 / 100: avg data time: 4.39e-02, avg batch time: 0.4945, average train loss: 1.7900
[09/26 12:56:23 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1697, average loss: 1.8351
[09/26 12:56:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/26 12:56:23 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 12:56:30 visual_prompt]: Epoch 5 / 100: avg data time: 4.35e-02, avg batch time: 0.4952, average train loss: 1.7549
[09/26 12:56:31 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 1.7638
[09/26 12:56:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 87.00	
[09/26 12:56:31 visual_prompt]: Best epoch 5: best metric: 0.225
[09/26 12:56:31 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 12:56:38 visual_prompt]: Epoch 6 / 100: avg data time: 4.29e-02, avg batch time: 0.4930, average train loss: 1.6952
[09/26 12:56:39 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1693, average loss: 1.6961
[09/26 12:56:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 90.50	
[09/26 12:56:39 visual_prompt]: Best epoch 6: best metric: 0.250
[09/26 12:56:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 12:56:46 visual_prompt]: Epoch 7 / 100: avg data time: 5.42e-02, avg batch time: 0.5045, average train loss: 1.7653
[09/26 12:56:48 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 1.7695
[09/26 12:56:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 13.00	top5: 86.50	
[09/26 12:56:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 12:56:54 visual_prompt]: Epoch 8 / 100: avg data time: 4.85e-02, avg batch time: 0.4980, average train loss: 1.6678
[09/26 12:56:56 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 1.6823
[09/26 12:56:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 90.50	
[09/26 12:56:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 12:57:03 visual_prompt]: Epoch 9 / 100: avg data time: 5.99e-02, avg batch time: 0.5083, average train loss: 1.6392
[09/26 12:57:04 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1695, average loss: 2.0632
[09/26 12:57:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 92.00	
[09/26 12:57:04 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 12:57:11 visual_prompt]: Epoch 10 / 100: avg data time: 4.25e-02, avg batch time: 0.4942, average train loss: 1.6487
[09/26 12:57:13 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1693, average loss: 1.9383
[09/26 12:57:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 90.50	
[09/26 12:57:13 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 12:57:19 visual_prompt]: Epoch 11 / 100: avg data time: 5.59e-02, avg batch time: 0.5057, average train loss: 1.7245
[09/26 12:57:21 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 1.7108
[09/26 12:57:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 94.00	
[09/26 12:57:21 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 12:57:28 visual_prompt]: Epoch 12 / 100: avg data time: 5.16e-02, avg batch time: 0.5021, average train loss: 1.4661
[09/26 12:57:29 visual_prompt]: Inference (val):avg data time: 4.88e-05, avg batch time: 0.1695, average loss: 2.0734
[09/26 12:57:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 93.50	
[09/26 12:57:29 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 12:57:36 visual_prompt]: Epoch 13 / 100: avg data time: 4.81e-02, avg batch time: 0.4985, average train loss: 1.4701
[09/26 12:57:38 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1699, average loss: 1.4752
[09/26 12:57:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 98.50	
[09/26 12:57:38 visual_prompt]: Best epoch 13: best metric: 0.340
[09/26 12:57:38 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 12:57:44 visual_prompt]: Epoch 14 / 100: avg data time: 4.54e-02, avg batch time: 0.4964, average train loss: 1.3763
[09/26 12:57:46 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1696, average loss: 1.3714
[09/26 12:57:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.50	
[09/26 12:57:46 visual_prompt]: Best epoch 14: best metric: 0.360
[09/26 12:57:46 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 12:57:53 visual_prompt]: Epoch 15 / 100: avg data time: 5.06e-02, avg batch time: 0.5014, average train loss: 1.3418
[09/26 12:57:54 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1692, average loss: 1.5005
[09/26 12:57:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 98.50	
[09/26 12:57:54 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 12:58:01 visual_prompt]: Epoch 16 / 100: avg data time: 6.00e-02, avg batch time: 0.5108, average train loss: 1.2655
[09/26 12:58:03 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1695, average loss: 1.4450
[09/26 12:58:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 12:58:03 visual_prompt]: Best epoch 16: best metric: 0.400
[09/26 12:58:03 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 12:58:10 visual_prompt]: Epoch 17 / 100: avg data time: 5.60e-02, avg batch time: 0.5061, average train loss: 1.3557
[09/26 12:58:11 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 1.4497
[09/26 12:58:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 97.00	
[09/26 12:58:11 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 12:58:18 visual_prompt]: Epoch 18 / 100: avg data time: 4.73e-02, avg batch time: 0.4993, average train loss: 1.2793
[09/26 12:58:19 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1698, average loss: 1.4693
[09/26 12:58:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 12:58:19 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 12:58:26 visual_prompt]: Epoch 19 / 100: avg data time: 5.24e-02, avg batch time: 0.5027, average train loss: 1.2581
[09/26 12:58:28 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1696, average loss: 1.6369
[09/26 12:58:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 96.50	
[09/26 12:58:28 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 12:58:34 visual_prompt]: Epoch 20 / 100: avg data time: 5.22e-02, avg batch time: 0.5013, average train loss: 1.3437
[09/26 12:58:36 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1697, average loss: 1.6712
[09/26 12:58:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 96.50	
[09/26 12:58:36 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 12:58:43 visual_prompt]: Epoch 21 / 100: avg data time: 4.37e-02, avg batch time: 0.4959, average train loss: 1.2641
[09/26 12:58:44 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1695, average loss: 1.7433
[09/26 12:58:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.00	
[09/26 12:58:44 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 12:58:51 visual_prompt]: Epoch 22 / 100: avg data time: 4.62e-02, avg batch time: 0.4964, average train loss: 1.2309
[09/26 12:58:52 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 1.4593
[09/26 12:58:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.50	
[09/26 12:58:52 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 12:58:59 visual_prompt]: Epoch 23 / 100: avg data time: 4.84e-02, avg batch time: 0.4985, average train loss: 1.2600
[09/26 12:59:01 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1697, average loss: 1.3963
[09/26 12:59:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 12:59:01 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 12:59:07 visual_prompt]: Epoch 24 / 100: avg data time: 4.49e-02, avg batch time: 0.4944, average train loss: 1.1895
[09/26 12:59:09 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1698, average loss: 1.4527
[09/26 12:59:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 12:59:09 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 12:59:16 visual_prompt]: Epoch 25 / 100: avg data time: 5.56e-02, avg batch time: 0.5062, average train loss: 1.1727
[09/26 12:59:17 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1692, average loss: 1.7422
[09/26 12:59:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.00	
[09/26 12:59:17 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 12:59:24 visual_prompt]: Epoch 26 / 100: avg data time: 5.54e-02, avg batch time: 0.5053, average train loss: 1.2259
[09/26 12:59:26 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1694, average loss: 1.5569
[09/26 12:59:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 12:59:26 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 12:59:33 visual_prompt]: Epoch 27 / 100: avg data time: 5.10e-02, avg batch time: 0.5029, average train loss: 1.1840
[09/26 12:59:34 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1697, average loss: 1.5554
[09/26 12:59:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 12:59:34 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 12:59:41 visual_prompt]: Epoch 28 / 100: avg data time: 5.72e-02, avg batch time: 0.5074, average train loss: 1.1608
[09/26 12:59:43 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 1.4392
[09/26 12:59:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 12:59:43 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 12:59:50 visual_prompt]: Epoch 29 / 100: avg data time: 4.36e-02, avg batch time: 0.4966, average train loss: 1.1317
[09/26 12:59:51 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1693, average loss: 1.4684
[09/26 12:59:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.00	
[09/26 12:59:51 visual_prompt]: Best epoch 29: best metric: 0.415
[09/26 12:59:51 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 12:59:58 visual_prompt]: Epoch 30 / 100: avg data time: 5.50e-02, avg batch time: 0.5040, average train loss: 1.0498
[09/26 12:59:59 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 1.4107
[09/26 12:59:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.00	
[09/26 12:59:59 visual_prompt]: Best epoch 30: best metric: 0.420
[09/26 12:59:59 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 13:00:06 visual_prompt]: Epoch 31 / 100: avg data time: 4.50e-02, avg batch time: 0.4954, average train loss: 1.0202
[09/26 13:00:08 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1697, average loss: 1.5580
[09/26 13:00:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 99.00	
[09/26 13:00:08 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 13:00:15 visual_prompt]: Epoch 32 / 100: avg data time: 5.99e-02, avg batch time: 0.5113, average train loss: 1.0850
[09/26 13:00:16 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1698, average loss: 1.3696
[09/26 13:00:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.50	
[09/26 13:00:16 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 13:00:23 visual_prompt]: Epoch 33 / 100: avg data time: 6.07e-02, avg batch time: 0.5097, average train loss: 0.9896
[09/26 13:00:25 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1697, average loss: 1.6677
[09/26 13:00:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 95.50	
[09/26 13:00:25 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 13:00:32 visual_prompt]: Epoch 34 / 100: avg data time: 6.21e-02, avg batch time: 0.5122, average train loss: 0.9807
[09/26 13:00:33 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.1698, average loss: 1.4112
[09/26 13:00:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.00	
[09/26 13:00:33 visual_prompt]: Best epoch 34: best metric: 0.435
[09/26 13:00:33 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 13:00:40 visual_prompt]: Epoch 35 / 100: avg data time: 6.28e-02, avg batch time: 0.5121, average train loss: 0.8538
[09/26 13:00:42 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 1.8454
[09/26 13:00:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 13:00:42 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 13:00:48 visual_prompt]: Epoch 36 / 100: avg data time: 5.83e-02, avg batch time: 0.5079, average train loss: 0.8621
[09/26 13:00:50 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1696, average loss: 1.6695
[09/26 13:00:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 96.00	
[09/26 13:00:50 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 13:00:57 visual_prompt]: Epoch 37 / 100: avg data time: 4.38e-02, avg batch time: 0.4946, average train loss: 0.9388
[09/26 13:00:58 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 1.6440
[09/26 13:00:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.50	
[09/26 13:00:58 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 13:01:05 visual_prompt]: Epoch 38 / 100: avg data time: 6.00e-02, avg batch time: 0.5095, average train loss: 0.9104
[09/26 13:01:07 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 1.5892
[09/26 13:01:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 13:01:07 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 13:01:14 visual_prompt]: Epoch 39 / 100: avg data time: 5.95e-02, avg batch time: 0.5089, average train loss: 0.8603
[09/26 13:01:15 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1696, average loss: 1.6284
[09/26 13:01:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 98.00	
[09/26 13:01:15 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 13:01:22 visual_prompt]: Epoch 40 / 100: avg data time: 5.97e-02, avg batch time: 0.5113, average train loss: 0.8275
[09/26 13:01:23 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1696, average loss: 1.5888
[09/26 13:01:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.50	
[09/26 13:01:23 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 13:01:30 visual_prompt]: Epoch 41 / 100: avg data time: 4.30e-02, avg batch time: 0.4938, average train loss: 0.7704
[09/26 13:01:32 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1695, average loss: 1.4385
[09/26 13:01:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 97.50	
[09/26 13:01:32 visual_prompt]: Best epoch 41: best metric: 0.440
[09/26 13:01:32 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 13:01:38 visual_prompt]: Epoch 42 / 100: avg data time: 4.67e-02, avg batch time: 0.4960, average train loss: 0.7545
[09/26 13:01:40 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1703, average loss: 1.4642
[09/26 13:01:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 13:01:40 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 13:01:47 visual_prompt]: Epoch 43 / 100: avg data time: 5.64e-02, avg batch time: 0.5071, average train loss: 0.7587
[09/26 13:01:48 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1699, average loss: 1.8287
[09/26 13:01:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.50	
[09/26 13:01:48 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 13:01:55 visual_prompt]: Epoch 44 / 100: avg data time: 4.75e-02, avg batch time: 0.4976, average train loss: 0.7438
[09/26 13:01:57 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1702, average loss: 2.0746
[09/26 13:01:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 13:01:57 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 13:02:04 visual_prompt]: Epoch 45 / 100: avg data time: 4.83e-02, avg batch time: 0.5012, average train loss: 0.8190
[09/26 13:02:05 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 1.6211
[09/26 13:02:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 13:02:05 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 13:02:12 visual_prompt]: Epoch 46 / 100: avg data time: 5.31e-02, avg batch time: 0.5050, average train loss: 0.7495
[09/26 13:02:13 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 1.6195
[09/26 13:02:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.00	
[09/26 13:02:13 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 13:02:20 visual_prompt]: Epoch 47 / 100: avg data time: 4.79e-02, avg batch time: 0.4996, average train loss: 0.6182
[09/26 13:02:22 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1695, average loss: 1.8558
[09/26 13:02:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.00	
[09/26 13:02:22 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 13:02:29 visual_prompt]: Epoch 48 / 100: avg data time: 5.64e-02, avg batch time: 0.5082, average train loss: 0.5822
[09/26 13:02:30 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1697, average loss: 1.8256
[09/26 13:02:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 95.50	
[09/26 13:02:30 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 13:02:37 visual_prompt]: Epoch 49 / 100: avg data time: 4.28e-02, avg batch time: 0.4962, average train loss: 0.5626
[09/26 13:02:38 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1699, average loss: 2.4737
[09/26 13:02:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 13:02:38 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 13:02:45 visual_prompt]: Epoch 50 / 100: avg data time: 6.18e-02, avg batch time: 0.5113, average train loss: 0.6704
[09/26 13:02:47 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1691, average loss: 1.7768
[09/26 13:02:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 98.50	
[09/26 13:02:47 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 13:02:54 visual_prompt]: Epoch 51 / 100: avg data time: 5.77e-02, avg batch time: 0.5085, average train loss: 0.6287
[09/26 13:02:55 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 1.9404
[09/26 13:02:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 98.50	
[09/26 13:02:55 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 13:03:02 visual_prompt]: Epoch 52 / 100: avg data time: 5.20e-02, avg batch time: 0.5013, average train loss: 0.5560
[09/26 13:03:04 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 1.7416
[09/26 13:03:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 97.50	
[09/26 13:03:04 visual_prompt]: Best epoch 52: best metric: 0.450
[09/26 13:03:04 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 13:03:11 visual_prompt]: Epoch 53 / 100: avg data time: 6.23e-02, avg batch time: 0.5113, average train loss: 0.4579
[09/26 13:03:12 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1693, average loss: 2.1643
[09/26 13:03:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 99.00	
[09/26 13:03:12 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 13:03:19 visual_prompt]: Epoch 54 / 100: avg data time: 5.34e-02, avg batch time: 0.5025, average train loss: 0.4968
[09/26 13:03:20 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1695, average loss: 1.8352
[09/26 13:03:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.50	
[09/26 13:03:20 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 13:03:27 visual_prompt]: Epoch 55 / 100: avg data time: 5.48e-02, avg batch time: 0.5056, average train loss: 0.5330
[09/26 13:03:29 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1696, average loss: 2.2200
[09/26 13:03:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.00	
[09/26 13:03:29 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 13:03:36 visual_prompt]: Epoch 56 / 100: avg data time: 4.60e-02, avg batch time: 0.4967, average train loss: 0.3937
[09/26 13:03:37 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 2.5515
[09/26 13:03:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 98.00	
[09/26 13:03:37 visual_prompt]: Best epoch 56: best metric: 0.460
[09/26 13:03:37 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 13:03:44 visual_prompt]: Epoch 57 / 100: avg data time: 5.72e-02, avg batch time: 0.5066, average train loss: 0.3659
[09/26 13:03:45 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1695, average loss: 2.2449
[09/26 13:03:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.50	
[09/26 13:03:45 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 13:03:52 visual_prompt]: Epoch 58 / 100: avg data time: 6.06e-02, avg batch time: 0.5102, average train loss: 0.3169
[09/26 13:03:54 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1694, average loss: 2.5111
[09/26 13:03:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.50	
[09/26 13:03:54 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 13:04:01 visual_prompt]: Epoch 59 / 100: avg data time: 5.81e-02, avg batch time: 0.5087, average train loss: 0.3853
[09/26 13:04:02 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1697, average loss: 2.0698
[09/26 13:04:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 97.00	
[09/26 13:04:02 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 13:04:09 visual_prompt]: Epoch 60 / 100: avg data time: 5.89e-02, avg batch time: 0.5087, average train loss: 0.4341
[09/26 13:04:11 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 2.6671
[09/26 13:04:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.50	
[09/26 13:04:11 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 13:04:18 visual_prompt]: Epoch 61 / 100: avg data time: 5.08e-02, avg batch time: 0.5027, average train loss: 0.3795
[09/26 13:04:19 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1694, average loss: 2.1038
[09/26 13:04:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 96.50	
[09/26 13:04:19 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 13:04:26 visual_prompt]: Epoch 62 / 100: avg data time: 6.79e-02, avg batch time: 0.5174, average train loss: 0.3220
[09/26 13:04:28 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 2.6052
[09/26 13:04:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/26 13:04:28 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 13:04:34 visual_prompt]: Epoch 63 / 100: avg data time: 4.60e-02, avg batch time: 0.4957, average train loss: 0.3040
[09/26 13:04:36 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 2.8718
[09/26 13:04:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 99.00	
[09/26 13:04:36 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 13:04:43 visual_prompt]: Epoch 64 / 100: avg data time: 5.84e-02, avg batch time: 0.5077, average train loss: 0.2417
[09/26 13:04:44 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1696, average loss: 2.8733
[09/26 13:04:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 13:04:44 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 13:04:51 visual_prompt]: Epoch 65 / 100: avg data time: 5.89e-02, avg batch time: 0.5081, average train loss: 0.3354
[09/26 13:04:53 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1697, average loss: 2.4824
[09/26 13:04:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.50	
[09/26 13:04:53 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 13:05:00 visual_prompt]: Epoch 66 / 100: avg data time: 6.21e-02, avg batch time: 0.5126, average train loss: 0.3079
[09/26 13:05:01 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1706, average loss: 2.7358
[09/26 13:05:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.50	
[09/26 13:05:01 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 13:05:08 visual_prompt]: Epoch 67 / 100: avg data time: 5.91e-02, avg batch time: 0.5095, average train loss: 0.2510
[09/26 13:05:10 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1696, average loss: 2.3365
[09/26 13:05:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.00	top5: 99.50	
[09/26 13:05:10 visual_prompt]: Best epoch 67: best metric: 0.470
[09/26 13:05:10 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 13:05:17 visual_prompt]: Epoch 68 / 100: avg data time: 5.89e-02, avg batch time: 0.5079, average train loss: 0.1579
[09/26 13:05:18 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1698, average loss: 2.5401
[09/26 13:05:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 99.50	
[09/26 13:05:18 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 13:05:25 visual_prompt]: Epoch 69 / 100: avg data time: 5.84e-02, avg batch time: 0.5080, average train loss: 0.0771
[09/26 13:05:27 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1697, average loss: 2.7310
[09/26 13:05:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 100.00	
[09/26 13:05:27 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 13:05:34 visual_prompt]: Epoch 70 / 100: avg data time: 5.52e-02, avg batch time: 0.5069, average train loss: 0.0927
[09/26 13:05:35 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1699, average loss: 2.9530
[09/26 13:05:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.00	
[09/26 13:05:35 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 13:05:42 visual_prompt]: Epoch 71 / 100: avg data time: 6.24e-02, avg batch time: 0.5119, average train loss: 0.0922
[09/26 13:05:44 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1692, average loss: 2.9164
[09/26 13:05:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 99.00	
[09/26 13:05:44 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 13:05:51 visual_prompt]: Epoch 72 / 100: avg data time: 5.76e-02, avg batch time: 0.5079, average train loss: 0.1187
[09/26 13:05:52 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1696, average loss: 3.1736
[09/26 13:05:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 98.00	
[09/26 13:05:52 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 13:05:59 visual_prompt]: Epoch 73 / 100: avg data time: 4.52e-02, avg batch time: 0.4973, average train loss: 0.1780
[09/26 13:06:00 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1699, average loss: 2.9636
[09/26 13:06:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 97.00	
[09/26 13:06:00 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 13:06:07 visual_prompt]: Epoch 74 / 100: avg data time: 4.62e-02, avg batch time: 0.4978, average train loss: 0.1868
[09/26 13:06:09 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1696, average loss: 3.0978
[09/26 13:06:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 13:06:09 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 13:06:16 visual_prompt]: Epoch 75 / 100: avg data time: 5.69e-02, avg batch time: 0.5063, average train loss: 0.0901
[09/26 13:06:17 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 3.0959
[09/26 13:06:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.50	
[09/26 13:06:17 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 13:06:24 visual_prompt]: Epoch 76 / 100: avg data time: 4.63e-02, avg batch time: 0.4980, average train loss: 0.0520
[09/26 13:06:25 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1700, average loss: 2.9510
[09/26 13:06:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 13:06:25 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 13:06:32 visual_prompt]: Epoch 77 / 100: avg data time: 5.36e-02, avg batch time: 0.5034, average train loss: 0.0472
[09/26 13:06:34 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1693, average loss: 3.2942
[09/26 13:06:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 13:06:34 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 13:06:41 visual_prompt]: Epoch 78 / 100: avg data time: 4.57e-02, avg batch time: 0.4966, average train loss: 0.0141
[09/26 13:06:42 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1698, average loss: 3.1859
[09/26 13:06:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.50	
[09/26 13:06:42 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 13:06:49 visual_prompt]: Epoch 79 / 100: avg data time: 5.10e-02, avg batch time: 0.5019, average train loss: 0.0202
[09/26 13:06:50 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.1695, average loss: 3.4370
[09/26 13:06:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.00	
[09/26 13:06:50 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 13:06:57 visual_prompt]: Epoch 80 / 100: avg data time: 6.06e-02, avg batch time: 0.5120, average train loss: 0.0165
[09/26 13:06:59 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1692, average loss: 3.3567
[09/26 13:06:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.50	
[09/26 13:06:59 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 13:07:06 visual_prompt]: Epoch 81 / 100: avg data time: 5.88e-02, avg batch time: 0.5102, average train loss: 0.0145
[09/26 13:07:07 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1699, average loss: 3.2996
[09/26 13:07:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.50	
[09/26 13:07:07 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 13:07:14 visual_prompt]: Epoch 82 / 100: avg data time: 5.92e-02, avg batch time: 0.5084, average train loss: 0.0092
[09/26 13:07:16 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1697, average loss: 3.3486
[09/26 13:07:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.50	
[09/26 13:07:16 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 13:07:23 visual_prompt]: Epoch 83 / 100: avg data time: 5.53e-02, avg batch time: 0.5051, average train loss: 0.0054
[09/26 13:07:24 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1697, average loss: 3.4160
[09/26 13:07:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.50	
[09/26 13:07:24 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 13:07:31 visual_prompt]: Epoch 84 / 100: avg data time: 5.83e-02, avg batch time: 0.5089, average train loss: 0.0051
[09/26 13:07:33 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 3.4289
[09/26 13:07:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 99.50	
[09/26 13:07:33 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 13:07:39 visual_prompt]: Epoch 85 / 100: avg data time: 4.57e-02, avg batch time: 0.4982, average train loss: 0.0049
[09/26 13:07:41 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 3.4763
[09/26 13:07:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/26 13:07:41 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 13:07:48 visual_prompt]: Epoch 86 / 100: avg data time: 4.78e-02, avg batch time: 0.4984, average train loss: 0.0037
[09/26 13:07:49 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1695, average loss: 3.5256
[09/26 13:07:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/26 13:07:49 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 13:07:56 visual_prompt]: Epoch 87 / 100: avg data time: 4.30e-02, avg batch time: 0.4952, average train loss: 0.0034
[09/26 13:07:57 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1697, average loss: 3.4989
[09/26 13:07:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/26 13:07:57 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 13:08:04 visual_prompt]: Epoch 88 / 100: avg data time: 5.34e-02, avg batch time: 0.5031, average train loss: 0.0028
[09/26 13:08:06 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1693, average loss: 3.5183
[09/26 13:08:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/26 13:08:06 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 13:08:13 visual_prompt]: Epoch 89 / 100: avg data time: 4.97e-02, avg batch time: 0.4993, average train loss: 0.0028
[09/26 13:08:14 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1696, average loss: 3.5148
[09/26 13:08:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/26 13:08:14 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 13:08:21 visual_prompt]: Epoch 90 / 100: avg data time: 4.90e-02, avg batch time: 0.5005, average train loss: 0.0025
[09/26 13:08:22 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1697, average loss: 3.5195
[09/26 13:08:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/26 13:08:22 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 13:08:29 visual_prompt]: Epoch 91 / 100: avg data time: 4.81e-02, avg batch time: 0.4983, average train loss: 0.0048
[09/26 13:08:31 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1698, average loss: 3.5274
[09/26 13:08:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.00	
[09/26 13:08:31 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 13:08:38 visual_prompt]: Epoch 92 / 100: avg data time: 5.05e-02, avg batch time: 0.5019, average train loss: 0.0027
[09/26 13:08:39 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1696, average loss: 3.5318
[09/26 13:08:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.00	
[09/26 13:08:39 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 13:08:46 visual_prompt]: Epoch 93 / 100: avg data time: 6.09e-02, avg batch time: 0.5112, average train loss: 0.0027
[09/26 13:08:48 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1697, average loss: 3.5336
[09/26 13:08:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.00	
[09/26 13:08:48 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 13:08:55 visual_prompt]: Epoch 94 / 100: avg data time: 5.86e-02, avg batch time: 0.5081, average train loss: 0.0026
[09/26 13:08:56 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1697, average loss: 3.5360
[09/26 13:08:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.00	
[09/26 13:08:56 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 13:09:03 visual_prompt]: Epoch 95 / 100: avg data time: 5.43e-02, avg batch time: 0.5044, average train loss: 0.0029
[09/26 13:09:04 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1700, average loss: 3.5371
[09/26 13:09:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.00	
[09/26 13:09:04 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 13:09:11 visual_prompt]: Epoch 96 / 100: avg data time: 5.93e-02, avg batch time: 0.5086, average train loss: 0.0023
[09/26 13:09:13 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 3.5384
[09/26 13:09:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.00	
[09/26 13:09:13 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 13:09:20 visual_prompt]: Epoch 97 / 100: avg data time: 5.79e-02, avg batch time: 0.5083, average train loss: 0.0043
[09/26 13:09:21 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1697, average loss: 3.5336
[09/26 13:09:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/26 13:09:21 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 13:09:28 visual_prompt]: Epoch 98 / 100: avg data time: 4.40e-02, avg batch time: 0.4953, average train loss: 0.0023
[09/26 13:09:30 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1700, average loss: 3.5333
[09/26 13:09:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/26 13:09:30 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 13:09:36 visual_prompt]: Epoch 99 / 100: avg data time: 4.26e-02, avg batch time: 0.4938, average train loss: 0.0021
[09/26 13:09:38 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1698, average loss: 3.5333
[09/26 13:09:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/26 13:09:38 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 13:09:44 visual_prompt]: Epoch 100 / 100: avg data time: 4.50e-02, avg batch time: 0.4956, average train loss: 0.0022
[09/26 13:09:46 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1697, average loss: 3.5334
[09/26 13:09:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/26 13:09:46 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:09:46 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:09:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:09:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:09:46 visual_prompt]: Training with config:
[09/26 13:09:46 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:09:46 visual_prompt]: Loading training data...
[09/26 13:09:46 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 13:09:47 visual_prompt]: Number of images: 800
[09/26 13:09:47 visual_prompt]: Number of classes: 6 / 6
[09/26 13:09:47 visual_prompt]: Loading validation data...
[09/26 13:09:47 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 13:09:48 visual_prompt]: Number of images: 200
[09/26 13:09:48 visual_prompt]: Number of classes: 6 / 6
[09/26 13:09:48 visual_prompt]: Constructing models...
[09/26 13:09:50 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 13:09:50 visual_prompt]: tuned percent:0.540
[09/26 13:09:50 visual_prompt]: Device used for model: 0
[09/26 13:09:50 visual_prompt]: Setting up Evaluator...
[09/26 13:09:50 visual_prompt]: Setting up Trainer...
[09/26 13:09:50 visual_prompt]: 	Setting up the optimizer...
[09/26 13:09:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:09:57 visual_prompt]: Epoch 1 / 100: avg data time: 5.74e-02, avg batch time: 0.5064, average train loss: 2.9790
[09/26 13:09:59 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1693, average loss: 2.9268
[09/26 13:09:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 13:09:59 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 13:09:59 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 13:10:05 visual_prompt]: Epoch 2 / 100: avg data time: 5.68e-02, avg batch time: 0.5054, average train loss: 2.6156
[09/26 13:10:07 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1686, average loss: 1.8193
[09/26 13:10:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 85.00	
[09/26 13:10:07 visual_prompt]: Best epoch 2: best metric: 0.185
[09/26 13:10:07 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 13:10:14 visual_prompt]: Epoch 3 / 100: avg data time: 4.97e-02, avg batch time: 0.4989, average train loss: 1.8632
[09/26 13:10:15 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1691, average loss: 1.7918
[09/26 13:10:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 86.50	
[09/26 13:10:15 visual_prompt]: Best epoch 3: best metric: 0.230
[09/26 13:10:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 13:10:22 visual_prompt]: Epoch 4 / 100: avg data time: 4.24e-02, avg batch time: 0.4938, average train loss: 1.8456
[09/26 13:10:24 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 1.8905
[09/26 13:10:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 13:10:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 13:10:30 visual_prompt]: Epoch 5 / 100: avg data time: 5.35e-02, avg batch time: 0.5020, average train loss: 1.8248
[09/26 13:10:32 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1690, average loss: 1.9215
[09/26 13:10:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:10:32 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 13:10:39 visual_prompt]: Epoch 6 / 100: avg data time: 5.30e-02, avg batch time: 0.5021, average train loss: 1.7699
[09/26 13:10:40 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1696, average loss: 1.8025
[09/26 13:10:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 87.50	
[09/26 13:10:40 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 13:10:47 visual_prompt]: Epoch 7 / 100: avg data time: 5.41e-02, avg batch time: 0.5045, average train loss: 1.8209
[09/26 13:10:49 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1692, average loss: 1.8891
[09/26 13:10:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 85.00	
[09/26 13:10:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 13:10:56 visual_prompt]: Epoch 8 / 100: avg data time: 4.51e-02, avg batch time: 0.4947, average train loss: 1.7229
[09/26 13:10:57 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1697, average loss: 1.8288
[09/26 13:10:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 90.00	
[09/26 13:10:57 visual_prompt]: Best epoch 8: best metric: 0.245
[09/26 13:10:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 13:11:04 visual_prompt]: Epoch 9 / 100: avg data time: 4.65e-02, avg batch time: 0.4979, average train loss: 1.7006
[09/26 13:11:05 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 1.8469
[09/26 13:11:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 91.00	
[09/26 13:11:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 13:11:12 visual_prompt]: Epoch 10 / 100: avg data time: 5.31e-02, avg batch time: 0.5027, average train loss: 1.6156
[09/26 13:11:14 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1696, average loss: 1.6472
[09/26 13:11:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 94.00	
[09/26 13:11:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 13:11:21 visual_prompt]: Epoch 11 / 100: avg data time: 5.61e-02, avg batch time: 0.5051, average train loss: 1.5362
[09/26 13:11:22 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 1.9186
[09/26 13:11:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 90.50	
[09/26 13:11:22 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 13:11:29 visual_prompt]: Epoch 12 / 100: avg data time: 5.82e-02, avg batch time: 0.5074, average train loss: 1.4818
[09/26 13:11:31 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1695, average loss: 1.5749
[09/26 13:11:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 96.00	
[09/26 13:11:31 visual_prompt]: Best epoch 12: best metric: 0.275
[09/26 13:11:31 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 13:11:37 visual_prompt]: Epoch 13 / 100: avg data time: 4.35e-02, avg batch time: 0.4965, average train loss: 1.3652
[09/26 13:11:39 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1693, average loss: 1.9172
[09/26 13:11:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 97.00	
[09/26 13:11:39 visual_prompt]: Best epoch 13: best metric: 0.305
[09/26 13:11:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 13:11:46 visual_prompt]: Epoch 14 / 100: avg data time: 5.26e-02, avg batch time: 0.5022, average train loss: 1.4871
[09/26 13:11:47 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1697, average loss: 1.5380
[09/26 13:11:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 97.50	
[09/26 13:11:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 13:11:54 visual_prompt]: Epoch 15 / 100: avg data time: 5.43e-02, avg batch time: 0.5039, average train loss: 1.3469
[09/26 13:11:56 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 1.4966
[09/26 13:11:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 95.00	
[09/26 13:11:56 visual_prompt]: Best epoch 15: best metric: 0.350
[09/26 13:11:56 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 13:12:02 visual_prompt]: Epoch 16 / 100: avg data time: 4.43e-02, avg batch time: 0.4943, average train loss: 1.2744
[09/26 13:12:04 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1699, average loss: 1.6922
[09/26 13:12:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 96.50	
[09/26 13:12:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 13:12:11 visual_prompt]: Epoch 17 / 100: avg data time: 4.92e-02, avg batch time: 0.5003, average train loss: 1.3177
[09/26 13:12:12 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1696, average loss: 1.4927
[09/26 13:12:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 98.00	
[09/26 13:12:12 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 13:12:19 visual_prompt]: Epoch 18 / 100: avg data time: 5.43e-02, avg batch time: 0.5036, average train loss: 1.2184
[09/26 13:12:20 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1697, average loss: 1.5338
[09/26 13:12:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 13:12:20 visual_prompt]: Best epoch 18: best metric: 0.405
[09/26 13:12:20 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 13:12:27 visual_prompt]: Epoch 19 / 100: avg data time: 4.49e-02, avg batch time: 0.4999, average train loss: 1.2186
[09/26 13:12:29 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 1.6968
[09/26 13:12:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 96.50	
[09/26 13:12:29 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 13:12:36 visual_prompt]: Epoch 20 / 100: avg data time: 5.38e-02, avg batch time: 0.5031, average train loss: 1.1779
[09/26 13:12:37 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1694, average loss: 1.5856
[09/26 13:12:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.00	
[09/26 13:12:37 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 13:12:44 visual_prompt]: Epoch 21 / 100: avg data time: 4.88e-02, avg batch time: 0.4987, average train loss: 1.0677
[09/26 13:12:46 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.1700, average loss: 1.4956
[09/26 13:12:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 13:12:46 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 13:12:52 visual_prompt]: Epoch 22 / 100: avg data time: 6.06e-02, avg batch time: 0.5095, average train loss: 1.0056
[09/26 13:12:54 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1697, average loss: 1.6704
[09/26 13:12:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 98.00	
[09/26 13:12:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 13:13:01 visual_prompt]: Epoch 23 / 100: avg data time: 5.12e-02, avg batch time: 0.5014, average train loss: 1.0414
[09/26 13:13:02 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1698, average loss: 1.6314
[09/26 13:13:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 13:13:02 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 13:13:09 visual_prompt]: Epoch 24 / 100: avg data time: 5.83e-02, avg batch time: 0.5088, average train loss: 1.0200
[09/26 13:13:11 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1695, average loss: 1.6410
[09/26 13:13:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 95.00	
[09/26 13:13:11 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 13:13:18 visual_prompt]: Epoch 25 / 100: avg data time: 5.50e-02, avg batch time: 0.5039, average train loss: 0.9228
[09/26 13:13:19 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 1.6238
[09/26 13:13:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 13:13:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 13:13:26 visual_prompt]: Epoch 26 / 100: avg data time: 4.14e-02, avg batch time: 0.4912, average train loss: 0.9120
[09/26 13:13:27 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1693, average loss: 1.6272
[09/26 13:13:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.00	
[09/26 13:13:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 13:13:34 visual_prompt]: Epoch 27 / 100: avg data time: 4.45e-02, avg batch time: 0.4980, average train loss: 0.9187
[09/26 13:13:35 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 1.6824
[09/26 13:13:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.00	
[09/26 13:13:35 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 13:13:42 visual_prompt]: Epoch 28 / 100: avg data time: 4.28e-02, avg batch time: 0.4958, average train loss: 1.0004
[09/26 13:13:44 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1701, average loss: 1.7219
[09/26 13:13:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 13:13:44 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 13:13:50 visual_prompt]: Epoch 29 / 100: avg data time: 4.36e-02, avg batch time: 0.4941, average train loss: 0.8940
[09/26 13:13:52 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1701, average loss: 1.6388
[09/26 13:13:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 95.50	
[09/26 13:13:52 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 13:13:59 visual_prompt]: Epoch 30 / 100: avg data time: 4.92e-02, avg batch time: 0.5002, average train loss: 0.8904
[09/26 13:14:00 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 1.8105
[09/26 13:14:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 13:14:00 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 13:14:07 visual_prompt]: Epoch 31 / 100: avg data time: 4.46e-02, avg batch time: 0.4950, average train loss: 0.8188
[09/26 13:14:09 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1700, average loss: 1.7554
[09/26 13:14:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 95.00	
[09/26 13:14:09 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 13:14:16 visual_prompt]: Epoch 32 / 100: avg data time: 6.32e-02, avg batch time: 0.5135, average train loss: 0.7358
[09/26 13:14:17 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1695, average loss: 1.8073
[09/26 13:14:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 13:14:17 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 13:14:24 visual_prompt]: Epoch 33 / 100: avg data time: 4.50e-02, avg batch time: 0.4965, average train loss: 0.7075
[09/26 13:14:25 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1697, average loss: 1.7050
[09/26 13:14:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 13:14:25 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 13:14:32 visual_prompt]: Epoch 34 / 100: avg data time: 4.43e-02, avg batch time: 0.4941, average train loss: 0.6693
[09/26 13:14:34 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1701, average loss: 1.8453
[09/26 13:14:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 13:14:34 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 13:14:40 visual_prompt]: Epoch 35 / 100: avg data time: 5.00e-02, avg batch time: 0.5021, average train loss: 0.5623
[09/26 13:14:42 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1694, average loss: 2.0656
[09/26 13:14:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 13:14:42 visual_prompt]: Best epoch 35: best metric: 0.415
[09/26 13:14:42 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 13:14:49 visual_prompt]: Epoch 36 / 100: avg data time: 4.54e-02, avg batch time: 0.4958, average train loss: 0.6173
[09/26 13:14:50 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 1.8256
[09/26 13:14:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 99.00	
[09/26 13:14:50 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 13:14:57 visual_prompt]: Epoch 37 / 100: avg data time: 4.83e-02, avg batch time: 0.4995, average train loss: 0.5411
[09/26 13:14:58 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1696, average loss: 1.8714
[09/26 13:14:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/26 13:14:58 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 13:15:05 visual_prompt]: Epoch 38 / 100: avg data time: 5.46e-02, avg batch time: 0.5069, average train loss: 0.4264
[09/26 13:15:07 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1698, average loss: 2.3544
[09/26 13:15:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 13:15:07 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 13:15:14 visual_prompt]: Epoch 39 / 100: avg data time: 5.94e-02, avg batch time: 0.5098, average train loss: 0.4726
[09/26 13:15:15 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1698, average loss: 2.3394
[09/26 13:15:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 13:15:15 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 13:15:22 visual_prompt]: Epoch 40 / 100: avg data time: 5.50e-02, avg batch time: 0.5046, average train loss: 0.5314
[09/26 13:15:24 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1697, average loss: 2.5493
[09/26 13:15:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 13:15:24 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 13:15:30 visual_prompt]: Epoch 41 / 100: avg data time: 4.45e-02, avg batch time: 0.4959, average train loss: 0.4774
[09/26 13:15:32 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1700, average loss: 2.4246
[09/26 13:15:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 13:15:32 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 13:15:39 visual_prompt]: Epoch 42 / 100: avg data time: 6.00e-02, avg batch time: 0.5098, average train loss: 0.4391
[09/26 13:15:40 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1697, average loss: 2.1976
[09/26 13:15:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 98.00	
[09/26 13:15:40 visual_prompt]: Best epoch 42: best metric: 0.425
[09/26 13:15:40 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 13:15:47 visual_prompt]: Epoch 43 / 100: avg data time: 4.72e-02, avg batch time: 0.4982, average train loss: 0.3176
[09/26 13:15:49 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1702, average loss: 2.2240
[09/26 13:15:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.00	
[09/26 13:15:49 visual_prompt]: Best epoch 43: best metric: 0.435
[09/26 13:15:49 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 13:15:56 visual_prompt]: Epoch 44 / 100: avg data time: 5.42e-02, avg batch time: 0.5046, average train loss: 0.2760
[09/26 13:15:57 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 2.6286
[09/26 13:15:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.50	
[09/26 13:15:57 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 13:16:04 visual_prompt]: Epoch 45 / 100: avg data time: 5.95e-02, avg batch time: 0.5103, average train loss: 0.2169
[09/26 13:16:06 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 3.1794
[09/26 13:16:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 95.00	
[09/26 13:16:06 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 13:16:12 visual_prompt]: Epoch 46 / 100: avg data time: 4.62e-02, avg batch time: 0.4978, average train loss: 0.2390
[09/26 13:16:14 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1698, average loss: 2.8307
[09/26 13:16:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.50	
[09/26 13:16:14 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 13:16:21 visual_prompt]: Epoch 47 / 100: avg data time: 5.77e-02, avg batch time: 0.5090, average train loss: 0.3505
[09/26 13:16:22 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1701, average loss: 3.0320
[09/26 13:16:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 95.50	
[09/26 13:16:22 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 13:16:29 visual_prompt]: Epoch 48 / 100: avg data time: 5.82e-02, avg batch time: 0.5090, average train loss: 0.4222
[09/26 13:16:31 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1697, average loss: 2.2147
[09/26 13:16:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.50	
[09/26 13:16:31 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 13:16:38 visual_prompt]: Epoch 49 / 100: avg data time: 5.47e-02, avg batch time: 0.5064, average train loss: 0.2727
[09/26 13:16:39 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1696, average loss: 2.3284
[09/26 13:16:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 13:16:39 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 13:16:46 visual_prompt]: Epoch 50 / 100: avg data time: 5.26e-02, avg batch time: 0.5040, average train loss: 0.2389
[09/26 13:16:47 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1697, average loss: 2.6598
[09/26 13:16:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 13:16:47 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 13:16:54 visual_prompt]: Epoch 51 / 100: avg data time: 5.67e-02, avg batch time: 0.5073, average train loss: 0.2170
[09/26 13:16:56 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1698, average loss: 3.0089
[09/26 13:16:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 13:16:56 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 13:17:03 visual_prompt]: Epoch 52 / 100: avg data time: 4.79e-02, avg batch time: 0.4987, average train loss: 0.1810
[09/26 13:17:04 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1695, average loss: 2.9335
[09/26 13:17:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 13:17:04 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 13:17:11 visual_prompt]: Epoch 53 / 100: avg data time: 4.42e-02, avg batch time: 0.4960, average train loss: 0.1851
[09/26 13:17:12 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1697, average loss: 3.3422
[09/26 13:17:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 13:17:12 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 13:17:19 visual_prompt]: Epoch 54 / 100: avg data time: 5.74e-02, avg batch time: 0.5068, average train loss: 0.2601
[09/26 13:17:21 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1702, average loss: 2.6681
[09/26 13:17:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.00	
[09/26 13:17:21 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 13:17:28 visual_prompt]: Epoch 55 / 100: avg data time: 5.98e-02, avg batch time: 0.5090, average train loss: 0.1993
[09/26 13:17:29 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1698, average loss: 2.6863
[09/26 13:17:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 13:17:29 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 13:17:36 visual_prompt]: Epoch 56 / 100: avg data time: 5.92e-02, avg batch time: 0.5084, average train loss: 0.1800
[09/26 13:17:38 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 3.0182
[09/26 13:17:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 13:17:38 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 13:17:45 visual_prompt]: Epoch 57 / 100: avg data time: 5.88e-02, avg batch time: 0.5090, average train loss: 0.1439
[09/26 13:17:46 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.1695, average loss: 3.1685
[09/26 13:17:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/26 13:17:46 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 13:17:53 visual_prompt]: Epoch 58 / 100: avg data time: 5.80e-02, avg batch time: 0.5074, average train loss: 0.1298
[09/26 13:17:55 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1698, average loss: 3.1162
[09/26 13:17:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.00	
[09/26 13:17:55 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 13:18:01 visual_prompt]: Epoch 59 / 100: avg data time: 4.52e-02, avg batch time: 0.4960, average train loss: 0.1112
[09/26 13:18:03 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1700, average loss: 3.2997
[09/26 13:18:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.50	
[09/26 13:18:03 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 13:18:10 visual_prompt]: Epoch 60 / 100: avg data time: 5.96e-02, avg batch time: 0.5089, average train loss: 0.1651
[09/26 13:18:11 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1694, average loss: 3.1583
[09/26 13:18:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 13:18:11 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 13:18:18 visual_prompt]: Epoch 61 / 100: avg data time: 4.72e-02, avg batch time: 0.4998, average train loss: 0.1114
[09/26 13:18:20 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1700, average loss: 3.5264
[09/26 13:18:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.00	
[09/26 13:18:20 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 13:18:26 visual_prompt]: Epoch 62 / 100: avg data time: 5.54e-02, avg batch time: 0.5057, average train loss: 0.0783
[09/26 13:18:28 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1700, average loss: 3.9233
[09/26 13:18:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 96.00	
[09/26 13:18:28 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 13:18:35 visual_prompt]: Epoch 63 / 100: avg data time: 5.78e-02, avg batch time: 0.5075, average train loss: 0.0552
[09/26 13:18:36 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1701, average loss: 3.6874
[09/26 13:18:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 13:18:36 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 13:18:43 visual_prompt]: Epoch 64 / 100: avg data time: 6.14e-02, avg batch time: 0.5118, average train loss: 0.0666
[09/26 13:18:45 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1700, average loss: 3.4463
[09/26 13:18:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 95.50	
[09/26 13:18:45 visual_prompt]: Best epoch 64: best metric: 0.445
[09/26 13:18:45 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 13:18:52 visual_prompt]: Epoch 65 / 100: avg data time: 5.61e-02, avg batch time: 0.5064, average train loss: 0.0762
[09/26 13:18:53 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1702, average loss: 3.6893
[09/26 13:18:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 95.00	
[09/26 13:18:53 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 13:19:00 visual_prompt]: Epoch 66 / 100: avg data time: 5.86e-02, avg batch time: 0.5094, average train loss: 0.0462
[09/26 13:19:02 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1697, average loss: 3.8412
[09/26 13:19:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.00	
[09/26 13:19:02 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 13:19:09 visual_prompt]: Epoch 67 / 100: avg data time: 4.64e-02, avg batch time: 0.4976, average train loss: 0.0203
[09/26 13:19:10 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1699, average loss: 4.1424
[09/26 13:19:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 95.50	
[09/26 13:19:10 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 13:19:17 visual_prompt]: Epoch 68 / 100: avg data time: 5.82e-02, avg batch time: 0.5081, average train loss: 0.0139
[09/26 13:19:19 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1695, average loss: 4.4563
[09/26 13:19:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.00	
[09/26 13:19:19 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 13:19:25 visual_prompt]: Epoch 69 / 100: avg data time: 5.97e-02, avg batch time: 0.5087, average train loss: 0.0095
[09/26 13:19:27 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1696, average loss: 4.9182
[09/26 13:19:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.00	
[09/26 13:19:27 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 13:19:34 visual_prompt]: Epoch 70 / 100: avg data time: 6.21e-02, avg batch time: 0.5123, average train loss: 0.0101
[09/26 13:19:36 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1700, average loss: 4.6027
[09/26 13:19:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 95.50	
[09/26 13:19:36 visual_prompt]: Best epoch 70: best metric: 0.455
[09/26 13:19:36 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 13:19:42 visual_prompt]: Epoch 71 / 100: avg data time: 5.67e-02, avg batch time: 0.5068, average train loss: 0.0595
[09/26 13:19:44 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1697, average loss: 4.3763
[09/26 13:19:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.50	
[09/26 13:19:44 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 13:19:51 visual_prompt]: Epoch 72 / 100: avg data time: 5.18e-02, avg batch time: 0.5021, average train loss: 0.0168
[09/26 13:19:52 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 4.5307
[09/26 13:19:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.50	
[09/26 13:19:52 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 13:19:59 visual_prompt]: Epoch 73 / 100: avg data time: 5.32e-02, avg batch time: 0.5035, average train loss: 0.0182
[09/26 13:20:01 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 4.4509
[09/26 13:20:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.00	
[09/26 13:20:01 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 13:20:08 visual_prompt]: Epoch 74 / 100: avg data time: 5.97e-02, avg batch time: 0.5089, average train loss: 0.0119
[09/26 13:20:09 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 5.2734
[09/26 13:20:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.50	
[09/26 13:20:09 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 13:20:16 visual_prompt]: Epoch 75 / 100: avg data time: 4.46e-02, avg batch time: 0.4948, average train loss: 0.0150
[09/26 13:20:17 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1700, average loss: 5.0289
[09/26 13:20:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.00	
[09/26 13:20:17 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 13:20:24 visual_prompt]: Epoch 76 / 100: avg data time: 4.77e-02, avg batch time: 0.4972, average train loss: 0.0079
[09/26 13:20:26 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1694, average loss: 5.1777
[09/26 13:20:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.00	
[09/26 13:20:26 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 13:20:32 visual_prompt]: Epoch 77 / 100: avg data time: 4.39e-02, avg batch time: 0.4957, average train loss: 0.0039
[09/26 13:20:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1696, average loss: 5.0089
[09/26 13:20:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 95.50	
[09/26 13:20:34 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 13:20:41 visual_prompt]: Epoch 78 / 100: avg data time: 4.90e-02, avg batch time: 0.4991, average train loss: 0.0046
[09/26 13:20:42 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1694, average loss: 4.8684
[09/26 13:20:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 96.00	
[09/26 13:20:42 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 13:20:49 visual_prompt]: Epoch 79 / 100: avg data time: 5.45e-02, avg batch time: 0.5051, average train loss: 0.0035
[09/26 13:20:51 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1697, average loss: 5.1465
[09/26 13:20:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 95.50	
[09/26 13:20:51 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 13:20:57 visual_prompt]: Epoch 80 / 100: avg data time: 4.37e-02, avg batch time: 0.4948, average train loss: 0.0036
[09/26 13:20:59 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1696, average loss: 5.2750
[09/26 13:20:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 13:20:59 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 13:21:06 visual_prompt]: Epoch 81 / 100: avg data time: 6.22e-02, avg batch time: 0.5115, average train loss: 0.0042
[09/26 13:21:07 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1699, average loss: 5.2493
[09/26 13:21:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 95.50	
[09/26 13:21:07 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 13:21:14 visual_prompt]: Epoch 82 / 100: avg data time: 5.97e-02, avg batch time: 0.5095, average train loss: 0.0023
[09/26 13:21:16 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 5.3269
[09/26 13:21:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 95.50	
[09/26 13:21:16 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 13:21:23 visual_prompt]: Epoch 83 / 100: avg data time: 4.57e-02, avg batch time: 0.4971, average train loss: 0.0029
[09/26 13:21:24 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1701, average loss: 5.4979
[09/26 13:21:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.00	
[09/26 13:21:24 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 13:21:31 visual_prompt]: Epoch 84 / 100: avg data time: 4.64e-02, avg batch time: 0.4979, average train loss: 0.0115
[09/26 13:21:32 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1698, average loss: 5.4794
[09/26 13:21:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 13:21:32 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 13:21:39 visual_prompt]: Epoch 85 / 100: avg data time: 5.11e-02, avg batch time: 0.5019, average train loss: 0.0063
[09/26 13:21:41 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1694, average loss: 5.3903
[09/26 13:21:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.50	
[09/26 13:21:41 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 13:21:47 visual_prompt]: Epoch 86 / 100: avg data time: 4.79e-02, avg batch time: 0.4981, average train loss: 0.0029
[09/26 13:21:49 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1699, average loss: 5.2963
[09/26 13:21:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.50	
[09/26 13:21:49 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 13:21:56 visual_prompt]: Epoch 87 / 100: avg data time: 5.01e-02, avg batch time: 0.5005, average train loss: 0.0021
[09/26 13:21:57 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1699, average loss: 5.3176
[09/26 13:21:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 13:21:57 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 13:22:04 visual_prompt]: Epoch 88 / 100: avg data time: 5.17e-02, avg batch time: 0.5011, average train loss: 0.0024
[09/26 13:22:05 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 5.3707
[09/26 13:22:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 13:22:05 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 13:22:12 visual_prompt]: Epoch 89 / 100: avg data time: 4.72e-02, avg batch time: 0.4984, average train loss: 0.0019
[09/26 13:22:14 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 5.4020
[09/26 13:22:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 13:22:14 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 13:22:21 visual_prompt]: Epoch 90 / 100: avg data time: 5.63e-02, avg batch time: 0.5077, average train loss: 0.0024
[09/26 13:22:22 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1699, average loss: 5.4309
[09/26 13:22:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.00	
[09/26 13:22:22 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 13:22:29 visual_prompt]: Epoch 91 / 100: avg data time: 6.24e-02, avg batch time: 0.5119, average train loss: 0.0028
[09/26 13:22:31 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1700, average loss: 5.4371
[09/26 13:22:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.00	
[09/26 13:22:31 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 13:22:37 visual_prompt]: Epoch 92 / 100: avg data time: 5.10e-02, avg batch time: 0.5008, average train loss: 0.0017
[09/26 13:22:39 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1700, average loss: 5.4400
[09/26 13:22:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 13:22:39 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 13:22:46 visual_prompt]: Epoch 93 / 100: avg data time: 4.73e-02, avg batch time: 0.4979, average train loss: 0.0017
[09/26 13:22:47 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 5.4570
[09/26 13:22:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.50	
[09/26 13:22:47 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 13:22:54 visual_prompt]: Epoch 94 / 100: avg data time: 5.08e-02, avg batch time: 0.5013, average train loss: 0.0024
[09/26 13:22:56 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1698, average loss: 5.4748
[09/26 13:22:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 13:22:56 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 13:23:03 visual_prompt]: Epoch 95 / 100: avg data time: 6.44e-02, avg batch time: 0.5135, average train loss: 0.0020
[09/26 13:23:04 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1695, average loss: 5.4853
[09/26 13:23:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 13:23:04 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 13:23:11 visual_prompt]: Epoch 96 / 100: avg data time: 4.96e-02, avg batch time: 0.4986, average train loss: 0.0021
[09/26 13:23:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1700, average loss: 5.4933
[09/26 13:23:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 13:23:13 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 13:23:19 visual_prompt]: Epoch 97 / 100: avg data time: 6.02e-02, avg batch time: 0.5107, average train loss: 0.0020
[09/26 13:23:21 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 5.4976
[09/26 13:23:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 13:23:21 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 13:23:28 visual_prompt]: Epoch 98 / 100: avg data time: 5.66e-02, avg batch time: 0.5070, average train loss: 0.0030
[09/26 13:23:29 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1695, average loss: 5.4973
[09/26 13:23:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 13:23:29 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 13:23:36 visual_prompt]: Epoch 99 / 100: avg data time: 4.51e-02, avg batch time: 0.4966, average train loss: 0.0019
[09/26 13:23:37 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1696, average loss: 5.4958
[09/26 13:23:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 13:23:37 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 13:23:44 visual_prompt]: Epoch 100 / 100: avg data time: 4.94e-02, avg batch time: 0.5001, average train loss: 0.0021
[09/26 13:23:46 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 5.4959
[09/26 13:23:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 13:23:46 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:23:46 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:23:46 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:23:46 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:23:46 visual_prompt]: Training with config:
[09/26 13:23:46 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:23:46 visual_prompt]: Loading training data...
[09/26 13:23:46 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 13:23:47 visual_prompt]: Number of images: 800
[09/26 13:23:47 visual_prompt]: Number of classes: 6 / 6
[09/26 13:23:47 visual_prompt]: Loading validation data...
[09/26 13:23:47 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 13:23:48 visual_prompt]: Number of images: 200
[09/26 13:23:48 visual_prompt]: Number of classes: 6 / 6
[09/26 13:23:48 visual_prompt]: Constructing models...
[09/26 13:23:50 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 13:23:50 visual_prompt]: tuned percent:0.540
[09/26 13:23:50 visual_prompt]: Device used for model: 0
[09/26 13:23:50 visual_prompt]: Setting up Evaluator...
[09/26 13:23:50 visual_prompt]: Setting up Trainer...
[09/26 13:23:50 visual_prompt]: 	Setting up the optimizer...
[09/26 13:23:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:23:57 visual_prompt]: Epoch 1 / 100: avg data time: 5.43e-02, avg batch time: 0.5055, average train loss: 2.9624
[09/26 13:23:59 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1687, average loss: 2.9268
[09/26 13:23:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 13:23:59 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 13:23:59 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 13:24:05 visual_prompt]: Epoch 2 / 100: avg data time: 5.87e-02, avg batch time: 0.5075, average train loss: 2.6981
[09/26 13:24:07 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1686, average loss: 1.8267
[09/26 13:24:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 86.50	
[09/26 13:24:07 visual_prompt]: Best epoch 2: best metric: 0.190
[09/26 13:24:07 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 13:24:14 visual_prompt]: Epoch 3 / 100: avg data time: 4.89e-02, avg batch time: 0.4980, average train loss: 1.8922
[09/26 13:24:15 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1689, average loss: 1.8003
[09/26 13:24:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 85.50	
[09/26 13:24:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 13:24:22 visual_prompt]: Epoch 4 / 100: avg data time: 5.05e-02, avg batch time: 0.4995, average train loss: 1.7952
[09/26 13:24:24 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1685, average loss: 1.7691
[09/26 13:24:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 90.00	
[09/26 13:24:24 visual_prompt]: Best epoch 4: best metric: 0.230
[09/26 13:24:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 13:24:30 visual_prompt]: Epoch 5 / 100: avg data time: 4.42e-02, avg batch time: 0.4928, average train loss: 1.7896
[09/26 13:24:32 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 1.8643
[09/26 13:24:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 83.00	
[09/26 13:24:32 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 13:24:39 visual_prompt]: Epoch 6 / 100: avg data time: 4.75e-02, avg batch time: 0.4967, average train loss: 1.7425
[09/26 13:24:40 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1695, average loss: 1.7122
[09/26 13:24:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 92.50	
[09/26 13:24:40 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 13:24:47 visual_prompt]: Epoch 7 / 100: avg data time: 6.19e-02, avg batch time: 0.5103, average train loss: 1.6704
[09/26 13:24:49 visual_prompt]: Inference (val):avg data time: 5.91e-05, avg batch time: 0.1731, average loss: 1.7876
[09/26 13:24:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 93.00	
[09/26 13:24:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 13:24:56 visual_prompt]: Epoch 8 / 100: avg data time: 5.11e-02, avg batch time: 0.5000, average train loss: 1.6319
[09/26 13:24:57 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 1.6655
[09/26 13:24:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 91.50	
[09/26 13:24:57 visual_prompt]: Best epoch 8: best metric: 0.280
[09/26 13:24:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 13:25:04 visual_prompt]: Epoch 9 / 100: avg data time: 4.58e-02, avg batch time: 0.4957, average train loss: 1.5687
[09/26 13:25:06 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.1698, average loss: 1.6027
[09/26 13:25:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 93.00	
[09/26 13:25:06 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 13:25:12 visual_prompt]: Epoch 10 / 100: avg data time: 4.30e-02, avg batch time: 0.4936, average train loss: 1.5434
[09/26 13:25:14 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1691, average loss: 1.6817
[09/26 13:25:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 91.00	
[09/26 13:25:14 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 13:25:21 visual_prompt]: Epoch 11 / 100: avg data time: 4.38e-02, avg batch time: 0.4946, average train loss: 1.4755
[09/26 13:25:22 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1698, average loss: 1.7072
[09/26 13:25:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 94.50	
[09/26 13:25:22 visual_prompt]: Best epoch 11: best metric: 0.295
[09/26 13:25:22 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 13:25:29 visual_prompt]: Epoch 12 / 100: avg data time: 5.75e-02, avg batch time: 0.5059, average train loss: 1.3995
[09/26 13:25:31 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1698, average loss: 1.5436
[09/26 13:25:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 97.00	
[09/26 13:25:31 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 13:25:37 visual_prompt]: Epoch 13 / 100: avg data time: 5.43e-02, avg batch time: 0.5032, average train loss: 1.3454
[09/26 13:25:39 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 1.4500
[09/26 13:25:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/26 13:25:39 visual_prompt]: Best epoch 13: best metric: 0.420
[09/26 13:25:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 13:25:46 visual_prompt]: Epoch 14 / 100: avg data time: 5.98e-02, avg batch time: 0.5091, average train loss: 1.2076
[09/26 13:25:47 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1695, average loss: 1.4915
[09/26 13:25:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 13:25:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 13:25:54 visual_prompt]: Epoch 15 / 100: avg data time: 4.53e-02, avg batch time: 0.4973, average train loss: 1.2424
[09/26 13:25:56 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1693, average loss: 1.5612
[09/26 13:25:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.50	
[09/26 13:25:56 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 13:26:03 visual_prompt]: Epoch 16 / 100: avg data time: 6.29e-02, avg batch time: 0.5115, average train loss: 1.1628
[09/26 13:26:04 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1695, average loss: 1.5711
[09/26 13:26:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 13:26:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 13:26:11 visual_prompt]: Epoch 17 / 100: avg data time: 4.99e-02, avg batch time: 0.5008, average train loss: 1.1153
[09/26 13:26:13 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1690, average loss: 1.5905
[09/26 13:26:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.00	
[09/26 13:26:13 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 13:26:19 visual_prompt]: Epoch 18 / 100: avg data time: 5.88e-02, avg batch time: 0.5093, average train loss: 1.1058
[09/26 13:26:21 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1693, average loss: 1.5029
[09/26 13:26:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 13:26:21 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 13:26:28 visual_prompt]: Epoch 19 / 100: avg data time: 4.99e-02, avg batch time: 0.5024, average train loss: 1.0423
[09/26 13:26:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1692, average loss: 1.7172
[09/26 13:26:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 95.50	
[09/26 13:26:29 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 13:26:36 visual_prompt]: Epoch 20 / 100: avg data time: 4.70e-02, avg batch time: 0.4967, average train loss: 0.9818
[09/26 13:26:38 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 1.9920
[09/26 13:26:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 95.00	
[09/26 13:26:38 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 13:26:44 visual_prompt]: Epoch 21 / 100: avg data time: 6.12e-02, avg batch time: 0.5120, average train loss: 0.9970
[09/26 13:26:46 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1694, average loss: 1.5710
[09/26 13:26:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 13:26:46 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 13:26:53 visual_prompt]: Epoch 22 / 100: avg data time: 5.05e-02, avg batch time: 0.4995, average train loss: 0.9327
[09/26 13:26:54 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1697, average loss: 1.6335
[09/26 13:26:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.50	
[09/26 13:26:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 13:27:01 visual_prompt]: Epoch 23 / 100: avg data time: 4.31e-02, avg batch time: 0.4937, average train loss: 0.9389
[09/26 13:27:03 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1692, average loss: 1.7378
[09/26 13:27:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 13:27:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 13:27:09 visual_prompt]: Epoch 24 / 100: avg data time: 5.43e-02, avg batch time: 0.5038, average train loss: 0.9231
[09/26 13:27:11 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1695, average loss: 1.8833
[09/26 13:27:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 13:27:11 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 13:27:18 visual_prompt]: Epoch 25 / 100: avg data time: 5.10e-02, avg batch time: 0.5006, average train loss: 0.9514
[09/26 13:27:19 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1694, average loss: 1.6955
[09/26 13:27:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 13:27:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 13:27:26 visual_prompt]: Epoch 26 / 100: avg data time: 5.54e-02, avg batch time: 0.5058, average train loss: 0.7527
[09/26 13:27:28 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 1.7708
[09/26 13:27:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 13:27:28 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 13:27:35 visual_prompt]: Epoch 27 / 100: avg data time: 5.75e-02, avg batch time: 0.5070, average train loss: 0.7388
[09/26 13:27:36 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1697, average loss: 1.9349
[09/26 13:27:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.00	
[09/26 13:27:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 13:27:43 visual_prompt]: Epoch 28 / 100: avg data time: 5.15e-02, avg batch time: 0.5010, average train loss: 0.6688
[09/26 13:27:44 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 2.2733
[09/26 13:27:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.00	
[09/26 13:27:44 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 13:27:51 visual_prompt]: Epoch 29 / 100: avg data time: 5.14e-02, avg batch time: 0.5016, average train loss: 0.6848
[09/26 13:27:53 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1698, average loss: 1.8698
[09/26 13:27:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.00	
[09/26 13:27:53 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 13:28:00 visual_prompt]: Epoch 30 / 100: avg data time: 6.07e-02, avg batch time: 0.5099, average train loss: 0.5769
[09/26 13:28:01 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 2.1716
[09/26 13:28:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.00	
[09/26 13:28:01 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 13:28:08 visual_prompt]: Epoch 31 / 100: avg data time: 4.61e-02, avg batch time: 0.4975, average train loss: 0.5230
[09/26 13:28:09 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1696, average loss: 2.0927
[09/26 13:28:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.50	
[09/26 13:28:09 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 13:28:16 visual_prompt]: Epoch 32 / 100: avg data time: 4.72e-02, avg batch time: 0.4965, average train loss: 0.5745
[09/26 13:28:18 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1689, average loss: 2.3945
[09/26 13:28:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 95.00	
[09/26 13:28:18 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 13:28:25 visual_prompt]: Epoch 33 / 100: avg data time: 5.00e-02, avg batch time: 0.5015, average train loss: 0.5422
[09/26 13:28:26 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1696, average loss: 2.5305
[09/26 13:28:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 13:28:26 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 13:28:33 visual_prompt]: Epoch 34 / 100: avg data time: 4.66e-02, avg batch time: 0.4976, average train loss: 0.5495
[09/26 13:28:34 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1695, average loss: 2.1947
[09/26 13:28:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.00	
[09/26 13:28:34 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 13:28:41 visual_prompt]: Epoch 35 / 100: avg data time: 5.77e-02, avg batch time: 0.5069, average train loss: 0.4986
[09/26 13:28:43 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1694, average loss: 2.8595
[09/26 13:28:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 13:28:43 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 13:28:50 visual_prompt]: Epoch 36 / 100: avg data time: 5.54e-02, avg batch time: 0.5061, average train loss: 0.4680
[09/26 13:28:51 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1695, average loss: 3.0206
[09/26 13:28:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 95.50	
[09/26 13:28:51 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 13:28:58 visual_prompt]: Epoch 37 / 100: avg data time: 5.62e-02, avg batch time: 0.5054, average train loss: 0.4486
[09/26 13:29:00 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1698, average loss: 2.4372
[09/26 13:29:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 13:29:00 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 13:29:06 visual_prompt]: Epoch 38 / 100: avg data time: 6.00e-02, avg batch time: 0.5105, average train loss: 0.3770
[09/26 13:29:08 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 2.6591
[09/26 13:29:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 95.50	
[09/26 13:29:08 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 13:29:15 visual_prompt]: Epoch 39 / 100: avg data time: 6.24e-02, avg batch time: 0.5112, average train loss: 0.3289
[09/26 13:29:16 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1690, average loss: 2.8868
[09/26 13:29:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.00	
[09/26 13:29:16 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 13:29:23 visual_prompt]: Epoch 40 / 100: avg data time: 6.03e-02, avg batch time: 0.5115, average train loss: 0.3119
[09/26 13:29:25 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 2.9710
[09/26 13:29:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 94.50	
[09/26 13:29:25 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 13:29:32 visual_prompt]: Epoch 41 / 100: avg data time: 4.90e-02, avg batch time: 0.4988, average train loss: 0.2743
[09/26 13:29:33 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1699, average loss: 3.4369
[09/26 13:29:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 13:29:33 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 13:29:40 visual_prompt]: Epoch 42 / 100: avg data time: 4.72e-02, avg batch time: 0.4980, average train loss: 0.2905
[09/26 13:29:42 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1697, average loss: 3.0909
[09/26 13:29:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.00	
[09/26 13:29:42 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 13:29:48 visual_prompt]: Epoch 43 / 100: avg data time: 5.57e-02, avg batch time: 0.5055, average train loss: 0.2279
[09/26 13:29:50 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 3.5134
[09/26 13:29:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 95.00	
[09/26 13:29:50 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 13:29:57 visual_prompt]: Epoch 44 / 100: avg data time: 5.21e-02, avg batch time: 0.5031, average train loss: 0.2763
[09/26 13:29:58 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 3.1387
[09/26 13:29:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 94.50	
[09/26 13:29:58 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 13:30:05 visual_prompt]: Epoch 45 / 100: avg data time: 5.40e-02, avg batch time: 0.5031, average train loss: 0.2802
[09/26 13:30:07 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 3.5071
[09/26 13:30:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 13:30:07 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 13:30:14 visual_prompt]: Epoch 46 / 100: avg data time: 5.84e-02, avg batch time: 0.5081, average train loss: 0.2154
[09/26 13:30:15 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1696, average loss: 3.7234
[09/26 13:30:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 13:30:15 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 13:30:22 visual_prompt]: Epoch 47 / 100: avg data time: 6.13e-02, avg batch time: 0.5101, average train loss: 0.2423
[09/26 13:30:24 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1696, average loss: 3.6230
[09/26 13:30:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 94.50	
[09/26 13:30:24 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 13:30:31 visual_prompt]: Epoch 48 / 100: avg data time: 5.63e-02, avg batch time: 0.5058, average train loss: 0.1840
[09/26 13:30:32 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1693, average loss: 4.4849
[09/26 13:30:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.00	
[09/26 13:30:32 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 13:30:39 visual_prompt]: Epoch 49 / 100: avg data time: 5.68e-02, avg batch time: 0.5059, average train loss: 0.1518
[09/26 13:30:41 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 4.7075
[09/26 13:30:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 95.00	
[09/26 13:30:41 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 13:30:48 visual_prompt]: Epoch 50 / 100: avg data time: 6.35e-02, avg batch time: 0.5131, average train loss: 0.1275
[09/26 13:30:49 visual_prompt]: Inference (val):avg data time: 4.94e-05, avg batch time: 0.1700, average loss: 4.2695
[09/26 13:30:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 95.50	
[09/26 13:30:49 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 13:30:56 visual_prompt]: Epoch 51 / 100: avg data time: 5.37e-02, avg batch time: 0.5036, average train loss: 0.1469
[09/26 13:30:57 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1693, average loss: 4.5309
[09/26 13:30:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 13:30:57 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 13:31:04 visual_prompt]: Epoch 52 / 100: avg data time: 5.94e-02, avg batch time: 0.5084, average train loss: 0.1986
[09/26 13:31:06 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 3.9892
[09/26 13:31:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 96.00	
[09/26 13:31:06 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 13:31:13 visual_prompt]: Epoch 53 / 100: avg data time: 4.61e-02, avg batch time: 0.4969, average train loss: 0.1598
[09/26 13:31:14 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 4.2253
[09/26 13:31:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 13:31:14 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 13:31:21 visual_prompt]: Epoch 54 / 100: avg data time: 5.82e-02, avg batch time: 0.5089, average train loss: 0.1257
[09/26 13:31:23 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 4.4063
[09/26 13:31:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 95.00	
[09/26 13:31:23 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 13:31:29 visual_prompt]: Epoch 55 / 100: avg data time: 5.60e-02, avg batch time: 0.5050, average train loss: 0.0740
[09/26 13:31:31 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1695, average loss: 4.5974
[09/26 13:31:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.50	
[09/26 13:31:31 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 13:31:38 visual_prompt]: Epoch 56 / 100: avg data time: 5.60e-02, avg batch time: 0.5080, average train loss: 0.0693
[09/26 13:31:39 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1695, average loss: 4.8444
[09/26 13:31:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.00	
[09/26 13:31:39 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 13:31:46 visual_prompt]: Epoch 57 / 100: avg data time: 5.86e-02, avg batch time: 0.5082, average train loss: 0.1130
[09/26 13:31:48 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1695, average loss: 4.8930
[09/26 13:31:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.00	
[09/26 13:31:48 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 13:31:55 visual_prompt]: Epoch 58 / 100: avg data time: 5.57e-02, avg batch time: 0.5047, average train loss: 0.0969
[09/26 13:31:56 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 5.0033
[09/26 13:31:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 94.00	
[09/26 13:31:56 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 13:32:03 visual_prompt]: Epoch 59 / 100: avg data time: 5.73e-02, avg batch time: 0.5071, average train loss: 0.0636
[09/26 13:32:05 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1696, average loss: 4.9570
[09/26 13:32:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 95.50	
[09/26 13:32:05 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 13:32:12 visual_prompt]: Epoch 60 / 100: avg data time: 5.56e-02, avg batch time: 0.5050, average train loss: 0.0509
[09/26 13:32:13 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1696, average loss: 5.5525
[09/26 13:32:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.00	
[09/26 13:32:13 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 13:32:20 visual_prompt]: Epoch 61 / 100: avg data time: 4.45e-02, avg batch time: 0.4953, average train loss: 0.0374
[09/26 13:32:22 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 5.7377
[09/26 13:32:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.00	
[09/26 13:32:22 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 13:32:28 visual_prompt]: Epoch 62 / 100: avg data time: 5.67e-02, avg batch time: 0.5068, average train loss: 0.0260
[09/26 13:32:30 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 5.4113
[09/26 13:32:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.50	
[09/26 13:32:30 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 13:32:37 visual_prompt]: Epoch 63 / 100: avg data time: 5.60e-02, avg batch time: 0.5060, average train loss: 0.0184
[09/26 13:32:38 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1691, average loss: 5.6715
[09/26 13:32:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.50	
[09/26 13:32:38 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 13:32:45 visual_prompt]: Epoch 64 / 100: avg data time: 4.56e-02, avg batch time: 0.4959, average train loss: 0.0138
[09/26 13:32:47 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1699, average loss: 6.2558
[09/26 13:32:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.50	
[09/26 13:32:47 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 13:32:54 visual_prompt]: Epoch 65 / 100: avg data time: 5.62e-02, avg batch time: 0.5058, average train loss: 0.0194
[09/26 13:32:55 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1696, average loss: 6.3285
[09/26 13:32:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 13:32:55 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 13:33:02 visual_prompt]: Epoch 66 / 100: avg data time: 5.30e-02, avg batch time: 0.5025, average train loss: 0.0276
[09/26 13:33:03 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 6.0639
[09/26 13:33:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 95.00	
[09/26 13:33:03 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 13:33:10 visual_prompt]: Epoch 67 / 100: avg data time: 5.44e-02, avg batch time: 0.5050, average train loss: 0.0270
[09/26 13:33:12 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1696, average loss: 6.5045
[09/26 13:33:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 13:33:12 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 13:33:19 visual_prompt]: Epoch 68 / 100: avg data time: 5.33e-02, avg batch time: 0.5021, average train loss: 0.0631
[09/26 13:33:20 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 6.0026
[09/26 13:33:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 95.50	
[09/26 13:33:20 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 13:33:27 visual_prompt]: Epoch 69 / 100: avg data time: 5.21e-02, avg batch time: 0.5030, average train loss: 0.0231
[09/26 13:33:28 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1699, average loss: 5.6045
[09/26 13:33:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 13:33:28 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 13:33:35 visual_prompt]: Epoch 70 / 100: avg data time: 6.24e-02, avg batch time: 0.5115, average train loss: 0.0214
[09/26 13:33:37 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1696, average loss: 5.6000
[09/26 13:33:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 95.50	
[09/26 13:33:37 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 13:33:44 visual_prompt]: Epoch 71 / 100: avg data time: 5.74e-02, avg batch time: 0.5069, average train loss: 0.0273
[09/26 13:33:46 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 5.9672
[09/26 13:33:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.50	
[09/26 13:33:46 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 13:33:52 visual_prompt]: Epoch 72 / 100: avg data time: 4.74e-02, avg batch time: 0.4975, average train loss: 0.0154
[09/26 13:33:54 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1696, average loss: 6.4729
[09/26 13:33:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 95.50	
[09/26 13:33:54 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 13:34:01 visual_prompt]: Epoch 73 / 100: avg data time: 4.57e-02, avg batch time: 0.4959, average train loss: 0.0107
[09/26 13:34:02 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1693, average loss: 6.6094
[09/26 13:34:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.50	
[09/26 13:34:02 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 13:34:09 visual_prompt]: Epoch 74 / 100: avg data time: 5.34e-02, avg batch time: 0.5030, average train loss: 0.0089
[09/26 13:34:10 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 6.7530
[09/26 13:34:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.50	
[09/26 13:34:10 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 13:34:17 visual_prompt]: Epoch 75 / 100: avg data time: 5.60e-02, avg batch time: 0.5053, average train loss: 0.0078
[09/26 13:34:19 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1694, average loss: 6.6897
[09/26 13:34:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 13:34:19 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 13:34:26 visual_prompt]: Epoch 76 / 100: avg data time: 5.65e-02, avg batch time: 0.5064, average train loss: 0.0058
[09/26 13:34:27 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1692, average loss: 6.8492
[09/26 13:34:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 95.50	
[09/26 13:34:27 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 13:34:34 visual_prompt]: Epoch 77 / 100: avg data time: 6.18e-02, avg batch time: 0.5109, average train loss: 0.0091
[09/26 13:34:36 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1700, average loss: 7.0477
[09/26 13:34:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 95.00	
[09/26 13:34:36 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 13:34:43 visual_prompt]: Epoch 78 / 100: avg data time: 5.67e-02, avg batch time: 0.5072, average train loss: 0.0035
[09/26 13:34:44 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1696, average loss: 6.9672
[09/26 13:34:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 95.00	
[09/26 13:34:44 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 13:34:51 visual_prompt]: Epoch 79 / 100: avg data time: 5.74e-02, avg batch time: 0.5063, average train loss: 0.0054
[09/26 13:34:53 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1697, average loss: 6.9826
[09/26 13:34:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 95.50	
[09/26 13:34:53 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 13:35:00 visual_prompt]: Epoch 80 / 100: avg data time: 6.16e-02, avg batch time: 0.5103, average train loss: 0.0032
[09/26 13:35:01 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1699, average loss: 7.0647
[09/26 13:35:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 95.50	
[09/26 13:35:01 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 13:35:08 visual_prompt]: Epoch 81 / 100: avg data time: 5.82e-02, avg batch time: 0.5065, average train loss: 0.0038
[09/26 13:35:10 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1696, average loss: 7.0143
[09/26 13:35:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 95.50	
[09/26 13:35:10 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 13:35:16 visual_prompt]: Epoch 82 / 100: avg data time: 5.47e-02, avg batch time: 0.5034, average train loss: 0.0028
[09/26 13:35:18 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 7.0094
[09/26 13:35:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 95.00	
[09/26 13:35:18 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 13:35:25 visual_prompt]: Epoch 83 / 100: avg data time: 5.93e-02, avg batch time: 0.5085, average train loss: 0.0031
[09/26 13:35:26 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1696, average loss: 7.0754
[09/26 13:35:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 95.50	
[09/26 13:35:26 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 13:35:33 visual_prompt]: Epoch 84 / 100: avg data time: 5.81e-02, avg batch time: 0.5069, average train loss: 0.0033
[09/26 13:35:35 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1692, average loss: 7.1709
[09/26 13:35:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 95.00	
[09/26 13:35:35 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 13:35:42 visual_prompt]: Epoch 85 / 100: avg data time: 5.08e-02, avg batch time: 0.5003, average train loss: 0.0022
[09/26 13:35:43 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1694, average loss: 7.2378
[09/26 13:35:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 94.50	
[09/26 13:35:43 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 13:35:50 visual_prompt]: Epoch 86 / 100: avg data time: 5.39e-02, avg batch time: 0.5040, average train loss: 0.0046
[09/26 13:35:51 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1697, average loss: 7.2911
[09/26 13:35:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 94.50	
[09/26 13:35:51 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 13:35:58 visual_prompt]: Epoch 87 / 100: avg data time: 5.39e-02, avg batch time: 0.5049, average train loss: 0.0027
[09/26 13:36:00 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1691, average loss: 7.4122
[09/26 13:36:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 95.00	
[09/26 13:36:00 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 13:36:07 visual_prompt]: Epoch 88 / 100: avg data time: 5.96e-02, avg batch time: 0.5089, average train loss: 0.0037
[09/26 13:36:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 7.3942
[09/26 13:36:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 95.50	
[09/26 13:36:08 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 13:36:15 visual_prompt]: Epoch 89 / 100: avg data time: 5.95e-02, avg batch time: 0.5097, average train loss: 0.0017
[09/26 13:36:17 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1691, average loss: 7.3539
[09/26 13:36:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 95.50	
[09/26 13:36:17 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 13:36:24 visual_prompt]: Epoch 90 / 100: avg data time: 5.85e-02, avg batch time: 0.5321, average train loss: 0.0027
[09/26 13:36:25 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1695, average loss: 7.3505
[09/26 13:36:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 95.50	
[09/26 13:36:25 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 13:36:32 visual_prompt]: Epoch 91 / 100: avg data time: 6.03e-02, avg batch time: 0.5094, average train loss: 0.0047
[09/26 13:36:34 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1695, average loss: 7.3119
[09/26 13:36:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 95.00	
[09/26 13:36:34 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 13:36:41 visual_prompt]: Epoch 92 / 100: avg data time: 5.95e-02, avg batch time: 0.5083, average train loss: 0.0029
[09/26 13:36:42 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1694, average loss: 7.2927
[09/26 13:36:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 95.00	
[09/26 13:36:42 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 13:36:49 visual_prompt]: Epoch 93 / 100: avg data time: 5.24e-02, avg batch time: 0.5021, average train loss: 0.0023
[09/26 13:36:51 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 7.3176
[09/26 13:36:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 95.50	
[09/26 13:36:51 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 13:36:58 visual_prompt]: Epoch 94 / 100: avg data time: 5.38e-02, avg batch time: 0.5029, average train loss: 0.0028
[09/26 13:36:59 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1696, average loss: 7.3276
[09/26 13:36:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 95.50	
[09/26 13:36:59 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 13:37:06 visual_prompt]: Epoch 95 / 100: avg data time: 5.86e-02, avg batch time: 0.5074, average train loss: 0.0021
[09/26 13:37:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 7.3345
[09/26 13:37:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.50	
[09/26 13:37:08 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 13:37:14 visual_prompt]: Epoch 96 / 100: avg data time: 4.81e-02, avg batch time: 0.4972, average train loss: 0.0026
[09/26 13:37:16 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1694, average loss: 7.3389
[09/26 13:37:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.50	
[09/26 13:37:16 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 13:37:23 visual_prompt]: Epoch 97 / 100: avg data time: 5.74e-02, avg batch time: 0.5064, average train loss: 0.0015
[09/26 13:37:24 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1689, average loss: 7.3414
[09/26 13:37:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.50	
[09/26 13:37:24 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 13:37:31 visual_prompt]: Epoch 98 / 100: avg data time: 5.66e-02, avg batch time: 0.5058, average train loss: 0.0042
[09/26 13:37:33 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 7.3390
[09/26 13:37:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.50	
[09/26 13:37:33 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 13:37:40 visual_prompt]: Epoch 99 / 100: avg data time: 6.45e-02, avg batch time: 0.5138, average train loss: 0.0041
[09/26 13:37:41 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1696, average loss: 7.3406
[09/26 13:37:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.50	
[09/26 13:37:41 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 13:37:48 visual_prompt]: Epoch 100 / 100: avg data time: 5.84e-02, avg batch time: 0.5086, average train loss: 0.0025
[09/26 13:37:50 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1694, average loss: 7.3416
[09/26 13:37:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.50	
[09/26 13:37:50 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:37:50 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:37:50 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:37:50 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:37:50 visual_prompt]: Training with config:
[09/26 13:37:50 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:37:50 visual_prompt]: Loading training data...
[09/26 13:37:50 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 13:37:51 visual_prompt]: Number of images: 800
[09/26 13:37:51 visual_prompt]: Number of classes: 6 / 6
[09/26 13:37:51 visual_prompt]: Loading validation data...
[09/26 13:37:51 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 13:37:51 visual_prompt]: Number of images: 200
[09/26 13:37:51 visual_prompt]: Number of classes: 6 / 6
[09/26 13:37:51 visual_prompt]: Constructing models...
[09/26 13:37:54 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 13:37:54 visual_prompt]: tuned percent:0.540
[09/26 13:37:54 visual_prompt]: Device used for model: 0
[09/26 13:37:54 visual_prompt]: Setting up Evaluator...
[09/26 13:37:54 visual_prompt]: Setting up Trainer...
[09/26 13:37:54 visual_prompt]: 	Setting up the optimizer...
[09/26 13:37:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:38:01 visual_prompt]: Epoch 1 / 100: avg data time: 6.00e-02, avg batch time: 0.5078, average train loss: 2.9842
[09/26 13:38:02 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1687, average loss: 2.9268
[09/26 13:38:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 13:38:02 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 13:38:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 13:38:09 visual_prompt]: Epoch 2 / 100: avg data time: 5.34e-02, avg batch time: 0.5010, average train loss: 2.7786
[09/26 13:38:11 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1687, average loss: 2.3113
[09/26 13:38:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 13:38:11 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 13:38:17 visual_prompt]: Epoch 3 / 100: avg data time: 4.80e-02, avg batch time: 0.4972, average train loss: 1.9542
[09/26 13:38:19 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 1.8614
[09/26 13:38:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 86.00	
[09/26 13:38:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 13:38:26 visual_prompt]: Epoch 4 / 100: avg data time: 6.03e-02, avg batch time: 0.5089, average train loss: 1.8303
[09/26 13:38:27 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 1.8315
[09/26 13:38:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 83.50	
[09/26 13:38:27 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 13:38:34 visual_prompt]: Epoch 5 / 100: avg data time: 5.89e-02, avg batch time: 0.5081, average train loss: 1.7849
[09/26 13:38:36 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1688, average loss: 1.7895
[09/26 13:38:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 84.00	
[09/26 13:38:36 visual_prompt]: Best epoch 5: best metric: 0.200
[09/26 13:38:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 13:38:43 visual_prompt]: Epoch 6 / 100: avg data time: 6.09e-02, avg batch time: 0.5091, average train loss: 1.7349
[09/26 13:38:44 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1688, average loss: 1.7697
[09/26 13:38:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 86.00	
[09/26 13:38:44 visual_prompt]: Best epoch 6: best metric: 0.215
[09/26 13:38:44 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 13:38:51 visual_prompt]: Epoch 7 / 100: avg data time: 4.77e-02, avg batch time: 0.4979, average train loss: 1.7736
[09/26 13:38:53 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1690, average loss: 1.8530
[09/26 13:38:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:38:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 13:38:59 visual_prompt]: Epoch 8 / 100: avg data time: 4.74e-02, avg batch time: 0.4976, average train loss: 1.7630
[09/26 13:39:01 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1689, average loss: 1.7507
[09/26 13:39:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 89.00	
[09/26 13:39:01 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 13:39:08 visual_prompt]: Epoch 9 / 100: avg data time: 6.07e-02, avg batch time: 0.5095, average train loss: 1.7941
[09/26 13:39:09 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1693, average loss: 1.8670
[09/26 13:39:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:39:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 13:39:16 visual_prompt]: Epoch 10 / 100: avg data time: 5.48e-02, avg batch time: 0.5028, average train loss: 1.7946
[09/26 13:39:18 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1691, average loss: 1.8414
[09/26 13:39:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 89.00	
[09/26 13:39:18 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 13:39:25 visual_prompt]: Epoch 11 / 100: avg data time: 5.67e-02, avg batch time: 0.5052, average train loss: 1.7191
[09/26 13:39:26 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1697, average loss: 1.8032
[09/26 13:39:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 85.00	
[09/26 13:39:26 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 13:39:33 visual_prompt]: Epoch 12 / 100: avg data time: 4.72e-02, avg batch time: 0.4965, average train loss: 1.7665
[09/26 13:39:34 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1693, average loss: 1.8555
[09/26 13:39:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:39:34 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 13:39:41 visual_prompt]: Epoch 13 / 100: avg data time: 5.76e-02, avg batch time: 0.5057, average train loss: 1.8184
[09/26 13:39:43 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 1.8699
[09/26 13:39:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:39:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 13:39:50 visual_prompt]: Epoch 14 / 100: avg data time: 5.18e-02, avg batch time: 0.5001, average train loss: 1.8134
[09/26 13:39:51 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1695, average loss: 1.8771
[09/26 13:39:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:39:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 13:39:58 visual_prompt]: Epoch 15 / 100: avg data time: 4.76e-02, avg batch time: 0.4979, average train loss: 1.7861
[09/26 13:39:59 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1690, average loss: 1.8031
[09/26 13:39:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 90.50	
[09/26 13:39:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 13:40:06 visual_prompt]: Epoch 16 / 100: avg data time: 4.97e-02, avg batch time: 0.4983, average train loss: 1.7549
[09/26 13:40:08 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 1.9332
[09/26 13:40:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.50	
[09/26 13:40:08 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 13:40:15 visual_prompt]: Epoch 17 / 100: avg data time: 4.27e-02, avg batch time: 0.4929, average train loss: 1.7822
[09/26 13:40:16 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1691, average loss: 1.8040
[09/26 13:40:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/26 13:40:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 13:40:23 visual_prompt]: Epoch 18 / 100: avg data time: 5.39e-02, avg batch time: 0.5034, average train loss: 1.7770
[09/26 13:40:25 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1695, average loss: 1.8070
[09/26 13:40:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 13:40:25 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 13:40:31 visual_prompt]: Epoch 19 / 100: avg data time: 5.56e-02, avg batch time: 0.5041, average train loss: 1.7922
[09/26 13:40:33 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 1.8358
[09/26 13:40:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:40:33 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 13:40:40 visual_prompt]: Epoch 20 / 100: avg data time: 5.02e-02, avg batch time: 0.4985, average train loss: 1.7807
[09/26 13:40:41 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1691, average loss: 1.8503
[09/26 13:40:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:40:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 13:40:48 visual_prompt]: Epoch 21 / 100: avg data time: 5.59e-02, avg batch time: 0.5045, average train loss: 1.7936
[09/26 13:40:50 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1688, average loss: 1.8432
[09/26 13:40:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 13:40:50 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 13:40:56 visual_prompt]: Epoch 22 / 100: avg data time: 4.35e-02, avg batch time: 0.4935, average train loss: 1.8148
[09/26 13:40:58 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1692, average loss: 1.8481
[09/26 13:40:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:40:58 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 13:41:05 visual_prompt]: Epoch 23 / 100: avg data time: 4.47e-02, avg batch time: 0.4929, average train loss: 1.7931
[09/26 13:41:06 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1695, average loss: 1.8185
[09/26 13:41:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 13:41:06 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 13:41:13 visual_prompt]: Epoch 24 / 100: avg data time: 5.74e-02, avg batch time: 0.5072, average train loss: 1.7864
[09/26 13:41:14 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1693, average loss: 1.8275
[09/26 13:41:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:41:14 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 13:41:21 visual_prompt]: Epoch 25 / 100: avg data time: 4.52e-02, avg batch time: 0.4963, average train loss: 1.7918
[09/26 13:41:23 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1695, average loss: 1.8253
[09/26 13:41:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 13:41:23 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 13:41:30 visual_prompt]: Epoch 26 / 100: avg data time: 5.97e-02, avg batch time: 0.5091, average train loss: 1.8003
[09/26 13:41:31 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1695, average loss: 1.9270
[09/26 13:41:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:41:31 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 13:41:38 visual_prompt]: Epoch 27 / 100: avg data time: 5.66e-02, avg batch time: 0.5054, average train loss: 1.8021
[09/26 13:41:40 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 1.8675
[09/26 13:41:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 85.00	
[09/26 13:41:40 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 13:41:46 visual_prompt]: Epoch 28 / 100: avg data time: 4.69e-02, avg batch time: 0.4970, average train loss: 1.7938
[09/26 13:41:48 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1691, average loss: 1.8399
[09/26 13:41:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:41:48 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 13:41:55 visual_prompt]: Epoch 29 / 100: avg data time: 5.50e-02, avg batch time: 0.5049, average train loss: 1.7768
[09/26 13:41:56 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1691, average loss: 1.8250
[09/26 13:41:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:41:56 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 13:42:03 visual_prompt]: Epoch 30 / 100: avg data time: 5.52e-02, avg batch time: 0.5041, average train loss: 1.8028
[09/26 13:42:05 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 1.8534
[09/26 13:42:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 13:42:05 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 13:42:11 visual_prompt]: Epoch 31 / 100: avg data time: 4.58e-02, avg batch time: 0.4961, average train loss: 1.7882
[09/26 13:42:13 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 1.8447
[09/26 13:42:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 84.00	
[09/26 13:42:13 visual_prompt]: Best epoch 31: best metric: 0.230
[09/26 13:42:13 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 13:42:20 visual_prompt]: Epoch 32 / 100: avg data time: 5.12e-02, avg batch time: 0.5010, average train loss: 1.7750
[09/26 13:42:21 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1692, average loss: 1.7933
[09/26 13:42:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:42:21 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 13:42:28 visual_prompt]: Epoch 33 / 100: avg data time: 4.33e-02, avg batch time: 0.4933, average train loss: 1.7631
[09/26 13:42:30 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 1.8211
[09/26 13:42:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:42:30 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 13:42:36 visual_prompt]: Epoch 34 / 100: avg data time: 5.92e-02, avg batch time: 0.5080, average train loss: 1.7869
[09/26 13:42:38 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1696, average loss: 1.8216
[09/26 13:42:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 84.00	
[09/26 13:42:38 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 13:42:45 visual_prompt]: Epoch 35 / 100: avg data time: 5.73e-02, avg batch time: 0.5056, average train loss: 1.7640
[09/26 13:42:46 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1692, average loss: 1.9164
[09/26 13:42:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 13:42:46 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 13:42:53 visual_prompt]: Epoch 36 / 100: avg data time: 4.68e-02, avg batch time: 0.4961, average train loss: 1.8079
[09/26 13:42:55 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1693, average loss: 1.8804
[09/26 13:42:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 83.50	
[09/26 13:42:55 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 13:43:02 visual_prompt]: Epoch 37 / 100: avg data time: 4.98e-02, avg batch time: 0.4994, average train loss: 1.7841
[09/26 13:43:03 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1691, average loss: 1.8654
[09/26 13:43:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:43:03 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 13:43:10 visual_prompt]: Epoch 38 / 100: avg data time: 5.77e-02, avg batch time: 0.5065, average train loss: 1.8216
[09/26 13:43:11 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1690, average loss: 1.8779
[09/26 13:43:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 13:43:11 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 13:43:18 visual_prompt]: Epoch 39 / 100: avg data time: 5.43e-02, avg batch time: 0.5037, average train loss: 1.8120
[09/26 13:43:20 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1687, average loss: 1.8183
[09/26 13:43:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 84.00	
[09/26 13:43:20 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 13:43:27 visual_prompt]: Epoch 40 / 100: avg data time: 5.89e-02, avg batch time: 0.5080, average train loss: 1.8439
[09/26 13:43:28 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1691, average loss: 1.8567
[09/26 13:43:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:43:28 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 13:43:35 visual_prompt]: Epoch 41 / 100: avg data time: 6.28e-02, avg batch time: 0.5109, average train loss: 1.7776
[09/26 13:43:37 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 1.8586
[09/26 13:43:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:43:37 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 13:43:44 visual_prompt]: Epoch 42 / 100: avg data time: 6.18e-02, avg batch time: 0.5104, average train loss: 1.7884
[09/26 13:43:45 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1688, average loss: 1.8285
[09/26 13:43:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:43:45 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 13:43:52 visual_prompt]: Epoch 43 / 100: avg data time: 6.18e-02, avg batch time: 0.5107, average train loss: 1.7816
[09/26 13:43:54 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1688, average loss: 1.9158
[09/26 13:43:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:43:54 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 13:44:00 visual_prompt]: Epoch 44 / 100: avg data time: 4.61e-02, avg batch time: 0.4958, average train loss: 1.8089
[09/26 13:44:02 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1690, average loss: 1.8071
[09/26 13:44:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:44:02 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 13:44:09 visual_prompt]: Epoch 45 / 100: avg data time: 5.48e-02, avg batch time: 0.5052, average train loss: 1.7760
[09/26 13:44:10 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 1.8244
[09/26 13:44:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 13:44:10 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 13:44:17 visual_prompt]: Epoch 46 / 100: avg data time: 4.46e-02, avg batch time: 0.4944, average train loss: 1.7713
[09/26 13:44:19 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1694, average loss: 1.8333
[09/26 13:44:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:44:19 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 13:44:26 visual_prompt]: Epoch 47 / 100: avg data time: 6.06e-02, avg batch time: 0.5094, average train loss: 1.7721
[09/26 13:44:27 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1689, average loss: 1.8169
[09/26 13:44:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:44:27 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 13:44:34 visual_prompt]: Epoch 48 / 100: avg data time: 5.48e-02, avg batch time: 0.5030, average train loss: 1.7696
[09/26 13:44:36 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1698, average loss: 1.8199
[09/26 13:44:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 84.00	
[09/26 13:44:36 visual_prompt]: Best epoch 48: best metric: 0.245
[09/26 13:44:36 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 13:44:42 visual_prompt]: Epoch 49 / 100: avg data time: 5.33e-02, avg batch time: 0.5019, average train loss: 1.7842
[09/26 13:44:44 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1693, average loss: 1.8285
[09/26 13:44:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:44:44 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 13:44:51 visual_prompt]: Epoch 50 / 100: avg data time: 6.03e-02, avg batch time: 0.5092, average train loss: 1.7778
[09/26 13:44:52 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1692, average loss: 1.7906
[09/26 13:44:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:44:52 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 13:44:59 visual_prompt]: Epoch 51 / 100: avg data time: 6.03e-02, avg batch time: 0.5114, average train loss: 1.7769
[09/26 13:45:01 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 1.8297
[09/26 13:45:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:45:01 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 13:45:08 visual_prompt]: Epoch 52 / 100: avg data time: 5.64e-02, avg batch time: 0.5055, average train loss: 1.7986
[09/26 13:45:09 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1691, average loss: 1.7949
[09/26 13:45:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:45:09 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 13:45:16 visual_prompt]: Epoch 53 / 100: avg data time: 5.47e-02, avg batch time: 0.5030, average train loss: 1.7718
[09/26 13:45:18 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 1.8148
[09/26 13:45:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:45:18 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 13:45:24 visual_prompt]: Epoch 54 / 100: avg data time: 5.59e-02, avg batch time: 0.5040, average train loss: 1.7817
[09/26 13:45:26 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1693, average loss: 1.8112
[09/26 13:45:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:45:26 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 13:45:33 visual_prompt]: Epoch 55 / 100: avg data time: 5.73e-02, avg batch time: 0.5051, average train loss: 1.7818
[09/26 13:45:34 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1688, average loss: 1.8106
[09/26 13:45:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:45:34 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 13:45:41 visual_prompt]: Epoch 56 / 100: avg data time: 5.94e-02, avg batch time: 0.5090, average train loss: 1.7995
[09/26 13:45:43 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 1.8061
[09/26 13:45:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:45:43 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 13:45:50 visual_prompt]: Epoch 57 / 100: avg data time: 5.57e-02, avg batch time: 0.5048, average train loss: 1.7879
[09/26 13:45:51 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1691, average loss: 1.8302
[09/26 13:45:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:45:51 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 13:45:58 visual_prompt]: Epoch 58 / 100: avg data time: 5.77e-02, avg batch time: 0.5076, average train loss: 1.7938
[09/26 13:46:00 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1689, average loss: 1.8163
[09/26 13:46:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:46:00 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 13:46:06 visual_prompt]: Epoch 59 / 100: avg data time: 4.57e-02, avg batch time: 0.4972, average train loss: 1.7687
[09/26 13:46:08 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1689, average loss: 1.8358
[09/26 13:46:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:46:08 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 13:46:15 visual_prompt]: Epoch 60 / 100: avg data time: 4.77e-02, avg batch time: 0.4977, average train loss: 1.7858
[09/26 13:46:16 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1693, average loss: 1.8441
[09/26 13:46:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:46:16 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 13:46:23 visual_prompt]: Epoch 61 / 100: avg data time: 4.55e-02, avg batch time: 0.4949, average train loss: 1.7747
[09/26 13:46:24 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1696, average loss: 1.8055
[09/26 13:46:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 84.00	
[09/26 13:46:24 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 13:46:31 visual_prompt]: Epoch 62 / 100: avg data time: 4.85e-02, avg batch time: 0.4992, average train loss: 1.7514
[09/26 13:46:33 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1695, average loss: 1.8607
[09/26 13:46:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:46:33 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 13:46:39 visual_prompt]: Epoch 63 / 100: avg data time: 4.88e-02, avg batch time: 0.4981, average train loss: 1.7421
[09/26 13:46:41 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 1.8040
[09/26 13:46:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:46:41 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 13:46:48 visual_prompt]: Epoch 64 / 100: avg data time: 4.93e-02, avg batch time: 0.4994, average train loss: 1.7708
[09/26 13:46:49 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1689, average loss: 1.8091
[09/26 13:46:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:46:49 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 13:46:56 visual_prompt]: Epoch 65 / 100: avg data time: 5.39e-02, avg batch time: 0.5034, average train loss: 1.7723
[09/26 13:46:58 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1691, average loss: 1.8013
[09/26 13:46:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 85.00	
[09/26 13:46:58 visual_prompt]: Best epoch 65: best metric: 0.250
[09/26 13:46:58 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 13:47:04 visual_prompt]: Epoch 66 / 100: avg data time: 6.18e-02, avg batch time: 0.5099, average train loss: 1.7543
[09/26 13:47:06 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1691, average loss: 1.7715
[09/26 13:47:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 84.00	
[09/26 13:47:06 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 13:47:13 visual_prompt]: Epoch 67 / 100: avg data time: 6.09e-02, avg batch time: 0.5101, average train loss: 1.7674
[09/26 13:47:15 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1695, average loss: 1.8106
[09/26 13:47:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:47:15 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 13:47:22 visual_prompt]: Epoch 68 / 100: avg data time: 5.92e-02, avg batch time: 0.5072, average train loss: 1.7820
[09/26 13:47:23 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1692, average loss: 1.8072
[09/26 13:47:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:47:23 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 13:47:30 visual_prompt]: Epoch 69 / 100: avg data time: 5.29e-02, avg batch time: 0.5011, average train loss: 1.7742
[09/26 13:47:31 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 1.8147
[09/26 13:47:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:47:31 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 13:47:38 visual_prompt]: Epoch 70 / 100: avg data time: 5.07e-02, avg batch time: 0.5012, average train loss: 1.7673
[09/26 13:47:40 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1688, average loss: 1.8052
[09/26 13:47:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:47:40 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 13:47:47 visual_prompt]: Epoch 71 / 100: avg data time: 6.07e-02, avg batch time: 0.5088, average train loss: 1.7685
[09/26 13:47:48 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1692, average loss: 1.8094
[09/26 13:47:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:47:48 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 13:47:55 visual_prompt]: Epoch 72 / 100: avg data time: 4.52e-02, avg batch time: 0.4975, average train loss: 1.7718
[09/26 13:47:56 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1689, average loss: 1.8238
[09/26 13:47:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:47:56 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 13:48:03 visual_prompt]: Epoch 73 / 100: avg data time: 5.69e-02, avg batch time: 0.5049, average train loss: 1.7698
[09/26 13:48:05 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1689, average loss: 1.8079
[09/26 13:48:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 13:48:05 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 13:48:12 visual_prompt]: Epoch 74 / 100: avg data time: 5.46e-02, avg batch time: 0.5030, average train loss: 1.7742
[09/26 13:48:13 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 1.8057
[09/26 13:48:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:48:13 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 13:48:20 visual_prompt]: Epoch 75 / 100: avg data time: 6.14e-02, avg batch time: 0.5099, average train loss: 1.7706
[09/26 13:48:21 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1687, average loss: 1.8116
[09/26 13:48:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:48:21 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 13:48:28 visual_prompt]: Epoch 76 / 100: avg data time: 5.87e-02, avg batch time: 0.5077, average train loss: 1.7690
[09/26 13:48:30 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1689, average loss: 1.8182
[09/26 13:48:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:48:30 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 13:48:37 visual_prompt]: Epoch 77 / 100: avg data time: 5.25e-02, avg batch time: 0.5015, average train loss: 1.7681
[09/26 13:48:38 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1691, average loss: 1.8117
[09/26 13:48:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:48:38 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 13:48:45 visual_prompt]: Epoch 78 / 100: avg data time: 6.20e-02, avg batch time: 0.5105, average train loss: 1.7657
[09/26 13:48:47 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 1.8085
[09/26 13:48:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:48:47 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 13:48:53 visual_prompt]: Epoch 79 / 100: avg data time: 4.50e-02, avg batch time: 0.4938, average train loss: 1.7507
[09/26 13:48:55 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1690, average loss: 1.7929
[09/26 13:48:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 84.00	
[09/26 13:48:55 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 13:49:02 visual_prompt]: Epoch 80 / 100: avg data time: 5.31e-02, avg batch time: 0.5017, average train loss: 1.7537
[09/26 13:49:04 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 1.7943
[09/26 13:49:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 84.00	
[09/26 13:49:04 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 13:49:11 visual_prompt]: Epoch 81 / 100: avg data time: 6.42e-02, avg batch time: 0.5124, average train loss: 1.7530
[09/26 13:49:12 visual_prompt]: Inference (val):avg data time: 4.77e-05, avg batch time: 0.1692, average loss: 1.7814
[09/26 13:49:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 84.00	
[09/26 13:49:12 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 13:49:19 visual_prompt]: Epoch 82 / 100: avg data time: 5.30e-02, avg batch time: 0.5008, average train loss: 1.7570
[09/26 13:49:21 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1694, average loss: 1.8197
[09/26 13:49:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:49:21 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 13:49:27 visual_prompt]: Epoch 83 / 100: avg data time: 5.84e-02, avg batch time: 0.5063, average train loss: 1.7687
[09/26 13:49:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1687, average loss: 1.8153
[09/26 13:49:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:49:29 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 13:49:36 visual_prompt]: Epoch 84 / 100: avg data time: 6.00e-02, avg batch time: 0.5098, average train loss: 1.7667
[09/26 13:49:37 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1692, average loss: 1.8013
[09/26 13:49:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 84.00	
[09/26 13:49:37 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 13:49:44 visual_prompt]: Epoch 85 / 100: avg data time: 5.02e-02, avg batch time: 0.4997, average train loss: 1.7673
[09/26 13:49:46 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 1.8158
[09/26 13:49:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:49:46 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 13:49:53 visual_prompt]: Epoch 86 / 100: avg data time: 5.82e-02, avg batch time: 0.5058, average train loss: 1.7638
[09/26 13:49:54 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1689, average loss: 1.8091
[09/26 13:49:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:49:54 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 13:50:01 visual_prompt]: Epoch 87 / 100: avg data time: 5.26e-02, avg batch time: 0.5007, average train loss: 1.7644
[09/26 13:50:02 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1690, average loss: 1.8134
[09/26 13:50:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:50:02 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 13:50:09 visual_prompt]: Epoch 88 / 100: avg data time: 5.02e-02, avg batch time: 0.4996, average train loss: 1.7621
[09/26 13:50:11 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1695, average loss: 1.8134
[09/26 13:50:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:50:11 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 13:50:18 visual_prompt]: Epoch 89 / 100: avg data time: 6.00e-02, avg batch time: 0.5078, average train loss: 1.7550
[09/26 13:50:19 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 1.7982
[09/26 13:50:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 13:50:19 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 13:50:26 visual_prompt]: Epoch 90 / 100: avg data time: 5.18e-02, avg batch time: 0.5000, average train loss: 1.7277
[09/26 13:50:28 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 1.7594
[09/26 13:50:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 84.00	
[09/26 13:50:28 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 13:50:34 visual_prompt]: Epoch 91 / 100: avg data time: 5.08e-02, avg batch time: 0.5011, average train loss: 1.7005
[09/26 13:50:36 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 1.7474
[09/26 13:50:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 84.00	
[09/26 13:50:36 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 13:50:43 visual_prompt]: Epoch 92 / 100: avg data time: 4.92e-02, avg batch time: 0.4978, average train loss: 1.5865
[09/26 13:50:44 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1691, average loss: 1.7221
[09/26 13:50:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 93.50	
[09/26 13:50:44 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 13:50:51 visual_prompt]: Epoch 93 / 100: avg data time: 6.63e-02, avg batch time: 0.5148, average train loss: 1.5037
[09/26 13:50:53 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1692, average loss: 1.6259
[09/26 13:50:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 92.50	
[09/26 13:50:53 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 13:51:00 visual_prompt]: Epoch 94 / 100: avg data time: 5.47e-02, avg batch time: 0.5042, average train loss: 1.4446
[09/26 13:51:01 visual_prompt]: Inference (val):avg data time: 4.68e-05, avg batch time: 0.1694, average loss: 1.5668
[09/26 13:51:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 93.50	
[09/26 13:51:01 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 13:51:08 visual_prompt]: Epoch 95 / 100: avg data time: 5.57e-02, avg batch time: 0.5038, average train loss: 1.4498
[09/26 13:51:10 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1694, average loss: 1.4433
[09/26 13:51:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 97.00	
[09/26 13:51:10 visual_prompt]: Best epoch 95: best metric: 0.320
[09/26 13:51:10 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 13:51:16 visual_prompt]: Epoch 96 / 100: avg data time: 5.62e-02, avg batch time: 0.5061, average train loss: 1.3486
[09/26 13:51:18 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1694, average loss: 1.4981
[09/26 13:51:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 96.50	
[09/26 13:51:18 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 13:51:25 visual_prompt]: Epoch 97 / 100: avg data time: 5.95e-02, avg batch time: 0.5090, average train loss: 1.3176
[09/26 13:51:26 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1694, average loss: 1.4428
[09/26 13:51:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 98.00	
[09/26 13:51:26 visual_prompt]: Best epoch 97: best metric: 0.330
[09/26 13:51:26 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 13:51:33 visual_prompt]: Epoch 98 / 100: avg data time: 5.79e-02, avg batch time: 0.5062, average train loss: 1.3118
[09/26 13:51:35 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1690, average loss: 1.4794
[09/26 13:51:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 97.00	
[09/26 13:51:35 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 13:51:42 visual_prompt]: Epoch 99 / 100: avg data time: 5.79e-02, avg batch time: 0.5081, average train loss: 1.2880
[09/26 13:51:43 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1696, average loss: 1.4451
[09/26 13:51:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 98.50	
[09/26 13:51:43 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 13:51:50 visual_prompt]: Epoch 100 / 100: avg data time: 5.20e-02, avg batch time: 0.5007, average train loss: 1.2877
[09/26 13:51:52 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1694, average loss: 1.4473
[09/26 13:51:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 98.00	
[09/26 13:51:52 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:51:52 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:51:52 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:51:52 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:51:52 visual_prompt]: Training with config:
[09/26 13:51:52 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:51:52 visual_prompt]: Loading training data...
[09/26 13:51:52 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 13:51:53 visual_prompt]: Number of images: 800
[09/26 13:51:53 visual_prompt]: Number of classes: 6 / 6
[09/26 13:51:53 visual_prompt]: Loading validation data...
[09/26 13:51:53 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 13:51:53 visual_prompt]: Number of images: 200
[09/26 13:51:53 visual_prompt]: Number of classes: 6 / 6
[09/26 13:51:53 visual_prompt]: Constructing models...
[09/26 13:51:56 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 13:51:56 visual_prompt]: tuned percent:0.540
[09/26 13:51:56 visual_prompt]: Device used for model: 0
[09/26 13:51:56 visual_prompt]: Setting up Evaluator...
[09/26 13:51:56 visual_prompt]: Setting up Trainer...
[09/26 13:51:56 visual_prompt]: 	Setting up the optimizer...
[09/26 13:51:56 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:52:03 visual_prompt]: Epoch 1 / 100: avg data time: 4.69e-02, avg batch time: 0.4953, average train loss: 2.9838
[09/26 13:52:04 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1685, average loss: 2.9268
[09/26 13:52:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 13:52:04 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 13:52:04 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 13:52:11 visual_prompt]: Epoch 2 / 100: avg data time: 5.96e-02, avg batch time: 0.5068, average train loss: 2.7888
[09/26 13:52:13 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1688, average loss: 2.2608
[09/26 13:52:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 13:52:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 13:52:20 visual_prompt]: Epoch 3 / 100: avg data time: 5.95e-02, avg batch time: 0.5067, average train loss: 1.9275
[09/26 13:52:21 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1691, average loss: 1.8738
[09/26 13:52:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 13.50	top5: 82.50	
[09/26 13:52:21 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 13:52:28 visual_prompt]: Epoch 4 / 100: avg data time: 5.42e-02, avg batch time: 0.5026, average train loss: 1.8658
[09/26 13:52:30 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1688, average loss: 1.8641
[09/26 13:52:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 13:52:30 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 13:52:36 visual_prompt]: Epoch 5 / 100: avg data time: 5.67e-02, avg batch time: 0.5053, average train loss: 1.7853
[09/26 13:52:38 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1690, average loss: 1.8030
[09/26 13:52:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 88.50	
[09/26 13:52:38 visual_prompt]: Best epoch 5: best metric: 0.245
[09/26 13:52:38 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 13:52:45 visual_prompt]: Epoch 6 / 100: avg data time: 5.56e-02, avg batch time: 0.5040, average train loss: 1.7491
[09/26 13:52:46 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1692, average loss: 1.7781
[09/26 13:52:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 84.50	
[09/26 13:52:46 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 13:52:53 visual_prompt]: Epoch 7 / 100: avg data time: 5.02e-02, avg batch time: 0.5001, average train loss: 1.6797
[09/26 13:52:55 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 1.9521
[09/26 13:52:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 86.00	
[09/26 13:52:55 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 13:53:01 visual_prompt]: Epoch 8 / 100: avg data time: 4.75e-02, avg batch time: 0.4977, average train loss: 1.6871
[09/26 13:53:03 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1695, average loss: 1.7480
[09/26 13:53:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 87.00	
[09/26 13:53:03 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 13:53:10 visual_prompt]: Epoch 9 / 100: avg data time: 5.86e-02, avg batch time: 0.5075, average train loss: 1.6035
[09/26 13:53:11 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1691, average loss: 1.6182
[09/26 13:53:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 92.00	
[09/26 13:53:11 visual_prompt]: Best epoch 9: best metric: 0.265
[09/26 13:53:11 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 13:53:18 visual_prompt]: Epoch 10 / 100: avg data time: 5.83e-02, avg batch time: 0.5072, average train loss: 1.5034
[09/26 13:53:20 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 1.6136
[09/26 13:53:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 94.00	
[09/26 13:53:20 visual_prompt]: Best epoch 10: best metric: 0.315
[09/26 13:53:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 13:53:27 visual_prompt]: Epoch 11 / 100: avg data time: 5.25e-02, avg batch time: 0.5029, average train loss: 1.5486
[09/26 13:53:28 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1694, average loss: 1.6089
[09/26 13:53:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 95.00	
[09/26 13:53:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 13:53:35 visual_prompt]: Epoch 12 / 100: avg data time: 5.64e-02, avg batch time: 0.5048, average train loss: 1.4569
[09/26 13:53:37 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 1.5482
[09/26 13:53:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 96.50	
[09/26 13:53:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 13:53:43 visual_prompt]: Epoch 13 / 100: avg data time: 4.99e-02, avg batch time: 0.4999, average train loss: 1.3771
[09/26 13:53:45 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1695, average loss: 1.5061
[09/26 13:53:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 97.50	
[09/26 13:53:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 13:53:52 visual_prompt]: Epoch 14 / 100: avg data time: 4.54e-02, avg batch time: 0.4965, average train loss: 1.3775
[09/26 13:53:53 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 1.4108
[09/26 13:53:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 95.50	
[09/26 13:53:53 visual_prompt]: Best epoch 14: best metric: 0.355
[09/26 13:53:53 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 13:54:00 visual_prompt]: Epoch 15 / 100: avg data time: 5.80e-02, avg batch time: 0.5082, average train loss: 1.3398
[09/26 13:54:02 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1691, average loss: 1.6341
[09/26 13:54:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 97.00	
[09/26 13:54:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 13:54:08 visual_prompt]: Epoch 16 / 100: avg data time: 4.13e-02, avg batch time: 0.4912, average train loss: 1.2926
[09/26 13:54:10 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1690, average loss: 1.5142
[09/26 13:54:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.50	
[09/26 13:54:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 13:54:17 visual_prompt]: Epoch 17 / 100: avg data time: 5.49e-02, avg batch time: 0.5043, average train loss: 1.3316
[09/26 13:54:18 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1698, average loss: 1.5496
[09/26 13:54:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 13:54:18 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 13:54:25 visual_prompt]: Epoch 18 / 100: avg data time: 4.38e-02, avg batch time: 0.4968, average train loss: 1.2103
[09/26 13:54:27 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 1.5074
[09/26 13:54:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.00	
[09/26 13:54:27 visual_prompt]: Best epoch 18: best metric: 0.360
[09/26 13:54:27 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 13:54:33 visual_prompt]: Epoch 19 / 100: avg data time: 4.22e-02, avg batch time: 0.4937, average train loss: 1.1630
[09/26 13:54:35 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1692, average loss: 1.5399
[09/26 13:54:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 13:54:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 13:54:42 visual_prompt]: Epoch 20 / 100: avg data time: 5.37e-02, avg batch time: 0.5031, average train loss: 1.1534
[09/26 13:54:43 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1694, average loss: 1.4645
[09/26 13:54:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 13:54:43 visual_prompt]: Best epoch 20: best metric: 0.375
[09/26 13:54:43 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 13:54:50 visual_prompt]: Epoch 21 / 100: avg data time: 4.92e-02, avg batch time: 0.5002, average train loss: 1.1186
[09/26 13:54:52 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1697, average loss: 1.4304
[09/26 13:54:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.50	
[09/26 13:54:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 13:54:59 visual_prompt]: Epoch 22 / 100: avg data time: 6.08e-02, avg batch time: 0.5106, average train loss: 1.1242
[09/26 13:55:00 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1694, average loss: 1.5020
[09/26 13:55:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 13:55:00 visual_prompt]: Best epoch 22: best metric: 0.380
[09/26 13:55:00 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 13:55:07 visual_prompt]: Epoch 23 / 100: avg data time: 4.77e-02, avg batch time: 0.4975, average train loss: 1.0611
[09/26 13:55:08 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1695, average loss: 1.5615
[09/26 13:55:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 13:55:08 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 13:55:15 visual_prompt]: Epoch 24 / 100: avg data time: 5.07e-02, avg batch time: 0.5001, average train loss: 1.0366
[09/26 13:55:17 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1699, average loss: 1.4943
[09/26 13:55:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 96.50	
[09/26 13:55:17 visual_prompt]: Best epoch 24: best metric: 0.440
[09/26 13:55:17 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 13:55:24 visual_prompt]: Epoch 25 / 100: avg data time: 5.81e-02, avg batch time: 0.5075, average train loss: 0.9770
[09/26 13:55:25 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1698, average loss: 1.6809
[09/26 13:55:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.50	
[09/26 13:55:25 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 13:55:32 visual_prompt]: Epoch 26 / 100: avg data time: 5.67e-02, avg batch time: 0.5056, average train loss: 1.0409
[09/26 13:55:34 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.1696, average loss: 2.0995
[09/26 13:55:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 92.00	
[09/26 13:55:34 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 13:55:41 visual_prompt]: Epoch 27 / 100: avg data time: 5.78e-02, avg batch time: 0.5068, average train loss: 1.0936
[09/26 13:55:42 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1697, average loss: 1.3415
[09/26 13:55:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 98.00	
[09/26 13:55:42 visual_prompt]: Best epoch 27: best metric: 0.450
[09/26 13:55:42 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 13:55:49 visual_prompt]: Epoch 28 / 100: avg data time: 4.38e-02, avg batch time: 0.4956, average train loss: 0.9752
[09/26 13:55:50 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 1.4976
[09/26 13:55:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 96.50	
[09/26 13:55:50 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 13:55:57 visual_prompt]: Epoch 29 / 100: avg data time: 5.22e-02, avg batch time: 0.5022, average train loss: 0.9254
[09/26 13:55:59 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1697, average loss: 1.4493
[09/26 13:55:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.00	
[09/26 13:55:59 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 13:56:06 visual_prompt]: Epoch 30 / 100: avg data time: 4.59e-02, avg batch time: 0.4986, average train loss: 0.9252
[09/26 13:56:07 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1694, average loss: 1.8436
[09/26 13:56:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 94.50	
[09/26 13:56:07 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 13:56:14 visual_prompt]: Epoch 31 / 100: avg data time: 6.00e-02, avg batch time: 0.5088, average train loss: 1.0536
[09/26 13:56:16 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1697, average loss: 1.4491
[09/26 13:56:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.50	
[09/26 13:56:16 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 13:56:22 visual_prompt]: Epoch 32 / 100: avg data time: 5.70e-02, avg batch time: 0.5064, average train loss: 0.8455
[09/26 13:56:24 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1699, average loss: 1.7745
[09/26 13:56:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 13:56:24 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 13:56:31 visual_prompt]: Epoch 33 / 100: avg data time: 5.80e-02, avg batch time: 0.5085, average train loss: 0.8029
[09/26 13:56:32 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1696, average loss: 1.6822
[09/26 13:56:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 13:56:32 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 13:56:39 visual_prompt]: Epoch 34 / 100: avg data time: 4.91e-02, avg batch time: 0.4989, average train loss: 0.7165
[09/26 13:56:41 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1696, average loss: 1.5431
[09/26 13:56:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.00	top5: 97.50	
[09/26 13:56:41 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 13:56:48 visual_prompt]: Epoch 35 / 100: avg data time: 4.73e-02, avg batch time: 0.4992, average train loss: 0.7562
[09/26 13:56:49 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 1.6831
[09/26 13:56:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.50	
[09/26 13:56:49 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 13:56:56 visual_prompt]: Epoch 36 / 100: avg data time: 5.23e-02, avg batch time: 0.5039, average train loss: 0.7269
[09/26 13:56:58 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1695, average loss: 1.6740
[09/26 13:56:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.50	
[09/26 13:56:58 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 13:57:04 visual_prompt]: Epoch 37 / 100: avg data time: 4.61e-02, avg batch time: 0.4961, average train loss: 0.6018
[09/26 13:57:06 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1697, average loss: 1.8128
[09/26 13:57:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.00	
[09/26 13:57:06 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 13:57:13 visual_prompt]: Epoch 38 / 100: avg data time: 4.37e-02, avg batch time: 0.4949, average train loss: 0.5509
[09/26 13:57:14 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 1.9768
[09/26 13:57:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 13:57:14 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 13:57:21 visual_prompt]: Epoch 39 / 100: avg data time: 5.99e-02, avg batch time: 0.5117, average train loss: 0.4942
[09/26 13:57:23 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1697, average loss: 2.6827
[09/26 13:57:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 93.50	
[09/26 13:57:23 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 13:57:30 visual_prompt]: Epoch 40 / 100: avg data time: 5.99e-02, avg batch time: 0.5097, average train loss: 0.7915
[09/26 13:57:31 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 1.7889
[09/26 13:57:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 13:57:31 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 13:57:38 visual_prompt]: Epoch 41 / 100: avg data time: 6.68e-02, avg batch time: 0.5158, average train loss: 0.6050
[09/26 13:57:40 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1696, average loss: 1.5938
[09/26 13:57:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 98.00	
[09/26 13:57:40 visual_prompt]: Best epoch 41: best metric: 0.455
[09/26 13:57:40 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 13:57:46 visual_prompt]: Epoch 42 / 100: avg data time: 5.39e-02, avg batch time: 0.5040, average train loss: 0.5154
[09/26 13:57:48 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 1.9261
[09/26 13:57:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 99.00	
[09/26 13:57:48 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 13:57:55 visual_prompt]: Epoch 43 / 100: avg data time: 5.19e-02, avg batch time: 0.5032, average train loss: 0.5740
[09/26 13:57:56 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1690, average loss: 2.0790
[09/26 13:57:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 97.00	
[09/26 13:57:56 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 13:58:03 visual_prompt]: Epoch 44 / 100: avg data time: 5.47e-02, avg batch time: 0.5049, average train loss: 0.4605
[09/26 13:58:05 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1698, average loss: 2.0941
[09/26 13:58:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.50	
[09/26 13:58:05 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 13:58:12 visual_prompt]: Epoch 45 / 100: avg data time: 5.96e-02, avg batch time: 0.5085, average train loss: 0.4436
[09/26 13:58:13 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 1.9994
[09/26 13:58:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.50	
[09/26 13:58:13 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 13:58:20 visual_prompt]: Epoch 46 / 100: avg data time: 5.51e-02, avg batch time: 0.5050, average train loss: 0.4330
[09/26 13:58:22 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1696, average loss: 2.3386
[09/26 13:58:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/26 13:58:22 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 13:58:28 visual_prompt]: Epoch 47 / 100: avg data time: 4.78e-02, avg batch time: 0.4974, average train loss: 0.3391
[09/26 13:58:30 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1695, average loss: 2.4057
[09/26 13:58:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.00	
[09/26 13:58:30 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 13:58:37 visual_prompt]: Epoch 48 / 100: avg data time: 5.42e-02, avg batch time: 0.5040, average train loss: 0.3244
[09/26 13:58:38 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1695, average loss: 2.7784
[09/26 13:58:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 13:58:38 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 13:58:45 visual_prompt]: Epoch 49 / 100: avg data time: 5.29e-02, avg batch time: 0.5030, average train loss: 0.3132
[09/26 13:58:47 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 2.5917
[09/26 13:58:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 100.00	
[09/26 13:58:47 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 13:58:54 visual_prompt]: Epoch 50 / 100: avg data time: 6.07e-02, avg batch time: 0.5108, average train loss: 0.3693
[09/26 13:58:55 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1694, average loss: 2.2816
[09/26 13:58:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.50	
[09/26 13:58:55 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 13:59:02 visual_prompt]: Epoch 51 / 100: avg data time: 4.73e-02, avg batch time: 0.4977, average train loss: 0.2748
[09/26 13:59:03 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1700, average loss: 2.7913
[09/26 13:59:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.50	
[09/26 13:59:03 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 13:59:10 visual_prompt]: Epoch 52 / 100: avg data time: 5.94e-02, avg batch time: 0.5087, average train loss: 0.2041
[09/26 13:59:12 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 3.4044
[09/26 13:59:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.00	
[09/26 13:59:12 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 13:59:19 visual_prompt]: Epoch 53 / 100: avg data time: 5.06e-02, avg batch time: 0.5003, average train loss: 0.2596
[09/26 13:59:20 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1695, average loss: 3.1163
[09/26 13:59:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 13:59:20 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 13:59:27 visual_prompt]: Epoch 54 / 100: avg data time: 5.23e-02, avg batch time: 0.5027, average train loss: 0.3687
[09/26 13:59:29 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 2.5381
[09/26 13:59:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 13:59:29 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 13:59:36 visual_prompt]: Epoch 55 / 100: avg data time: 5.85e-02, avg batch time: 0.5086, average train loss: 0.2503
[09/26 13:59:37 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1698, average loss: 2.5906
[09/26 13:59:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 99.00	
[09/26 13:59:37 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 13:59:44 visual_prompt]: Epoch 56 / 100: avg data time: 5.26e-02, avg batch time: 0.5026, average train loss: 0.2532
[09/26 13:59:46 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1692, average loss: 2.5783
[09/26 13:59:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 13:59:46 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 13:59:52 visual_prompt]: Epoch 57 / 100: avg data time: 5.68e-02, avg batch time: 0.5057, average train loss: 0.1873
[09/26 13:59:54 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1697, average loss: 3.1953
[09/26 13:59:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 13:59:54 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 14:00:01 visual_prompt]: Epoch 58 / 100: avg data time: 5.52e-02, avg batch time: 0.5041, average train loss: 0.3175
[09/26 14:00:02 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1698, average loss: 2.7270
[09/26 14:00:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 14:00:02 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 14:00:09 visual_prompt]: Epoch 59 / 100: avg data time: 6.46e-02, avg batch time: 0.5135, average train loss: 0.2560
[09/26 14:00:11 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1692, average loss: 2.5211
[09/26 14:00:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.00	
[09/26 14:00:11 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 14:00:18 visual_prompt]: Epoch 60 / 100: avg data time: 6.39e-02, avg batch time: 0.5130, average train loss: 0.5331
[09/26 14:00:19 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1697, average loss: 2.3124
[09/26 14:00:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 14:00:19 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 14:00:26 visual_prompt]: Epoch 61 / 100: avg data time: 5.91e-02, avg batch time: 0.5093, average train loss: 0.2601
[09/26 14:00:28 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1698, average loss: 2.8932
[09/26 14:00:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 14:00:28 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 14:00:35 visual_prompt]: Epoch 62 / 100: avg data time: 5.65e-02, avg batch time: 0.5060, average train loss: 0.1723
[09/26 14:00:36 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 3.0722
[09/26 14:00:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 14:00:36 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 14:00:43 visual_prompt]: Epoch 63 / 100: avg data time: 6.28e-02, avg batch time: 0.5134, average train loss: 0.0849
[09/26 14:00:45 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1690, average loss: 3.5688
[09/26 14:00:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 14:00:45 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 14:00:52 visual_prompt]: Epoch 64 / 100: avg data time: 5.42e-02, avg batch time: 0.5051, average train loss: 0.0665
[09/26 14:00:53 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 3.6625
[09/26 14:00:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 14:00:53 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 14:01:00 visual_prompt]: Epoch 65 / 100: avg data time: 5.05e-02, avg batch time: 0.5011, average train loss: 0.0677
[09/26 14:01:02 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1696, average loss: 3.7105
[09/26 14:01:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 95.50	
[09/26 14:01:02 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 14:01:08 visual_prompt]: Epoch 66 / 100: avg data time: 5.61e-02, avg batch time: 0.5051, average train loss: 0.0513
[09/26 14:01:10 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 3.6811
[09/26 14:01:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 95.50	
[09/26 14:01:10 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 14:01:17 visual_prompt]: Epoch 67 / 100: avg data time: 6.35e-02, avg batch time: 0.5128, average train loss: 0.0484
[09/26 14:01:19 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1695, average loss: 3.6094
[09/26 14:01:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.00	
[09/26 14:01:19 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 14:01:25 visual_prompt]: Epoch 68 / 100: avg data time: 5.26e-02, avg batch time: 0.5024, average train loss: 0.0453
[09/26 14:01:27 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1699, average loss: 3.7173
[09/26 14:01:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 14:01:27 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 14:01:34 visual_prompt]: Epoch 69 / 100: avg data time: 5.99e-02, avg batch time: 0.5089, average train loss: 0.0450
[09/26 14:01:35 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1698, average loss: 3.8176
[09/26 14:01:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.00	
[09/26 14:01:35 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 14:01:42 visual_prompt]: Epoch 70 / 100: avg data time: 5.23e-02, avg batch time: 0.5031, average train loss: 0.0347
[09/26 14:01:44 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1702, average loss: 3.6461
[09/26 14:01:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.50	
[09/26 14:01:44 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 14:01:51 visual_prompt]: Epoch 71 / 100: avg data time: 5.85e-02, avg batch time: 0.5079, average train loss: 0.0125
[09/26 14:01:52 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1696, average loss: 3.8046
[09/26 14:01:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.00	
[09/26 14:01:52 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 14:01:59 visual_prompt]: Epoch 72 / 100: avg data time: 5.92e-02, avg batch time: 0.5101, average train loss: 0.0166
[09/26 14:02:01 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1696, average loss: 3.7743
[09/26 14:02:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.50	
[09/26 14:02:01 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 14:02:07 visual_prompt]: Epoch 73 / 100: avg data time: 5.97e-02, avg batch time: 0.5089, average train loss: 0.0085
[09/26 14:02:09 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1699, average loss: 3.7204
[09/26 14:02:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 14:02:09 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 14:02:16 visual_prompt]: Epoch 74 / 100: avg data time: 5.46e-02, avg batch time: 0.5035, average train loss: 0.0054
[09/26 14:02:17 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1695, average loss: 3.7280
[09/26 14:02:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 14:02:17 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 14:02:24 visual_prompt]: Epoch 75 / 100: avg data time: 6.17e-02, avg batch time: 0.5124, average train loss: 0.0051
[09/26 14:02:26 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 3.7807
[09/26 14:02:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.50	
[09/26 14:02:26 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 14:02:33 visual_prompt]: Epoch 76 / 100: avg data time: 5.71e-02, avg batch time: 0.5072, average train loss: 0.0040
[09/26 14:02:34 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1694, average loss: 3.8075
[09/26 14:02:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.50	
[09/26 14:02:34 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 14:02:41 visual_prompt]: Epoch 77 / 100: avg data time: 5.07e-02, avg batch time: 0.5004, average train loss: 0.0033
[09/26 14:02:42 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 3.8170
[09/26 14:02:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.50	
[09/26 14:02:42 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 14:02:49 visual_prompt]: Epoch 78 / 100: avg data time: 4.62e-02, avg batch time: 0.4979, average train loss: 0.0029
[09/26 14:02:51 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1696, average loss: 3.8344
[09/26 14:02:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.50	
[09/26 14:02:51 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 14:02:58 visual_prompt]: Epoch 79 / 100: avg data time: 6.24e-02, avg batch time: 0.5129, average train loss: 0.0034
[09/26 14:02:59 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1695, average loss: 3.8894
[09/26 14:02:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 14:02:59 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 14:03:06 visual_prompt]: Epoch 80 / 100: avg data time: 5.14e-02, avg batch time: 0.5006, average train loss: 0.0029
[09/26 14:03:08 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1698, average loss: 3.9039
[09/26 14:03:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 14:03:08 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 14:03:15 visual_prompt]: Epoch 81 / 100: avg data time: 5.73e-02, avg batch time: 0.5077, average train loss: 0.0028
[09/26 14:03:16 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1700, average loss: 3.9025
[09/26 14:03:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.00	
[09/26 14:03:16 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 14:03:23 visual_prompt]: Epoch 82 / 100: avg data time: 4.39e-02, avg batch time: 0.4931, average train loss: 0.0028
[09/26 14:03:24 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1697, average loss: 3.8812
[09/26 14:03:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.00	
[09/26 14:03:24 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 14:03:31 visual_prompt]: Epoch 83 / 100: avg data time: 6.17e-02, avg batch time: 0.5112, average train loss: 0.0025
[09/26 14:03:33 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1692, average loss: 3.8765
[09/26 14:03:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.00	
[09/26 14:03:33 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 14:03:40 visual_prompt]: Epoch 84 / 100: avg data time: 6.53e-02, avg batch time: 0.5156, average train loss: 0.0025
[09/26 14:03:41 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1699, average loss: 3.8928
[09/26 14:03:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 14:03:41 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 14:03:48 visual_prompt]: Epoch 85 / 100: avg data time: 4.68e-02, avg batch time: 0.4975, average train loss: 0.0032
[09/26 14:03:50 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1699, average loss: 3.8924
[09/26 14:03:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.00	
[09/26 14:03:50 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 14:03:56 visual_prompt]: Epoch 86 / 100: avg data time: 5.92e-02, avg batch time: 0.5103, average train loss: 0.0038
[09/26 14:03:58 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1697, average loss: 3.9499
[09/26 14:03:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 14:03:58 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 14:04:05 visual_prompt]: Epoch 87 / 100: avg data time: 6.27e-02, avg batch time: 0.5115, average train loss: 0.0030
[09/26 14:04:06 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1698, average loss: 3.9277
[09/26 14:04:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 14:04:06 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 14:04:13 visual_prompt]: Epoch 88 / 100: avg data time: 5.41e-02, avg batch time: 0.5037, average train loss: 0.0025
[09/26 14:04:15 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1697, average loss: 3.9047
[09/26 14:04:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 14:04:15 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 14:04:22 visual_prompt]: Epoch 89 / 100: avg data time: 6.09e-02, avg batch time: 0.5122, average train loss: 0.0024
[09/26 14:04:23 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1697, average loss: 3.8932
[09/26 14:04:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 14:04:23 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 14:04:30 visual_prompt]: Epoch 90 / 100: avg data time: 4.75e-02, avg batch time: 0.4967, average train loss: 0.0026
[09/26 14:04:32 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1696, average loss: 3.8842
[09/26 14:04:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.00	
[09/26 14:04:32 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 14:04:39 visual_prompt]: Epoch 91 / 100: avg data time: 6.24e-02, avg batch time: 0.5133, average train loss: 0.0025
[09/26 14:04:40 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1694, average loss: 3.8792
[09/26 14:04:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.00	
[09/26 14:04:40 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 14:04:47 visual_prompt]: Epoch 92 / 100: avg data time: 5.71e-02, avg batch time: 0.5075, average train loss: 0.0025
[09/26 14:04:48 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1694, average loss: 3.8766
[09/26 14:04:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.00	
[09/26 14:04:48 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 14:04:55 visual_prompt]: Epoch 93 / 100: avg data time: 5.53e-02, avg batch time: 0.5045, average train loss: 0.0025
[09/26 14:04:57 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1696, average loss: 3.8744
[09/26 14:04:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.00	
[09/26 14:04:57 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 14:05:04 visual_prompt]: Epoch 94 / 100: avg data time: 5.30e-02, avg batch time: 0.5027, average train loss: 0.0023
[09/26 14:05:05 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1697, average loss: 3.8733
[09/26 14:05:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.00	
[09/26 14:05:05 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 14:05:12 visual_prompt]: Epoch 95 / 100: avg data time: 5.76e-02, avg batch time: 0.5066, average train loss: 0.0030
[09/26 14:05:14 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 3.8720
[09/26 14:05:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.00	
[09/26 14:05:14 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 14:05:20 visual_prompt]: Epoch 96 / 100: avg data time: 4.50e-02, avg batch time: 0.4964, average train loss: 0.0023
[09/26 14:05:22 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1697, average loss: 3.8698
[09/26 14:05:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.00	
[09/26 14:05:22 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 14:05:29 visual_prompt]: Epoch 97 / 100: avg data time: 4.93e-02, avg batch time: 0.4999, average train loss: 0.0026
[09/26 14:05:30 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.1698, average loss: 3.8694
[09/26 14:05:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.00	
[09/26 14:05:30 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 14:05:37 visual_prompt]: Epoch 98 / 100: avg data time: 5.54e-02, avg batch time: 0.5045, average train loss: 0.0023
[09/26 14:05:39 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1695, average loss: 3.8691
[09/26 14:05:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.00	
[09/26 14:05:39 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 14:05:46 visual_prompt]: Epoch 99 / 100: avg data time: 5.72e-02, avg batch time: 0.5066, average train loss: 0.0024
[09/26 14:05:47 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1693, average loss: 3.8691
[09/26 14:05:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.00	
[09/26 14:05:47 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 14:05:54 visual_prompt]: Epoch 100 / 100: avg data time: 5.71e-02, avg batch time: 0.5066, average train loss: 0.0023
[09/26 14:05:56 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1699, average loss: 3.8691
[09/26 14:05:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.00	
[09/26 14:05:56 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:05:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:05:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:05:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:05:56 visual_prompt]: Training with config:
[09/26 14:05:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:05:56 visual_prompt]: Loading training data...
[09/26 14:05:56 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 14:05:57 visual_prompt]: Number of images: 800
[09/26 14:05:57 visual_prompt]: Number of classes: 6 / 6
[09/26 14:05:57 visual_prompt]: Loading validation data...
[09/26 14:05:57 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 14:05:57 visual_prompt]: Number of images: 200
[09/26 14:05:57 visual_prompt]: Number of classes: 6 / 6
[09/26 14:05:57 visual_prompt]: Constructing models...
[09/26 14:06:00 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 14:06:00 visual_prompt]: tuned percent:0.540
[09/26 14:06:00 visual_prompt]: Device used for model: 0
[09/26 14:06:00 visual_prompt]: Setting up Evaluator...
[09/26 14:06:00 visual_prompt]: Setting up Trainer...
[09/26 14:06:00 visual_prompt]: 	Setting up the optimizer...
[09/26 14:06:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:06:07 visual_prompt]: Epoch 1 / 100: avg data time: 5.92e-02, avg batch time: 0.5059, average train loss: 2.9808
[09/26 14:06:08 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1687, average loss: 2.9268
[09/26 14:06:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 14:06:08 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 14:06:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 14:06:15 visual_prompt]: Epoch 2 / 100: avg data time: 6.49e-02, avg batch time: 0.5123, average train loss: 2.8171
[09/26 14:06:17 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1686, average loss: 2.3104
[09/26 14:06:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 82.50	
[09/26 14:06:17 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 14:06:24 visual_prompt]: Epoch 3 / 100: avg data time: 5.81e-02, avg batch time: 0.5071, average train loss: 1.9272
[09/26 14:06:25 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1690, average loss: 1.8457
[09/26 14:06:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 86.50	
[09/26 14:06:25 visual_prompt]: Best epoch 3: best metric: 0.195
[09/26 14:06:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 14:06:32 visual_prompt]: Epoch 4 / 100: avg data time: 4.95e-02, avg batch time: 0.4984, average train loss: 1.8054
[09/26 14:06:34 visual_prompt]: Inference (val):avg data time: 4.05e-04, avg batch time: 0.2646, average loss: 1.8058
[09/26 14:06:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 87.00	
[09/26 14:06:34 visual_prompt]: Best epoch 4: best metric: 0.220
[09/26 14:06:34 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 14:06:41 visual_prompt]: Epoch 5 / 100: avg data time: 4.53e-02, avg batch time: 0.4940, average train loss: 1.7889
[09/26 14:06:42 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1694, average loss: 1.7965
[09/26 14:06:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 84.00	
[09/26 14:06:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 14:06:49 visual_prompt]: Epoch 6 / 100: avg data time: 5.77e-02, avg batch time: 0.5068, average train loss: 1.7539
[09/26 14:06:51 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 1.7993
[09/26 14:06:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 80.50	
[09/26 14:06:51 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 14:06:58 visual_prompt]: Epoch 7 / 100: avg data time: 5.90e-02, avg batch time: 0.5070, average train loss: 1.6822
[09/26 14:06:59 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 1.9779
[09/26 14:06:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 87.50	
[09/26 14:06:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 14:07:06 visual_prompt]: Epoch 8 / 100: avg data time: 5.13e-02, avg batch time: 0.4997, average train loss: 1.7167
[09/26 14:07:07 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1694, average loss: 1.8041
[09/26 14:07:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 89.50	
[09/26 14:07:07 visual_prompt]: Best epoch 8: best metric: 0.225
[09/26 14:07:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 14:07:14 visual_prompt]: Epoch 9 / 100: avg data time: 5.13e-02, avg batch time: 0.5009, average train loss: 1.6564
[09/26 14:07:16 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1691, average loss: 1.7244
[09/26 14:07:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 90.00	
[09/26 14:07:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 14:07:23 visual_prompt]: Epoch 10 / 100: avg data time: 4.66e-02, avg batch time: 0.4973, average train loss: 1.5986
[09/26 14:07:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1695, average loss: 1.6912
[09/26 14:07:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 92.00	
[09/26 14:07:24 visual_prompt]: Best epoch 10: best metric: 0.240
[09/26 14:07:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 14:07:31 visual_prompt]: Epoch 11 / 100: avg data time: 4.44e-02, avg batch time: 0.4949, average train loss: 1.5247
[09/26 14:07:33 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1689, average loss: 1.7053
[09/26 14:07:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 92.50	
[09/26 14:07:33 visual_prompt]: Best epoch 11: best metric: 0.290
[09/26 14:07:33 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 14:07:39 visual_prompt]: Epoch 12 / 100: avg data time: 5.26e-02, avg batch time: 0.5017, average train loss: 1.4813
[09/26 14:07:41 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 1.5949
[09/26 14:07:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 95.00	
[09/26 14:07:41 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 14:07:48 visual_prompt]: Epoch 13 / 100: avg data time: 4.48e-02, avg batch time: 0.4944, average train loss: 1.4663
[09/26 14:07:49 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 1.5530
[09/26 14:07:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 97.00	
[09/26 14:07:49 visual_prompt]: Best epoch 13: best metric: 0.310
[09/26 14:07:49 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 14:07:56 visual_prompt]: Epoch 14 / 100: avg data time: 4.56e-02, avg batch time: 0.4969, average train loss: 1.3730
[09/26 14:07:57 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 1.4849
[09/26 14:07:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 96.00	
[09/26 14:07:57 visual_prompt]: Best epoch 14: best metric: 0.325
[09/26 14:07:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 14:08:04 visual_prompt]: Epoch 15 / 100: avg data time: 4.92e-02, avg batch time: 0.4987, average train loss: 1.3035
[09/26 14:08:06 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 1.4563
[09/26 14:08:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:08:06 visual_prompt]: Best epoch 15: best metric: 0.345
[09/26 14:08:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 14:08:13 visual_prompt]: Epoch 16 / 100: avg data time: 4.76e-02, avg batch time: 0.4990, average train loss: 1.2737
[09/26 14:08:14 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 1.5484
[09/26 14:08:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.00	
[09/26 14:08:14 visual_prompt]: Best epoch 16: best metric: 0.350
[09/26 14:08:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 14:08:21 visual_prompt]: Epoch 17 / 100: avg data time: 5.95e-02, avg batch time: 0.5080, average train loss: 1.2496
[09/26 14:08:23 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1696, average loss: 1.4704
[09/26 14:08:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 95.50	
[09/26 14:08:23 visual_prompt]: Best epoch 17: best metric: 0.365
[09/26 14:08:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 14:08:29 visual_prompt]: Epoch 18 / 100: avg data time: 4.93e-02, avg batch time: 0.4977, average train loss: 1.1596
[09/26 14:08:31 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 1.5955
[09/26 14:08:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 14:08:31 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 14:08:38 visual_prompt]: Epoch 19 / 100: avg data time: 5.42e-02, avg batch time: 0.5046, average train loss: 1.0924
[09/26 14:08:39 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1693, average loss: 1.4299
[09/26 14:08:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 14:08:39 visual_prompt]: Best epoch 19: best metric: 0.400
[09/26 14:08:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 14:08:46 visual_prompt]: Epoch 20 / 100: avg data time: 4.81e-02, avg batch time: 0.4984, average train loss: 1.0468
[09/26 14:08:48 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1693, average loss: 1.6210
[09/26 14:08:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 14:08:48 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 14:08:54 visual_prompt]: Epoch 21 / 100: avg data time: 4.67e-02, avg batch time: 0.4973, average train loss: 1.0959
[09/26 14:08:56 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 1.5149
[09/26 14:08:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.00	
[09/26 14:08:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 14:09:03 visual_prompt]: Epoch 22 / 100: avg data time: 4.92e-02, avg batch time: 0.5005, average train loss: 0.9844
[09/26 14:09:04 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1693, average loss: 2.1217
[09/26 14:09:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 93.50	
[09/26 14:09:04 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 14:09:11 visual_prompt]: Epoch 23 / 100: avg data time: 6.25e-02, avg batch time: 0.5119, average train loss: 1.0776
[09/26 14:09:13 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1691, average loss: 1.4819
[09/26 14:09:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 14:09:13 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 14:09:19 visual_prompt]: Epoch 24 / 100: avg data time: 4.39e-02, avg batch time: 0.4954, average train loss: 0.8869
[09/26 14:09:21 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1693, average loss: 1.6241
[09/26 14:09:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 14:09:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 14:09:28 visual_prompt]: Epoch 25 / 100: avg data time: 4.40e-02, avg batch time: 0.4943, average train loss: 0.8275
[09/26 14:09:29 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 1.8842
[09/26 14:09:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 14:09:29 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 14:09:36 visual_prompt]: Epoch 26 / 100: avg data time: 4.11e-02, avg batch time: 0.4915, average train loss: 0.8013
[09/26 14:09:38 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 1.9474
[09/26 14:09:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 95.50	
[09/26 14:09:38 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 14:09:44 visual_prompt]: Epoch 27 / 100: avg data time: 4.47e-02, avg batch time: 0.4954, average train loss: 0.7867
[09/26 14:09:46 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.8104
[09/26 14:09:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 14:09:46 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 14:09:53 visual_prompt]: Epoch 28 / 100: avg data time: 4.31e-02, avg batch time: 0.4928, average train loss: 0.9011
[09/26 14:09:54 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1697, average loss: 1.7422
[09/26 14:09:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.50	
[09/26 14:09:54 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 14:10:01 visual_prompt]: Epoch 29 / 100: avg data time: 4.52e-02, avg batch time: 0.4949, average train loss: 0.8165
[09/26 14:10:02 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1696, average loss: 1.7903
[09/26 14:10:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 14:10:02 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 14:10:09 visual_prompt]: Epoch 30 / 100: avg data time: 4.36e-02, avg batch time: 0.4940, average train loss: 0.7013
[09/26 14:10:11 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.1698, average loss: 1.8567
[09/26 14:10:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 14:10:11 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 14:10:17 visual_prompt]: Epoch 31 / 100: avg data time: 5.84e-02, avg batch time: 0.5082, average train loss: 0.6328
[09/26 14:10:19 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1699, average loss: 1.9713
[09/26 14:10:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.00	
[09/26 14:10:19 visual_prompt]: Best epoch 31: best metric: 0.405
[09/26 14:10:19 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 14:10:26 visual_prompt]: Epoch 32 / 100: avg data time: 4.73e-02, avg batch time: 0.4987, average train loss: 0.5880
[09/26 14:10:27 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1697, average loss: 2.0451
[09/26 14:10:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 95.00	
[09/26 14:10:27 visual_prompt]: Best epoch 32: best metric: 0.420
[09/26 14:10:27 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 14:10:34 visual_prompt]: Epoch 33 / 100: avg data time: 6.85e-02, avg batch time: 0.5176, average train loss: 0.5704
[09/26 14:10:36 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.8687
[09/26 14:10:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.00	
[09/26 14:10:36 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 14:10:43 visual_prompt]: Epoch 34 / 100: avg data time: 5.37e-02, avg batch time: 0.5032, average train loss: 0.6035
[09/26 14:10:44 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1699, average loss: 2.0925
[09/26 14:10:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/26 14:10:44 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 14:10:51 visual_prompt]: Epoch 35 / 100: avg data time: 5.16e-02, avg batch time: 0.5014, average train loss: 0.5773
[09/26 14:10:52 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1697, average loss: 2.2527
[09/26 14:10:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.00	
[09/26 14:10:52 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 14:10:59 visual_prompt]: Epoch 36 / 100: avg data time: 5.89e-02, avg batch time: 0.5086, average train loss: 0.5781
[09/26 14:11:01 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1698, average loss: 2.1568
[09/26 14:11:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.50	
[09/26 14:11:01 visual_prompt]: Best epoch 36: best metric: 0.425
[09/26 14:11:01 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 14:11:08 visual_prompt]: Epoch 37 / 100: avg data time: 5.72e-02, avg batch time: 0.5066, average train loss: 0.4981
[09/26 14:11:09 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 2.1006
[09/26 14:11:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 96.50	
[09/26 14:11:09 visual_prompt]: Best epoch 37: best metric: 0.435
[09/26 14:11:09 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 14:11:16 visual_prompt]: Epoch 38 / 100: avg data time: 4.53e-02, avg batch time: 0.4963, average train loss: 0.3671
[09/26 14:11:18 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1698, average loss: 2.3029
[09/26 14:11:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 95.00	
[09/26 14:11:18 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 14:11:24 visual_prompt]: Epoch 39 / 100: avg data time: 4.63e-02, avg batch time: 0.4969, average train loss: 0.2529
[09/26 14:11:26 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.1701, average loss: 2.7051
[09/26 14:11:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 14:11:26 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 14:11:33 visual_prompt]: Epoch 40 / 100: avg data time: 5.71e-02, avg batch time: 0.5072, average train loss: 0.2598
[09/26 14:11:34 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1698, average loss: 3.1360
[09/26 14:11:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.00	
[09/26 14:11:34 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 14:11:41 visual_prompt]: Epoch 41 / 100: avg data time: 5.40e-02, avg batch time: 0.5035, average train loss: 0.2828
[09/26 14:11:42 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1694, average loss: 2.9540
[09/26 14:11:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 14:11:42 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 14:11:49 visual_prompt]: Epoch 42 / 100: avg data time: 4.39e-02, avg batch time: 0.4951, average train loss: 0.2844
[09/26 14:11:51 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1696, average loss: 2.7706
[09/26 14:11:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 97.50	
[09/26 14:11:51 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 14:11:58 visual_prompt]: Epoch 43 / 100: avg data time: 5.84e-02, avg batch time: 0.5095, average train loss: 0.4147
[09/26 14:11:59 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1700, average loss: 3.3903
[09/26 14:11:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 96.00	
[09/26 14:11:59 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 14:12:06 visual_prompt]: Epoch 44 / 100: avg data time: 6.30e-02, avg batch time: 0.5118, average train loss: 0.5467
[09/26 14:12:08 visual_prompt]: Inference (val):avg data time: 4.51e-05, avg batch time: 0.1696, average loss: 2.2380
[09/26 14:12:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.50	
[09/26 14:12:08 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 14:12:15 visual_prompt]: Epoch 45 / 100: avg data time: 6.07e-02, avg batch time: 0.5098, average train loss: 0.4316
[09/26 14:12:16 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1696, average loss: 2.4383
[09/26 14:12:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 14:12:16 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 14:12:23 visual_prompt]: Epoch 46 / 100: avg data time: 5.71e-02, avg batch time: 0.5059, average train loss: 0.2668
[09/26 14:12:24 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1695, average loss: 2.6806
[09/26 14:12:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 14:12:24 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 14:12:31 visual_prompt]: Epoch 47 / 100: avg data time: 4.38e-02, avg batch time: 0.4934, average train loss: 0.1925
[09/26 14:12:33 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1699, average loss: 2.8389
[09/26 14:12:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.50	
[09/26 14:12:33 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 14:12:40 visual_prompt]: Epoch 48 / 100: avg data time: 6.30e-02, avg batch time: 0.5130, average train loss: 0.1367
[09/26 14:12:41 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1701, average loss: 3.1604
[09/26 14:12:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 14:12:41 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 14:12:48 visual_prompt]: Epoch 49 / 100: avg data time: 6.18e-02, avg batch time: 0.5118, average train loss: 0.1389
[09/26 14:12:50 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 3.3258
[09/26 14:12:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 14:12:50 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 14:12:56 visual_prompt]: Epoch 50 / 100: avg data time: 5.77e-02, avg batch time: 0.5068, average train loss: 0.1060
[09/26 14:12:58 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1695, average loss: 3.8253
[09/26 14:12:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.00	
[09/26 14:12:58 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 14:13:05 visual_prompt]: Epoch 51 / 100: avg data time: 5.30e-02, avg batch time: 0.5039, average train loss: 0.0963
[09/26 14:13:06 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1697, average loss: 4.3403
[09/26 14:13:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 14:13:06 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 14:13:13 visual_prompt]: Epoch 52 / 100: avg data time: 5.43e-02, avg batch time: 0.5033, average train loss: 0.1021
[09/26 14:13:15 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1699, average loss: 4.0747
[09/26 14:13:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 14:13:15 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 14:13:21 visual_prompt]: Epoch 53 / 100: avg data time: 4.96e-02, avg batch time: 0.5013, average train loss: 0.1059
[09/26 14:13:23 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1698, average loss: 4.3615
[09/26 14:13:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 14:13:23 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 14:13:30 visual_prompt]: Epoch 54 / 100: avg data time: 5.68e-02, avg batch time: 0.5070, average train loss: 0.2064
[09/26 14:13:31 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 3.6978
[09/26 14:13:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 14:13:31 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 14:13:38 visual_prompt]: Epoch 55 / 100: avg data time: 5.45e-02, avg batch time: 0.5041, average train loss: 0.3047
[09/26 14:13:40 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.1692, average loss: 3.6015
[09/26 14:13:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 95.50	
[09/26 14:13:40 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 14:13:47 visual_prompt]: Epoch 56 / 100: avg data time: 5.41e-02, avg batch time: 0.5039, average train loss: 0.1940
[09/26 14:13:48 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1696, average loss: 3.3679
[09/26 14:13:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.00	
[09/26 14:13:48 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 14:13:55 visual_prompt]: Epoch 57 / 100: avg data time: 6.37e-02, avg batch time: 0.5140, average train loss: 0.0893
[09/26 14:13:57 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 3.4276
[09/26 14:13:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.50	
[09/26 14:13:57 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 14:14:04 visual_prompt]: Epoch 58 / 100: avg data time: 5.68e-02, avg batch time: 0.5064, average train loss: 0.0635
[09/26 14:14:05 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1698, average loss: 3.4739
[09/26 14:14:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.50	
[09/26 14:14:05 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 14:14:12 visual_prompt]: Epoch 59 / 100: avg data time: 4.63e-02, avg batch time: 0.4982, average train loss: 0.0305
[09/26 14:14:14 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1692, average loss: 3.8225
[09/26 14:14:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 14:14:14 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 14:14:20 visual_prompt]: Epoch 60 / 100: avg data time: 5.16e-02, avg batch time: 0.5011, average train loss: 0.0399
[09/26 14:14:22 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1695, average loss: 4.0285
[09/26 14:14:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 99.00	
[09/26 14:14:22 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 14:14:29 visual_prompt]: Epoch 61 / 100: avg data time: 5.45e-02, avg batch time: 0.5057, average train loss: 0.0287
[09/26 14:14:30 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1694, average loss: 4.1507
[09/26 14:14:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.50	
[09/26 14:14:30 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 14:14:37 visual_prompt]: Epoch 62 / 100: avg data time: 6.14e-02, avg batch time: 0.5105, average train loss: 0.0255
[09/26 14:14:39 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 4.1661
[09/26 14:14:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 14:14:39 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 14:14:46 visual_prompt]: Epoch 63 / 100: avg data time: 5.77e-02, avg batch time: 0.5078, average train loss: 0.0101
[09/26 14:14:47 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.1696, average loss: 4.3956
[09/26 14:14:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 14:14:47 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 14:14:54 visual_prompt]: Epoch 64 / 100: avg data time: 4.58e-02, avg batch time: 0.4957, average train loss: 0.0094
[09/26 14:14:56 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 4.4662
[09/26 14:14:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:14:56 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 14:15:02 visual_prompt]: Epoch 65 / 100: avg data time: 5.05e-02, avg batch time: 0.5012, average train loss: 0.0080
[09/26 14:15:04 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 4.5822
[09/26 14:15:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 14:15:04 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 14:15:11 visual_prompt]: Epoch 66 / 100: avg data time: 5.68e-02, avg batch time: 0.5059, average train loss: 0.0069
[09/26 14:15:12 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 4.6278
[09/26 14:15:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.00	
[09/26 14:15:12 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 14:15:19 visual_prompt]: Epoch 67 / 100: avg data time: 6.03e-02, avg batch time: 0.5104, average train loss: 0.0080
[09/26 14:15:21 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 4.6158
[09/26 14:15:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 14:15:21 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 14:15:28 visual_prompt]: Epoch 68 / 100: avg data time: 5.60e-02, avg batch time: 0.5070, average train loss: 0.0056
[09/26 14:15:29 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1697, average loss: 4.7376
[09/26 14:15:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.50	
[09/26 14:15:29 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 14:15:36 visual_prompt]: Epoch 69 / 100: avg data time: 5.74e-02, avg batch time: 0.5070, average train loss: 0.0034
[09/26 14:15:38 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 4.8675
[09/26 14:15:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 14:15:38 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 14:15:44 visual_prompt]: Epoch 70 / 100: avg data time: 4.57e-02, avg batch time: 0.4956, average train loss: 0.0036
[09/26 14:15:46 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1699, average loss: 4.9177
[09/26 14:15:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:15:46 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 14:15:53 visual_prompt]: Epoch 71 / 100: avg data time: 4.84e-02, avg batch time: 0.5005, average train loss: 0.0039
[09/26 14:15:54 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1698, average loss: 5.0077
[09/26 14:15:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.00	
[09/26 14:15:54 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 14:16:01 visual_prompt]: Epoch 72 / 100: avg data time: 5.90e-02, avg batch time: 0.5080, average train loss: 0.0049
[09/26 14:16:03 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1700, average loss: 5.3674
[09/26 14:16:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:16:03 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 14:16:10 visual_prompt]: Epoch 73 / 100: avg data time: 5.86e-02, avg batch time: 0.5095, average train loss: 0.0052
[09/26 14:16:11 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1697, average loss: 5.5043
[09/26 14:16:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.50	
[09/26 14:16:11 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 14:16:18 visual_prompt]: Epoch 74 / 100: avg data time: 5.17e-02, avg batch time: 0.5029, average train loss: 0.0040
[09/26 14:16:19 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 5.4815
[09/26 14:16:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 14:16:19 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 14:16:26 visual_prompt]: Epoch 75 / 100: avg data time: 4.77e-02, avg batch time: 0.4986, average train loss: 0.0022
[09/26 14:16:28 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 5.4556
[09/26 14:16:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 14:16:28 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 14:16:35 visual_prompt]: Epoch 76 / 100: avg data time: 5.18e-02, avg batch time: 0.5021, average train loss: 0.0022
[09/26 14:16:36 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1701, average loss: 5.4858
[09/26 14:16:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.50	
[09/26 14:16:36 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 14:16:43 visual_prompt]: Epoch 77 / 100: avg data time: 6.69e-02, avg batch time: 0.5182, average train loss: 0.0017
[09/26 14:16:45 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1693, average loss: 5.5362
[09/26 14:16:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 14:16:45 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 14:16:52 visual_prompt]: Epoch 78 / 100: avg data time: 5.28e-02, avg batch time: 0.5033, average train loss: 0.0022
[09/26 14:16:53 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1700, average loss: 5.5471
[09/26 14:16:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:16:53 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 14:17:00 visual_prompt]: Epoch 79 / 100: avg data time: 4.66e-02, avg batch time: 0.4955, average train loss: 0.0015
[09/26 14:17:01 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1694, average loss: 5.5521
[09/26 14:17:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:17:01 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 14:17:08 visual_prompt]: Epoch 80 / 100: avg data time: 5.21e-02, avg batch time: 0.5023, average train loss: 0.0013
[09/26 14:17:10 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1696, average loss: 5.5739
[09/26 14:17:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:17:10 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 14:17:17 visual_prompt]: Epoch 81 / 100: avg data time: 5.45e-02, avg batch time: 0.5035, average train loss: 0.0016
[09/26 14:17:18 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1695, average loss: 5.6025
[09/26 14:17:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:17:18 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 14:17:25 visual_prompt]: Epoch 82 / 100: avg data time: 5.57e-02, avg batch time: 0.5049, average train loss: 0.0012
[09/26 14:17:27 visual_prompt]: Inference (val):avg data time: 4.68e-05, avg batch time: 0.1692, average loss: 5.6239
[09/26 14:17:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 14:17:27 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 14:17:34 visual_prompt]: Epoch 83 / 100: avg data time: 5.93e-02, avg batch time: 0.5082, average train loss: 0.0014
[09/26 14:17:35 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1699, average loss: 5.6393
[09/26 14:17:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 14:17:35 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 14:17:42 visual_prompt]: Epoch 84 / 100: avg data time: 5.66e-02, avg batch time: 0.5065, average train loss: 0.0014
[09/26 14:17:44 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1693, average loss: 5.6463
[09/26 14:17:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 14:17:44 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 14:17:50 visual_prompt]: Epoch 85 / 100: avg data time: 5.40e-02, avg batch time: 0.5040, average train loss: 0.0013
[09/26 14:17:52 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1695, average loss: 5.6523
[09/26 14:17:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 14:17:52 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 14:17:59 visual_prompt]: Epoch 86 / 100: avg data time: 5.46e-02, avg batch time: 0.5047, average train loss: 0.0015
[09/26 14:18:00 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1695, average loss: 5.6614
[09/26 14:18:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 14:18:00 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 14:18:07 visual_prompt]: Epoch 87 / 100: avg data time: 5.53e-02, avg batch time: 0.5046, average train loss: 0.0011
[09/26 14:18:09 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 5.6741
[09/26 14:18:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:18:09 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 14:18:16 visual_prompt]: Epoch 88 / 100: avg data time: 5.35e-02, avg batch time: 0.5039, average train loss: 0.0009
[09/26 14:18:17 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1699, average loss: 5.6816
[09/26 14:18:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:18:17 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 14:18:24 visual_prompt]: Epoch 89 / 100: avg data time: 4.79e-02, avg batch time: 0.4983, average train loss: 0.0014
[09/26 14:18:25 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 5.6894
[09/26 14:18:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:18:25 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 14:18:32 visual_prompt]: Epoch 90 / 100: avg data time: 6.10e-02, avg batch time: 0.5107, average train loss: 0.0013
[09/26 14:18:34 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 5.6948
[09/26 14:18:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:18:34 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 14:18:41 visual_prompt]: Epoch 91 / 100: avg data time: 5.74e-02, avg batch time: 0.5071, average train loss: 0.0013
[09/26 14:18:42 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1696, average loss: 5.6952
[09/26 14:18:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.00	
[09/26 14:18:42 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 14:18:49 visual_prompt]: Epoch 92 / 100: avg data time: 5.59e-02, avg batch time: 0.5050, average train loss: 0.0050
[09/26 14:18:51 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 5.6873
[09/26 14:18:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:18:51 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 14:18:57 visual_prompt]: Epoch 93 / 100: avg data time: 5.64e-02, avg batch time: 0.5069, average train loss: 0.0014
[09/26 14:18:59 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1699, average loss: 5.6865
[09/26 14:18:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 14:18:59 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 14:19:06 visual_prompt]: Epoch 94 / 100: avg data time: 5.86e-02, avg batch time: 0.5078, average train loss: 0.0017
[09/26 14:19:08 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.1695, average loss: 5.6869
[09/26 14:19:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 14:19:08 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 14:19:14 visual_prompt]: Epoch 95 / 100: avg data time: 4.52e-02, avg batch time: 0.4974, average train loss: 0.0018
[09/26 14:19:16 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 5.6898
[09/26 14:19:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 14:19:16 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 14:19:23 visual_prompt]: Epoch 96 / 100: avg data time: 5.74e-02, avg batch time: 0.5067, average train loss: 0.0014
[09/26 14:19:24 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1695, average loss: 5.6919
[09/26 14:19:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 14:19:24 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 14:19:31 visual_prompt]: Epoch 97 / 100: avg data time: 5.25e-02, avg batch time: 0.5016, average train loss: 0.0008
[09/26 14:19:33 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1694, average loss: 5.6925
[09/26 14:19:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 14:19:33 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 14:19:39 visual_prompt]: Epoch 98 / 100: avg data time: 5.38e-02, avg batch time: 0.5052, average train loss: 0.0010
[09/26 14:19:41 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1697, average loss: 5.6928
[09/26 14:19:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 14:19:41 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 14:19:48 visual_prompt]: Epoch 99 / 100: avg data time: 6.30e-02, avg batch time: 0.5127, average train loss: 0.0015
[09/26 14:19:50 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1698, average loss: 5.6930
[09/26 14:19:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 14:19:50 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 14:19:57 visual_prompt]: Epoch 100 / 100: avg data time: 6.14e-02, avg batch time: 0.5118, average train loss: 0.0024
[09/26 14:19:58 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 5.6931
[09/26 14:19:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 14:19:58 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:19:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:19:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:19:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:19:58 visual_prompt]: Training with config:
[09/26 14:19:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:19:58 visual_prompt]: Loading training data...
[09/26 14:19:58 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 14:19:59 visual_prompt]: Number of images: 800
[09/26 14:19:59 visual_prompt]: Number of classes: 6 / 6
[09/26 14:19:59 visual_prompt]: Loading validation data...
[09/26 14:19:59 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 14:20:00 visual_prompt]: Number of images: 200
[09/26 14:20:00 visual_prompt]: Number of classes: 6 / 6
[09/26 14:20:00 visual_prompt]: Constructing models...
[09/26 14:20:02 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 14:20:02 visual_prompt]: tuned percent:0.540
[09/26 14:20:02 visual_prompt]: Device used for model: 0
[09/26 14:20:02 visual_prompt]: Setting up Evaluator...
[09/26 14:20:02 visual_prompt]: Setting up Trainer...
[09/26 14:20:02 visual_prompt]: 	Setting up the optimizer...
[09/26 14:20:02 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:20:09 visual_prompt]: Epoch 1 / 100: avg data time: 4.33e-02, avg batch time: 0.4931, average train loss: 2.9764
[09/26 14:20:11 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1688, average loss: 2.9268
[09/26 14:20:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 14:20:11 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 14:20:11 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 14:20:18 visual_prompt]: Epoch 2 / 100: avg data time: 5.59e-02, avg batch time: 0.5034, average train loss: 2.7376
[09/26 14:20:19 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1686, average loss: 2.3663
[09/26 14:20:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 14:20:19 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 14:20:26 visual_prompt]: Epoch 3 / 100: avg data time: 5.63e-02, avg batch time: 0.5050, average train loss: 1.9977
[09/26 14:20:28 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1692, average loss: 1.9257
[09/26 14:20:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 14:20:28 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 14:20:34 visual_prompt]: Epoch 4 / 100: avg data time: 5.17e-02, avg batch time: 0.5007, average train loss: 1.8030
[09/26 14:20:36 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1688, average loss: 1.8201
[09/26 14:20:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 14:20:36 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 14:20:43 visual_prompt]: Epoch 5 / 100: avg data time: 4.79e-02, avg batch time: 0.4975, average train loss: 1.7778
[09/26 14:20:44 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1692, average loss: 1.7910
[09/26 14:20:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 84.50	
[09/26 14:20:44 visual_prompt]: Best epoch 5: best metric: 0.215
[09/26 14:20:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 14:20:51 visual_prompt]: Epoch 6 / 100: avg data time: 6.26e-02, avg batch time: 0.5125, average train loss: 1.7717
[09/26 14:20:53 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 1.8012
[09/26 14:20:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.00	top5: 85.00	
[09/26 14:20:53 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 14:21:00 visual_prompt]: Epoch 7 / 100: avg data time: 4.81e-02, avg batch time: 0.4975, average train loss: 1.7171
[09/26 14:21:01 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1696, average loss: 1.7813
[09/26 14:21:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 88.00	
[09/26 14:21:01 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 14:21:08 visual_prompt]: Epoch 8 / 100: avg data time: 5.71e-02, avg batch time: 0.5052, average train loss: 1.7545
[09/26 14:21:09 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 1.7213
[09/26 14:21:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 93.00	
[09/26 14:21:09 visual_prompt]: Best epoch 8: best metric: 0.240
[09/26 14:21:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 14:21:16 visual_prompt]: Epoch 9 / 100: avg data time: 5.90e-02, avg batch time: 0.5073, average train loss: 1.6422
[09/26 14:21:18 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1689, average loss: 1.8545
[09/26 14:21:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 91.50	
[09/26 14:21:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 14:21:24 visual_prompt]: Epoch 10 / 100: avg data time: 4.15e-02, avg batch time: 0.4904, average train loss: 1.6257
[09/26 14:21:26 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 1.6356
[09/26 14:21:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 93.00	
[09/26 14:21:26 visual_prompt]: Best epoch 10: best metric: 0.290
[09/26 14:21:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 14:21:33 visual_prompt]: Epoch 11 / 100: avg data time: 5.20e-02, avg batch time: 0.5019, average train loss: 1.5255
[09/26 14:21:34 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1688, average loss: 1.6160
[09/26 14:21:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 94.00	
[09/26 14:21:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 14:21:41 visual_prompt]: Epoch 12 / 100: avg data time: 5.02e-02, avg batch time: 0.5013, average train loss: 1.4606
[09/26 14:21:43 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1695, average loss: 1.8084
[09/26 14:21:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 92.50	
[09/26 14:21:43 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 14:21:49 visual_prompt]: Epoch 13 / 100: avg data time: 4.41e-02, avg batch time: 0.4986, average train loss: 1.4424
[09/26 14:21:51 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1694, average loss: 1.5844
[09/26 14:21:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 95.00	
[09/26 14:21:51 visual_prompt]: Best epoch 13: best metric: 0.345
[09/26 14:21:51 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 14:21:58 visual_prompt]: Epoch 14 / 100: avg data time: 5.68e-02, avg batch time: 0.5065, average train loss: 1.3898
[09/26 14:21:59 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 1.5308
[09/26 14:21:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 95.50	
[09/26 14:21:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 14:22:06 visual_prompt]: Epoch 15 / 100: avg data time: 4.73e-02, avg batch time: 0.4973, average train loss: 1.3668
[09/26 14:22:08 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1694, average loss: 1.5468
[09/26 14:22:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 96.00	
[09/26 14:22:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 14:22:15 visual_prompt]: Epoch 16 / 100: avg data time: 4.73e-02, avg batch time: 0.4975, average train loss: 1.2734
[09/26 14:22:16 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1697, average loss: 1.6289
[09/26 14:22:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 94.00	
[09/26 14:22:16 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 14:22:23 visual_prompt]: Epoch 17 / 100: avg data time: 5.52e-02, avg batch time: 0.5039, average train loss: 1.2196
[09/26 14:22:24 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1695, average loss: 1.5931
[09/26 14:22:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 95.50	
[09/26 14:22:24 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 14:22:31 visual_prompt]: Epoch 18 / 100: avg data time: 5.94e-02, avg batch time: 0.5098, average train loss: 1.2116
[09/26 14:22:33 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1698, average loss: 1.4090
[09/26 14:22:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 14:22:33 visual_prompt]: Best epoch 18: best metric: 0.400
[09/26 14:22:33 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 14:22:40 visual_prompt]: Epoch 19 / 100: avg data time: 6.10e-02, avg batch time: 0.5096, average train loss: 1.0928
[09/26 14:22:41 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1700, average loss: 1.6381
[09/26 14:22:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 94.50	
[09/26 14:22:41 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 14:22:48 visual_prompt]: Epoch 20 / 100: avg data time: 6.19e-02, avg batch time: 0.5108, average train loss: 1.1501
[09/26 14:22:50 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1691, average loss: 1.6131
[09/26 14:22:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.50	
[09/26 14:22:50 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 14:22:57 visual_prompt]: Epoch 21 / 100: avg data time: 4.49e-02, avg batch time: 0.4947, average train loss: 1.1109
[09/26 14:22:58 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1698, average loss: 1.5864
[09/26 14:22:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 14:22:58 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 14:23:05 visual_prompt]: Epoch 22 / 100: avg data time: 6.11e-02, avg batch time: 0.5096, average train loss: 1.0555
[09/26 14:23:07 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1693, average loss: 1.5788
[09/26 14:23:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 14:23:07 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 14:23:14 visual_prompt]: Epoch 23 / 100: avg data time: 5.84e-02, avg batch time: 0.5076, average train loss: 0.9958
[09/26 14:23:15 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1694, average loss: 1.5960
[09/26 14:23:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.00	
[09/26 14:23:15 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 14:23:22 visual_prompt]: Epoch 24 / 100: avg data time: 4.79e-02, avg batch time: 0.4990, average train loss: 1.0292
[09/26 14:23:24 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 1.6894
[09/26 14:23:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 14:23:24 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 14:23:30 visual_prompt]: Epoch 25 / 100: avg data time: 5.66e-02, avg batch time: 0.5066, average train loss: 0.9486
[09/26 14:23:32 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1692, average loss: 1.6990
[09/26 14:23:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 14:23:32 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 14:23:39 visual_prompt]: Epoch 26 / 100: avg data time: 5.35e-02, avg batch time: 0.5042, average train loss: 0.8123
[09/26 14:23:40 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1696, average loss: 1.7066
[09/26 14:23:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 14:23:40 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 14:23:47 visual_prompt]: Epoch 27 / 100: avg data time: 4.52e-02, avg batch time: 0.4946, average train loss: 0.7487
[09/26 14:23:49 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1691, average loss: 1.8584
[09/26 14:23:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 14:23:49 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 14:23:56 visual_prompt]: Epoch 28 / 100: avg data time: 5.35e-02, avg batch time: 0.5035, average train loss: 0.7286
[09/26 14:23:57 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 1.9787
[09/26 14:23:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 14:23:57 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 14:24:04 visual_prompt]: Epoch 29 / 100: avg data time: 5.83e-02, avg batch time: 0.5072, average train loss: 0.7038
[09/26 14:24:06 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1696, average loss: 1.8439
[09/26 14:24:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 14:24:06 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 14:24:12 visual_prompt]: Epoch 30 / 100: avg data time: 4.68e-02, avg batch time: 0.4989, average train loss: 0.6832
[09/26 14:24:14 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 2.0049
[09/26 14:24:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.50	
[09/26 14:24:14 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 14:24:21 visual_prompt]: Epoch 31 / 100: avg data time: 4.34e-02, avg batch time: 0.4954, average train loss: 0.6140
[09/26 14:24:22 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1696, average loss: 2.1507
[09/26 14:24:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 14:24:22 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 14:24:29 visual_prompt]: Epoch 32 / 100: avg data time: 5.24e-02, avg batch time: 0.5025, average train loss: 0.6262
[09/26 14:24:31 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1697, average loss: 2.0935
[09/26 14:24:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 14:24:31 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 14:24:38 visual_prompt]: Epoch 33 / 100: avg data time: 5.75e-02, avg batch time: 0.5080, average train loss: 0.5489
[09/26 14:24:39 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1693, average loss: 2.3080
[09/26 14:24:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 14:24:39 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 14:24:46 visual_prompt]: Epoch 34 / 100: avg data time: 5.30e-02, avg batch time: 0.5019, average train loss: 0.6380
[09/26 14:24:47 visual_prompt]: Inference (val):avg data time: 4.68e-05, avg batch time: 0.1693, average loss: 2.1483
[09/26 14:24:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.50	
[09/26 14:24:47 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 14:24:54 visual_prompt]: Epoch 35 / 100: avg data time: 5.95e-02, avg batch time: 0.5093, average train loss: 0.4345
[09/26 14:24:56 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1696, average loss: 2.1178
[09/26 14:24:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 14:24:56 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 14:25:03 visual_prompt]: Epoch 36 / 100: avg data time: 5.92e-02, avg batch time: 0.5090, average train loss: 0.4634
[09/26 14:25:04 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 2.1988
[09/26 14:25:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.00	
[09/26 14:25:04 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 14:25:11 visual_prompt]: Epoch 37 / 100: avg data time: 5.01e-02, avg batch time: 0.5008, average train loss: 0.5756
[09/26 14:25:13 visual_prompt]: Inference (val):avg data time: 5.04e-05, avg batch time: 0.1695, average loss: 2.0784
[09/26 14:25:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 14:25:13 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 14:25:20 visual_prompt]: Epoch 38 / 100: avg data time: 4.61e-02, avg batch time: 0.4961, average train loss: 0.4872
[09/26 14:25:21 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 2.5984
[09/26 14:25:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.00	
[09/26 14:25:21 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 14:25:28 visual_prompt]: Epoch 39 / 100: avg data time: 5.91e-02, avg batch time: 0.5081, average train loss: 0.4495
[09/26 14:25:29 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1692, average loss: 2.7667
[09/26 14:25:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 14:25:29 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 14:25:36 visual_prompt]: Epoch 40 / 100: avg data time: 5.75e-02, avg batch time: 0.5071, average train loss: 0.3529
[09/26 14:25:38 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1695, average loss: 2.7141
[09/26 14:25:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 14:25:38 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 14:25:45 visual_prompt]: Epoch 41 / 100: avg data time: 5.88e-02, avg batch time: 0.5091, average train loss: 0.3623
[09/26 14:25:46 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 3.3369
[09/26 14:25:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 14:25:46 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 14:25:53 visual_prompt]: Epoch 42 / 100: avg data time: 5.69e-02, avg batch time: 0.5059, average train loss: 0.3903
[09/26 14:25:55 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 2.7245
[09/26 14:25:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 14:25:55 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 14:26:02 visual_prompt]: Epoch 43 / 100: avg data time: 5.53e-02, avg batch time: 0.5052, average train loss: 0.3374
[09/26 14:26:03 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 2.7606
[09/26 14:26:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 14:26:03 visual_prompt]: Best epoch 43: best metric: 0.410
[09/26 14:26:03 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 14:26:10 visual_prompt]: Epoch 44 / 100: avg data time: 6.09e-02, avg batch time: 0.5101, average train loss: 0.2396
[09/26 14:26:12 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1695, average loss: 2.9523
[09/26 14:26:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 97.00	
[09/26 14:26:12 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 14:26:18 visual_prompt]: Epoch 45 / 100: avg data time: 5.42e-02, avg batch time: 0.5034, average train loss: 0.1770
[09/26 14:26:20 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1694, average loss: 3.9303
[09/26 14:26:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 14:26:20 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 14:26:27 visual_prompt]: Epoch 46 / 100: avg data time: 4.50e-02, avg batch time: 0.4970, average train loss: 0.2738
[09/26 14:26:28 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1691, average loss: 3.0949
[09/26 14:26:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.00	
[09/26 14:26:28 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 14:26:35 visual_prompt]: Epoch 47 / 100: avg data time: 4.76e-02, avg batch time: 0.4967, average train loss: 0.2641
[09/26 14:26:37 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 3.3865
[09/26 14:26:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 95.50	
[09/26 14:26:37 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 14:26:43 visual_prompt]: Epoch 48 / 100: avg data time: 4.96e-02, avg batch time: 0.4990, average train loss: 0.2076
[09/26 14:26:45 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1695, average loss: 3.1225
[09/26 14:26:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 14:26:45 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 14:26:52 visual_prompt]: Epoch 49 / 100: avg data time: 4.97e-02, avg batch time: 0.5008, average train loss: 0.1574
[09/26 14:26:53 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1694, average loss: 3.5029
[09/26 14:26:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.00	
[09/26 14:26:53 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 14:27:00 visual_prompt]: Epoch 50 / 100: avg data time: 5.56e-02, avg batch time: 0.5044, average train loss: 0.1171
[09/26 14:27:02 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1695, average loss: 4.3512
[09/26 14:27:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 14:27:02 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 14:27:09 visual_prompt]: Epoch 51 / 100: avg data time: 6.21e-02, avg batch time: 0.5122, average train loss: 0.1101
[09/26 14:27:10 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 4.1320
[09/26 14:27:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 95.50	
[09/26 14:27:10 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 14:27:17 visual_prompt]: Epoch 52 / 100: avg data time: 4.67e-02, avg batch time: 0.4978, average train loss: 0.1344
[09/26 14:27:18 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1701, average loss: 3.8045
[09/26 14:27:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.50	
[09/26 14:27:18 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 14:27:25 visual_prompt]: Epoch 53 / 100: avg data time: 4.36e-02, avg batch time: 0.4946, average train loss: 0.1080
[09/26 14:27:27 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1700, average loss: 4.1446
[09/26 14:27:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.00	
[09/26 14:27:27 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 14:27:33 visual_prompt]: Epoch 54 / 100: avg data time: 5.07e-02, avg batch time: 0.5002, average train loss: 0.1222
[09/26 14:27:35 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1699, average loss: 4.4648
[09/26 14:27:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 14:27:35 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 14:27:42 visual_prompt]: Epoch 55 / 100: avg data time: 5.89e-02, avg batch time: 0.5098, average train loss: 0.0873
[09/26 14:27:43 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1693, average loss: 4.1581
[09/26 14:27:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 14:27:43 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 14:27:50 visual_prompt]: Epoch 56 / 100: avg data time: 4.58e-02, avg batch time: 0.4987, average train loss: 0.1129
[09/26 14:27:52 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 4.7698
[09/26 14:27:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.50	
[09/26 14:27:52 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 14:27:59 visual_prompt]: Epoch 57 / 100: avg data time: 6.27e-02, avg batch time: 0.5121, average train loss: 0.0865
[09/26 14:28:00 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1697, average loss: 4.5420
[09/26 14:28:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 14:28:00 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 14:28:07 visual_prompt]: Epoch 58 / 100: avg data time: 6.40e-02, avg batch time: 0.5134, average train loss: 0.0602
[09/26 14:28:09 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1700, average loss: 4.9827
[09/26 14:28:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.00	
[09/26 14:28:09 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 14:28:15 visual_prompt]: Epoch 59 / 100: avg data time: 5.24e-02, avg batch time: 0.5033, average train loss: 0.0714
[09/26 14:28:17 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1694, average loss: 4.5149
[09/26 14:28:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.50	
[09/26 14:28:17 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 14:28:24 visual_prompt]: Epoch 60 / 100: avg data time: 5.61e-02, avg batch time: 0.5073, average train loss: 0.1137
[09/26 14:28:25 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 4.2309
[09/26 14:28:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 95.00	
[09/26 14:28:25 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 14:28:32 visual_prompt]: Epoch 61 / 100: avg data time: 4.74e-02, avg batch time: 0.4976, average train loss: 0.1237
[09/26 14:28:34 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1698, average loss: 4.7266
[09/26 14:28:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 14:28:34 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 14:28:41 visual_prompt]: Epoch 62 / 100: avg data time: 4.44e-02, avg batch time: 0.4969, average train loss: 0.0646
[09/26 14:28:42 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1701, average loss: 4.5039
[09/26 14:28:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.00	
[09/26 14:28:42 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 14:28:49 visual_prompt]: Epoch 63 / 100: avg data time: 6.09e-02, avg batch time: 0.5107, average train loss: 0.0711
[09/26 14:28:51 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1701, average loss: 4.9374
[09/26 14:28:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 14:28:51 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 14:28:58 visual_prompt]: Epoch 64 / 100: avg data time: 6.26e-02, avg batch time: 0.5114, average train loss: 0.0301
[09/26 14:28:59 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1693, average loss: 5.3013
[09/26 14:28:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 95.50	
[09/26 14:28:59 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 14:29:06 visual_prompt]: Epoch 65 / 100: avg data time: 4.92e-02, avg batch time: 0.5005, average train loss: 0.0164
[09/26 14:29:07 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1698, average loss: 5.0936
[09/26 14:29:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 14:29:07 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 14:29:14 visual_prompt]: Epoch 66 / 100: avg data time: 5.66e-02, avg batch time: 0.5059, average train loss: 0.0135
[09/26 14:29:16 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 5.9235
[09/26 14:29:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 95.50	
[09/26 14:29:16 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 14:29:23 visual_prompt]: Epoch 67 / 100: avg data time: 4.46e-02, avg batch time: 0.4952, average train loss: 0.0132
[09/26 14:29:24 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1699, average loss: 6.0073
[09/26 14:29:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 95.50	
[09/26 14:29:24 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 14:29:31 visual_prompt]: Epoch 68 / 100: avg data time: 6.36e-02, avg batch time: 0.5147, average train loss: 0.0059
[09/26 14:29:33 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1695, average loss: 5.9105
[09/26 14:29:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 14:29:33 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 14:29:40 visual_prompt]: Epoch 69 / 100: avg data time: 5.40e-02, avg batch time: 0.5037, average train loss: 0.0047
[09/26 14:29:41 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1696, average loss: 5.8576
[09/26 14:29:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 14:29:41 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 14:29:48 visual_prompt]: Epoch 70 / 100: avg data time: 6.12e-02, avg batch time: 0.5102, average train loss: 0.0036
[09/26 14:29:50 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 6.1030
[09/26 14:29:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 14:29:50 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 14:29:56 visual_prompt]: Epoch 71 / 100: avg data time: 6.00e-02, avg batch time: 0.5090, average train loss: 0.0035
[09/26 14:29:58 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1696, average loss: 6.2396
[09/26 14:29:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.50	
[09/26 14:29:58 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 14:30:05 visual_prompt]: Epoch 72 / 100: avg data time: 4.62e-02, avg batch time: 0.4956, average train loss: 0.0021
[09/26 14:30:06 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 6.5202
[09/26 14:30:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.00	
[09/26 14:30:06 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 14:30:13 visual_prompt]: Epoch 73 / 100: avg data time: 4.68e-02, avg batch time: 0.4971, average train loss: 0.0024
[09/26 14:30:15 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1697, average loss: 6.4896
[09/26 14:30:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.50	
[09/26 14:30:15 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 14:30:21 visual_prompt]: Epoch 74 / 100: avg data time: 5.95e-02, avg batch time: 0.5087, average train loss: 0.0030
[09/26 14:30:23 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1698, average loss: 6.3990
[09/26 14:30:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 97.00	
[09/26 14:30:23 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 14:30:30 visual_prompt]: Epoch 75 / 100: avg data time: 4.99e-02, avg batch time: 0.5005, average train loss: 0.0018
[09/26 14:30:31 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 6.1672
[09/26 14:30:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:30:31 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 14:30:38 visual_prompt]: Epoch 76 / 100: avg data time: 5.61e-02, avg batch time: 0.5062, average train loss: 0.0022
[09/26 14:30:40 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1696, average loss: 6.3446
[09/26 14:30:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 14:30:40 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 14:30:47 visual_prompt]: Epoch 77 / 100: avg data time: 5.27e-02, avg batch time: 0.5029, average train loss: 0.0012
[09/26 14:30:48 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 6.4043
[09/26 14:30:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 14:30:48 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 14:30:55 visual_prompt]: Epoch 78 / 100: avg data time: 6.10e-02, avg batch time: 0.5108, average train loss: 0.0048
[09/26 14:30:57 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 6.2649
[09/26 14:30:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 14:30:57 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 14:31:04 visual_prompt]: Epoch 79 / 100: avg data time: 5.38e-02, avg batch time: 0.5053, average train loss: 0.0017
[09/26 14:31:05 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1696, average loss: 6.5960
[09/26 14:31:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:31:05 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 14:31:12 visual_prompt]: Epoch 80 / 100: avg data time: 4.79e-02, avg batch time: 0.4982, average train loss: 0.0030
[09/26 14:31:13 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 6.8529
[09/26 14:31:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.00	
[09/26 14:31:13 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 14:31:20 visual_prompt]: Epoch 81 / 100: avg data time: 4.94e-02, avg batch time: 0.4987, average train loss: 0.0019
[09/26 14:31:22 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 6.8811
[09/26 14:31:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:31:22 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 14:31:28 visual_prompt]: Epoch 82 / 100: avg data time: 4.41e-02, avg batch time: 0.4938, average train loss: 0.0012
[09/26 14:31:30 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1696, average loss: 6.8465
[09/26 14:31:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.00	
[09/26 14:31:30 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 14:31:37 visual_prompt]: Epoch 83 / 100: avg data time: 4.95e-02, avg batch time: 0.4999, average train loss: 0.0018
[09/26 14:31:38 visual_prompt]: Inference (val):avg data time: 4.45e-05, avg batch time: 0.1696, average loss: 6.8120
[09/26 14:31:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.00	
[09/26 14:31:38 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 14:31:45 visual_prompt]: Epoch 84 / 100: avg data time: 4.77e-02, avg batch time: 0.4970, average train loss: 0.0019
[09/26 14:31:47 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1697, average loss: 6.7880
[09/26 14:31:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.00	
[09/26 14:31:47 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 14:31:53 visual_prompt]: Epoch 85 / 100: avg data time: 4.61e-02, avg batch time: 0.4974, average train loss: 0.0025
[09/26 14:31:55 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 6.8528
[09/26 14:31:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.00	
[09/26 14:31:55 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 14:32:02 visual_prompt]: Epoch 86 / 100: avg data time: 6.05e-02, avg batch time: 0.5109, average train loss: 0.0025
[09/26 14:32:03 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1698, average loss: 6.9263
[09/26 14:32:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.00	
[09/26 14:32:03 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 14:32:10 visual_prompt]: Epoch 87 / 100: avg data time: 4.94e-02, avg batch time: 0.4983, average train loss: 0.0025
[09/26 14:32:12 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1697, average loss: 6.9513
[09/26 14:32:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:32:12 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 14:32:19 visual_prompt]: Epoch 88 / 100: avg data time: 5.51e-02, avg batch time: 0.5042, average train loss: 0.0015
[09/26 14:32:20 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1696, average loss: 6.9436
[09/26 14:32:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:32:20 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 14:32:27 visual_prompt]: Epoch 89 / 100: avg data time: 5.62e-02, avg batch time: 0.5062, average train loss: 0.0007
[09/26 14:32:28 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1698, average loss: 6.9551
[09/26 14:32:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:32:28 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 14:32:35 visual_prompt]: Epoch 90 / 100: avg data time: 5.84e-02, avg batch time: 0.5073, average train loss: 0.0024
[09/26 14:32:37 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1699, average loss: 6.9581
[09/26 14:32:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:32:37 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 14:32:44 visual_prompt]: Epoch 91 / 100: avg data time: 5.23e-02, avg batch time: 0.5032, average train loss: 0.0013
[09/26 14:32:45 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1691, average loss: 6.9524
[09/26 14:32:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:32:45 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 14:32:52 visual_prompt]: Epoch 92 / 100: avg data time: 6.24e-02, avg batch time: 0.5114, average train loss: 0.0013
[09/26 14:32:54 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 6.9508
[09/26 14:32:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:32:54 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 14:33:01 visual_prompt]: Epoch 93 / 100: avg data time: 5.15e-02, avg batch time: 0.5012, average train loss: 0.0022
[09/26 14:33:02 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1696, average loss: 6.9358
[09/26 14:33:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:33:02 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 14:33:09 visual_prompt]: Epoch 94 / 100: avg data time: 5.54e-02, avg batch time: 0.5058, average train loss: 0.0011
[09/26 14:33:10 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1695, average loss: 6.9279
[09/26 14:33:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.00	
[09/26 14:33:10 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 14:33:17 visual_prompt]: Epoch 95 / 100: avg data time: 5.65e-02, avg batch time: 0.5058, average train loss: 0.0025
[09/26 14:33:19 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1696, average loss: 6.9191
[09/26 14:33:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.00	
[09/26 14:33:19 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 14:33:26 visual_prompt]: Epoch 96 / 100: avg data time: 5.17e-02, avg batch time: 0.5006, average train loss: 0.0008
[09/26 14:33:27 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1692, average loss: 6.9111
[09/26 14:33:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:33:27 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 14:33:34 visual_prompt]: Epoch 97 / 100: avg data time: 6.10e-02, avg batch time: 0.5102, average train loss: 0.0012
[09/26 14:33:36 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1697, average loss: 6.9102
[09/26 14:33:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:33:36 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 14:33:42 visual_prompt]: Epoch 98 / 100: avg data time: 5.30e-02, avg batch time: 0.5023, average train loss: 0.0013
[09/26 14:33:44 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1695, average loss: 6.9113
[09/26 14:33:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:33:44 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 14:33:51 visual_prompt]: Epoch 99 / 100: avg data time: 5.60e-02, avg batch time: 0.5052, average train loss: 0.0023
[09/26 14:33:53 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1696, average loss: 6.9120
[09/26 14:33:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:33:53 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 14:34:00 visual_prompt]: Epoch 100 / 100: avg data time: 5.72e-02, avg batch time: 0.5062, average train loss: 0.0014
[09/26 14:34:01 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1696, average loss: 6.9124
[09/26 14:34:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:34:01 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:34:01 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:34:01 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:34:01 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:34:01 visual_prompt]: Training with config:
[09/26 14:34:01 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:34:01 visual_prompt]: Loading training data...
[09/26 14:34:01 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 14:34:02 visual_prompt]: Number of images: 800
[09/26 14:34:02 visual_prompt]: Number of classes: 6 / 6
[09/26 14:34:02 visual_prompt]: Loading validation data...
[09/26 14:34:02 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 14:34:03 visual_prompt]: Number of images: 200
[09/26 14:34:03 visual_prompt]: Number of classes: 6 / 6
[09/26 14:34:03 visual_prompt]: Constructing models...
[09/26 14:34:05 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 14:34:05 visual_prompt]: tuned percent:0.540
[09/26 14:34:05 visual_prompt]: Device used for model: 0
[09/26 14:34:05 visual_prompt]: Setting up Evaluator...
[09/26 14:34:05 visual_prompt]: Setting up Trainer...
[09/26 14:34:05 visual_prompt]: 	Setting up the optimizer...
[09/26 14:34:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:34:12 visual_prompt]: Epoch 1 / 100: avg data time: 5.57e-02, avg batch time: 0.5029, average train loss: 2.9556
[09/26 14:34:14 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1689, average loss: 2.9268
[09/26 14:34:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 14:34:14 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 14:34:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 14:34:21 visual_prompt]: Epoch 2 / 100: avg data time: 5.40e-02, avg batch time: 0.5021, average train loss: 2.2388
[09/26 14:34:22 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1685, average loss: 1.9482
[09/26 14:34:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 82.50	
[09/26 14:34:22 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 14:34:29 visual_prompt]: Epoch 3 / 100: avg data time: 4.54e-02, avg batch time: 0.4947, average train loss: 1.8250
[09/26 14:34:30 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 1.8484
[09/26 14:34:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 14:34:30 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 14:34:37 visual_prompt]: Epoch 4 / 100: avg data time: 5.86e-02, avg batch time: 0.5073, average train loss: 1.7749
[09/26 14:34:39 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1696, average loss: 1.7957
[09/26 14:34:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 86.00	
[09/26 14:34:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 14:34:46 visual_prompt]: Epoch 5 / 100: avg data time: 4.67e-02, avg batch time: 0.4971, average train loss: 1.7685
[09/26 14:34:47 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1692, average loss: 1.8417
[09/26 14:34:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 14:34:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 14:34:54 visual_prompt]: Epoch 6 / 100: avg data time: 4.57e-02, avg batch time: 0.4958, average train loss: 1.7267
[09/26 14:34:56 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 1.7524
[09/26 14:34:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 84.00	
[09/26 14:34:56 visual_prompt]: Best epoch 6: best metric: 0.245
[09/26 14:34:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 14:35:02 visual_prompt]: Epoch 7 / 100: avg data time: 4.35e-02, avg batch time: 0.4938, average train loss: 1.6762
[09/26 14:35:04 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1694, average loss: 2.1763
[09/26 14:35:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 80.50	
[09/26 14:35:04 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 14:35:11 visual_prompt]: Epoch 8 / 100: avg data time: 5.19e-02, avg batch time: 0.5002, average train loss: 1.6961
[09/26 14:35:12 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1694, average loss: 1.7435
[09/26 14:35:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.00	top5: 89.50	
[09/26 14:35:12 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 14:35:19 visual_prompt]: Epoch 9 / 100: avg data time: 5.95e-02, avg batch time: 0.5094, average train loss: 1.6197
[09/26 14:35:21 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 1.6659
[09/26 14:35:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 91.00	
[09/26 14:35:21 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 14:35:27 visual_prompt]: Epoch 10 / 100: avg data time: 4.87e-02, avg batch time: 0.4980, average train loss: 1.5703
[09/26 14:35:29 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1690, average loss: 1.8731
[09/26 14:35:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 85.00	
[09/26 14:35:29 visual_prompt]: Best epoch 10: best metric: 0.255
[09/26 14:35:29 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 14:35:36 visual_prompt]: Epoch 11 / 100: avg data time: 6.29e-02, avg batch time: 0.5115, average train loss: 1.5644
[09/26 14:35:37 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1698, average loss: 1.7307
[09/26 14:35:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 92.00	
[09/26 14:35:37 visual_prompt]: Best epoch 11: best metric: 0.265
[09/26 14:35:37 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 14:35:44 visual_prompt]: Epoch 12 / 100: avg data time: 4.88e-02, avg batch time: 0.4994, average train loss: 1.5143
[09/26 14:35:46 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 1.5581
[09/26 14:35:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 96.00	
[09/26 14:35:46 visual_prompt]: Best epoch 12: best metric: 0.270
[09/26 14:35:46 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 14:35:53 visual_prompt]: Epoch 13 / 100: avg data time: 6.49e-02, avg batch time: 0.5155, average train loss: 1.4501
[09/26 14:35:54 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1692, average loss: 1.7364
[09/26 14:35:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 94.50	
[09/26 14:35:54 visual_prompt]: Best epoch 13: best metric: 0.280
[09/26 14:35:54 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 14:36:01 visual_prompt]: Epoch 14 / 100: avg data time: 4.59e-02, avg batch time: 0.4959, average train loss: 1.4735
[09/26 14:36:03 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1695, average loss: 1.6134
[09/26 14:36:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 96.00	
[09/26 14:36:03 visual_prompt]: Best epoch 14: best metric: 0.290
[09/26 14:36:03 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 14:36:10 visual_prompt]: Epoch 15 / 100: avg data time: 6.14e-02, avg batch time: 0.5103, average train loss: 1.4546
[09/26 14:36:11 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 1.8798
[09/26 14:36:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 93.00	
[09/26 14:36:11 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 14:36:18 visual_prompt]: Epoch 16 / 100: avg data time: 6.63e-02, avg batch time: 0.5153, average train loss: 1.4198
[09/26 14:36:20 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 1.4773
[09/26 14:36:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 98.00	
[09/26 14:36:20 visual_prompt]: Best epoch 16: best metric: 0.310
[09/26 14:36:20 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 14:36:26 visual_prompt]: Epoch 17 / 100: avg data time: 5.22e-02, avg batch time: 0.5022, average train loss: 1.3810
[09/26 14:36:28 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1694, average loss: 1.4619
[09/26 14:36:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 98.00	
[09/26 14:36:28 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 14:36:35 visual_prompt]: Epoch 18 / 100: avg data time: 4.69e-02, avg batch time: 0.4973, average train loss: 1.4010
[09/26 14:36:36 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1696, average loss: 1.4221
[09/26 14:36:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 99.00	
[09/26 14:36:36 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 14:36:43 visual_prompt]: Epoch 19 / 100: avg data time: 5.03e-02, avg batch time: 0.5001, average train loss: 1.3361
[09/26 14:36:45 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1694, average loss: 1.4410
[09/26 14:36:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.50	
[09/26 14:36:45 visual_prompt]: Best epoch 19: best metric: 0.350
[09/26 14:36:45 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 14:36:51 visual_prompt]: Epoch 20 / 100: avg data time: 4.27e-02, avg batch time: 0.4930, average train loss: 1.3889
[09/26 14:36:53 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.1697, average loss: 1.4536
[09/26 14:36:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 97.50	
[09/26 14:36:53 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 14:37:00 visual_prompt]: Epoch 21 / 100: avg data time: 4.49e-02, avg batch time: 0.4952, average train loss: 1.4342
[09/26 14:37:01 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1693, average loss: 1.8090
[09/26 14:37:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 93.50	
[09/26 14:37:01 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 14:37:08 visual_prompt]: Epoch 22 / 100: avg data time: 5.80e-02, avg batch time: 0.5078, average train loss: 1.4124
[09/26 14:37:10 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1692, average loss: 1.5212
[09/26 14:37:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 96.50	
[09/26 14:37:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 14:37:17 visual_prompt]: Epoch 23 / 100: avg data time: 5.50e-02, avg batch time: 0.5044, average train loss: 1.3570
[09/26 14:37:18 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1701, average loss: 1.5571
[09/26 14:37:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 96.00	
[09/26 14:37:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 14:37:25 visual_prompt]: Epoch 24 / 100: avg data time: 5.90e-02, avg batch time: 0.5086, average train loss: 1.3140
[09/26 14:37:27 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1696, average loss: 1.5762
[09/26 14:37:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 95.00	
[09/26 14:37:27 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 14:37:34 visual_prompt]: Epoch 25 / 100: avg data time: 6.13e-02, avg batch time: 0.5111, average train loss: 1.3160
[09/26 14:37:35 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1697, average loss: 1.3465
[09/26 14:37:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 14:37:35 visual_prompt]: Best epoch 25: best metric: 0.370
[09/26 14:37:35 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 14:37:42 visual_prompt]: Epoch 26 / 100: avg data time: 6.87e-02, avg batch time: 0.5185, average train loss: 1.3048
[09/26 14:37:44 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1692, average loss: 1.4441
[09/26 14:37:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.00	
[09/26 14:37:44 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 14:37:51 visual_prompt]: Epoch 27 / 100: avg data time: 6.25e-02, avg batch time: 0.5126, average train loss: 1.2823
[09/26 14:37:53 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 1.5150
[09/26 14:37:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.50	
[09/26 14:37:53 visual_prompt]: Best epoch 27: best metric: 0.375
[09/26 14:37:53 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 14:38:00 visual_prompt]: Epoch 28 / 100: avg data time: 6.18e-02, avg batch time: 0.5115, average train loss: 1.3651
[09/26 14:38:01 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1697, average loss: 1.5541
[09/26 14:38:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 97.50	
[09/26 14:38:01 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 14:38:08 visual_prompt]: Epoch 29 / 100: avg data time: 4.91e-02, avg batch time: 0.5002, average train loss: 1.3635
[09/26 14:38:09 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 1.8730
[09/26 14:38:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 95.50	
[09/26 14:38:09 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 14:38:16 visual_prompt]: Epoch 30 / 100: avg data time: 5.79e-02, avg batch time: 0.5077, average train loss: 1.5165
[09/26 14:38:18 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 1.6273
[09/26 14:38:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 95.50	
[09/26 14:38:18 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 14:38:25 visual_prompt]: Epoch 31 / 100: avg data time: 4.46e-02, avg batch time: 0.4937, average train loss: 1.4985
[09/26 14:38:26 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1698, average loss: 1.4480
[09/26 14:38:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.00	
[09/26 14:38:26 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 14:38:33 visual_prompt]: Epoch 32 / 100: avg data time: 5.58e-02, avg batch time: 0.5048, average train loss: 1.3758
[09/26 14:38:34 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1697, average loss: 1.4197
[09/26 14:38:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 14:38:34 visual_prompt]: Best epoch 32: best metric: 0.400
[09/26 14:38:34 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 14:38:41 visual_prompt]: Epoch 33 / 100: avg data time: 4.95e-02, avg batch time: 0.5012, average train loss: 1.2874
[09/26 14:38:43 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1697, average loss: 1.4311
[09/26 14:38:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 14:38:43 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 14:38:50 visual_prompt]: Epoch 34 / 100: avg data time: 5.10e-02, avg batch time: 0.5018, average train loss: 1.2833
[09/26 14:38:51 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1695, average loss: 1.3347
[09/26 14:38:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.50	
[09/26 14:38:51 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 14:38:58 visual_prompt]: Epoch 35 / 100: avg data time: 5.47e-02, avg batch time: 0.5055, average train loss: 1.2520
[09/26 14:39:00 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1697, average loss: 1.5177
[09/26 14:39:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 14:39:00 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 14:39:06 visual_prompt]: Epoch 36 / 100: avg data time: 6.07e-02, avg batch time: 0.5115, average train loss: 1.1926
[09/26 14:39:08 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1699, average loss: 1.7541
[09/26 14:39:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 93.50	
[09/26 14:39:08 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 14:39:15 visual_prompt]: Epoch 37 / 100: avg data time: 5.26e-02, avg batch time: 0.5025, average train loss: 1.8578
[09/26 14:39:17 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1697, average loss: 1.8201
[09/26 14:39:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/26 14:39:17 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 14:39:24 visual_prompt]: Epoch 38 / 100: avg data time: 5.55e-02, avg batch time: 0.5053, average train loss: 1.7380
[09/26 14:39:25 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1696, average loss: 1.7498
[09/26 14:39:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 94.00	
[09/26 14:39:25 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 14:39:32 visual_prompt]: Epoch 39 / 100: avg data time: 6.08e-02, avg batch time: 0.5094, average train loss: 1.7227
[09/26 14:39:34 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 1.8702
[09/26 14:39:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.00	top5: 84.00	
[09/26 14:39:34 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 14:39:41 visual_prompt]: Epoch 40 / 100: avg data time: 5.87e-02, avg batch time: 0.5085, average train loss: 1.7504
[09/26 14:39:42 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1695, average loss: 1.7357
[09/26 14:39:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 91.50	
[09/26 14:39:42 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 14:39:49 visual_prompt]: Epoch 41 / 100: avg data time: 4.64e-02, avg batch time: 0.4975, average train loss: 1.5445
[09/26 14:39:50 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1696, average loss: 1.8485
[09/26 14:39:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 94.00	
[09/26 14:39:50 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 14:39:57 visual_prompt]: Epoch 42 / 100: avg data time: 5.99e-02, avg batch time: 0.5106, average train loss: 1.8589
[09/26 14:39:59 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1691, average loss: 1.8228
[09/26 14:39:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 14:39:59 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 14:40:06 visual_prompt]: Epoch 43 / 100: avg data time: 5.45e-02, avg batch time: 0.5053, average train loss: 1.7680
[09/26 14:40:07 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1695, average loss: 1.8152
[09/26 14:40:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.00	
[09/26 14:40:07 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 14:40:14 visual_prompt]: Epoch 44 / 100: avg data time: 6.00e-02, avg batch time: 0.5101, average train loss: 1.7635
[09/26 14:40:15 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1697, average loss: 1.8006
[09/26 14:40:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 14:40:15 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 14:40:22 visual_prompt]: Epoch 45 / 100: avg data time: 5.53e-02, avg batch time: 0.5048, average train loss: 1.7522
[09/26 14:40:24 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 1.7664
[09/26 14:40:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 87.50	
[09/26 14:40:24 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 14:40:31 visual_prompt]: Epoch 46 / 100: avg data time: 5.79e-02, avg batch time: 0.5081, average train loss: 1.7190
[09/26 14:40:32 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1696, average loss: 1.7016
[09/26 14:40:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 91.00	
[09/26 14:40:32 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 14:40:39 visual_prompt]: Epoch 47 / 100: avg data time: 5.60e-02, avg batch time: 0.5056, average train loss: 1.7205
[09/26 14:40:41 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 1.6734
[09/26 14:40:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 94.50	
[09/26 14:40:41 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 14:40:48 visual_prompt]: Epoch 48 / 100: avg data time: 5.85e-02, avg batch time: 0.5078, average train loss: 1.5478
[09/26 14:40:49 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1694, average loss: 2.1017
[09/26 14:40:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 89.00	
[09/26 14:40:49 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 14:40:56 visual_prompt]: Epoch 49 / 100: avg data time: 5.11e-02, avg batch time: 0.5000, average train loss: 1.5343
[09/26 14:40:57 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1692, average loss: 1.5226
[09/26 14:40:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 95.50	
[09/26 14:40:57 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 14:41:04 visual_prompt]: Epoch 50 / 100: avg data time: 5.76e-02, avg batch time: 0.5071, average train loss: 1.4834
[09/26 14:41:06 visual_prompt]: Inference (val):avg data time: 4.60e-05, avg batch time: 0.1699, average loss: 1.6817
[09/26 14:41:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 90.00	
[09/26 14:41:06 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 14:41:13 visual_prompt]: Epoch 51 / 100: avg data time: 6.08e-02, avg batch time: 0.5093, average train loss: 1.4583
[09/26 14:41:14 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1700, average loss: 1.7113
[09/26 14:41:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 95.50	
[09/26 14:41:14 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 14:41:21 visual_prompt]: Epoch 52 / 100: avg data time: 5.03e-02, avg batch time: 0.5005, average train loss: 1.4080
[09/26 14:41:23 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1693, average loss: 1.4574
[09/26 14:41:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 97.00	
[09/26 14:41:23 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 14:41:30 visual_prompt]: Epoch 53 / 100: avg data time: 5.81e-02, avg batch time: 0.5065, average train loss: 1.3536
[09/26 14:41:31 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1698, average loss: 1.4727
[09/26 14:41:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 98.50	
[09/26 14:41:31 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 14:41:38 visual_prompt]: Epoch 54 / 100: avg data time: 5.60e-02, avg batch time: 0.5049, average train loss: 1.3514
[09/26 14:41:40 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1690, average loss: 1.4283
[09/26 14:41:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 99.50	
[09/26 14:41:40 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 14:41:47 visual_prompt]: Epoch 55 / 100: avg data time: 5.95e-02, avg batch time: 0.5080, average train loss: 1.5679
[09/26 14:41:48 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1693, average loss: 1.7009
[09/26 14:41:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 92.00	
[09/26 14:41:48 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 14:41:55 visual_prompt]: Epoch 56 / 100: avg data time: 5.60e-02, avg batch time: 0.5051, average train loss: 1.5959
[09/26 14:41:56 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1693, average loss: 1.4575
[09/26 14:41:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 98.00	
[09/26 14:41:56 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 14:42:03 visual_prompt]: Epoch 57 / 100: avg data time: 6.32e-02, avg batch time: 0.5118, average train loss: 1.4305
[09/26 14:42:05 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 1.4723
[09/26 14:42:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 97.50	
[09/26 14:42:05 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 14:42:12 visual_prompt]: Epoch 58 / 100: avg data time: 5.95e-02, avg batch time: 0.5083, average train loss: 1.3432
[09/26 14:42:13 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1694, average loss: 1.4080
[09/26 14:42:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 99.00	
[09/26 14:42:13 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 14:42:20 visual_prompt]: Epoch 59 / 100: avg data time: 4.88e-02, avg batch time: 0.4996, average train loss: 1.3178
[09/26 14:42:22 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 1.4219
[09/26 14:42:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 97.00	
[09/26 14:42:22 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 14:42:29 visual_prompt]: Epoch 60 / 100: avg data time: 5.29e-02, avg batch time: 0.5029, average train loss: 1.3120
[09/26 14:42:30 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 1.4564
[09/26 14:42:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.00	
[09/26 14:42:30 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 14:42:37 visual_prompt]: Epoch 61 / 100: avg data time: 5.77e-02, avg batch time: 0.5071, average train loss: 1.2673
[09/26 14:42:38 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1691, average loss: 1.5059
[09/26 14:42:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 97.50	
[09/26 14:42:38 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 14:42:45 visual_prompt]: Epoch 62 / 100: avg data time: 4.96e-02, avg batch time: 0.4993, average train loss: 1.2690
[09/26 14:42:47 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1697, average loss: 1.3842
[09/26 14:42:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 99.00	
[09/26 14:42:47 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 14:42:54 visual_prompt]: Epoch 63 / 100: avg data time: 5.76e-02, avg batch time: 0.5086, average train loss: 1.2664
[09/26 14:42:55 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 1.5357
[09/26 14:42:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 97.50	
[09/26 14:42:55 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 14:43:02 visual_prompt]: Epoch 64 / 100: avg data time: 6.40e-02, avg batch time: 0.5136, average train loss: 1.2989
[09/26 14:43:04 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 1.4424
[09/26 14:43:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.00	
[09/26 14:43:04 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 14:43:11 visual_prompt]: Epoch 65 / 100: avg data time: 5.52e-02, avg batch time: 0.5065, average train loss: 1.2084
[09/26 14:43:12 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1697, average loss: 1.4142
[09/26 14:43:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.50	
[09/26 14:43:12 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 14:43:19 visual_prompt]: Epoch 66 / 100: avg data time: 6.28e-02, avg batch time: 0.5115, average train loss: 1.2274
[09/26 14:43:21 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1695, average loss: 1.4709
[09/26 14:43:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 97.50	
[09/26 14:43:21 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 14:43:28 visual_prompt]: Epoch 67 / 100: avg data time: 5.80e-02, avg batch time: 0.5071, average train loss: 1.2201
[09/26 14:43:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1698, average loss: 1.3688
[09/26 14:43:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 14:43:29 visual_prompt]: Best epoch 67: best metric: 0.410
[09/26 14:43:29 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 14:43:36 visual_prompt]: Epoch 68 / 100: avg data time: 5.55e-02, avg batch time: 0.5048, average train loss: 1.2025
[09/26 14:43:38 visual_prompt]: Inference (val):avg data time: 4.86e-05, avg batch time: 0.1692, average loss: 1.3903
[09/26 14:43:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 98.00	
[09/26 14:43:38 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 14:43:45 visual_prompt]: Epoch 69 / 100: avg data time: 5.99e-02, avg batch time: 0.5086, average train loss: 1.1507
[09/26 14:43:46 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1695, average loss: 1.4503
[09/26 14:43:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 14:43:46 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 14:43:53 visual_prompt]: Epoch 70 / 100: avg data time: 5.61e-02, avg batch time: 0.5062, average train loss: 1.1166
[09/26 14:43:54 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1696, average loss: 1.5243
[09/26 14:43:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.50	
[09/26 14:43:54 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 14:44:01 visual_prompt]: Epoch 71 / 100: avg data time: 6.26e-02, avg batch time: 0.5113, average train loss: 1.1632
[09/26 14:44:03 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 1.4192
[09/26 14:44:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.00	
[09/26 14:44:03 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 14:44:10 visual_prompt]: Epoch 72 / 100: avg data time: 5.50e-02, avg batch time: 0.5040, average train loss: 1.2251
[09/26 14:44:11 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 1.3616
[09/26 14:44:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.50	
[09/26 14:44:11 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 14:44:18 visual_prompt]: Epoch 73 / 100: avg data time: 6.23e-02, avg batch time: 0.5109, average train loss: 1.2174
[09/26 14:44:20 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 1.4334
[09/26 14:44:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.00	
[09/26 14:44:20 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 14:44:27 visual_prompt]: Epoch 74 / 100: avg data time: 4.51e-02, avg batch time: 0.4965, average train loss: 1.0902
[09/26 14:44:28 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 1.4307
[09/26 14:44:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.00	
[09/26 14:44:28 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 14:44:35 visual_prompt]: Epoch 75 / 100: avg data time: 5.75e-02, avg batch time: 0.5063, average train loss: 1.1147
[09/26 14:44:37 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 1.3735
[09/26 14:44:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 14:44:37 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 14:44:44 visual_prompt]: Epoch 76 / 100: avg data time: 6.08e-02, avg batch time: 0.5094, average train loss: 1.0447
[09/26 14:44:45 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1698, average loss: 1.3803
[09/26 14:44:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 14:44:45 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 14:44:52 visual_prompt]: Epoch 77 / 100: avg data time: 5.58e-02, avg batch time: 0.5053, average train loss: 1.0285
[09/26 14:44:53 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1690, average loss: 1.3351
[09/26 14:44:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.00	
[09/26 14:44:53 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 14:45:00 visual_prompt]: Epoch 78 / 100: avg data time: 5.42e-02, avg batch time: 0.5040, average train loss: 0.9770
[09/26 14:45:02 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1693, average loss: 1.4624
[09/26 14:45:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 14:45:02 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 14:45:09 visual_prompt]: Epoch 79 / 100: avg data time: 6.20e-02, avg batch time: 0.5107, average train loss: 0.9678
[09/26 14:45:10 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 1.4402
[09/26 14:45:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.50	
[09/26 14:45:10 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 14:45:17 visual_prompt]: Epoch 80 / 100: avg data time: 5.11e-02, avg batch time: 0.4997, average train loss: 0.9552
[09/26 14:45:19 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.1701, average loss: 1.5363
[09/26 14:45:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 14:45:19 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 14:45:26 visual_prompt]: Epoch 81 / 100: avg data time: 5.39e-02, avg batch time: 0.5032, average train loss: 0.9241
[09/26 14:45:27 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1697, average loss: 1.4058
[09/26 14:45:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 14:45:27 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 14:45:34 visual_prompt]: Epoch 82 / 100: avg data time: 5.40e-02, avg batch time: 0.5034, average train loss: 0.9025
[09/26 14:45:36 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 1.5938
[09/26 14:45:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 14:45:36 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 14:45:42 visual_prompt]: Epoch 83 / 100: avg data time: 4.89e-02, avg batch time: 0.4982, average train loss: 0.8542
[09/26 14:45:44 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1697, average loss: 1.4679
[09/26 14:45:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 99.00	
[09/26 14:45:44 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 14:45:51 visual_prompt]: Epoch 84 / 100: avg data time: 5.05e-02, avg batch time: 0.5024, average train loss: 0.8133
[09/26 14:45:52 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1699, average loss: 1.5397
[09/26 14:45:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 14:45:52 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 14:45:59 visual_prompt]: Epoch 85 / 100: avg data time: 6.12e-02, avg batch time: 0.5103, average train loss: 0.7819
[09/26 14:46:01 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1694, average loss: 1.6069
[09/26 14:46:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.00	
[09/26 14:46:01 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 14:46:08 visual_prompt]: Epoch 86 / 100: avg data time: 6.54e-02, avg batch time: 0.5142, average train loss: 0.7559
[09/26 14:46:09 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1692, average loss: 1.5987
[09/26 14:46:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 14:46:09 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 14:46:16 visual_prompt]: Epoch 87 / 100: avg data time: 5.65e-02, avg batch time: 0.5052, average train loss: 0.7435
[09/26 14:46:18 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1699, average loss: 1.6492
[09/26 14:46:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 99.00	
[09/26 14:46:18 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 14:46:25 visual_prompt]: Epoch 88 / 100: avg data time: 6.26e-02, avg batch time: 0.5120, average train loss: 0.7356
[09/26 14:46:26 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1698, average loss: 1.6466
[09/26 14:46:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 99.00	
[09/26 14:46:26 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 14:46:33 visual_prompt]: Epoch 89 / 100: avg data time: 5.40e-02, avg batch time: 0.5029, average train loss: 0.7041
[09/26 14:46:35 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1699, average loss: 1.6915
[09/26 14:46:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 99.00	
[09/26 14:46:35 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 14:46:41 visual_prompt]: Epoch 90 / 100: avg data time: 5.25e-02, avg batch time: 0.5014, average train loss: 0.6672
[09/26 14:46:43 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1697, average loss: 1.6830
[09/26 14:46:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.00	
[09/26 14:46:43 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 14:46:50 visual_prompt]: Epoch 91 / 100: avg data time: 5.15e-02, avg batch time: 0.5030, average train loss: 0.6496
[09/26 14:46:51 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 1.6034
[09/26 14:46:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 14:46:51 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 14:46:58 visual_prompt]: Epoch 92 / 100: avg data time: 5.83e-02, avg batch time: 0.5076, average train loss: 0.6261
[09/26 14:47:00 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1695, average loss: 1.6565
[09/26 14:47:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.00	
[09/26 14:47:00 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 14:47:07 visual_prompt]: Epoch 93 / 100: avg data time: 6.53e-02, avg batch time: 0.5139, average train loss: 0.5956
[09/26 14:47:08 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 1.6267
[09/26 14:47:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 14:47:08 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 14:47:15 visual_prompt]: Epoch 94 / 100: avg data time: 5.70e-02, avg batch time: 0.5071, average train loss: 0.5713
[09/26 14:47:17 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1697, average loss: 1.6409
[09/26 14:47:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.00	
[09/26 14:47:17 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 14:47:24 visual_prompt]: Epoch 95 / 100: avg data time: 5.84e-02, avg batch time: 0.5073, average train loss: 0.5488
[09/26 14:47:25 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 1.6750
[09/26 14:47:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/26 14:47:25 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 14:47:32 visual_prompt]: Epoch 96 / 100: avg data time: 6.11e-02, avg batch time: 0.5101, average train loss: 0.5367
[09/26 14:47:34 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1697, average loss: 1.6787
[09/26 14:47:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 14:47:34 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 14:47:40 visual_prompt]: Epoch 97 / 100: avg data time: 5.65e-02, avg batch time: 0.5057, average train loss: 0.5131
[09/26 14:47:42 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1692, average loss: 1.6668
[09/26 14:47:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 14:47:42 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 14:47:49 visual_prompt]: Epoch 98 / 100: avg data time: 4.73e-02, avg batch time: 0.4963, average train loss: 0.5151
[09/26 14:47:50 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 1.6933
[09/26 14:47:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 14:47:50 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 14:47:57 visual_prompt]: Epoch 99 / 100: avg data time: 5.43e-02, avg batch time: 0.5040, average train loss: 0.5090
[09/26 14:47:59 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1697, average loss: 1.6903
[09/26 14:47:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 14:47:59 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 14:48:06 visual_prompt]: Epoch 100 / 100: avg data time: 5.85e-02, avg batch time: 0.5080, average train loss: 0.5024
[09/26 14:48:07 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1696, average loss: 1.6947
[09/26 14:48:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 14:48:07 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:48:07 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:48:07 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:48:07 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:48:07 visual_prompt]: Training with config:
[09/26 14:48:07 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:48:07 visual_prompt]: Loading training data...
[09/26 14:48:07 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 14:48:09 visual_prompt]: Number of images: 800
[09/26 14:48:09 visual_prompt]: Number of classes: 6 / 6
[09/26 14:48:09 visual_prompt]: Loading validation data...
[09/26 14:48:09 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 14:48:09 visual_prompt]: Number of images: 200
[09/26 14:48:09 visual_prompt]: Number of classes: 6 / 6
[09/26 14:48:09 visual_prompt]: Constructing models...
[09/26 14:48:11 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 14:48:11 visual_prompt]: tuned percent:0.540
[09/26 14:48:11 visual_prompt]: Device used for model: 0
[09/26 14:48:11 visual_prompt]: Setting up Evaluator...
[09/26 14:48:11 visual_prompt]: Setting up Trainer...
[09/26 14:48:11 visual_prompt]: 	Setting up the optimizer...
[09/26 14:48:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:48:18 visual_prompt]: Epoch 1 / 100: avg data time: 6.37e-02, avg batch time: 0.5105, average train loss: 2.9619
[09/26 14:48:20 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1690, average loss: 2.9268
[09/26 14:48:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 14:48:20 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 14:48:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 14:48:27 visual_prompt]: Epoch 2 / 100: avg data time: 5.44e-02, avg batch time: 0.5027, average train loss: 2.2243
[09/26 14:48:28 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1688, average loss: 1.9702
[09/26 14:48:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 14:48:28 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 14:48:35 visual_prompt]: Epoch 3 / 100: avg data time: 4.82e-02, avg batch time: 0.4957, average train loss: 1.8302
[09/26 14:48:37 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 1.8433
[09/26 14:48:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 87.00	
[09/26 14:48:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 14:48:43 visual_prompt]: Epoch 4 / 100: avg data time: 5.18e-02, avg batch time: 0.5012, average train loss: 1.7849
[09/26 14:48:45 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 1.8039
[09/26 14:48:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 84.00	
[09/26 14:48:45 visual_prompt]: Best epoch 4: best metric: 0.180
[09/26 14:48:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 14:48:52 visual_prompt]: Epoch 5 / 100: avg data time: 4.55e-02, avg batch time: 0.4956, average train loss: 1.7639
[09/26 14:48:53 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1692, average loss: 1.8172
[09/26 14:48:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 14:48:53 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 14:49:00 visual_prompt]: Epoch 6 / 100: avg data time: 5.81e-02, avg batch time: 0.5080, average train loss: 1.7374
[09/26 14:49:01 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1695, average loss: 1.7829
[09/26 14:49:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 88.50	
[09/26 14:49:01 visual_prompt]: Best epoch 6: best metric: 0.190
[09/26 14:49:01 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 14:49:08 visual_prompt]: Epoch 7 / 100: avg data time: 4.95e-02, avg batch time: 0.4991, average train loss: 1.6960
[09/26 14:49:10 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1694, average loss: 1.7560
[09/26 14:49:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 90.50	
[09/26 14:49:10 visual_prompt]: Best epoch 7: best metric: 0.200
[09/26 14:49:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 14:49:17 visual_prompt]: Epoch 8 / 100: avg data time: 4.99e-02, avg batch time: 0.5000, average train loss: 1.6233
[09/26 14:49:18 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1690, average loss: 1.7634
[09/26 14:49:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 87.00	
[09/26 14:49:18 visual_prompt]: Best epoch 8: best metric: 0.235
[09/26 14:49:18 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 14:49:25 visual_prompt]: Epoch 9 / 100: avg data time: 5.08e-02, avg batch time: 0.5010, average train loss: 1.6278
[09/26 14:49:27 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1697, average loss: 1.7620
[09/26 14:49:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 89.50	
[09/26 14:49:27 visual_prompt]: Best epoch 9: best metric: 0.265
[09/26 14:49:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 14:49:33 visual_prompt]: Epoch 10 / 100: avg data time: 5.14e-02, avg batch time: 0.5011, average train loss: 1.5931
[09/26 14:49:35 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1692, average loss: 1.6762
[09/26 14:49:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 91.00	
[09/26 14:49:35 visual_prompt]: Best epoch 10: best metric: 0.280
[09/26 14:49:35 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 14:49:42 visual_prompt]: Epoch 11 / 100: avg data time: 6.17e-02, avg batch time: 0.5110, average train loss: 1.5321
[09/26 14:49:43 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1694, average loss: 1.6712
[09/26 14:49:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 93.50	
[09/26 14:49:43 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 14:49:50 visual_prompt]: Epoch 12 / 100: avg data time: 5.73e-02, avg batch time: 0.5070, average train loss: 1.4609
[09/26 14:49:52 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1696, average loss: 1.7000
[09/26 14:49:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 91.50	
[09/26 14:49:52 visual_prompt]: Best epoch 12: best metric: 0.315
[09/26 14:49:52 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 14:49:59 visual_prompt]: Epoch 13 / 100: avg data time: 5.87e-02, avg batch time: 0.5077, average train loss: 1.4347
[09/26 14:50:00 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 1.7351
[09/26 14:50:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 93.50	
[09/26 14:50:00 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 14:50:07 visual_prompt]: Epoch 14 / 100: avg data time: 5.42e-02, avg batch time: 0.5035, average train loss: 1.4591
[09/26 14:50:09 visual_prompt]: Inference (val):avg data time: 5.01e-05, avg batch time: 0.1693, average loss: 1.5734
[09/26 14:50:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 93.00	
[09/26 14:50:09 visual_prompt]: Best epoch 14: best metric: 0.320
[09/26 14:50:09 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 14:50:16 visual_prompt]: Epoch 15 / 100: avg data time: 5.44e-02, avg batch time: 0.5040, average train loss: 1.3933
[09/26 14:50:17 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1698, average loss: 1.8383
[09/26 14:50:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 93.50	
[09/26 14:50:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 14:50:24 visual_prompt]: Epoch 16 / 100: avg data time: 5.31e-02, avg batch time: 0.5020, average train loss: 1.3792
[09/26 14:50:25 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 1.5149
[09/26 14:50:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 96.00	
[09/26 14:50:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 14:50:32 visual_prompt]: Epoch 17 / 100: avg data time: 5.39e-02, avg batch time: 0.5041, average train loss: 1.2870
[09/26 14:50:34 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 1.5146
[09/26 14:50:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 97.50	
[09/26 14:50:34 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 14:50:41 visual_prompt]: Epoch 18 / 100: avg data time: 5.76e-02, avg batch time: 0.5072, average train loss: 1.2478
[09/26 14:50:42 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 1.5066
[09/26 14:50:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 96.50	
[09/26 14:50:42 visual_prompt]: Best epoch 18: best metric: 0.340
[09/26 14:50:42 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 14:50:49 visual_prompt]: Epoch 19 / 100: avg data time: 4.34e-02, avg batch time: 0.4941, average train loss: 1.2145
[09/26 14:50:50 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1690, average loss: 1.6033
[09/26 14:50:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 96.50	
[09/26 14:50:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 14:50:57 visual_prompt]: Epoch 20 / 100: avg data time: 4.40e-02, avg batch time: 0.4962, average train loss: 1.2328
[09/26 14:50:59 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 1.4820
[09/26 14:50:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 98.50	
[09/26 14:50:59 visual_prompt]: Best epoch 20: best metric: 0.355
[09/26 14:50:59 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 14:51:06 visual_prompt]: Epoch 21 / 100: avg data time: 4.39e-02, avg batch time: 0.4950, average train loss: 1.1579
[09/26 14:51:07 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1697, average loss: 1.8158
[09/26 14:51:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 98.00	
[09/26 14:51:07 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 14:51:14 visual_prompt]: Epoch 22 / 100: avg data time: 5.33e-02, avg batch time: 0.5028, average train loss: 1.1079
[09/26 14:51:16 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1694, average loss: 1.5296
[09/26 14:51:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 14:51:16 visual_prompt]: Best epoch 22: best metric: 0.385
[09/26 14:51:16 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 14:51:22 visual_prompt]: Epoch 23 / 100: avg data time: 4.37e-02, avg batch time: 0.4957, average train loss: 1.0615
[09/26 14:51:24 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1697, average loss: 1.4599
[09/26 14:51:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 14:51:24 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 14:51:31 visual_prompt]: Epoch 24 / 100: avg data time: 6.15e-02, avg batch time: 0.5102, average train loss: 0.9858
[09/26 14:51:32 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1695, average loss: 1.4850
[09/26 14:51:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 14:51:32 visual_prompt]: Best epoch 24: best metric: 0.395
[09/26 14:51:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 14:51:39 visual_prompt]: Epoch 25 / 100: avg data time: 5.59e-02, avg batch time: 0.5061, average train loss: 0.9199
[09/26 14:51:41 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.1695, average loss: 2.0500
[09/26 14:51:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 98.00	
[09/26 14:51:41 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 14:51:48 visual_prompt]: Epoch 26 / 100: avg data time: 5.21e-02, avg batch time: 0.5022, average train loss: 1.0263
[09/26 14:51:49 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 1.6400
[09/26 14:51:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 14:51:49 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 14:51:56 visual_prompt]: Epoch 27 / 100: avg data time: 6.10e-02, avg batch time: 0.5109, average train loss: 0.9921
[09/26 14:51:58 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1697, average loss: 1.6293
[09/26 14:51:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 14:51:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 14:52:05 visual_prompt]: Epoch 28 / 100: avg data time: 6.69e-02, avg batch time: 0.5161, average train loss: 0.8869
[09/26 14:52:06 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1700, average loss: 1.7214
[09/26 14:52:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 14:52:06 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 14:52:13 visual_prompt]: Epoch 29 / 100: avg data time: 4.80e-02, avg batch time: 0.4987, average train loss: 0.8123
[09/26 14:52:14 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1691, average loss: 1.7871
[09/26 14:52:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.50	
[09/26 14:52:14 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 14:52:21 visual_prompt]: Epoch 30 / 100: avg data time: 5.68e-02, avg batch time: 0.5069, average train loss: 0.9587
[09/26 14:52:23 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 1.7412
[09/26 14:52:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.00	
[09/26 14:52:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 14:52:30 visual_prompt]: Epoch 31 / 100: avg data time: 5.19e-02, avg batch time: 0.5012, average train loss: 0.9300
[09/26 14:52:31 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1694, average loss: 1.7721
[09/26 14:52:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.00	
[09/26 14:52:31 visual_prompt]: Best epoch 31: best metric: 0.410
[09/26 14:52:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 14:52:38 visual_prompt]: Epoch 32 / 100: avg data time: 6.05e-02, avg batch time: 0.5096, average train loss: 0.9028
[09/26 14:52:40 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 1.8932
[09/26 14:52:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 14:52:40 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 14:52:47 visual_prompt]: Epoch 33 / 100: avg data time: 5.37e-02, avg batch time: 0.5035, average train loss: 0.7585
[09/26 14:52:48 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 1.8401
[09/26 14:52:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 14:52:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 14:52:55 visual_prompt]: Epoch 34 / 100: avg data time: 4.45e-02, avg batch time: 0.4965, average train loss: 0.7722
[09/26 14:52:56 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1695, average loss: 2.0901
[09/26 14:52:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 98.00	
[09/26 14:52:56 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 14:53:03 visual_prompt]: Epoch 35 / 100: avg data time: 5.22e-02, avg batch time: 0.5019, average train loss: 0.8051
[09/26 14:53:05 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1695, average loss: 1.5517
[09/26 14:53:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.50	
[09/26 14:53:05 visual_prompt]: Best epoch 35: best metric: 0.415
[09/26 14:53:05 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 14:53:12 visual_prompt]: Epoch 36 / 100: avg data time: 4.71e-02, avg batch time: 0.4976, average train loss: 0.7124
[09/26 14:53:13 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 1.7105
[09/26 14:53:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.50	
[09/26 14:53:13 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 14:53:20 visual_prompt]: Epoch 37 / 100: avg data time: 5.32e-02, avg batch time: 0.5026, average train loss: 0.5789
[09/26 14:53:22 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1700, average loss: 2.1584
[09/26 14:53:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 14:53:22 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 14:53:29 visual_prompt]: Epoch 38 / 100: avg data time: 6.10e-02, avg batch time: 0.5098, average train loss: 0.4446
[09/26 14:53:30 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1694, average loss: 2.1986
[09/26 14:53:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.50	
[09/26 14:53:30 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 14:53:37 visual_prompt]: Epoch 39 / 100: avg data time: 4.72e-02, avg batch time: 0.4992, average train loss: 0.4701
[09/26 14:53:38 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 2.6385
[09/26 14:53:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 14:53:38 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 14:53:45 visual_prompt]: Epoch 40 / 100: avg data time: 5.37e-02, avg batch time: 0.5041, average train loss: 0.4774
[09/26 14:53:47 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1700, average loss: 2.3227
[09/26 14:53:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 14:53:47 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 14:53:54 visual_prompt]: Epoch 41 / 100: avg data time: 5.32e-02, avg batch time: 0.5040, average train loss: 0.5616
[09/26 14:53:55 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1698, average loss: 2.3357
[09/26 14:53:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 96.50	
[09/26 14:53:55 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 14:54:02 visual_prompt]: Epoch 42 / 100: avg data time: 6.10e-02, avg batch time: 0.5100, average train loss: 0.4643
[09/26 14:54:04 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 2.4912
[09/26 14:54:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.00	
[09/26 14:54:04 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 14:54:11 visual_prompt]: Epoch 43 / 100: avg data time: 5.83e-02, avg batch time: 0.5087, average train loss: 0.3925
[09/26 14:54:12 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1694, average loss: 2.8600
[09/26 14:54:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 14:54:12 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 14:54:19 visual_prompt]: Epoch 44 / 100: avg data time: 6.00e-02, avg batch time: 0.5090, average train loss: 0.5249
[09/26 14:54:21 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.1698, average loss: 2.2662
[09/26 14:54:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 94.00	
[09/26 14:54:21 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 14:54:27 visual_prompt]: Epoch 45 / 100: avg data time: 4.84e-02, avg batch time: 0.4990, average train loss: 0.4715
[09/26 14:54:29 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1698, average loss: 2.4743
[09/26 14:54:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 14:54:29 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 14:54:36 visual_prompt]: Epoch 46 / 100: avg data time: 5.04e-02, avg batch time: 0.5009, average train loss: 0.3119
[09/26 14:54:38 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 3.3783
[09/26 14:54:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 95.50	
[09/26 14:54:38 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 14:54:45 visual_prompt]: Epoch 47 / 100: avg data time: 6.55e-02, avg batch time: 0.5144, average train loss: 0.2264
[09/26 14:54:46 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 3.3693
[09/26 14:54:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.00	
[09/26 14:54:46 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 14:54:53 visual_prompt]: Epoch 48 / 100: avg data time: 5.69e-02, avg batch time: 0.5060, average train loss: 0.2985
[09/26 14:54:55 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1691, average loss: 3.3469
[09/26 14:54:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 14:54:55 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 14:55:02 visual_prompt]: Epoch 49 / 100: avg data time: 5.12e-02, avg batch time: 0.5003, average train loss: 0.3628
[09/26 14:55:03 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1695, average loss: 2.8166
[09/26 14:55:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.00	
[09/26 14:55:03 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 14:55:10 visual_prompt]: Epoch 50 / 100: avg data time: 4.82e-02, avg batch time: 0.4978, average train loss: 0.3194
[09/26 14:55:11 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1692, average loss: 2.8606
[09/26 14:55:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.00	
[09/26 14:55:11 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 14:55:18 visual_prompt]: Epoch 51 / 100: avg data time: 5.75e-02, avg batch time: 0.5065, average train loss: 0.2785
[09/26 14:55:20 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 2.8704
[09/26 14:55:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 97.50	
[09/26 14:55:20 visual_prompt]: Best epoch 51: best metric: 0.435
[09/26 14:55:20 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 14:55:27 visual_prompt]: Epoch 52 / 100: avg data time: 5.81e-02, avg batch time: 0.5078, average train loss: 0.1771
[09/26 14:55:28 visual_prompt]: Inference (val):avg data time: 4.89e-05, avg batch time: 0.1695, average loss: 2.9418
[09/26 14:55:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.50	
[09/26 14:55:28 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 14:55:35 visual_prompt]: Epoch 53 / 100: avg data time: 6.49e-02, avg batch time: 0.5147, average train loss: 0.1648
[09/26 14:55:37 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 3.9148
[09/26 14:55:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 95.50	
[09/26 14:55:37 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 14:55:44 visual_prompt]: Epoch 54 / 100: avg data time: 5.64e-02, avg batch time: 0.5058, average train loss: 0.2407
[09/26 14:55:45 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1693, average loss: 3.1328
[09/26 14:55:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 14:55:45 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 14:55:52 visual_prompt]: Epoch 55 / 100: avg data time: 4.73e-02, avg batch time: 0.4972, average train loss: 0.2160
[09/26 14:55:54 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1699, average loss: 3.3640
[09/26 14:55:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 14:55:54 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 14:56:01 visual_prompt]: Epoch 56 / 100: avg data time: 5.64e-02, avg batch time: 0.5069, average train loss: 0.1527
[09/26 14:56:02 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1699, average loss: 3.5198
[09/26 14:56:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 97.50	
[09/26 14:56:02 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 14:56:09 visual_prompt]: Epoch 57 / 100: avg data time: 6.22e-02, avg batch time: 0.5122, average train loss: 0.1353
[09/26 14:56:11 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1696, average loss: 3.3013
[09/26 14:56:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.00	
[09/26 14:56:11 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 14:56:18 visual_prompt]: Epoch 58 / 100: avg data time: 6.16e-02, avg batch time: 0.5117, average train loss: 0.1073
[09/26 14:56:19 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1696, average loss: 3.7057
[09/26 14:56:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/26 14:56:19 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 14:56:26 visual_prompt]: Epoch 59 / 100: avg data time: 4.49e-02, avg batch time: 0.4941, average train loss: 0.1350
[09/26 14:56:28 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1694, average loss: 3.7696
[09/26 14:56:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 94.50	
[09/26 14:56:28 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 14:56:34 visual_prompt]: Epoch 60 / 100: avg data time: 4.87e-02, avg batch time: 0.4993, average train loss: 0.0899
[09/26 14:56:36 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 4.2689
[09/26 14:56:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 95.00	
[09/26 14:56:36 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 14:56:43 visual_prompt]: Epoch 61 / 100: avg data time: 5.44e-02, avg batch time: 0.5043, average train loss: 0.0679
[09/26 14:56:44 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 4.2978
[09/26 14:56:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.50	
[09/26 14:56:44 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 14:56:51 visual_prompt]: Epoch 62 / 100: avg data time: 4.48e-02, avg batch time: 0.4951, average train loss: 0.0471
[09/26 14:56:53 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1692, average loss: 4.1880
[09/26 14:56:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 96.00	
[09/26 14:56:53 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 14:56:59 visual_prompt]: Epoch 63 / 100: avg data time: 4.46e-02, avg batch time: 0.4938, average train loss: 0.0238
[09/26 14:57:01 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1697, average loss: 4.5469
[09/26 14:57:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.50	
[09/26 14:57:01 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 14:57:08 visual_prompt]: Epoch 64 / 100: avg data time: 5.36e-02, avg batch time: 0.5033, average train loss: 0.0371
[09/26 14:57:09 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.1695, average loss: 4.7350
[09/26 14:57:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.00	
[09/26 14:57:09 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 14:57:16 visual_prompt]: Epoch 65 / 100: avg data time: 4.10e-02, avg batch time: 0.4917, average train loss: 0.0254
[09/26 14:57:17 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1698, average loss: 4.5780
[09/26 14:57:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.00	
[09/26 14:57:17 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 14:57:24 visual_prompt]: Epoch 66 / 100: avg data time: 5.40e-02, avg batch time: 0.5032, average train loss: 0.0084
[09/26 14:57:26 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1698, average loss: 5.1375
[09/26 14:57:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 14:57:26 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 14:57:33 visual_prompt]: Epoch 67 / 100: avg data time: 5.44e-02, avg batch time: 0.5041, average train loss: 0.0137
[09/26 14:57:34 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1695, average loss: 4.9401
[09/26 14:57:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 14:57:34 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 14:57:41 visual_prompt]: Epoch 68 / 100: avg data time: 6.08e-02, avg batch time: 0.5096, average train loss: 0.0082
[09/26 14:57:43 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1699, average loss: 4.7587
[09/26 14:57:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 14:57:43 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 14:57:50 visual_prompt]: Epoch 69 / 100: avg data time: 6.09e-02, avg batch time: 0.5100, average train loss: 0.0042
[09/26 14:57:51 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1695, average loss: 4.7753
[09/26 14:57:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 14:57:51 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 14:57:58 visual_prompt]: Epoch 70 / 100: avg data time: 5.48e-02, avg batch time: 0.5039, average train loss: 0.0091
[09/26 14:58:00 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1701, average loss: 5.1842
[09/26 14:58:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.00	
[09/26 14:58:00 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 14:58:06 visual_prompt]: Epoch 71 / 100: avg data time: 4.62e-02, avg batch time: 0.4975, average train loss: 0.0034
[09/26 14:58:08 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1693, average loss: 5.1255
[09/26 14:58:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.50	
[09/26 14:58:08 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 14:58:15 visual_prompt]: Epoch 72 / 100: avg data time: 5.60e-02, avg batch time: 0.5056, average train loss: 0.0064
[09/26 14:58:16 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1698, average loss: 4.9619
[09/26 14:58:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 14:58:16 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 14:58:23 visual_prompt]: Epoch 73 / 100: avg data time: 5.86e-02, avg batch time: 0.5086, average train loss: 0.0029
[09/26 14:58:25 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1694, average loss: 4.9858
[09/26 14:58:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.00	
[09/26 14:58:25 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 14:58:32 visual_prompt]: Epoch 74 / 100: avg data time: 5.02e-02, avg batch time: 0.5003, average train loss: 0.0025
[09/26 14:58:33 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1697, average loss: 4.9985
[09/26 14:58:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 14:58:33 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 14:58:40 visual_prompt]: Epoch 75 / 100: avg data time: 4.87e-02, avg batch time: 0.4990, average train loss: 0.0022
[09/26 14:58:41 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1698, average loss: 5.1068
[09/26 14:58:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.50	
[09/26 14:58:41 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 14:58:48 visual_prompt]: Epoch 76 / 100: avg data time: 5.83e-02, avg batch time: 0.5074, average train loss: 0.0039
[09/26 14:58:50 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1695, average loss: 5.1078
[09/26 14:58:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 14:58:50 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 14:58:57 visual_prompt]: Epoch 77 / 100: avg data time: 5.74e-02, avg batch time: 0.5062, average train loss: 0.0013
[09/26 14:58:58 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1698, average loss: 5.0838
[09/26 14:58:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 14:58:58 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 14:59:05 visual_prompt]: Epoch 78 / 100: avg data time: 5.26e-02, avg batch time: 0.5018, average train loss: 0.0045
[09/26 14:59:06 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1695, average loss: 5.1512
[09/26 14:59:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.00	
[09/26 14:59:06 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 14:59:13 visual_prompt]: Epoch 79 / 100: avg data time: 5.95e-02, avg batch time: 0.5101, average train loss: 0.0139
[09/26 14:59:15 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 5.1236
[09/26 14:59:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.50	
[09/26 14:59:15 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 14:59:22 visual_prompt]: Epoch 80 / 100: avg data time: 5.60e-02, avg batch time: 0.5053, average train loss: 0.0021
[09/26 14:59:23 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1697, average loss: 5.0634
[09/26 14:59:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 14:59:23 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 14:59:30 visual_prompt]: Epoch 81 / 100: avg data time: 6.40e-02, avg batch time: 0.5148, average train loss: 0.0029
[09/26 14:59:32 visual_prompt]: Inference (val):avg data time: 4.28e-05, avg batch time: 0.1699, average loss: 5.0507
[09/26 14:59:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.00	
[09/26 14:59:32 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 14:59:39 visual_prompt]: Epoch 82 / 100: avg data time: 5.24e-02, avg batch time: 0.5022, average train loss: 0.0034
[09/26 14:59:40 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1697, average loss: 5.0604
[09/26 14:59:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 14:59:40 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 14:59:47 visual_prompt]: Epoch 83 / 100: avg data time: 4.87e-02, avg batch time: 0.5006, average train loss: 0.0031
[09/26 14:59:49 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 5.0749
[09/26 14:59:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 14:59:49 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 14:59:56 visual_prompt]: Epoch 84 / 100: avg data time: 6.11e-02, avg batch time: 0.5107, average train loss: 0.0025
[09/26 14:59:57 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1697, average loss: 5.0843
[09/26 14:59:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.00	
[09/26 14:59:57 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 15:00:04 visual_prompt]: Epoch 85 / 100: avg data time: 4.50e-02, avg batch time: 0.4964, average train loss: 0.0013
[09/26 15:00:06 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1695, average loss: 5.0902
[09/26 15:00:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.00	
[09/26 15:00:06 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 15:00:12 visual_prompt]: Epoch 86 / 100: avg data time: 5.11e-02, avg batch time: 0.5012, average train loss: 0.0016
[09/26 15:00:14 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1696, average loss: 5.0962
[09/26 15:00:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.00	
[09/26 15:00:14 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 15:00:21 visual_prompt]: Epoch 87 / 100: avg data time: 4.59e-02, avg batch time: 0.4957, average train loss: 0.0050
[09/26 15:00:22 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 5.1713
[09/26 15:00:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 15:00:22 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 15:00:29 visual_prompt]: Epoch 88 / 100: avg data time: 6.17e-02, avg batch time: 0.5109, average train loss: 0.0012
[09/26 15:00:31 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1696, average loss: 5.1911
[09/26 15:00:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 15:00:31 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 15:00:37 visual_prompt]: Epoch 89 / 100: avg data time: 4.54e-02, avg batch time: 0.4964, average train loss: 0.0015
[09/26 15:00:39 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1693, average loss: 5.1906
[09/26 15:00:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 15:00:39 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 15:00:46 visual_prompt]: Epoch 90 / 100: avg data time: 5.69e-02, avg batch time: 0.5088, average train loss: 0.0015
[09/26 15:00:47 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1697, average loss: 5.1816
[09/26 15:00:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:00:47 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 15:00:54 visual_prompt]: Epoch 91 / 100: avg data time: 6.22e-02, avg batch time: 0.5112, average train loss: 0.0011
[09/26 15:00:56 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1700, average loss: 5.1791
[09/26 15:00:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:00:56 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 15:01:03 visual_prompt]: Epoch 92 / 100: avg data time: 5.63e-02, avg batch time: 0.5060, average train loss: 0.0014
[09/26 15:01:04 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1694, average loss: 5.1772
[09/26 15:01:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:01:04 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 15:01:11 visual_prompt]: Epoch 93 / 100: avg data time: 5.20e-02, avg batch time: 0.5032, average train loss: 0.0014
[09/26 15:01:13 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1696, average loss: 5.1773
[09/26 15:01:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:01:13 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 15:01:20 visual_prompt]: Epoch 94 / 100: avg data time: 5.01e-02, avg batch time: 0.4998, average train loss: 0.0012
[09/26 15:01:21 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 5.1797
[09/26 15:01:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:01:21 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 15:01:28 visual_prompt]: Epoch 95 / 100: avg data time: 5.66e-02, avg batch time: 0.5063, average train loss: 0.0011
[09/26 15:01:29 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1699, average loss: 5.1797
[09/26 15:01:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:01:29 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 15:01:36 visual_prompt]: Epoch 96 / 100: avg data time: 5.45e-02, avg batch time: 0.5057, average train loss: 0.0012
[09/26 15:01:38 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 5.1796
[09/26 15:01:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:01:38 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 15:01:45 visual_prompt]: Epoch 97 / 100: avg data time: 4.37e-02, avg batch time: 0.4950, average train loss: 0.0012
[09/26 15:01:46 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1697, average loss: 5.1792
[09/26 15:01:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:01:46 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 15:01:53 visual_prompt]: Epoch 98 / 100: avg data time: 4.37e-02, avg batch time: 0.4929, average train loss: 0.0017
[09/26 15:01:54 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1694, average loss: 5.1784
[09/26 15:01:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:01:54 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 15:02:01 visual_prompt]: Epoch 99 / 100: avg data time: 5.13e-02, avg batch time: 0.5015, average train loss: 0.0012
[09/26 15:02:03 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 5.1782
[09/26 15:02:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:02:03 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 15:02:10 visual_prompt]: Epoch 100 / 100: avg data time: 5.94e-02, avg batch time: 0.5094, average train loss: 0.0014
[09/26 15:02:11 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1695, average loss: 5.1782
[09/26 15:02:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:02:11 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 15:02:11 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 15:02:11 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 15:02:11 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 15:02:11 visual_prompt]: Training with config:
[09/26 15:02:11 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 15:02:11 visual_prompt]: Loading training data...
[09/26 15:02:11 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 15:02:13 visual_prompt]: Number of images: 800
[09/26 15:02:13 visual_prompt]: Number of classes: 6 / 6
[09/26 15:02:13 visual_prompt]: Loading validation data...
[09/26 15:02:13 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 15:02:13 visual_prompt]: Number of images: 200
[09/26 15:02:13 visual_prompt]: Number of classes: 6 / 6
[09/26 15:02:13 visual_prompt]: Constructing models...
[09/26 15:02:16 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 15:02:16 visual_prompt]: tuned percent:0.540
[09/26 15:02:16 visual_prompt]: Device used for model: 0
[09/26 15:02:16 visual_prompt]: Setting up Evaluator...
[09/26 15:02:16 visual_prompt]: Setting up Trainer...
[09/26 15:02:16 visual_prompt]: 	Setting up the optimizer...
[09/26 15:02:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 15:02:22 visual_prompt]: Epoch 1 / 100: avg data time: 4.33e-02, avg batch time: 0.4928, average train loss: 2.9742
[09/26 15:02:24 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1691, average loss: 2.9268
[09/26 15:02:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 15:02:24 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 15:02:24 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 15:02:31 visual_prompt]: Epoch 2 / 100: avg data time: 5.35e-02, avg batch time: 0.5012, average train loss: 2.2004
[09/26 15:02:32 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1689, average loss: 1.9226
[09/26 15:02:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 15:02:32 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 15:02:39 visual_prompt]: Epoch 3 / 100: avg data time: 4.24e-02, avg batch time: 0.4939, average train loss: 1.7921
[09/26 15:02:40 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1691, average loss: 1.8456
[09/26 15:02:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 83.50	
[09/26 15:02:40 visual_prompt]: Best epoch 3: best metric: 0.180
[09/26 15:02:40 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 15:02:47 visual_prompt]: Epoch 4 / 100: avg data time: 5.40e-02, avg batch time: 0.5037, average train loss: 1.7887
[09/26 15:02:49 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1692, average loss: 1.8031
[09/26 15:02:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 85.00	
[09/26 15:02:49 visual_prompt]: Best epoch 4: best metric: 0.200
[09/26 15:02:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 15:02:56 visual_prompt]: Epoch 5 / 100: avg data time: 5.67e-02, avg batch time: 0.5058, average train loss: 1.7513
[09/26 15:02:57 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1692, average loss: 1.7716
[09/26 15:02:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 87.00	
[09/26 15:02:57 visual_prompt]: Best epoch 5: best metric: 0.240
[09/26 15:02:57 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 15:03:04 visual_prompt]: Epoch 6 / 100: avg data time: 5.96e-02, avg batch time: 0.5077, average train loss: 1.7247
[09/26 15:03:06 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1691, average loss: 1.7479
[09/26 15:03:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 85.50	
[09/26 15:03:06 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 15:03:12 visual_prompt]: Epoch 7 / 100: avg data time: 4.71e-02, avg batch time: 0.4964, average train loss: 1.6920
[09/26 15:03:14 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 1.7620
[09/26 15:03:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 86.50	
[09/26 15:03:14 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 15:03:21 visual_prompt]: Epoch 8 / 100: avg data time: 4.31e-02, avg batch time: 0.4923, average train loss: 1.6303
[09/26 15:03:22 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1694, average loss: 1.8412
[09/26 15:03:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 89.00	
[09/26 15:03:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 15:03:29 visual_prompt]: Epoch 9 / 100: avg data time: 5.01e-02, avg batch time: 0.4998, average train loss: 1.6913
[09/26 15:03:31 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1690, average loss: 1.8516
[09/26 15:03:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 85.50	
[09/26 15:03:31 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 15:03:37 visual_prompt]: Epoch 10 / 100: avg data time: 5.75e-02, avg batch time: 0.5066, average train loss: 1.6109
[09/26 15:03:39 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 1.7414
[09/26 15:03:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 92.00	
[09/26 15:03:39 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 15:03:46 visual_prompt]: Epoch 11 / 100: avg data time: 4.56e-02, avg batch time: 0.4959, average train loss: 1.5552
[09/26 15:03:47 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 1.7431
[09/26 15:03:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 87.50	
[09/26 15:03:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 15:03:54 visual_prompt]: Epoch 12 / 100: avg data time: 4.54e-02, avg batch time: 0.4953, average train loss: 1.4764
[09/26 15:03:55 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 1.7360
[09/26 15:03:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 88.00	
[09/26 15:03:55 visual_prompt]: Best epoch 12: best metric: 0.245
[09/26 15:03:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 15:04:02 visual_prompt]: Epoch 13 / 100: avg data time: 4.78e-02, avg batch time: 0.4982, average train loss: 1.4673
[09/26 15:04:04 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1697, average loss: 1.7862
[09/26 15:04:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 90.00	
[09/26 15:04:04 visual_prompt]: Best epoch 13: best metric: 0.275
[09/26 15:04:04 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 15:04:11 visual_prompt]: Epoch 14 / 100: avg data time: 5.97e-02, avg batch time: 0.5094, average train loss: 1.3940
[09/26 15:04:12 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1696, average loss: 1.6596
[09/26 15:04:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 91.00	
[09/26 15:04:12 visual_prompt]: Best epoch 14: best metric: 0.285
[09/26 15:04:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 15:04:19 visual_prompt]: Epoch 15 / 100: avg data time: 5.09e-02, avg batch time: 0.5004, average train loss: 1.3665
[09/26 15:04:21 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1695, average loss: 1.8241
[09/26 15:04:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 87.50	
[09/26 15:04:21 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 15:04:28 visual_prompt]: Epoch 16 / 100: avg data time: 6.01e-02, avg batch time: 0.5091, average train loss: 1.3586
[09/26 15:04:29 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1699, average loss: 1.5985
[09/26 15:04:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 96.00	
[09/26 15:04:29 visual_prompt]: Best epoch 16: best metric: 0.340
[09/26 15:04:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 15:04:36 visual_prompt]: Epoch 17 / 100: avg data time: 4.43e-02, avg batch time: 0.4940, average train loss: 1.3119
[09/26 15:04:37 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1700, average loss: 1.6656
[09/26 15:04:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 91.00	
[09/26 15:04:37 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 15:04:44 visual_prompt]: Epoch 18 / 100: avg data time: 5.66e-02, avg batch time: 0.5064, average train loss: 1.3624
[09/26 15:04:46 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1696, average loss: 1.5447
[09/26 15:04:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.00	
[09/26 15:04:46 visual_prompt]: Best epoch 18: best metric: 0.350
[09/26 15:04:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 15:04:53 visual_prompt]: Epoch 19 / 100: avg data time: 6.01e-02, avg batch time: 0.5098, average train loss: 1.2904
[09/26 15:04:54 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1697, average loss: 1.7079
[09/26 15:04:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 95.50	
[09/26 15:04:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 15:05:01 visual_prompt]: Epoch 20 / 100: avg data time: 4.23e-02, avg batch time: 0.4928, average train loss: 1.2099
[09/26 15:05:03 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1695, average loss: 1.4902
[09/26 15:05:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 95.50	
[09/26 15:05:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 15:05:10 visual_prompt]: Epoch 21 / 100: avg data time: 6.53e-02, avg batch time: 0.5147, average train loss: 1.1210
[09/26 15:05:11 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1698, average loss: 1.4069
[09/26 15:05:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 15:05:11 visual_prompt]: Best epoch 21: best metric: 0.385
[09/26 15:05:11 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 15:05:18 visual_prompt]: Epoch 22 / 100: avg data time: 5.06e-02, avg batch time: 0.5012, average train loss: 1.0583
[09/26 15:05:20 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1697, average loss: 1.4679
[09/26 15:05:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 15:05:20 visual_prompt]: Best epoch 22: best metric: 0.395
[09/26 15:05:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 15:05:27 visual_prompt]: Epoch 23 / 100: avg data time: 6.19e-02, avg batch time: 0.5117, average train loss: 0.9812
[09/26 15:05:28 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1696, average loss: 1.8039
[09/26 15:05:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 15:05:28 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 15:05:35 visual_prompt]: Epoch 24 / 100: avg data time: 5.77e-02, avg batch time: 0.5080, average train loss: 1.0910
[09/26 15:05:37 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1692, average loss: 1.7455
[09/26 15:05:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 95.00	
[09/26 15:05:37 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 15:05:44 visual_prompt]: Epoch 25 / 100: avg data time: 6.02e-02, avg batch time: 0.5091, average train loss: 1.0579
[09/26 15:05:45 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 1.5209
[09/26 15:05:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/26 15:05:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 15:05:52 visual_prompt]: Epoch 26 / 100: avg data time: 5.50e-02, avg batch time: 0.5051, average train loss: 0.9741
[09/26 15:05:53 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 1.5229
[09/26 15:05:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.50	
[09/26 15:05:53 visual_prompt]: Best epoch 26: best metric: 0.420
[09/26 15:05:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 15:06:00 visual_prompt]: Epoch 27 / 100: avg data time: 4.78e-02, avg batch time: 0.4995, average train loss: 0.8924
[09/26 15:06:02 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1695, average loss: 1.5335
[09/26 15:06:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.50	
[09/26 15:06:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 15:06:08 visual_prompt]: Epoch 28 / 100: avg data time: 4.61e-02, avg batch time: 0.4977, average train loss: 0.8457
[09/26 15:06:10 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1700, average loss: 1.6828
[09/26 15:06:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.50	
[09/26 15:06:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 15:06:17 visual_prompt]: Epoch 29 / 100: avg data time: 5.88e-02, avg batch time: 0.5094, average train loss: 0.7710
[09/26 15:06:18 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1694, average loss: 1.5847
[09/26 15:06:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 15:06:18 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 15:06:25 visual_prompt]: Epoch 30 / 100: avg data time: 5.25e-02, avg batch time: 0.5032, average train loss: 0.7092
[09/26 15:06:27 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1699, average loss: 1.8717
[09/26 15:06:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 99.00	
[09/26 15:06:27 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 15:06:34 visual_prompt]: Epoch 31 / 100: avg data time: 6.27e-02, avg batch time: 0.5123, average train loss: 0.7191
[09/26 15:06:35 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 2.1405
[09/26 15:06:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.50	
[09/26 15:06:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 15:06:42 visual_prompt]: Epoch 32 / 100: avg data time: 5.65e-02, avg batch time: 0.5071, average train loss: 0.6927
[09/26 15:06:44 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1694, average loss: 1.8745
[09/26 15:06:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 15:06:44 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 15:06:51 visual_prompt]: Epoch 33 / 100: avg data time: 5.10e-02, avg batch time: 0.5011, average train loss: 0.6550
[09/26 15:06:52 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1697, average loss: 1.7803
[09/26 15:06:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 98.00	
[09/26 15:06:52 visual_prompt]: Best epoch 33: best metric: 0.425
[09/26 15:06:52 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 15:06:59 visual_prompt]: Epoch 34 / 100: avg data time: 6.36e-02, avg batch time: 0.5126, average train loss: 0.7717
[09/26 15:07:01 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1697, average loss: 1.9195
[09/26 15:07:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 15:07:01 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 15:07:08 visual_prompt]: Epoch 35 / 100: avg data time: 5.70e-02, avg batch time: 0.5063, average train loss: 0.7078
[09/26 15:07:09 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 2.0675
[09/26 15:07:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.50	
[09/26 15:07:09 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 15:07:16 visual_prompt]: Epoch 36 / 100: avg data time: 4.60e-02, avg batch time: 0.4963, average train loss: 0.6151
[09/26 15:07:17 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1696, average loss: 2.0248
[09/26 15:07:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.00	
[09/26 15:07:17 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 15:07:24 visual_prompt]: Epoch 37 / 100: avg data time: 5.93e-02, avg batch time: 0.5095, average train loss: 0.4564
[09/26 15:07:26 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1696, average loss: 2.3404
[09/26 15:07:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 98.00	
[09/26 15:07:26 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 15:07:33 visual_prompt]: Epoch 38 / 100: avg data time: 5.27e-02, avg batch time: 0.5025, average train loss: 0.4074
[09/26 15:07:34 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1695, average loss: 2.6198
[09/26 15:07:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 96.00	
[09/26 15:07:34 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 15:07:41 visual_prompt]: Epoch 39 / 100: avg data time: 5.09e-02, avg batch time: 0.5017, average train loss: 0.4058
[09/26 15:07:43 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1695, average loss: 2.7675
[09/26 15:07:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 15:07:43 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 15:07:49 visual_prompt]: Epoch 40 / 100: avg data time: 4.73e-02, avg batch time: 0.4989, average train loss: 0.4210
[09/26 15:07:51 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 2.6692
[09/26 15:07:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.00	
[09/26 15:07:51 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 15:07:58 visual_prompt]: Epoch 41 / 100: avg data time: 5.06e-02, avg batch time: 0.5007, average train loss: 0.4092
[09/26 15:07:59 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1696, average loss: 2.8619
[09/26 15:07:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 96.50	
[09/26 15:07:59 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 15:08:06 visual_prompt]: Epoch 42 / 100: avg data time: 4.91e-02, avg batch time: 0.4999, average train loss: 0.4335
[09/26 15:08:08 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1695, average loss: 2.3866
[09/26 15:08:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 15:08:08 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 15:08:14 visual_prompt]: Epoch 43 / 100: avg data time: 5.85e-02, avg batch time: 0.5090, average train loss: 0.5502
[09/26 15:08:16 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1697, average loss: 2.4506
[09/26 15:08:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.50	
[09/26 15:08:16 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 15:08:23 visual_prompt]: Epoch 44 / 100: avg data time: 5.59e-02, avg batch time: 0.5058, average train loss: 0.3871
[09/26 15:08:24 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1693, average loss: 2.3230
[09/26 15:08:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 95.50	
[09/26 15:08:24 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 15:08:31 visual_prompt]: Epoch 45 / 100: avg data time: 6.59e-02, avg batch time: 0.5150, average train loss: 0.2985
[09/26 15:08:33 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1691, average loss: 2.8089
[09/26 15:08:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 15:08:33 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 15:08:40 visual_prompt]: Epoch 46 / 100: avg data time: 5.76e-02, avg batch time: 0.5074, average train loss: 0.2684
[09/26 15:08:41 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1692, average loss: 3.0694
[09/26 15:08:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 15:08:41 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 15:08:48 visual_prompt]: Epoch 47 / 100: avg data time: 6.60e-02, avg batch time: 0.5150, average train loss: 0.2904
[09/26 15:08:50 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1698, average loss: 3.0655
[09/26 15:08:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 94.00	
[09/26 15:08:50 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 15:08:57 visual_prompt]: Epoch 48 / 100: avg data time: 6.49e-02, avg batch time: 0.5139, average train loss: 0.2655
[09/26 15:08:59 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1695, average loss: 3.2965
[09/26 15:08:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:08:59 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 15:09:06 visual_prompt]: Epoch 49 / 100: avg data time: 6.76e-02, avg batch time: 0.5166, average train loss: 0.2057
[09/26 15:09:07 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 3.4901
[09/26 15:09:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.50	
[09/26 15:09:07 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 15:09:14 visual_prompt]: Epoch 50 / 100: avg data time: 5.00e-02, avg batch time: 0.5000, average train loss: 0.3156
[09/26 15:09:16 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1690, average loss: 3.4014
[09/26 15:09:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.00	
[09/26 15:09:16 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 15:09:22 visual_prompt]: Epoch 51 / 100: avg data time: 5.06e-02, avg batch time: 0.5002, average train loss: 0.2949
[09/26 15:09:24 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 3.0868
[09/26 15:09:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:09:24 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 15:09:31 visual_prompt]: Epoch 52 / 100: avg data time: 4.92e-02, avg batch time: 0.5007, average train loss: 0.1946
[09/26 15:09:32 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1698, average loss: 3.3259
[09/26 15:09:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.00	
[09/26 15:09:32 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 15:09:39 visual_prompt]: Epoch 53 / 100: avg data time: 5.94e-02, avg batch time: 0.5106, average train loss: 0.1462
[09/26 15:09:41 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1697, average loss: 3.7844
[09/26 15:09:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.00	
[09/26 15:09:41 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 15:09:48 visual_prompt]: Epoch 54 / 100: avg data time: 5.64e-02, avg batch time: 0.5068, average train loss: 0.1183
[09/26 15:09:49 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1693, average loss: 3.7857
[09/26 15:09:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 95.50	
[09/26 15:09:49 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 15:09:56 visual_prompt]: Epoch 55 / 100: avg data time: 5.37e-02, avg batch time: 0.5026, average train loss: 0.1155
[09/26 15:09:58 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1695, average loss: 4.1343
[09/26 15:09:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:09:58 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 15:10:05 visual_prompt]: Epoch 56 / 100: avg data time: 6.04e-02, avg batch time: 0.5106, average train loss: 0.0986
[09/26 15:10:06 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1691, average loss: 4.1181
[09/26 15:10:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.00	
[09/26 15:10:06 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 15:10:13 visual_prompt]: Epoch 57 / 100: avg data time: 5.49e-02, avg batch time: 0.5049, average train loss: 0.0799
[09/26 15:10:14 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1693, average loss: 3.9819
[09/26 15:10:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.00	
[09/26 15:10:14 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 15:10:21 visual_prompt]: Epoch 58 / 100: avg data time: 5.58e-02, avg batch time: 0.5054, average train loss: 0.0666
[09/26 15:10:23 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1694, average loss: 4.3299
[09/26 15:10:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.00	
[09/26 15:10:23 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 15:10:30 visual_prompt]: Epoch 59 / 100: avg data time: 5.73e-02, avg batch time: 0.5073, average train loss: 0.0653
[09/26 15:10:31 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 4.5754
[09/26 15:10:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.00	
[09/26 15:10:31 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 15:10:38 visual_prompt]: Epoch 60 / 100: avg data time: 5.76e-02, avg batch time: 0.5073, average train loss: 0.0694
[09/26 15:10:40 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1696, average loss: 4.0287
[09/26 15:10:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.00	
[09/26 15:10:40 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 15:10:47 visual_prompt]: Epoch 61 / 100: avg data time: 6.17e-02, avg batch time: 0.5109, average train loss: 0.0637
[09/26 15:10:48 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1770, average loss: 3.9995
[09/26 15:10:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 15:10:48 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 15:10:55 visual_prompt]: Epoch 62 / 100: avg data time: 4.98e-02, avg batch time: 0.5011, average train loss: 0.0648
[09/26 15:10:57 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1699, average loss: 4.2445
[09/26 15:10:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 96.50	
[09/26 15:10:57 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 15:11:04 visual_prompt]: Epoch 63 / 100: avg data time: 5.56e-02, avg batch time: 0.5060, average train loss: 0.0412
[09/26 15:11:05 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 4.5749
[09/26 15:11:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 98.00	
[09/26 15:11:05 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 15:11:12 visual_prompt]: Epoch 64 / 100: avg data time: 5.62e-02, avg batch time: 0.5058, average train loss: 0.0306
[09/26 15:11:14 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 4.7818
[09/26 15:11:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.00	
[09/26 15:11:14 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 15:11:20 visual_prompt]: Epoch 65 / 100: avg data time: 5.20e-02, avg batch time: 0.5023, average train loss: 0.0154
[09/26 15:11:22 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1696, average loss: 5.2074
[09/26 15:11:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:11:22 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 15:11:29 visual_prompt]: Epoch 66 / 100: avg data time: 4.94e-02, avg batch time: 0.5001, average train loss: 0.0094
[09/26 15:11:30 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1692, average loss: 5.1232
[09/26 15:11:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.50	
[09/26 15:11:30 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 15:11:37 visual_prompt]: Epoch 67 / 100: avg data time: 4.47e-02, avg batch time: 0.4958, average train loss: 0.0079
[09/26 15:11:39 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1694, average loss: 5.1106
[09/26 15:11:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/26 15:11:39 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 15:11:45 visual_prompt]: Epoch 68 / 100: avg data time: 4.78e-02, avg batch time: 0.4973, average train loss: 0.0048
[09/26 15:11:47 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1697, average loss: 5.1482
[09/26 15:11:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.00	
[09/26 15:11:47 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 15:11:54 visual_prompt]: Epoch 69 / 100: avg data time: 4.81e-02, avg batch time: 0.4983, average train loss: 0.0045
[09/26 15:11:55 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1696, average loss: 5.3639
[09/26 15:11:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 15:11:55 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 15:12:02 visual_prompt]: Epoch 70 / 100: avg data time: 5.59e-02, avg batch time: 0.5064, average train loss: 0.0066
[09/26 15:12:04 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1694, average loss: 5.5781
[09/26 15:12:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.00	
[09/26 15:12:04 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 15:12:11 visual_prompt]: Epoch 71 / 100: avg data time: 5.76e-02, avg batch time: 0.5085, average train loss: 0.0226
[09/26 15:12:12 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1698, average loss: 5.1664
[09/26 15:12:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 15:12:12 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 15:12:19 visual_prompt]: Epoch 72 / 100: avg data time: 6.51e-02, avg batch time: 0.5140, average train loss: 0.0115
[09/26 15:12:21 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1692, average loss: 5.3193
[09/26 15:12:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.00	
[09/26 15:12:21 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 15:12:28 visual_prompt]: Epoch 73 / 100: avg data time: 4.60e-02, avg batch time: 0.4960, average train loss: 0.0051
[09/26 15:12:29 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1696, average loss: 5.3321
[09/26 15:12:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 15:12:29 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 15:12:36 visual_prompt]: Epoch 74 / 100: avg data time: 4.30e-02, avg batch time: 0.4933, average train loss: 0.0060
[09/26 15:12:38 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1693, average loss: 5.2355
[09/26 15:12:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 15:12:38 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 15:12:44 visual_prompt]: Epoch 75 / 100: avg data time: 4.83e-02, avg batch time: 0.4985, average train loss: 0.0051
[09/26 15:12:46 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 5.2659
[09/26 15:12:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 15:12:46 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 15:12:53 visual_prompt]: Epoch 76 / 100: avg data time: 4.68e-02, avg batch time: 0.4982, average train loss: 0.0036
[09/26 15:12:54 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1697, average loss: 5.2720
[09/26 15:12:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:12:54 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 15:13:01 visual_prompt]: Epoch 77 / 100: avg data time: 6.30e-02, avg batch time: 0.5131, average train loss: 0.0027
[09/26 15:13:03 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1697, average loss: 5.3386
[09/26 15:13:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:13:03 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 15:13:10 visual_prompt]: Epoch 78 / 100: avg data time: 5.65e-02, avg batch time: 0.5059, average train loss: 0.0030
[09/26 15:13:11 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 5.3924
[09/26 15:13:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:13:11 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 15:13:18 visual_prompt]: Epoch 79 / 100: avg data time: 4.79e-02, avg batch time: 0.4993, average train loss: 0.0014
[09/26 15:13:19 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 5.4110
[09/26 15:13:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 15:13:19 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 15:13:26 visual_prompt]: Epoch 80 / 100: avg data time: 4.48e-02, avg batch time: 0.4944, average train loss: 0.0029
[09/26 15:13:28 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 5.4727
[09/26 15:13:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 15:13:28 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 15:13:34 visual_prompt]: Epoch 81 / 100: avg data time: 4.50e-02, avg batch time: 0.4959, average train loss: 0.0030
[09/26 15:13:36 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 5.4558
[09/26 15:13:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:13:36 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 15:13:43 visual_prompt]: Epoch 82 / 100: avg data time: 4.98e-02, avg batch time: 0.5005, average train loss: 0.0030
[09/26 15:13:44 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 5.4944
[09/26 15:13:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 15:13:44 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 15:13:51 visual_prompt]: Epoch 83 / 100: avg data time: 5.81e-02, avg batch time: 0.5081, average train loss: 0.0017
[09/26 15:13:53 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1697, average loss: 5.5264
[09/26 15:13:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 15:13:53 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 15:14:00 visual_prompt]: Epoch 84 / 100: avg data time: 5.19e-02, avg batch time: 0.5019, average train loss: 0.0021
[09/26 15:14:01 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1691, average loss: 5.5361
[09/26 15:14:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 15:14:01 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 15:14:08 visual_prompt]: Epoch 85 / 100: avg data time: 5.44e-02, avg batch time: 0.5041, average train loss: 0.0018
[09/26 15:14:10 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1696, average loss: 5.5478
[09/26 15:14:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:14:10 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 15:14:16 visual_prompt]: Epoch 86 / 100: avg data time: 4.41e-02, avg batch time: 0.4948, average train loss: 0.0019
[09/26 15:14:18 visual_prompt]: Inference (val):avg data time: 4.75e-05, avg batch time: 0.1700, average loss: 5.5357
[09/26 15:14:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 15:14:18 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 15:14:25 visual_prompt]: Epoch 87 / 100: avg data time: 5.59e-02, avg batch time: 0.5049, average train loss: 0.0016
[09/26 15:14:26 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1694, average loss: 5.5394
[09/26 15:14:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 15:14:26 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 15:14:33 visual_prompt]: Epoch 88 / 100: avg data time: 4.99e-02, avg batch time: 0.4999, average train loss: 0.0014
[09/26 15:14:35 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 5.5447
[09/26 15:14:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 15:14:35 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 15:14:41 visual_prompt]: Epoch 89 / 100: avg data time: 5.34e-02, avg batch time: 0.5025, average train loss: 0.0016
[09/26 15:14:43 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1698, average loss: 5.5566
[09/26 15:14:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:14:43 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 15:14:50 visual_prompt]: Epoch 90 / 100: avg data time: 6.24e-02, avg batch time: 0.5121, average train loss: 0.0014
[09/26 15:14:52 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 5.5621
[09/26 15:14:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 15:14:52 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 15:14:58 visual_prompt]: Epoch 91 / 100: avg data time: 5.81e-02, avg batch time: 0.5070, average train loss: 0.0015
[09/26 15:15:00 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1698, average loss: 5.5653
[09/26 15:15:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:15:00 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 15:15:07 visual_prompt]: Epoch 92 / 100: avg data time: 4.87e-02, avg batch time: 0.4993, average train loss: 0.0014
[09/26 15:15:08 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1691, average loss: 5.5697
[09/26 15:15:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:15:08 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 15:15:15 visual_prompt]: Epoch 93 / 100: avg data time: 5.20e-02, avg batch time: 0.5016, average train loss: 0.0015
[09/26 15:15:17 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1694, average loss: 5.5726
[09/26 15:15:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:15:17 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 15:15:24 visual_prompt]: Epoch 94 / 100: avg data time: 5.89e-02, avg batch time: 0.5091, average train loss: 0.0016
[09/26 15:15:25 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1697, average loss: 5.5738
[09/26 15:15:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:15:25 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 15:15:32 visual_prompt]: Epoch 95 / 100: avg data time: 4.43e-02, avg batch time: 0.4942, average train loss: 0.0015
[09/26 15:15:33 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1695, average loss: 5.5751
[09/26 15:15:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:15:34 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 15:15:40 visual_prompt]: Epoch 96 / 100: avg data time: 5.74e-02, avg batch time: 0.5077, average train loss: 0.0021
[09/26 15:15:42 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 5.5726
[09/26 15:15:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:15:42 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 15:15:49 visual_prompt]: Epoch 97 / 100: avg data time: 5.66e-02, avg batch time: 0.5071, average train loss: 0.0015
[09/26 15:15:50 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1695, average loss: 5.5729
[09/26 15:15:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:15:50 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 15:15:57 visual_prompt]: Epoch 98 / 100: avg data time: 4.93e-02, avg batch time: 0.5018, average train loss: 0.0014
[09/26 15:15:59 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1694, average loss: 5.5736
[09/26 15:15:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:15:59 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 15:16:06 visual_prompt]: Epoch 99 / 100: avg data time: 5.33e-02, avg batch time: 0.5030, average train loss: 0.0014
[09/26 15:16:07 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1698, average loss: 5.5739
[09/26 15:16:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:16:07 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 15:16:14 visual_prompt]: Epoch 100 / 100: avg data time: 5.68e-02, avg batch time: 0.5055, average train loss: 0.0020
[09/26 15:16:16 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1696, average loss: 5.5740
[09/26 15:16:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:16:16 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 15:16:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 15:16:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 15:16:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 15:16:16 visual_prompt]: Training with config:
[09/26 15:16:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 15:16:16 visual_prompt]: Loading training data...
[09/26 15:16:16 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 15:16:17 visual_prompt]: Number of images: 800
[09/26 15:16:17 visual_prompt]: Number of classes: 6 / 6
[09/26 15:16:17 visual_prompt]: Loading validation data...
[09/26 15:16:17 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 15:16:17 visual_prompt]: Number of images: 200
[09/26 15:16:17 visual_prompt]: Number of classes: 6 / 6
[09/26 15:16:17 visual_prompt]: Constructing models...
[09/26 15:16:20 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 15:16:20 visual_prompt]: tuned percent:0.540
[09/26 15:16:20 visual_prompt]: Device used for model: 0
[09/26 15:16:20 visual_prompt]: Setting up Evaluator...
[09/26 15:16:20 visual_prompt]: Setting up Trainer...
[09/26 15:16:20 visual_prompt]: 	Setting up the optimizer...
[09/26 15:16:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 15:16:27 visual_prompt]: Epoch 1 / 100: avg data time: 5.36e-02, avg batch time: 0.5011, average train loss: 2.9777
[09/26 15:16:28 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.1688, average loss: 2.9268
[09/26 15:16:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 15:16:28 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 15:16:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 15:16:35 visual_prompt]: Epoch 2 / 100: avg data time: 5.67e-02, avg batch time: 0.5039, average train loss: 2.4089
[09/26 15:16:37 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1687, average loss: 2.0118
[09/26 15:16:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 82.50	
[09/26 15:16:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 15:16:44 visual_prompt]: Epoch 3 / 100: avg data time: 6.06e-02, avg batch time: 0.5095, average train loss: 1.8725
[09/26 15:16:45 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1689, average loss: 1.8414
[09/26 15:16:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.00	top5: 86.50	
[09/26 15:16:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 15:16:52 visual_prompt]: Epoch 4 / 100: avg data time: 6.16e-02, avg batch time: 0.5093, average train loss: 1.7798
[09/26 15:16:54 visual_prompt]: Inference (val):avg data time: 4.51e-05, avg batch time: 0.1692, average loss: 1.8305
[09/26 15:16:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 15:16:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 15:17:01 visual_prompt]: Epoch 5 / 100: avg data time: 6.03e-02, avg batch time: 0.5083, average train loss: 1.7691
[09/26 15:17:02 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1690, average loss: 1.8019
[09/26 15:17:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 84.00	
[09/26 15:17:02 visual_prompt]: Best epoch 5: best metric: 0.225
[09/26 15:17:02 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 15:17:09 visual_prompt]: Epoch 6 / 100: avg data time: 4.26e-02, avg batch time: 0.4909, average train loss: 1.7553
[09/26 15:17:10 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1691, average loss: 1.7670
[09/26 15:17:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 86.50	
[09/26 15:17:10 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 15:17:17 visual_prompt]: Epoch 7 / 100: avg data time: 4.36e-02, avg batch time: 0.4934, average train loss: 1.6939
[09/26 15:17:19 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1689, average loss: 1.7160
[09/26 15:17:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 87.50	
[09/26 15:17:19 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 15:17:26 visual_prompt]: Epoch 8 / 100: avg data time: 5.28e-02, avg batch time: 0.5024, average train loss: 1.6477
[09/26 15:17:27 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 1.7110
[09/26 15:17:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 90.00	
[09/26 15:17:27 visual_prompt]: Best epoch 8: best metric: 0.245
[09/26 15:17:27 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 15:17:34 visual_prompt]: Epoch 9 / 100: avg data time: 4.64e-02, avg batch time: 0.4961, average train loss: 1.6156
[09/26 15:17:35 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 1.6793
[09/26 15:17:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 87.50	
[09/26 15:17:35 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 15:17:42 visual_prompt]: Epoch 10 / 100: avg data time: 5.87e-02, avg batch time: 0.5074, average train loss: 1.5808
[09/26 15:17:44 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1696, average loss: 1.8180
[09/26 15:17:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 92.00	
[09/26 15:17:44 visual_prompt]: Best epoch 10: best metric: 0.275
[09/26 15:17:44 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 15:17:50 visual_prompt]: Epoch 11 / 100: avg data time: 4.49e-02, avg batch time: 0.4949, average train loss: 1.5705
[09/26 15:17:52 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1694, average loss: 1.6974
[09/26 15:17:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 91.50	
[09/26 15:17:52 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 15:17:59 visual_prompt]: Epoch 12 / 100: avg data time: 6.33e-02, avg batch time: 0.5142, average train loss: 1.5746
[09/26 15:18:01 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 1.6305
[09/26 15:18:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 91.00	
[09/26 15:18:01 visual_prompt]: Best epoch 12: best metric: 0.295
[09/26 15:18:01 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 15:18:07 visual_prompt]: Epoch 13 / 100: avg data time: 4.70e-02, avg batch time: 0.4972, average train loss: 1.4830
[09/26 15:18:09 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 1.6562
[09/26 15:18:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 93.00	
[09/26 15:18:09 visual_prompt]: Best epoch 13: best metric: 0.310
[09/26 15:18:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 15:18:16 visual_prompt]: Epoch 14 / 100: avg data time: 6.08e-02, avg batch time: 0.5097, average train loss: 1.4262
[09/26 15:18:17 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 1.6524
[09/26 15:18:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 94.50	
[09/26 15:18:17 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 15:18:24 visual_prompt]: Epoch 15 / 100: avg data time: 5.83e-02, avg batch time: 0.5083, average train loss: 1.3930
[09/26 15:18:26 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1693, average loss: 1.6614
[09/26 15:18:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 96.00	
[09/26 15:18:26 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 15:18:33 visual_prompt]: Epoch 16 / 100: avg data time: 4.99e-02, avg batch time: 0.4985, average train loss: 1.3706
[09/26 15:18:34 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1694, average loss: 1.5968
[09/26 15:18:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 94.50	
[09/26 15:18:34 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 15:18:41 visual_prompt]: Epoch 17 / 100: avg data time: 4.50e-02, avg batch time: 0.4949, average train loss: 1.3101
[09/26 15:18:43 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1695, average loss: 1.6790
[09/26 15:18:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 95.50	
[09/26 15:18:43 visual_prompt]: Best epoch 17: best metric: 0.315
[09/26 15:18:43 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 15:18:49 visual_prompt]: Epoch 18 / 100: avg data time: 5.09e-02, avg batch time: 0.5014, average train loss: 1.2808
[09/26 15:18:51 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 1.6686
[09/26 15:18:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.00	
[09/26 15:18:51 visual_prompt]: Best epoch 18: best metric: 0.330
[09/26 15:18:51 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 15:18:58 visual_prompt]: Epoch 19 / 100: avg data time: 6.04e-02, avg batch time: 0.5090, average train loss: 1.2521
[09/26 15:18:59 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 1.5420
[09/26 15:18:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.50	
[09/26 15:18:59 visual_prompt]: Best epoch 19: best metric: 0.355
[09/26 15:18:59 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 15:19:06 visual_prompt]: Epoch 20 / 100: avg data time: 5.87e-02, avg batch time: 0.5091, average train loss: 1.1116
[09/26 15:19:08 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1693, average loss: 1.5144
[09/26 15:19:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 15:19:08 visual_prompt]: Best epoch 20: best metric: 0.375
[09/26 15:19:08 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 15:19:15 visual_prompt]: Epoch 21 / 100: avg data time: 5.57e-02, avg batch time: 0.5056, average train loss: 1.0822
[09/26 15:19:16 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1696, average loss: 1.5578
[09/26 15:19:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.00	
[09/26 15:19:16 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 15:19:23 visual_prompt]: Epoch 22 / 100: avg data time: 6.09e-02, avg batch time: 0.5104, average train loss: 1.0660
[09/26 15:19:25 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1700, average loss: 1.5791
[09/26 15:19:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.50	
[09/26 15:19:25 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 15:19:32 visual_prompt]: Epoch 23 / 100: avg data time: 5.73e-02, avg batch time: 0.5063, average train loss: 1.0839
[09/26 15:19:33 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1695, average loss: 1.5389
[09/26 15:19:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 15:19:33 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 15:19:40 visual_prompt]: Epoch 24 / 100: avg data time: 5.68e-02, avg batch time: 0.5070, average train loss: 1.0045
[09/26 15:19:42 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 1.5568
[09/26 15:19:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 15:19:42 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 15:19:48 visual_prompt]: Epoch 25 / 100: avg data time: 4.56e-02, avg batch time: 0.4951, average train loss: 0.8973
[09/26 15:19:50 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1693, average loss: 1.6408
[09/26 15:19:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 15:19:50 visual_prompt]: Best epoch 25: best metric: 0.395
[09/26 15:19:50 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 15:19:57 visual_prompt]: Epoch 26 / 100: avg data time: 4.64e-02, avg batch time: 0.4973, average train loss: 0.9919
[09/26 15:19:58 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1695, average loss: 1.6742
[09/26 15:19:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 97.50	
[09/26 15:19:58 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 15:20:05 visual_prompt]: Epoch 27 / 100: avg data time: 5.65e-02, avg batch time: 0.5059, average train loss: 0.8473
[09/26 15:20:07 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1698, average loss: 1.7753
[09/26 15:20:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.50	
[09/26 15:20:07 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 15:20:13 visual_prompt]: Epoch 28 / 100: avg data time: 5.45e-02, avg batch time: 0.5042, average train loss: 0.7335
[09/26 15:20:15 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1699, average loss: 1.8114
[09/26 15:20:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 15:20:15 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 15:20:22 visual_prompt]: Epoch 29 / 100: avg data time: 6.38e-02, avg batch time: 0.5135, average train loss: 0.7334
[09/26 15:20:24 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 1.9324
[09/26 15:20:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 98.50	
[09/26 15:20:24 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 15:20:31 visual_prompt]: Epoch 30 / 100: avg data time: 6.02e-02, avg batch time: 0.5093, average train loss: 0.7102
[09/26 15:20:32 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1693, average loss: 1.8780
[09/26 15:20:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 15:20:32 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 15:20:39 visual_prompt]: Epoch 31 / 100: avg data time: 5.99e-02, avg batch time: 0.5085, average train loss: 0.6352
[09/26 15:20:41 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1692, average loss: 2.1923
[09/26 15:20:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 15:20:41 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 15:20:47 visual_prompt]: Epoch 32 / 100: avg data time: 4.85e-02, avg batch time: 0.4990, average train loss: 0.6339
[09/26 15:20:49 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 2.1205
[09/26 15:20:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 97.50	
[09/26 15:20:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 15:20:56 visual_prompt]: Epoch 33 / 100: avg data time: 5.89e-02, avg batch time: 0.5101, average train loss: 0.6502
[09/26 15:20:57 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1693, average loss: 2.1749
[09/26 15:20:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.00	
[09/26 15:20:57 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 15:21:04 visual_prompt]: Epoch 34 / 100: avg data time: 5.77e-02, avg batch time: 0.5073, average train loss: 0.5415
[09/26 15:21:06 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 2.2104
[09/26 15:21:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.00	
[09/26 15:21:06 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 15:21:13 visual_prompt]: Epoch 35 / 100: avg data time: 5.66e-02, avg batch time: 0.5062, average train loss: 0.5076
[09/26 15:21:14 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1690, average loss: 2.5367
[09/26 15:21:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 15:21:14 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 15:21:21 visual_prompt]: Epoch 36 / 100: avg data time: 6.37e-02, avg batch time: 0.5132, average train loss: 0.4296
[09/26 15:21:23 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 2.5113
[09/26 15:21:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 15:21:23 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 15:21:30 visual_prompt]: Epoch 37 / 100: avg data time: 5.66e-02, avg batch time: 0.5064, average train loss: 0.4285
[09/26 15:21:31 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1698, average loss: 2.7686
[09/26 15:21:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 95.50	
[09/26 15:21:31 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 15:21:38 visual_prompt]: Epoch 38 / 100: avg data time: 5.65e-02, avg batch time: 0.5059, average train loss: 0.3475
[09/26 15:21:40 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1693, average loss: 3.0407
[09/26 15:21:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 97.00	
[09/26 15:21:40 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 15:21:46 visual_prompt]: Epoch 39 / 100: avg data time: 5.30e-02, avg batch time: 0.5035, average train loss: 0.3359
[09/26 15:21:48 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 2.8472
[09/26 15:21:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 15:21:48 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 15:21:55 visual_prompt]: Epoch 40 / 100: avg data time: 5.56e-02, avg batch time: 0.5058, average train loss: 0.3660
[09/26 15:21:56 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 2.9898
[09/26 15:21:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 95.00	
[09/26 15:21:56 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 15:22:03 visual_prompt]: Epoch 41 / 100: avg data time: 5.29e-02, avg batch time: 0.5025, average train loss: 0.3516
[09/26 15:22:05 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1697, average loss: 2.6451
[09/26 15:22:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 95.00	
[09/26 15:22:05 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 15:22:12 visual_prompt]: Epoch 42 / 100: avg data time: 5.78e-02, avg batch time: 0.5069, average train loss: 0.2654
[09/26 15:22:13 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1699, average loss: 3.8559
[09/26 15:22:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.00	
[09/26 15:22:13 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 15:22:20 visual_prompt]: Epoch 43 / 100: avg data time: 6.15e-02, avg batch time: 0.5103, average train loss: 0.2451
[09/26 15:22:22 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1694, average loss: 3.6977
[09/26 15:22:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 98.00	
[09/26 15:22:22 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 15:22:29 visual_prompt]: Epoch 44 / 100: avg data time: 4.61e-02, avg batch time: 0.4969, average train loss: 0.2255
[09/26 15:22:30 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1698, average loss: 3.3904
[09/26 15:22:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 94.50	
[09/26 15:22:30 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 15:22:37 visual_prompt]: Epoch 45 / 100: avg data time: 5.60e-02, avg batch time: 0.5060, average train loss: 0.2185
[09/26 15:22:39 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1695, average loss: 3.9508
[09/26 15:22:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 94.00	
[09/26 15:22:39 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 15:22:46 visual_prompt]: Epoch 46 / 100: avg data time: 6.29e-02, avg batch time: 0.5151, average train loss: 0.2261
[09/26 15:22:47 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1696, average loss: 3.9782
[09/26 15:22:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 93.50	
[09/26 15:22:47 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 15:22:54 visual_prompt]: Epoch 47 / 100: avg data time: 6.55e-02, avg batch time: 0.5161, average train loss: 0.1897
[09/26 15:22:56 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1697, average loss: 3.7482
[09/26 15:22:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 95.00	
[09/26 15:22:56 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 15:23:03 visual_prompt]: Epoch 48 / 100: avg data time: 5.06e-02, avg batch time: 0.5016, average train loss: 0.2508
[09/26 15:23:04 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1692, average loss: 3.9296
[09/26 15:23:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 94.00	
[09/26 15:23:04 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 15:23:11 visual_prompt]: Epoch 49 / 100: avg data time: 5.61e-02, avg batch time: 0.5058, average train loss: 0.2559
[09/26 15:23:13 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 3.7109
[09/26 15:23:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/26 15:23:13 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 15:23:19 visual_prompt]: Epoch 50 / 100: avg data time: 4.45e-02, avg batch time: 0.4954, average train loss: 0.2303
[09/26 15:23:21 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 3.6395
[09/26 15:23:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.50	
[09/26 15:23:21 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 15:23:28 visual_prompt]: Epoch 51 / 100: avg data time: 4.91e-02, avg batch time: 0.5014, average train loss: 0.1618
[09/26 15:23:29 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1697, average loss: 3.2869
[09/26 15:23:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 98.50	
[09/26 15:23:29 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 15:23:36 visual_prompt]: Epoch 52 / 100: avg data time: 5.82e-02, avg batch time: 0.5076, average train loss: 0.0762
[09/26 15:23:38 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1695, average loss: 3.6679
[09/26 15:23:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 15:23:38 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 15:23:44 visual_prompt]: Epoch 53 / 100: avg data time: 5.26e-02, avg batch time: 0.5032, average train loss: 0.0572
[09/26 15:23:46 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1697, average loss: 4.2073
[09/26 15:23:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.50	
[09/26 15:23:46 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 15:23:53 visual_prompt]: Epoch 54 / 100: avg data time: 5.33e-02, avg batch time: 0.5027, average train loss: 0.0361
[09/26 15:23:54 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1697, average loss: 4.7409
[09/26 15:23:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.00	
[09/26 15:23:54 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 15:24:01 visual_prompt]: Epoch 55 / 100: avg data time: 5.97e-02, avg batch time: 0.5091, average train loss: 0.0293
[09/26 15:24:03 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1696, average loss: 4.9586
[09/26 15:24:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 15:24:03 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 15:24:10 visual_prompt]: Epoch 56 / 100: avg data time: 5.79e-02, avg batch time: 0.5071, average train loss: 0.0464
[09/26 15:24:11 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 5.1706
[09/26 15:24:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 94.50	
[09/26 15:24:11 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 15:24:18 visual_prompt]: Epoch 57 / 100: avg data time: 6.06e-02, avg batch time: 0.5103, average train loss: 0.0491
[09/26 15:24:20 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 4.9821
[09/26 15:24:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 94.00	
[09/26 15:24:20 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 15:24:27 visual_prompt]: Epoch 58 / 100: avg data time: 5.71e-02, avg batch time: 0.5057, average train loss: 0.0834
[09/26 15:24:28 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 4.7254
[09/26 15:24:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.50	
[09/26 15:24:28 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 15:24:35 visual_prompt]: Epoch 59 / 100: avg data time: 5.04e-02, avg batch time: 0.4994, average train loss: 0.1312
[09/26 15:24:36 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1698, average loss: 4.9187
[09/26 15:24:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 15:24:36 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 15:24:43 visual_prompt]: Epoch 60 / 100: avg data time: 5.15e-02, avg batch time: 0.5019, average train loss: 0.2214
[09/26 15:24:45 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1695, average loss: 4.0462
[09/26 15:24:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 98.50	
[09/26 15:24:45 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 15:24:52 visual_prompt]: Epoch 61 / 100: avg data time: 5.36e-02, avg batch time: 0.5039, average train loss: 0.2163
[09/26 15:24:53 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 4.2344
[09/26 15:24:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 15:24:53 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 15:25:00 visual_prompt]: Epoch 62 / 100: avg data time: 4.69e-02, avg batch time: 0.5005, average train loss: 0.0834
[09/26 15:25:02 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1696, average loss: 3.8792
[09/26 15:25:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 95.50	
[09/26 15:25:02 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 15:25:08 visual_prompt]: Epoch 63 / 100: avg data time: 5.63e-02, avg batch time: 0.5069, average train loss: 0.0563
[09/26 15:25:10 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1697, average loss: 4.2448
[09/26 15:25:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.00	
[09/26 15:25:10 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 15:25:17 visual_prompt]: Epoch 64 / 100: avg data time: 5.57e-02, avg batch time: 0.5065, average train loss: 0.0325
[09/26 15:25:18 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1699, average loss: 4.4350
[09/26 15:25:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 15:25:18 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 15:25:25 visual_prompt]: Epoch 65 / 100: avg data time: 5.80e-02, avg batch time: 0.5071, average train loss: 0.0142
[09/26 15:25:27 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 4.5191
[09/26 15:25:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.50	
[09/26 15:25:27 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 15:25:34 visual_prompt]: Epoch 66 / 100: avg data time: 5.11e-02, avg batch time: 0.5012, average train loss: 0.0109
[09/26 15:25:35 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 4.8804
[09/26 15:25:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.50	
[09/26 15:25:35 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 15:25:42 visual_prompt]: Epoch 67 / 100: avg data time: 5.24e-02, avg batch time: 0.5026, average train loss: 0.0094
[09/26 15:25:43 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1696, average loss: 4.9640
[09/26 15:25:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 15:25:43 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 15:25:50 visual_prompt]: Epoch 68 / 100: avg data time: 6.03e-02, avg batch time: 0.5098, average train loss: 0.0143
[09/26 15:25:52 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1697, average loss: 5.0554
[09/26 15:25:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.50	
[09/26 15:25:52 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 15:25:59 visual_prompt]: Epoch 69 / 100: avg data time: 4.78e-02, avg batch time: 0.4991, average train loss: 0.0149
[09/26 15:26:00 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1697, average loss: 5.0485
[09/26 15:26:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 15:26:00 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 15:26:07 visual_prompt]: Epoch 70 / 100: avg data time: 5.25e-02, avg batch time: 0.5031, average train loss: 0.0064
[09/26 15:26:09 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1698, average loss: 5.0545
[09/26 15:26:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 95.00	
[09/26 15:26:09 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 15:26:16 visual_prompt]: Epoch 71 / 100: avg data time: 6.44e-02, avg batch time: 0.5136, average train loss: 0.0101
[09/26 15:26:17 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1696, average loss: 4.9666
[09/26 15:26:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 95.50	
[09/26 15:26:17 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 15:26:24 visual_prompt]: Epoch 72 / 100: avg data time: 5.21e-02, avg batch time: 0.5024, average train loss: 0.0103
[09/26 15:26:26 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1696, average loss: 5.0973
[09/26 15:26:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 15:26:26 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 15:26:32 visual_prompt]: Epoch 73 / 100: avg data time: 4.86e-02, avg batch time: 0.4975, average train loss: 0.0075
[09/26 15:26:34 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1699, average loss: 5.3512
[09/26 15:26:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 15:26:34 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 15:26:41 visual_prompt]: Epoch 74 / 100: avg data time: 6.63e-02, avg batch time: 0.5156, average train loss: 0.0048
[09/26 15:26:42 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1694, average loss: 5.4349
[09/26 15:26:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.50	
[09/26 15:26:42 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 15:26:49 visual_prompt]: Epoch 75 / 100: avg data time: 5.07e-02, avg batch time: 0.5019, average train loss: 0.0054
[09/26 15:26:51 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1696, average loss: 5.4928
[09/26 15:26:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.50	
[09/26 15:26:51 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 15:26:57 visual_prompt]: Epoch 76 / 100: avg data time: 4.84e-02, avg batch time: 0.4973, average train loss: 0.0031
[09/26 15:26:59 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1693, average loss: 5.5435
[09/26 15:26:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.00	
[09/26 15:26:59 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 15:27:06 visual_prompt]: Epoch 77 / 100: avg data time: 4.49e-02, avg batch time: 0.4939, average train loss: 0.0042
[09/26 15:27:07 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1693, average loss: 5.4613
[09/26 15:27:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 96.50	
[09/26 15:27:07 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 15:27:14 visual_prompt]: Epoch 78 / 100: avg data time: 5.67e-02, avg batch time: 0.5063, average train loss: 0.0042
[09/26 15:27:16 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 5.5433
[09/26 15:27:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 96.50	
[09/26 15:27:16 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 15:27:22 visual_prompt]: Epoch 79 / 100: avg data time: 5.97e-02, avg batch time: 0.5088, average train loss: 0.0042
[09/26 15:27:24 visual_prompt]: Inference (val):avg data time: 6.06e-05, avg batch time: 0.1705, average loss: 5.5208
[09/26 15:27:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:27:24 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 15:27:31 visual_prompt]: Epoch 80 / 100: avg data time: 4.89e-02, avg batch time: 0.4999, average train loss: 0.0038
[09/26 15:27:32 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 5.5343
[09/26 15:27:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 15:27:32 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 15:27:39 visual_prompt]: Epoch 81 / 100: avg data time: 5.46e-02, avg batch time: 0.5042, average train loss: 0.0024
[09/26 15:27:41 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1697, average loss: 5.5691
[09/26 15:27:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 15:27:41 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 15:27:48 visual_prompt]: Epoch 82 / 100: avg data time: 6.07e-02, avg batch time: 0.5097, average train loss: 0.0024
[09/26 15:27:49 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1697, average loss: 5.5964
[09/26 15:27:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 15:27:49 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 15:27:56 visual_prompt]: Epoch 83 / 100: avg data time: 4.56e-02, avg batch time: 0.4957, average train loss: 0.0033
[09/26 15:27:58 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1697, average loss: 5.6207
[09/26 15:27:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:27:58 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 15:28:05 visual_prompt]: Epoch 84 / 100: avg data time: 5.89e-02, avg batch time: 0.5088, average train loss: 0.0028
[09/26 15:28:06 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1696, average loss: 5.6514
[09/26 15:28:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 15:28:06 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 15:28:13 visual_prompt]: Epoch 85 / 100: avg data time: 5.71e-02, avg batch time: 0.5065, average train loss: 0.0019
[09/26 15:28:15 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1692, average loss: 5.6789
[09/26 15:28:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:28:15 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 15:28:21 visual_prompt]: Epoch 86 / 100: avg data time: 4.63e-02, avg batch time: 0.4967, average train loss: 0.0024
[09/26 15:28:23 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1695, average loss: 5.7065
[09/26 15:28:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 15:28:23 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 15:28:30 visual_prompt]: Epoch 87 / 100: avg data time: 5.91e-02, avg batch time: 0.5079, average train loss: 0.0021
[09/26 15:28:31 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1691, average loss: 5.7228
[09/26 15:28:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.00	
[09/26 15:28:31 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 15:28:38 visual_prompt]: Epoch 88 / 100: avg data time: 4.58e-02, avg batch time: 0.4976, average train loss: 0.0028
[09/26 15:28:40 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 5.7212
[09/26 15:28:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:28:40 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 15:28:46 visual_prompt]: Epoch 89 / 100: avg data time: 4.58e-02, avg batch time: 0.4963, average train loss: 0.0018
[09/26 15:28:48 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 5.7283
[09/26 15:28:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:28:48 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 15:28:55 visual_prompt]: Epoch 90 / 100: avg data time: 5.05e-02, avg batch time: 0.5004, average train loss: 0.0024
[09/26 15:28:56 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 5.7218
[09/26 15:28:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:28:56 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 15:29:03 visual_prompt]: Epoch 91 / 100: avg data time: 5.47e-02, avg batch time: 0.5049, average train loss: 0.0023
[09/26 15:29:05 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1691, average loss: 5.7270
[09/26 15:29:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:29:05 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 15:29:11 visual_prompt]: Epoch 92 / 100: avg data time: 4.43e-02, avg batch time: 0.4952, average train loss: 0.0023
[09/26 15:29:13 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1699, average loss: 5.7291
[09/26 15:29:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:29:13 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 15:29:20 visual_prompt]: Epoch 93 / 100: avg data time: 5.34e-02, avg batch time: 0.5046, average train loss: 0.0023
[09/26 15:29:21 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1697, average loss: 5.7316
[09/26 15:29:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:29:21 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 15:29:28 visual_prompt]: Epoch 94 / 100: avg data time: 5.68e-02, avg batch time: 0.5067, average train loss: 0.0023
[09/26 15:29:30 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1698, average loss: 5.7349
[09/26 15:29:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:29:30 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 15:29:37 visual_prompt]: Epoch 95 / 100: avg data time: 5.72e-02, avg batch time: 0.5063, average train loss: 0.0020
[09/26 15:29:38 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1698, average loss: 5.7367
[09/26 15:29:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:29:38 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 15:29:45 visual_prompt]: Epoch 96 / 100: avg data time: 5.99e-02, avg batch time: 0.5104, average train loss: 0.0020
[09/26 15:29:47 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.1694, average loss: 5.7381
[09/26 15:29:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:29:47 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 15:29:54 visual_prompt]: Epoch 97 / 100: avg data time: 6.12e-02, avg batch time: 0.5115, average train loss: 0.0018
[09/26 15:29:55 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1695, average loss: 5.7391
[09/26 15:29:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:29:55 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 15:30:02 visual_prompt]: Epoch 98 / 100: avg data time: 6.10e-02, avg batch time: 0.5101, average train loss: 0.0024
[09/26 15:30:04 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1693, average loss: 5.7396
[09/26 15:30:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:30:04 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 15:30:10 visual_prompt]: Epoch 99 / 100: avg data time: 4.30e-02, avg batch time: 0.4950, average train loss: 0.0022
[09/26 15:30:12 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1695, average loss: 5.7399
[09/26 15:30:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:30:12 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 15:30:19 visual_prompt]: Epoch 100 / 100: avg data time: 6.24e-02, avg batch time: 0.5114, average train loss: 0.0017
[09/26 15:30:21 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1699, average loss: 5.7399
[09/26 15:30:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.50	
[09/26 15:30:21 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 15:30:21 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 15:30:21 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 15:30:21 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 15:30:21 visual_prompt]: Training with config:
[09/26 15:30:21 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 15:30:21 visual_prompt]: Loading training data...
[09/26 15:30:21 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 15:30:22 visual_prompt]: Number of images: 800
[09/26 15:30:22 visual_prompt]: Number of classes: 6 / 6
[09/26 15:30:22 visual_prompt]: Loading validation data...
[09/26 15:30:22 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 15:30:22 visual_prompt]: Number of images: 200
[09/26 15:30:22 visual_prompt]: Number of classes: 6 / 6
[09/26 15:30:22 visual_prompt]: Constructing models...
[09/26 15:30:25 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 15:30:25 visual_prompt]: tuned percent:0.540
[09/26 15:30:25 visual_prompt]: Device used for model: 0
[09/26 15:30:25 visual_prompt]: Setting up Evaluator...
[09/26 15:30:25 visual_prompt]: Setting up Trainer...
[09/26 15:30:25 visual_prompt]: 	Setting up the optimizer...
[09/26 15:30:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 15:30:32 visual_prompt]: Epoch 1 / 100: avg data time: 4.70e-02, avg batch time: 0.4958, average train loss: 2.9880
[09/26 15:30:33 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1688, average loss: 2.9268
[09/26 15:30:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 15:30:33 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 15:30:33 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 15:30:40 visual_prompt]: Epoch 2 / 100: avg data time: 5.71e-02, avg batch time: 0.5055, average train loss: 2.0779
[09/26 15:30:42 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1688, average loss: 1.8256
[09/26 15:30:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 15:30:42 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 15:30:49 visual_prompt]: Epoch 3 / 100: avg data time: 5.66e-02, avg batch time: 0.5042, average train loss: 1.7859
[09/26 15:30:50 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1691, average loss: 1.8375
[09/26 15:30:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 86.00	
[09/26 15:30:50 visual_prompt]: Best epoch 3: best metric: 0.205
[09/26 15:30:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 15:30:57 visual_prompt]: Epoch 4 / 100: avg data time: 5.20e-02, avg batch time: 0.5024, average train loss: 1.7867
[09/26 15:30:59 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1689, average loss: 1.8134
[09/26 15:30:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 15:30:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 15:31:05 visual_prompt]: Epoch 5 / 100: avg data time: 5.38e-02, avg batch time: 0.5028, average train loss: 1.7709
[09/26 15:31:07 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1692, average loss: 1.8063
[09/26 15:31:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 85.00	
[09/26 15:31:07 visual_prompt]: Best epoch 5: best metric: 0.235
[09/26 15:31:07 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 15:31:14 visual_prompt]: Epoch 6 / 100: avg data time: 5.98e-02, avg batch time: 0.5086, average train loss: 1.7547
[09/26 15:31:15 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1696, average loss: 1.8105
[09/26 15:31:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.00	top5: 84.50	
[09/26 15:31:15 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 15:31:22 visual_prompt]: Epoch 7 / 100: avg data time: 5.93e-02, avg batch time: 0.5091, average train loss: 1.7196
[09/26 15:31:24 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1691, average loss: 1.8185
[09/26 15:31:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 84.00	
[09/26 15:31:24 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 15:31:31 visual_prompt]: Epoch 8 / 100: avg data time: 5.85e-02, avg batch time: 0.5075, average train loss: 1.7057
[09/26 15:31:32 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1691, average loss: 1.8144
[09/26 15:31:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 87.00	
[09/26 15:31:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 15:31:39 visual_prompt]: Epoch 9 / 100: avg data time: 5.33e-02, avg batch time: 0.5030, average train loss: 1.6491
[09/26 15:31:41 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1691, average loss: 1.7647
[09/26 15:31:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 88.50	
[09/26 15:31:41 visual_prompt]: Best epoch 9: best metric: 0.250
[09/26 15:31:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 15:31:47 visual_prompt]: Epoch 10 / 100: avg data time: 4.90e-02, avg batch time: 0.4991, average train loss: 1.6562
[09/26 15:31:49 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 1.9055
[09/26 15:31:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.50	
[09/26 15:31:49 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 15:31:56 visual_prompt]: Epoch 11 / 100: avg data time: 6.54e-02, avg batch time: 0.5139, average train loss: 1.6262
[09/26 15:31:57 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1699, average loss: 1.7471
[09/26 15:31:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 91.50	
[09/26 15:31:57 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 15:32:04 visual_prompt]: Epoch 12 / 100: avg data time: 5.99e-02, avg batch time: 0.5095, average train loss: 1.5640
[09/26 15:32:06 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 1.7073
[09/26 15:32:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 89.50	
[09/26 15:32:06 visual_prompt]: Best epoch 12: best metric: 0.260
[09/26 15:32:06 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 15:32:13 visual_prompt]: Epoch 13 / 100: avg data time: 6.10e-02, avg batch time: 0.5108, average train loss: 1.5780
[09/26 15:32:14 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1697, average loss: 1.6898
[09/26 15:32:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 92.00	
[09/26 15:32:14 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 15:32:21 visual_prompt]: Epoch 14 / 100: avg data time: 5.66e-02, avg batch time: 0.5052, average train loss: 1.5020
[09/26 15:32:23 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1694, average loss: 1.8279
[09/26 15:32:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.50	top5: 91.50	
[09/26 15:32:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 15:32:30 visual_prompt]: Epoch 15 / 100: avg data time: 5.60e-02, avg batch time: 0.5057, average train loss: 1.6497
[09/26 15:32:31 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 1.7111
[09/26 15:32:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 90.50	
[09/26 15:32:31 visual_prompt]: Best epoch 15: best metric: 0.275
[09/26 15:32:31 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 15:32:38 visual_prompt]: Epoch 16 / 100: avg data time: 5.63e-02, avg batch time: 0.5048, average train loss: 1.5157
[09/26 15:32:40 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 1.5652
[09/26 15:32:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 94.50	
[09/26 15:32:40 visual_prompt]: Best epoch 16: best metric: 0.340
[09/26 15:32:40 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 15:32:47 visual_prompt]: Epoch 17 / 100: avg data time: 6.14e-02, avg batch time: 0.5109, average train loss: 1.5230
[09/26 15:32:48 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1699, average loss: 1.6118
[09/26 15:32:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 95.00	
[09/26 15:32:48 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 15:32:55 visual_prompt]: Epoch 18 / 100: avg data time: 5.51e-02, avg batch time: 0.5045, average train loss: 1.4506
[09/26 15:32:56 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 1.5904
[09/26 15:32:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 96.50	
[09/26 15:32:56 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 15:33:03 visual_prompt]: Epoch 19 / 100: avg data time: 5.91e-02, avg batch time: 0.5091, average train loss: 1.3877
[09/26 15:33:05 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 1.5682
[09/26 15:33:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 94.00	
[09/26 15:33:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 15:33:12 visual_prompt]: Epoch 20 / 100: avg data time: 5.47e-02, avg batch time: 0.5046, average train loss: 1.3751
[09/26 15:33:13 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1693, average loss: 1.4678
[09/26 15:33:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 97.00	
[09/26 15:33:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 15:33:20 visual_prompt]: Epoch 21 / 100: avg data time: 5.90e-02, avg batch time: 0.5097, average train loss: 1.2973
[09/26 15:33:22 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1695, average loss: 1.4643
[09/26 15:33:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 15:33:22 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 15:33:29 visual_prompt]: Epoch 22 / 100: avg data time: 5.33e-02, avg batch time: 0.5023, average train loss: 1.3501
[09/26 15:33:30 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1692, average loss: 1.6605
[09/26 15:33:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 96.00	
[09/26 15:33:30 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 15:33:37 visual_prompt]: Epoch 23 / 100: avg data time: 5.49e-02, avg batch time: 0.5038, average train loss: 1.2731
[09/26 15:33:39 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.6985
[09/26 15:33:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 93.50	
[09/26 15:33:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 15:33:45 visual_prompt]: Epoch 24 / 100: avg data time: 4.28e-02, avg batch time: 0.4932, average train loss: 1.3389
[09/26 15:33:47 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1699, average loss: 1.5580
[09/26 15:33:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 95.50	
[09/26 15:33:47 visual_prompt]: Best epoch 24: best metric: 0.345
[09/26 15:33:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 15:33:54 visual_prompt]: Epoch 25 / 100: avg data time: 5.35e-02, avg batch time: 0.5025, average train loss: 1.2045
[09/26 15:33:55 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1697, average loss: 1.4895
[09/26 15:33:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 98.00	
[09/26 15:33:55 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 15:34:02 visual_prompt]: Epoch 26 / 100: avg data time: 5.36e-02, avg batch time: 0.5036, average train loss: 1.1547
[09/26 15:34:04 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1696, average loss: 1.5410
[09/26 15:34:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 15:34:04 visual_prompt]: Best epoch 26: best metric: 0.380
[09/26 15:34:04 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 15:34:11 visual_prompt]: Epoch 27 / 100: avg data time: 5.84e-02, avg batch time: 0.5073, average train loss: 1.1151
[09/26 15:34:12 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1698, average loss: 1.9921
[09/26 15:34:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 95.50	
[09/26 15:34:12 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 15:34:19 visual_prompt]: Epoch 28 / 100: avg data time: 6.45e-02, avg batch time: 0.5132, average train loss: 1.2415
[09/26 15:34:21 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1692, average loss: 1.6202
[09/26 15:34:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 94.00	
[09/26 15:34:21 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 15:34:27 visual_prompt]: Epoch 29 / 100: avg data time: 5.34e-02, avg batch time: 0.5034, average train loss: 1.1858
[09/26 15:34:29 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1693, average loss: 1.4944
[09/26 15:34:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 15:34:29 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 15:34:36 visual_prompt]: Epoch 30 / 100: avg data time: 5.81e-02, avg batch time: 0.5074, average train loss: 1.0957
[09/26 15:34:38 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1697, average loss: 1.4822
[09/26 15:34:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 15:34:38 visual_prompt]: Best epoch 30: best metric: 0.395
[09/26 15:34:38 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 15:34:44 visual_prompt]: Epoch 31 / 100: avg data time: 5.59e-02, avg batch time: 0.5057, average train loss: 1.0737
[09/26 15:34:46 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1695, average loss: 1.5010
[09/26 15:34:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.50	
[09/26 15:34:46 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 15:34:53 visual_prompt]: Epoch 32 / 100: avg data time: 5.89e-02, avg batch time: 0.5095, average train loss: 1.0835
[09/26 15:34:54 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1695, average loss: 1.4822
[09/26 15:34:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 15:34:54 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 15:35:01 visual_prompt]: Epoch 33 / 100: avg data time: 5.66e-02, avg batch time: 0.5064, average train loss: 1.0177
[09/26 15:35:03 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 1.6436
[09/26 15:35:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.50	
[09/26 15:35:03 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 15:35:10 visual_prompt]: Epoch 34 / 100: avg data time: 5.42e-02, avg batch time: 0.5044, average train loss: 1.0075
[09/26 15:35:11 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 1.6265
[09/26 15:35:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.00	
[09/26 15:35:11 visual_prompt]: Best epoch 34: best metric: 0.405
[09/26 15:35:11 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 15:35:18 visual_prompt]: Epoch 35 / 100: avg data time: 4.42e-02, avg batch time: 0.4953, average train loss: 0.9969
[09/26 15:35:19 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1695, average loss: 1.5847
[09/26 15:35:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 15:35:19 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 15:35:26 visual_prompt]: Epoch 36 / 100: avg data time: 5.35e-02, avg batch time: 0.5029, average train loss: 0.9417
[09/26 15:35:28 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 1.5473
[09/26 15:35:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 15:35:28 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 15:35:35 visual_prompt]: Epoch 37 / 100: avg data time: 4.75e-02, avg batch time: 0.4992, average train loss: 0.9460
[09/26 15:35:36 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 1.7319
[09/26 15:35:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 93.50	
[09/26 15:35:36 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 15:35:43 visual_prompt]: Epoch 38 / 100: avg data time: 5.64e-02, avg batch time: 0.5054, average train loss: 0.9888
[09/26 15:35:45 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1696, average loss: 1.7134
[09/26 15:35:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 15:35:45 visual_prompt]: Best epoch 38: best metric: 0.415
[09/26 15:35:45 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 15:35:51 visual_prompt]: Epoch 39 / 100: avg data time: 5.75e-02, avg batch time: 0.5065, average train loss: 0.8740
[09/26 15:35:53 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1699, average loss: 1.7886
[09/26 15:35:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 95.00	
[09/26 15:35:53 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 15:36:00 visual_prompt]: Epoch 40 / 100: avg data time: 5.99e-02, avg batch time: 0.5112, average train loss: 0.8330
[09/26 15:36:02 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1691, average loss: 1.5918
[09/26 15:36:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 15:36:02 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 15:36:08 visual_prompt]: Epoch 41 / 100: avg data time: 4.77e-02, avg batch time: 0.5007, average train loss: 0.8226
[09/26 15:36:10 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1698, average loss: 1.5698
[09/26 15:36:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 98.50	
[09/26 15:36:10 visual_prompt]: Best epoch 41: best metric: 0.430
[09/26 15:36:10 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 15:36:17 visual_prompt]: Epoch 42 / 100: avg data time: 5.56e-02, avg batch time: 0.5063, average train loss: 0.7282
[09/26 15:36:18 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1701, average loss: 1.7893
[09/26 15:36:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.00	
[09/26 15:36:18 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 15:36:25 visual_prompt]: Epoch 43 / 100: avg data time: 6.23e-02, avg batch time: 0.5121, average train loss: 0.6648
[09/26 15:36:27 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1700, average loss: 1.7499
[09/26 15:36:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 15:36:27 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 15:36:34 visual_prompt]: Epoch 44 / 100: avg data time: 4.91e-02, avg batch time: 0.4998, average train loss: 0.6606
[09/26 15:36:35 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1698, average loss: 1.7563
[09/26 15:36:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 15:36:35 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 15:36:42 visual_prompt]: Epoch 45 / 100: avg data time: 4.76e-02, avg batch time: 0.4979, average train loss: 0.7270
[09/26 15:36:43 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1696, average loss: 1.8723
[09/26 15:36:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 95.50	
[09/26 15:36:44 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 15:36:50 visual_prompt]: Epoch 46 / 100: avg data time: 5.87e-02, avg batch time: 0.5084, average train loss: 0.6259
[09/26 15:36:52 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 1.8759
[09/26 15:36:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.50	
[09/26 15:36:52 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 15:36:59 visual_prompt]: Epoch 47 / 100: avg data time: 5.60e-02, avg batch time: 0.5051, average train loss: 0.6901
[09/26 15:37:00 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 1.6846
[09/26 15:37:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 15:37:00 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 15:37:07 visual_prompt]: Epoch 48 / 100: avg data time: 4.66e-02, avg batch time: 0.4979, average train loss: 0.6367
[09/26 15:37:09 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1694, average loss: 1.8483
[09/26 15:37:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.50	
[09/26 15:37:09 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 15:37:16 visual_prompt]: Epoch 49 / 100: avg data time: 5.03e-02, avg batch time: 0.5005, average train loss: 0.5691
[09/26 15:37:17 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 2.0092
[09/26 15:37:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.00	
[09/26 15:37:17 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 15:37:24 visual_prompt]: Epoch 50 / 100: avg data time: 5.79e-02, avg batch time: 0.5081, average train loss: 0.5966
[09/26 15:37:26 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 1.8433
[09/26 15:37:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.50	
[09/26 15:37:26 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 15:37:32 visual_prompt]: Epoch 51 / 100: avg data time: 5.29e-02, avg batch time: 0.5031, average train loss: 0.5448
[09/26 15:37:34 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 2.2668
[09/26 15:37:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 15:37:34 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 15:37:41 visual_prompt]: Epoch 52 / 100: avg data time: 5.69e-02, avg batch time: 0.5070, average train loss: 0.5351
[09/26 15:37:42 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1698, average loss: 2.0527
[09/26 15:37:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 98.00	
[09/26 15:37:42 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 15:37:49 visual_prompt]: Epoch 53 / 100: avg data time: 6.01e-02, avg batch time: 0.5100, average train loss: 0.4527
[09/26 15:37:51 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 1.9041
[09/26 15:37:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.00	
[09/26 15:37:51 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 15:37:58 visual_prompt]: Epoch 54 / 100: avg data time: 5.56e-02, avg batch time: 0.5047, average train loss: 0.4108
[09/26 15:37:59 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1692, average loss: 2.0374
[09/26 15:37:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:37:59 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 15:38:06 visual_prompt]: Epoch 55 / 100: avg data time: 4.97e-02, avg batch time: 0.4997, average train loss: 0.3457
[09/26 15:38:08 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1695, average loss: 2.3201
[09/26 15:38:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.00	
[09/26 15:38:08 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 15:38:15 visual_prompt]: Epoch 56 / 100: avg data time: 6.09e-02, avg batch time: 0.5098, average train loss: 0.3480
[09/26 15:38:16 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 2.1938
[09/26 15:38:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 96.50	
[09/26 15:38:16 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 15:38:23 visual_prompt]: Epoch 57 / 100: avg data time: 6.24e-02, avg batch time: 0.5118, average train loss: 0.4344
[09/26 15:38:25 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1697, average loss: 1.9042
[09/26 15:38:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 47.00	top5: 96.50	
[09/26 15:38:25 visual_prompt]: Best epoch 57: best metric: 0.470
[09/26 15:38:25 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 15:38:32 visual_prompt]: Epoch 58 / 100: avg data time: 5.96e-02, avg batch time: 0.5085, average train loss: 0.3264
[09/26 15:38:33 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1697, average loss: 2.1406
[09/26 15:38:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 15:38:33 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 15:38:40 visual_prompt]: Epoch 59 / 100: avg data time: 5.91e-02, avg batch time: 0.5096, average train loss: 0.2744
[09/26 15:38:42 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 2.2695
[09/26 15:38:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.00	
[09/26 15:38:42 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 15:38:48 visual_prompt]: Epoch 60 / 100: avg data time: 4.54e-02, avg batch time: 0.4957, average train loss: 0.2112
[09/26 15:38:50 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 2.2979
[09/26 15:38:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.50	
[09/26 15:38:50 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 15:38:57 visual_prompt]: Epoch 61 / 100: avg data time: 5.12e-02, avg batch time: 0.5001, average train loss: 0.2289
[09/26 15:38:58 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1697, average loss: 2.4267
[09/26 15:38:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 15:38:58 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 15:39:05 visual_prompt]: Epoch 62 / 100: avg data time: 6.04e-02, avg batch time: 0.5103, average train loss: 0.2761
[09/26 15:39:07 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1691, average loss: 2.4951
[09/26 15:39:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.50	
[09/26 15:39:07 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 15:39:14 visual_prompt]: Epoch 63 / 100: avg data time: 5.54e-02, avg batch time: 0.5053, average train loss: 0.2396
[09/26 15:39:15 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1693, average loss: 2.3604
[09/26 15:39:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.00	
[09/26 15:39:15 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 15:39:22 visual_prompt]: Epoch 64 / 100: avg data time: 5.76e-02, avg batch time: 0.5069, average train loss: 0.1597
[09/26 15:39:24 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1697, average loss: 2.5168
[09/26 15:39:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 15:39:24 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 15:39:30 visual_prompt]: Epoch 65 / 100: avg data time: 5.27e-02, avg batch time: 0.5023, average train loss: 0.1033
[09/26 15:39:32 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.1695, average loss: 2.5407
[09/26 15:39:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.50	
[09/26 15:39:32 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 15:39:39 visual_prompt]: Epoch 66 / 100: avg data time: 5.33e-02, avg batch time: 0.5024, average train loss: 0.1274
[09/26 15:39:40 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1697, average loss: 2.5189
[09/26 15:39:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 15:39:40 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 15:39:48 visual_prompt]: Epoch 67 / 100: avg data time: 6.21e-02, avg batch time: 0.5122, average train loss: 0.1187
[09/26 15:39:49 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1695, average loss: 2.5750
[09/26 15:39:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.00	
[09/26 15:39:49 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 15:39:56 visual_prompt]: Epoch 68 / 100: avg data time: 5.20e-02, avg batch time: 0.5026, average train loss: 0.2325
[09/26 15:39:57 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1692, average loss: 2.6173
[09/26 15:39:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 15:39:57 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 15:40:04 visual_prompt]: Epoch 69 / 100: avg data time: 5.70e-02, avg batch time: 0.5059, average train loss: 0.1666
[09/26 15:40:06 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 2.6635
[09/26 15:40:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.50	
[09/26 15:40:06 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 15:40:13 visual_prompt]: Epoch 70 / 100: avg data time: 4.19e-02, avg batch time: 0.4931, average train loss: 0.1523
[09/26 15:40:14 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1699, average loss: 2.5632
[09/26 15:40:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 15:40:14 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 15:40:21 visual_prompt]: Epoch 71 / 100: avg data time: 4.47e-02, avg batch time: 0.4953, average train loss: 0.0772
[09/26 15:40:22 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1693, average loss: 2.5110
[09/26 15:40:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 15:40:22 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 15:40:29 visual_prompt]: Epoch 72 / 100: avg data time: 4.16e-02, avg batch time: 0.4910, average train loss: 0.0444
[09/26 15:40:31 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1694, average loss: 2.6037
[09/26 15:40:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.00	
[09/26 15:40:31 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 15:40:37 visual_prompt]: Epoch 73 / 100: avg data time: 4.65e-02, avg batch time: 0.4969, average train loss: 0.0492
[09/26 15:40:39 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1695, average loss: 2.7027
[09/26 15:40:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 15:40:39 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 15:40:46 visual_prompt]: Epoch 74 / 100: avg data time: 4.84e-02, avg batch time: 0.4995, average train loss: 0.0351
[09/26 15:40:47 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1695, average loss: 2.7020
[09/26 15:40:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.00	
[09/26 15:40:47 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 15:40:54 visual_prompt]: Epoch 75 / 100: avg data time: 5.82e-02, avg batch time: 0.5069, average train loss: 0.0375
[09/26 15:40:56 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 2.7175
[09/26 15:40:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 15:40:56 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 15:41:03 visual_prompt]: Epoch 76 / 100: avg data time: 6.03e-02, avg batch time: 0.5091, average train loss: 0.0294
[09/26 15:41:04 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 2.7510
[09/26 15:41:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 15:41:04 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 15:41:11 visual_prompt]: Epoch 77 / 100: avg data time: 6.00e-02, avg batch time: 0.5103, average train loss: 0.0300
[09/26 15:41:13 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 2.7684
[09/26 15:41:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 98.00	
[09/26 15:41:13 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 15:41:19 visual_prompt]: Epoch 78 / 100: avg data time: 5.01e-02, avg batch time: 0.5008, average train loss: 0.0186
[09/26 15:41:21 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1696, average loss: 2.9034
[09/26 15:41:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 15:41:21 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 15:41:28 visual_prompt]: Epoch 79 / 100: avg data time: 5.23e-02, avg batch time: 0.5030, average train loss: 0.0181
[09/26 15:41:29 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1698, average loss: 2.9363
[09/26 15:41:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 15:41:29 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 15:41:36 visual_prompt]: Epoch 80 / 100: avg data time: 5.02e-02, avg batch time: 0.5005, average train loss: 0.0144
[09/26 15:41:38 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1696, average loss: 2.9851
[09/26 15:41:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 15:41:38 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 15:41:44 visual_prompt]: Epoch 81 / 100: avg data time: 5.37e-02, avg batch time: 0.5051, average train loss: 0.0183
[09/26 15:41:46 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1691, average loss: 2.9629
[09/26 15:41:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.50	
[09/26 15:41:46 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 15:41:53 visual_prompt]: Epoch 82 / 100: avg data time: 4.16e-02, avg batch time: 0.4930, average train loss: 0.0135
[09/26 15:41:54 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 2.9819
[09/26 15:41:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.00	
[09/26 15:41:54 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 15:42:01 visual_prompt]: Epoch 83 / 100: avg data time: 5.07e-02, avg batch time: 0.5022, average train loss: 0.0117
[09/26 15:42:03 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1697, average loss: 3.0432
[09/26 15:42:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.50	
[09/26 15:42:03 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 15:42:10 visual_prompt]: Epoch 84 / 100: avg data time: 5.39e-02, avg batch time: 0.5029, average train loss: 0.0112
[09/26 15:42:11 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1695, average loss: 3.0462
[09/26 15:42:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 15:42:11 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 15:42:18 visual_prompt]: Epoch 85 / 100: avg data time: 5.71e-02, avg batch time: 0.5060, average train loss: 0.0107
[09/26 15:42:20 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1695, average loss: 3.0410
[09/26 15:42:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.50	
[09/26 15:42:20 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 15:42:26 visual_prompt]: Epoch 86 / 100: avg data time: 5.73e-02, avg batch time: 0.5074, average train loss: 0.0108
[09/26 15:42:28 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1693, average loss: 3.0385
[09/26 15:42:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.50	
[09/26 15:42:28 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 15:42:35 visual_prompt]: Epoch 87 / 100: avg data time: 5.44e-02, avg batch time: 0.5038, average train loss: 0.0112
[09/26 15:42:36 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1695, average loss: 3.0357
[09/26 15:42:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 15:42:36 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 15:42:43 visual_prompt]: Epoch 88 / 100: avg data time: 5.54e-02, avg batch time: 0.5048, average train loss: 0.0102
[09/26 15:42:45 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1696, average loss: 3.0156
[09/26 15:42:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:42:45 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 15:42:52 visual_prompt]: Epoch 89 / 100: avg data time: 5.56e-02, avg batch time: 0.5053, average train loss: 0.0106
[09/26 15:42:53 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1691, average loss: 3.0233
[09/26 15:42:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 15:42:53 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 15:43:00 visual_prompt]: Epoch 90 / 100: avg data time: 5.43e-02, avg batch time: 0.5039, average train loss: 0.0100
[09/26 15:43:02 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 3.0339
[09/26 15:43:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 15:43:02 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 15:43:08 visual_prompt]: Epoch 91 / 100: avg data time: 5.54e-02, avg batch time: 0.5049, average train loss: 0.0101
[09/26 15:43:10 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1700, average loss: 3.0411
[09/26 15:43:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:43:10 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 15:43:17 visual_prompt]: Epoch 92 / 100: avg data time: 4.98e-02, avg batch time: 0.4997, average train loss: 0.0097
[09/26 15:43:18 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 3.0454
[09/26 15:43:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:43:18 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 15:43:25 visual_prompt]: Epoch 93 / 100: avg data time: 5.76e-02, avg batch time: 0.5075, average train loss: 0.0100
[09/26 15:43:27 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1697, average loss: 3.0476
[09/26 15:43:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:43:27 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 15:43:34 visual_prompt]: Epoch 94 / 100: avg data time: 5.41e-02, avg batch time: 0.5044, average train loss: 0.0098
[09/26 15:43:35 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 3.0490
[09/26 15:43:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:43:35 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 15:43:42 visual_prompt]: Epoch 95 / 100: avg data time: 4.96e-02, avg batch time: 0.4996, average train loss: 0.0100
[09/26 15:43:44 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1696, average loss: 3.0496
[09/26 15:43:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:43:44 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 15:43:50 visual_prompt]: Epoch 96 / 100: avg data time: 5.07e-02, avg batch time: 0.5002, average train loss: 0.0099
[09/26 15:43:52 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1699, average loss: 3.0505
[09/26 15:43:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:43:52 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 15:43:59 visual_prompt]: Epoch 97 / 100: avg data time: 5.70e-02, avg batch time: 0.5064, average train loss: 0.0095
[09/26 15:44:00 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 3.0508
[09/26 15:44:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:44:00 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 15:44:07 visual_prompt]: Epoch 98 / 100: avg data time: 4.76e-02, avg batch time: 0.4980, average train loss: 0.0095
[09/26 15:44:09 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 3.0510
[09/26 15:44:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:44:09 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 15:44:15 visual_prompt]: Epoch 99 / 100: avg data time: 5.66e-02, avg batch time: 0.5058, average train loss: 0.0098
[09/26 15:44:17 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 3.0512
[09/26 15:44:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:44:17 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 15:44:24 visual_prompt]: Epoch 100 / 100: avg data time: 6.08e-02, avg batch time: 0.5106, average train loss: 0.0094
[09/26 15:44:26 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1698, average loss: 3.0512
[09/26 15:44:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:44:26 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 15:44:26 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 15:44:26 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 15:44:26 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 15:44:26 visual_prompt]: Training with config:
[09/26 15:44:26 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 15:44:26 visual_prompt]: Loading training data...
[09/26 15:44:26 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 15:44:27 visual_prompt]: Number of images: 800
[09/26 15:44:27 visual_prompt]: Number of classes: 6 / 6
[09/26 15:44:27 visual_prompt]: Loading validation data...
[09/26 15:44:27 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 15:44:27 visual_prompt]: Number of images: 200
[09/26 15:44:27 visual_prompt]: Number of classes: 6 / 6
[09/26 15:44:27 visual_prompt]: Constructing models...
[09/26 15:44:30 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 15:44:30 visual_prompt]: tuned percent:0.540
[09/26 15:44:30 visual_prompt]: Device used for model: 0
[09/26 15:44:30 visual_prompt]: Setting up Evaluator...
[09/26 15:44:30 visual_prompt]: Setting up Trainer...
[09/26 15:44:30 visual_prompt]: 	Setting up the optimizer...
[09/26 15:44:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 15:44:37 visual_prompt]: Epoch 1 / 100: avg data time: 6.19e-02, avg batch time: 0.5091, average train loss: 2.9681
[09/26 15:44:38 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 2.9268
[09/26 15:44:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 15:44:38 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 15:44:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 15:44:45 visual_prompt]: Epoch 2 / 100: avg data time: 4.60e-02, avg batch time: 0.4957, average train loss: 2.1371
[09/26 15:44:47 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1687, average loss: 1.8115
[09/26 15:44:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.50	
[09/26 15:44:47 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 15:44:53 visual_prompt]: Epoch 3 / 100: avg data time: 5.68e-02, avg batch time: 0.5055, average train loss: 1.8069
[09/26 15:44:55 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 1.8673
[09/26 15:44:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 85.00	
[09/26 15:44:55 visual_prompt]: Best epoch 3: best metric: 0.180
[09/26 15:44:55 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 15:45:02 visual_prompt]: Epoch 4 / 100: avg data time: 4.68e-02, avg batch time: 0.4966, average train loss: 1.7818
[09/26 15:45:03 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1691, average loss: 1.7993
[09/26 15:45:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 15:45:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 15:45:10 visual_prompt]: Epoch 5 / 100: avg data time: 5.30e-02, avg batch time: 0.5015, average train loss: 1.7786
[09/26 15:45:12 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 1.7997
[09/26 15:45:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.50	
[09/26 15:45:12 visual_prompt]: Best epoch 5: best metric: 0.205
[09/26 15:45:12 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 15:45:19 visual_prompt]: Epoch 6 / 100: avg data time: 5.89e-02, avg batch time: 0.5073, average train loss: 1.7613
[09/26 15:45:20 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 1.8306
[09/26 15:45:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 15:45:20 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 15:45:27 visual_prompt]: Epoch 7 / 100: avg data time: 6.05e-02, avg batch time: 0.5091, average train loss: 1.7351
[09/26 15:45:29 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 1.7962
[09/26 15:45:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 86.50	
[09/26 15:45:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 15:45:35 visual_prompt]: Epoch 8 / 100: avg data time: 4.81e-02, avg batch time: 0.4979, average train loss: 1.7055
[09/26 15:45:37 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1693, average loss: 1.7145
[09/26 15:45:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 86.00	
[09/26 15:45:37 visual_prompt]: Best epoch 8: best metric: 0.240
[09/26 15:45:37 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 15:45:44 visual_prompt]: Epoch 9 / 100: avg data time: 5.88e-02, avg batch time: 0.5086, average train loss: 1.6262
[09/26 15:45:45 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1696, average loss: 1.7543
[09/26 15:45:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 85.50	
[09/26 15:45:45 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 15:45:52 visual_prompt]: Epoch 10 / 100: avg data time: 5.46e-02, avg batch time: 0.5030, average train loss: 1.6829
[09/26 15:45:54 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1694, average loss: 1.7855
[09/26 15:45:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 87.50	
[09/26 15:45:54 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 15:46:00 visual_prompt]: Epoch 11 / 100: avg data time: 5.06e-02, avg batch time: 0.4998, average train loss: 1.5639
[09/26 15:46:02 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1691, average loss: 1.7340
[09/26 15:46:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 89.00	
[09/26 15:46:02 visual_prompt]: Best epoch 11: best metric: 0.285
[09/26 15:46:02 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 15:46:09 visual_prompt]: Epoch 12 / 100: avg data time: 6.16e-02, avg batch time: 0.5101, average train loss: 1.6137
[09/26 15:46:11 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 1.8680
[09/26 15:46:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 90.00	
[09/26 15:46:11 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 15:46:17 visual_prompt]: Epoch 13 / 100: avg data time: 4.65e-02, avg batch time: 0.4997, average train loss: 1.5721
[09/26 15:46:19 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 1.7941
[09/26 15:46:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 87.50	
[09/26 15:46:19 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 15:46:26 visual_prompt]: Epoch 14 / 100: avg data time: 6.09e-02, avg batch time: 0.5103, average train loss: 1.5083
[09/26 15:46:27 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1695, average loss: 1.6772
[09/26 15:46:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 92.00	
[09/26 15:46:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 15:46:34 visual_prompt]: Epoch 15 / 100: avg data time: 5.48e-02, avg batch time: 0.5041, average train loss: 1.4738
[09/26 15:46:36 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1696, average loss: 1.6423
[09/26 15:46:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 89.00	
[09/26 15:46:36 visual_prompt]: Best epoch 15: best metric: 0.295
[09/26 15:46:36 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 15:46:43 visual_prompt]: Epoch 16 / 100: avg data time: 6.05e-02, avg batch time: 0.5103, average train loss: 1.4809
[09/26 15:46:44 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 1.8047
[09/26 15:46:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 91.00	
[09/26 15:46:44 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 15:46:51 visual_prompt]: Epoch 17 / 100: avg data time: 4.88e-02, avg batch time: 0.4997, average train loss: 1.4204
[09/26 15:46:53 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 1.7327
[09/26 15:46:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 91.50	
[09/26 15:46:53 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 15:47:00 visual_prompt]: Epoch 18 / 100: avg data time: 5.86e-02, avg batch time: 0.5095, average train loss: 1.4039
[09/26 15:47:01 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1696, average loss: 1.6349
[09/26 15:47:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 95.00	
[09/26 15:47:01 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 15:47:08 visual_prompt]: Epoch 19 / 100: avg data time: 4.51e-02, avg batch time: 0.4949, average train loss: 1.3541
[09/26 15:47:09 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1697, average loss: 1.6913
[09/26 15:47:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 89.50	
[09/26 15:47:09 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 15:47:16 visual_prompt]: Epoch 20 / 100: avg data time: 4.76e-02, avg batch time: 0.4985, average train loss: 1.3753
[09/26 15:47:18 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1693, average loss: 1.6424
[09/26 15:47:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 93.50	
[09/26 15:47:18 visual_prompt]: Best epoch 20: best metric: 0.360
[09/26 15:47:18 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 15:47:24 visual_prompt]: Epoch 21 / 100: avg data time: 5.26e-02, avg batch time: 0.5013, average train loss: 1.3639
[09/26 15:47:26 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1697, average loss: 1.8658
[09/26 15:47:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 93.00	
[09/26 15:47:26 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 15:47:33 visual_prompt]: Epoch 22 / 100: avg data time: 5.67e-02, avg batch time: 0.5077, average train loss: 1.3123
[09/26 15:47:34 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1695, average loss: 1.5392
[09/26 15:47:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.00	
[09/26 15:47:34 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 15:47:41 visual_prompt]: Epoch 23 / 100: avg data time: 5.85e-02, avg batch time: 0.5088, average train loss: 1.2171
[09/26 15:47:43 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1692, average loss: 1.6499
[09/26 15:47:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 96.00	
[09/26 15:47:43 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 15:47:50 visual_prompt]: Epoch 24 / 100: avg data time: 5.69e-02, avg batch time: 0.5067, average train loss: 1.1859
[09/26 15:47:51 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1697, average loss: 1.6262
[09/26 15:47:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 95.00	
[09/26 15:47:51 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 15:47:58 visual_prompt]: Epoch 25 / 100: avg data time: 5.78e-02, avg batch time: 0.5080, average train loss: 1.2024
[09/26 15:48:00 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1691, average loss: 1.5793
[09/26 15:48:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:48:00 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 15:48:06 visual_prompt]: Epoch 26 / 100: avg data time: 5.69e-02, avg batch time: 0.5061, average train loss: 1.1606
[09/26 15:48:08 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 1.5786
[09/26 15:48:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 96.50	
[09/26 15:48:08 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 15:48:15 visual_prompt]: Epoch 27 / 100: avg data time: 5.83e-02, avg batch time: 0.5072, average train loss: 1.0858
[09/26 15:48:16 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 1.5895
[09/26 15:48:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.50	
[09/26 15:48:16 visual_prompt]: Best epoch 27: best metric: 0.370
[09/26 15:48:16 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 15:48:23 visual_prompt]: Epoch 28 / 100: avg data time: 4.86e-02, avg batch time: 0.4992, average train loss: 1.0537
[09/26 15:48:25 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1695, average loss: 1.5908
[09/26 15:48:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:48:25 visual_prompt]: Best epoch 28: best metric: 0.375
[09/26 15:48:25 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 15:48:32 visual_prompt]: Epoch 29 / 100: avg data time: 4.66e-02, avg batch time: 0.4978, average train loss: 1.0556
[09/26 15:48:33 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 1.7667
[09/26 15:48:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 92.50	
[09/26 15:48:33 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 15:48:40 visual_prompt]: Epoch 30 / 100: avg data time: 6.04e-02, avg batch time: 0.5092, average train loss: 1.0114
[09/26 15:48:42 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1694, average loss: 1.8373
[09/26 15:48:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 94.00	
[09/26 15:48:42 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 15:48:48 visual_prompt]: Epoch 31 / 100: avg data time: 5.80e-02, avg batch time: 0.5073, average train loss: 0.9534
[09/26 15:48:50 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1698, average loss: 1.6306
[09/26 15:48:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 15:48:50 visual_prompt]: Best epoch 31: best metric: 0.400
[09/26 15:48:50 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 15:48:57 visual_prompt]: Epoch 32 / 100: avg data time: 5.56e-02, avg batch time: 0.5043, average train loss: 0.8582
[09/26 15:48:58 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 1.6230
[09/26 15:48:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 15:48:58 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 15:49:05 visual_prompt]: Epoch 33 / 100: avg data time: 4.92e-02, avg batch time: 0.4992, average train loss: 0.8879
[09/26 15:49:07 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1696, average loss: 1.9419
[09/26 15:49:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 94.00	
[09/26 15:49:07 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 15:49:14 visual_prompt]: Epoch 34 / 100: avg data time: 5.71e-02, avg batch time: 0.5062, average train loss: 0.9365
[09/26 15:49:15 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1698, average loss: 1.7581
[09/26 15:49:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.50	
[09/26 15:49:15 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 15:49:22 visual_prompt]: Epoch 35 / 100: avg data time: 4.93e-02, avg batch time: 0.4989, average train loss: 0.8234
[09/26 15:49:24 visual_prompt]: Inference (val):avg data time: 5.39e-05, avg batch time: 0.1692, average loss: 1.7264
[09/26 15:49:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 97.00	
[09/26 15:49:24 visual_prompt]: Best epoch 35: best metric: 0.425
[09/26 15:49:24 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 15:49:30 visual_prompt]: Epoch 36 / 100: avg data time: 5.04e-02, avg batch time: 0.5007, average train loss: 0.7035
[09/26 15:49:32 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1692, average loss: 2.2707
[09/26 15:49:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.00	
[09/26 15:49:32 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 15:49:39 visual_prompt]: Epoch 37 / 100: avg data time: 5.59e-02, avg batch time: 0.5046, average train loss: 0.7425
[09/26 15:49:40 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1694, average loss: 1.9940
[09/26 15:49:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 15:49:40 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 15:49:47 visual_prompt]: Epoch 38 / 100: avg data time: 5.36e-02, avg batch time: 0.5030, average train loss: 0.7001
[09/26 15:49:49 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 2.0323
[09/26 15:49:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 15:49:49 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 15:49:56 visual_prompt]: Epoch 39 / 100: avg data time: 5.02e-02, avg batch time: 0.4990, average train loss: 0.6545
[09/26 15:49:57 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 2.1540
[09/26 15:49:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.00	
[09/26 15:49:57 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 15:50:04 visual_prompt]: Epoch 40 / 100: avg data time: 4.48e-02, avg batch time: 0.4956, average train loss: 0.5775
[09/26 15:50:05 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 2.3283
[09/26 15:50:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 15:50:05 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 15:50:12 visual_prompt]: Epoch 41 / 100: avg data time: 5.00e-02, avg batch time: 0.4993, average train loss: 0.4915
[09/26 15:50:14 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1696, average loss: 2.2969
[09/26 15:50:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:50:14 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 15:50:21 visual_prompt]: Epoch 42 / 100: avg data time: 5.49e-02, avg batch time: 0.5053, average train loss: 0.5345
[09/26 15:50:22 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1690, average loss: 2.3829
[09/26 15:50:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:50:22 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 15:50:29 visual_prompt]: Epoch 43 / 100: avg data time: 5.02e-02, avg batch time: 0.5006, average train loss: 0.7162
[09/26 15:50:31 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 2.0218
[09/26 15:50:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 15:50:31 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 15:50:37 visual_prompt]: Epoch 44 / 100: avg data time: 4.49e-02, avg batch time: 0.4938, average train loss: 0.5681
[09/26 15:50:39 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1693, average loss: 2.5130
[09/26 15:50:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 95.50	
[09/26 15:50:39 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 15:50:46 visual_prompt]: Epoch 45 / 100: avg data time: 5.82e-02, avg batch time: 0.5080, average train loss: 0.4166
[09/26 15:50:47 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 2.4291
[09/26 15:50:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.50	
[09/26 15:50:47 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 15:50:54 visual_prompt]: Epoch 46 / 100: avg data time: 5.85e-02, avg batch time: 0.5082, average train loss: 0.3786
[09/26 15:50:56 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1695, average loss: 3.0140
[09/26 15:50:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 95.50	
[09/26 15:50:56 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 15:51:03 visual_prompt]: Epoch 47 / 100: avg data time: 6.09e-02, avg batch time: 0.5108, average train loss: 0.4162
[09/26 15:51:04 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1692, average loss: 2.7167
[09/26 15:51:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.00	
[09/26 15:51:04 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 15:51:11 visual_prompt]: Epoch 48 / 100: avg data time: 5.45e-02, avg batch time: 0.5050, average train loss: 0.4074
[09/26 15:51:13 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1697, average loss: 2.6747
[09/26 15:51:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.00	
[09/26 15:51:13 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 15:51:19 visual_prompt]: Epoch 49 / 100: avg data time: 4.30e-02, avg batch time: 0.4932, average train loss: 0.3174
[09/26 15:51:21 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1694, average loss: 3.2169
[09/26 15:51:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:51:21 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 15:51:28 visual_prompt]: Epoch 50 / 100: avg data time: 5.45e-02, avg batch time: 0.5039, average train loss: 0.2944
[09/26 15:51:30 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1691, average loss: 3.0072
[09/26 15:51:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 97.50	
[09/26 15:51:30 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 15:51:36 visual_prompt]: Epoch 51 / 100: avg data time: 4.02e-02, avg batch time: 0.4919, average train loss: 0.2453
[09/26 15:51:38 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1692, average loss: 3.3227
[09/26 15:51:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 95.00	
[09/26 15:51:38 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 15:51:44 visual_prompt]: Epoch 52 / 100: avg data time: 4.54e-02, avg batch time: 0.4961, average train loss: 0.4120
[09/26 15:51:46 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1692, average loss: 2.9339
[09/26 15:51:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.00	
[09/26 15:51:46 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 15:51:53 visual_prompt]: Epoch 53 / 100: avg data time: 4.17e-02, avg batch time: 0.4925, average train loss: 0.2611
[09/26 15:51:54 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 3.2001
[09/26 15:51:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 97.00	
[09/26 15:51:54 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 15:52:01 visual_prompt]: Epoch 54 / 100: avg data time: 5.56e-02, avg batch time: 0.5045, average train loss: 0.1786
[09/26 15:52:02 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 3.3428
[09/26 15:52:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:52:02 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 15:52:09 visual_prompt]: Epoch 55 / 100: avg data time: 5.29e-02, avg batch time: 0.5030, average train loss: 0.1234
[09/26 15:52:11 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 4.0015
[09/26 15:52:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 94.50	
[09/26 15:52:11 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 15:52:18 visual_prompt]: Epoch 56 / 100: avg data time: 5.54e-02, avg batch time: 0.5043, average train loss: 0.1771
[09/26 15:52:19 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1696, average loss: 3.9732
[09/26 15:52:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 96.00	
[09/26 15:52:19 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 15:52:26 visual_prompt]: Epoch 57 / 100: avg data time: 5.19e-02, avg batch time: 0.5027, average train loss: 0.1911
[09/26 15:52:27 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1695, average loss: 3.6779
[09/26 15:52:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.00	
[09/26 15:52:27 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 15:52:34 visual_prompt]: Epoch 58 / 100: avg data time: 4.91e-02, avg batch time: 0.4990, average train loss: 0.1210
[09/26 15:52:36 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 3.9989
[09/26 15:52:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 95.50	
[09/26 15:52:36 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 15:52:43 visual_prompt]: Epoch 59 / 100: avg data time: 4.86e-02, avg batch time: 0.4985, average train loss: 0.1790
[09/26 15:52:44 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1698, average loss: 4.0539
[09/26 15:52:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 15:52:44 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 15:52:51 visual_prompt]: Epoch 60 / 100: avg data time: 4.66e-02, avg batch time: 0.4973, average train loss: 0.0780
[09/26 15:52:52 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1696, average loss: 4.2750
[09/26 15:52:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.00	
[09/26 15:52:52 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 15:52:59 visual_prompt]: Epoch 61 / 100: avg data time: 4.82e-02, avg batch time: 0.4982, average train loss: 0.0679
[09/26 15:53:01 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1697, average loss: 4.6319
[09/26 15:53:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 15:53:01 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 15:53:08 visual_prompt]: Epoch 62 / 100: avg data time: 5.48e-02, avg batch time: 0.5055, average train loss: 0.0474
[09/26 15:53:09 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1691, average loss: 4.6254
[09/26 15:53:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 15:53:09 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 15:53:16 visual_prompt]: Epoch 63 / 100: avg data time: 5.51e-02, avg batch time: 0.5040, average train loss: 0.0647
[09/26 15:53:18 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1698, average loss: 5.0081
[09/26 15:53:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 15:53:18 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 15:53:25 visual_prompt]: Epoch 64 / 100: avg data time: 6.37e-02, avg batch time: 0.5133, average train loss: 0.0503
[09/26 15:53:26 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1695, average loss: 4.8479
[09/26 15:53:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.50	
[09/26 15:53:26 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 15:53:33 visual_prompt]: Epoch 65 / 100: avg data time: 5.65e-02, avg batch time: 0.5069, average train loss: 0.0377
[09/26 15:53:35 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1696, average loss: 5.1270
[09/26 15:53:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 15:53:35 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 15:53:42 visual_prompt]: Epoch 66 / 100: avg data time: 6.08e-02, avg batch time: 0.5112, average train loss: 0.0278
[09/26 15:53:43 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1694, average loss: 5.0699
[09/26 15:53:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.00	
[09/26 15:53:43 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 15:53:50 visual_prompt]: Epoch 67 / 100: avg data time: 5.84e-02, avg batch time: 0.5072, average train loss: 0.0184
[09/26 15:53:52 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 5.0508
[09/26 15:53:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 15:53:52 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 15:53:58 visual_prompt]: Epoch 68 / 100: avg data time: 4.98e-02, avg batch time: 0.4997, average train loss: 0.0116
[09/26 15:54:00 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1694, average loss: 5.3415
[09/26 15:54:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.50	
[09/26 15:54:00 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 15:54:07 visual_prompt]: Epoch 69 / 100: avg data time: 5.58e-02, avg batch time: 0.5049, average train loss: 0.0141
[09/26 15:54:08 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 5.1038
[09/26 15:54:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 15:54:08 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 15:54:15 visual_prompt]: Epoch 70 / 100: avg data time: 4.75e-02, avg batch time: 0.4964, average train loss: 0.0086
[09/26 15:54:17 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1695, average loss: 5.2060
[09/26 15:54:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 15:54:17 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 15:54:24 visual_prompt]: Epoch 71 / 100: avg data time: 6.44e-02, avg batch time: 0.5137, average train loss: 0.0091
[09/26 15:54:25 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1695, average loss: 5.3502
[09/26 15:54:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.00	
[09/26 15:54:25 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 15:54:32 visual_prompt]: Epoch 72 / 100: avg data time: 4.61e-02, avg batch time: 0.4975, average train loss: 0.0241
[09/26 15:54:33 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 5.2523
[09/26 15:54:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 15:54:33 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 15:54:40 visual_prompt]: Epoch 73 / 100: avg data time: 4.74e-02, avg batch time: 0.4967, average train loss: 0.0240
[09/26 15:54:42 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1695, average loss: 5.4182
[09/26 15:54:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 15:54:42 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 15:54:49 visual_prompt]: Epoch 74 / 100: avg data time: 5.99e-02, avg batch time: 0.5096, average train loss: 0.0094
[09/26 15:54:50 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1696, average loss: 5.3696
[09/26 15:54:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 15:54:50 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 15:54:57 visual_prompt]: Epoch 75 / 100: avg data time: 5.00e-02, avg batch time: 0.4988, average train loss: 0.0078
[09/26 15:54:59 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 5.5226
[09/26 15:54:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 15:54:59 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 15:55:05 visual_prompt]: Epoch 76 / 100: avg data time: 4.44e-02, avg batch time: 0.4957, average train loss: 0.0060
[09/26 15:55:07 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1698, average loss: 5.5708
[09/26 15:55:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 15:55:07 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 15:55:14 visual_prompt]: Epoch 77 / 100: avg data time: 5.62e-02, avg batch time: 0.5048, average train loss: 0.0039
[09/26 15:55:15 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1698, average loss: 5.4781
[09/26 15:55:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 15:55:15 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 15:55:22 visual_prompt]: Epoch 78 / 100: avg data time: 5.67e-02, avg batch time: 0.5064, average train loss: 0.0025
[09/26 15:55:24 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1697, average loss: 5.4163
[09/26 15:55:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 15:55:24 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 15:55:31 visual_prompt]: Epoch 79 / 100: avg data time: 5.44e-02, avg batch time: 0.5047, average train loss: 0.0053
[09/26 15:55:32 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1692, average loss: 5.5185
[09/26 15:55:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 15:55:32 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 15:55:39 visual_prompt]: Epoch 80 / 100: avg data time: 5.26e-02, avg batch time: 0.5025, average train loss: 0.0073
[09/26 15:55:41 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1695, average loss: 5.6472
[09/26 15:55:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.50	
[09/26 15:55:41 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 15:55:48 visual_prompt]: Epoch 81 / 100: avg data time: 6.85e-02, avg batch time: 0.5172, average train loss: 0.0044
[09/26 15:55:49 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1696, average loss: 5.6810
[09/26 15:55:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.50	
[09/26 15:55:49 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 15:55:56 visual_prompt]: Epoch 82 / 100: avg data time: 5.92e-02, avg batch time: 0.5080, average train loss: 0.0037
[09/26 15:55:58 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1696, average loss: 5.6873
[09/26 15:55:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 15:55:58 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 15:56:04 visual_prompt]: Epoch 83 / 100: avg data time: 5.91e-02, avg batch time: 0.5081, average train loss: 0.0025
[09/26 15:56:06 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 5.6704
[09/26 15:56:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 96.50	
[09/26 15:56:06 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 15:56:13 visual_prompt]: Epoch 84 / 100: avg data time: 4.68e-02, avg batch time: 0.4972, average train loss: 0.0030
[09/26 15:56:14 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1696, average loss: 5.6519
[09/26 15:56:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.00	
[09/26 15:56:14 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 15:56:21 visual_prompt]: Epoch 85 / 100: avg data time: 4.47e-02, avg batch time: 0.4937, average train loss: 0.0020
[09/26 15:56:23 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1697, average loss: 5.6543
[09/26 15:56:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 95.50	
[09/26 15:56:23 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 15:56:29 visual_prompt]: Epoch 86 / 100: avg data time: 4.73e-02, avg batch time: 0.4977, average train loss: 0.0027
[09/26 15:56:31 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 5.6552
[09/26 15:56:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.00	
[09/26 15:56:31 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 15:56:38 visual_prompt]: Epoch 87 / 100: avg data time: 4.93e-02, avg batch time: 0.5033, average train loss: 0.0024
[09/26 15:56:39 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 5.6564
[09/26 15:56:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.00	
[09/26 15:56:39 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 15:56:46 visual_prompt]: Epoch 88 / 100: avg data time: 5.80e-02, avg batch time: 0.5071, average train loss: 0.0019
[09/26 15:56:48 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1695, average loss: 5.6548
[09/26 15:56:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.00	
[09/26 15:56:48 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 15:56:54 visual_prompt]: Epoch 89 / 100: avg data time: 5.53e-02, avg batch time: 0.5045, average train loss: 0.0022
[09/26 15:56:56 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1694, average loss: 5.6558
[09/26 15:56:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.00	
[09/26 15:56:56 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 15:57:03 visual_prompt]: Epoch 90 / 100: avg data time: 5.58e-02, avg batch time: 0.5045, average train loss: 0.0020
[09/26 15:57:04 visual_prompt]: Inference (val):avg data time: 4.41e-05, avg batch time: 0.1697, average loss: 5.6634
[09/26 15:57:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.00	
[09/26 15:57:04 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 15:57:11 visual_prompt]: Epoch 91 / 100: avg data time: 5.43e-02, avg batch time: 0.5046, average train loss: 0.0018
[09/26 15:57:13 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1694, average loss: 5.6709
[09/26 15:57:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 96.00	
[09/26 15:57:13 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 15:57:20 visual_prompt]: Epoch 92 / 100: avg data time: 4.63e-02, avg batch time: 0.4961, average train loss: 0.0019
[09/26 15:57:21 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.1695, average loss: 5.6728
[09/26 15:57:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:57:21 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 15:57:28 visual_prompt]: Epoch 93 / 100: avg data time: 5.48e-02, avg batch time: 0.5035, average train loss: 0.0020
[09/26 15:57:30 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1698, average loss: 5.6742
[09/26 15:57:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:57:30 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 15:57:36 visual_prompt]: Epoch 94 / 100: avg data time: 5.48e-02, avg batch time: 0.5073, average train loss: 0.0043
[09/26 15:57:38 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1692, average loss: 5.6686
[09/26 15:57:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:57:38 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 15:57:45 visual_prompt]: Epoch 95 / 100: avg data time: 5.97e-02, avg batch time: 0.5085, average train loss: 0.0015
[09/26 15:57:46 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1695, average loss: 5.6626
[09/26 15:57:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:57:46 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 15:57:53 visual_prompt]: Epoch 96 / 100: avg data time: 5.91e-02, avg batch time: 0.5096, average train loss: 0.0019
[09/26 15:57:55 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 5.6625
[09/26 15:57:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:57:55 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 15:58:02 visual_prompt]: Epoch 97 / 100: avg data time: 4.65e-02, avg batch time: 0.4972, average train loss: 0.0018
[09/26 15:58:03 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 5.6628
[09/26 15:58:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:58:03 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 15:58:10 visual_prompt]: Epoch 98 / 100: avg data time: 5.10e-02, avg batch time: 0.5020, average train loss: 0.0057
[09/26 15:58:12 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 5.6634
[09/26 15:58:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:58:12 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 15:58:18 visual_prompt]: Epoch 99 / 100: avg data time: 4.48e-02, avg batch time: 0.4963, average train loss: 0.0018
[09/26 15:58:20 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1695, average loss: 5.6637
[09/26 15:58:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:58:20 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 15:58:27 visual_prompt]: Epoch 100 / 100: avg data time: 4.63e-02, avg batch time: 0.4960, average train loss: 0.0014
[09/26 15:58:28 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1695, average loss: 5.6637
[09/26 15:58:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 15:58:28 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 15:58:28 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 15:58:28 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 15:58:28 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 15:58:28 visual_prompt]: Training with config:
[09/26 15:58:28 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 15:58:28 visual_prompt]: Loading training data...
[09/26 15:58:28 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 15:58:30 visual_prompt]: Number of images: 800
[09/26 15:58:30 visual_prompt]: Number of classes: 6 / 6
[09/26 15:58:30 visual_prompt]: Loading validation data...
[09/26 15:58:30 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 15:58:30 visual_prompt]: Number of images: 200
[09/26 15:58:30 visual_prompt]: Number of classes: 6 / 6
[09/26 15:58:30 visual_prompt]: Constructing models...
[09/26 15:58:32 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 15:58:32 visual_prompt]: tuned percent:0.540
[09/26 15:58:33 visual_prompt]: Device used for model: 0
[09/26 15:58:33 visual_prompt]: Setting up Evaluator...
[09/26 15:58:33 visual_prompt]: Setting up Trainer...
[09/26 15:58:33 visual_prompt]: 	Setting up the optimizer...
[09/26 15:58:33 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 15:58:39 visual_prompt]: Epoch 1 / 100: avg data time: 4.88e-02, avg batch time: 0.4964, average train loss: 2.9584
[09/26 15:58:41 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1688, average loss: 2.9268
[09/26 15:58:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 15:58:41 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 15:58:41 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 15:58:48 visual_prompt]: Epoch 2 / 100: avg data time: 5.78e-02, avg batch time: 0.5058, average train loss: 2.0802
[09/26 15:58:49 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1686, average loss: 1.8350
[09/26 15:58:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/26 15:58:49 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 15:58:56 visual_prompt]: Epoch 3 / 100: avg data time: 5.67e-02, avg batch time: 0.5051, average train loss: 1.7977
[09/26 15:58:58 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1689, average loss: 1.8408
[09/26 15:58:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 85.00	
[09/26 15:58:58 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 15:59:05 visual_prompt]: Epoch 4 / 100: avg data time: 5.36e-02, avg batch time: 0.5053, average train loss: 1.7833
[09/26 15:59:06 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 1.8019
[09/26 15:59:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 83.50	
[09/26 15:59:06 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 15:59:13 visual_prompt]: Epoch 5 / 100: avg data time: 4.51e-02, avg batch time: 0.4950, average train loss: 1.7676
[09/26 15:59:14 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1690, average loss: 1.8114
[09/26 15:59:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/26 15:59:14 visual_prompt]: Best epoch 5: best metric: 0.205
[09/26 15:59:14 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 15:59:21 visual_prompt]: Epoch 6 / 100: avg data time: 5.75e-02, avg batch time: 0.5067, average train loss: 1.7624
[09/26 15:59:23 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1687, average loss: 1.8366
[09/26 15:59:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.50	
[09/26 15:59:23 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 15:59:30 visual_prompt]: Epoch 7 / 100: avg data time: 4.86e-02, avg batch time: 0.4992, average train loss: 1.7687
[09/26 15:59:31 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 1.7817
[09/26 15:59:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 84.00	
[09/26 15:59:31 visual_prompt]: Best epoch 7: best metric: 0.245
[09/26 15:59:31 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 15:59:38 visual_prompt]: Epoch 8 / 100: avg data time: 5.12e-02, avg batch time: 0.5013, average train loss: 1.7123
[09/26 15:59:40 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1689, average loss: 1.8174
[09/26 15:59:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 16.50	top5: 86.00	
[09/26 15:59:40 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 15:59:46 visual_prompt]: Epoch 9 / 100: avg data time: 5.74e-02, avg batch time: 0.5070, average train loss: 1.6654
[09/26 15:59:48 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1691, average loss: 1.7697
[09/26 15:59:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 86.00	
[09/26 15:59:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 15:59:55 visual_prompt]: Epoch 10 / 100: avg data time: 5.56e-02, avg batch time: 0.5057, average train loss: 1.6252
[09/26 15:59:56 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1690, average loss: 1.7720
[09/26 15:59:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 88.50	
[09/26 15:59:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 16:00:03 visual_prompt]: Epoch 11 / 100: avg data time: 5.01e-02, avg batch time: 0.4996, average train loss: 1.5733
[09/26 16:00:05 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1691, average loss: 1.7266
[09/26 16:00:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 89.50	
[09/26 16:00:05 visual_prompt]: Best epoch 11: best metric: 0.285
[09/26 16:00:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 16:00:12 visual_prompt]: Epoch 12 / 100: avg data time: 5.21e-02, avg batch time: 0.5019, average train loss: 1.6257
[09/26 16:00:13 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1695, average loss: 1.7766
[09/26 16:00:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 90.00	
[09/26 16:00:13 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 16:00:20 visual_prompt]: Epoch 13 / 100: avg data time: 5.99e-02, avg batch time: 0.5087, average train loss: 1.5532
[09/26 16:00:22 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1697, average loss: 1.7457
[09/26 16:00:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 89.50	
[09/26 16:00:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 16:00:29 visual_prompt]: Epoch 14 / 100: avg data time: 5.65e-02, avg batch time: 0.5073, average train loss: 1.5143
[09/26 16:00:30 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 1.7283
[09/26 16:00:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 92.00	
[09/26 16:00:30 visual_prompt]: Best epoch 14: best metric: 0.295
[09/26 16:00:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 16:00:37 visual_prompt]: Epoch 15 / 100: avg data time: 5.96e-02, avg batch time: 0.5091, average train loss: 1.4997
[09/26 16:00:39 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 1.8566
[09/26 16:00:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.50	top5: 88.00	
[09/26 16:00:39 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 16:00:46 visual_prompt]: Epoch 16 / 100: avg data time: 4.65e-02, avg batch time: 0.4975, average train loss: 1.4674
[09/26 16:00:47 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1696, average loss: 1.8058
[09/26 16:00:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 88.00	
[09/26 16:00:47 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 16:00:54 visual_prompt]: Epoch 17 / 100: avg data time: 5.52e-02, avg batch time: 0.5056, average train loss: 1.4675
[09/26 16:00:55 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1691, average loss: 1.7788
[09/26 16:00:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 91.00	
[09/26 16:00:55 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 16:01:02 visual_prompt]: Epoch 18 / 100: avg data time: 4.53e-02, avg batch time: 0.4945, average train loss: 1.3962
[09/26 16:01:04 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1692, average loss: 1.7275
[09/26 16:01:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 92.50	
[09/26 16:01:04 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 16:01:11 visual_prompt]: Epoch 19 / 100: avg data time: 5.60e-02, avg batch time: 0.5059, average train loss: 1.4162
[09/26 16:01:12 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1695, average loss: 1.6623
[09/26 16:01:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 91.50	
[09/26 16:01:12 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 16:01:19 visual_prompt]: Epoch 20 / 100: avg data time: 5.23e-02, avg batch time: 0.5029, average train loss: 1.3856
[09/26 16:01:20 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 1.7667
[09/26 16:01:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 88.50	
[09/26 16:01:20 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 16:01:27 visual_prompt]: Epoch 21 / 100: avg data time: 5.72e-02, avg batch time: 0.5059, average train loss: 1.3549
[09/26 16:01:29 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1689, average loss: 1.7356
[09/26 16:01:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 93.50	
[09/26 16:01:29 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 16:01:36 visual_prompt]: Epoch 22 / 100: avg data time: 4.55e-02, avg batch time: 0.4958, average train loss: 1.3295
[09/26 16:01:37 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 1.6139
[09/26 16:01:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 95.00	
[09/26 16:01:37 visual_prompt]: Best epoch 22: best metric: 0.320
[09/26 16:01:37 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 16:01:44 visual_prompt]: Epoch 23 / 100: avg data time: 5.60e-02, avg batch time: 0.5054, average train loss: 1.2836
[09/26 16:01:46 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1693, average loss: 1.5622
[09/26 16:01:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 16:01:46 visual_prompt]: Best epoch 23: best metric: 0.375
[09/26 16:01:46 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 16:01:52 visual_prompt]: Epoch 24 / 100: avg data time: 4.53e-02, avg batch time: 0.4961, average train loss: 1.2493
[09/26 16:01:54 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1693, average loss: 1.5562
[09/26 16:01:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 95.50	
[09/26 16:01:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 16:02:01 visual_prompt]: Epoch 25 / 100: avg data time: 5.61e-02, avg batch time: 0.5051, average train loss: 1.1647
[09/26 16:02:02 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1692, average loss: 1.5788
[09/26 16:02:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 95.00	
[09/26 16:02:02 visual_prompt]: Best epoch 25: best metric: 0.380
[09/26 16:02:02 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 16:02:09 visual_prompt]: Epoch 26 / 100: avg data time: 6.03e-02, avg batch time: 0.5101, average train loss: 1.1257
[09/26 16:02:11 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1692, average loss: 1.5077
[09/26 16:02:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 16:02:11 visual_prompt]: Best epoch 26: best metric: 0.385
[09/26 16:02:11 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 16:02:18 visual_prompt]: Epoch 27 / 100: avg data time: 4.93e-02, avg batch time: 0.4984, average train loss: 1.0852
[09/26 16:02:19 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 1.6739
[09/26 16:02:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 98.00	
[09/26 16:02:19 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 16:02:26 visual_prompt]: Epoch 28 / 100: avg data time: 6.03e-02, avg batch time: 0.5112, average train loss: 1.0787
[09/26 16:02:28 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1693, average loss: 1.7084
[09/26 16:02:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.00	
[09/26 16:02:28 visual_prompt]: Best epoch 28: best metric: 0.405
[09/26 16:02:28 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 16:02:35 visual_prompt]: Epoch 29 / 100: avg data time: 6.44e-02, avg batch time: 0.5146, average train loss: 1.0227
[09/26 16:02:36 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1696, average loss: 1.7109
[09/26 16:02:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 93.50	
[09/26 16:02:36 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 16:02:43 visual_prompt]: Epoch 30 / 100: avg data time: 6.39e-02, avg batch time: 0.5126, average train loss: 1.0007
[09/26 16:02:45 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1691, average loss: 1.9764
[09/26 16:02:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 92.50	
[09/26 16:02:45 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 16:02:52 visual_prompt]: Epoch 31 / 100: avg data time: 5.28e-02, avg batch time: 0.5035, average train loss: 1.1379
[09/26 16:02:53 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 1.8463
[09/26 16:02:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 93.00	
[09/26 16:02:53 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 16:03:00 visual_prompt]: Epoch 32 / 100: avg data time: 6.59e-02, avg batch time: 0.5148, average train loss: 1.0755
[09/26 16:03:02 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1694, average loss: 1.6432
[09/26 16:03:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 96.00	
[09/26 16:03:02 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 16:03:08 visual_prompt]: Epoch 33 / 100: avg data time: 5.06e-02, avg batch time: 0.5013, average train loss: 0.9803
[09/26 16:03:10 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1695, average loss: 1.5767
[09/26 16:03:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 16:03:10 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 16:03:17 visual_prompt]: Epoch 34 / 100: avg data time: 5.83e-02, avg batch time: 0.5074, average train loss: 0.8922
[09/26 16:03:19 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1692, average loss: 1.7200
[09/26 16:03:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 16:03:19 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 16:03:25 visual_prompt]: Epoch 35 / 100: avg data time: 4.82e-02, avg batch time: 0.4978, average train loss: 0.8948
[09/26 16:03:27 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1692, average loss: 1.6752
[09/26 16:03:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 96.50	
[09/26 16:03:27 visual_prompt]: Best epoch 35: best metric: 0.430
[09/26 16:03:27 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 16:03:34 visual_prompt]: Epoch 36 / 100: avg data time: 5.83e-02, avg batch time: 0.5075, average train loss: 0.7469
[09/26 16:03:35 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1696, average loss: 1.8843
[09/26 16:03:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 95.50	
[09/26 16:03:35 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 16:03:42 visual_prompt]: Epoch 37 / 100: avg data time: 5.88e-02, avg batch time: 0.5080, average train loss: 0.7111
[09/26 16:03:44 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1694, average loss: 1.9015
[09/26 16:03:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 96.50	
[09/26 16:03:44 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 16:03:51 visual_prompt]: Epoch 38 / 100: avg data time: 6.08e-02, avg batch time: 0.5097, average train loss: 0.6605
[09/26 16:03:52 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1695, average loss: 1.8760
[09/26 16:03:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.50	
[09/26 16:03:52 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 16:03:59 visual_prompt]: Epoch 39 / 100: avg data time: 5.43e-02, avg batch time: 0.5027, average train loss: 0.6533
[09/26 16:04:01 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1697, average loss: 1.9957
[09/26 16:04:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.00	top5: 96.00	
[09/26 16:04:01 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 16:04:08 visual_prompt]: Epoch 40 / 100: avg data time: 5.85e-02, avg batch time: 0.5070, average train loss: 0.6360
[09/26 16:04:09 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1698, average loss: 1.9591
[09/26 16:04:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 97.50	
[09/26 16:04:09 visual_prompt]: Best epoch 40: best metric: 0.440
[09/26 16:04:09 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 16:04:16 visual_prompt]: Epoch 41 / 100: avg data time: 5.88e-02, avg batch time: 0.5079, average train loss: 0.6668
[09/26 16:04:18 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1694, average loss: 2.0665
[09/26 16:04:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.00	
[09/26 16:04:18 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 16:04:24 visual_prompt]: Epoch 42 / 100: avg data time: 4.69e-02, avg batch time: 0.4980, average train loss: 0.7430
[09/26 16:04:26 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1695, average loss: 2.3482
[09/26 16:04:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 94.50	
[09/26 16:04:26 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 16:04:33 visual_prompt]: Epoch 43 / 100: avg data time: 5.44e-02, avg batch time: 0.5040, average train loss: 0.6112
[09/26 16:04:34 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1690, average loss: 2.2631
[09/26 16:04:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 96.50	
[09/26 16:04:34 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 16:04:41 visual_prompt]: Epoch 44 / 100: avg data time: 4.61e-02, avg batch time: 0.4952, average train loss: 0.5313
[09/26 16:04:42 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 2.9008
[09/26 16:04:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 95.50	
[09/26 16:04:42 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 16:04:49 visual_prompt]: Epoch 45 / 100: avg data time: 5.24e-02, avg batch time: 0.5016, average train loss: 0.4454
[09/26 16:04:51 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 2.2316
[09/26 16:04:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 97.00	
[09/26 16:04:51 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 16:04:58 visual_prompt]: Epoch 46 / 100: avg data time: 4.75e-02, avg batch time: 0.4998, average train loss: 0.3952
[09/26 16:04:59 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 2.6799
[09/26 16:04:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.00	top5: 94.50	
[09/26 16:04:59 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 16:05:06 visual_prompt]: Epoch 47 / 100: avg data time: 5.04e-02, avg batch time: 0.5003, average train loss: 0.3875
[09/26 16:05:07 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 2.9806
[09/26 16:05:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 95.50	
[09/26 16:05:07 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 16:05:14 visual_prompt]: Epoch 48 / 100: avg data time: 4.44e-02, avg batch time: 0.4979, average train loss: 0.3868
[09/26 16:05:16 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1698, average loss: 3.2722
[09/26 16:05:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 95.00	
[09/26 16:05:16 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 16:05:22 visual_prompt]: Epoch 49 / 100: avg data time: 4.50e-02, avg batch time: 0.4952, average train loss: 0.3545
[09/26 16:05:24 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1695, average loss: 2.5837
[09/26 16:05:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 97.50	
[09/26 16:05:24 visual_prompt]: Best epoch 49: best metric: 0.445
[09/26 16:05:24 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 16:05:31 visual_prompt]: Epoch 50 / 100: avg data time: 5.56e-02, avg batch time: 0.5066, average train loss: 0.2969
[09/26 16:05:33 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1694, average loss: 3.3987
[09/26 16:05:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.00	
[09/26 16:05:33 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 16:05:39 visual_prompt]: Epoch 51 / 100: avg data time: 5.72e-02, avg batch time: 0.5061, average train loss: 0.2917
[09/26 16:05:41 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1691, average loss: 4.0548
[09/26 16:05:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 16:05:41 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 16:05:48 visual_prompt]: Epoch 52 / 100: avg data time: 5.53e-02, avg batch time: 0.5061, average train loss: 0.2685
[09/26 16:05:49 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1691, average loss: 2.8585
[09/26 16:05:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 16:05:49 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 16:05:56 visual_prompt]: Epoch 53 / 100: avg data time: 5.92e-02, avg batch time: 0.5102, average train loss: 0.2813
[09/26 16:05:58 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1695, average loss: 3.2104
[09/26 16:05:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.00	
[09/26 16:05:58 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 16:06:05 visual_prompt]: Epoch 54 / 100: avg data time: 4.44e-02, avg batch time: 0.4950, average train loss: 0.2746
[09/26 16:06:06 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1696, average loss: 3.8140
[09/26 16:06:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.50	
[09/26 16:06:06 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 16:06:13 visual_prompt]: Epoch 55 / 100: avg data time: 6.03e-02, avg batch time: 0.5097, average train loss: 0.2481
[09/26 16:06:15 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1692, average loss: 3.3702
[09/26 16:06:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 16:06:15 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 16:06:21 visual_prompt]: Epoch 56 / 100: avg data time: 4.21e-02, avg batch time: 0.4934, average train loss: 0.1963
[09/26 16:06:23 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 3.1256
[09/26 16:06:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.00	
[09/26 16:06:23 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 16:06:30 visual_prompt]: Epoch 57 / 100: avg data time: 4.39e-02, avg batch time: 0.4943, average train loss: 0.1520
[09/26 16:06:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 3.7142
[09/26 16:06:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 95.00	
[09/26 16:06:31 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 16:06:38 visual_prompt]: Epoch 58 / 100: avg data time: 4.75e-02, avg batch time: 0.4975, average train loss: 0.1211
[09/26 16:06:39 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1696, average loss: 3.5065
[09/26 16:06:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.00	
[09/26 16:06:39 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 16:06:46 visual_prompt]: Epoch 59 / 100: avg data time: 5.81e-02, avg batch time: 0.5073, average train loss: 0.1153
[09/26 16:06:48 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 4.1848
[09/26 16:06:48 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.50	
[09/26 16:06:48 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 16:06:55 visual_prompt]: Epoch 60 / 100: avg data time: 5.20e-02, avg batch time: 0.5012, average train loss: 0.1153
[09/26 16:06:56 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1689, average loss: 4.1293
[09/26 16:06:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 16:06:56 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 16:07:03 visual_prompt]: Epoch 61 / 100: avg data time: 6.41e-02, avg batch time: 0.5130, average train loss: 0.1148
[09/26 16:07:05 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1696, average loss: 4.5710
[09/26 16:07:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 16:07:05 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 16:07:12 visual_prompt]: Epoch 62 / 100: avg data time: 4.87e-02, avg batch time: 0.5006, average train loss: 0.1475
[09/26 16:07:13 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 4.7528
[09/26 16:07:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.50	
[09/26 16:07:13 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 16:07:20 visual_prompt]: Epoch 63 / 100: avg data time: 6.01e-02, avg batch time: 0.5099, average train loss: 0.1843
[09/26 16:07:22 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1692, average loss: 4.3234
[09/26 16:07:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 16:07:22 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 16:07:29 visual_prompt]: Epoch 64 / 100: avg data time: 5.78e-02, avg batch time: 0.5066, average train loss: 0.1008
[09/26 16:07:30 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1696, average loss: 3.8205
[09/26 16:07:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.50	
[09/26 16:07:30 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 16:07:37 visual_prompt]: Epoch 65 / 100: avg data time: 5.59e-02, avg batch time: 0.5054, average train loss: 0.0861
[09/26 16:07:39 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1694, average loss: 4.1424
[09/26 16:07:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 96.50	
[09/26 16:07:39 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 16:07:46 visual_prompt]: Epoch 66 / 100: avg data time: 6.13e-02, avg batch time: 0.5112, average train loss: 0.0622
[09/26 16:07:47 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 4.3218
[09/26 16:07:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.00	
[09/26 16:07:47 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 16:07:54 visual_prompt]: Epoch 67 / 100: avg data time: 6.31e-02, avg batch time: 0.5128, average train loss: 0.0500
[09/26 16:07:56 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1695, average loss: 4.2808
[09/26 16:07:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 99.00	
[09/26 16:07:56 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 16:08:03 visual_prompt]: Epoch 68 / 100: avg data time: 6.61e-02, avg batch time: 0.5154, average train loss: 0.0488
[09/26 16:08:04 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1695, average loss: 4.6576
[09/26 16:08:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 98.00	
[09/26 16:08:04 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 16:08:11 visual_prompt]: Epoch 69 / 100: avg data time: 4.76e-02, avg batch time: 0.4980, average train loss: 0.0334
[09/26 16:08:12 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 4.3965
[09/26 16:08:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 16:08:12 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 16:08:19 visual_prompt]: Epoch 70 / 100: avg data time: 6.50e-02, avg batch time: 0.5148, average train loss: 0.0208
[09/26 16:08:21 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1689, average loss: 4.4828
[09/26 16:08:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.50	
[09/26 16:08:21 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 16:08:28 visual_prompt]: Epoch 71 / 100: avg data time: 5.58e-02, avg batch time: 0.5060, average train loss: 0.0171
[09/26 16:08:29 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1692, average loss: 4.9452
[09/26 16:08:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 16:08:29 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 16:08:37 visual_prompt]: Epoch 72 / 100: avg data time: 5.27e-02, avg batch time: 0.5287, average train loss: 0.0208
[09/26 16:08:38 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1692, average loss: 4.7337
[09/26 16:08:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/26 16:08:38 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 16:08:45 visual_prompt]: Epoch 73 / 100: avg data time: 5.35e-02, avg batch time: 0.5026, average train loss: 0.0185
[09/26 16:08:46 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1692, average loss: 4.8160
[09/26 16:08:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 16:08:46 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 16:08:53 visual_prompt]: Epoch 74 / 100: avg data time: 4.66e-02, avg batch time: 0.4972, average train loss: 0.0108
[09/26 16:08:55 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 4.9022
[09/26 16:08:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 97.50	
[09/26 16:08:55 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 16:09:02 visual_prompt]: Epoch 75 / 100: avg data time: 5.35e-02, avg batch time: 0.5032, average train loss: 0.0195
[09/26 16:09:03 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1696, average loss: 4.9532
[09/26 16:09:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 98.50	
[09/26 16:09:03 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 16:09:10 visual_prompt]: Epoch 76 / 100: avg data time: 5.02e-02, avg batch time: 0.5007, average train loss: 0.0139
[09/26 16:09:11 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 5.1329
[09/26 16:09:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 96.50	
[09/26 16:09:11 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 16:09:18 visual_prompt]: Epoch 77 / 100: avg data time: 5.39e-02, avg batch time: 0.5037, average train loss: 0.0158
[09/26 16:09:20 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1696, average loss: 5.2999
[09/26 16:09:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 16:09:20 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 16:09:27 visual_prompt]: Epoch 78 / 100: avg data time: 5.27e-02, avg batch time: 0.5025, average train loss: 0.0122
[09/26 16:09:28 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1692, average loss: 5.3629
[09/26 16:09:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.00	
[09/26 16:09:28 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 16:09:35 visual_prompt]: Epoch 79 / 100: avg data time: 5.12e-02, avg batch time: 0.5009, average train loss: 0.0107
[09/26 16:09:37 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1695, average loss: 5.3355
[09/26 16:09:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.00	
[09/26 16:09:37 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 16:09:43 visual_prompt]: Epoch 80 / 100: avg data time: 6.12e-02, avg batch time: 0.5116, average train loss: 0.0079
[09/26 16:09:45 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1691, average loss: 5.2494
[09/26 16:09:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 16:09:45 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 16:09:52 visual_prompt]: Epoch 81 / 100: avg data time: 6.51e-02, avg batch time: 0.5154, average train loss: 0.0127
[09/26 16:09:54 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1691, average loss: 5.1963
[09/26 16:09:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 16:09:54 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 16:10:01 visual_prompt]: Epoch 82 / 100: avg data time: 5.85e-02, avg batch time: 0.5081, average train loss: 0.0048
[09/26 16:10:02 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1694, average loss: 5.2294
[09/26 16:10:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 16:10:02 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 16:10:09 visual_prompt]: Epoch 83 / 100: avg data time: 4.84e-02, avg batch time: 0.4999, average train loss: 0.0044
[09/26 16:10:10 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1695, average loss: 5.2389
[09/26 16:10:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.50	
[09/26 16:10:10 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 16:10:17 visual_prompt]: Epoch 84 / 100: avg data time: 6.17e-02, avg batch time: 0.5114, average train loss: 0.0033
[09/26 16:10:19 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1694, average loss: 5.2690
[09/26 16:10:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 16:10:19 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 16:10:26 visual_prompt]: Epoch 85 / 100: avg data time: 5.53e-02, avg batch time: 0.5061, average train loss: 0.0066
[09/26 16:10:27 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1696, average loss: 5.3360
[09/26 16:10:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.00	
[09/26 16:10:27 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 16:10:34 visual_prompt]: Epoch 86 / 100: avg data time: 4.39e-02, avg batch time: 0.4937, average train loss: 0.0042
[09/26 16:10:35 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 5.3563
[09/26 16:10:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 98.00	
[09/26 16:10:35 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 16:10:42 visual_prompt]: Epoch 87 / 100: avg data time: 5.89e-02, avg batch time: 0.5086, average train loss: 0.0042
[09/26 16:10:44 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1695, average loss: 5.4114
[09/26 16:10:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.00	top5: 97.50	
[09/26 16:10:44 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 16:10:51 visual_prompt]: Epoch 88 / 100: avg data time: 5.67e-02, avg batch time: 0.5057, average train loss: 0.0036
[09/26 16:10:52 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1690, average loss: 5.4510
[09/26 16:10:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 16:10:52 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 16:10:59 visual_prompt]: Epoch 89 / 100: avg data time: 4.44e-02, avg batch time: 0.4939, average train loss: 0.0051
[09/26 16:11:01 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1697, average loss: 5.4683
[09/26 16:11:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 16:11:01 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 16:11:07 visual_prompt]: Epoch 90 / 100: avg data time: 5.79e-02, avg batch time: 0.5081, average train loss: 0.0026
[09/26 16:11:09 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1697, average loss: 5.4685
[09/26 16:11:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.50	
[09/26 16:11:09 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 16:11:16 visual_prompt]: Epoch 91 / 100: avg data time: 5.78e-02, avg batch time: 0.5069, average train loss: 0.0030
[09/26 16:11:18 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1692, average loss: 5.4736
[09/26 16:11:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 16:11:18 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 16:11:25 visual_prompt]: Epoch 92 / 100: avg data time: 6.35e-02, avg batch time: 0.5119, average train loss: 0.0025
[09/26 16:11:26 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.1691, average loss: 5.4659
[09/26 16:11:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.50	
[09/26 16:11:26 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 16:11:33 visual_prompt]: Epoch 93 / 100: avg data time: 6.56e-02, avg batch time: 0.5142, average train loss: 0.0044
[09/26 16:11:35 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 5.4477
[09/26 16:11:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 16:11:35 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 16:11:41 visual_prompt]: Epoch 94 / 100: avg data time: 4.82e-02, avg batch time: 0.4990, average train loss: 0.0030
[09/26 16:11:43 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1697, average loss: 5.4469
[09/26 16:11:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 97.50	
[09/26 16:11:43 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 16:11:50 visual_prompt]: Epoch 95 / 100: avg data time: 5.21e-02, avg batch time: 0.5007, average train loss: 0.0026
[09/26 16:11:51 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1696, average loss: 5.4501
[09/26 16:11:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 16:11:51 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 16:11:58 visual_prompt]: Epoch 96 / 100: avg data time: 5.40e-02, avg batch time: 0.5028, average train loss: 0.0032
[09/26 16:12:00 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1696, average loss: 5.4534
[09/26 16:12:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 16:12:00 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 16:12:07 visual_prompt]: Epoch 97 / 100: avg data time: 5.00e-02, avg batch time: 0.5017, average train loss: 0.0064
[09/26 16:12:08 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1695, average loss: 5.4541
[09/26 16:12:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 16:12:08 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 16:12:15 visual_prompt]: Epoch 98 / 100: avg data time: 5.80e-02, avg batch time: 0.5073, average train loss: 0.0043
[09/26 16:12:17 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1697, average loss: 5.4549
[09/26 16:12:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 16:12:17 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 16:12:23 visual_prompt]: Epoch 99 / 100: avg data time: 5.55e-02, avg batch time: 0.5048, average train loss: 0.0027
[09/26 16:12:25 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 5.4554
[09/26 16:12:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 16:12:25 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 16:12:32 visual_prompt]: Epoch 100 / 100: avg data time: 6.16e-02, avg batch time: 0.5112, average train loss: 0.0033
[09/26 16:12:33 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 5.4557
[09/26 16:12:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 97.50	
[09/26 16:12:33 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 16:12:33 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 16:12:33 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 16:12:33 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 16:12:33 visual_prompt]: Training with config:
[09/26 16:12:33 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 16:12:33 visual_prompt]: Loading training data...
[09/26 16:12:33 visual_prompt]: Constructing vtab-dmlab dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 16:12:35 visual_prompt]: Number of images: 800
[09/26 16:12:35 visual_prompt]: Number of classes: 6 / 6
[09/26 16:12:35 visual_prompt]: Loading validation data...
[09/26 16:12:35 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/26 16:12:35 visual_prompt]: Number of images: 200
[09/26 16:12:35 visual_prompt]: Number of classes: 6 / 6
[09/26 16:12:35 visual_prompt]: Constructing models...
[09/26 16:12:38 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/26 16:12:38 visual_prompt]: tuned percent:0.540
[09/26 16:12:38 visual_prompt]: Device used for model: 0
[09/26 16:12:38 visual_prompt]: Setting up Evaluator...
[09/26 16:12:38 visual_prompt]: Setting up Trainer...
[09/26 16:12:38 visual_prompt]: 	Setting up the optimizer...
[09/26 16:12:38 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 16:12:45 visual_prompt]: Epoch 1 / 100: avg data time: 6.41e-02, avg batch time: 0.5119, average train loss: 2.9809
[09/26 16:12:46 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1687, average loss: 2.9268
[09/26 16:12:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 85.50	
[09/26 16:12:46 visual_prompt]: Best epoch 1: best metric: 0.175
[09/26 16:12:46 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 16:12:53 visual_prompt]: Epoch 2 / 100: avg data time: 5.73e-02, avg batch time: 0.5045, average train loss: 2.0874
[09/26 16:12:55 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1688, average loss: 1.8351
[09/26 16:12:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/26 16:12:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 16:13:02 visual_prompt]: Epoch 3 / 100: avg data time: 6.37e-02, avg batch time: 0.5110, average train loss: 1.7925
[09/26 16:13:03 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 1.8556
[09/26 16:13:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 84.00	
[09/26 16:13:03 visual_prompt]: Best epoch 3: best metric: 0.185
[09/26 16:13:03 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 16:13:10 visual_prompt]: Epoch 4 / 100: avg data time: 5.87e-02, avg batch time: 0.5062, average train loss: 1.7826
[09/26 16:13:12 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1694, average loss: 1.8175
[09/26 16:13:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/26 16:13:12 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 16:13:18 visual_prompt]: Epoch 5 / 100: avg data time: 5.93e-02, avg batch time: 0.5069, average train loss: 1.7729
[09/26 16:13:20 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1691, average loss: 1.8322
[09/26 16:13:20 visual_prompt]: Classification results with val_vtab-dmlab: top1: 19.00	top5: 84.00	
[09/26 16:13:20 visual_prompt]: Best epoch 5: best metric: 0.190
[09/26 16:13:20 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 16:13:27 visual_prompt]: Epoch 6 / 100: avg data time: 6.37e-02, avg batch time: 0.5119, average train loss: 1.7542
[09/26 16:13:28 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 1.7802
[09/26 16:13:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 84.00	
[09/26 16:13:28 visual_prompt]: Best epoch 6: best metric: 0.200
[09/26 16:13:28 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 16:13:35 visual_prompt]: Epoch 7 / 100: avg data time: 5.96e-02, avg batch time: 0.5082, average train loss: 1.7062
[09/26 16:13:37 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 1.7810
[09/26 16:13:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.00	top5: 84.50	
[09/26 16:13:37 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 16:13:44 visual_prompt]: Epoch 8 / 100: avg data time: 5.51e-02, avg batch time: 0.5065, average train loss: 1.6348
[09/26 16:13:45 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1693, average loss: 1.7374
[09/26 16:13:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.50	top5: 90.00	
[09/26 16:13:45 visual_prompt]: Best epoch 8: best metric: 0.235
[09/26 16:13:45 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 16:13:52 visual_prompt]: Epoch 9 / 100: avg data time: 5.16e-02, avg batch time: 0.5010, average train loss: 1.6027
[09/26 16:13:54 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 1.7617
[09/26 16:13:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 90.00	
[09/26 16:13:54 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 16:14:00 visual_prompt]: Epoch 10 / 100: avg data time: 4.48e-02, avg batch time: 0.4959, average train loss: 1.5782
[09/26 16:14:02 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1691, average loss: 1.8643
[09/26 16:14:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 87.00	
[09/26 16:14:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 16:14:09 visual_prompt]: Epoch 11 / 100: avg data time: 5.28e-02, avg batch time: 0.5046, average train loss: 1.5554
[09/26 16:14:11 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 1.6937
[09/26 16:14:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 91.00	
[09/26 16:14:11 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 16:14:17 visual_prompt]: Epoch 12 / 100: avg data time: 4.66e-02, avg batch time: 0.4963, average train loss: 1.5676
[09/26 16:14:19 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1694, average loss: 1.7518
[09/26 16:14:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 92.00	
[09/26 16:14:19 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 16:14:26 visual_prompt]: Epoch 13 / 100: avg data time: 4.37e-02, avg batch time: 0.4951, average train loss: 1.5321
[09/26 16:14:27 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1694, average loss: 1.6977
[09/26 16:14:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 92.00	
[09/26 16:14:27 visual_prompt]: Best epoch 13: best metric: 0.260
[09/26 16:14:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 16:14:34 visual_prompt]: Epoch 14 / 100: avg data time: 6.20e-02, avg batch time: 0.5109, average train loss: 1.4840
[09/26 16:14:36 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1695, average loss: 1.6861
[09/26 16:14:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 92.00	
[09/26 16:14:36 visual_prompt]: Best epoch 14: best metric: 0.325
[09/26 16:14:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 16:14:42 visual_prompt]: Epoch 15 / 100: avg data time: 5.35e-02, avg batch time: 0.5024, average train loss: 1.4835
[09/26 16:14:44 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 1.6716
[09/26 16:14:44 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 92.50	
[09/26 16:14:44 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 16:14:51 visual_prompt]: Epoch 16 / 100: avg data time: 6.25e-02, avg batch time: 0.5122, average train loss: 1.4685
[09/26 16:14:53 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1696, average loss: 1.7800
[09/26 16:14:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 90.00	
[09/26 16:14:53 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 16:15:00 visual_prompt]: Epoch 17 / 100: avg data time: 6.07e-02, avg batch time: 0.5112, average train loss: 1.4884
[09/26 16:15:01 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 1.6335
[09/26 16:15:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.50	top5: 93.00	
[09/26 16:15:01 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 16:15:08 visual_prompt]: Epoch 18 / 100: avg data time: 5.17e-02, avg batch time: 0.4999, average train loss: 1.3929
[09/26 16:15:10 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 1.7269
[09/26 16:15:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 94.00	
[09/26 16:15:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 16:15:16 visual_prompt]: Epoch 19 / 100: avg data time: 5.60e-02, avg batch time: 0.5048, average train loss: 1.4142
[09/26 16:15:18 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1696, average loss: 1.7266
[09/26 16:15:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 93.50	
[09/26 16:15:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 16:15:25 visual_prompt]: Epoch 20 / 100: avg data time: 5.90e-02, avg batch time: 0.5075, average train loss: 1.3871
[09/26 16:15:26 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1691, average loss: 1.6838
[09/26 16:15:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 94.50	
[09/26 16:15:26 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 16:15:33 visual_prompt]: Epoch 21 / 100: avg data time: 6.21e-02, avg batch time: 0.5126, average train loss: 1.3343
[09/26 16:15:35 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 1.6283
[09/26 16:15:35 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 94.00	
[09/26 16:15:35 visual_prompt]: Best epoch 21: best metric: 0.350
[09/26 16:15:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 16:15:42 visual_prompt]: Epoch 22 / 100: avg data time: 5.65e-02, avg batch time: 0.5055, average train loss: 1.3269
[09/26 16:15:43 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1694, average loss: 1.6397
[09/26 16:15:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 94.00	
[09/26 16:15:43 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 16:15:50 visual_prompt]: Epoch 23 / 100: avg data time: 5.52e-02, avg batch time: 0.5040, average train loss: 1.2764
[09/26 16:15:52 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 1.6950
[09/26 16:15:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 95.00	
[09/26 16:15:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 16:15:58 visual_prompt]: Epoch 24 / 100: avg data time: 5.55e-02, avg batch time: 0.5053, average train loss: 1.2503
[09/26 16:16:00 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1697, average loss: 1.6140
[09/26 16:16:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.50	top5: 93.00	
[09/26 16:16:00 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 16:16:07 visual_prompt]: Epoch 25 / 100: avg data time: 5.32e-02, avg batch time: 0.5035, average train loss: 1.2341
[09/26 16:16:08 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1691, average loss: 1.6389
[09/26 16:16:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 94.00	
[09/26 16:16:08 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 16:16:15 visual_prompt]: Epoch 26 / 100: avg data time: 5.35e-02, avg batch time: 0.5023, average train loss: 1.2178
[09/26 16:16:17 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 1.7502
[09/26 16:16:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 96.00	
[09/26 16:16:17 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 16:16:24 visual_prompt]: Epoch 27 / 100: avg data time: 5.79e-02, avg batch time: 0.5076, average train loss: 1.1748
[09/26 16:16:25 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1694, average loss: 1.5287
[09/26 16:16:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 94.50	
[09/26 16:16:25 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 16:16:32 visual_prompt]: Epoch 28 / 100: avg data time: 5.07e-02, avg batch time: 0.4996, average train loss: 1.1002
[09/26 16:16:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1694, average loss: 1.7034
[09/26 16:16:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 95.00	
[09/26 16:16:34 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 16:16:40 visual_prompt]: Epoch 29 / 100: avg data time: 4.49e-02, avg batch time: 0.4938, average train loss: 1.0596
[09/26 16:16:42 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1691, average loss: 1.5984
[09/26 16:16:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 96.50	
[09/26 16:16:42 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 16:16:49 visual_prompt]: Epoch 30 / 100: avg data time: 5.65e-02, avg batch time: 0.5054, average train loss: 1.0192
[09/26 16:16:50 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1698, average loss: 1.6078
[09/26 16:16:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.00	
[09/26 16:16:50 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 16:16:57 visual_prompt]: Epoch 31 / 100: avg data time: 6.23e-02, avg batch time: 0.5111, average train loss: 0.9680
[09/26 16:16:59 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1691, average loss: 1.7709
[09/26 16:16:59 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 96.00	
[09/26 16:16:59 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 16:17:06 visual_prompt]: Epoch 32 / 100: avg data time: 6.54e-02, avg batch time: 0.5148, average train loss: 0.9084
[09/26 16:17:07 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1693, average loss: 1.9439
[09/26 16:17:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 97.00	
[09/26 16:17:07 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 16:17:14 visual_prompt]: Epoch 33 / 100: avg data time: 5.43e-02, avg batch time: 0.5036, average train loss: 0.9525
[09/26 16:17:16 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1695, average loss: 1.8363
[09/26 16:17:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 94.50	
[09/26 16:17:16 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 16:17:23 visual_prompt]: Epoch 34 / 100: avg data time: 5.98e-02, avg batch time: 0.5082, average train loss: 0.8469
[09/26 16:17:24 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1694, average loss: 2.2225
[09/26 16:17:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 95.50	
[09/26 16:17:24 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 16:17:31 visual_prompt]: Epoch 35 / 100: avg data time: 4.74e-02, avg batch time: 0.4980, average train loss: 1.0061
[09/26 16:17:32 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1693, average loss: 1.7084
[09/26 16:17:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 97.00	
[09/26 16:17:32 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 16:17:39 visual_prompt]: Epoch 36 / 100: avg data time: 6.80e-02, avg batch time: 0.5166, average train loss: 0.8555
[09/26 16:17:41 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 1.8466
[09/26 16:17:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 94.50	
[09/26 16:17:41 visual_prompt]: Best epoch 36: best metric: 0.365
[09/26 16:17:41 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 16:17:48 visual_prompt]: Epoch 37 / 100: avg data time: 5.25e-02, avg batch time: 0.5021, average train loss: 0.8072
[09/26 16:17:49 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1695, average loss: 1.8661
[09/26 16:17:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 98.00	
[09/26 16:17:49 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 16:17:56 visual_prompt]: Epoch 38 / 100: avg data time: 5.10e-02, avg batch time: 0.5010, average train loss: 0.8035
[09/26 16:17:58 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1694, average loss: 1.9840
[09/26 16:17:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 95.00	
[09/26 16:17:58 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 16:18:05 visual_prompt]: Epoch 39 / 100: avg data time: 4.93e-02, avg batch time: 0.5003, average train loss: 0.6779
[09/26 16:18:06 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1697, average loss: 2.2515
[09/26 16:18:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 16:18:06 visual_prompt]: Best epoch 39: best metric: 0.375
[09/26 16:18:06 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 16:18:13 visual_prompt]: Epoch 40 / 100: avg data time: 6.52e-02, avg batch time: 0.5141, average train loss: 0.5962
[09/26 16:18:15 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1693, average loss: 2.3063
[09/26 16:18:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 98.00	
[09/26 16:18:15 visual_prompt]: Best epoch 40: best metric: 0.395
[09/26 16:18:15 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 16:18:22 visual_prompt]: Epoch 41 / 100: avg data time: 5.90e-02, avg batch time: 0.5075, average train loss: 0.6148
[09/26 16:18:23 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1696, average loss: 2.2709
[09/26 16:18:23 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.00	top5: 95.50	
[09/26 16:18:23 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 16:18:30 visual_prompt]: Epoch 42 / 100: avg data time: 5.72e-02, avg batch time: 0.5065, average train loss: 0.6354
[09/26 16:18:32 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 2.2185
[09/26 16:18:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 97.00	
[09/26 16:18:32 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 16:18:39 visual_prompt]: Epoch 43 / 100: avg data time: 5.93e-02, avg batch time: 0.5082, average train loss: 0.4746
[09/26 16:18:40 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1695, average loss: 2.8482
[09/26 16:18:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 94.50	
[09/26 16:18:40 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 16:18:47 visual_prompt]: Epoch 44 / 100: avg data time: 5.95e-02, avg batch time: 0.5083, average train loss: 0.4950
[09/26 16:18:49 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1693, average loss: 2.5187
[09/26 16:18:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 95.50	
[09/26 16:18:49 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 16:18:56 visual_prompt]: Epoch 45 / 100: avg data time: 6.22e-02, avg batch time: 0.5108, average train loss: 0.4001
[09/26 16:18:57 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1693, average loss: 2.5902
[09/26 16:18:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 95.00	
[09/26 16:18:57 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 16:19:04 visual_prompt]: Epoch 46 / 100: avg data time: 4.86e-02, avg batch time: 0.5009, average train loss: 0.3421
[09/26 16:19:05 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1697, average loss: 3.2758
[09/26 16:19:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 16:19:05 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 16:19:12 visual_prompt]: Epoch 47 / 100: avg data time: 5.89e-02, avg batch time: 0.5076, average train loss: 0.3702
[09/26 16:19:14 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1695, average loss: 2.8584
[09/26 16:19:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 96.00	
[09/26 16:19:14 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 16:19:21 visual_prompt]: Epoch 48 / 100: avg data time: 5.30e-02, avg batch time: 0.5031, average train loss: 0.4613
[09/26 16:19:22 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 3.2926
[09/26 16:19:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 95.50	
[09/26 16:19:22 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 16:19:29 visual_prompt]: Epoch 49 / 100: avg data time: 4.91e-02, avg batch time: 0.5001, average train loss: 0.4512
[09/26 16:19:31 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 2.3891
[09/26 16:19:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 95.50	
[09/26 16:19:31 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 16:19:37 visual_prompt]: Epoch 50 / 100: avg data time: 5.56e-02, avg batch time: 0.5052, average train loss: 0.2804
[09/26 16:19:39 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 3.2629
[09/26 16:19:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 97.50	
[09/26 16:19:39 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 16:19:46 visual_prompt]: Epoch 51 / 100: avg data time: 4.52e-02, avg batch time: 0.4961, average train loss: 0.2993
[09/26 16:19:47 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1690, average loss: 3.7872
[09/26 16:19:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 34.50	top5: 96.50	
[09/26 16:19:47 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 16:19:54 visual_prompt]: Epoch 52 / 100: avg data time: 4.68e-02, avg batch time: 0.4968, average train loss: 0.2429
[09/26 16:19:55 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1694, average loss: 3.3060
[09/26 16:19:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 16:19:55 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 16:20:02 visual_prompt]: Epoch 53 / 100: avg data time: 5.79e-02, avg batch time: 0.5079, average train loss: 0.2344
[09/26 16:20:04 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1691, average loss: 4.0505
[09/26 16:20:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 95.50	
[09/26 16:20:04 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 16:20:11 visual_prompt]: Epoch 54 / 100: avg data time: 4.74e-02, avg batch time: 0.4972, average train loss: 0.1719
[09/26 16:20:12 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 4.3157
[09/26 16:20:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 95.00	
[09/26 16:20:12 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 16:20:19 visual_prompt]: Epoch 55 / 100: avg data time: 5.84e-02, avg batch time: 0.5082, average train loss: 0.1810
[09/26 16:20:21 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1695, average loss: 3.8767
[09/26 16:20:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 95.50	
[09/26 16:20:21 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 16:20:28 visual_prompt]: Epoch 56 / 100: avg data time: 4.89e-02, avg batch time: 0.5003, average train loss: 0.1199
[09/26 16:20:29 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1694, average loss: 4.0459
[09/26 16:20:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 96.00	
[09/26 16:20:29 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 16:20:36 visual_prompt]: Epoch 57 / 100: avg data time: 4.73e-02, avg batch time: 0.4989, average train loss: 0.1274
[09/26 16:20:38 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1695, average loss: 4.3981
[09/26 16:20:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 16:20:38 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 16:20:44 visual_prompt]: Epoch 58 / 100: avg data time: 5.10e-02, avg batch time: 0.5011, average train loss: 0.1000
[09/26 16:20:46 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1696, average loss: 4.3571
[09/26 16:20:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 95.50	
[09/26 16:20:46 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 16:20:53 visual_prompt]: Epoch 59 / 100: avg data time: 5.16e-02, avg batch time: 0.5022, average train loss: 0.0742
[09/26 16:20:54 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1694, average loss: 4.5389
[09/26 16:20:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 96.50	
[09/26 16:20:54 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 16:21:01 visual_prompt]: Epoch 60 / 100: avg data time: 4.96e-02, avg batch time: 0.4988, average train loss: 0.0697
[09/26 16:21:03 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1698, average loss: 5.3435
[09/26 16:21:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 97.00	
[09/26 16:21:03 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 16:21:09 visual_prompt]: Epoch 61 / 100: avg data time: 4.70e-02, avg batch time: 0.4966, average train loss: 0.0854
[09/26 16:21:11 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1696, average loss: 5.8569
[09/26 16:21:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.50	top5: 96.00	
[09/26 16:21:11 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 16:21:18 visual_prompt]: Epoch 62 / 100: avg data time: 5.83e-02, avg batch time: 0.5085, average train loss: 0.1314
[09/26 16:21:19 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 6.0506
[09/26 16:21:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 94.50	
[09/26 16:21:19 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 16:21:26 visual_prompt]: Epoch 63 / 100: avg data time: 6.13e-02, avg batch time: 0.5116, average train loss: 0.1857
[09/26 16:21:28 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1695, average loss: 4.8472
[09/26 16:21:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 94.00	
[09/26 16:21:28 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 16:21:35 visual_prompt]: Epoch 64 / 100: avg data time: 5.39e-02, avg batch time: 0.5028, average train loss: 0.1157
[09/26 16:21:36 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1695, average loss: 4.1743
[09/26 16:21:36 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 95.50	
[09/26 16:21:36 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 16:21:43 visual_prompt]: Epoch 65 / 100: avg data time: 5.22e-02, avg batch time: 0.5024, average train loss: 0.0743
[09/26 16:21:45 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 4.5407
[09/26 16:21:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 94.50	
[09/26 16:21:45 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 16:21:51 visual_prompt]: Epoch 66 / 100: avg data time: 4.55e-02, avg batch time: 0.4963, average train loss: 0.0556
[09/26 16:21:53 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 4.9949
[09/26 16:21:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 95.50	
[09/26 16:21:53 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 16:22:00 visual_prompt]: Epoch 67 / 100: avg data time: 4.75e-02, avg batch time: 0.4984, average train loss: 0.0280
[09/26 16:22:01 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1695, average loss: 5.1417
[09/26 16:22:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 96.50	
[09/26 16:22:01 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 16:22:08 visual_prompt]: Epoch 68 / 100: avg data time: 4.60e-02, avg batch time: 0.4974, average train loss: 0.0209
[09/26 16:22:09 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 5.0912
[09/26 16:22:09 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 16:22:09 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 16:22:16 visual_prompt]: Epoch 69 / 100: avg data time: 5.18e-02, avg batch time: 0.5019, average train loss: 0.0145
[09/26 16:22:18 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1698, average loss: 5.2637
[09/26 16:22:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 16:22:18 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 16:22:25 visual_prompt]: Epoch 70 / 100: avg data time: 4.44e-02, avg batch time: 0.4962, average train loss: 0.0134
[09/26 16:22:26 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 5.5839
[09/26 16:22:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 16:22:26 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 16:22:33 visual_prompt]: Epoch 71 / 100: avg data time: 4.71e-02, avg batch time: 0.4975, average train loss: 0.0108
[09/26 16:22:34 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1695, average loss: 5.6585
[09/26 16:22:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 16:22:34 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 16:22:41 visual_prompt]: Epoch 72 / 100: avg data time: 5.53e-02, avg batch time: 0.5044, average train loss: 0.0138
[09/26 16:22:43 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1701, average loss: 5.2258
[09/26 16:22:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 96.50	
[09/26 16:22:43 visual_prompt]: Best epoch 72: best metric: 0.405
[09/26 16:22:43 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 16:22:50 visual_prompt]: Epoch 73 / 100: avg data time: 5.63e-02, avg batch time: 0.5051, average train loss: 0.0119
[09/26 16:22:51 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1699, average loss: 5.5389
[09/26 16:22:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.50	
[09/26 16:22:51 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 16:22:58 visual_prompt]: Epoch 74 / 100: avg data time: 6.01e-02, avg batch time: 0.5094, average train loss: 0.0063
[09/26 16:23:00 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 5.7372
[09/26 16:23:00 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 16:23:00 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 16:23:07 visual_prompt]: Epoch 75 / 100: avg data time: 6.72e-02, avg batch time: 0.5159, average train loss: 0.0045
[09/26 16:23:08 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1694, average loss: 5.6211
[09/26 16:23:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 16:23:08 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 16:23:15 visual_prompt]: Epoch 76 / 100: avg data time: 4.47e-02, avg batch time: 0.4948, average train loss: 0.0087
[09/26 16:23:16 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1692, average loss: 5.7197
[09/26 16:23:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 16:23:16 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 16:23:23 visual_prompt]: Epoch 77 / 100: avg data time: 5.29e-02, avg batch time: 0.5018, average train loss: 0.0075
[09/26 16:23:25 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 5.5869
[09/26 16:23:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.00	
[09/26 16:23:25 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 16:23:32 visual_prompt]: Epoch 78 / 100: avg data time: 6.53e-02, avg batch time: 0.5148, average train loss: 0.0069
[09/26 16:23:33 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1690, average loss: 5.6729
[09/26 16:23:33 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 97.00	
[09/26 16:23:33 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 16:23:40 visual_prompt]: Epoch 79 / 100: avg data time: 5.24e-02, avg batch time: 0.5036, average train loss: 0.0060
[09/26 16:23:42 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1695, average loss: 5.7861
[09/26 16:23:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 16:23:42 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 16:23:49 visual_prompt]: Epoch 80 / 100: avg data time: 5.56e-02, avg batch time: 0.5055, average train loss: 0.0051
[09/26 16:23:50 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1700, average loss: 5.8293
[09/26 16:23:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 97.50	
[09/26 16:23:50 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 16:23:57 visual_prompt]: Epoch 81 / 100: avg data time: 5.53e-02, avg batch time: 0.5049, average train loss: 0.0048
[09/26 16:23:58 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1697, average loss: 5.6673
[09/26 16:23:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.50	
[09/26 16:23:58 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 16:24:05 visual_prompt]: Epoch 82 / 100: avg data time: 5.54e-02, avg batch time: 0.5052, average train loss: 0.0044
[09/26 16:24:07 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 5.6648
[09/26 16:24:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.50	
[09/26 16:24:07 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 16:24:14 visual_prompt]: Epoch 83 / 100: avg data time: 5.60e-02, avg batch time: 0.5058, average train loss: 0.0040
[09/26 16:24:15 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1693, average loss: 5.6917
[09/26 16:24:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.50	
[09/26 16:24:15 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 16:24:22 visual_prompt]: Epoch 84 / 100: avg data time: 5.95e-02, avg batch time: 0.5094, average train loss: 0.0028
[09/26 16:24:24 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1691, average loss: 5.7350
[09/26 16:24:24 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 97.00	
[09/26 16:24:24 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 16:24:30 visual_prompt]: Epoch 85 / 100: avg data time: 4.68e-02, avg batch time: 0.4966, average train loss: 0.0043
[09/26 16:24:32 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1697, average loss: 5.7958
[09/26 16:24:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 16:24:32 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 16:24:39 visual_prompt]: Epoch 86 / 100: avg data time: 5.30e-02, avg batch time: 0.5022, average train loss: 0.0027
[09/26 16:24:40 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1697, average loss: 5.8237
[09/26 16:24:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 97.00	
[09/26 16:24:40 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 16:24:47 visual_prompt]: Epoch 87 / 100: avg data time: 4.38e-02, avg batch time: 0.4932, average train loss: 0.0048
[09/26 16:24:49 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1691, average loss: 5.8539
[09/26 16:24:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 16:24:49 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 16:24:55 visual_prompt]: Epoch 88 / 100: avg data time: 4.96e-02, avg batch time: 0.4982, average train loss: 0.0044
[09/26 16:24:57 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1697, average loss: 5.8597
[09/26 16:24:57 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 16:24:57 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 16:25:04 visual_prompt]: Epoch 89 / 100: avg data time: 5.31e-02, avg batch time: 0.5040, average train loss: 0.0060
[09/26 16:25:05 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1690, average loss: 5.8453
[09/26 16:25:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 16:25:05 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 16:25:12 visual_prompt]: Epoch 90 / 100: avg data time: 5.75e-02, avg batch time: 0.5074, average train loss: 0.0042
[09/26 16:25:14 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 5.8766
[09/26 16:25:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 97.00	
[09/26 16:25:14 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 16:25:21 visual_prompt]: Epoch 91 / 100: avg data time: 4.92e-02, avg batch time: 0.4996, average train loss: 0.0029
[09/26 16:25:22 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1693, average loss: 5.8852
[09/26 16:25:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 16:25:22 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 16:25:29 visual_prompt]: Epoch 92 / 100: avg data time: 4.50e-02, avg batch time: 0.4950, average train loss: 0.0037
[09/26 16:25:30 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1692, average loss: 5.8830
[09/26 16:25:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 97.00	
[09/26 16:25:30 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 16:25:37 visual_prompt]: Epoch 93 / 100: avg data time: 5.85e-02, avg batch time: 0.5073, average train loss: 0.0030
[09/26 16:25:39 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 5.8664
[09/26 16:25:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 16:25:39 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 16:25:46 visual_prompt]: Epoch 94 / 100: avg data time: 5.64e-02, avg batch time: 0.5070, average train loss: 0.0043
[09/26 16:25:47 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 5.8657
[09/26 16:25:47 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 16:25:47 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 16:25:54 visual_prompt]: Epoch 95 / 100: avg data time: 5.79e-02, avg batch time: 0.5072, average train loss: 0.0041
[09/26 16:25:56 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1697, average loss: 5.8669
[09/26 16:25:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 16:25:56 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 16:26:02 visual_prompt]: Epoch 96 / 100: avg data time: 5.71e-02, avg batch time: 0.5074, average train loss: 0.0025
[09/26 16:26:04 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 5.8674
[09/26 16:26:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 16:26:04 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 16:26:11 visual_prompt]: Epoch 97 / 100: avg data time: 6.16e-02, avg batch time: 0.5114, average train loss: 0.0033
[09/26 16:26:12 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 5.8663
[09/26 16:26:12 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 16:26:12 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 16:26:19 visual_prompt]: Epoch 98 / 100: avg data time: 4.13e-02, avg batch time: 0.4918, average train loss: 0.0058
[09/26 16:26:21 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 5.8679
[09/26 16:26:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 16:26:21 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 16:26:28 visual_prompt]: Epoch 99 / 100: avg data time: 5.41e-02, avg batch time: 0.5039, average train loss: 0.0108
[09/26 16:26:29 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 5.8662
[09/26 16:26:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
[09/26 16:26:29 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 16:26:36 visual_prompt]: Epoch 100 / 100: avg data time: 4.97e-02, avg batch time: 0.5017, average train loss: 0.0031
[09/26 16:26:37 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1697, average loss: 5.8661
[09/26 16:26:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.50	top5: 97.00	
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 289, in <module>
    main(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 284, in main
    train(cfg, args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 113, in train
    seed(cfg)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 136, in seed
    torch.manual_seed(SEED)
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/random.py", line 36, in manual_seed
    seed = int(seed)
           ^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'tuple'

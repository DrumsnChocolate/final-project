/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/13 19:11:08 visual_prompt]: Rank of current process: 0. World size: 1
[11/13 19:11:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/13 19:11:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/13 19:11:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/13 19:11:08 visual_prompt]: Training with config:
[11/13 19:11:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.005_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/13 19:11:08 visual_prompt]: Loading training data...
[11/13 19:11:08 visual_prompt]: Constructing mammo-cbis dataset train...
[11/13 19:11:08 visual_prompt]: Loading validation data...
[11/13 19:11:08 visual_prompt]: Constructing mammo-cbis dataset val...
[11/13 19:11:08 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/13 19:11:10 visual_prompt]: Enable all parameters update during training
[11/13 19:11:10 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/13 19:11:10 visual_prompt]: tuned percent:100.000
[11/13 19:11:11 visual_prompt]: Device used for model: 0
[11/13 19:11:11 visual_prompt]: Setting up Evaluator...
[11/13 19:11:11 visual_prompt]: Setting up Trainer...
[11/13 19:11:11 visual_prompt]: 	Setting up the optimizer...
[11/13 19:11:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/13 19:17:41 visual_prompt]: Epoch 1 / 100: avg data time: 1.05e+01, avg batch time: 11.1437, average train loss: 6.9791
[11/13 19:18:25 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1802, average loss: 6.3857
[11/13 19:18:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/13 19:18:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/13 19:24:53 visual_prompt]: Epoch 2 / 100: avg data time: 1.05e+01, avg batch time: 11.0894, average train loss: 3.9256
[11/13 19:25:37 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1778, average loss: 1.0828
[11/13 19:25:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/13 19:25:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/13 19:32:03 visual_prompt]: Epoch 3 / 100: avg data time: 1.05e+01, avg batch time: 11.0411, average train loss: 0.9834
[11/13 19:32:47 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1752, average loss: 0.7383
[11/13 19:32:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.75	
[11/13 19:32:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/13 19:39:13 visual_prompt]: Epoch 4 / 100: avg data time: 1.05e+01, avg batch time: 11.0362, average train loss: 0.8579
[11/13 19:39:57 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1712, average loss: 0.6852
[11/13 19:39:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.68	
[11/13 19:39:57 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/13 19:46:24 visual_prompt]: Epoch 5 / 100: avg data time: 1.05e+01, avg batch time: 11.0412, average train loss: 0.7967
[11/13 19:47:07 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1785, average loss: 0.9260
[11/13 19:47:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.99	
[11/13 19:47:07 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/13 19:53:34 visual_prompt]: Epoch 6 / 100: avg data time: 1.05e+01, avg batch time: 11.0522, average train loss: 0.7849
[11/13 19:54:18 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1745, average loss: 0.8692
[11/13 19:54:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.99	
[11/13 19:54:18 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/13 20:00:45 visual_prompt]: Epoch 7 / 100: avg data time: 1.05e+01, avg batch time: 11.0504, average train loss: 0.8302
[11/13 20:01:29 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1757, average loss: 0.7134
[11/13 20:01:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 56.09	
[11/13 20:01:29 visual_prompt]: Best epoch 7: best metric: -0.713
[11/13 20:01:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/13 20:07:56 visual_prompt]: Epoch 8 / 100: avg data time: 1.05e+01, avg batch time: 11.0491, average train loss: 0.7933
[11/13 20:08:39 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1706, average loss: 0.9476
[11/13 20:08:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[11/13 20:08:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/13 20:15:06 visual_prompt]: Epoch 9 / 100: avg data time: 1.05e+01, avg batch time: 11.0338, average train loss: 0.8801
[11/13 20:15:49 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1770, average loss: 0.7957
[11/13 20:15:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.37	
[11/13 20:15:49 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/13 20:22:16 visual_prompt]: Epoch 10 / 100: avg data time: 1.05e+01, avg batch time: 11.0489, average train loss: 0.7332
[11/13 20:23:00 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1758, average loss: 0.9599
[11/13 20:23:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.88	
[11/13 20:23:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/13 20:29:27 visual_prompt]: Epoch 11 / 100: avg data time: 1.05e+01, avg batch time: 11.0424, average train loss: 0.7308
[11/13 20:30:10 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1731, average loss: 0.6863
[11/13 20:30:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.49	
[11/13 20:30:10 visual_prompt]: Best epoch 11: best metric: -0.686
[11/13 20:30:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/13 20:36:37 visual_prompt]: Epoch 12 / 100: avg data time: 1.05e+01, avg batch time: 11.0517, average train loss: 0.7546
[11/13 20:37:21 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1774, average loss: 0.8950
[11/13 20:37:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.32	
[11/13 20:37:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/13 20:43:48 visual_prompt]: Epoch 13 / 100: avg data time: 1.05e+01, avg batch time: 11.0438, average train loss: 0.7412
[11/13 20:44:31 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1788, average loss: 0.7153
[11/13 20:44:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 54.43	
[11/13 20:44:31 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/13 20:50:58 visual_prompt]: Epoch 14 / 100: avg data time: 1.05e+01, avg batch time: 11.0490, average train loss: 0.7038
[11/13 20:51:42 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1835, average loss: 0.6802
[11/13 20:51:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.29	
[11/13 20:51:42 visual_prompt]: Best epoch 14: best metric: -0.680
[11/13 20:51:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/13 20:58:10 visual_prompt]: Epoch 15 / 100: avg data time: 1.05e+01, avg batch time: 11.0708, average train loss: 0.7309
[11/13 20:58:53 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1751, average loss: 0.6958
[11/13 20:58:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.02	
[11/13 20:58:53 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/13 21:05:20 visual_prompt]: Epoch 16 / 100: avg data time: 1.05e+01, avg batch time: 11.0341, average train loss: 0.7284
[11/13 21:06:03 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1744, average loss: 0.7002
[11/13 21:06:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.10	
[11/13 21:06:03 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/13 21:12:30 visual_prompt]: Epoch 17 / 100: avg data time: 1.05e+01, avg batch time: 11.0447, average train loss: 0.7242
[11/13 21:13:14 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1717, average loss: 0.9191
[11/13 21:13:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.68	
[11/13 21:13:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/13 21:19:40 visual_prompt]: Epoch 18 / 100: avg data time: 1.05e+01, avg batch time: 11.0423, average train loss: 0.7764
[11/13 21:20:24 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1774, average loss: 0.6813
[11/13 21:20:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.21	
[11/13 21:20:24 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/13 21:26:52 visual_prompt]: Epoch 19 / 100: avg data time: 1.05e+01, avg batch time: 11.0694, average train loss: 0.6973
[11/13 21:27:35 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1717, average loss: 0.7781
[11/13 21:27:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.34	
[11/13 21:27:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/13 21:34:02 visual_prompt]: Epoch 20 / 100: avg data time: 1.05e+01, avg batch time: 11.0507, average train loss: 0.7387
[11/13 21:34:46 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1797, average loss: 0.7487
[11/13 21:34:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 57.24	
[11/13 21:34:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/13 21:41:13 visual_prompt]: Epoch 21 / 100: avg data time: 1.05e+01, avg batch time: 11.0539, average train loss: 0.6933
[11/13 21:41:57 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1765, average loss: 0.6942
[11/13 21:41:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.48	
[11/13 21:41:57 visual_prompt]: Stopping early.
[11/13 21:41:57 visual_prompt]: Rank of current process: 0. World size: 1
[11/13 21:41:57 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/13 21:41:57 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/13 21:41:57 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/13 21:41:57 visual_prompt]: Training with config:
[11/13 21:41:57 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.005_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/13 21:41:57 visual_prompt]: Loading training data...
[11/13 21:41:57 visual_prompt]: Constructing mammo-cbis dataset train...
[11/13 21:41:57 visual_prompt]: Loading validation data...
[11/13 21:41:57 visual_prompt]: Constructing mammo-cbis dataset val...
[11/13 21:41:57 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/13 21:42:03 visual_prompt]: Enable all parameters update during training
[11/13 21:42:03 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/13 21:42:03 visual_prompt]: tuned percent:100.000
[11/13 21:42:03 visual_prompt]: Device used for model: 0
[11/13 21:42:03 visual_prompt]: Setting up Evaluator...
[11/13 21:42:03 visual_prompt]: Setting up Trainer...
[11/13 21:42:03 visual_prompt]: 	Setting up the optimizer...
[11/13 21:42:03 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/13 21:48:25 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9165, average train loss: 6.9791
[11/13 21:49:08 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1763, average loss: 6.3857
[11/13 21:49:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/13 21:49:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/13 21:55:29 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.8899, average train loss: 7.1642
[11/13 21:56:13 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1735, average loss: 1.1776
[11/13 21:56:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 55.47	
[11/13 21:56:13 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/13 22:02:35 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9191, average train loss: 3.3523
[11/13 22:03:19 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1728, average loss: 1.1512
[11/13 22:03:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.33	
[11/13 22:03:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/13 22:09:40 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.9004, average train loss: 1.1312
[11/13 22:10:24 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1734, average loss: 1.3375
[11/13 22:10:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.19	
[11/13 22:10:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/13 22:16:46 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.9174, average train loss: 2.5304
[11/13 22:17:29 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1688, average loss: 0.7134
[11/13 22:17:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 54.01	
[11/13 22:17:29 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/13 22:23:52 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9288, average train loss: 2.5519
[11/13 22:24:36 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1778, average loss: 3.9798
[11/13 22:24:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.87	
[11/13 22:24:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/13 22:30:58 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9249, average train loss: 1.8713
[11/13 22:31:42 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1701, average loss: 1.1052
[11/13 22:31:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.46	
[11/13 22:31:42 visual_prompt]: Best epoch 7: best metric: -1.105
[11/13 22:31:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/13 22:38:04 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9255, average train loss: 1.1114
[11/13 22:38:48 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1783, average loss: 0.7703
[11/13 22:38:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 54.86	
[11/13 22:38:48 visual_prompt]: Best epoch 8: best metric: -0.770
[11/13 22:38:48 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/13 22:45:10 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.9298, average train loss: 0.8901
[11/13 22:45:54 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1764, average loss: 2.3425
[11/13 22:45:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.45	
[11/13 22:45:54 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/13 22:52:17 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.9290, average train loss: 1.5516
[11/13 22:53:00 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1756, average loss: 0.9545
[11/13 22:53:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/13 22:53:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/13 22:59:23 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.9279, average train loss: 3.1054
[11/13 23:00:06 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1712, average loss: 1.3142
[11/13 23:00:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.80	
[11/13 23:00:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/13 23:06:29 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.9270, average train loss: 1.0938
[11/13 23:07:12 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1781, average loss: 1.0759
[11/13 23:07:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.48	
[11/13 23:07:12 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/13 23:13:35 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.9277, average train loss: 0.8543
[11/13 23:14:19 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1732, average loss: 0.9467
[11/13 23:14:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[11/13 23:14:19 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/13 23:20:41 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.9248, average train loss: 0.9961
[11/13 23:21:25 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1766, average loss: 0.9333
[11/13 23:21:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.33	
[11/13 23:21:25 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/13 23:27:48 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9542, average train loss: 0.9825
[11/13 23:28:32 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1760, average loss: 0.8608
[11/13 23:28:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.63	
[11/13 23:28:32 visual_prompt]: Stopping early.
[11/13 23:28:32 visual_prompt]: Rank of current process: 0. World size: 1
[11/13 23:28:32 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/13 23:28:32 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/13 23:28:32 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/13 23:28:32 visual_prompt]: Training with config:
[11/13 23:28:32 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/13 23:28:32 visual_prompt]: Loading training data...
[11/13 23:28:32 visual_prompt]: Constructing mammo-cbis dataset train...
[11/13 23:28:32 visual_prompt]: Loading validation data...
[11/13 23:28:32 visual_prompt]: Constructing mammo-cbis dataset val...
[11/13 23:28:32 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/13 23:28:33 visual_prompt]: Enable all parameters update during training
[11/13 23:28:33 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/13 23:28:33 visual_prompt]: tuned percent:100.000
[11/13 23:28:33 visual_prompt]: Device used for model: 0
[11/13 23:28:33 visual_prompt]: Setting up Evaluator...
[11/13 23:28:33 visual_prompt]: Setting up Trainer...
[11/13 23:28:33 visual_prompt]: 	Setting up the optimizer...
[11/13 23:28:33 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/13 23:34:55 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9004, average train loss: 6.9791
[11/13 23:35:38 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1757, average loss: 6.3857
[11/13 23:35:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/13 23:35:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/13 23:42:00 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.9016, average train loss: 2.9795
[11/13 23:42:44 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1783, average loss: 1.0191
[11/13 23:42:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.43	
[11/13 23:42:44 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/13 23:49:05 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.8997, average train loss: 0.9363
[11/13 23:49:49 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1755, average loss: 0.7546
[11/13 23:49:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.68	
[11/13 23:49:49 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/13 23:56:11 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.8975, average train loss: 0.8156
[11/13 23:56:54 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1716, average loss: 0.6800
[11/13 23:56:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.91	
[11/13 23:56:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/14 00:03:16 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.8938, average train loss: 0.7900
[11/14 00:03:59 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1777, average loss: 0.6689
[11/14 00:03:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.97	
[11/14 00:03:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/14 00:10:21 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9008, average train loss: 0.8251
[11/14 00:11:04 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1754, average loss: 0.7127
[11/14 00:11:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.79	
[11/14 00:11:04 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/14 00:17:26 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9088, average train loss: 0.7236
[11/14 00:18:10 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1741, average loss: 0.6896
[11/14 00:18:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.59	
[11/14 00:18:10 visual_prompt]: Best epoch 7: best metric: -0.690
[11/14 00:18:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/14 00:24:32 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9070, average train loss: 0.7060
[11/14 00:25:15 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1692, average loss: 0.6624
[11/14 00:25:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.49	
[11/14 00:25:15 visual_prompt]: Best epoch 8: best metric: -0.662
[11/14 00:25:15 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/14 00:31:36 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.8840, average train loss: 0.7656
[11/14 00:32:20 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1771, average loss: 0.6902
[11/14 00:32:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 64.84	
[11/14 00:32:20 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/14 00:38:41 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.8883, average train loss: 0.6893
[11/14 00:39:25 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1780, average loss: 0.7023
[11/14 00:39:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 67.18	
[11/14 00:39:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/14 00:45:46 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.8908, average train loss: 0.6646
[11/14 00:46:30 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1778, average loss: 0.6843
[11/14 00:46:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 67.84	
[11/14 00:46:30 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/14 00:52:51 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.8959, average train loss: 0.7185
[11/14 00:53:35 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1750, average loss: 0.7410
[11/14 00:53:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.83	
[11/14 00:53:35 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/14 00:59:56 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.8929, average train loss: 0.6984
[11/14 01:00:40 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1703, average loss: 0.6548
[11/14 01:00:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.62	
[11/14 01:00:40 visual_prompt]: Best epoch 13: best metric: -0.655
[11/14 01:00:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/14 01:07:01 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.8917, average train loss: 0.6217
[11/14 01:07:45 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1749, average loss: 0.6592
[11/14 01:07:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.11	
[11/14 01:07:45 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/14 01:14:07 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9126, average train loss: 0.6511
[11/14 01:14:50 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1714, average loss: 0.7042
[11/14 01:14:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 65.20	
[11/14 01:14:50 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/14 01:21:12 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.8906, average train loss: 0.5915
[11/14 01:21:55 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1751, average loss: 0.6971
[11/14 01:21:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 67.61	
[11/14 01:21:55 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/14 01:28:17 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.8949, average train loss: 0.6150
[11/14 01:29:00 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1768, average loss: 0.7181
[11/14 01:29:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 65.96	
[11/14 01:29:00 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/14 01:35:22 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.8920, average train loss: 0.5716
[11/14 01:36:05 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1763, average loss: 0.6619
[11/14 01:36:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.74	
[11/14 01:36:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/14 01:42:27 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9070, average train loss: 0.5241
[11/14 01:43:11 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1760, average loss: 0.6877
[11/14 01:43:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.08	
[11/14 01:43:11 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/14 01:49:32 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.9025, average train loss: 0.5085
[11/14 01:50:16 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1766, average loss: 0.7454
[11/14 01:50:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.68	
[11/14 01:50:16 visual_prompt]: Stopping early.
[11/14 01:50:16 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 01:50:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 01:50:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 01:50:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 01:50:16 visual_prompt]: Training with config:
[11/14 01:50:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 01:50:16 visual_prompt]: Loading training data...
[11/14 01:50:16 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 01:50:16 visual_prompt]: Loading validation data...
[11/14 01:50:16 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 01:50:16 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 01:50:18 visual_prompt]: Enable all parameters update during training
[11/14 01:50:18 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 01:50:18 visual_prompt]: tuned percent:100.000
[11/14 01:50:18 visual_prompt]: Device used for model: 0
[11/14 01:50:18 visual_prompt]: Setting up Evaluator...
[11/14 01:50:18 visual_prompt]: Setting up Trainer...
[11/14 01:50:18 visual_prompt]: 	Setting up the optimizer...
[11/14 01:50:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 01:56:40 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9200, average train loss: 6.9791
[11/14 01:57:24 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1805, average loss: 6.3857
[11/14 01:57:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 01:57:24 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/14 02:03:45 visual_prompt]: Epoch 2 / 100: avg data time: 1.03e+01, avg batch time: 10.8915, average train loss: 2.9795
[11/14 02:04:29 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1784, average loss: 1.0191
[11/14 02:04:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.43	
[11/14 02:04:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/14 02:10:51 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9030, average train loss: 0.9363
[11/14 02:11:34 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1728, average loss: 0.7546
[11/14 02:11:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.68	
[11/14 02:11:34 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/14 02:17:56 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.8964, average train loss: 0.8156
[11/14 02:18:39 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1789, average loss: 0.6800
[11/14 02:18:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.91	
[11/14 02:18:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/14 02:25:00 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.8856, average train loss: 0.7900
[11/14 02:25:44 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1782, average loss: 0.6689
[11/14 02:25:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.97	
[11/14 02:25:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/14 02:32:06 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.8989, average train loss: 0.8251
[11/14 02:32:49 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1733, average loss: 0.7127
[11/14 02:32:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.79	
[11/14 02:32:49 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/14 02:39:11 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9015, average train loss: 0.7236
[11/14 02:39:54 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1779, average loss: 0.6896
[11/14 02:39:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.59	
[11/14 02:39:54 visual_prompt]: Best epoch 7: best metric: -0.690
[11/14 02:39:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/14 02:46:16 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9045, average train loss: 0.7060
[11/14 02:47:00 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1705, average loss: 0.6624
[11/14 02:47:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.49	
[11/14 02:47:00 visual_prompt]: Best epoch 8: best metric: -0.662
[11/14 02:47:00 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/14 02:53:20 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.8476, average train loss: 0.7656
[11/14 02:54:02 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1804, average loss: 0.6902
[11/14 02:54:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 64.84	
[11/14 02:54:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/14 03:00:22 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.8414, average train loss: 0.6893
[11/14 03:01:06 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1739, average loss: 0.7023
[11/14 03:01:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 67.18	
[11/14 03:01:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/14 03:07:27 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.8887, average train loss: 0.6646
[11/14 03:08:10 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1780, average loss: 0.6843
[11/14 03:08:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 67.84	
[11/14 03:08:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/14 03:14:32 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.8893, average train loss: 0.7185
[11/14 03:15:15 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1714, average loss: 0.7410
[11/14 03:15:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.83	
[11/14 03:15:15 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/14 03:21:36 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.8886, average train loss: 0.6984
[11/14 03:22:20 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1801, average loss: 0.6548
[11/14 03:22:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.62	
[11/14 03:22:20 visual_prompt]: Best epoch 13: best metric: -0.655
[11/14 03:22:20 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/14 03:28:41 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.8854, average train loss: 0.6217
[11/14 03:29:25 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1746, average loss: 0.6592
[11/14 03:29:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.11	
[11/14 03:29:25 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/14 03:35:47 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9058, average train loss: 0.6511
[11/14 03:36:30 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1715, average loss: 0.7042
[11/14 03:36:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 65.20	
[11/14 03:36:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/14 03:42:52 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.8844, average train loss: 0.5915
[11/14 03:43:35 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1755, average loss: 0.6971
[11/14 03:43:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 67.61	
[11/14 03:43:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/14 03:49:57 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.8968, average train loss: 0.6150
[11/14 03:50:40 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1785, average loss: 0.7181
[11/14 03:50:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 65.96	
[11/14 03:50:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/14 03:57:02 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.9075, average train loss: 0.5716
[11/14 03:57:46 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1758, average loss: 0.6619
[11/14 03:57:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.74	
[11/14 03:57:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/14 04:04:08 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9150, average train loss: 0.5241
[11/14 04:04:52 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1792, average loss: 0.6877
[11/14 04:04:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.08	
[11/14 04:04:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/14 04:11:10 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e+01, avg batch time: 10.7873, average train loss: 0.5085
[11/14 04:11:53 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1768, average loss: 0.7454
[11/14 04:11:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.68	
[11/14 04:11:53 visual_prompt]: Stopping early.
[11/14 04:11:53 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 04:11:53 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 04:11:53 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 04:11:53 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 04:11:53 visual_prompt]: Training with config:
[11/14 04:11:53 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 04:11:53 visual_prompt]: Loading training data...
[11/14 04:11:53 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 04:11:53 visual_prompt]: Loading validation data...
[11/14 04:11:53 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 04:11:53 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 04:11:55 visual_prompt]: Enable all parameters update during training
[11/14 04:11:55 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 04:11:55 visual_prompt]: tuned percent:100.000
[11/14 04:11:55 visual_prompt]: Device used for model: 0
[11/14 04:11:55 visual_prompt]: Setting up Evaluator...
[11/14 04:11:55 visual_prompt]: Setting up Trainer...
[11/14 04:11:55 visual_prompt]: 	Setting up the optimizer...
[11/14 04:11:55 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 04:18:17 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9157, average train loss: 6.9791
[11/14 04:19:01 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1774, average loss: 6.3857
[11/14 04:19:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 04:19:01 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/14 04:25:22 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.9021, average train loss: 2.9795
[11/14 04:26:06 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1754, average loss: 1.0191
[11/14 04:26:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.43	
[11/14 04:26:06 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/14 04:32:28 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9029, average train loss: 0.9363
[11/14 04:33:11 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1780, average loss: 0.7546
[11/14 04:33:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.68	
[11/14 04:33:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/14 04:39:33 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.8874, average train loss: 0.8156
[11/14 04:40:16 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1751, average loss: 0.6800
[11/14 04:40:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.91	
[11/14 04:40:16 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/14 04:46:37 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.8885, average train loss: 0.7900
[11/14 04:47:21 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1779, average loss: 0.6689
[11/14 04:47:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.97	
[11/14 04:47:21 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/14 04:53:43 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9012, average train loss: 0.8251
[11/14 04:54:26 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1753, average loss: 0.7127
[11/14 04:54:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.79	
[11/14 04:54:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/14 05:00:48 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9040, average train loss: 0.7236
[11/14 05:01:32 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1780, average loss: 0.6896
[11/14 05:01:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.59	
[11/14 05:01:32 visual_prompt]: Best epoch 7: best metric: -0.690
[11/14 05:01:32 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/14 05:07:54 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9142, average train loss: 0.7060
[11/14 05:08:38 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1772, average loss: 0.6624
[11/14 05:08:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.49	
[11/14 05:08:38 visual_prompt]: Best epoch 8: best metric: -0.662
[11/14 05:08:38 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/14 05:14:59 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.8920, average train loss: 0.7656
[11/14 05:15:43 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1754, average loss: 0.6902
[11/14 05:15:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 64.84	
[11/14 05:15:43 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/14 05:22:04 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.8854, average train loss: 0.6893
[11/14 05:22:48 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1760, average loss: 0.7023
[11/14 05:22:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 67.18	
[11/14 05:22:48 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/14 05:29:09 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.8984, average train loss: 0.6646
[11/14 05:29:53 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1787, average loss: 0.6843
[11/14 05:29:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 67.84	
[11/14 05:29:53 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/14 05:36:14 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.8901, average train loss: 0.7185
[11/14 05:36:58 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1732, average loss: 0.7410
[11/14 05:36:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.83	
[11/14 05:36:58 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/14 05:43:19 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.8890, average train loss: 0.6984
[11/14 05:44:03 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1804, average loss: 0.6548
[11/14 05:44:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.62	
[11/14 05:44:03 visual_prompt]: Best epoch 13: best metric: -0.655
[11/14 05:44:03 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/14 05:50:25 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.8998, average train loss: 0.6217
[11/14 05:51:08 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1712, average loss: 0.6592
[11/14 05:51:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.11	
[11/14 05:51:08 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/14 05:57:31 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9143, average train loss: 0.6511
[11/14 05:58:14 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1693, average loss: 0.7042
[11/14 05:58:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 65.20	
[11/14 05:58:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/14 06:04:35 visual_prompt]: Epoch 16 / 100: avg data time: 1.03e+01, avg batch time: 10.8864, average train loss: 0.5915
[11/14 06:05:19 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1757, average loss: 0.6971
[11/14 06:05:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 67.61	
[11/14 06:05:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/14 06:11:40 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 10.8878, average train loss: 0.6150
[11/14 06:12:24 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1757, average loss: 0.7181
[11/14 06:12:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 65.96	
[11/14 06:12:24 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/14 06:18:45 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.8931, average train loss: 0.5716
[11/14 06:19:29 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1737, average loss: 0.6619
[11/14 06:19:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.74	
[11/14 06:19:29 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/14 06:25:51 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9174, average train loss: 0.5241
[11/14 06:26:35 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1764, average loss: 0.6877
[11/14 06:26:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.08	
[11/14 06:26:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/14 06:32:57 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.9052, average train loss: 0.5085
[11/14 06:33:41 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1743, average loss: 0.7454
[11/14 06:33:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.68	
[11/14 06:33:41 visual_prompt]: Stopping early.
[11/14 06:33:41 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 06:33:41 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 06:33:41 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 06:33:41 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 06:33:41 visual_prompt]: Training with config:
[11/14 06:33:41 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 06:33:41 visual_prompt]: Loading training data...
[11/14 06:33:41 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 06:33:41 visual_prompt]: Loading validation data...
[11/14 06:33:41 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 06:33:41 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 06:33:42 visual_prompt]: Enable all parameters update during training
[11/14 06:33:42 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 06:33:42 visual_prompt]: tuned percent:100.000
[11/14 06:33:42 visual_prompt]: Device used for model: 0
[11/14 06:33:42 visual_prompt]: Setting up Evaluator...
[11/14 06:33:42 visual_prompt]: Setting up Trainer...
[11/14 06:33:42 visual_prompt]: 	Setting up the optimizer...
[11/14 06:33:42 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 06:40:04 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9071, average train loss: 6.9791
[11/14 06:40:48 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1773, average loss: 6.3857
[11/14 06:40:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 06:40:48 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/14 06:47:09 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.8978, average train loss: 4.7319
[11/14 06:47:53 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1779, average loss: 1.0649
[11/14 06:47:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 51.03	
[11/14 06:47:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/14 06:54:16 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9227, average train loss: 0.9320
[11/14 06:54:59 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1740, average loss: 0.7129
[11/14 06:54:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.31	
[11/14 06:54:59 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/14 07:01:21 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.9044, average train loss: 0.7922
[11/14 07:02:05 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.1760, average loss: 0.7879
[11/14 07:02:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.62	
[11/14 07:02:05 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/14 07:08:28 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.9214, average train loss: 0.8139
[11/14 07:09:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1782, average loss: 0.6712
[11/14 07:09:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 59.10	
[11/14 07:09:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/14 07:15:33 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.8939, average train loss: 0.7711
[11/14 07:16:16 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1715, average loss: 0.7228
[11/14 07:16:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.26	
[11/14 07:16:16 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/14 07:22:38 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.8971, average train loss: 0.7319
[11/14 07:23:21 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1746, average loss: 0.8022
[11/14 07:23:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.77	
[11/14 07:23:21 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/14 07:29:43 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.8961, average train loss: 0.8037
[11/14 07:30:27 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1798, average loss: 0.7268
[11/14 07:30:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 60.55	
[11/14 07:30:27 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/14 07:36:48 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.8926, average train loss: 0.7855
[11/14 07:37:32 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1712, average loss: 0.7112
[11/14 07:37:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 61.17	
[11/14 07:37:32 visual_prompt]: Best epoch 9: best metric: -0.711
[11/14 07:37:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/14 07:43:53 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.8933, average train loss: 0.7169
[11/14 07:44:37 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1777, average loss: 0.7654
[11/14 07:44:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 60.58	
[11/14 07:44:37 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/14 07:50:58 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.8880, average train loss: 0.6711
[11/14 07:51:42 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1768, average loss: 0.7312
[11/14 07:51:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.53	
[11/14 07:51:42 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/14 07:58:02 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 10.8626, average train loss: 0.6936
[11/14 07:58:45 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1706, average loss: 0.7229
[11/14 07:58:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.16	
[11/14 07:58:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/14 08:05:04 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.8329, average train loss: 0.7212
[11/14 08:05:48 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1787, average loss: 0.7368
[11/14 08:05:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.90	
[11/14 08:05:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/14 08:12:09 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.8851, average train loss: 0.6471
[11/14 08:12:53 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1796, average loss: 0.6434
[11/14 08:12:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 62.60	
[11/14 08:12:53 visual_prompt]: Best epoch 14: best metric: -0.643
[11/14 08:12:53 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/14 08:19:15 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9068, average train loss: 0.7018
[11/14 08:19:58 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1780, average loss: 0.6955
[11/14 08:19:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.30	
[11/14 08:19:58 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/14 08:26:20 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.8931, average train loss: 0.6473
[11/14 08:27:03 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1690, average loss: 0.9287
[11/14 08:27:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 62.04	
[11/14 08:27:03 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/14 08:33:25 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.8900, average train loss: 0.6998
[11/14 08:34:08 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1745, average loss: 0.7083
[11/14 08:34:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 62.37	
[11/14 08:34:08 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/14 08:40:29 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.8918, average train loss: 0.6598
[11/14 08:41:13 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1760, average loss: 0.6687
[11/14 08:41:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 63.06	
[11/14 08:41:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/14 08:47:35 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9137, average train loss: 0.6186
[11/14 08:48:19 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1716, average loss: 0.7331
[11/14 08:48:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 62.74	
[11/14 08:48:19 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/14 08:54:40 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.8928, average train loss: 0.6756
[11/14 08:55:24 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1748, average loss: 0.7132
[11/14 08:55:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.61	
[11/14 08:55:24 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/14 09:01:46 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.9028, average train loss: 0.6154
[11/14 09:02:29 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1763, average loss: 0.7064
[11/14 09:02:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.66	
[11/14 09:02:29 visual_prompt]: Stopping early.
[11/14 09:02:29 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 09:02:29 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 09:02:29 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 09:02:29 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 09:02:29 visual_prompt]: Training with config:
[11/14 09:02:29 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 09:02:29 visual_prompt]: Loading training data...
[11/14 09:02:29 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 09:02:29 visual_prompt]: Loading validation data...
[11/14 09:02:29 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 09:02:29 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 09:02:31 visual_prompt]: Enable all parameters update during training
[11/14 09:02:31 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 09:02:31 visual_prompt]: tuned percent:100.000
[11/14 09:02:31 visual_prompt]: Device used for model: 0
[11/14 09:02:31 visual_prompt]: Setting up Evaluator...
[11/14 09:02:31 visual_prompt]: Setting up Trainer...
[11/14 09:02:31 visual_prompt]: 	Setting up the optimizer...
[11/14 09:02:31 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 09:08:54 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9354, average train loss: 6.9791
[11/14 09:09:38 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1768, average loss: 6.3857
[11/14 09:09:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 09:09:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/14 09:15:55 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.7871, average train loss: 3.1853
[11/14 09:16:38 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1763, average loss: 0.8753
[11/14 09:16:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.69	
[11/14 09:16:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/14 09:23:00 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9127, average train loss: 0.9599
[11/14 09:23:44 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1778, average loss: 0.7334
[11/14 09:23:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.81	
[11/14 09:23:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/14 09:30:06 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.8983, average train loss: 0.7770
[11/14 09:30:49 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1777, average loss: 0.6492
[11/14 09:30:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.12	
[11/14 09:30:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/14 09:37:11 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.8936, average train loss: 0.7371
[11/14 09:37:55 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1774, average loss: 0.7053
[11/14 09:37:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 69.47	
[11/14 09:37:55 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/14 09:44:17 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9068, average train loss: 0.7302
[11/14 09:45:00 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1780, average loss: 0.6638
[11/14 09:45:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.38	
[11/14 09:45:00 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/14 09:51:22 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9110, average train loss: 0.6865
[11/14 09:52:06 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1778, average loss: 0.6231
[11/14 09:52:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.25	
[11/14 09:52:06 visual_prompt]: Best epoch 7: best metric: -0.623
[11/14 09:52:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/14 09:58:28 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9168, average train loss: 0.6633
[11/14 09:59:12 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1764, average loss: 0.6259
[11/14 09:59:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.63	
[11/14 09:59:12 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/14 10:05:34 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.8998, average train loss: 0.6471
[11/14 10:06:17 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1773, average loss: 0.8654
[11/14 10:06:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 71.31	
[11/14 10:06:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/14 10:12:39 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.8930, average train loss: 0.6220
[11/14 10:13:23 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1744, average loss: 0.8537
[11/14 10:13:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 69.46	
[11/14 10:13:23 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/14 10:19:44 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.8952, average train loss: 0.5798
[11/14 10:20:28 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1759, average loss: 0.9848
[11/14 10:20:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 70.86	
[11/14 10:20:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/14 10:26:49 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.8983, average train loss: 0.5611
[11/14 10:27:33 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1743, average loss: 0.8405
[11/14 10:27:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 73.09	
[11/14 10:27:33 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/14 10:33:55 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.8972, average train loss: 0.5306
[11/14 10:34:38 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1797, average loss: 0.8182
[11/14 10:34:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.86	
[11/14 10:34:38 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/14 10:41:00 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.8926, average train loss: 0.5167
[11/14 10:41:43 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1786, average loss: 0.7376
[11/14 10:41:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.13	
[11/14 10:41:43 visual_prompt]: Stopping early.
[11/14 10:41:43 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 10:41:43 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 10:41:43 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 10:41:43 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 10:41:43 visual_prompt]: Training with config:
[11/14 10:41:43 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 10:41:43 visual_prompt]: Loading training data...
[11/14 10:41:43 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 10:41:44 visual_prompt]: Loading validation data...
[11/14 10:41:44 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 10:41:44 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 10:41:45 visual_prompt]: Enable all parameters update during training
[11/14 10:41:45 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 10:41:45 visual_prompt]: tuned percent:100.000
[11/14 10:41:45 visual_prompt]: Device used for model: 0
[11/14 10:41:45 visual_prompt]: Setting up Evaluator...
[11/14 10:41:45 visual_prompt]: Setting up Trainer...
[11/14 10:41:45 visual_prompt]: 	Setting up the optimizer...
[11/14 10:41:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 10:48:07 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9052, average train loss: 6.9791
[11/14 10:48:51 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1716, average loss: 6.3857
[11/14 10:48:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 10:48:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/14 10:55:12 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.8985, average train loss: 3.1853
[11/14 10:55:56 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1787, average loss: 0.8753
[11/14 10:55:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.69	
[11/14 10:55:56 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/14 11:02:18 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9062, average train loss: 0.9599
[11/14 11:03:02 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1783, average loss: 0.7334
[11/14 11:03:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.81	
[11/14 11:03:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/14 11:10:15 visual_prompt]: Epoch 4 / 100: avg data time: 1.18e+01, avg batch time: 12.3706, average train loss: 0.7770
[11/14 11:11:00 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1691, average loss: 0.6492
[11/14 11:11:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.12	
[11/14 11:11:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/14 11:17:26 visual_prompt]: Epoch 5 / 100: avg data time: 1.05e+01, avg batch time: 11.0044, average train loss: 0.7371
[11/14 11:18:10 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1795, average loss: 0.7053
[11/14 11:18:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 69.47	
[11/14 11:18:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/14 11:24:34 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9711, average train loss: 0.7302
[11/14 11:25:17 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1770, average loss: 0.6638
[11/14 11:25:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.38	
[11/14 11:25:17 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/14 11:31:41 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9530, average train loss: 0.6865
[11/14 11:32:25 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1739, average loss: 0.6231
[11/14 11:32:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.25	
[11/14 11:32:25 visual_prompt]: Best epoch 7: best metric: -0.623
[11/14 11:32:25 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/14 11:38:47 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9152, average train loss: 0.6633
[11/14 11:39:30 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1731, average loss: 0.6259
[11/14 11:39:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.63	
[11/14 11:39:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/14 11:45:53 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.9096, average train loss: 0.6471
[11/14 11:46:36 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1786, average loss: 0.8654
[11/14 11:46:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 71.31	
[11/14 11:46:36 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/14 11:52:58 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.8971, average train loss: 0.6220
[11/14 11:53:41 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 0.8537
[11/14 11:53:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 69.46	
[11/14 11:53:41 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/14 12:00:03 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.8998, average train loss: 0.5798
[11/14 12:00:47 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1735, average loss: 0.9848
[11/14 12:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 70.86	
[11/14 12:00:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/14 12:07:09 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.9044, average train loss: 0.5611
[11/14 12:07:53 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1779, average loss: 0.8405
[11/14 12:07:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 73.09	
[11/14 12:07:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/14 12:14:14 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.9004, average train loss: 0.5306
[11/14 12:14:58 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1742, average loss: 0.8182
[11/14 12:14:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.86	
[11/14 12:14:58 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/14 12:21:20 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.8985, average train loss: 0.5167
[11/14 12:22:03 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1770, average loss: 0.7376
[11/14 12:22:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.13	
[11/14 12:22:03 visual_prompt]: Stopping early.
[11/14 12:22:04 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 12:22:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 12:22:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 12:22:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 12:22:04 visual_prompt]: Training with config:
[11/14 12:22:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 12:22:04 visual_prompt]: Loading training data...
[11/14 12:22:04 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 12:22:04 visual_prompt]: Loading validation data...
[11/14 12:22:04 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 12:22:04 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 12:22:09 visual_prompt]: Enable all parameters update during training
[11/14 12:22:09 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 12:22:09 visual_prompt]: tuned percent:100.000
[11/14 12:22:09 visual_prompt]: Device used for model: 0
[11/14 12:22:09 visual_prompt]: Setting up Evaluator...
[11/14 12:22:09 visual_prompt]: Setting up Trainer...
[11/14 12:22:09 visual_prompt]: 	Setting up the optimizer...
[11/14 12:22:09 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 12:28:31 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9064, average train loss: 6.9791
[11/14 12:29:14 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1718, average loss: 6.3857
[11/14 12:29:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 12:29:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/14 12:35:36 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.8967, average train loss: 3.1853
[11/14 12:36:19 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1691, average loss: 0.8753
[11/14 12:36:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.69	
[11/14 12:36:19 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/14 12:42:41 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9060, average train loss: 0.9599
[11/14 12:43:25 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1719, average loss: 0.7334
[11/14 12:43:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.81	
[11/14 12:43:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/14 12:49:47 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.8949, average train loss: 0.7770
[11/14 12:50:30 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1737, average loss: 0.6492
[11/14 12:50:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.12	
[11/14 12:50:30 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/14 12:56:52 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.9094, average train loss: 0.7371
[11/14 12:57:36 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1717, average loss: 0.7053
[11/14 12:57:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 69.47	
[11/14 12:57:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/14 13:03:57 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.8844, average train loss: 0.7302
[11/14 13:04:40 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1783, average loss: 0.6638
[11/14 13:04:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.38	
[11/14 13:04:40 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/14 13:11:00 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.8630, average train loss: 0.6865
[11/14 13:11:44 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1795, average loss: 0.6231
[11/14 13:11:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.25	
[11/14 13:11:44 visual_prompt]: Best epoch 7: best metric: -0.623
[11/14 13:11:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/14 13:18:06 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9122, average train loss: 0.6633
[11/14 13:18:50 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1782, average loss: 0.6259
[11/14 13:18:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.63	
[11/14 13:18:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/14 13:25:11 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.8978, average train loss: 0.6471
[11/14 13:25:55 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1725, average loss: 0.8654
[11/14 13:25:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 71.31	
[11/14 13:25:55 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/14 13:32:16 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.8974, average train loss: 0.6220
[11/14 13:33:00 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1787, average loss: 0.8537
[11/14 13:33:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 69.46	
[11/14 13:33:00 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/14 13:39:22 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.9005, average train loss: 0.5798
[11/14 13:40:05 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1710, average loss: 0.9848
[11/14 13:40:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 70.86	
[11/14 13:40:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/14 13:46:31 visual_prompt]: Epoch 12 / 100: avg data time: 1.05e+01, avg batch time: 10.9978, average train loss: 0.5611
[11/14 13:47:14 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1822, average loss: 0.8405
[11/14 13:47:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 73.09	
[11/14 13:47:14 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/14 13:53:37 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.9254, average train loss: 0.5306
[11/14 13:54:20 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1739, average loss: 0.8182
[11/14 13:54:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.86	
[11/14 13:54:20 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/14 14:00:43 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.9099, average train loss: 0.5167
[11/14 14:01:26 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1707, average loss: 0.7376
[11/14 14:01:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.13	
[11/14 14:01:26 visual_prompt]: Stopping early.
[11/14 14:01:26 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 14:01:26 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 14:01:26 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 14:01:26 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 14:01:26 visual_prompt]: Training with config:
[11/14 14:01:26 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 14:01:26 visual_prompt]: Loading training data...
[11/14 14:01:26 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 14:01:26 visual_prompt]: Loading validation data...
[11/14 14:01:26 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 14:01:26 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 14:01:28 visual_prompt]: Enable all parameters update during training
[11/14 14:01:28 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 14:01:28 visual_prompt]: tuned percent:100.000
[11/14 14:01:28 visual_prompt]: Device used for model: 0
[11/14 14:01:28 visual_prompt]: Setting up Evaluator...
[11/14 14:01:28 visual_prompt]: Setting up Trainer...
[11/14 14:01:28 visual_prompt]: 	Setting up the optimizer...
[11/14 14:01:28 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 14:07:50 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9037, average train loss: 6.9791
[11/14 14:08:33 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1775, average loss: 6.3857
[11/14 14:08:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 14:08:33 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/14 14:14:55 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.8972, average train loss: 3.7696
[11/14 14:15:39 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1744, average loss: 0.8737
[11/14 14:15:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 52.91	
[11/14 14:15:39 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/14 14:22:01 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9097, average train loss: 0.9347
[11/14 14:22:44 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1775, average loss: 0.7837
[11/14 14:22:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 53.09	
[11/14 14:22:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/14 14:29:06 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.9155, average train loss: 0.8079
[11/14 14:29:54 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1826, average loss: 0.7091
[11/14 14:29:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 53.89	
[11/14 14:29:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/14 14:37:12 visual_prompt]: Epoch 5 / 100: avg data time: 1.20e+01, avg batch time: 12.5125, average train loss: 0.7853
[11/14 14:37:57 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1816, average loss: 0.7754
[11/14 14:37:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 56.30	
[11/14 14:37:57 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/14 14:44:31 visual_prompt]: Epoch 6 / 100: avg data time: 1.07e+01, avg batch time: 11.2409, average train loss: 0.7465
[11/14 14:45:16 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1797, average loss: 0.6865
[11/14 14:45:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 58.04	
[11/14 14:45:16 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/14 14:51:49 visual_prompt]: Epoch 7 / 100: avg data time: 1.07e+01, avg batch time: 11.2325, average train loss: 0.7096
[11/14 14:52:34 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1796, average loss: 0.6843
[11/14 14:52:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 59.42	
[11/14 14:52:34 visual_prompt]: Best epoch 7: best metric: -0.684
[11/14 14:52:34 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/14 14:59:08 visual_prompt]: Epoch 8 / 100: avg data time: 1.07e+01, avg batch time: 11.2371, average train loss: 0.7223
[11/14 14:59:52 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1723, average loss: 0.6571
[11/14 14:59:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 59.36	
[11/14 14:59:52 visual_prompt]: Best epoch 8: best metric: -0.657
[11/14 14:59:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/14 15:06:17 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.9943, average train loss: 0.6881
[11/14 15:07:00 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1743, average loss: 0.7079
[11/14 15:07:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.67	
[11/14 15:07:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/14 15:13:18 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.8050, average train loss: 0.7266
[11/14 15:14:02 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1818, average loss: 0.8433
[11/14 15:14:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 59.97	
[11/14 15:14:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/14 15:20:19 visual_prompt]: Epoch 11 / 100: avg data time: 1.02e+01, avg batch time: 10.7889, average train loss: 0.6806
[11/14 15:21:03 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1791, average loss: 0.6778
[11/14 15:21:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.03	
[11/14 15:21:03 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/14 15:27:18 visual_prompt]: Epoch 12 / 100: avg data time: 1.02e+01, avg batch time: 10.7086, average train loss: 0.6837
[11/14 15:28:00 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1811, average loss: 0.9075
[11/14 15:28:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.03	
[11/14 15:28:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/14 15:34:16 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.7313, average train loss: 0.6630
[11/14 15:34:59 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1810, average loss: 0.6550
[11/14 15:34:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 61.25	
[11/14 15:34:59 visual_prompt]: Best epoch 13: best metric: -0.655
[11/14 15:34:59 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/14 15:41:17 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.7907, average train loss: 0.6617
[11/14 15:42:01 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1805, average loss: 0.6443
[11/14 15:42:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 61.14	
[11/14 15:42:01 visual_prompt]: Best epoch 14: best metric: -0.644
[11/14 15:42:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/14 15:48:20 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.8269, average train loss: 0.6704
[11/14 15:49:03 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1764, average loss: 0.7420
[11/14 15:49:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.24	
[11/14 15:49:03 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/14 15:55:20 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.7789, average train loss: 0.6734
[11/14 15:56:03 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1770, average loss: 0.6462
[11/14 15:56:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 62.21	
[11/14 15:56:03 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/14 16:02:21 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.7705, average train loss: 0.6687
[11/14 16:03:04 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1809, average loss: 0.7409
[11/14 16:03:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.45	
[11/14 16:03:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/14 16:09:21 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.7757, average train loss: 0.6681
[11/14 16:10:04 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1773, average loss: 0.7146
[11/14 16:10:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 62.45	
[11/14 16:10:04 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/14 16:16:21 visual_prompt]: Epoch 19 / 100: avg data time: 1.02e+01, avg batch time: 10.7743, average train loss: 0.6348
[11/14 16:17:05 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1818, average loss: 0.6495
[11/14 16:17:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 62.18	
[11/14 16:17:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/14 16:23:22 visual_prompt]: Epoch 20 / 100: avg data time: 1.02e+01, avg batch time: 10.7847, average train loss: 0.6323
[11/14 16:24:05 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1746, average loss: 0.6532
[11/14 16:24:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 61.87	
[11/14 16:24:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/14 16:30:23 visual_prompt]: Epoch 21 / 100: avg data time: 1.02e+01, avg batch time: 10.7712, average train loss: 0.6121
[11/14 16:31:06 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.1807, average loss: 0.6540
[11/14 16:31:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 61.38	
[11/14 16:31:06 visual_prompt]: Stopping early.
[11/14 16:31:06 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 16:31:06 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 16:31:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 16:31:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 16:31:06 visual_prompt]: Training with config:
[11/14 16:31:06 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.01/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 16:31:06 visual_prompt]: Loading training data...
[11/14 16:31:06 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 16:31:06 visual_prompt]: Loading validation data...
[11/14 16:31:06 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 16:31:06 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 16:31:08 visual_prompt]: Enable all parameters update during training
[11/14 16:31:08 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 16:31:08 visual_prompt]: tuned percent:100.000
[11/14 16:31:08 visual_prompt]: Device used for model: 0
[11/14 16:31:08 visual_prompt]: Setting up Evaluator...
[11/14 16:31:08 visual_prompt]: Setting up Trainer...
[11/14 16:31:08 visual_prompt]: 	Setting up the optimizer...
[11/14 16:31:08 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 16:37:32 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9696, average train loss: 6.9791
[11/14 16:38:20 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1766, average loss: 6.3857
[11/14 16:38:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 16:38:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/14 16:44:44 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.9629, average train loss: 2.2494
[11/14 16:45:28 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1792, average loss: 0.8412
[11/14 16:45:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 51.72	
[11/14 16:45:28 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/14 16:51:51 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9241, average train loss: 0.8962
[11/14 16:52:34 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1776, average loss: 0.7044
[11/14 16:52:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.92	
[11/14 16:52:34 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/14 16:58:57 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.9203, average train loss: 0.7820
[11/14 16:59:40 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1781, average loss: 0.7135
[11/14 16:59:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.72	
[11/14 16:59:40 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/14 17:06:02 visual_prompt]: Epoch 5 / 100: avg data time: 1.03e+01, avg batch time: 10.8987, average train loss: 0.7455
[11/14 17:06:46 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1839, average loss: 0.7162
[11/14 17:06:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.65	
[11/14 17:06:46 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/14 17:13:08 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9191, average train loss: 0.6861
[11/14 17:13:52 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1754, average loss: 0.6859
[11/14 17:13:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.65	
[11/14 17:13:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/14 17:20:14 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9188, average train loss: 0.6416
[11/14 17:20:58 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1768, average loss: 0.6250
[11/14 17:20:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 68.25	
[11/14 17:20:58 visual_prompt]: Best epoch 7: best metric: -0.625
[11/14 17:20:58 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/14 17:27:20 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9230, average train loss: 0.6677
[11/14 17:28:04 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1757, average loss: 0.8172
[11/14 17:28:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.84	
[11/14 17:28:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/14 17:34:30 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 11.0065, average train loss: 0.5855
[11/14 17:35:14 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1841, average loss: 0.6318
[11/14 17:35:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 68.24	
[11/14 17:35:14 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/14 17:41:42 visual_prompt]: Epoch 10 / 100: avg data time: 1.05e+01, avg batch time: 11.0847, average train loss: 0.5394
[11/14 17:42:26 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1787, average loss: 0.6388
[11/14 17:42:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.99	
[11/14 17:42:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/14 17:48:54 visual_prompt]: Epoch 11 / 100: avg data time: 1.05e+01, avg batch time: 11.0686, average train loss: 0.4939
[11/14 17:49:38 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1819, average loss: 0.8244
[11/14 17:49:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.68	
[11/14 17:49:38 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/14 17:56:05 visual_prompt]: Epoch 12 / 100: avg data time: 1.05e+01, avg batch time: 11.0524, average train loss: 0.5270
[11/14 17:56:50 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1790, average loss: 0.8798
[11/14 17:56:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 69.06	
[11/14 17:56:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/14 18:03:17 visual_prompt]: Epoch 13 / 100: avg data time: 1.05e+01, avg batch time: 11.0605, average train loss: 0.4742
[11/14 18:04:01 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1816, average loss: 0.7313
[11/14 18:04:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.39	
[11/14 18:04:02 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/14 18:10:29 visual_prompt]: Epoch 14 / 100: avg data time: 1.05e+01, avg batch time: 11.0562, average train loss: 0.4177
[11/14 18:11:13 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1801, average loss: 0.6655
[11/14 18:11:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.63	
[11/14 18:11:13 visual_prompt]: Stopping early.
[11/14 18:11:13 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 18:11:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 18:11:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 18:11:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 18:11:13 visual_prompt]: Training with config:
[11/14 18:11:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 18:11:13 visual_prompt]: Loading training data...
[11/14 18:11:13 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 18:11:13 visual_prompt]: Loading validation data...
[11/14 18:11:13 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 18:11:13 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 18:11:15 visual_prompt]: Enable all parameters update during training
[11/14 18:11:15 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 18:11:15 visual_prompt]: tuned percent:100.000
[11/14 18:11:15 visual_prompt]: Device used for model: 0
[11/14 18:11:15 visual_prompt]: Setting up Evaluator...
[11/14 18:11:15 visual_prompt]: Setting up Trainer...
[11/14 18:11:15 visual_prompt]: 	Setting up the optimizer...
[11/14 18:11:15 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 18:17:42 visual_prompt]: Epoch 1 / 100: avg data time: 1.05e+01, avg batch time: 11.0570, average train loss: 6.9791
[11/14 18:18:27 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1814, average loss: 6.3857
[11/14 18:18:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 18:18:27 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/14 18:24:45 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.8010, average train loss: 2.2494
[11/14 18:25:30 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1810, average loss: 0.8412
[11/14 18:25:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 51.72	
[11/14 18:25:30 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/14 18:31:57 visual_prompt]: Epoch 3 / 100: avg data time: 1.05e+01, avg batch time: 11.0689, average train loss: 0.8962
[11/14 18:32:42 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1789, average loss: 0.7044
[11/14 18:32:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.92	
[11/14 18:32:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/14 18:39:00 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.7965, average train loss: 0.7820
[11/14 18:39:43 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1769, average loss: 0.7135
[11/14 18:39:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.72	
[11/14 18:39:43 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/14 18:45:56 visual_prompt]: Epoch 5 / 100: avg data time: 1.01e+01, avg batch time: 10.6328, average train loss: 0.7455
[11/14 18:46:38 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1742, average loss: 0.7162
[11/14 18:46:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.65	
[11/14 18:46:38 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/14 18:54:03 visual_prompt]: Epoch 6 / 100: avg data time: 1.21e+01, avg batch time: 12.6912, average train loss: 0.6861
[11/14 18:54:59 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1798, average loss: 0.6859
[11/14 18:54:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.65	
[11/14 18:54:59 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/14 19:02:57 visual_prompt]: Epoch 7 / 100: avg data time: 1.31e+01, avg batch time: 13.6454, average train loss: 0.6416
[11/14 19:04:05 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1840, average loss: 0.6250
[11/14 19:04:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 68.25	
[11/14 19:04:05 visual_prompt]: Best epoch 7: best metric: -0.625
[11/14 19:04:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/14 19:12:15 visual_prompt]: Epoch 8 / 100: avg data time: 1.34e+01, avg batch time: 13.9849, average train loss: 0.6677
[11/14 19:13:16 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1719, average loss: 0.8172
[11/14 19:13:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.84	
[11/14 19:13:16 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/14 19:21:22 visual_prompt]: Epoch 9 / 100: avg data time: 1.34e+01, avg batch time: 13.8862, average train loss: 0.5855
[11/14 19:22:26 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1771, average loss: 0.6318
[11/14 19:22:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 68.24	
[11/14 19:22:26 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/14 19:30:13 visual_prompt]: Epoch 10 / 100: avg data time: 1.28e+01, avg batch time: 13.3333, average train loss: 0.5394
[11/14 19:31:15 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1723, average loss: 0.6388
[11/14 19:31:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.99	
[11/14 19:31:15 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/14 19:39:17 visual_prompt]: Epoch 11 / 100: avg data time: 1.32e+01, avg batch time: 13.7506, average train loss: 0.4939
[11/14 19:40:14 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1728, average loss: 0.8244
[11/14 19:40:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.68	
[11/14 19:40:14 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/14 19:48:05 visual_prompt]: Epoch 12 / 100: avg data time: 1.29e+01, avg batch time: 13.4473, average train loss: 0.5270
[11/14 19:49:09 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1735, average loss: 0.8798
[11/14 19:49:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 69.06	
[11/14 19:49:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/14 19:56:49 visual_prompt]: Epoch 13 / 100: avg data time: 1.26e+01, avg batch time: 13.1614, average train loss: 0.4742
[11/14 19:57:47 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1775, average loss: 0.7313
[11/14 19:57:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.39	
[11/14 19:57:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/14 20:05:41 visual_prompt]: Epoch 14 / 100: avg data time: 1.30e+01, avg batch time: 13.5333, average train loss: 0.4177
[11/14 20:06:44 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1813, average loss: 0.6655
[11/14 20:06:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.63	
[11/14 20:06:44 visual_prompt]: Stopping early.
[11/14 20:06:44 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 20:06:44 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 20:06:44 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 20:06:44 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 20:06:44 visual_prompt]: Training with config:
[11/14 20:06:44 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.0001/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 20:06:44 visual_prompt]: Loading training data...
[11/14 20:06:44 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 20:06:44 visual_prompt]: Loading validation data...
[11/14 20:06:44 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 20:06:44 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 20:07:04 visual_prompt]: Enable all parameters update during training
[11/14 20:07:04 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 20:07:04 visual_prompt]: tuned percent:100.000
[11/14 20:07:04 visual_prompt]: Device used for model: 0
[11/14 20:07:04 visual_prompt]: Setting up Evaluator...
[11/14 20:07:04 visual_prompt]: Setting up Trainer...
[11/14 20:07:04 visual_prompt]: 	Setting up the optimizer...
[11/14 20:07:04 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 20:14:38 visual_prompt]: Epoch 1 / 100: avg data time: 1.24e+01, avg batch time: 12.9483, average train loss: 6.9791
[11/14 20:15:47 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1740, average loss: 6.3857
[11/14 20:15:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 20:15:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/14 20:22:15 visual_prompt]: Epoch 2 / 100: avg data time: 1.05e+01, avg batch time: 11.0726, average train loss: 2.2494
[11/14 20:22:58 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1829, average loss: 0.8412
[11/14 20:22:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 51.72	
[11/14 20:22:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/14 20:29:21 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9284, average train loss: 0.8962
[11/14 20:30:05 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1777, average loss: 0.7044
[11/14 20:30:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.92	
[11/14 20:30:05 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/14 20:36:24 visual_prompt]: Epoch 4 / 100: avg data time: 1.03e+01, avg batch time: 10.8334, average train loss: 0.7820
[11/14 20:37:08 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1799, average loss: 0.7135
[11/14 20:37:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.72	
[11/14 20:37:08 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/14 20:43:25 visual_prompt]: Epoch 5 / 100: avg data time: 1.02e+01, avg batch time: 10.7807, average train loss: 0.7455
[11/14 20:44:08 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1791, average loss: 0.7162
[11/14 20:44:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.65	
[11/14 20:44:08 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/14 20:50:27 visual_prompt]: Epoch 6 / 100: avg data time: 1.03e+01, avg batch time: 10.8313, average train loss: 0.6861
[11/14 20:51:11 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1743, average loss: 0.6859
[11/14 20:51:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.65	
[11/14 20:51:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/14 20:57:26 visual_prompt]: Epoch 7 / 100: avg data time: 1.02e+01, avg batch time: 10.7272, average train loss: 0.6416
[11/14 20:58:10 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1809, average loss: 0.6250
[11/14 20:58:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 68.25	
[11/14 20:58:10 visual_prompt]: Best epoch 7: best metric: -0.625
[11/14 20:58:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/14 21:04:24 visual_prompt]: Epoch 8 / 100: avg data time: 1.01e+01, avg batch time: 10.6757, average train loss: 0.6677
[11/14 21:05:06 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1742, average loss: 0.8172
[11/14 21:05:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.84	
[11/14 21:05:06 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/14 21:11:19 visual_prompt]: Epoch 9 / 100: avg data time: 1.01e+01, avg batch time: 10.6378, average train loss: 0.5855
[11/14 21:12:02 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1746, average loss: 0.6318
[11/14 21:12:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 68.24	
[11/14 21:12:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/14 21:18:14 visual_prompt]: Epoch 10 / 100: avg data time: 1.01e+01, avg batch time: 10.6259, average train loss: 0.5394
[11/14 21:18:56 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1830, average loss: 0.6388
[11/14 21:18:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.99	
[11/14 21:18:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/14 21:25:09 visual_prompt]: Epoch 11 / 100: avg data time: 1.01e+01, avg batch time: 10.6340, average train loss: 0.4939
[11/14 21:25:51 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1802, average loss: 0.8244
[11/14 21:25:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.68	
[11/14 21:25:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/14 21:32:03 visual_prompt]: Epoch 12 / 100: avg data time: 1.01e+01, avg batch time: 10.6279, average train loss: 0.5270
[11/14 21:32:46 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1785, average loss: 0.8798
[11/14 21:32:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 69.06	
[11/14 21:32:46 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/14 21:38:58 visual_prompt]: Epoch 13 / 100: avg data time: 1.01e+01, avg batch time: 10.6220, average train loss: 0.4742
[11/14 21:39:41 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1752, average loss: 0.7313
[11/14 21:39:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.39	
[11/14 21:39:41 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/14 21:45:56 visual_prompt]: Epoch 14 / 100: avg data time: 1.02e+01, avg batch time: 10.7115, average train loss: 0.4177
[11/14 21:46:39 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1814, average loss: 0.6655
[11/14 21:46:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.63	
[11/14 21:46:39 visual_prompt]: Stopping early.
[11/14 21:46:39 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 21:46:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 21:46:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 21:46:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 21:46:39 visual_prompt]: Training with config:
[11/14 21:46:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.0/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 21:46:39 visual_prompt]: Loading training data...
[11/14 21:46:39 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 21:46:39 visual_prompt]: Loading validation data...
[11/14 21:46:39 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 21:46:39 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 21:46:41 visual_prompt]: Enable all parameters update during training
[11/14 21:46:41 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 21:46:41 visual_prompt]: tuned percent:100.000
[11/14 21:46:41 visual_prompt]: Device used for model: 0
[11/14 21:46:41 visual_prompt]: Setting up Evaluator...
[11/14 21:46:41 visual_prompt]: Setting up Trainer...
[11/14 21:46:41 visual_prompt]: 	Setting up the optimizer...
[11/14 21:46:41 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 21:52:58 visual_prompt]: Epoch 1 / 100: avg data time: 1.02e+01, avg batch time: 10.7496, average train loss: 6.9791
[11/14 21:53:41 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1758, average loss: 6.3857
[11/14 21:53:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 21:53:41 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/14 21:59:55 visual_prompt]: Epoch 2 / 100: avg data time: 1.01e+01, avg batch time: 10.6759, average train loss: 2.4083
[11/14 22:00:38 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1796, average loss: 1.1167
[11/14 22:00:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 50.01	
[11/14 22:00:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/14 22:07:06 visual_prompt]: Epoch 3 / 100: avg data time: 1.05e+01, avg batch time: 11.0894, average train loss: 1.1687
[11/14 22:08:00 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.1795, average loss: 1.2126
[11/14 22:08:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 53.98	
[11/14 22:08:00 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/14 22:15:22 visual_prompt]: Epoch 4 / 100: avg data time: 1.21e+01, avg batch time: 12.6152, average train loss: 1.0270
[11/14 22:16:19 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1745, average loss: 0.9335
[11/14 22:16:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 54.21	
[11/14 22:16:19 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/14 22:23:36 visual_prompt]: Epoch 5 / 100: avg data time: 1.19e+01, avg batch time: 12.4785, average train loss: 0.9454
[11/14 22:24:43 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1775, average loss: 0.9480
[11/14 22:24:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 51.83	
[11/14 22:24:43 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/14 22:32:07 visual_prompt]: Epoch 6 / 100: avg data time: 1.21e+01, avg batch time: 12.6673, average train loss: 0.9100
[11/14 22:33:05 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1767, average loss: 0.9315
[11/14 22:33:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 52.85	
[11/14 22:33:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/14 22:40:23 visual_prompt]: Epoch 7 / 100: avg data time: 1.20e+01, avg batch time: 12.5035, average train loss: 0.8703
[11/14 22:41:22 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1785, average loss: 1.0649
[11/14 22:41:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 52.85	
[11/14 22:41:22 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/14 22:49:24 visual_prompt]: Epoch 8 / 100: avg data time: 1.32e+01, avg batch time: 13.7614, average train loss: 0.8369
[11/14 22:50:22 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1745, average loss: 0.7743
[11/14 22:50:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 53.98	
[11/14 22:50:22 visual_prompt]: Best epoch 8: best metric: -0.774
[11/14 22:50:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/14 22:58:02 visual_prompt]: Epoch 9 / 100: avg data time: 1.26e+01, avg batch time: 13.1311, average train loss: 0.7901
[11/14 22:58:54 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1724, average loss: 0.9431
[11/14 22:58:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 54.30	
[11/14 22:58:54 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/14 23:06:22 visual_prompt]: Epoch 10 / 100: avg data time: 1.23e+01, avg batch time: 12.7897, average train loss: 0.8237
[11/14 23:07:12 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1756, average loss: 0.7581
[11/14 23:07:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 54.80	
[11/14 23:07:12 visual_prompt]: Best epoch 10: best metric: -0.758
[11/14 23:07:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/14 23:14:40 visual_prompt]: Epoch 11 / 100: avg data time: 1.22e+01, avg batch time: 12.7772, average train loss: 0.7639
[11/14 23:15:30 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1782, average loss: 0.9742
[11/14 23:15:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.16	
[11/14 23:15:30 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/14 23:23:04 visual_prompt]: Epoch 12 / 100: avg data time: 1.24e+01, avg batch time: 12.9655, average train loss: 0.7347
[11/14 23:24:02 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1721, average loss: 0.8635
[11/14 23:24:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.80	
[11/14 23:24:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/14 23:31:23 visual_prompt]: Epoch 13 / 100: avg data time: 1.21e+01, avg batch time: 12.5824, average train loss: 0.7321
[11/14 23:32:06 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1782, average loss: 0.8714
[11/14 23:32:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.47	
[11/14 23:32:06 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/14 23:38:20 visual_prompt]: Epoch 14 / 100: avg data time: 1.01e+01, avg batch time: 10.6778, average train loss: 0.7440
[11/14 23:39:02 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1797, average loss: 0.7656
[11/14 23:39:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.45	
[11/14 23:39:02 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/14 23:45:18 visual_prompt]: Epoch 15 / 100: avg data time: 1.02e+01, avg batch time: 10.7395, average train loss: 0.7128
[11/14 23:46:01 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1818, average loss: 0.8723
[11/14 23:46:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.15	
[11/14 23:46:01 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/14 23:52:17 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.7151, average train loss: 0.7028
[11/14 23:53:00 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1818, average loss: 0.8690
[11/14 23:53:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.74	
[11/14 23:53:00 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/14 23:59:15 visual_prompt]: Epoch 17 / 100: avg data time: 1.01e+01, avg batch time: 10.6981, average train loss: 0.7061
[11/14 23:59:58 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1808, average loss: 0.7900
[11/14 23:59:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 57.69	
[11/14 23:59:58 visual_prompt]: Stopping early.
[11/14 23:59:58 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 23:59:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 23:59:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/14 23:59:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 23:59:58 visual_prompt]: Training with config:
[11/14 23:59:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/test/seed9805/lrNone_wdNone/patience7/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 9805, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': None, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': None, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 23:59:58 visual_prompt]: Loading training data...
[11/14 23:59:58 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 23:59:58 visual_prompt]: Loading validation data...
[11/14 23:59:58 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 23:59:58 visual_prompt]: Loading test data...
[11/14 23:59:58 visual_prompt]: Constructing mammo-cbis dataset test...
[11/14 23:59:58 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 23:59:59 visual_prompt]: Enable all parameters update during training
[11/14 23:59:59 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 23:59:59 visual_prompt]: tuned percent:100.000
[11/14 23:59:59 visual_prompt]: Device used for model: 0
[11/14 23:59:59 visual_prompt]: Setting up Evaluator...
[11/14 23:59:59 visual_prompt]: Setting up Trainer...
[11/14 23:59:59 visual_prompt]: 	Setting up the optimizer...
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 99, in <module>
    main(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 94, in main
    train(cfg, args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 43, in train
    trainer = Trainer(cfg, model, evaluator, cur_device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/engine/trainer.py", line 46, in __init__
    self.optimizer = make_optimizer([self.model], cfg.SOLVER)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/solver/optimizer.py", line 36, in make_optimizer
    if train_params.WEIGHT_DECAY > 0:
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: '>' not supported between instances of 'NoneType' and 'int'
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

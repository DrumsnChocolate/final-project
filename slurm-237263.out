/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/25 22:00:48 visual_prompt]: Rank of current process: 0. World size: 1
[09/25 22:00:50 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/25 22:00:50 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/25 22:00:50 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/25 22:00:50 visual_prompt]: Training with config:
[09/25 22:00:50 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/25 22:00:50 visual_prompt]: Loading training data...
2023-09-25 22:00:50.878416: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-25 22:00:51.260182: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-25 22:00:53.371603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[09/25 22:00:57 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 22:01:00 visual_prompt]: Number of images: 800
[09/25 22:01:00 visual_prompt]: Number of classes: 47 / 47
[09/25 22:01:00 visual_prompt]: Loading validation data...
[09/25 22:01:00 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 22:01:01 visual_prompt]: Number of images: 200
[09/25 22:01:01 visual_prompt]: Number of classes: 47 / 47
[09/25 22:01:01 visual_prompt]: Constructing models...
[09/25 22:01:03 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/25 22:01:03 visual_prompt]: tuned percent:0.576
[09/25 22:01:06 visual_prompt]: Device used for model: 0
[09/25 22:01:06 visual_prompt]: Setting up Evaluator...
[09/25 22:01:06 visual_prompt]: Setting up Trainer...
[09/25 22:01:06 visual_prompt]: 	Setting up the optimizer...
[09/25 22:01:06 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/25 22:01:18 visual_prompt]: Epoch 1 / 100: avg data time: 1.78e-01, avg batch time: 0.8841, average train loss: 3.9339
[09/25 22:01:19 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1668, average loss: 3.9045
[09/25 22:01:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 22:01:19 visual_prompt]: Best epoch 1: best metric: 0.040
[09/25 22:01:19 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/25 22:01:25 visual_prompt]: Epoch 2 / 100: avg data time: 3.31e-02, avg batch time: 0.4754, average train loss: 4.4965
[09/25 22:01:27 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1664, average loss: 4.3548
[09/25 22:01:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.50	
[09/25 22:01:27 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/25 22:01:33 visual_prompt]: Epoch 3 / 100: avg data time: 3.56e-02, avg batch time: 0.4770, average train loss: 6.2304
[09/25 22:01:34 visual_prompt]: Inference (val):avg data time: 1.82e-05, avg batch time: 0.1657, average loss: 5.8067
[09/25 22:01:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:01:34 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/25 22:01:41 visual_prompt]: Epoch 4 / 100: avg data time: 4.55e-02, avg batch time: 0.4851, average train loss: 8.9884
[09/25 22:01:42 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1660, average loss: 15.2282
[09/25 22:01:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.00	
[09/25 22:01:42 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/25 22:01:48 visual_prompt]: Epoch 5 / 100: avg data time: 3.30e-02, avg batch time: 0.4745, average train loss: 25.5231
[09/25 22:01:49 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1667, average loss: 47.8106
[09/25 22:01:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 22:01:49 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/25 22:01:56 visual_prompt]: Epoch 6 / 100: avg data time: 4.74e-02, avg batch time: 0.4895, average train loss: 57.9913
[09/25 22:01:57 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1670, average loss: 72.4077
[09/25 22:01:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.00	
[09/25 22:01:57 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/25 22:02:04 visual_prompt]: Epoch 7 / 100: avg data time: 4.64e-02, avg batch time: 0.4895, average train loss: 96.3448
[09/25 22:02:05 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1672, average loss: 106.1029
[09/25 22:02:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 22:02:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/25 22:02:12 visual_prompt]: Epoch 8 / 100: avg data time: 5.03e-02, avg batch time: 0.4946, average train loss: 121.4568
[09/25 22:02:13 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1672, average loss: 104.2609
[09/25 22:02:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 12.00	
[09/25 22:02:13 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/25 22:02:19 visual_prompt]: Epoch 9 / 100: avg data time: 3.25e-02, avg batch time: 0.4773, average train loss: 147.9032
[09/25 22:02:21 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1678, average loss: 136.8993
[09/25 22:02:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.50	
[09/25 22:02:21 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/25 22:02:27 visual_prompt]: Epoch 10 / 100: avg data time: 4.27e-02, avg batch time: 0.4865, average train loss: 167.1612
[09/25 22:02:28 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1677, average loss: 193.4287
[09/25 22:02:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.00	
[09/25 22:02:28 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/25 22:02:35 visual_prompt]: Epoch 11 / 100: avg data time: 3.59e-02, avg batch time: 0.4815, average train loss: 184.9019
[09/25 22:02:36 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1677, average loss: 192.5710
[09/25 22:02:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 22:02:36 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/25 22:02:43 visual_prompt]: Epoch 12 / 100: avg data time: 4.89e-02, avg batch time: 0.4928, average train loss: 202.6724
[09/25 22:02:44 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1687, average loss: 240.9324
[09/25 22:02:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 22:02:44 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/25 22:02:50 visual_prompt]: Epoch 13 / 100: avg data time: 3.97e-02, avg batch time: 0.4873, average train loss: 242.2119
[09/25 22:02:52 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1687, average loss: 187.9323
[09/25 22:02:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/25 22:02:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/25 22:02:58 visual_prompt]: Epoch 14 / 100: avg data time: 4.58e-02, avg batch time: 0.4914, average train loss: 246.1718
[09/25 22:02:59 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1688, average loss: 222.0196
[09/25 22:02:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/25 22:02:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/25 22:03:06 visual_prompt]: Epoch 15 / 100: avg data time: 3.87e-02, avg batch time: 0.4847, average train loss: 244.7276
[09/25 22:03:07 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 241.1033
[09/25 22:03:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/25 22:03:07 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/25 22:03:14 visual_prompt]: Epoch 16 / 100: avg data time: 3.69e-02, avg batch time: 0.4839, average train loss: 262.1415
[09/25 22:03:15 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1693, average loss: 259.4966
[09/25 22:03:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/25 22:03:15 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/25 22:03:22 visual_prompt]: Epoch 17 / 100: avg data time: 5.41e-02, avg batch time: 0.5018, average train loss: 251.2750
[09/25 22:03:23 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1688, average loss: 253.7366
[09/25 22:03:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 14.00	
[09/25 22:03:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/25 22:03:30 visual_prompt]: Epoch 18 / 100: avg data time: 4.29e-02, avg batch time: 0.4923, average train loss: 215.1298
[09/25 22:03:31 visual_prompt]: Inference (val):avg data time: 1.72e-05, avg batch time: 0.1692, average loss: 250.4905
[09/25 22:03:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 13.00	
[09/25 22:03:31 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/25 22:03:37 visual_prompt]: Epoch 19 / 100: avg data time: 3.48e-02, avg batch time: 0.4848, average train loss: 231.8828
[09/25 22:03:38 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1692, average loss: 236.7275
[09/25 22:03:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/25 22:03:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/25 22:03:45 visual_prompt]: Epoch 20 / 100: avg data time: 3.76e-02, avg batch time: 0.4886, average train loss: 224.4181
[09/25 22:03:46 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1694, average loss: 202.5445
[09/25 22:03:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 22:03:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/25 22:03:53 visual_prompt]: Epoch 21 / 100: avg data time: 4.28e-02, avg batch time: 0.4908, average train loss: 215.8170
[09/25 22:03:54 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1690, average loss: 229.8243
[09/25 22:03:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.00	
[09/25 22:03:54 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/25 22:04:01 visual_prompt]: Epoch 22 / 100: avg data time: 4.15e-02, avg batch time: 0.4918, average train loss: 226.5881
[09/25 22:04:02 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1693, average loss: 253.4220
[09/25 22:04:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.50	
[09/25 22:04:02 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/25 22:04:09 visual_prompt]: Epoch 23 / 100: avg data time: 4.09e-02, avg batch time: 0.4902, average train loss: 212.0426
[09/25 22:04:10 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1694, average loss: 283.1734
[09/25 22:04:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.50	
[09/25 22:04:10 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/25 22:04:16 visual_prompt]: Epoch 24 / 100: avg data time: 5.06e-02, avg batch time: 0.5000, average train loss: 213.1316
[09/25 22:04:18 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1691, average loss: 216.5067
[09/25 22:04:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/25 22:04:18 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/25 22:04:24 visual_prompt]: Epoch 25 / 100: avg data time: 4.47e-02, avg batch time: 0.4926, average train loss: 216.3289
[09/25 22:04:26 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1694, average loss: 212.8815
[09/25 22:04:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 13.00	
[09/25 22:04:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/25 22:04:32 visual_prompt]: Epoch 26 / 100: avg data time: 4.37e-02, avg batch time: 0.4925, average train loss: 202.0133
[09/25 22:04:33 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1695, average loss: 231.2200
[09/25 22:04:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/25 22:04:33 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/25 22:04:40 visual_prompt]: Epoch 27 / 100: avg data time: 4.38e-02, avg batch time: 0.4915, average train loss: 205.9198
[09/25 22:04:41 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1691, average loss: 220.7670
[09/25 22:04:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/25 22:04:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/25 22:04:48 visual_prompt]: Epoch 28 / 100: avg data time: 4.07e-02, avg batch time: 0.4893, average train loss: 194.5555
[09/25 22:04:49 visual_prompt]: Inference (val):avg data time: 1.59e-05, avg batch time: 0.1693, average loss: 165.5885
[09/25 22:04:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 22:04:49 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/25 22:04:56 visual_prompt]: Epoch 29 / 100: avg data time: 4.15e-02, avg batch time: 0.4893, average train loss: 206.3386
[09/25 22:04:57 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1699, average loss: 180.1577
[09/25 22:04:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/25 22:04:57 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/25 22:05:04 visual_prompt]: Epoch 30 / 100: avg data time: 4.22e-02, avg batch time: 0.4921, average train loss: 200.8436
[09/25 22:05:05 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1699, average loss: 181.5345
[09/25 22:05:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.00	
[09/25 22:05:05 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/25 22:05:11 visual_prompt]: Epoch 31 / 100: avg data time: 3.57e-02, avg batch time: 0.4855, average train loss: 186.4705
[09/25 22:05:13 visual_prompt]: Inference (val):avg data time: 1.81e-05, avg batch time: 0.1697, average loss: 177.1194
[09/25 22:05:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 9.50	
[09/25 22:05:13 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/25 22:05:19 visual_prompt]: Epoch 32 / 100: avg data time: 4.72e-02, avg batch time: 0.4952, average train loss: 205.2205
[09/25 22:05:20 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1693, average loss: 234.0032
[09/25 22:05:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/25 22:05:20 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/25 22:05:27 visual_prompt]: Epoch 33 / 100: avg data time: 4.06e-02, avg batch time: 0.4903, average train loss: 231.8244
[09/25 22:05:28 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1696, average loss: 222.6701
[09/25 22:05:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.00	
[09/25 22:05:28 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/25 22:05:35 visual_prompt]: Epoch 34 / 100: avg data time: 3.63e-02, avg batch time: 0.4846, average train loss: 228.5252
[09/25 22:05:36 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1695, average loss: 211.7964
[09/25 22:05:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/25 22:05:36 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/25 22:05:43 visual_prompt]: Epoch 35 / 100: avg data time: 3.82e-02, avg batch time: 0.4884, average train loss: 208.7241
[09/25 22:05:44 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1694, average loss: 166.6229
[09/25 22:05:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.00	
[09/25 22:05:44 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/25 22:05:50 visual_prompt]: Epoch 36 / 100: avg data time: 4.36e-02, avg batch time: 0.4933, average train loss: 170.7428
[09/25 22:05:52 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1703, average loss: 173.4180
[09/25 22:05:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.00	
[09/25 22:05:52 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/25 22:05:58 visual_prompt]: Epoch 37 / 100: avg data time: 3.49e-02, avg batch time: 0.4837, average train loss: 176.0643
[09/25 22:05:59 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 198.0836
[09/25 22:05:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.00	
[09/25 22:05:59 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/25 22:06:06 visual_prompt]: Epoch 38 / 100: avg data time: 4.24e-02, avg batch time: 0.4907, average train loss: 186.0431
[09/25 22:06:07 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1698, average loss: 169.5206
[09/25 22:06:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 15.00	
[09/25 22:06:07 visual_prompt]: Best epoch 38: best metric: 0.050
[09/25 22:06:07 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/25 22:06:14 visual_prompt]: Epoch 39 / 100: avg data time: 4.07e-02, avg batch time: 0.4891, average train loss: 192.0977
[09/25 22:06:15 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1696, average loss: 200.1247
[09/25 22:06:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/25 22:06:15 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/25 22:06:21 visual_prompt]: Epoch 40 / 100: avg data time: 3.32e-02, avg batch time: 0.4823, average train loss: 183.8880
[09/25 22:06:22 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1695, average loss: 168.2029
[09/25 22:06:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/25 22:06:22 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/25 22:06:29 visual_prompt]: Epoch 41 / 100: avg data time: 4.43e-02, avg batch time: 0.4928, average train loss: 185.9192
[09/25 22:06:30 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1697, average loss: 212.1549
[09/25 22:06:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 22:06:30 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/25 22:06:37 visual_prompt]: Epoch 42 / 100: avg data time: 3.19e-02, avg batch time: 0.4844, average train loss: 185.0672
[09/25 22:06:38 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1698, average loss: 200.9360
[09/25 22:06:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:06:38 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/25 22:06:45 visual_prompt]: Epoch 43 / 100: avg data time: 4.14e-02, avg batch time: 0.4906, average train loss: 142.7287
[09/25 22:06:46 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1699, average loss: 135.9980
[09/25 22:06:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 22:06:46 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/25 22:06:52 visual_prompt]: Epoch 44 / 100: avg data time: 3.30e-02, avg batch time: 0.4839, average train loss: 165.5704
[09/25 22:06:53 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1703, average loss: 188.3469
[09/25 22:06:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 22:06:53 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/25 22:07:00 visual_prompt]: Epoch 45 / 100: avg data time: 5.31e-02, avg batch time: 0.5016, average train loss: 171.5312
[09/25 22:07:01 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1697, average loss: 160.1755
[09/25 22:07:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.00	
[09/25 22:07:01 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/25 22:07:08 visual_prompt]: Epoch 46 / 100: avg data time: 3.89e-02, avg batch time: 0.4885, average train loss: 148.3785
[09/25 22:07:09 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1698, average loss: 132.3660
[09/25 22:07:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 5.50	
[09/25 22:07:09 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/25 22:07:16 visual_prompt]: Epoch 47 / 100: avg data time: 4.52e-02, avg batch time: 0.4956, average train loss: 126.9958
[09/25 22:07:17 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1697, average loss: 131.6038
[09/25 22:07:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.50	
[09/25 22:07:17 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/25 22:07:24 visual_prompt]: Epoch 48 / 100: avg data time: 4.86e-02, avg batch time: 0.4975, average train loss: 126.7113
[09/25 22:07:25 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 99.3967
[09/25 22:07:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 11.50	
[09/25 22:07:25 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/25 22:07:32 visual_prompt]: Epoch 49 / 100: avg data time: 5.27e-02, avg batch time: 0.5017, average train loss: 109.6754
[09/25 22:07:33 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1699, average loss: 153.0273
[09/25 22:07:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 22:07:33 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/25 22:07:39 visual_prompt]: Epoch 50 / 100: avg data time: 4.14e-02, avg batch time: 0.4908, average train loss: 132.1690
[09/25 22:07:41 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1704, average loss: 139.2925
[09/25 22:07:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 22:07:41 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/25 22:07:47 visual_prompt]: Epoch 51 / 100: avg data time: 3.47e-02, avg batch time: 0.4845, average train loss: 134.0287
[09/25 22:07:49 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1697, average loss: 125.2086
[09/25 22:07:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.00	
[09/25 22:07:49 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/25 22:07:55 visual_prompt]: Epoch 52 / 100: avg data time: 3.42e-02, avg batch time: 0.4852, average train loss: 129.1458
[09/25 22:07:56 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1699, average loss: 102.5506
[09/25 22:07:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.50	
[09/25 22:07:56 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/25 22:08:03 visual_prompt]: Epoch 53 / 100: avg data time: 3.37e-02, avg batch time: 0.4853, average train loss: 105.2318
[09/25 22:08:04 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1698, average loss: 132.5815
[09/25 22:08:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.50	
[09/25 22:08:04 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/25 22:08:11 visual_prompt]: Epoch 54 / 100: avg data time: 4.58e-02, avg batch time: 0.4948, average train loss: 109.6038
[09/25 22:08:12 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1695, average loss: 118.3219
[09/25 22:08:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/25 22:08:12 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/25 22:08:19 visual_prompt]: Epoch 55 / 100: avg data time: 4.41e-02, avg batch time: 0.4937, average train loss: 122.0130
[09/25 22:08:20 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1696, average loss: 101.8958
[09/25 22:08:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 22:08:20 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/25 22:08:26 visual_prompt]: Epoch 56 / 100: avg data time: 3.80e-02, avg batch time: 0.4880, average train loss: 96.2601
[09/25 22:08:28 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 98.2696
[09/25 22:08:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/25 22:08:28 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/25 22:08:34 visual_prompt]: Epoch 57 / 100: avg data time: 4.47e-02, avg batch time: 0.4937, average train loss: 102.5919
[09/25 22:08:35 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1693, average loss: 119.5300
[09/25 22:08:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/25 22:08:35 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/25 22:08:42 visual_prompt]: Epoch 58 / 100: avg data time: 3.82e-02, avg batch time: 0.4869, average train loss: 121.5066
[09/25 22:08:43 visual_prompt]: Inference (val):avg data time: 1.87e-05, avg batch time: 0.1698, average loss: 132.7546
[09/25 22:08:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/25 22:08:43 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/25 22:08:50 visual_prompt]: Epoch 59 / 100: avg data time: 4.83e-02, avg batch time: 0.4969, average train loss: 132.1336
[09/25 22:08:51 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1700, average loss: 120.7212
[09/25 22:08:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 22:08:51 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/25 22:08:58 visual_prompt]: Epoch 60 / 100: avg data time: 4.32e-02, avg batch time: 0.4928, average train loss: 120.9567
[09/25 22:08:59 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1700, average loss: 106.7046
[09/25 22:08:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/25 22:08:59 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/25 22:09:05 visual_prompt]: Epoch 61 / 100: avg data time: 3.79e-02, avg batch time: 0.4864, average train loss: 93.3377
[09/25 22:09:07 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1701, average loss: 88.0009
[09/25 22:09:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.50	
[09/25 22:09:07 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/25 22:09:13 visual_prompt]: Epoch 62 / 100: avg data time: 4.42e-02, avg batch time: 0.4945, average train loss: 95.8363
[09/25 22:09:14 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1701, average loss: 96.7849
[09/25 22:09:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.50	
[09/25 22:09:14 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/25 22:09:21 visual_prompt]: Epoch 63 / 100: avg data time: 4.77e-02, avg batch time: 0.4960, average train loss: 92.4903
[09/25 22:09:22 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1700, average loss: 89.8175
[09/25 22:09:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 22:09:22 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/25 22:09:29 visual_prompt]: Epoch 64 / 100: avg data time: 3.52e-02, avg batch time: 0.4847, average train loss: 92.9184
[09/25 22:09:30 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1698, average loss: 77.6983
[09/25 22:09:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 10.50	
[09/25 22:09:30 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/25 22:09:37 visual_prompt]: Epoch 65 / 100: avg data time: 3.98e-02, avg batch time: 0.4898, average train loss: 84.3468
[09/25 22:09:38 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1699, average loss: 79.6912
[09/25 22:09:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.00	
[09/25 22:09:38 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/25 22:09:44 visual_prompt]: Epoch 66 / 100: avg data time: 3.46e-02, avg batch time: 0.4837, average train loss: 77.5589
[09/25 22:09:46 visual_prompt]: Inference (val):avg data time: 1.77e-05, avg batch time: 0.1697, average loss: 69.8579
[09/25 22:09:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/25 22:09:46 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/25 22:09:52 visual_prompt]: Epoch 67 / 100: avg data time: 4.32e-02, avg batch time: 0.4940, average train loss: 70.0896
[09/25 22:09:53 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1699, average loss: 152.3661
[09/25 22:09:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.50	
[09/25 22:09:53 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/25 22:10:00 visual_prompt]: Epoch 68 / 100: avg data time: 4.00e-02, avg batch time: 0.4918, average train loss: 63.8031
[09/25 22:10:01 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1696, average loss: 52.5821
[09/25 22:10:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.00	
[09/25 22:10:01 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/25 22:10:08 visual_prompt]: Epoch 69 / 100: avg data time: 3.88e-02, avg batch time: 0.4884, average train loss: 67.0036
[09/25 22:10:09 visual_prompt]: Inference (val):avg data time: 1.79e-05, avg batch time: 0.1697, average loss: 52.4367
[09/25 22:10:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 22:10:09 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/25 22:10:15 visual_prompt]: Epoch 70 / 100: avg data time: 3.79e-02, avg batch time: 0.4876, average train loss: 57.0593
[09/25 22:10:17 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1695, average loss: 74.3202
[09/25 22:10:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 22:10:17 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/25 22:10:23 visual_prompt]: Epoch 71 / 100: avg data time: 4.29e-02, avg batch time: 0.4922, average train loss: 61.5357
[09/25 22:10:25 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1699, average loss: 42.0840
[09/25 22:10:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.50	
[09/25 22:10:25 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/25 22:10:31 visual_prompt]: Epoch 72 / 100: avg data time: 4.54e-02, avg batch time: 0.4949, average train loss: 57.2225
[09/25 22:10:32 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1703, average loss: 70.8076
[09/25 22:10:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.00	
[09/25 22:10:32 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/25 22:10:39 visual_prompt]: Epoch 73 / 100: avg data time: 4.44e-02, avg batch time: 0.4955, average train loss: 53.5250
[09/25 22:10:40 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1693, average loss: 34.8534
[09/25 22:10:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 22:10:40 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/25 22:10:47 visual_prompt]: Epoch 74 / 100: avg data time: 4.54e-02, avg batch time: 0.4944, average train loss: 38.3059
[09/25 22:10:48 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1697, average loss: 26.5064
[09/25 22:10:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.50	
[09/25 22:10:48 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/25 22:10:55 visual_prompt]: Epoch 75 / 100: avg data time: 3.39e-02, avg batch time: 0.4837, average train loss: 44.8911
[09/25 22:10:56 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1700, average loss: 27.5679
[09/25 22:10:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 22:10:56 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/25 22:11:02 visual_prompt]: Epoch 76 / 100: avg data time: 4.64e-02, avg batch time: 0.4974, average train loss: 27.0026
[09/25 22:11:04 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1695, average loss: 32.6139
[09/25 22:11:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.50	
[09/25 22:11:04 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/25 22:11:10 visual_prompt]: Epoch 77 / 100: avg data time: 4.77e-02, avg batch time: 0.4961, average train loss: 29.3047
[09/25 22:11:12 visual_prompt]: Inference (val):avg data time: 1.79e-05, avg batch time: 0.1697, average loss: 20.4935
[09/25 22:11:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 15.50	
[09/25 22:11:12 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/25 22:11:18 visual_prompt]: Epoch 78 / 100: avg data time: 3.52e-02, avg batch time: 0.4848, average train loss: 22.6067
[09/25 22:11:19 visual_prompt]: Inference (val):avg data time: 1.79e-05, avg batch time: 0.1697, average loss: 22.3595
[09/25 22:11:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 10.00	
[09/25 22:11:19 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/25 22:11:26 visual_prompt]: Epoch 79 / 100: avg data time: 4.35e-02, avg batch time: 0.4930, average train loss: 19.5951
[09/25 22:11:27 visual_prompt]: Inference (val):avg data time: 1.76e-05, avg batch time: 0.1695, average loss: 16.5600
[09/25 22:11:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/25 22:11:27 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/25 22:11:34 visual_prompt]: Epoch 80 / 100: avg data time: 3.41e-02, avg batch time: 0.4827, average train loss: 18.6772
[09/25 22:11:35 visual_prompt]: Inference (val):avg data time: 1.83e-05, avg batch time: 0.1697, average loss: 15.8109
[09/25 22:11:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/25 22:11:35 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/25 22:11:41 visual_prompt]: Epoch 81 / 100: avg data time: 4.07e-02, avg batch time: 0.4905, average train loss: 14.8198
[09/25 22:11:43 visual_prompt]: Inference (val):avg data time: 1.87e-05, avg batch time: 0.1698, average loss: 10.2449
[09/25 22:11:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.00	
[09/25 22:11:43 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/25 22:11:49 visual_prompt]: Epoch 82 / 100: avg data time: 4.04e-02, avg batch time: 0.4916, average train loss: 14.0157
[09/25 22:11:50 visual_prompt]: Inference (val):avg data time: 1.83e-05, avg batch time: 0.1695, average loss: 9.3123
[09/25 22:11:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 22:11:50 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/25 22:11:57 visual_prompt]: Epoch 83 / 100: avg data time: 3.54e-02, avg batch time: 0.4849, average train loss: 10.2539
[09/25 22:11:58 visual_prompt]: Inference (val):avg data time: 1.53e-05, avg batch time: 0.1700, average loss: 8.0802
[09/25 22:11:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.00	
[09/25 22:11:58 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/25 22:12:05 visual_prompt]: Epoch 84 / 100: avg data time: 3.47e-02, avg batch time: 0.4860, average train loss: 7.9096
[09/25 22:12:06 visual_prompt]: Inference (val):avg data time: 1.67e-05, avg batch time: 0.1699, average loss: 6.8485
[09/25 22:12:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.50	
[09/25 22:12:06 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/25 22:12:12 visual_prompt]: Epoch 85 / 100: avg data time: 3.78e-02, avg batch time: 0.4869, average train loss: 7.9989
[09/25 22:12:14 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1697, average loss: 5.9827
[09/25 22:12:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 7.00	
[09/25 22:12:14 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/25 22:12:20 visual_prompt]: Epoch 86 / 100: avg data time: 4.09e-02, avg batch time: 0.4907, average train loss: 7.2645
[09/25 22:12:21 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1701, average loss: 5.8870
[09/25 22:12:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 22:12:21 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/25 22:12:28 visual_prompt]: Epoch 87 / 100: avg data time: 4.05e-02, avg batch time: 0.4910, average train loss: 7.0650
[09/25 22:12:29 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1696, average loss: 5.2157
[09/25 22:12:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:12:29 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/25 22:12:36 visual_prompt]: Epoch 88 / 100: avg data time: 3.72e-02, avg batch time: 0.4882, average train loss: 6.3707
[09/25 22:12:37 visual_prompt]: Inference (val):avg data time: 1.78e-05, avg batch time: 0.1693, average loss: 5.0760
[09/25 22:12:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 7.00	
[09/25 22:12:37 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/25 22:12:44 visual_prompt]: Epoch 89 / 100: avg data time: 3.39e-02, avg batch time: 0.4859, average train loss: 6.1095
[09/25 22:12:45 visual_prompt]: Inference (val):avg data time: 1.74e-05, avg batch time: 0.1696, average loss: 4.8556
[09/25 22:12:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.50	
[09/25 22:12:45 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/25 22:12:51 visual_prompt]: Epoch 90 / 100: avg data time: 3.94e-02, avg batch time: 0.4931, average train loss: 5.6410
[09/25 22:12:53 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1697, average loss: 4.6708
[09/25 22:12:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.50	
[09/25 22:12:53 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/25 22:12:59 visual_prompt]: Epoch 91 / 100: avg data time: 5.12e-02, avg batch time: 0.4997, average train loss: 5.2092
[09/25 22:13:01 visual_prompt]: Inference (val):avg data time: 1.67e-05, avg batch time: 0.1696, average loss: 4.9867
[09/25 22:13:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/25 22:13:01 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/25 22:13:07 visual_prompt]: Epoch 92 / 100: avg data time: 4.98e-02, avg batch time: 0.4988, average train loss: 5.0301
[09/25 22:13:09 visual_prompt]: Inference (val):avg data time: 1.82e-05, avg batch time: 0.1699, average loss: 4.5301
[09/25 22:13:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/25 22:13:09 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/25 22:13:15 visual_prompt]: Epoch 93 / 100: avg data time: 5.25e-02, avg batch time: 0.5012, average train loss: 4.6672
[09/25 22:13:16 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1700, average loss: 4.2907
[09/25 22:13:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.00	
[09/25 22:13:16 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/25 22:13:23 visual_prompt]: Epoch 94 / 100: avg data time: 4.85e-02, avg batch time: 0.4988, average train loss: 4.6187
[09/25 22:13:24 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1694, average loss: 4.1821
[09/25 22:13:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 14.00	
[09/25 22:13:24 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/25 22:13:31 visual_prompt]: Epoch 95 / 100: avg data time: 5.08e-02, avg batch time: 0.5052, average train loss: 4.4699
[09/25 22:13:32 visual_prompt]: Inference (val):avg data time: 1.60e-05, avg batch time: 0.1700, average loss: 4.0766
[09/25 22:13:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.00	
[09/25 22:13:32 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/25 22:13:39 visual_prompt]: Epoch 96 / 100: avg data time: 4.22e-02, avg batch time: 0.4925, average train loss: 4.0943
[09/25 22:13:40 visual_prompt]: Inference (val):avg data time: 1.87e-05, avg batch time: 0.1696, average loss: 4.0951
[09/25 22:13:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 9.00	
[09/25 22:13:40 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/25 22:13:47 visual_prompt]: Epoch 97 / 100: avg data time: 4.07e-02, avg batch time: 0.4916, average train loss: 4.0372
[09/25 22:13:48 visual_prompt]: Inference (val):avg data time: 1.91e-05, avg batch time: 0.1696, average loss: 4.0113
[09/25 22:13:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/25 22:13:48 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/25 22:13:54 visual_prompt]: Epoch 98 / 100: avg data time: 3.35e-02, avg batch time: 0.4847, average train loss: 3.9718
[09/25 22:13:56 visual_prompt]: Inference (val):avg data time: 1.84e-05, avg batch time: 0.1698, average loss: 4.0128
[09/25 22:13:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 22:13:56 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/25 22:14:02 visual_prompt]: Epoch 99 / 100: avg data time: 5.02e-02, avg batch time: 0.4998, average train loss: 3.9007
[09/25 22:14:03 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1696, average loss: 3.8964
[09/25 22:14:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.50	
[09/25 22:14:03 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/25 22:14:10 visual_prompt]: Epoch 100 / 100: avg data time: 4.32e-02, avg batch time: 0.4929, average train loss: 3.8148
[09/25 22:14:11 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1699, average loss: 3.8889
[09/25 22:14:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/25 22:14:11 visual_prompt]: Rank of current process: 0. World size: 1
[09/25 22:14:11 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/25 22:14:11 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/25 22:14:11 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/25 22:14:11 visual_prompt]: Training with config:
[09/25 22:14:11 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/25 22:14:11 visual_prompt]: Loading training data...
[09/25 22:14:11 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 22:14:13 visual_prompt]: Number of images: 800
[09/25 22:14:13 visual_prompt]: Number of classes: 47 / 47
[09/25 22:14:13 visual_prompt]: Loading validation data...
[09/25 22:14:13 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 22:14:14 visual_prompt]: Number of images: 200
[09/25 22:14:14 visual_prompt]: Number of classes: 47 / 47
[09/25 22:14:14 visual_prompt]: Constructing models...
[09/25 22:14:17 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/25 22:14:17 visual_prompt]: tuned percent:0.576
[09/25 22:14:17 visual_prompt]: Device used for model: 0
[09/25 22:14:17 visual_prompt]: Setting up Evaluator...
[09/25 22:14:17 visual_prompt]: Setting up Trainer...
[09/25 22:14:17 visual_prompt]: 	Setting up the optimizer...
[09/25 22:14:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/25 22:14:23 visual_prompt]: Epoch 1 / 100: avg data time: 4.03e-02, avg batch time: 0.4880, average train loss: 3.9269
[09/25 22:14:25 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1690, average loss: 3.9045
[09/25 22:14:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 22:14:25 visual_prompt]: Best epoch 1: best metric: 0.040
[09/25 22:14:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/25 22:14:31 visual_prompt]: Epoch 2 / 100: avg data time: 4.93e-02, avg batch time: 0.4974, average train loss: 5.2182
[09/25 22:14:33 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1693, average loss: 7.2355
[09/25 22:14:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.50	
[09/25 22:14:33 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/25 22:14:39 visual_prompt]: Epoch 3 / 100: avg data time: 4.78e-02, avg batch time: 0.4952, average train loss: 9.8331
[09/25 22:14:41 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1690, average loss: 27.4551
[09/25 22:14:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.50	
[09/25 22:14:41 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/25 22:14:47 visual_prompt]: Epoch 4 / 100: avg data time: 4.72e-02, avg batch time: 0.4942, average train loss: 23.2521
[09/25 22:14:49 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1690, average loss: 36.2380
[09/25 22:14:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.00	
[09/25 22:14:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/25 22:14:55 visual_prompt]: Epoch 5 / 100: avg data time: 4.86e-02, avg batch time: 0.4963, average train loss: 35.4487
[09/25 22:14:57 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1690, average loss: 46.2603
[09/25 22:14:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 10.50	
[09/25 22:14:57 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/25 22:15:03 visual_prompt]: Epoch 6 / 100: avg data time: 4.62e-02, avg batch time: 0.4938, average train loss: 64.5704
[09/25 22:15:05 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 87.9154
[09/25 22:15:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 11.00	
[09/25 22:15:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/25 22:15:11 visual_prompt]: Epoch 7 / 100: avg data time: 5.44e-02, avg batch time: 0.5018, average train loss: 147.6430
[09/25 22:15:13 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1693, average loss: 171.2868
[09/25 22:15:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 14.00	
[09/25 22:15:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/25 22:15:19 visual_prompt]: Epoch 8 / 100: avg data time: 6.13e-02, avg batch time: 0.5091, average train loss: 170.9976
[09/25 22:15:21 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 248.5714
[09/25 22:15:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 22:15:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/25 22:15:27 visual_prompt]: Epoch 9 / 100: avg data time: 5.38e-02, avg batch time: 0.5023, average train loss: 248.5486
[09/25 22:15:29 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 256.4443
[09/25 22:15:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 22:15:29 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/25 22:15:35 visual_prompt]: Epoch 10 / 100: avg data time: 4.05e-02, avg batch time: 0.4887, average train loss: 243.3513
[09/25 22:15:37 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 145.1702
[09/25 22:15:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 7.50	
[09/25 22:15:37 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/25 22:15:43 visual_prompt]: Epoch 11 / 100: avg data time: 4.46e-02, avg batch time: 0.4934, average train loss: 237.2992
[09/25 22:15:45 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1697, average loss: 198.1209
[09/25 22:15:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.50	
[09/25 22:15:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/25 22:15:51 visual_prompt]: Epoch 12 / 100: avg data time: 4.03e-02, avg batch time: 0.4901, average train loss: 236.3143
[09/25 22:15:53 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1695, average loss: 183.0179
[09/25 22:15:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.50	
[09/25 22:15:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/25 22:15:59 visual_prompt]: Epoch 13 / 100: avg data time: 3.57e-02, avg batch time: 0.4867, average train loss: 238.0364
[09/25 22:16:00 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1694, average loss: 197.4994
[09/25 22:16:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 14.50	
[09/25 22:16:00 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/25 22:16:07 visual_prompt]: Epoch 14 / 100: avg data time: 5.45e-02, avg batch time: 0.5027, average train loss: 251.6041
[09/25 22:16:08 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 242.1604
[09/25 22:16:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 22:16:08 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/25 22:16:15 visual_prompt]: Epoch 15 / 100: avg data time: 4.36e-02, avg batch time: 0.4940, average train loss: 240.8386
[09/25 22:16:16 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1692, average loss: 249.4640
[09/25 22:16:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 22:16:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/25 22:16:23 visual_prompt]: Epoch 16 / 100: avg data time: 4.59e-02, avg batch time: 0.4949, average train loss: 228.7710
[09/25 22:16:25 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1697, average loss: 219.3326
[09/25 22:16:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.50	
[09/25 22:16:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/25 22:16:31 visual_prompt]: Epoch 17 / 100: avg data time: 3.51e-02, avg batch time: 0.4861, average train loss: 264.9301
[09/25 22:16:32 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 277.8388
[09/25 22:16:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.50	
[09/25 22:16:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/25 22:16:39 visual_prompt]: Epoch 18 / 100: avg data time: 5.53e-02, avg batch time: 0.5036, average train loss: 265.0148
[09/25 22:16:40 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1695, average loss: 248.6638
[09/25 22:16:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 22:16:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/25 22:16:47 visual_prompt]: Epoch 19 / 100: avg data time: 3.57e-02, avg batch time: 0.4868, average train loss: 202.8862
[09/25 22:16:48 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1691, average loss: 234.3419
[09/25 22:16:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/25 22:16:48 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/25 22:16:55 visual_prompt]: Epoch 20 / 100: avg data time: 4.90e-02, avg batch time: 0.4973, average train loss: 219.9208
[09/25 22:16:56 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1695, average loss: 284.4024
[09/25 22:16:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 12.50	
[09/25 22:16:56 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/25 22:17:03 visual_prompt]: Epoch 21 / 100: avg data time: 4.95e-02, avg batch time: 0.4978, average train loss: 275.8172
[09/25 22:17:04 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1693, average loss: 319.2090
[09/25 22:17:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.50	
[09/25 22:17:04 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/25 22:17:11 visual_prompt]: Epoch 22 / 100: avg data time: 5.25e-02, avg batch time: 0.5014, average train loss: 246.8881
[09/25 22:17:12 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1691, average loss: 254.0600
[09/25 22:17:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 22:17:12 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/25 22:17:19 visual_prompt]: Epoch 23 / 100: avg data time: 5.14e-02, avg batch time: 0.5011, average train loss: 211.9899
[09/25 22:17:20 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 265.1919
[09/25 22:17:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.00	
[09/25 22:17:20 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/25 22:17:27 visual_prompt]: Epoch 24 / 100: avg data time: 4.02e-02, avg batch time: 0.4896, average train loss: 208.9486
[09/25 22:17:28 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1691, average loss: 254.8405
[09/25 22:17:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/25 22:17:28 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/25 22:17:35 visual_prompt]: Epoch 25 / 100: avg data time: 5.32e-02, avg batch time: 0.5012, average train loss: 247.8831
[09/25 22:17:36 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1693, average loss: 245.7773
[09/25 22:17:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 22:17:36 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/25 22:17:43 visual_prompt]: Epoch 26 / 100: avg data time: 3.94e-02, avg batch time: 0.4898, average train loss: 198.9418
[09/25 22:17:44 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1695, average loss: 234.5391
[09/25 22:17:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 22:17:44 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/25 22:17:51 visual_prompt]: Epoch 27 / 100: avg data time: 4.86e-02, avg batch time: 0.4980, average train loss: 248.0711
[09/25 22:17:52 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 197.7989
[09/25 22:17:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.50	
[09/25 22:17:52 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/25 22:17:59 visual_prompt]: Epoch 28 / 100: avg data time: 4.37e-02, avg batch time: 0.4934, average train loss: 238.0686
[09/25 22:18:00 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1698, average loss: 228.1812
[09/25 22:18:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.50	
[09/25 22:18:00 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/25 22:18:07 visual_prompt]: Epoch 29 / 100: avg data time: 4.85e-02, avg batch time: 0.4980, average train loss: 253.5009
[09/25 22:18:08 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1700, average loss: 242.1795
[09/25 22:18:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/25 22:18:08 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/25 22:18:15 visual_prompt]: Epoch 30 / 100: avg data time: 4.20e-02, avg batch time: 0.4927, average train loss: 218.5456
[09/25 22:18:16 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1691, average loss: 213.3315
[09/25 22:18:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.00	
[09/25 22:18:16 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/25 22:18:23 visual_prompt]: Epoch 31 / 100: avg data time: 4.07e-02, avg batch time: 0.4904, average train loss: 216.9656
[09/25 22:18:24 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1695, average loss: 182.7756
[09/25 22:18:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.50	
[09/25 22:18:24 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/25 22:18:31 visual_prompt]: Epoch 32 / 100: avg data time: 5.16e-02, avg batch time: 0.5017, average train loss: 206.3723
[09/25 22:18:32 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1696, average loss: 222.8253
[09/25 22:18:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.50	
[09/25 22:18:32 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/25 22:18:39 visual_prompt]: Epoch 33 / 100: avg data time: 4.61e-02, avg batch time: 0.4957, average train loss: 169.3260
[09/25 22:18:40 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1698, average loss: 155.5119
[09/25 22:18:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 22:18:40 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/25 22:18:46 visual_prompt]: Epoch 34 / 100: avg data time: 3.71e-02, avg batch time: 0.4872, average train loss: 199.6602
[09/25 22:18:48 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 248.8761
[09/25 22:18:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/25 22:18:48 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/25 22:18:54 visual_prompt]: Epoch 35 / 100: avg data time: 4.83e-02, avg batch time: 0.4983, average train loss: 207.1840
[09/25 22:18:56 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1697, average loss: 182.5875
[09/25 22:18:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.00	
[09/25 22:18:56 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/25 22:19:03 visual_prompt]: Epoch 36 / 100: avg data time: 5.64e-02, avg batch time: 0.5050, average train loss: 204.7082
[09/25 22:19:04 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1700, average loss: 261.5254
[09/25 22:19:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/25 22:19:04 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/25 22:19:10 visual_prompt]: Epoch 37 / 100: avg data time: 4.46e-02, avg batch time: 0.4946, average train loss: 206.2028
[09/25 22:19:12 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1701, average loss: 203.7257
[09/25 22:19:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 22:19:12 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/25 22:19:18 visual_prompt]: Epoch 38 / 100: avg data time: 4.24e-02, avg batch time: 0.4910, average train loss: 222.9178
[09/25 22:19:20 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1698, average loss: 165.7409
[09/25 22:19:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.00	
[09/25 22:19:20 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/25 22:19:26 visual_prompt]: Epoch 39 / 100: avg data time: 3.73e-02, avg batch time: 0.4882, average train loss: 192.9214
[09/25 22:19:27 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1695, average loss: 148.8687
[09/25 22:19:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.50	
[09/25 22:19:27 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/25 22:19:34 visual_prompt]: Epoch 40 / 100: avg data time: 3.48e-02, avg batch time: 0.4861, average train loss: 181.6625
[09/25 22:19:35 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1697, average loss: 178.4505
[09/25 22:19:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 22:19:35 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/25 22:19:42 visual_prompt]: Epoch 41 / 100: avg data time: 3.58e-02, avg batch time: 0.4853, average train loss: 172.8952
[09/25 22:19:43 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1698, average loss: 212.6530
[09/25 22:19:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/25 22:19:43 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/25 22:19:50 visual_prompt]: Epoch 42 / 100: avg data time: 3.80e-02, avg batch time: 0.4867, average train loss: 170.7272
[09/25 22:19:51 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1690, average loss: 172.1413
[09/25 22:19:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/25 22:19:51 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/25 22:19:57 visual_prompt]: Epoch 43 / 100: avg data time: 3.82e-02, avg batch time: 0.4889, average train loss: 158.8898
[09/25 22:19:59 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1694, average loss: 171.6607
[09/25 22:19:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.00	
[09/25 22:19:59 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/25 22:20:05 visual_prompt]: Epoch 44 / 100: avg data time: 4.09e-02, avg batch time: 0.4906, average train loss: 189.2225
[09/25 22:20:07 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 171.4981
[09/25 22:20:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 22:20:07 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/25 22:20:13 visual_prompt]: Epoch 45 / 100: avg data time: 3.74e-02, avg batch time: 0.4868, average train loss: 171.4761
[09/25 22:20:14 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1696, average loss: 157.8787
[09/25 22:20:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.50	
[09/25 22:20:14 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/25 22:20:21 visual_prompt]: Epoch 46 / 100: avg data time: 4.40e-02, avg batch time: 0.4935, average train loss: 142.6213
[09/25 22:20:22 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1693, average loss: 152.5950
[09/25 22:20:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 22:20:22 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/25 22:20:29 visual_prompt]: Epoch 47 / 100: avg data time: 4.26e-02, avg batch time: 0.4926, average train loss: 167.0430
[09/25 22:20:30 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1698, average loss: 148.5839
[09/25 22:20:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 22:20:30 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/25 22:20:37 visual_prompt]: Epoch 48 / 100: avg data time: 3.63e-02, avg batch time: 0.4881, average train loss: 200.4248
[09/25 22:20:38 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1699, average loss: 145.4048
[09/25 22:20:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/25 22:20:38 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/25 22:20:45 visual_prompt]: Epoch 49 / 100: avg data time: 4.22e-02, avg batch time: 0.4924, average train loss: 262.5798
[09/25 22:20:46 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1700, average loss: 186.5366
[09/25 22:20:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 14.00	
[09/25 22:20:46 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/25 22:20:53 visual_prompt]: Epoch 50 / 100: avg data time: 3.56e-02, avg batch time: 0.4876, average train loss: 145.3884
[09/25 22:20:54 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1699, average loss: 167.7579
[09/25 22:20:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/25 22:20:54 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/25 22:21:01 visual_prompt]: Epoch 51 / 100: avg data time: 5.16e-02, avg batch time: 0.5004, average train loss: 152.9368
[09/25 22:21:02 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 111.8479
[09/25 22:21:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/25 22:21:02 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/25 22:21:08 visual_prompt]: Epoch 52 / 100: avg data time: 3.36e-02, avg batch time: 0.4836, average train loss: 129.3968
[09/25 22:21:10 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1694, average loss: 96.0952
[09/25 22:21:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 22:21:10 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/25 22:21:16 visual_prompt]: Epoch 53 / 100: avg data time: 4.20e-02, avg batch time: 0.4931, average train loss: 127.6144
[09/25 22:21:18 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1698, average loss: 134.4775
[09/25 22:21:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.50	
[09/25 22:21:18 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/25 22:21:24 visual_prompt]: Epoch 54 / 100: avg data time: 3.18e-02, avg batch time: 0.4843, average train loss: 138.2869
[09/25 22:21:26 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1693, average loss: 162.1505
[09/25 22:21:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 7.00	
[09/25 22:21:26 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/25 22:21:32 visual_prompt]: Epoch 55 / 100: avg data time: 3.89e-02, avg batch time: 0.4884, average train loss: 159.1524
[09/25 22:21:33 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1698, average loss: 134.4023
[09/25 22:21:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 9.00	
[09/25 22:21:33 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/25 22:21:40 visual_prompt]: Epoch 56 / 100: avg data time: 3.55e-02, avg batch time: 0.4855, average train loss: 133.4997
[09/25 22:21:41 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1688, average loss: 132.1188
[09/25 22:21:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 14.00	
[09/25 22:21:41 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/25 22:21:48 visual_prompt]: Epoch 57 / 100: avg data time: 3.79e-02, avg batch time: 0.4871, average train loss: 127.4021
[09/25 22:21:49 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1700, average loss: 127.4868
[09/25 22:21:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/25 22:21:49 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/25 22:21:56 visual_prompt]: Epoch 58 / 100: avg data time: 3.79e-02, avg batch time: 0.4869, average train loss: 131.4872
[09/25 22:21:57 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 100.8723
[09/25 22:21:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 22:21:57 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/25 22:22:04 visual_prompt]: Epoch 59 / 100: avg data time: 4.09e-02, avg batch time: 0.4901, average train loss: 116.3031
[09/25 22:22:05 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1696, average loss: 109.4538
[09/25 22:22:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 22:22:05 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/25 22:22:12 visual_prompt]: Epoch 60 / 100: avg data time: 3.79e-02, avg batch time: 0.4888, average train loss: 118.9846
[09/25 22:22:13 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1695, average loss: 119.5668
[09/25 22:22:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 22:22:13 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/25 22:22:20 visual_prompt]: Epoch 61 / 100: avg data time: 3.64e-02, avg batch time: 0.4870, average train loss: 123.5012
[09/25 22:22:21 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1696, average loss: 85.7753
[09/25 22:22:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 14.00	
[09/25 22:22:21 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/25 22:22:27 visual_prompt]: Epoch 62 / 100: avg data time: 3.88e-02, avg batch time: 0.4881, average train loss: 101.5443
[09/25 22:22:29 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1697, average loss: 86.7446
[09/25 22:22:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 13.50	
[09/25 22:22:29 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/25 22:22:35 visual_prompt]: Epoch 63 / 100: avg data time: 4.39e-02, avg batch time: 0.4951, average train loss: 82.6155
[09/25 22:22:37 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1701, average loss: 87.6837
[09/25 22:22:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 22:22:37 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/25 22:22:43 visual_prompt]: Epoch 64 / 100: avg data time: 5.17e-02, avg batch time: 0.5005, average train loss: 91.1609
[09/25 22:22:45 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1695, average loss: 75.0223
[09/25 22:22:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.00	
[09/25 22:22:45 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/25 22:22:51 visual_prompt]: Epoch 65 / 100: avg data time: 3.97e-02, avg batch time: 0.4901, average train loss: 87.2271
[09/25 22:22:53 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1697, average loss: 84.7722
[09/25 22:22:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.50	
[09/25 22:22:53 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/25 22:22:59 visual_prompt]: Epoch 66 / 100: avg data time: 4.51e-02, avg batch time: 0.4944, average train loss: 79.2738
[09/25 22:23:01 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1693, average loss: 62.7983
[09/25 22:23:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 22:23:01 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/25 22:23:07 visual_prompt]: Epoch 67 / 100: avg data time: 3.46e-02, avg batch time: 0.4859, average train loss: 73.4878
[09/25 22:23:09 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1691, average loss: 57.5521
[09/25 22:23:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/25 22:23:09 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/25 22:23:15 visual_prompt]: Epoch 68 / 100: avg data time: 3.40e-02, avg batch time: 0.4836, average train loss: 73.8393
[09/25 22:23:16 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1694, average loss: 57.9410
[09/25 22:23:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/25 22:23:16 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/25 22:23:23 visual_prompt]: Epoch 69 / 100: avg data time: 4.41e-02, avg batch time: 0.4932, average train loss: 58.7987
[09/25 22:23:24 visual_prompt]: Inference (val):avg data time: 1.88e-05, avg batch time: 0.1692, average loss: 60.2659
[09/25 22:23:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 14.00	
[09/25 22:23:24 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/25 22:23:31 visual_prompt]: Epoch 70 / 100: avg data time: 4.75e-02, avg batch time: 0.4961, average train loss: 55.7546
[09/25 22:23:33 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 56.2417
[09/25 22:23:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.50	
[09/25 22:23:33 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/25 22:23:39 visual_prompt]: Epoch 71 / 100: avg data time: 3.96e-02, avg batch time: 0.4890, average train loss: 55.6135
[09/25 22:23:41 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1694, average loss: 46.5846
[09/25 22:23:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.50	
[09/25 22:23:41 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/25 22:23:47 visual_prompt]: Epoch 72 / 100: avg data time: 5.04e-02, avg batch time: 0.4989, average train loss: 59.1535
[09/25 22:23:49 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1695, average loss: 60.9257
[09/25 22:23:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.50	
[09/25 22:23:49 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/25 22:23:55 visual_prompt]: Epoch 73 / 100: avg data time: 4.17e-02, avg batch time: 0.4921, average train loss: 55.9285
[09/25 22:23:56 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1693, average loss: 57.3957
[09/25 22:23:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.50	
[09/25 22:23:56 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/25 22:24:03 visual_prompt]: Epoch 74 / 100: avg data time: 3.91e-02, avg batch time: 0.4881, average train loss: 55.5453
[09/25 22:24:04 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1695, average loss: 40.9498
[09/25 22:24:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 13.00	
[09/25 22:24:04 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/25 22:24:11 visual_prompt]: Epoch 75 / 100: avg data time: 3.69e-02, avg batch time: 0.4861, average train loss: 40.1795
[09/25 22:24:12 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.1693, average loss: 39.9193
[09/25 22:24:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.00	
[09/25 22:24:12 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/25 22:24:19 visual_prompt]: Epoch 76 / 100: avg data time: 4.62e-02, avg batch time: 0.4957, average train loss: 33.7551
[09/25 22:24:20 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1694, average loss: 25.2830
[09/25 22:24:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.00	
[09/25 22:24:20 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/25 22:24:27 visual_prompt]: Epoch 77 / 100: avg data time: 4.68e-02, avg batch time: 0.4961, average train loss: 26.0830
[09/25 22:24:28 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1692, average loss: 23.5634
[09/25 22:24:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:24:28 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/25 22:24:35 visual_prompt]: Epoch 78 / 100: avg data time: 5.15e-02, avg batch time: 0.5007, average train loss: 23.8362
[09/25 22:24:36 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1697, average loss: 26.3042
[09/25 22:24:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/25 22:24:36 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/25 22:24:43 visual_prompt]: Epoch 79 / 100: avg data time: 5.06e-02, avg batch time: 0.5004, average train loss: 25.3552
[09/25 22:24:44 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1690, average loss: 14.5956
[09/25 22:24:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.50	
[09/25 22:24:44 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/25 22:24:51 visual_prompt]: Epoch 80 / 100: avg data time: 4.52e-02, avg batch time: 0.4946, average train loss: 25.2474
[09/25 22:24:52 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1694, average loss: 18.7666
[09/25 22:24:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.00	
[09/25 22:24:52 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/25 22:24:59 visual_prompt]: Epoch 81 / 100: avg data time: 5.12e-02, avg batch time: 0.4995, average train loss: 20.4555
[09/25 22:25:00 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1696, average loss: 17.7949
[09/25 22:25:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 22:25:00 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/25 22:25:07 visual_prompt]: Epoch 82 / 100: avg data time: 4.80e-02, avg batch time: 0.4970, average train loss: 14.9861
[09/25 22:25:08 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1693, average loss: 10.6734
[09/25 22:25:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/25 22:25:08 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/25 22:25:15 visual_prompt]: Epoch 83 / 100: avg data time: 3.91e-02, avg batch time: 0.4875, average train loss: 9.6804
[09/25 22:25:16 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1693, average loss: 6.3643
[09/25 22:25:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 22:25:16 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/25 22:25:23 visual_prompt]: Epoch 84 / 100: avg data time: 3.76e-02, avg batch time: 0.4877, average train loss: 5.5973
[09/25 22:25:24 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1695, average loss: 4.6942
[09/25 22:25:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 22:25:24 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/25 22:25:31 visual_prompt]: Epoch 85 / 100: avg data time: 5.02e-02, avg batch time: 0.4996, average train loss: 4.4890
[09/25 22:25:32 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1695, average loss: 4.4411
[09/25 22:25:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.50	
[09/25 22:25:32 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/25 22:25:39 visual_prompt]: Epoch 86 / 100: avg data time: 3.64e-02, avg batch time: 0.4858, average train loss: 4.3371
[09/25 22:25:40 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1697, average loss: 4.4741
[09/25 22:25:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 9.50	
[09/25 22:25:40 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/25 22:25:46 visual_prompt]: Epoch 87 / 100: avg data time: 4.21e-02, avg batch time: 0.4908, average train loss: 4.2376
[09/25 22:25:48 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1700, average loss: 4.1506
[09/25 22:25:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:25:48 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/25 22:25:54 visual_prompt]: Epoch 88 / 100: avg data time: 4.03e-02, avg batch time: 0.4905, average train loss: 4.1250
[09/25 22:25:56 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1696, average loss: 4.0478
[09/25 22:25:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 14.50	
[09/25 22:25:56 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/25 22:26:02 visual_prompt]: Epoch 89 / 100: avg data time: 5.00e-02, avg batch time: 0.4991, average train loss: 4.0590
[09/25 22:26:04 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1700, average loss: 4.2488
[09/25 22:26:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 22:26:04 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/25 22:26:10 visual_prompt]: Epoch 90 / 100: avg data time: 4.99e-02, avg batch time: 0.4988, average train loss: 4.0251
[09/25 22:26:12 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1697, average loss: 3.9537
[09/25 22:26:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 14.00	
[09/25 22:26:12 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/25 22:26:18 visual_prompt]: Epoch 91 / 100: avg data time: 3.74e-02, avg batch time: 0.4890, average train loss: 3.7921
[09/25 22:26:20 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1694, average loss: 4.0806
[09/25 22:26:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 14.00	
[09/25 22:26:20 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/25 22:26:26 visual_prompt]: Epoch 92 / 100: avg data time: 4.00e-02, avg batch time: 0.4897, average train loss: 3.6108
[09/25 22:26:27 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1698, average loss: 3.5964
[09/25 22:26:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.00	top5: 30.50	
[09/25 22:26:27 visual_prompt]: Best epoch 92: best metric: 0.070
[09/25 22:26:27 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/25 22:26:34 visual_prompt]: Epoch 93 / 100: avg data time: 5.13e-02, avg batch time: 0.5013, average train loss: 3.0847
[09/25 22:26:35 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1694, average loss: 3.2722
[09/25 22:26:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.50	top5: 43.00	
[09/25 22:26:35 visual_prompt]: Best epoch 93: best metric: 0.125
[09/25 22:26:35 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/25 22:26:42 visual_prompt]: Epoch 94 / 100: avg data time: 4.48e-02, avg batch time: 0.4936, average train loss: 2.3749
[09/25 22:26:43 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1692, average loss: 2.8441
[09/25 22:26:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 29.50	top5: 64.50	
[09/25 22:26:43 visual_prompt]: Best epoch 94: best metric: 0.295
[09/25 22:26:43 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/25 22:26:50 visual_prompt]: Epoch 95 / 100: avg data time: 4.56e-02, avg batch time: 0.4952, average train loss: 1.7644
[09/25 22:26:51 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1692, average loss: 2.2205
[09/25 22:26:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.00	top5: 75.00	
[09/25 22:26:51 visual_prompt]: Best epoch 95: best metric: 0.410
[09/25 22:26:51 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/25 22:26:58 visual_prompt]: Epoch 96 / 100: avg data time: 5.57e-02, avg batch time: 0.5044, average train loss: 1.1782
[09/25 22:26:59 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1694, average loss: 2.0213
[09/25 22:26:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.00	top5: 81.00	
[09/25 22:26:59 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/25 22:27:06 visual_prompt]: Epoch 97 / 100: avg data time: 5.25e-02, avg batch time: 0.5013, average train loss: 0.7836
[09/25 22:27:07 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1697, average loss: 1.9498
[09/25 22:27:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 82.50	
[09/25 22:27:07 visual_prompt]: Best epoch 97: best metric: 0.450
[09/25 22:27:07 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/25 22:27:14 visual_prompt]: Epoch 98 / 100: avg data time: 4.15e-02, avg batch time: 0.4916, average train loss: 0.5644
[09/25 22:27:15 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.8562
[09/25 22:27:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 82.50	
[09/25 22:27:15 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/25 22:27:22 visual_prompt]: Epoch 99 / 100: avg data time: 3.76e-02, avg batch time: 0.4878, average train loss: 0.4325
[09/25 22:27:23 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 1.8496
[09/25 22:27:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.00	top5: 84.50	
[09/25 22:27:23 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/25 22:27:30 visual_prompt]: Epoch 100 / 100: avg data time: 4.89e-02, avg batch time: 0.4996, average train loss: 0.3933
[09/25 22:27:31 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1695, average loss: 1.8449
[09/25 22:27:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 85.00	
[09/25 22:27:31 visual_prompt]: Rank of current process: 0. World size: 1
[09/25 22:27:31 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/25 22:27:31 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/25 22:27:31 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/25 22:27:31 visual_prompt]: Training with config:
[09/25 22:27:31 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/25 22:27:31 visual_prompt]: Loading training data...
[09/25 22:27:31 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 22:27:33 visual_prompt]: Number of images: 800
[09/25 22:27:33 visual_prompt]: Number of classes: 47 / 47
[09/25 22:27:33 visual_prompt]: Loading validation data...
[09/25 22:27:33 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 22:27:34 visual_prompt]: Number of images: 200
[09/25 22:27:34 visual_prompt]: Number of classes: 47 / 47
[09/25 22:27:34 visual_prompt]: Constructing models...
[09/25 22:27:37 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/25 22:27:37 visual_prompt]: tuned percent:0.576
[09/25 22:27:37 visual_prompt]: Device used for model: 0
[09/25 22:27:37 visual_prompt]: Setting up Evaluator...
[09/25 22:27:37 visual_prompt]: Setting up Trainer...
[09/25 22:27:37 visual_prompt]: 	Setting up the optimizer...
[09/25 22:27:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/25 22:27:43 visual_prompt]: Epoch 1 / 100: avg data time: 4.22e-02, avg batch time: 0.4888, average train loss: 3.9288
[09/25 22:27:45 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1688, average loss: 3.9045
[09/25 22:27:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 22:27:45 visual_prompt]: Best epoch 1: best metric: 0.040
[09/25 22:27:45 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/25 22:27:51 visual_prompt]: Epoch 2 / 100: avg data time: 4.11e-02, avg batch time: 0.4886, average train loss: 4.6122
[09/25 22:27:53 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1691, average loss: 4.8104
[09/25 22:27:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/25 22:27:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/25 22:27:59 visual_prompt]: Epoch 3 / 100: avg data time: 3.59e-02, avg batch time: 0.4856, average train loss: 10.0731
[09/25 22:28:00 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1693, average loss: 54.6165
[09/25 22:28:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.00	
[09/25 22:28:00 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/25 22:28:07 visual_prompt]: Epoch 4 / 100: avg data time: 4.91e-02, avg batch time: 0.4964, average train loss: 36.4463
[09/25 22:28:08 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 52.6274
[09/25 22:28:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.50	
[09/25 22:28:08 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/25 22:28:15 visual_prompt]: Epoch 5 / 100: avg data time: 4.12e-02, avg batch time: 0.4890, average train loss: 60.0212
[09/25 22:28:16 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1691, average loss: 63.8297
[09/25 22:28:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 14.50	
[09/25 22:28:16 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/25 22:28:23 visual_prompt]: Epoch 6 / 100: avg data time: 3.61e-02, avg batch time: 0.4836, average train loss: 94.2532
[09/25 22:28:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 92.0369
[09/25 22:28:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 14.50	
[09/25 22:28:24 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/25 22:28:31 visual_prompt]: Epoch 7 / 100: avg data time: 4.55e-02, avg batch time: 0.4936, average train loss: 108.2125
[09/25 22:28:32 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 134.2390
[09/25 22:28:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.50	
[09/25 22:28:32 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/25 22:28:39 visual_prompt]: Epoch 8 / 100: avg data time: 4.07e-02, avg batch time: 0.4890, average train loss: 158.8453
[09/25 22:28:40 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1693, average loss: 177.4697
[09/25 22:28:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.00	
[09/25 22:28:40 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/25 22:28:47 visual_prompt]: Epoch 9 / 100: avg data time: 4.02e-02, avg batch time: 0.4892, average train loss: 198.8626
[09/25 22:28:48 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 247.9014
[09/25 22:28:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 22:28:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/25 22:28:55 visual_prompt]: Epoch 10 / 100: avg data time: 3.64e-02, avg batch time: 0.4862, average train loss: 296.5069
[09/25 22:28:56 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1698, average loss: 245.1385
[09/25 22:28:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 22:28:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/25 22:29:02 visual_prompt]: Epoch 11 / 100: avg data time: 3.67e-02, avg batch time: 0.4881, average train loss: 229.8907
[09/25 22:29:04 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1694, average loss: 223.1814
[09/25 22:29:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.00	
[09/25 22:29:04 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/25 22:29:10 visual_prompt]: Epoch 12 / 100: avg data time: 5.09e-02, avg batch time: 0.4994, average train loss: 233.2144
[09/25 22:29:12 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1690, average loss: 220.3772
[09/25 22:29:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 10.50	
[09/25 22:29:12 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/25 22:29:18 visual_prompt]: Epoch 13 / 100: avg data time: 3.77e-02, avg batch time: 0.4863, average train loss: 223.6407
[09/25 22:29:20 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 211.3860
[09/25 22:29:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/25 22:29:20 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/25 22:29:26 visual_prompt]: Epoch 14 / 100: avg data time: 4.34e-02, avg batch time: 0.4917, average train loss: 404.1467
[09/25 22:29:28 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 355.5064
[09/25 22:29:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 22:29:28 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/25 22:29:34 visual_prompt]: Epoch 15 / 100: avg data time: 4.52e-02, avg batch time: 0.4937, average train loss: 354.9230
[09/25 22:29:36 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1695, average loss: 372.2650
[09/25 22:29:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 22:29:36 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/25 22:29:42 visual_prompt]: Epoch 16 / 100: avg data time: 3.55e-02, avg batch time: 0.4865, average train loss: 333.9029
[09/25 22:29:44 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1695, average loss: 298.5080
[09/25 22:29:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/25 22:29:44 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/25 22:29:50 visual_prompt]: Epoch 17 / 100: avg data time: 3.99e-02, avg batch time: 0.4911, average train loss: 377.5742
[09/25 22:29:52 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1695, average loss: 365.4316
[09/25 22:29:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.00	
[09/25 22:29:52 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/25 22:29:58 visual_prompt]: Epoch 18 / 100: avg data time: 4.02e-02, avg batch time: 0.4905, average train loss: 338.2100
[09/25 22:29:59 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1692, average loss: 323.2268
[09/25 22:29:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/25 22:29:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/25 22:30:06 visual_prompt]: Epoch 19 / 100: avg data time: 3.54e-02, avg batch time: 0.4897, average train loss: 300.3736
[09/25 22:30:07 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1695, average loss: 236.8563
[09/25 22:30:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.50	
[09/25 22:30:07 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/25 22:30:14 visual_prompt]: Epoch 20 / 100: avg data time: 4.53e-02, avg batch time: 0.4950, average train loss: 287.9716
[09/25 22:30:15 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1697, average loss: 255.9493
[09/25 22:30:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.50	
[09/25 22:30:15 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/25 22:30:22 visual_prompt]: Epoch 21 / 100: avg data time: 4.12e-02, avg batch time: 0.4909, average train loss: 262.1960
[09/25 22:30:23 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1757, average loss: 253.8002
[09/25 22:30:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 14.00	
[09/25 22:30:23 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/25 22:30:30 visual_prompt]: Epoch 22 / 100: avg data time: 3.82e-02, avg batch time: 0.4897, average train loss: 253.5235
[09/25 22:30:31 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1696, average loss: 277.5353
[09/25 22:30:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 22:30:31 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/25 22:30:38 visual_prompt]: Epoch 23 / 100: avg data time: 4.12e-02, avg batch time: 0.4914, average train loss: 208.2083
[09/25 22:30:39 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1691, average loss: 225.0246
[09/25 22:30:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.50	
[09/25 22:30:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/25 22:30:46 visual_prompt]: Epoch 24 / 100: avg data time: 4.45e-02, avg batch time: 0.4941, average train loss: 336.0908
[09/25 22:30:47 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1691, average loss: 455.2782
[09/25 22:30:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 22:30:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/25 22:30:54 visual_prompt]: Epoch 25 / 100: avg data time: 4.38e-02, avg batch time: 0.4928, average train loss: 498.1985
[09/25 22:30:55 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1688, average loss: 442.5841
[09/25 22:30:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.50	
[09/25 22:30:55 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/25 22:31:02 visual_prompt]: Epoch 26 / 100: avg data time: 3.59e-02, avg batch time: 0.4872, average train loss: 423.6194
[09/25 22:31:03 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1699, average loss: 422.8271
[09/25 22:31:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.00	
[09/25 22:31:03 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/25 22:31:10 visual_prompt]: Epoch 27 / 100: avg data time: 3.51e-02, avg batch time: 0.4847, average train loss: 312.5794
[09/25 22:31:11 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1695, average loss: 241.1287
[09/25 22:31:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.50	
[09/25 22:31:11 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/25 22:31:17 visual_prompt]: Epoch 28 / 100: avg data time: 3.39e-02, avg batch time: 0.4839, average train loss: 318.4073
[09/25 22:31:19 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 319.2567
[09/25 22:31:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/25 22:31:19 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/25 22:31:25 visual_prompt]: Epoch 29 / 100: avg data time: 4.51e-02, avg batch time: 0.4939, average train loss: 374.1012
[09/25 22:31:27 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1699, average loss: 621.5041
[09/25 22:31:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.50	
[09/25 22:31:27 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/25 22:31:33 visual_prompt]: Epoch 30 / 100: avg data time: 3.53e-02, avg batch time: 0.4867, average train loss: 295.9783
[09/25 22:31:35 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1698, average loss: 262.6272
[09/25 22:31:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/25 22:31:35 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/25 22:31:41 visual_prompt]: Epoch 31 / 100: avg data time: 3.82e-02, avg batch time: 0.4884, average train loss: 310.4917
[09/25 22:31:43 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1693, average loss: 301.7375
[09/25 22:31:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.50	
[09/25 22:31:43 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/25 22:31:49 visual_prompt]: Epoch 32 / 100: avg data time: 3.63e-02, avg batch time: 0.4851, average train loss: 252.8864
[09/25 22:31:50 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1692, average loss: 132.3701
[09/25 22:31:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 22:31:50 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/25 22:31:57 visual_prompt]: Epoch 33 / 100: avg data time: 4.96e-02, avg batch time: 0.4997, average train loss: 210.4832
[09/25 22:31:59 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1687, average loss: 269.9097
[09/25 22:31:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.50	
[09/25 22:31:59 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/25 22:32:05 visual_prompt]: Epoch 34 / 100: avg data time: 4.71e-02, avg batch time: 0.4958, average train loss: 272.3050
[09/25 22:32:07 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 270.9045
[09/25 22:32:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 22:32:07 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/25 22:32:13 visual_prompt]: Epoch 35 / 100: avg data time: 3.78e-02, avg batch time: 0.4865, average train loss: 283.9326
[09/25 22:32:14 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1696, average loss: 289.6794
[09/25 22:32:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.00	
[09/25 22:32:14 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/25 22:32:21 visual_prompt]: Epoch 36 / 100: avg data time: 3.55e-02, avg batch time: 0.4841, average train loss: 321.0643
[09/25 22:32:22 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1696, average loss: 211.0198
[09/25 22:32:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.50	
[09/25 22:32:22 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/25 22:32:29 visual_prompt]: Epoch 37 / 100: avg data time: 3.88e-02, avg batch time: 0.4892, average train loss: 358.8474
[09/25 22:32:30 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1699, average loss: 319.2070
[09/25 22:32:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.00	
[09/25 22:32:30 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/25 22:32:37 visual_prompt]: Epoch 38 / 100: avg data time: 4.94e-02, avg batch time: 0.4989, average train loss: 478.2271
[09/25 22:32:38 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 447.8563
[09/25 22:32:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/25 22:32:38 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/25 22:32:45 visual_prompt]: Epoch 39 / 100: avg data time: 3.68e-02, avg batch time: 0.4869, average train loss: 420.5359
[09/25 22:32:46 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 318.5198
[09/25 22:32:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 15.50	
[09/25 22:32:46 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/25 22:32:53 visual_prompt]: Epoch 40 / 100: avg data time: 4.16e-02, avg batch time: 0.4920, average train loss: 273.2160
[09/25 22:32:54 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 280.0907
[09/25 22:32:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 22:32:54 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/25 22:33:01 visual_prompt]: Epoch 41 / 100: avg data time: 4.71e-02, avg batch time: 0.4967, average train loss: 318.5793
[09/25 22:33:02 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1698, average loss: 325.5268
[09/25 22:33:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.50	
[09/25 22:33:02 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/25 22:33:09 visual_prompt]: Epoch 42 / 100: avg data time: 5.00e-02, avg batch time: 0.4994, average train loss: 297.6927
[09/25 22:33:10 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1697, average loss: 223.0703
[09/25 22:33:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.50	
[09/25 22:33:10 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/25 22:33:17 visual_prompt]: Epoch 43 / 100: avg data time: 3.66e-02, avg batch time: 0.4865, average train loss: 253.2910
[09/25 22:33:18 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1693, average loss: 257.0456
[09/25 22:33:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.50	
[09/25 22:33:18 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/25 22:33:25 visual_prompt]: Epoch 44 / 100: avg data time: 4.27e-02, avg batch time: 0.4931, average train loss: 218.8686
[09/25 22:33:26 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1695, average loss: 238.2723
[09/25 22:33:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.50	
[09/25 22:33:26 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/25 22:33:33 visual_prompt]: Epoch 45 / 100: avg data time: 5.04e-02, avg batch time: 0.4986, average train loss: 251.8250
[09/25 22:33:34 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1695, average loss: 194.8826
[09/25 22:33:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/25 22:33:34 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/25 22:33:41 visual_prompt]: Epoch 46 / 100: avg data time: 3.28e-02, avg batch time: 0.4824, average train loss: 221.7365
[09/25 22:33:42 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1695, average loss: 206.2406
[09/25 22:33:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:33:42 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/25 22:33:49 visual_prompt]: Epoch 47 / 100: avg data time: 5.19e-02, avg batch time: 0.5002, average train loss: 206.1557
[09/25 22:33:50 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1696, average loss: 180.4415
[09/25 22:33:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 22:33:50 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/25 22:33:57 visual_prompt]: Epoch 48 / 100: avg data time: 5.01e-02, avg batch time: 0.4985, average train loss: 182.7220
[09/25 22:33:58 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1700, average loss: 168.4548
[09/25 22:33:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.50	
[09/25 22:33:58 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/25 22:34:05 visual_prompt]: Epoch 49 / 100: avg data time: 4.68e-02, avg batch time: 0.4955, average train loss: 178.1206
[09/25 22:34:06 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1696, average loss: 165.3295
[09/25 22:34:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 22:34:06 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/25 22:34:13 visual_prompt]: Epoch 50 / 100: avg data time: 5.18e-02, avg batch time: 0.5005, average train loss: 158.6889
[09/25 22:34:14 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1695, average loss: 126.2772
[09/25 22:34:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.00	
[09/25 22:34:14 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/25 22:34:21 visual_prompt]: Epoch 51 / 100: avg data time: 4.85e-02, avg batch time: 0.4981, average train loss: 157.6125
[09/25 22:34:22 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1695, average loss: 157.5255
[09/25 22:34:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/25 22:34:22 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/25 22:34:29 visual_prompt]: Epoch 52 / 100: avg data time: 3.79e-02, avg batch time: 0.4880, average train loss: 153.3602
[09/25 22:34:30 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1698, average loss: 102.5108
[09/25 22:34:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/25 22:34:30 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/25 22:34:37 visual_prompt]: Epoch 53 / 100: avg data time: 4.01e-02, avg batch time: 0.4888, average train loss: 159.5789
[09/25 22:34:38 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1694, average loss: 267.1973
[09/25 22:34:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 7.50	
[09/25 22:34:38 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/25 22:34:45 visual_prompt]: Epoch 54 / 100: avg data time: 4.79e-02, avg batch time: 0.4971, average train loss: 202.2391
[09/25 22:34:46 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 188.4913
[09/25 22:34:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.00	
[09/25 22:34:46 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/25 22:34:53 visual_prompt]: Epoch 55 / 100: avg data time: 5.12e-02, avg batch time: 0.5013, average train loss: 169.0498
[09/25 22:34:54 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1697, average loss: 151.8840
[09/25 22:34:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.00	
[09/25 22:34:54 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/25 22:35:01 visual_prompt]: Epoch 56 / 100: avg data time: 3.64e-02, avg batch time: 0.4850, average train loss: 153.4486
[09/25 22:35:02 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1694, average loss: 144.9359
[09/25 22:35:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 22:35:02 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/25 22:35:09 visual_prompt]: Epoch 57 / 100: avg data time: 4.91e-02, avg batch time: 0.4978, average train loss: 142.3225
[09/25 22:35:10 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1694, average loss: 148.2914
[09/25 22:35:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.00	
[09/25 22:35:10 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/25 22:35:17 visual_prompt]: Epoch 58 / 100: avg data time: 4.08e-02, avg batch time: 0.4917, average train loss: 151.6409
[09/25 22:35:18 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1689, average loss: 132.1424
[09/25 22:35:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 22:35:18 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/25 22:35:25 visual_prompt]: Epoch 59 / 100: avg data time: 4.47e-02, avg batch time: 0.4941, average train loss: 114.5634
[09/25 22:35:26 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 96.5956
[09/25 22:35:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/25 22:35:26 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/25 22:35:33 visual_prompt]: Epoch 60 / 100: avg data time: 4.83e-02, avg batch time: 0.4970, average train loss: 101.2492
[09/25 22:35:34 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.1696, average loss: 109.4221
[09/25 22:35:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/25 22:35:34 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/25 22:35:41 visual_prompt]: Epoch 61 / 100: avg data time: 5.03e-02, avg batch time: 0.4983, average train loss: 94.5343
[09/25 22:35:42 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 97.4463
[09/25 22:35:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/25 22:35:42 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/25 22:35:49 visual_prompt]: Epoch 62 / 100: avg data time: 4.45e-02, avg batch time: 0.4937, average train loss: 100.1860
[09/25 22:35:50 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1694, average loss: 80.8071
[09/25 22:35:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:35:50 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/25 22:35:57 visual_prompt]: Epoch 63 / 100: avg data time: 5.37e-02, avg batch time: 0.5029, average train loss: 93.3707
[09/25 22:35:58 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1689, average loss: 97.4550
[09/25 22:35:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 10.50	
[09/25 22:35:58 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/25 22:36:05 visual_prompt]: Epoch 64 / 100: avg data time: 4.93e-02, avg batch time: 0.4981, average train loss: 94.0217
[09/25 22:36:06 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1697, average loss: 86.1250
[09/25 22:36:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 22:36:06 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/25 22:36:12 visual_prompt]: Epoch 65 / 100: avg data time: 3.50e-02, avg batch time: 0.4847, average train loss: 92.2742
[09/25 22:36:14 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1691, average loss: 92.7258
[09/25 22:36:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.00	
[09/25 22:36:14 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/25 22:36:20 visual_prompt]: Epoch 66 / 100: avg data time: 4.84e-02, avg batch time: 0.4979, average train loss: 83.8672
[09/25 22:36:22 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1697, average loss: 86.8655
[09/25 22:36:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/25 22:36:22 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/25 22:36:29 visual_prompt]: Epoch 67 / 100: avg data time: 4.83e-02, avg batch time: 0.4981, average train loss: 92.1769
[09/25 22:36:30 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1696, average loss: 92.4533
[09/25 22:36:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:36:30 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/25 22:36:37 visual_prompt]: Epoch 68 / 100: avg data time: 5.18e-02, avg batch time: 0.5008, average train loss: 94.4472
[09/25 22:36:38 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1698, average loss: 74.2668
[09/25 22:36:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/25 22:36:38 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/25 22:36:45 visual_prompt]: Epoch 69 / 100: avg data time: 4.89e-02, avg batch time: 0.4981, average train loss: 68.5790
[09/25 22:36:46 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1699, average loss: 73.7292
[09/25 22:36:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 22:36:46 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/25 22:36:53 visual_prompt]: Epoch 70 / 100: avg data time: 3.66e-02, avg batch time: 0.4865, average train loss: 71.0169
[09/25 22:36:54 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1697, average loss: 56.6102
[09/25 22:36:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/25 22:36:54 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/25 22:37:01 visual_prompt]: Epoch 71 / 100: avg data time: 4.86e-02, avg batch time: 0.4986, average train loss: 48.0412
[09/25 22:37:02 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1695, average loss: 36.8275
[09/25 22:37:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 22:37:02 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/25 22:37:08 visual_prompt]: Epoch 72 / 100: avg data time: 4.76e-02, avg batch time: 0.4970, average train loss: 38.9008
[09/25 22:37:10 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 38.1825
[09/25 22:37:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/25 22:37:10 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/25 22:37:17 visual_prompt]: Epoch 73 / 100: avg data time: 5.27e-02, avg batch time: 0.5018, average train loss: 30.7206
[09/25 22:37:18 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1694, average loss: 34.3549
[09/25 22:37:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 22:37:18 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/25 22:37:25 visual_prompt]: Epoch 74 / 100: avg data time: 5.21e-02, avg batch time: 0.5009, average train loss: 27.5299
[09/25 22:37:26 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1697, average loss: 26.0112
[09/25 22:37:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 13.50	
[09/25 22:37:26 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/25 22:37:33 visual_prompt]: Epoch 75 / 100: avg data time: 4.76e-02, avg batch time: 0.4972, average train loss: 17.9109
[09/25 22:37:34 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1693, average loss: 13.6242
[09/25 22:37:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/25 22:37:34 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/25 22:37:41 visual_prompt]: Epoch 76 / 100: avg data time: 4.80e-02, avg batch time: 0.4975, average train loss: 12.8757
[09/25 22:37:42 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1695, average loss: 14.2304
[09/25 22:37:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.00	
[09/25 22:37:42 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/25 22:37:49 visual_prompt]: Epoch 77 / 100: avg data time: 4.82e-02, avg batch time: 0.4983, average train loss: 11.2111
[09/25 22:37:50 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.1700, average loss: 9.4593
[09/25 22:37:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.50	
[09/25 22:37:50 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/25 22:37:57 visual_prompt]: Epoch 78 / 100: avg data time: 4.92e-02, avg batch time: 0.4977, average train loss: 6.9657
[09/25 22:37:58 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1696, average loss: 6.2241
[09/25 22:37:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.50	
[09/25 22:37:58 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/25 22:38:05 visual_prompt]: Epoch 79 / 100: avg data time: 5.62e-02, avg batch time: 0.5048, average train loss: 5.2130
[09/25 22:38:06 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1697, average loss: 4.8272
[09/25 22:38:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 13.50	
[09/25 22:38:06 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/25 22:38:13 visual_prompt]: Epoch 80 / 100: avg data time: 4.77e-02, avg batch time: 0.4972, average train loss: 4.5825
[09/25 22:38:14 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 4.5747
[09/25 22:38:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 9.50	
[09/25 22:38:14 visual_prompt]: Best epoch 80: best metric: 0.045
[09/25 22:38:14 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/25 22:38:21 visual_prompt]: Epoch 81 / 100: avg data time: 4.28e-02, avg batch time: 0.4926, average train loss: 4.4047
[09/25 22:38:22 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1700, average loss: 4.4302
[09/25 22:38:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 14.50	
[09/25 22:38:22 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/25 22:38:29 visual_prompt]: Epoch 82 / 100: avg data time: 5.07e-02, avg batch time: 0.4997, average train loss: 4.2054
[09/25 22:38:30 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1694, average loss: 4.2932
[09/25 22:38:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 17.50	
[09/25 22:38:30 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/25 22:38:37 visual_prompt]: Epoch 83 / 100: avg data time: 4.69e-02, avg batch time: 0.4964, average train loss: 4.2060
[09/25 22:38:38 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 4.1132
[09/25 22:38:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 16.00	
[09/25 22:38:38 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/25 22:38:45 visual_prompt]: Epoch 84 / 100: avg data time: 5.02e-02, avg batch time: 0.4990, average train loss: 3.9779
[09/25 22:38:46 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1693, average loss: 3.8889
[09/25 22:38:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 18.50	
[09/25 22:38:46 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/25 22:38:53 visual_prompt]: Epoch 85 / 100: avg data time: 5.50e-02, avg batch time: 0.5050, average train loss: 3.8498
[09/25 22:38:54 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 3.8008
[09/25 22:38:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.00	top5: 23.00	
[09/25 22:38:54 visual_prompt]: Best epoch 85: best metric: 0.070
[09/25 22:38:54 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/25 22:39:01 visual_prompt]: Epoch 86 / 100: avg data time: 4.88e-02, avg batch time: 0.4976, average train loss: 3.6527
[09/25 22:39:03 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 3.9450
[09/25 22:39:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 17.50	
[09/25 22:39:03 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/25 22:39:09 visual_prompt]: Epoch 87 / 100: avg data time: 4.91e-02, avg batch time: 0.4988, average train loss: 3.5911
[09/25 22:39:11 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1691, average loss: 3.6732
[09/25 22:39:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 24.00	
[09/25 22:39:11 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/25 22:39:17 visual_prompt]: Epoch 88 / 100: avg data time: 5.23e-02, avg batch time: 0.5021, average train loss: 3.3987
[09/25 22:39:19 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1695, average loss: 3.6620
[09/25 22:39:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 23.00	
[09/25 22:39:19 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/25 22:39:25 visual_prompt]: Epoch 89 / 100: avg data time: 4.00e-02, avg batch time: 0.4918, average train loss: 3.2808
[09/25 22:39:27 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1697, average loss: 3.4913
[09/25 22:39:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 34.00	
[09/25 22:39:27 visual_prompt]: Best epoch 89: best metric: 0.095
[09/25 22:39:27 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/25 22:39:33 visual_prompt]: Epoch 90 / 100: avg data time: 3.74e-02, avg batch time: 0.4882, average train loss: 3.1265
[09/25 22:39:35 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1696, average loss: 3.4273
[09/25 22:39:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 35.00	
[09/25 22:39:35 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/25 22:39:41 visual_prompt]: Epoch 91 / 100: avg data time: 4.85e-02, avg batch time: 0.4976, average train loss: 3.0145
[09/25 22:39:43 visual_prompt]: Inference (val):avg data time: 4.45e-05, avg batch time: 0.1694, average loss: 3.3191
[09/25 22:39:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.50	top5: 39.50	
[09/25 22:39:43 visual_prompt]: Best epoch 91: best metric: 0.105
[09/25 22:39:43 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/25 22:39:49 visual_prompt]: Epoch 92 / 100: avg data time: 4.93e-02, avg batch time: 0.4978, average train loss: 2.8655
[09/25 22:39:51 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1698, average loss: 3.1793
[09/25 22:39:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 19.00	top5: 48.50	
[09/25 22:39:51 visual_prompt]: Best epoch 92: best metric: 0.190
[09/25 22:39:51 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/25 22:39:57 visual_prompt]: Epoch 93 / 100: avg data time: 5.28e-02, avg batch time: 0.5020, average train loss: 2.7511
[09/25 22:39:59 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1696, average loss: 3.1625
[09/25 22:39:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 19.50	top5: 49.50	
[09/25 22:39:59 visual_prompt]: Best epoch 93: best metric: 0.195
[09/25 22:39:59 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/25 22:40:05 visual_prompt]: Epoch 94 / 100: avg data time: 5.50e-02, avg batch time: 0.5037, average train loss: 2.6453
[09/25 22:40:07 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 3.0447
[09/25 22:40:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.00	top5: 50.00	
[09/25 22:40:07 visual_prompt]: Best epoch 94: best metric: 0.230
[09/25 22:40:07 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/25 22:40:14 visual_prompt]: Epoch 95 / 100: avg data time: 4.51e-02, avg batch time: 0.4947, average train loss: 2.5382
[09/25 22:40:15 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1695, average loss: 3.0488
[09/25 22:40:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 51.50	
[09/25 22:40:15 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/25 22:40:22 visual_prompt]: Epoch 96 / 100: avg data time: 4.82e-02, avg batch time: 0.4974, average train loss: 2.4494
[09/25 22:40:23 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 3.0031
[09/25 22:40:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 20.50	top5: 46.50	
[09/25 22:40:23 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/25 22:40:30 visual_prompt]: Epoch 97 / 100: avg data time: 3.81e-02, avg batch time: 0.4882, average train loss: 2.3986
[09/25 22:40:31 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 2.9835
[09/25 22:40:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.50	top5: 50.50	
[09/25 22:40:31 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/25 22:40:38 visual_prompt]: Epoch 98 / 100: avg data time: 4.34e-02, avg batch time: 0.4935, average train loss: 2.3260
[09/25 22:40:39 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1698, average loss: 2.9496
[09/25 22:40:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 51.50	
[09/25 22:40:39 visual_prompt]: Best epoch 98: best metric: 0.245
[09/25 22:40:39 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/25 22:40:46 visual_prompt]: Epoch 99 / 100: avg data time: 4.79e-02, avg batch time: 0.4971, average train loss: 2.2829
[09/25 22:40:47 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1698, average loss: 2.9548
[09/25 22:40:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.00	top5: 51.50	
[09/25 22:40:47 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/25 22:40:54 visual_prompt]: Epoch 100 / 100: avg data time: 5.98e-02, avg batch time: 0.5084, average train loss: 2.2762
[09/25 22:40:55 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 2.9523
[09/25 22:40:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.00	top5: 51.50	
[09/25 22:40:55 visual_prompt]: Rank of current process: 0. World size: 1
[09/25 22:40:55 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/25 22:40:55 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/25 22:40:55 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/25 22:40:55 visual_prompt]: Training with config:
[09/25 22:40:55 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/25 22:40:55 visual_prompt]: Loading training data...
[09/25 22:40:55 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 22:40:57 visual_prompt]: Number of images: 800
[09/25 22:40:57 visual_prompt]: Number of classes: 47 / 47
[09/25 22:40:57 visual_prompt]: Loading validation data...
[09/25 22:40:57 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 22:40:58 visual_prompt]: Number of images: 200
[09/25 22:40:58 visual_prompt]: Number of classes: 47 / 47
[09/25 22:40:58 visual_prompt]: Constructing models...
[09/25 22:41:00 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/25 22:41:00 visual_prompt]: tuned percent:0.576
[09/25 22:41:00 visual_prompt]: Device used for model: 0
[09/25 22:41:00 visual_prompt]: Setting up Evaluator...
[09/25 22:41:00 visual_prompt]: Setting up Trainer...
[09/25 22:41:00 visual_prompt]: 	Setting up the optimizer...
[09/25 22:41:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/25 22:41:07 visual_prompt]: Epoch 1 / 100: avg data time: 4.02e-02, avg batch time: 0.4878, average train loss: 3.9338
[09/25 22:41:08 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1690, average loss: 3.9045
[09/25 22:41:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 22:41:08 visual_prompt]: Best epoch 1: best metric: 0.040
[09/25 22:41:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/25 22:41:15 visual_prompt]: Epoch 2 / 100: avg data time: 4.78e-02, avg batch time: 0.4950, average train loss: 5.3391
[09/25 22:41:16 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1689, average loss: 7.3862
[09/25 22:41:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/25 22:41:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/25 22:41:23 visual_prompt]: Epoch 3 / 100: avg data time: 5.05e-02, avg batch time: 0.4981, average train loss: 13.3620
[09/25 22:41:24 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 25.5029
[09/25 22:41:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.00	
[09/25 22:41:24 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/25 22:41:31 visual_prompt]: Epoch 4 / 100: avg data time: 3.46e-02, avg batch time: 0.4818, average train loss: 41.4667
[09/25 22:41:32 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 56.0242
[09/25 22:41:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 16.50	
[09/25 22:41:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/25 22:41:39 visual_prompt]: Epoch 5 / 100: avg data time: 4.74e-02, avg batch time: 0.4950, average train loss: 103.2947
[09/25 22:41:40 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 131.3539
[09/25 22:41:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 22:41:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/25 22:41:47 visual_prompt]: Epoch 6 / 100: avg data time: 5.32e-02, avg batch time: 0.5017, average train loss: 187.2815
[09/25 22:41:48 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1694, average loss: 228.8713
[09/25 22:41:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.50	
[09/25 22:41:48 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/25 22:41:55 visual_prompt]: Epoch 7 / 100: avg data time: 5.10e-02, avg batch time: 0.4983, average train loss: 296.5908
[09/25 22:41:56 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1694, average loss: 309.2310
[09/25 22:41:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 10.00	
[09/25 22:41:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/25 22:42:03 visual_prompt]: Epoch 8 / 100: avg data time: 4.40e-02, avg batch time: 0.4923, average train loss: 381.2249
[09/25 22:42:04 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1691, average loss: 171.7732
[09/25 22:42:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 13.50	
[09/25 22:42:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/25 22:42:11 visual_prompt]: Epoch 9 / 100: avg data time: 4.16e-02, avg batch time: 0.4898, average train loss: 268.3932
[09/25 22:42:12 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 283.0117
[09/25 22:42:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 14.00	
[09/25 22:42:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/25 22:42:19 visual_prompt]: Epoch 10 / 100: avg data time: 5.25e-02, avg batch time: 0.5005, average train loss: 407.7447
[09/25 22:42:20 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 437.2904
[09/25 22:42:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 22:42:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/25 22:42:27 visual_prompt]: Epoch 11 / 100: avg data time: 5.37e-02, avg batch time: 0.5023, average train loss: 391.2088
[09/25 22:42:29 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 327.6452
[09/25 22:42:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 13.00	
[09/25 22:42:29 visual_prompt]: Best epoch 11: best metric: 0.050
[09/25 22:42:29 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/25 22:42:35 visual_prompt]: Epoch 12 / 100: avg data time: 4.65e-02, avg batch time: 0.4948, average train loss: 345.9123
[09/25 22:42:37 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1695, average loss: 358.8102
[09/25 22:42:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 22:42:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/25 22:42:43 visual_prompt]: Epoch 13 / 100: avg data time: 5.00e-02, avg batch time: 0.4983, average train loss: 330.5061
[09/25 22:42:45 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1695, average loss: 272.6457
[09/25 22:42:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 22:42:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/25 22:42:51 visual_prompt]: Epoch 14 / 100: avg data time: 3.94e-02, avg batch time: 0.4896, average train loss: 290.3290
[09/25 22:42:53 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 229.1073
[09/25 22:42:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 22:42:53 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/25 22:42:59 visual_prompt]: Epoch 15 / 100: avg data time: 3.90e-02, avg batch time: 0.4903, average train loss: 215.0627
[09/25 22:43:01 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1698, average loss: 234.0197
[09/25 22:43:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/25 22:43:01 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/25 22:43:07 visual_prompt]: Epoch 16 / 100: avg data time: 5.36e-02, avg batch time: 0.5031, average train loss: 205.9163
[09/25 22:43:09 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1699, average loss: 202.1153
[09/25 22:43:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/25 22:43:09 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/25 22:43:15 visual_prompt]: Epoch 17 / 100: avg data time: 4.99e-02, avg batch time: 0.4998, average train loss: 180.9348
[09/25 22:43:17 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1696, average loss: 158.6078
[09/25 22:43:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.00	
[09/25 22:43:17 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/25 22:43:24 visual_prompt]: Epoch 18 / 100: avg data time: 5.34e-02, avg batch time: 0.5022, average train loss: 157.3238
[09/25 22:43:25 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 179.1790
[09/25 22:43:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:43:25 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/25 22:43:32 visual_prompt]: Epoch 19 / 100: avg data time: 4.53e-02, avg batch time: 0.4948, average train loss: 152.3558
[09/25 22:43:33 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1697, average loss: 152.2071
[09/25 22:43:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 16.00	
[09/25 22:43:33 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/25 22:43:40 visual_prompt]: Epoch 20 / 100: avg data time: 4.65e-02, avg batch time: 0.4955, average train loss: 140.0704
[09/25 22:43:41 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1696, average loss: 175.7422
[09/25 22:43:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/25 22:43:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/25 22:43:48 visual_prompt]: Epoch 21 / 100: avg data time: 3.89e-02, avg batch time: 0.4896, average train loss: 159.2066
[09/25 22:43:49 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 155.0609
[09/25 22:43:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.50	top5: 13.50	
[09/25 22:43:49 visual_prompt]: Best epoch 21: best metric: 0.055
[09/25 22:43:49 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/25 22:43:56 visual_prompt]: Epoch 22 / 100: avg data time: 5.53e-02, avg batch time: 0.5038, average train loss: 142.1770
[09/25 22:43:57 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1695, average loss: 141.9319
[09/25 22:43:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.50	
[09/25 22:43:57 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/25 22:44:04 visual_prompt]: Epoch 23 / 100: avg data time: 5.47e-02, avg batch time: 0.5042, average train loss: 119.9383
[09/25 22:44:05 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1697, average loss: 129.8730
[09/25 22:44:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.00	
[09/25 22:44:05 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/25 22:44:12 visual_prompt]: Epoch 24 / 100: avg data time: 3.81e-02, avg batch time: 0.4888, average train loss: 108.0963
[09/25 22:44:13 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1698, average loss: 94.2367
[09/25 22:44:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 15.50	
[09/25 22:44:13 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/25 22:44:20 visual_prompt]: Epoch 25 / 100: avg data time: 4.86e-02, avg batch time: 0.4980, average train loss: 85.1016
[09/25 22:44:21 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1696, average loss: 81.2612
[09/25 22:44:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 14.50	
[09/25 22:44:21 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/25 22:44:28 visual_prompt]: Epoch 26 / 100: avg data time: 3.64e-02, avg batch time: 0.4868, average train loss: 69.9347
[09/25 22:44:29 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1696, average loss: 72.5379
[09/25 22:44:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.50	top5: 17.50	
[09/25 22:44:29 visual_prompt]: Best epoch 26: best metric: 0.075
[09/25 22:44:29 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/25 22:44:36 visual_prompt]: Epoch 27 / 100: avg data time: 5.15e-02, avg batch time: 0.5014, average train loss: 82.5759
[09/25 22:44:37 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1697, average loss: 77.9385
[09/25 22:44:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 15.50	
[09/25 22:44:37 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/25 22:44:44 visual_prompt]: Epoch 28 / 100: avg data time: 4.91e-02, avg batch time: 0.4976, average train loss: 72.1905
[09/25 22:44:45 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 65.6868
[09/25 22:44:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 16.50	
[09/25 22:44:45 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/25 22:44:52 visual_prompt]: Epoch 29 / 100: avg data time: 5.22e-02, avg batch time: 0.5016, average train loss: 53.0997
[09/25 22:44:53 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1699, average loss: 68.5039
[09/25 22:44:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 18.50	
[09/25 22:44:53 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/25 22:45:00 visual_prompt]: Epoch 30 / 100: avg data time: 4.80e-02, avg batch time: 0.4975, average train loss: 52.6702
[09/25 22:45:01 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1695, average loss: 45.5671
[09/25 22:45:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 24.00	
[09/25 22:45:01 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/25 22:45:08 visual_prompt]: Epoch 31 / 100: avg data time: 5.18e-02, avg batch time: 0.5009, average train loss: 45.3782
[09/25 22:45:09 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 33.0369
[09/25 22:45:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.50	top5: 20.50	
[09/25 22:45:09 visual_prompt]: Best epoch 31: best metric: 0.085
[09/25 22:45:09 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/25 22:45:16 visual_prompt]: Epoch 32 / 100: avg data time: 4.41e-02, avg batch time: 0.4934, average train loss: 33.6173
[09/25 22:45:17 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1698, average loss: 39.4449
[09/25 22:45:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.50	top5: 13.00	
[09/25 22:45:17 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/25 22:45:24 visual_prompt]: Epoch 33 / 100: avg data time: 3.43e-02, avg batch time: 0.4859, average train loss: 33.6366
[09/25 22:45:25 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1697, average loss: 34.0170
[09/25 22:45:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 18.50	
[09/25 22:45:25 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/25 22:45:32 visual_prompt]: Epoch 34 / 100: avg data time: 5.09e-02, avg batch time: 0.5002, average train loss: 27.1948
[09/25 22:45:33 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1699, average loss: 38.1633
[09/25 22:45:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.00	top5: 21.50	
[09/25 22:45:33 visual_prompt]: Best epoch 34: best metric: 0.090
[09/25 22:45:33 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/25 22:45:40 visual_prompt]: Epoch 35 / 100: avg data time: 4.96e-02, avg batch time: 0.4991, average train loss: 30.6051
[09/25 22:45:42 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1695, average loss: 32.4682
[09/25 22:45:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 23.50	
[09/25 22:45:42 visual_prompt]: Best epoch 35: best metric: 0.110
[09/25 22:45:42 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/25 22:45:48 visual_prompt]: Epoch 36 / 100: avg data time: 5.14e-02, avg batch time: 0.4999, average train loss: 27.3398
[09/25 22:45:50 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1704, average loss: 24.8542
[09/25 22:45:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 27.00	
[09/25 22:45:50 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/25 22:45:56 visual_prompt]: Epoch 37 / 100: avg data time: 3.90e-02, avg batch time: 0.4894, average train loss: 20.7171
[09/25 22:45:58 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1695, average loss: 29.9071
[09/25 22:45:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 32.50	
[09/25 22:45:58 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/25 22:46:05 visual_prompt]: Epoch 38 / 100: avg data time: 5.27e-02, avg batch time: 0.5011, average train loss: 19.9879
[09/25 22:46:06 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1694, average loss: 26.3956
[09/25 22:46:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.50	top5: 31.50	
[09/25 22:46:06 visual_prompt]: Best epoch 38: best metric: 0.135
[09/25 22:46:06 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/25 22:46:13 visual_prompt]: Epoch 39 / 100: avg data time: 5.06e-02, avg batch time: 0.4998, average train loss: 16.0873
[09/25 22:46:14 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1694, average loss: 16.0806
[09/25 22:46:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.00	top5: 32.50	
[09/25 22:46:14 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/25 22:46:21 visual_prompt]: Epoch 40 / 100: avg data time: 4.38e-02, avg batch time: 0.4935, average train loss: 12.8687
[09/25 22:46:22 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1695, average loss: 20.8038
[09/25 22:46:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.50	top5: 26.00	
[09/25 22:46:22 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/25 22:46:29 visual_prompt]: Epoch 41 / 100: avg data time: 4.31e-02, avg batch time: 0.4929, average train loss: 12.3158
[09/25 22:46:30 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1695, average loss: 14.7982
[09/25 22:46:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 37.50	
[09/25 22:46:30 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/25 22:46:37 visual_prompt]: Epoch 42 / 100: avg data time: 4.34e-02, avg batch time: 0.4927, average train loss: 9.7283
[09/25 22:46:38 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1697, average loss: 18.1191
[09/25 22:46:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.50	top5: 33.00	
[09/25 22:46:38 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/25 22:46:45 visual_prompt]: Epoch 43 / 100: avg data time: 5.58e-02, avg batch time: 0.5047, average train loss: 10.2884
[09/25 22:46:46 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1697, average loss: 14.6207
[09/25 22:46:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.50	top5: 33.00	
[09/25 22:46:46 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/25 22:46:53 visual_prompt]: Epoch 44 / 100: avg data time: 5.59e-02, avg batch time: 0.5052, average train loss: 9.3056
[09/25 22:46:54 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 18.8447
[09/25 22:46:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 38.00	
[09/25 22:46:54 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/25 22:47:01 visual_prompt]: Epoch 45 / 100: avg data time: 5.14e-02, avg batch time: 0.5004, average train loss: 10.0605
[09/25 22:47:02 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1698, average loss: 13.2735
[09/25 22:47:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.00	top5: 33.50	
[09/25 22:47:02 visual_prompt]: Best epoch 45: best metric: 0.150
[09/25 22:47:02 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/25 22:47:09 visual_prompt]: Epoch 46 / 100: avg data time: 3.94e-02, avg batch time: 0.4891, average train loss: 9.6077
[09/25 22:47:10 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1697, average loss: 13.1991
[09/25 22:47:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 14.00	top5: 36.00	
[09/25 22:47:10 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/25 22:47:17 visual_prompt]: Epoch 47 / 100: avg data time: 4.85e-02, avg batch time: 0.4972, average train loss: 8.4874
[09/25 22:47:18 visual_prompt]: Inference (val):avg data time: 4.98e-05, avg batch time: 0.1700, average loss: 12.8340
[09/25 22:47:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.00	top5: 40.50	
[09/25 22:47:18 visual_prompt]: Best epoch 47: best metric: 0.160
[09/25 22:47:18 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/25 22:47:25 visual_prompt]: Epoch 48 / 100: avg data time: 5.31e-02, avg batch time: 0.5026, average train loss: 7.6382
[09/25 22:47:26 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1696, average loss: 13.1565
[09/25 22:47:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.50	top5: 41.00	
[09/25 22:47:26 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/25 22:47:33 visual_prompt]: Epoch 49 / 100: avg data time: 4.53e-02, avg batch time: 0.4945, average train loss: 7.3038
[09/25 22:47:34 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1697, average loss: 14.1076
[09/25 22:47:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.00	top5: 37.50	
[09/25 22:47:34 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/25 22:47:41 visual_prompt]: Epoch 50 / 100: avg data time: 4.95e-02, avg batch time: 0.4996, average train loss: 6.8402
[09/25 22:47:42 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1692, average loss: 15.3078
[09/25 22:47:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.50	top5: 33.50	
[09/25 22:47:42 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/25 22:47:49 visual_prompt]: Epoch 51 / 100: avg data time: 3.59e-02, avg batch time: 0.4855, average train loss: 6.6717
[09/25 22:47:50 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1697, average loss: 12.9467
[09/25 22:47:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.50	top5: 37.50	
[09/25 22:47:50 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/25 22:47:57 visual_prompt]: Epoch 52 / 100: avg data time: 3.73e-02, avg batch time: 0.4872, average train loss: 5.7141
[09/25 22:47:58 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1698, average loss: 14.8615
[09/25 22:47:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.50	top5: 38.00	
[09/25 22:47:58 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/25 22:48:05 visual_prompt]: Epoch 53 / 100: avg data time: 5.18e-02, avg batch time: 0.5007, average train loss: 6.0551
[09/25 22:48:06 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1698, average loss: 11.9169
[09/25 22:48:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 17.00	top5: 38.50	
[09/25 22:48:06 visual_prompt]: Best epoch 53: best metric: 0.170
[09/25 22:48:06 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/25 22:48:13 visual_prompt]: Epoch 54 / 100: avg data time: 5.64e-02, avg batch time: 0.5051, average train loss: 5.6122
[09/25 22:48:15 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1697, average loss: 12.2818
[09/25 22:48:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.50	top5: 46.50	
[09/25 22:48:15 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/25 22:48:21 visual_prompt]: Epoch 55 / 100: avg data time: 4.31e-02, avg batch time: 0.4929, average train loss: 5.6535
[09/25 22:48:22 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1697, average loss: 11.9332
[09/25 22:48:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.00	top5: 44.50	
[09/25 22:48:22 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/25 22:48:29 visual_prompt]: Epoch 56 / 100: avg data time: 4.31e-02, avg batch time: 0.4924, average train loss: 5.0182
[09/25 22:48:30 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1696, average loss: 14.4650
[09/25 22:48:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 43.50	
[09/25 22:48:30 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/25 22:48:37 visual_prompt]: Epoch 57 / 100: avg data time: 3.82e-02, avg batch time: 0.4877, average train loss: 4.9990
[09/25 22:48:38 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1699, average loss: 7.2646
[09/25 22:48:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.50	top5: 54.50	
[09/25 22:48:38 visual_prompt]: Best epoch 57: best metric: 0.275
[09/25 22:48:38 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/25 22:48:45 visual_prompt]: Epoch 58 / 100: avg data time: 4.37e-02, avg batch time: 0.4938, average train loss: 4.8486
[09/25 22:48:46 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1697, average loss: 12.9959
[09/25 22:48:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.50	top5: 41.00	
[09/25 22:48:46 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/25 22:48:53 visual_prompt]: Epoch 59 / 100: avg data time: 4.11e-02, avg batch time: 0.4924, average train loss: 4.4709
[09/25 22:48:54 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1696, average loss: 9.8806
[09/25 22:48:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.00	top5: 43.50	
[09/25 22:48:54 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/25 22:49:01 visual_prompt]: Epoch 60 / 100: avg data time: 4.47e-02, avg batch time: 0.4943, average train loss: 4.6447
[09/25 22:49:02 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1698, average loss: 10.3145
[09/25 22:49:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 19.00	top5: 51.50	
[09/25 22:49:02 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/25 22:49:09 visual_prompt]: Epoch 61 / 100: avg data time: 4.64e-02, avg batch time: 0.4961, average train loss: 4.4461
[09/25 22:49:11 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1697, average loss: 8.1016
[09/25 22:49:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.50	top5: 50.50	
[09/25 22:49:11 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/25 22:49:17 visual_prompt]: Epoch 62 / 100: avg data time: 3.60e-02, avg batch time: 0.4855, average train loss: 3.9341
[09/25 22:49:18 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1699, average loss: 11.9322
[09/25 22:49:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 17.50	top5: 43.50	
[09/25 22:49:18 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/25 22:49:25 visual_prompt]: Epoch 63 / 100: avg data time: 3.92e-02, avg batch time: 0.4893, average train loss: 4.1019
[09/25 22:49:26 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1698, average loss: 9.5647
[09/25 22:49:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.00	top5: 50.00	
[09/25 22:49:26 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/25 22:49:33 visual_prompt]: Epoch 64 / 100: avg data time: 4.22e-02, avg batch time: 0.4917, average train loss: 3.8985
[09/25 22:49:34 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1699, average loss: 9.5283
[09/25 22:49:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 21.50	top5: 52.00	
[09/25 22:49:34 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/25 22:49:41 visual_prompt]: Epoch 65 / 100: avg data time: 3.89e-02, avg batch time: 0.4896, average train loss: 3.3629
[09/25 22:49:42 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1695, average loss: 8.7414
[09/25 22:49:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 53.00	
[09/25 22:49:42 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/25 22:49:49 visual_prompt]: Epoch 66 / 100: avg data time: 3.70e-02, avg batch time: 0.4862, average train loss: 2.9179
[09/25 22:49:50 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1696, average loss: 9.5660
[09/25 22:49:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 20.00	top5: 50.00	
[09/25 22:49:50 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/25 22:49:57 visual_prompt]: Epoch 67 / 100: avg data time: 5.77e-02, avg batch time: 0.5074, average train loss: 3.1017
[09/25 22:49:58 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1697, average loss: 10.2541
[09/25 22:49:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 19.00	top5: 48.50	
[09/25 22:49:58 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/25 22:50:05 visual_prompt]: Epoch 68 / 100: avg data time: 5.85e-02, avg batch time: 0.5081, average train loss: 3.1801
[09/25 22:50:07 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1700, average loss: 10.2709
[09/25 22:50:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 19.00	top5: 45.50	
[09/25 22:50:07 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/25 22:50:13 visual_prompt]: Epoch 69 / 100: avg data time: 4.22e-02, avg batch time: 0.4922, average train loss: 3.1410
[09/25 22:50:15 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1703, average loss: 9.0962
[09/25 22:50:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 21.50	top5: 52.00	
[09/25 22:50:15 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/25 22:50:21 visual_prompt]: Epoch 70 / 100: avg data time: 4.87e-02, avg batch time: 0.4990, average train loss: 2.8216
[09/25 22:50:23 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 9.9135
[09/25 22:50:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 20.00	top5: 49.00	
[09/25 22:50:23 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/25 22:50:29 visual_prompt]: Epoch 71 / 100: avg data time: 3.56e-02, avg batch time: 0.4851, average train loss: 2.9519
[09/25 22:50:31 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1699, average loss: 8.9908
[09/25 22:50:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 51.00	
[09/25 22:50:31 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/25 22:50:37 visual_prompt]: Epoch 72 / 100: avg data time: 3.35e-02, avg batch time: 0.4838, average train loss: 2.8174
[09/25 22:50:39 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1693, average loss: 8.6650
[09/25 22:50:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.50	top5: 55.50	
[09/25 22:50:39 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/25 22:50:45 visual_prompt]: Epoch 73 / 100: avg data time: 3.61e-02, avg batch time: 0.4870, average train loss: 2.9271
[09/25 22:50:47 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1699, average loss: 8.3710
[09/25 22:50:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.50	top5: 54.50	
[09/25 22:50:47 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/25 22:50:53 visual_prompt]: Epoch 74 / 100: avg data time: 4.34e-02, avg batch time: 0.4929, average train loss: 2.6878
[09/25 22:50:55 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1698, average loss: 7.7613
[09/25 22:50:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.50	top5: 55.00	
[09/25 22:50:55 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/25 22:51:01 visual_prompt]: Epoch 75 / 100: avg data time: 3.90e-02, avg batch time: 0.4920, average train loss: 2.4199
[09/25 22:51:03 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1697, average loss: 11.5522
[09/25 22:51:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.50	top5: 45.00	
[09/25 22:51:03 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/25 22:51:09 visual_prompt]: Epoch 76 / 100: avg data time: 4.42e-02, avg batch time: 0.4940, average train loss: 2.4454
[09/25 22:51:11 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1695, average loss: 9.1206
[09/25 22:51:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 18.50	top5: 50.00	
[09/25 22:51:11 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/25 22:51:17 visual_prompt]: Epoch 77 / 100: avg data time: 4.42e-02, avg batch time: 0.4942, average train loss: 2.1265
[09/25 22:51:19 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1698, average loss: 10.4051
[09/25 22:51:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 21.50	top5: 48.50	
[09/25 22:51:19 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/25 22:51:25 visual_prompt]: Epoch 78 / 100: avg data time: 4.87e-02, avg batch time: 0.4993, average train loss: 2.3099
[09/25 22:51:27 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1699, average loss: 8.9901
[09/25 22:51:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.50	top5: 51.00	
[09/25 22:51:27 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/25 22:51:34 visual_prompt]: Epoch 79 / 100: avg data time: 4.81e-02, avg batch time: 0.5107, average train loss: 2.3712
[09/25 22:51:35 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1698, average loss: 9.8365
[09/25 22:51:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 19.50	top5: 51.00	
[09/25 22:51:35 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/25 22:51:42 visual_prompt]: Epoch 80 / 100: avg data time: 4.44e-02, avg batch time: 0.4942, average train loss: 2.3475
[09/25 22:51:43 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1698, average loss: 7.8388
[09/25 22:51:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.50	top5: 54.50	
[09/25 22:51:43 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/25 22:51:50 visual_prompt]: Epoch 81 / 100: avg data time: 3.65e-02, avg batch time: 0.4872, average train loss: 2.0646
[09/25 22:51:51 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1692, average loss: 9.2329
[09/25 22:51:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 49.50	
[09/25 22:51:51 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/25 22:51:58 visual_prompt]: Epoch 82 / 100: avg data time: 4.93e-02, avg batch time: 0.4982, average train loss: 2.0855
[09/25 22:51:59 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 9.0293
[09/25 22:51:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 21.50	top5: 51.50	
[09/25 22:51:59 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/25 22:52:06 visual_prompt]: Epoch 83 / 100: avg data time: 3.94e-02, avg batch time: 0.4911, average train loss: 2.0177
[09/25 22:52:07 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1695, average loss: 7.4147
[09/25 22:52:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.00	top5: 57.50	
[09/25 22:52:07 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/25 22:52:14 visual_prompt]: Epoch 84 / 100: avg data time: 3.48e-02, avg batch time: 0.4848, average train loss: 1.8925
[09/25 22:52:15 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 9.0143
[09/25 22:52:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 18.00	top5: 51.00	
[09/25 22:52:15 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/25 22:52:22 visual_prompt]: Epoch 85 / 100: avg data time: 3.70e-02, avg batch time: 0.4882, average train loss: 1.9096
[09/25 22:52:23 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1697, average loss: 8.1650
[09/25 22:52:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.00	top5: 55.00	
[09/25 22:52:23 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/25 22:52:30 visual_prompt]: Epoch 86 / 100: avg data time: 4.00e-02, avg batch time: 0.4896, average train loss: 1.8936
[09/25 22:52:31 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1697, average loss: 8.1907
[09/25 22:52:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.00	top5: 54.50	
[09/25 22:52:31 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/25 22:52:38 visual_prompt]: Epoch 87 / 100: avg data time: 4.78e-02, avg batch time: 0.4975, average train loss: 1.8971
[09/25 22:52:39 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 8.5104
[09/25 22:52:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.00	top5: 54.00	
[09/25 22:52:39 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/25 22:52:46 visual_prompt]: Epoch 88 / 100: avg data time: 4.57e-02, avg batch time: 0.4960, average train loss: 1.7373
[09/25 22:52:47 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1700, average loss: 8.3470
[09/25 22:52:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 54.50	
[09/25 22:52:47 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/25 22:52:54 visual_prompt]: Epoch 89 / 100: avg data time: 4.39e-02, avg batch time: 0.4938, average train loss: 1.7297
[09/25 22:52:55 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1709, average loss: 8.2952
[09/25 22:52:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.50	top5: 56.00	
[09/25 22:52:55 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/25 22:53:02 visual_prompt]: Epoch 90 / 100: avg data time: 4.01e-02, avg batch time: 0.4904, average train loss: 1.8077
[09/25 22:53:03 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1699, average loss: 8.3757
[09/25 22:53:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.00	top5: 54.50	
[09/25 22:53:03 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/25 22:53:10 visual_prompt]: Epoch 91 / 100: avg data time: 4.96e-02, avg batch time: 0.4999, average train loss: 1.7105
[09/25 22:53:11 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1695, average loss: 8.7903
[09/25 22:53:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.00	top5: 52.00	
[09/25 22:53:11 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/25 22:53:18 visual_prompt]: Epoch 92 / 100: avg data time: 4.55e-02, avg batch time: 0.4946, average train loss: 1.7487
[09/25 22:53:19 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 8.5446
[09/25 22:53:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.00	top5: 53.50	
[09/25 22:53:19 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/25 22:53:26 visual_prompt]: Epoch 93 / 100: avg data time: 4.16e-02, avg batch time: 0.4918, average train loss: 1.6410
[09/25 22:53:27 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1697, average loss: 8.0176
[09/25 22:53:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 56.00	
[09/25 22:53:27 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/25 22:53:34 visual_prompt]: Epoch 94 / 100: avg data time: 4.11e-02, avg batch time: 0.4903, average train loss: 1.7038
[09/25 22:53:35 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1694, average loss: 8.5377
[09/25 22:53:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.50	top5: 52.00	
[09/25 22:53:35 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/25 22:53:42 visual_prompt]: Epoch 95 / 100: avg data time: 4.78e-02, avg batch time: 0.4975, average train loss: 1.6128
[09/25 22:53:43 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1698, average loss: 8.5962
[09/25 22:53:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.00	top5: 52.50	
[09/25 22:53:43 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/25 22:53:50 visual_prompt]: Epoch 96 / 100: avg data time: 3.99e-02, avg batch time: 0.4915, average train loss: 1.8291
[09/25 22:53:51 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1691, average loss: 8.3450
[09/25 22:53:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.50	top5: 53.00	
[09/25 22:53:51 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/25 22:53:58 visual_prompt]: Epoch 97 / 100: avg data time: 3.96e-02, avg batch time: 0.4904, average train loss: 1.5242
[09/25 22:53:59 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1695, average loss: 8.2790
[09/25 22:53:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.00	top5: 52.50	
[09/25 22:53:59 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/25 22:54:06 visual_prompt]: Epoch 98 / 100: avg data time: 5.07e-02, avg batch time: 0.4995, average train loss: 1.6792
[09/25 22:54:07 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1699, average loss: 8.2837
[09/25 22:54:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 53.00	
[09/25 22:54:07 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/25 22:54:14 visual_prompt]: Epoch 99 / 100: avg data time: 4.48e-02, avg batch time: 0.4943, average train loss: 1.5715
[09/25 22:54:16 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1699, average loss: 8.3172
[09/25 22:54:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.00	top5: 53.00	
[09/25 22:54:16 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/25 22:54:22 visual_prompt]: Epoch 100 / 100: avg data time: 3.70e-02, avg batch time: 0.4866, average train loss: 1.7427
[09/25 22:54:24 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1699, average loss: 8.3246
[09/25 22:54:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.00	top5: 53.00	
[09/25 22:54:24 visual_prompt]: Rank of current process: 0. World size: 1
[09/25 22:54:24 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/25 22:54:24 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/25 22:54:24 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/25 22:54:24 visual_prompt]: Training with config:
[09/25 22:54:24 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/25 22:54:24 visual_prompt]: Loading training data...
[09/25 22:54:24 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 22:54:26 visual_prompt]: Number of images: 800
[09/25 22:54:26 visual_prompt]: Number of classes: 47 / 47
[09/25 22:54:26 visual_prompt]: Loading validation data...
[09/25 22:54:26 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 22:54:26 visual_prompt]: Number of images: 200
[09/25 22:54:26 visual_prompt]: Number of classes: 47 / 47
[09/25 22:54:26 visual_prompt]: Constructing models...
[09/25 22:54:29 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/25 22:54:29 visual_prompt]: tuned percent:0.576
[09/25 22:54:29 visual_prompt]: Device used for model: 0
[09/25 22:54:29 visual_prompt]: Setting up Evaluator...
[09/25 22:54:29 visual_prompt]: Setting up Trainer...
[09/25 22:54:29 visual_prompt]: 	Setting up the optimizer...
[09/25 22:54:29 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/25 22:54:35 visual_prompt]: Epoch 1 / 100: avg data time: 3.95e-02, avg batch time: 0.4875, average train loss: 3.9315
[09/25 22:54:37 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1687, average loss: 3.9045
[09/25 22:54:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 22:54:37 visual_prompt]: Best epoch 1: best metric: 0.040
[09/25 22:54:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/25 22:54:43 visual_prompt]: Epoch 2 / 100: avg data time: 4.61e-02, avg batch time: 0.4948, average train loss: 4.3181
[09/25 22:54:45 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1688, average loss: 4.4928
[09/25 22:54:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.50	
[09/25 22:54:45 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/25 22:54:52 visual_prompt]: Epoch 3 / 100: avg data time: 5.25e-02, avg batch time: 0.4999, average train loss: 5.0560
[09/25 22:54:53 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1686, average loss: 5.8759
[09/25 22:54:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 6.50	
[09/25 22:54:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/25 22:55:00 visual_prompt]: Epoch 4 / 100: avg data time: 4.86e-02, avg batch time: 0.4969, average train loss: 6.8103
[09/25 22:55:01 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 7.2542
[09/25 22:55:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/25 22:55:01 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/25 22:55:08 visual_prompt]: Epoch 5 / 100: avg data time: 6.14e-02, avg batch time: 0.5088, average train loss: 9.5345
[09/25 22:55:09 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.1688, average loss: 9.4895
[09/25 22:55:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.50	
[09/25 22:55:09 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/25 22:55:16 visual_prompt]: Epoch 6 / 100: avg data time: 3.96e-02, avg batch time: 0.4890, average train loss: 12.8164
[09/25 22:55:17 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1690, average loss: 13.4992
[09/25 22:55:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/25 22:55:17 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/25 22:55:24 visual_prompt]: Epoch 7 / 100: avg data time: 4.82e-02, avg batch time: 0.4952, average train loss: 24.5774
[09/25 22:55:25 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1688, average loss: 18.6184
[09/25 22:55:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 17.50	
[09/25 22:55:25 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/25 22:55:32 visual_prompt]: Epoch 8 / 100: avg data time: 4.88e-02, avg batch time: 0.4966, average train loss: 35.1941
[09/25 22:55:33 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1688, average loss: 57.5832
[09/25 22:55:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/25 22:55:33 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/25 22:55:40 visual_prompt]: Epoch 9 / 100: avg data time: 3.83e-02, avg batch time: 0.4864, average train loss: 59.9387
[09/25 22:55:41 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1693, average loss: 74.4044
[09/25 22:55:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 14.00	
[09/25 22:55:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/25 22:55:48 visual_prompt]: Epoch 10 / 100: avg data time: 5.13e-02, avg batch time: 0.5000, average train loss: 74.0643
[09/25 22:55:49 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1692, average loss: 83.9294
[09/25 22:55:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:55:49 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/25 22:55:56 visual_prompt]: Epoch 11 / 100: avg data time: 5.12e-02, avg batch time: 0.4988, average train loss: 82.4993
[09/25 22:55:58 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 114.1259
[09/25 22:55:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:55:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/25 22:56:04 visual_prompt]: Epoch 12 / 100: avg data time: 3.98e-02, avg batch time: 0.4892, average train loss: 80.2268
[09/25 22:56:06 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1687, average loss: 79.4565
[09/25 22:56:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 22:56:06 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/25 22:56:12 visual_prompt]: Epoch 13 / 100: avg data time: 4.44e-02, avg batch time: 0.4947, average train loss: 84.0209
[09/25 22:56:14 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1694, average loss: 79.2462
[09/25 22:56:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 22:56:14 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/25 22:56:20 visual_prompt]: Epoch 14 / 100: avg data time: 5.10e-02, avg batch time: 0.4997, average train loss: 89.0837
[09/25 22:56:22 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 106.0651
[09/25 22:56:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 12.00	
[09/25 22:56:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/25 22:56:29 visual_prompt]: Epoch 15 / 100: avg data time: 4.86e-02, avg batch time: 0.4968, average train loss: 98.8032
[09/25 22:56:30 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 98.6969
[09/25 22:56:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.00	
[09/25 22:56:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/25 22:56:37 visual_prompt]: Epoch 16 / 100: avg data time: 4.27e-02, avg batch time: 0.4922, average train loss: 106.4755
[09/25 22:56:38 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 118.6609
[09/25 22:56:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 22:56:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/25 22:56:44 visual_prompt]: Epoch 17 / 100: avg data time: 3.38e-02, avg batch time: 0.4847, average train loss: 118.7627
[09/25 22:56:46 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1689, average loss: 106.0664
[09/25 22:56:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.50	
[09/25 22:56:46 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/25 22:56:52 visual_prompt]: Epoch 18 / 100: avg data time: 4.88e-02, avg batch time: 0.4967, average train loss: 136.9641
[09/25 22:56:54 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 188.4094
[09/25 22:56:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/25 22:56:54 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/25 22:57:01 visual_prompt]: Epoch 19 / 100: avg data time: 4.97e-02, avg batch time: 0.4989, average train loss: 135.5252
[09/25 22:57:02 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1693, average loss: 121.6199
[09/25 22:57:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 10.00	
[09/25 22:57:02 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/25 22:57:09 visual_prompt]: Epoch 20 / 100: avg data time: 4.62e-02, avg batch time: 0.4950, average train loss: 123.5554
[09/25 22:57:10 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 109.8380
[09/25 22:57:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.00	
[09/25 22:57:10 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/25 22:57:17 visual_prompt]: Epoch 21 / 100: avg data time: 4.69e-02, avg batch time: 0.4957, average train loss: 129.0590
[09/25 22:57:18 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1691, average loss: 115.1530
[09/25 22:57:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 13.00	
[09/25 22:57:18 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/25 22:57:25 visual_prompt]: Epoch 22 / 100: avg data time: 4.56e-02, avg batch time: 0.4978, average train loss: 122.2982
[09/25 22:57:26 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1691, average loss: 115.2701
[09/25 22:57:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.00	
[09/25 22:57:26 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/25 22:57:33 visual_prompt]: Epoch 23 / 100: avg data time: 4.02e-02, avg batch time: 0.4912, average train loss: 118.2378
[09/25 22:57:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 108.3954
[09/25 22:57:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/25 22:57:34 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/25 22:57:41 visual_prompt]: Epoch 24 / 100: avg data time: 4.71e-02, avg batch time: 0.4957, average train loss: 98.5689
[09/25 22:57:42 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 133.5057
[09/25 22:57:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 22:57:42 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/25 22:57:49 visual_prompt]: Epoch 25 / 100: avg data time: 5.31e-02, avg batch time: 0.5018, average train loss: 109.2245
[09/25 22:57:50 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1694, average loss: 93.3590
[09/25 22:57:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/25 22:57:50 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/25 22:57:57 visual_prompt]: Epoch 26 / 100: avg data time: 5.53e-02, avg batch time: 0.5032, average train loss: 103.7081
[09/25 22:57:58 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1693, average loss: 97.9917
[09/25 22:57:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 16.00	
[09/25 22:57:58 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/25 22:58:05 visual_prompt]: Epoch 27 / 100: avg data time: 5.43e-02, avg batch time: 0.5032, average train loss: 101.7109
[09/25 22:58:07 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1692, average loss: 93.3020
[09/25 22:58:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.50	
[09/25 22:58:07 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/25 22:58:13 visual_prompt]: Epoch 28 / 100: avg data time: 5.54e-02, avg batch time: 0.5040, average train loss: 99.7192
[09/25 22:58:15 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1696, average loss: 90.2227
[09/25 22:58:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/25 22:58:15 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/25 22:58:21 visual_prompt]: Epoch 29 / 100: avg data time: 3.62e-02, avg batch time: 0.4857, average train loss: 84.2553
[09/25 22:58:23 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 92.4580
[09/25 22:58:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 22:58:23 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/25 22:58:29 visual_prompt]: Epoch 30 / 100: avg data time: 5.17e-02, avg batch time: 0.5006, average train loss: 96.7573
[09/25 22:58:31 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1696, average loss: 114.8721
[09/25 22:58:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 12.00	
[09/25 22:58:31 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/25 22:58:37 visual_prompt]: Epoch 31 / 100: avg data time: 5.16e-02, avg batch time: 0.4996, average train loss: 106.9309
[09/25 22:58:39 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1692, average loss: 124.0542
[09/25 22:58:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.50	
[09/25 22:58:39 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/25 22:58:45 visual_prompt]: Epoch 32 / 100: avg data time: 4.49e-02, avg batch time: 0.4939, average train loss: 107.5464
[09/25 22:58:47 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1693, average loss: 87.4683
[09/25 22:58:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 22:58:47 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/25 22:58:54 visual_prompt]: Epoch 33 / 100: avg data time: 5.03e-02, avg batch time: 0.4994, average train loss: 89.4623
[09/25 22:58:55 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1692, average loss: 78.2774
[09/25 22:58:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:58:55 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/25 22:59:02 visual_prompt]: Epoch 34 / 100: avg data time: 3.86e-02, avg batch time: 0.4878, average train loss: 89.2032
[09/25 22:59:03 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1689, average loss: 94.3986
[09/25 22:59:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.50	
[09/25 22:59:03 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/25 22:59:10 visual_prompt]: Epoch 35 / 100: avg data time: 5.48e-02, avg batch time: 0.5032, average train loss: 98.4747
[09/25 22:59:11 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 96.3355
[09/25 22:59:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.50	
[09/25 22:59:11 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/25 22:59:18 visual_prompt]: Epoch 36 / 100: avg data time: 4.39e-02, avg batch time: 0.4917, average train loss: 100.7626
[09/25 22:59:19 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 93.9734
[09/25 22:59:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/25 22:59:19 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/25 22:59:26 visual_prompt]: Epoch 37 / 100: avg data time: 4.14e-02, avg batch time: 0.4896, average train loss: 80.1296
[09/25 22:59:27 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1692, average loss: 70.5666
[09/25 22:59:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.00	
[09/25 22:59:27 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/25 22:59:34 visual_prompt]: Epoch 38 / 100: avg data time: 5.50e-02, avg batch time: 0.5031, average train loss: 81.2957
[09/25 22:59:35 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1694, average loss: 86.5935
[09/25 22:59:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.00	
[09/25 22:59:35 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/25 22:59:42 visual_prompt]: Epoch 39 / 100: avg data time: 5.05e-02, avg batch time: 0.5000, average train loss: 86.3373
[09/25 22:59:43 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 96.5207
[09/25 22:59:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 22:59:43 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/25 22:59:50 visual_prompt]: Epoch 40 / 100: avg data time: 5.12e-02, avg batch time: 0.5004, average train loss: 92.8626
[09/25 22:59:51 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1699, average loss: 70.4157
[09/25 22:59:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 22:59:51 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/25 22:59:58 visual_prompt]: Epoch 41 / 100: avg data time: 4.62e-02, avg batch time: 0.4967, average train loss: 78.3311
[09/25 22:59:59 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1698, average loss: 66.6798
[09/25 22:59:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.50	
[09/25 22:59:59 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/25 23:00:06 visual_prompt]: Epoch 42 / 100: avg data time: 4.58e-02, avg batch time: 0.4951, average train loss: 82.2748
[09/25 23:00:07 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1697, average loss: 74.8737
[09/25 23:00:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/25 23:00:07 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/25 23:00:14 visual_prompt]: Epoch 43 / 100: avg data time: 4.81e-02, avg batch time: 0.4961, average train loss: 94.8928
[09/25 23:00:15 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1694, average loss: 86.7799
[09/25 23:00:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.00	
[09/25 23:00:15 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/25 23:00:22 visual_prompt]: Epoch 44 / 100: avg data time: 5.02e-02, avg batch time: 0.4995, average train loss: 90.2370
[09/25 23:00:24 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1693, average loss: 83.7451
[09/25 23:00:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 23:00:24 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/25 23:00:30 visual_prompt]: Epoch 45 / 100: avg data time: 4.54e-02, avg batch time: 0.4938, average train loss: 90.7051
[09/25 23:00:32 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1693, average loss: 86.2430
[09/25 23:00:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 10.50	
[09/25 23:00:32 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/25 23:00:38 visual_prompt]: Epoch 46 / 100: avg data time: 5.32e-02, avg batch time: 0.5020, average train loss: 90.1570
[09/25 23:00:40 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.1691, average loss: 70.1042
[09/25 23:00:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 23:00:40 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/25 23:00:46 visual_prompt]: Epoch 47 / 100: avg data time: 4.51e-02, avg batch time: 0.4940, average train loss: 74.7538
[09/25 23:00:48 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 103.0221
[09/25 23:00:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 23:00:48 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/25 23:00:54 visual_prompt]: Epoch 48 / 100: avg data time: 4.94e-02, avg batch time: 0.4973, average train loss: 72.9629
[09/25 23:00:56 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 68.7459
[09/25 23:00:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 23:00:56 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/25 23:01:03 visual_prompt]: Epoch 49 / 100: avg data time: 5.60e-02, avg batch time: 0.5038, average train loss: 64.9409
[09/25 23:01:04 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1694, average loss: 63.2899
[09/25 23:01:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.50	
[09/25 23:01:04 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/25 23:01:11 visual_prompt]: Epoch 50 / 100: avg data time: 5.37e-02, avg batch time: 0.5023, average train loss: 69.8195
[09/25 23:01:12 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.1688, average loss: 70.4229
[09/25 23:01:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 14.50	
[09/25 23:01:12 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/25 23:01:19 visual_prompt]: Epoch 51 / 100: avg data time: 5.27e-02, avg batch time: 0.5012, average train loss: 76.2972
[09/25 23:01:20 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 62.9988
[09/25 23:01:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/25 23:01:20 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/25 23:01:27 visual_prompt]: Epoch 52 / 100: avg data time: 5.05e-02, avg batch time: 0.4983, average train loss: 60.5310
[09/25 23:01:28 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1690, average loss: 52.8623
[09/25 23:01:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 13.50	
[09/25 23:01:28 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/25 23:01:35 visual_prompt]: Epoch 53 / 100: avg data time: 4.93e-02, avg batch time: 0.4981, average train loss: 60.9972
[09/25 23:01:37 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1687, average loss: 49.3157
[09/25 23:01:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.00	
[09/25 23:01:37 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/25 23:01:43 visual_prompt]: Epoch 54 / 100: avg data time: 4.85e-02, avg batch time: 0.4969, average train loss: 58.0433
[09/25 23:01:45 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1691, average loss: 42.9898
[09/25 23:01:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 23:01:45 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/25 23:01:51 visual_prompt]: Epoch 55 / 100: avg data time: 4.83e-02, avg batch time: 0.4978, average train loss: 50.6701
[09/25 23:01:53 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1691, average loss: 44.1379
[09/25 23:01:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/25 23:01:53 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/25 23:01:59 visual_prompt]: Epoch 56 / 100: avg data time: 5.32e-02, avg batch time: 0.5018, average train loss: 51.8702
[09/25 23:02:01 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 50.5959
[09/25 23:02:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 23:02:01 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/25 23:02:08 visual_prompt]: Epoch 57 / 100: avg data time: 4.59e-02, avg batch time: 0.5128, average train loss: 53.3741
[09/25 23:02:09 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1691, average loss: 48.3749
[09/25 23:02:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/25 23:02:09 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/25 23:02:16 visual_prompt]: Epoch 58 / 100: avg data time: 5.21e-02, avg batch time: 0.4999, average train loss: 47.1144
[09/25 23:02:17 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1693, average loss: 43.4886
[09/25 23:02:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 10.00	
[09/25 23:02:17 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/25 23:02:24 visual_prompt]: Epoch 59 / 100: avg data time: 5.01e-02, avg batch time: 0.4997, average train loss: 44.0375
[09/25 23:02:25 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1693, average loss: 38.9139
[09/25 23:02:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/25 23:02:25 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/25 23:02:32 visual_prompt]: Epoch 60 / 100: avg data time: 5.39e-02, avg batch time: 0.5016, average train loss: 41.9452
[09/25 23:02:33 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1696, average loss: 36.6559
[09/25 23:02:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.00	
[09/25 23:02:33 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/25 23:02:40 visual_prompt]: Epoch 61 / 100: avg data time: 5.45e-02, avg batch time: 0.5042, average train loss: 33.7036
[09/25 23:02:41 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1689, average loss: 32.6889
[09/25 23:02:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.50	
[09/25 23:02:41 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/25 23:02:48 visual_prompt]: Epoch 62 / 100: avg data time: 4.20e-02, avg batch time: 0.4914, average train loss: 31.4064
[09/25 23:02:49 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 32.0868
[09/25 23:02:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.00	
[09/25 23:02:49 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/25 23:02:56 visual_prompt]: Epoch 63 / 100: avg data time: 5.32e-02, avg batch time: 0.5027, average train loss: 33.3943
[09/25 23:02:58 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 29.7949
[09/25 23:02:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/25 23:02:58 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/25 23:03:04 visual_prompt]: Epoch 64 / 100: avg data time: 5.01e-02, avg batch time: 0.4987, average train loss: 30.8199
[09/25 23:03:06 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 26.6325
[09/25 23:03:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/25 23:03:06 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/25 23:03:12 visual_prompt]: Epoch 65 / 100: avg data time: 5.07e-02, avg batch time: 0.4996, average train loss: 28.6785
[09/25 23:03:14 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1691, average loss: 31.2635
[09/25 23:03:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.50	
[09/25 23:03:14 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/25 23:03:20 visual_prompt]: Epoch 66 / 100: avg data time: 4.80e-02, avg batch time: 0.4962, average train loss: 28.9137
[09/25 23:03:22 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 27.9679
[09/25 23:03:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 23:03:22 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/25 23:03:28 visual_prompt]: Epoch 67 / 100: avg data time: 5.08e-02, avg batch time: 0.4997, average train loss: 24.4499
[09/25 23:03:30 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 14.7234
[09/25 23:03:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 10.00	
[09/25 23:03:30 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/25 23:03:37 visual_prompt]: Epoch 68 / 100: avg data time: 4.79e-02, avg batch time: 0.4958, average train loss: 19.7481
[09/25 23:03:38 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1687, average loss: 18.3529
[09/25 23:03:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.50	
[09/25 23:03:38 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/25 23:03:44 visual_prompt]: Epoch 69 / 100: avg data time: 4.01e-02, avg batch time: 0.4888, average train loss: 18.7175
[09/25 23:03:46 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1696, average loss: 12.7171
[09/25 23:03:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 23:03:46 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/25 23:03:53 visual_prompt]: Epoch 70 / 100: avg data time: 4.62e-02, avg batch time: 0.4952, average train loss: 17.8589
[09/25 23:03:54 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 9.9082
[09/25 23:03:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 15.00	
[09/25 23:03:54 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/25 23:04:01 visual_prompt]: Epoch 71 / 100: avg data time: 3.86e-02, avg batch time: 0.4879, average train loss: 16.0449
[09/25 23:04:02 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 10.4950
[09/25 23:04:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 16.00	
[09/25 23:04:02 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/25 23:04:09 visual_prompt]: Epoch 72 / 100: avg data time: 4.72e-02, avg batch time: 0.4966, average train loss: 15.4597
[09/25 23:04:10 visual_prompt]: Inference (val):avg data time: 5.28e-05, avg batch time: 0.1697, average loss: 9.6023
[09/25 23:04:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/25 23:04:10 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/25 23:04:17 visual_prompt]: Epoch 73 / 100: avg data time: 3.69e-02, avg batch time: 0.4857, average train loss: 10.1630
[09/25 23:04:18 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1690, average loss: 6.5088
[09/25 23:04:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/25 23:04:18 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/25 23:04:24 visual_prompt]: Epoch 74 / 100: avg data time: 3.69e-02, avg batch time: 0.4868, average train loss: 8.1519
[09/25 23:04:26 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1697, average loss: 7.1482
[09/25 23:04:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 23:04:26 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/25 23:04:32 visual_prompt]: Epoch 75 / 100: avg data time: 4.49e-02, avg batch time: 0.4954, average train loss: 7.9783
[09/25 23:04:34 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 5.8522
[09/25 23:04:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.00	
[09/25 23:04:34 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/25 23:04:40 visual_prompt]: Epoch 76 / 100: avg data time: 3.83e-02, avg batch time: 0.4872, average train loss: 6.4690
[09/25 23:04:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1695, average loss: 5.9614
[09/25 23:04:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.00	
[09/25 23:04:42 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/25 23:04:48 visual_prompt]: Epoch 77 / 100: avg data time: 5.34e-02, avg batch time: 0.5020, average train loss: 7.8186
[09/25 23:04:50 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1691, average loss: 7.6806
[09/25 23:04:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.00	
[09/25 23:04:50 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/25 23:04:56 visual_prompt]: Epoch 78 / 100: avg data time: 3.93e-02, avg batch time: 0.4895, average train loss: 8.3329
[09/25 23:04:58 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1691, average loss: 5.7991
[09/25 23:04:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 10.50	
[09/25 23:04:58 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/25 23:05:05 visual_prompt]: Epoch 79 / 100: avg data time: 5.24e-02, avg batch time: 0.5001, average train loss: 5.7650
[09/25 23:05:06 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1694, average loss: 4.7073
[09/25 23:05:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 23:05:06 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/25 23:05:13 visual_prompt]: Epoch 80 / 100: avg data time: 5.14e-02, avg batch time: 0.5003, average train loss: 4.8751
[09/25 23:05:14 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 4.8477
[09/25 23:05:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 23:05:14 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/25 23:05:21 visual_prompt]: Epoch 81 / 100: avg data time: 4.70e-02, avg batch time: 0.4959, average train loss: 4.8026
[09/25 23:05:22 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1692, average loss: 4.3615
[09/25 23:05:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 8.50	
[09/25 23:05:22 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/25 23:05:29 visual_prompt]: Epoch 82 / 100: avg data time: 4.42e-02, avg batch time: 0.4953, average train loss: 4.4740
[09/25 23:05:30 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 4.3598
[09/25 23:05:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/25 23:05:30 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/25 23:05:37 visual_prompt]: Epoch 83 / 100: avg data time: 4.33e-02, avg batch time: 0.4928, average train loss: 4.2302
[09/25 23:05:38 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1693, average loss: 4.4274
[09/25 23:05:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 5.00	
[09/25 23:05:38 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/25 23:05:45 visual_prompt]: Epoch 84 / 100: avg data time: 3.82e-02, avg batch time: 0.4865, average train loss: 4.0280
[09/25 23:05:46 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1692, average loss: 3.9735
[09/25 23:05:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/25 23:05:46 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/25 23:05:53 visual_prompt]: Epoch 85 / 100: avg data time: 3.87e-02, avg batch time: 0.4875, average train loss: 3.9871
[09/25 23:05:54 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 3.9890
[09/25 23:05:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.00	
[09/25 23:05:54 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/25 23:06:01 visual_prompt]: Epoch 86 / 100: avg data time: 5.51e-02, avg batch time: 0.5043, average train loss: 4.0041
[09/25 23:06:02 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1691, average loss: 4.0137
[09/25 23:06:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 23:06:02 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/25 23:06:09 visual_prompt]: Epoch 87 / 100: avg data time: 4.74e-02, avg batch time: 0.4956, average train loss: 3.9513
[09/25 23:06:10 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.1691, average loss: 3.9489
[09/25 23:06:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 23:06:10 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/25 23:06:17 visual_prompt]: Epoch 88 / 100: avg data time: 4.87e-02, avg batch time: 0.4969, average train loss: 3.9296
[09/25 23:06:18 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1693, average loss: 3.9346
[09/25 23:06:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/25 23:06:18 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/25 23:06:25 visual_prompt]: Epoch 89 / 100: avg data time: 4.33e-02, avg batch time: 0.4919, average train loss: 3.9054
[09/25 23:06:26 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1693, average loss: 3.9418
[09/25 23:06:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/25 23:06:26 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/25 23:06:33 visual_prompt]: Epoch 90 / 100: avg data time: 4.73e-02, avg batch time: 0.4955, average train loss: 3.8926
[09/25 23:06:34 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1692, average loss: 3.9490
[09/25 23:06:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/25 23:06:34 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/25 23:06:41 visual_prompt]: Epoch 91 / 100: avg data time: 4.59e-02, avg batch time: 0.4939, average train loss: 3.9097
[09/25 23:06:42 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1694, average loss: 3.9810
[09/25 23:06:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 23:06:42 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/25 23:06:49 visual_prompt]: Epoch 92 / 100: avg data time: 4.46e-02, avg batch time: 0.4955, average train loss: 3.8809
[09/25 23:06:50 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1695, average loss: 3.8919
[09/25 23:06:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 7.50	
[09/25 23:06:50 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/25 23:06:57 visual_prompt]: Epoch 93 / 100: avg data time: 3.75e-02, avg batch time: 0.4887, average train loss: 3.8290
[09/25 23:06:58 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1698, average loss: 3.8624
[09/25 23:06:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.00	
[09/25 23:06:58 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/25 23:07:05 visual_prompt]: Epoch 94 / 100: avg data time: 4.85e-02, avg batch time: 0.4973, average train loss: 3.8819
[09/25 23:07:06 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 3.8900
[09/25 23:07:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.50	
[09/25 23:07:06 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/25 23:07:13 visual_prompt]: Epoch 95 / 100: avg data time: 3.96e-02, avg batch time: 0.4880, average train loss: 3.8387
[09/25 23:07:14 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1696, average loss: 3.9396
[09/25 23:07:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 23:07:14 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/25 23:07:21 visual_prompt]: Epoch 96 / 100: avg data time: 5.01e-02, avg batch time: 0.5000, average train loss: 3.8293
[09/25 23:07:22 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1690, average loss: 3.8845
[09/25 23:07:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/25 23:07:22 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/25 23:07:29 visual_prompt]: Epoch 97 / 100: avg data time: 5.11e-02, avg batch time: 0.5007, average train loss: 3.8065
[09/25 23:07:31 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1693, average loss: 3.8569
[09/25 23:07:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/25 23:07:31 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/25 23:07:37 visual_prompt]: Epoch 98 / 100: avg data time: 4.86e-02, avg batch time: 0.4973, average train loss: 3.7742
[09/25 23:07:39 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1696, average loss: 3.8686
[09/25 23:07:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 14.00	
[09/25 23:07:39 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/25 23:07:45 visual_prompt]: Epoch 99 / 100: avg data time: 5.36e-02, avg batch time: 0.5025, average train loss: 3.7357
[09/25 23:07:47 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 3.8428
[09/25 23:07:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/25 23:07:47 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/25 23:07:53 visual_prompt]: Epoch 100 / 100: avg data time: 4.76e-02, avg batch time: 0.4965, average train loss: 3.7276
[09/25 23:07:55 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 3.7964
[09/25 23:07:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 15.50	
[09/25 23:07:55 visual_prompt]: Rank of current process: 0. World size: 1
[09/25 23:07:55 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/25 23:07:55 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/25 23:07:55 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/25 23:07:55 visual_prompt]: Training with config:
[09/25 23:07:55 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/25 23:07:55 visual_prompt]: Loading training data...
[09/25 23:07:55 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 23:07:57 visual_prompt]: Number of images: 800
[09/25 23:07:57 visual_prompt]: Number of classes: 47 / 47
[09/25 23:07:57 visual_prompt]: Loading validation data...
[09/25 23:07:57 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 23:07:57 visual_prompt]: Number of images: 200
[09/25 23:07:57 visual_prompt]: Number of classes: 47 / 47
[09/25 23:07:57 visual_prompt]: Constructing models...
[09/25 23:08:00 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/25 23:08:00 visual_prompt]: tuned percent:0.576
[09/25 23:08:00 visual_prompt]: Device used for model: 0
[09/25 23:08:00 visual_prompt]: Setting up Evaluator...
[09/25 23:08:00 visual_prompt]: Setting up Trainer...
[09/25 23:08:00 visual_prompt]: 	Setting up the optimizer...
[09/25 23:08:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/25 23:08:07 visual_prompt]: Epoch 1 / 100: avg data time: 5.51e-02, avg batch time: 0.5048, average train loss: 3.9264
[09/25 23:08:08 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1688, average loss: 3.9045
[09/25 23:08:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 23:08:08 visual_prompt]: Best epoch 1: best metric: 0.040
[09/25 23:08:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/25 23:08:15 visual_prompt]: Epoch 2 / 100: avg data time: 5.18e-02, avg batch time: 0.4983, average train loss: 4.1713
[09/25 23:08:16 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1689, average loss: 4.5796
[09/25 23:08:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 15.50	
[09/25 23:08:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/25 23:08:23 visual_prompt]: Epoch 3 / 100: avg data time: 5.54e-02, avg batch time: 0.5022, average train loss: 4.5565
[09/25 23:08:24 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 6.0882
[09/25 23:08:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.00	
[09/25 23:08:24 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/25 23:08:31 visual_prompt]: Epoch 4 / 100: avg data time: 3.86e-02, avg batch time: 0.4892, average train loss: 7.7887
[09/25 23:08:32 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 7.0337
[09/25 23:08:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 13.00	
[09/25 23:08:32 visual_prompt]: Best epoch 4: best metric: 0.045
[09/25 23:08:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/25 23:08:39 visual_prompt]: Epoch 5 / 100: avg data time: 4.26e-02, avg batch time: 0.4908, average train loss: 15.4972
[09/25 23:08:40 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 23.1816
[09/25 23:08:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 14.00	
[09/25 23:08:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/25 23:08:47 visual_prompt]: Epoch 6 / 100: avg data time: 5.17e-02, avg batch time: 0.5006, average train loss: 32.4510
[09/25 23:08:48 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1689, average loss: 37.1622
[09/25 23:08:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/25 23:08:48 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/25 23:08:55 visual_prompt]: Epoch 7 / 100: avg data time: 5.14e-02, avg batch time: 0.5000, average train loss: 40.4847
[09/25 23:08:57 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1686, average loss: 37.6931
[09/25 23:08:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 23:08:57 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/25 23:09:03 visual_prompt]: Epoch 8 / 100: avg data time: 5.46e-02, avg batch time: 0.5027, average train loss: 59.9391
[09/25 23:09:05 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 67.4845
[09/25 23:09:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.00	
[09/25 23:09:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/25 23:09:11 visual_prompt]: Epoch 9 / 100: avg data time: 5.12e-02, avg batch time: 0.4998, average train loss: 85.7613
[09/25 23:09:13 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1690, average loss: 72.4144
[09/25 23:09:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.00	
[09/25 23:09:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/25 23:09:19 visual_prompt]: Epoch 10 / 100: avg data time: 3.96e-02, avg batch time: 0.4883, average train loss: 96.8407
[09/25 23:09:21 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1692, average loss: 114.2673
[09/25 23:09:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/25 23:09:21 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/25 23:09:28 visual_prompt]: Epoch 11 / 100: avg data time: 5.20e-02, avg batch time: 0.5007, average train loss: 137.1001
[09/25 23:09:29 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1690, average loss: 80.5930
[09/25 23:09:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.00	
[09/25 23:09:29 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/25 23:09:36 visual_prompt]: Epoch 12 / 100: avg data time: 4.40e-02, avg batch time: 0.4933, average train loss: 136.7849
[09/25 23:09:37 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 134.7693
[09/25 23:09:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 23:09:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/25 23:09:44 visual_prompt]: Epoch 13 / 100: avg data time: 5.05e-02, avg batch time: 0.4990, average train loss: 141.4936
[09/25 23:09:45 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1693, average loss: 126.5717
[09/25 23:09:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 17.00	
[09/25 23:09:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/25 23:09:52 visual_prompt]: Epoch 14 / 100: avg data time: 5.95e-02, avg batch time: 0.5072, average train loss: 159.2125
[09/25 23:09:53 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1692, average loss: 104.8989
[09/25 23:09:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 9.50	
[09/25 23:09:53 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/25 23:10:00 visual_prompt]: Epoch 15 / 100: avg data time: 5.43e-02, avg batch time: 0.5031, average train loss: 122.6933
[09/25 23:10:01 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1692, average loss: 148.3395
[09/25 23:10:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 23:10:01 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/25 23:10:08 visual_prompt]: Epoch 16 / 100: avg data time: 5.47e-02, avg batch time: 0.5022, average train loss: 155.6515
[09/25 23:10:10 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 202.7375
[09/25 23:10:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.50	
[09/25 23:10:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/25 23:10:16 visual_prompt]: Epoch 17 / 100: avg data time: 4.20e-02, avg batch time: 0.4924, average train loss: 157.5012
[09/25 23:10:18 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 142.5915
[09/25 23:10:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/25 23:10:18 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/25 23:10:24 visual_prompt]: Epoch 18 / 100: avg data time: 5.42e-02, avg batch time: 0.5040, average train loss: 136.7842
[09/25 23:10:26 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1689, average loss: 117.7903
[09/25 23:10:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/25 23:10:26 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/25 23:10:33 visual_prompt]: Epoch 19 / 100: avg data time: 5.09e-02, avg batch time: 0.4992, average train loss: 152.4730
[09/25 23:10:34 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1694, average loss: 136.6944
[09/25 23:10:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 9.00	
[09/25 23:10:34 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/25 23:10:41 visual_prompt]: Epoch 20 / 100: avg data time: 4.43e-02, avg batch time: 0.4933, average train loss: 128.9354
[09/25 23:10:42 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1686, average loss: 132.9975
[09/25 23:10:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/25 23:10:42 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/25 23:10:49 visual_prompt]: Epoch 21 / 100: avg data time: 5.55e-02, avg batch time: 0.5031, average train loss: 123.5381
[09/25 23:10:50 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1687, average loss: 113.0285
[09/25 23:10:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.00	
[09/25 23:10:50 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/25 23:10:57 visual_prompt]: Epoch 22 / 100: avg data time: 4.87e-02, avg batch time: 0.4965, average train loss: 130.3738
[09/25 23:10:58 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1693, average loss: 135.6013
[09/25 23:10:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 23:10:58 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/25 23:11:05 visual_prompt]: Epoch 23 / 100: avg data time: 3.84e-02, avg batch time: 0.4880, average train loss: 102.1837
[09/25 23:11:06 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1693, average loss: 117.4315
[09/25 23:11:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 16.50	
[09/25 23:11:06 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/25 23:11:13 visual_prompt]: Epoch 24 / 100: avg data time: 5.12e-02, avg batch time: 0.5004, average train loss: 131.3068
[09/25 23:11:14 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1687, average loss: 124.0575
[09/25 23:11:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/25 23:11:14 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/25 23:11:21 visual_prompt]: Epoch 25 / 100: avg data time: 4.06e-02, avg batch time: 0.4885, average train loss: 140.8247
[09/25 23:11:22 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1689, average loss: 147.1260
[09/25 23:11:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/25 23:11:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/25 23:11:29 visual_prompt]: Epoch 26 / 100: avg data time: 5.31e-02, avg batch time: 0.5018, average train loss: 118.4601
[09/25 23:11:30 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 115.2427
[09/25 23:11:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 16.00	
[09/25 23:11:30 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/25 23:11:37 visual_prompt]: Epoch 27 / 100: avg data time: 4.85e-02, avg batch time: 0.4960, average train loss: 130.0803
[09/25 23:11:39 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1691, average loss: 192.2880
[09/25 23:11:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/25 23:11:39 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/25 23:11:45 visual_prompt]: Epoch 28 / 100: avg data time: 4.96e-02, avg batch time: 0.4978, average train loss: 145.1793
[09/25 23:11:47 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1694, average loss: 157.7652
[09/25 23:11:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 23:11:47 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/25 23:11:53 visual_prompt]: Epoch 29 / 100: avg data time: 4.12e-02, avg batch time: 0.4902, average train loss: 160.2548
[09/25 23:11:55 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1692, average loss: 114.0288
[09/25 23:11:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 12.50	
[09/25 23:11:55 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/25 23:12:01 visual_prompt]: Epoch 30 / 100: avg data time: 5.30e-02, avg batch time: 0.5008, average train loss: 136.9561
[09/25 23:12:03 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 122.2402
[09/25 23:12:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 10.50	
[09/25 23:12:03 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/25 23:12:10 visual_prompt]: Epoch 31 / 100: avg data time: 3.87e-02, avg batch time: 0.4901, average train loss: 119.1666
[09/25 23:12:11 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 129.8842
[09/25 23:12:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.00	
[09/25 23:12:11 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/25 23:12:18 visual_prompt]: Epoch 32 / 100: avg data time: 5.27e-02, avg batch time: 0.5012, average train loss: 133.7990
[09/25 23:12:19 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 159.4210
[09/25 23:12:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/25 23:12:19 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/25 23:12:26 visual_prompt]: Epoch 33 / 100: avg data time: 4.16e-02, avg batch time: 0.4925, average train loss: 88.2753
[09/25 23:12:27 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1689, average loss: 95.9857
[09/25 23:12:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.00	
[09/25 23:12:27 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/25 23:12:34 visual_prompt]: Epoch 34 / 100: avg data time: 4.27e-02, avg batch time: 0.4910, average train loss: 94.7515
[09/25 23:12:35 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1694, average loss: 99.2441
[09/25 23:12:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.00	
[09/25 23:12:35 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/25 23:12:42 visual_prompt]: Epoch 35 / 100: avg data time: 4.95e-02, avg batch time: 0.4974, average train loss: 110.3964
[09/25 23:12:43 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1695, average loss: 85.8979
[09/25 23:12:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 16.50	
[09/25 23:12:43 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/25 23:12:50 visual_prompt]: Epoch 36 / 100: avg data time: 3.85e-02, avg batch time: 0.4893, average train loss: 94.2854
[09/25 23:12:51 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1692, average loss: 107.0192
[09/25 23:12:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 15.00	
[09/25 23:12:51 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/25 23:12:58 visual_prompt]: Epoch 37 / 100: avg data time: 5.11e-02, avg batch time: 0.5007, average train loss: 116.8093
[09/25 23:12:59 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1695, average loss: 80.7642
[09/25 23:12:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.50	
[09/25 23:12:59 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/25 23:13:06 visual_prompt]: Epoch 38 / 100: avg data time: 5.12e-02, avg batch time: 0.4991, average train loss: 114.3097
[09/25 23:13:08 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1690, average loss: 101.4165
[09/25 23:13:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.00	
[09/25 23:13:08 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/25 23:13:14 visual_prompt]: Epoch 39 / 100: avg data time: 4.40e-02, avg batch time: 0.4936, average train loss: 100.0677
[09/25 23:13:16 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1690, average loss: 97.6909
[09/25 23:13:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.50	
[09/25 23:13:16 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/25 23:13:22 visual_prompt]: Epoch 40 / 100: avg data time: 4.81e-02, avg batch time: 0.4989, average train loss: 98.8189
[09/25 23:13:24 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.1693, average loss: 79.0780
[09/25 23:13:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 23:13:24 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/25 23:13:31 visual_prompt]: Epoch 41 / 100: avg data time: 5.11e-02, avg batch time: 0.4986, average train loss: 83.7483
[09/25 23:13:32 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1690, average loss: 134.6504
[09/25 23:13:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 23:13:32 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/25 23:13:39 visual_prompt]: Epoch 42 / 100: avg data time: 4.86e-02, avg batch time: 0.4967, average train loss: 94.6485
[09/25 23:13:40 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1693, average loss: 88.6947
[09/25 23:13:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.00	
[09/25 23:13:40 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/25 23:13:47 visual_prompt]: Epoch 43 / 100: avg data time: 5.08e-02, avg batch time: 0.4988, average train loss: 73.0893
[09/25 23:13:48 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1692, average loss: 71.4166
[09/25 23:13:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.50	
[09/25 23:13:48 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/25 23:13:55 visual_prompt]: Epoch 44 / 100: avg data time: 5.49e-02, avg batch time: 0.5025, average train loss: 86.8789
[09/25 23:13:56 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1688, average loss: 98.0810
[09/25 23:13:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.50	
[09/25 23:13:56 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/25 23:14:03 visual_prompt]: Epoch 45 / 100: avg data time: 5.57e-02, avg batch time: 0.5036, average train loss: 98.1381
[09/25 23:14:05 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1694, average loss: 92.4670
[09/25 23:14:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/25 23:14:05 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/25 23:14:11 visual_prompt]: Epoch 46 / 100: avg data time: 5.24e-02, avg batch time: 0.5006, average train loss: 100.3523
[09/25 23:14:13 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1686, average loss: 83.8299
[09/25 23:14:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 15.00	
[09/25 23:14:13 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/25 23:14:19 visual_prompt]: Epoch 47 / 100: avg data time: 4.41e-02, avg batch time: 0.4923, average train loss: 93.1363
[09/25 23:14:21 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 82.2978
[09/25 23:14:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/25 23:14:21 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/25 23:14:27 visual_prompt]: Epoch 48 / 100: avg data time: 3.73e-02, avg batch time: 0.4874, average train loss: 90.7288
[09/25 23:14:29 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1688, average loss: 77.9056
[09/25 23:14:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 10.00	
[09/25 23:14:29 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/25 23:14:35 visual_prompt]: Epoch 49 / 100: avg data time: 5.33e-02, avg batch time: 0.5012, average train loss: 78.0125
[09/25 23:14:37 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1690, average loss: 80.6898
[09/25 23:14:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 23:14:37 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/25 23:14:44 visual_prompt]: Epoch 50 / 100: avg data time: 4.24e-02, avg batch time: 0.4919, average train loss: 88.1323
[09/25 23:14:45 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1688, average loss: 75.6587
[09/25 23:14:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/25 23:14:45 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/25 23:14:52 visual_prompt]: Epoch 51 / 100: avg data time: 5.46e-02, avg batch time: 0.5030, average train loss: 73.8791
[09/25 23:14:53 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1691, average loss: 62.2289
[09/25 23:14:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.00	
[09/25 23:14:53 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/25 23:15:00 visual_prompt]: Epoch 52 / 100: avg data time: 4.77e-02, avg batch time: 0.4963, average train loss: 68.8431
[09/25 23:15:01 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 81.0014
[09/25 23:15:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 23:15:01 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/25 23:15:08 visual_prompt]: Epoch 53 / 100: avg data time: 5.19e-02, avg batch time: 0.5002, average train loss: 73.7255
[09/25 23:15:09 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1694, average loss: 91.4435
[09/25 23:15:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.00	
[09/25 23:15:09 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/25 23:15:16 visual_prompt]: Epoch 54 / 100: avg data time: 5.28e-02, avg batch time: 0.5006, average train loss: 83.0399
[09/25 23:15:17 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1691, average loss: 113.1760
[09/25 23:15:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 7.50	
[09/25 23:15:17 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/25 23:15:24 visual_prompt]: Epoch 55 / 100: avg data time: 5.13e-02, avg batch time: 0.5003, average train loss: 75.8371
[09/25 23:15:25 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.1691, average loss: 52.3497
[09/25 23:15:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.00	
[09/25 23:15:25 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/25 23:15:32 visual_prompt]: Epoch 56 / 100: avg data time: 4.88e-02, avg batch time: 0.4960, average train loss: 58.3168
[09/25 23:15:33 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1688, average loss: 68.3100
[09/25 23:15:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 23:15:33 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/25 23:15:40 visual_prompt]: Epoch 57 / 100: avg data time: 4.08e-02, avg batch time: 0.4897, average train loss: 62.8690
[09/25 23:15:41 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1689, average loss: 56.7513
[09/25 23:15:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 23:15:41 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/25 23:15:48 visual_prompt]: Epoch 58 / 100: avg data time: 3.44e-02, avg batch time: 0.4850, average train loss: 68.9093
[09/25 23:15:49 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1692, average loss: 58.3427
[09/25 23:15:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.00	
[09/25 23:15:49 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/25 23:15:56 visual_prompt]: Epoch 59 / 100: avg data time: 3.78e-02, avg batch time: 0.4858, average train loss: 53.8256
[09/25 23:15:57 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1690, average loss: 32.8297
[09/25 23:15:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/25 23:15:57 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/25 23:16:04 visual_prompt]: Epoch 60 / 100: avg data time: 4.27e-02, avg batch time: 0.4922, average train loss: 52.4970
[09/25 23:16:05 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 61.4143
[09/25 23:16:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 23:16:05 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/25 23:16:12 visual_prompt]: Epoch 61 / 100: avg data time: 3.72e-02, avg batch time: 0.4866, average train loss: 61.0470
[09/25 23:16:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 41.0952
[09/25 23:16:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 6.50	
[09/25 23:16:13 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/25 23:16:20 visual_prompt]: Epoch 62 / 100: avg data time: 3.77e-02, avg batch time: 0.4872, average train loss: 42.8703
[09/25 23:16:21 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1688, average loss: 32.2426
[09/25 23:16:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 23:16:21 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/25 23:16:28 visual_prompt]: Epoch 63 / 100: avg data time: 3.80e-02, avg batch time: 0.4873, average train loss: 35.2905
[09/25 23:16:29 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 33.5885
[09/25 23:16:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 23:16:29 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/25 23:16:36 visual_prompt]: Epoch 64 / 100: avg data time: 4.02e-02, avg batch time: 0.4894, average train loss: 38.5141
[09/25 23:16:37 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 36.7515
[09/25 23:16:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 7.50	
[09/25 23:16:37 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/25 23:16:44 visual_prompt]: Epoch 65 / 100: avg data time: 3.90e-02, avg batch time: 0.4878, average train loss: 33.3556
[09/25 23:16:45 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1698, average loss: 43.2208
[09/25 23:16:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.50	
[09/25 23:16:45 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/25 23:16:52 visual_prompt]: Epoch 66 / 100: avg data time: 4.17e-02, avg batch time: 0.4903, average train loss: 35.9420
[09/25 23:16:53 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1690, average loss: 35.7553
[09/25 23:16:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 23:16:53 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/25 23:17:00 visual_prompt]: Epoch 67 / 100: avg data time: 4.49e-02, avg batch time: 0.4932, average train loss: 30.0022
[09/25 23:17:01 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1691, average loss: 28.2124
[09/25 23:17:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 6.50	
[09/25 23:17:01 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/25 23:17:08 visual_prompt]: Epoch 68 / 100: avg data time: 3.85e-02, avg batch time: 0.4871, average train loss: 26.4403
[09/25 23:17:09 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 29.6112
[09/25 23:17:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 23:17:09 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/25 23:17:16 visual_prompt]: Epoch 69 / 100: avg data time: 4.53e-02, avg batch time: 0.4944, average train loss: 33.0155
[09/25 23:17:17 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 20.9570
[09/25 23:17:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.00	
[09/25 23:17:17 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/25 23:17:24 visual_prompt]: Epoch 70 / 100: avg data time: 4.77e-02, avg batch time: 0.4966, average train loss: 26.3958
[09/25 23:17:25 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 21.5182
[09/25 23:17:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/25 23:17:25 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/25 23:17:32 visual_prompt]: Epoch 71 / 100: avg data time: 3.81e-02, avg batch time: 0.4871, average train loss: 20.8542
[09/25 23:17:33 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 16.4417
[09/25 23:17:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.50	
[09/25 23:17:33 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/25 23:17:40 visual_prompt]: Epoch 72 / 100: avg data time: 4.47e-02, avg batch time: 0.4937, average train loss: 14.4025
[09/25 23:17:41 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1691, average loss: 11.5265
[09/25 23:17:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/25 23:17:41 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/25 23:17:48 visual_prompt]: Epoch 73 / 100: avg data time: 3.92e-02, avg batch time: 0.4891, average train loss: 12.1629
[09/25 23:17:49 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 7.8587
[09/25 23:17:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.50	
[09/25 23:17:49 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/25 23:17:56 visual_prompt]: Epoch 74 / 100: avg data time: 4.90e-02, avg batch time: 0.4980, average train loss: 7.8427
[09/25 23:17:57 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1688, average loss: 6.5259
[09/25 23:17:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.00	
[09/25 23:17:57 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/25 23:18:04 visual_prompt]: Epoch 75 / 100: avg data time: 4.52e-02, avg batch time: 0.4939, average train loss: 5.9850
[09/25 23:18:06 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1690, average loss: 5.1212
[09/25 23:18:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 7.50	
[09/25 23:18:06 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/25 23:18:12 visual_prompt]: Epoch 76 / 100: avg data time: 3.85e-02, avg batch time: 0.4898, average train loss: 4.7374
[09/25 23:18:14 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1694, average loss: 4.8195
[09/25 23:18:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.00	top5: 8.00	
[09/25 23:18:14 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/25 23:18:20 visual_prompt]: Epoch 77 / 100: avg data time: 4.30e-02, avg batch time: 0.4926, average train loss: 4.9546
[09/25 23:18:22 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1689, average loss: 4.4768
[09/25 23:18:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 23:18:22 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/25 23:18:28 visual_prompt]: Epoch 78 / 100: avg data time: 5.42e-02, avg batch time: 0.5018, average train loss: 4.4905
[09/25 23:18:30 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 4.4371
[09/25 23:18:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 23:18:30 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/25 23:18:36 visual_prompt]: Epoch 79 / 100: avg data time: 3.93e-02, avg batch time: 0.4889, average train loss: 4.5677
[09/25 23:18:38 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1693, average loss: 4.5466
[09/25 23:18:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.50	
[09/25 23:18:38 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/25 23:18:44 visual_prompt]: Epoch 80 / 100: avg data time: 3.67e-02, avg batch time: 0.4867, average train loss: 4.4796
[09/25 23:18:46 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1688, average loss: 4.1977
[09/25 23:18:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 23:18:46 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/25 23:18:52 visual_prompt]: Epoch 81 / 100: avg data time: 4.32e-02, avg batch time: 0.4926, average train loss: 4.1501
[09/25 23:18:54 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1689, average loss: 4.0536
[09/25 23:18:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 23:18:54 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/25 23:19:00 visual_prompt]: Epoch 82 / 100: avg data time: 4.32e-02, avg batch time: 0.4929, average train loss: 4.0762
[09/25 23:19:02 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1691, average loss: 4.0194
[09/25 23:19:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/25 23:19:02 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/25 23:19:08 visual_prompt]: Epoch 83 / 100: avg data time: 4.52e-02, avg batch time: 0.4933, average train loss: 4.0232
[09/25 23:19:10 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 4.0369
[09/25 23:19:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.00	
[09/25 23:19:10 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/25 23:19:16 visual_prompt]: Epoch 84 / 100: avg data time: 4.34e-02, avg batch time: 0.4915, average train loss: 4.0052
[09/25 23:19:18 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1692, average loss: 3.9369
[09/25 23:19:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.50	
[09/25 23:19:18 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/25 23:19:25 visual_prompt]: Epoch 85 / 100: avg data time: 4.77e-02, avg batch time: 0.4963, average train loss: 3.9530
[09/25 23:19:26 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1695, average loss: 3.9444
[09/25 23:19:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/25 23:19:26 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/25 23:19:33 visual_prompt]: Epoch 86 / 100: avg data time: 5.24e-02, avg batch time: 0.5012, average train loss: 3.9211
[09/25 23:19:34 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 3.9291
[09/25 23:19:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.00	
[09/25 23:19:34 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/25 23:19:41 visual_prompt]: Epoch 87 / 100: avg data time: 5.01e-02, avg batch time: 0.4987, average train loss: 3.9171
[09/25 23:19:42 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1691, average loss: 3.9025
[09/25 23:19:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.50	
[09/25 23:19:42 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/25 23:19:49 visual_prompt]: Epoch 88 / 100: avg data time: 4.60e-02, avg batch time: 0.4943, average train loss: 3.8485
[09/25 23:19:50 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1694, average loss: 3.9052
[09/25 23:19:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.50	
[09/25 23:19:50 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/25 23:19:57 visual_prompt]: Epoch 89 / 100: avg data time: 4.99e-02, avg batch time: 0.4979, average train loss: 3.7787
[09/25 23:19:58 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 3.7793
[09/25 23:19:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 15.00	
[09/25 23:19:58 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/25 23:20:05 visual_prompt]: Epoch 90 / 100: avg data time: 4.86e-02, avg batch time: 0.4969, average train loss: 3.6738
[09/25 23:20:07 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1695, average loss: 3.6242
[09/25 23:20:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 22.00	
[09/25 23:20:07 visual_prompt]: Best epoch 90: best metric: 0.095
[09/25 23:20:07 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/25 23:20:13 visual_prompt]: Epoch 91 / 100: avg data time: 5.11e-02, avg batch time: 0.4993, average train loss: 3.6412
[09/25 23:20:15 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1692, average loss: 3.7631
[09/25 23:20:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 22.50	
[09/25 23:20:15 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/25 23:20:21 visual_prompt]: Epoch 92 / 100: avg data time: 5.44e-02, avg batch time: 0.5024, average train loss: 3.6175
[09/25 23:20:23 visual_prompt]: Inference (val):avg data time: 5.26e-05, avg batch time: 0.1694, average loss: 3.6505
[09/25 23:20:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 29.50	
[09/25 23:20:23 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/25 23:20:30 visual_prompt]: Epoch 93 / 100: avg data time: 5.06e-02, avg batch time: 0.5003, average train loss: 3.2754
[09/25 23:20:31 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1692, average loss: 3.3887
[09/25 23:20:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.50	top5: 38.00	
[09/25 23:20:31 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/25 23:20:38 visual_prompt]: Epoch 94 / 100: avg data time: 5.21e-02, avg batch time: 0.5010, average train loss: 2.9035
[09/25 23:20:39 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 3.0421
[09/25 23:20:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.50	top5: 48.50	
[09/25 23:20:39 visual_prompt]: Best epoch 94: best metric: 0.235
[09/25 23:20:39 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/25 23:20:46 visual_prompt]: Epoch 95 / 100: avg data time: 3.89e-02, avg batch time: 0.4888, average train loss: 2.2441
[09/25 23:20:47 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1695, average loss: 2.5447
[09/25 23:20:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 28.00	top5: 67.00	
[09/25 23:20:47 visual_prompt]: Best epoch 95: best metric: 0.280
[09/25 23:20:47 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/25 23:20:54 visual_prompt]: Epoch 96 / 100: avg data time: 4.38e-02, avg batch time: 0.4923, average train loss: 1.6406
[09/25 23:20:55 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1696, average loss: 2.2950
[09/25 23:20:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.00	top5: 76.50	
[09/25 23:20:55 visual_prompt]: Best epoch 96: best metric: 0.370
[09/25 23:20:55 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/25 23:21:02 visual_prompt]: Epoch 97 / 100: avg data time: 4.85e-02, avg batch time: 0.4985, average train loss: 1.2630
[09/25 23:21:03 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1690, average loss: 2.0918
[09/25 23:21:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 78.00	
[09/25 23:21:03 visual_prompt]: Best epoch 97: best metric: 0.415
[09/25 23:21:03 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/25 23:21:10 visual_prompt]: Epoch 98 / 100: avg data time: 3.59e-02, avg batch time: 0.4872, average train loss: 0.9899
[09/25 23:21:11 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1693, average loss: 2.0436
[09/25 23:21:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 78.00	
[09/25 23:21:11 visual_prompt]: Best epoch 98: best metric: 0.435
[09/25 23:21:11 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/25 23:21:18 visual_prompt]: Epoch 99 / 100: avg data time: 4.89e-02, avg batch time: 0.4980, average train loss: 0.8430
[09/25 23:21:19 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 1.9781
[09/25 23:21:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 80.00	
[09/25 23:21:19 visual_prompt]: Best epoch 99: best metric: 0.450
[09/25 23:21:19 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/25 23:21:26 visual_prompt]: Epoch 100 / 100: avg data time: 5.55e-02, avg batch time: 0.5040, average train loss: 0.7726
[09/25 23:21:27 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1691, average loss: 1.9737
[09/25 23:21:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 81.00	
[09/25 23:21:28 visual_prompt]: Rank of current process: 0. World size: 1
[09/25 23:21:28 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/25 23:21:28 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/25 23:21:28 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/25 23:21:28 visual_prompt]: Training with config:
[09/25 23:21:28 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/25 23:21:28 visual_prompt]: Loading training data...
[09/25 23:21:28 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 23:21:30 visual_prompt]: Number of images: 800
[09/25 23:21:30 visual_prompt]: Number of classes: 47 / 47
[09/25 23:21:30 visual_prompt]: Loading validation data...
[09/25 23:21:30 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 23:21:30 visual_prompt]: Number of images: 200
[09/25 23:21:30 visual_prompt]: Number of classes: 47 / 47
[09/25 23:21:30 visual_prompt]: Constructing models...
[09/25 23:21:33 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/25 23:21:33 visual_prompt]: tuned percent:0.576
[09/25 23:21:33 visual_prompt]: Device used for model: 0
[09/25 23:21:33 visual_prompt]: Setting up Evaluator...
[09/25 23:21:33 visual_prompt]: Setting up Trainer...
[09/25 23:21:33 visual_prompt]: 	Setting up the optimizer...
[09/25 23:21:33 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/25 23:21:39 visual_prompt]: Epoch 1 / 100: avg data time: 4.82e-02, avg batch time: 0.4959, average train loss: 3.9218
[09/25 23:21:41 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1692, average loss: 3.9045
[09/25 23:21:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 23:21:41 visual_prompt]: Best epoch 1: best metric: 0.040
[09/25 23:21:41 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/25 23:21:48 visual_prompt]: Epoch 2 / 100: avg data time: 4.52e-02, avg batch time: 0.4939, average train loss: 4.2750
[09/25 23:21:49 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1688, average loss: 4.2522
[09/25 23:21:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 9.50	
[09/25 23:21:49 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/25 23:21:56 visual_prompt]: Epoch 3 / 100: avg data time: 4.46e-02, avg batch time: 0.4914, average train loss: 4.2752
[09/25 23:21:57 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1690, average loss: 4.8278
[09/25 23:21:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 13.00	
[09/25 23:21:57 visual_prompt]: Best epoch 3: best metric: 0.060
[09/25 23:21:57 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/25 23:22:04 visual_prompt]: Epoch 4 / 100: avg data time: 3.41e-02, avg batch time: 0.4821, average train loss: 4.6703
[09/25 23:22:05 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1690, average loss: 4.4519
[09/25 23:22:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.00	top5: 21.50	
[09/25 23:22:05 visual_prompt]: Best epoch 4: best metric: 0.090
[09/25 23:22:05 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/25 23:22:11 visual_prompt]: Epoch 5 / 100: avg data time: 3.84e-02, avg batch time: 0.4867, average train loss: 6.8652
[09/25 23:22:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1689, average loss: 6.8581
[09/25 23:22:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/25 23:22:13 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/25 23:22:20 visual_prompt]: Epoch 6 / 100: avg data time: 5.39e-02, avg batch time: 0.5017, average train loss: 20.1334
[09/25 23:22:21 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1688, average loss: 57.4504
[09/25 23:22:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 23:22:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/25 23:22:28 visual_prompt]: Epoch 7 / 100: avg data time: 3.77e-02, avg batch time: 0.4848, average train loss: 44.4017
[09/25 23:22:29 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 36.5317
[09/25 23:22:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.00	
[09/25 23:22:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/25 23:22:36 visual_prompt]: Epoch 8 / 100: avg data time: 3.57e-02, avg batch time: 0.4835, average train loss: 66.3922
[09/25 23:22:37 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1687, average loss: 77.2601
[09/25 23:22:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 23:22:37 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/25 23:22:44 visual_prompt]: Epoch 9 / 100: avg data time: 4.82e-02, avg batch time: 0.4960, average train loss: 94.6065
[09/25 23:22:45 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1688, average loss: 88.9659
[09/25 23:22:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.50	
[09/25 23:22:45 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/25 23:22:52 visual_prompt]: Epoch 10 / 100: avg data time: 5.27e-02, avg batch time: 0.5008, average train loss: 113.5740
[09/25 23:22:53 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1686, average loss: 119.0798
[09/25 23:22:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.50	
[09/25 23:22:53 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/25 23:23:00 visual_prompt]: Epoch 11 / 100: avg data time: 4.31e-02, avg batch time: 0.4919, average train loss: 129.0967
[09/25 23:23:01 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1687, average loss: 122.7774
[09/25 23:23:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 23:23:01 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/25 23:23:08 visual_prompt]: Epoch 12 / 100: avg data time: 3.79e-02, avg batch time: 0.4894, average train loss: 135.5990
[09/25 23:23:09 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1690, average loss: 177.2360
[09/25 23:23:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.00	
[09/25 23:23:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/25 23:23:16 visual_prompt]: Epoch 13 / 100: avg data time: 4.70e-02, avg batch time: 0.4955, average train loss: 140.8471
[09/25 23:23:17 visual_prompt]: Inference (val):avg data time: 5.41e-05, avg batch time: 0.1692, average loss: 131.8372
[09/25 23:23:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/25 23:23:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/25 23:23:24 visual_prompt]: Epoch 14 / 100: avg data time: 3.76e-02, avg batch time: 0.4862, average train loss: 139.1436
[09/25 23:23:25 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1692, average loss: 122.2892
[09/25 23:23:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.00	
[09/25 23:23:25 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/25 23:23:32 visual_prompt]: Epoch 15 / 100: avg data time: 5.35e-02, avg batch time: 0.5027, average train loss: 96.7775
[09/25 23:23:33 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1691, average loss: 187.2400
[09/25 23:23:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 23:23:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/25 23:23:40 visual_prompt]: Epoch 16 / 100: avg data time: 4.39e-02, avg batch time: 0.4931, average train loss: 164.4745
[09/25 23:23:41 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1692, average loss: 172.8927
[09/25 23:23:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 23:23:41 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/25 23:23:48 visual_prompt]: Epoch 17 / 100: avg data time: 4.67e-02, avg batch time: 0.4981, average train loss: 144.6452
[09/25 23:23:50 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1694, average loss: 145.2361
[09/25 23:23:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/25 23:23:50 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/25 23:23:56 visual_prompt]: Epoch 18 / 100: avg data time: 5.29e-02, avg batch time: 0.5025, average train loss: 177.9967
[09/25 23:23:58 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 158.7112
[09/25 23:23:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/25 23:23:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/25 23:24:04 visual_prompt]: Epoch 19 / 100: avg data time: 4.30e-02, avg batch time: 0.4930, average train loss: 163.3396
[09/25 23:24:06 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1694, average loss: 162.1754
[09/25 23:24:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.00	
[09/25 23:24:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/25 23:24:12 visual_prompt]: Epoch 20 / 100: avg data time: 3.70e-02, avg batch time: 0.4847, average train loss: 158.1010
[09/25 23:24:14 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1689, average loss: 129.3321
[09/25 23:24:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 23:24:14 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/25 23:24:20 visual_prompt]: Epoch 21 / 100: avg data time: 4.84e-02, avg batch time: 0.4967, average train loss: 126.1707
[09/25 23:24:22 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 136.8964
[09/25 23:24:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.50	
[09/25 23:24:22 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/25 23:24:29 visual_prompt]: Epoch 22 / 100: avg data time: 4.68e-02, avg batch time: 0.4969, average train loss: 119.1225
[09/25 23:24:30 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1692, average loss: 100.9823
[09/25 23:24:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.50	
[09/25 23:24:30 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/25 23:24:37 visual_prompt]: Epoch 23 / 100: avg data time: 4.83e-02, avg batch time: 0.4965, average train loss: 117.0886
[09/25 23:24:38 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 130.0007
[09/25 23:24:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 23:24:38 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/25 23:24:45 visual_prompt]: Epoch 24 / 100: avg data time: 5.18e-02, avg batch time: 0.4998, average train loss: 116.3287
[09/25 23:24:46 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1689, average loss: 90.1824
[09/25 23:24:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 23:24:46 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/25 23:24:53 visual_prompt]: Epoch 25 / 100: avg data time: 5.65e-02, avg batch time: 0.5058, average train loss: 122.3279
[09/25 23:24:54 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1689, average loss: 170.2275
[09/25 23:24:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/25 23:24:54 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/25 23:25:01 visual_prompt]: Epoch 26 / 100: avg data time: 5.70e-02, avg batch time: 0.5048, average train loss: 130.5474
[09/25 23:25:02 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1686, average loss: 151.2959
[09/25 23:25:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.50	
[09/25 23:25:02 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/25 23:25:09 visual_prompt]: Epoch 27 / 100: avg data time: 4.15e-02, avg batch time: 0.4894, average train loss: 152.1349
[09/25 23:25:10 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1688, average loss: 136.8227
[09/25 23:25:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.50	
[09/25 23:25:10 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/25 23:25:17 visual_prompt]: Epoch 28 / 100: avg data time: 4.80e-02, avg batch time: 0.4956, average train loss: 135.2197
[09/25 23:25:19 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1690, average loss: 131.2703
[09/25 23:25:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 10.50	
[09/25 23:25:19 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/25 23:25:25 visual_prompt]: Epoch 29 / 100: avg data time: 4.66e-02, avg batch time: 0.4938, average train loss: 116.2376
[09/25 23:25:27 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1691, average loss: 104.5935
[09/25 23:25:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.00	
[09/25 23:25:27 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/25 23:25:33 visual_prompt]: Epoch 30 / 100: avg data time: 4.47e-02, avg batch time: 0.4942, average train loss: 101.3696
[09/25 23:25:35 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1690, average loss: 93.4036
[09/25 23:25:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.00	
[09/25 23:25:35 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/25 23:25:41 visual_prompt]: Epoch 31 / 100: avg data time: 3.57e-02, avg batch time: 0.4837, average train loss: 96.3903
[09/25 23:25:43 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 98.6184
[09/25 23:25:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 23:25:43 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/25 23:25:49 visual_prompt]: Epoch 32 / 100: avg data time: 3.75e-02, avg batch time: 0.4875, average train loss: 115.7994
[09/25 23:25:50 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1689, average loss: 180.5374
[09/25 23:25:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.50	
[09/25 23:25:50 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/25 23:25:57 visual_prompt]: Epoch 33 / 100: avg data time: 3.91e-02, avg batch time: 0.4876, average train loss: 139.3340
[09/25 23:25:59 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1692, average loss: 78.0366
[09/25 23:25:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 13.00	
[09/25 23:25:59 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/25 23:26:05 visual_prompt]: Epoch 34 / 100: avg data time: 4.82e-02, avg batch time: 0.4966, average train loss: 85.1938
[09/25 23:26:07 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1692, average loss: 98.9033
[09/25 23:26:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 23:26:07 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/25 23:26:13 visual_prompt]: Epoch 35 / 100: avg data time: 4.52e-02, avg batch time: 0.4939, average train loss: 105.3134
[09/25 23:26:14 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 122.6678
[09/25 23:26:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 23:26:15 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/25 23:26:21 visual_prompt]: Epoch 36 / 100: avg data time: 3.66e-02, avg batch time: 0.4882, average train loss: 141.2733
[09/25 23:26:22 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 111.5802
[09/25 23:26:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 14.50	
[09/25 23:26:22 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/25 23:26:29 visual_prompt]: Epoch 37 / 100: avg data time: 3.75e-02, avg batch time: 0.4883, average train loss: 114.2461
[09/25 23:26:30 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 104.8587
[09/25 23:26:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/25 23:26:30 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/25 23:26:37 visual_prompt]: Epoch 38 / 100: avg data time: 4.83e-02, avg batch time: 0.4964, average train loss: 102.0000
[09/25 23:26:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1692, average loss: 110.2344
[09/25 23:26:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.50	
[09/25 23:26:39 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/25 23:26:45 visual_prompt]: Epoch 39 / 100: avg data time: 3.82e-02, avg batch time: 0.4870, average train loss: 91.7777
[09/25 23:26:46 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 92.7627
[09/25 23:26:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 23:26:46 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/25 23:26:53 visual_prompt]: Epoch 40 / 100: avg data time: 3.97e-02, avg batch time: 0.4895, average train loss: 95.7398
[09/25 23:26:54 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 91.7816
[09/25 23:26:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 23:26:54 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/25 23:27:01 visual_prompt]: Epoch 41 / 100: avg data time: 4.52e-02, avg batch time: 0.4936, average train loss: 86.8024
[09/25 23:27:02 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 97.6365
[09/25 23:27:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/25 23:27:02 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/25 23:27:09 visual_prompt]: Epoch 42 / 100: avg data time: 4.48e-02, avg batch time: 0.4938, average train loss: 86.5534
[09/25 23:27:10 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1689, average loss: 75.1613
[09/25 23:27:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.00	
[09/25 23:27:10 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/25 23:27:17 visual_prompt]: Epoch 43 / 100: avg data time: 3.86e-02, avg batch time: 0.4866, average train loss: 81.1193
[09/25 23:27:18 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1689, average loss: 68.2810
[09/25 23:27:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/25 23:27:18 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/25 23:27:25 visual_prompt]: Epoch 44 / 100: avg data time: 5.14e-02, avg batch time: 0.4997, average train loss: 69.5518
[09/25 23:27:27 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1693, average loss: 77.2286
[09/25 23:27:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 23:27:27 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/25 23:27:33 visual_prompt]: Epoch 45 / 100: avg data time: 4.29e-02, avg batch time: 0.4920, average train loss: 77.0095
[09/25 23:27:35 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1694, average loss: 86.8128
[09/25 23:27:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.00	
[09/25 23:27:35 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/25 23:27:41 visual_prompt]: Epoch 46 / 100: avg data time: 4.58e-02, avg batch time: 0.4952, average train loss: 74.5503
[09/25 23:27:43 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1692, average loss: 68.5277
[09/25 23:27:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 13.50	
[09/25 23:27:43 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/25 23:27:49 visual_prompt]: Epoch 47 / 100: avg data time: 4.15e-02, avg batch time: 0.4902, average train loss: 66.6306
[09/25 23:27:51 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 79.2306
[09/25 23:27:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 23:27:51 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/25 23:27:57 visual_prompt]: Epoch 48 / 100: avg data time: 5.45e-02, avg batch time: 0.5021, average train loss: 72.3266
[09/25 23:27:59 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 60.8649
[09/25 23:27:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 23:27:59 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/25 23:28:05 visual_prompt]: Epoch 49 / 100: avg data time: 4.47e-02, avg batch time: 0.4931, average train loss: 64.6959
[09/25 23:28:07 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 58.2582
[09/25 23:28:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.50	
[09/25 23:28:07 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/25 23:28:13 visual_prompt]: Epoch 50 / 100: avg data time: 3.69e-02, avg batch time: 0.4866, average train loss: 68.2164
[09/25 23:28:15 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1688, average loss: 76.0008
[09/25 23:28:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/25 23:28:15 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/25 23:28:21 visual_prompt]: Epoch 51 / 100: avg data time: 4.15e-02, avg batch time: 0.4926, average train loss: 75.9229
[09/25 23:28:23 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1690, average loss: 69.8888
[09/25 23:28:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 23:28:23 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/25 23:28:29 visual_prompt]: Epoch 52 / 100: avg data time: 4.14e-02, avg batch time: 0.4916, average train loss: 63.6832
[09/25 23:28:31 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 70.8222
[09/25 23:28:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/25 23:28:31 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/25 23:28:38 visual_prompt]: Epoch 53 / 100: avg data time: 5.42e-02, avg batch time: 0.5020, average train loss: 66.9675
[09/25 23:28:39 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1690, average loss: 60.8167
[09/25 23:28:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 10.50	
[09/25 23:28:39 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/25 23:28:46 visual_prompt]: Epoch 54 / 100: avg data time: 4.32e-02, avg batch time: 0.4909, average train loss: 54.7859
[09/25 23:28:47 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1688, average loss: 56.2241
[09/25 23:28:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 23:28:47 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/25 23:28:54 visual_prompt]: Epoch 55 / 100: avg data time: 4.73e-02, avg batch time: 0.4951, average train loss: 61.1913
[09/25 23:28:55 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 61.6447
[09/25 23:28:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 15.00	
[09/25 23:28:55 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/25 23:29:02 visual_prompt]: Epoch 56 / 100: avg data time: 5.24e-02, avg batch time: 0.5007, average train loss: 53.4326
[09/25 23:29:03 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1689, average loss: 49.6066
[09/25 23:29:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.00	
[09/25 23:29:03 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/25 23:29:10 visual_prompt]: Epoch 57 / 100: avg data time: 5.44e-02, avg batch time: 0.5020, average train loss: 47.3789
[09/25 23:29:11 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1689, average loss: 43.9836
[09/25 23:29:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 23:29:11 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/25 23:29:18 visual_prompt]: Epoch 58 / 100: avg data time: 4.64e-02, avg batch time: 0.4950, average train loss: 44.8332
[09/25 23:29:19 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 48.0514
[09/25 23:29:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 23:29:19 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/25 23:29:26 visual_prompt]: Epoch 59 / 100: avg data time: 4.76e-02, avg batch time: 0.4963, average train loss: 42.3906
[09/25 23:29:27 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1689, average loss: 39.4006
[09/25 23:29:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 13.50	
[09/25 23:29:27 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/25 23:29:34 visual_prompt]: Epoch 60 / 100: avg data time: 5.13e-02, avg batch time: 0.4994, average train loss: 36.6968
[09/25 23:29:36 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1688, average loss: 28.4180
[09/25 23:29:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 23:29:36 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/25 23:29:42 visual_prompt]: Epoch 61 / 100: avg data time: 5.48e-02, avg batch time: 0.5034, average train loss: 29.1770
[09/25 23:29:44 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1687, average loss: 32.0671
[09/25 23:29:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/25 23:29:44 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/25 23:29:50 visual_prompt]: Epoch 62 / 100: avg data time: 5.22e-02, avg batch time: 0.5007, average train loss: 25.8000
[09/25 23:29:52 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1690, average loss: 16.1403
[09/25 23:29:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.50	
[09/25 23:29:52 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/25 23:29:59 visual_prompt]: Epoch 63 / 100: avg data time: 5.00e-02, avg batch time: 0.4979, average train loss: 15.1332
[09/25 23:30:00 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1687, average loss: 15.9368
[09/25 23:30:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/25 23:30:00 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/25 23:30:07 visual_prompt]: Epoch 64 / 100: avg data time: 4.68e-02, avg batch time: 0.4957, average train loss: 16.1689
[09/25 23:30:08 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 13.4927
[09/25 23:30:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 23:30:08 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/25 23:30:15 visual_prompt]: Epoch 65 / 100: avg data time: 5.45e-02, avg batch time: 0.5027, average train loss: 10.5896
[09/25 23:30:16 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 8.4525
[09/25 23:30:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/25 23:30:16 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/25 23:30:23 visual_prompt]: Epoch 66 / 100: avg data time: 5.68e-02, avg batch time: 0.5058, average train loss: 5.9193
[09/25 23:30:24 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1690, average loss: 5.6645
[09/25 23:30:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/25 23:30:24 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/25 23:30:31 visual_prompt]: Epoch 67 / 100: avg data time: 4.86e-02, avg batch time: 0.4969, average train loss: 4.7797
[09/25 23:30:33 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 5.0850
[09/25 23:30:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 16.00	
[09/25 23:30:33 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/25 23:30:39 visual_prompt]: Epoch 68 / 100: avg data time: 5.44e-02, avg batch time: 0.5032, average train loss: 4.5469
[09/25 23:30:41 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 4.5061
[09/25 23:30:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 21.00	
[09/25 23:30:41 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/25 23:30:47 visual_prompt]: Epoch 69 / 100: avg data time: 3.89e-02, avg batch time: 0.4873, average train loss: 4.2636
[09/25 23:30:49 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1687, average loss: 4.1152
[09/25 23:30:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 22.00	
[09/25 23:30:49 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/25 23:30:55 visual_prompt]: Epoch 70 / 100: avg data time: 5.37e-02, avg batch time: 0.5020, average train loss: 3.7760
[09/25 23:30:57 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 3.9540
[09/25 23:30:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.50	top5: 23.00	
[09/25 23:30:57 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/25 23:31:03 visual_prompt]: Epoch 71 / 100: avg data time: 4.81e-02, avg batch time: 0.4984, average train loss: 3.5061
[09/25 23:31:05 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1690, average loss: 3.7806
[09/25 23:31:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.50	top5: 25.50	
[09/25 23:31:05 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/25 23:31:12 visual_prompt]: Epoch 72 / 100: avg data time: 5.17e-02, avg batch time: 0.5000, average train loss: 3.3771
[09/25 23:31:13 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 3.6324
[09/25 23:31:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.00	top5: 33.00	
[09/25 23:31:13 visual_prompt]: Best epoch 72: best metric: 0.100
[09/25 23:31:13 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/25 23:31:20 visual_prompt]: Epoch 73 / 100: avg data time: 5.28e-02, avg batch time: 0.5025, average train loss: 3.3228
[09/25 23:31:21 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1693, average loss: 3.4169
[09/25 23:31:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.50	top5: 36.50	
[09/25 23:31:21 visual_prompt]: Best epoch 73: best metric: 0.135
[09/25 23:31:21 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/25 23:31:28 visual_prompt]: Epoch 74 / 100: avg data time: 3.63e-02, avg batch time: 0.4855, average train loss: 2.9185
[09/25 23:31:29 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 3.1702
[09/25 23:31:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 18.00	top5: 43.50	
[09/25 23:31:29 visual_prompt]: Best epoch 74: best metric: 0.180
[09/25 23:31:29 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/25 23:31:36 visual_prompt]: Epoch 75 / 100: avg data time: 5.23e-02, avg batch time: 0.5013, average train loss: 2.4824
[09/25 23:31:37 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1693, average loss: 3.2381
[09/25 23:31:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.50	top5: 48.50	
[09/25 23:31:37 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/25 23:31:44 visual_prompt]: Epoch 76 / 100: avg data time: 5.14e-02, avg batch time: 0.5002, average train loss: 2.2721
[09/25 23:31:45 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 2.8320
[09/25 23:31:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 28.50	top5: 58.00	
[09/25 23:31:45 visual_prompt]: Best epoch 76: best metric: 0.285
[09/25 23:31:45 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/25 23:31:52 visual_prompt]: Epoch 77 / 100: avg data time: 5.69e-02, avg batch time: 0.5054, average train loss: 1.9324
[09/25 23:31:53 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1688, average loss: 3.0450
[09/25 23:31:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 59.00	
[09/25 23:31:53 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/25 23:32:00 visual_prompt]: Epoch 78 / 100: avg data time: 5.02e-02, avg batch time: 0.4989, average train loss: 1.5911
[09/25 23:32:02 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1696, average loss: 2.5591
[09/25 23:32:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 36.50	top5: 69.50	
[09/25 23:32:02 visual_prompt]: Best epoch 78: best metric: 0.365
[09/25 23:32:02 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/25 23:32:08 visual_prompt]: Epoch 79 / 100: avg data time: 5.51e-02, avg batch time: 0.5035, average train loss: 1.2044
[09/25 23:32:10 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 2.5792
[09/25 23:32:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.50	top5: 71.50	
[09/25 23:32:10 visual_prompt]: Best epoch 79: best metric: 0.375
[09/25 23:32:10 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/25 23:32:16 visual_prompt]: Epoch 80 / 100: avg data time: 5.17e-02, avg batch time: 0.5014, average train loss: 0.9439
[09/25 23:32:18 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 2.3967
[09/25 23:32:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 79.50	
[09/25 23:32:18 visual_prompt]: Best epoch 80: best metric: 0.425
[09/25 23:32:18 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/25 23:32:24 visual_prompt]: Epoch 81 / 100: avg data time: 3.94e-02, avg batch time: 0.4881, average train loss: 0.5943
[09/25 23:32:26 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1689, average loss: 2.3664
[09/25 23:32:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 83.50	
[09/25 23:32:26 visual_prompt]: Best epoch 81: best metric: 0.455
[09/25 23:32:26 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/25 23:32:32 visual_prompt]: Epoch 82 / 100: avg data time: 5.05e-02, avg batch time: 0.4988, average train loss: 0.3108
[09/25 23:32:34 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1690, average loss: 2.3863
[09/25 23:32:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 80.50	
[09/25 23:32:34 visual_prompt]: Best epoch 82: best metric: 0.485
[09/25 23:32:34 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/25 23:32:41 visual_prompt]: Epoch 83 / 100: avg data time: 4.93e-02, avg batch time: 0.4988, average train loss: 0.1807
[09/25 23:32:42 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 2.3029
[09/25 23:32:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 82.50	
[09/25 23:32:42 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/25 23:32:49 visual_prompt]: Epoch 84 / 100: avg data time: 5.62e-02, avg batch time: 0.5044, average train loss: 0.0904
[09/25 23:32:50 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1694, average loss: 2.3729
[09/25 23:32:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.00	
[09/25 23:32:50 visual_prompt]: Best epoch 84: best metric: 0.515
[09/25 23:32:50 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/25 23:32:57 visual_prompt]: Epoch 85 / 100: avg data time: 5.82e-02, avg batch time: 0.5077, average train loss: 0.0516
[09/25 23:32:58 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 2.8071
[09/25 23:32:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 85.50	
[09/25 23:32:58 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/25 23:33:05 visual_prompt]: Epoch 86 / 100: avg data time: 3.75e-02, avg batch time: 0.4885, average train loss: 0.0337
[09/25 23:33:06 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1696, average loss: 2.5563
[09/25 23:33:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 86.00	
[09/25 23:33:06 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/25 23:33:13 visual_prompt]: Epoch 87 / 100: avg data time: 5.56e-02, avg batch time: 0.5047, average train loss: 0.0115
[09/25 23:33:14 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 2.4658
[09/25 23:33:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 87.00	
[09/25 23:33:14 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/25 23:33:21 visual_prompt]: Epoch 88 / 100: avg data time: 5.23e-02, avg batch time: 0.5008, average train loss: 0.0077
[09/25 23:33:23 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1692, average loss: 2.4750
[09/25 23:33:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 85.50	
[09/25 23:33:23 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/25 23:33:29 visual_prompt]: Epoch 89 / 100: avg data time: 4.93e-02, avg batch time: 0.4997, average train loss: 0.0065
[09/25 23:33:31 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1695, average loss: 2.4961
[09/25 23:33:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 86.50	
[09/25 23:33:31 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/25 23:33:37 visual_prompt]: Epoch 90 / 100: avg data time: 5.74e-02, avg batch time: 0.5063, average train loss: 0.0101
[09/25 23:33:39 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 2.3778
[09/25 23:33:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 85.50	
[09/25 23:33:39 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/25 23:33:46 visual_prompt]: Epoch 91 / 100: avg data time: 5.13e-02, avg batch time: 0.4999, average train loss: 0.0082
[09/25 23:33:47 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 2.3744
[09/25 23:33:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 86.00	
[09/25 23:33:47 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/25 23:33:54 visual_prompt]: Epoch 92 / 100: avg data time: 5.09e-02, avg batch time: 0.4995, average train loss: 0.0061
[09/25 23:33:55 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 2.3952
[09/25 23:33:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 85.50	
[09/25 23:33:55 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/25 23:34:02 visual_prompt]: Epoch 93 / 100: avg data time: 3.82e-02, avg batch time: 0.4876, average train loss: 0.0054
[09/25 23:34:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 2.3961
[09/25 23:34:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 85.50	
[09/25 23:34:03 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/25 23:34:10 visual_prompt]: Epoch 94 / 100: avg data time: 5.20e-02, avg batch time: 0.5004, average train loss: 0.0055
[09/25 23:34:11 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 2.3861
[09/25 23:34:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 86.00	
[09/25 23:34:11 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/25 23:34:18 visual_prompt]: Epoch 95 / 100: avg data time: 5.88e-02, avg batch time: 0.5073, average train loss: 0.0055
[09/25 23:34:19 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 2.3824
[09/25 23:34:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 86.00	
[09/25 23:34:19 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/25 23:34:26 visual_prompt]: Epoch 96 / 100: avg data time: 5.58e-02, avg batch time: 0.5051, average train loss: 0.0049
[09/25 23:34:27 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1697, average loss: 2.3799
[09/25 23:34:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 86.00	
[09/25 23:34:27 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/25 23:34:34 visual_prompt]: Epoch 97 / 100: avg data time: 5.74e-02, avg batch time: 0.5067, average train loss: 0.0052
[09/25 23:34:36 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1693, average loss: 2.3801
[09/25 23:34:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 86.00	
[09/25 23:34:36 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/25 23:34:42 visual_prompt]: Epoch 98 / 100: avg data time: 5.34e-02, avg batch time: 0.5030, average train loss: 0.0049
[09/25 23:34:44 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1693, average loss: 2.3797
[09/25 23:34:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 86.00	
[09/25 23:34:44 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/25 23:34:50 visual_prompt]: Epoch 99 / 100: avg data time: 4.19e-02, avg batch time: 0.4928, average train loss: 0.0053
[09/25 23:34:52 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 2.3795
[09/25 23:34:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 86.00	
[09/25 23:34:52 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/25 23:34:59 visual_prompt]: Epoch 100 / 100: avg data time: 5.16e-02, avg batch time: 0.5017, average train loss: 0.0051
[09/25 23:35:00 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1696, average loss: 2.3794
[09/25 23:35:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 86.00	
[09/25 23:35:00 visual_prompt]: Rank of current process: 0. World size: 1
[09/25 23:35:00 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/25 23:35:00 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/25 23:35:00 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/25 23:35:00 visual_prompt]: Training with config:
[09/25 23:35:00 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/25 23:35:00 visual_prompt]: Loading training data...
[09/25 23:35:00 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 23:35:02 visual_prompt]: Number of images: 800
[09/25 23:35:02 visual_prompt]: Number of classes: 47 / 47
[09/25 23:35:02 visual_prompt]: Loading validation data...
[09/25 23:35:02 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 23:35:03 visual_prompt]: Number of images: 200
[09/25 23:35:03 visual_prompt]: Number of classes: 47 / 47
[09/25 23:35:03 visual_prompt]: Constructing models...
[09/25 23:35:05 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/25 23:35:05 visual_prompt]: tuned percent:0.576
[09/25 23:35:05 visual_prompt]: Device used for model: 0
[09/25 23:35:05 visual_prompt]: Setting up Evaluator...
[09/25 23:35:05 visual_prompt]: Setting up Trainer...
[09/25 23:35:05 visual_prompt]: 	Setting up the optimizer...
[09/25 23:35:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/25 23:35:12 visual_prompt]: Epoch 1 / 100: avg data time: 5.14e-02, avg batch time: 0.4986, average train loss: 3.9290
[09/25 23:35:13 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1690, average loss: 3.9045
[09/25 23:35:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 23:35:13 visual_prompt]: Best epoch 1: best metric: 0.040
[09/25 23:35:13 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/25 23:35:20 visual_prompt]: Epoch 2 / 100: avg data time: 4.96e-02, avg batch time: 0.4968, average train loss: 4.2796
[09/25 23:35:22 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1685, average loss: 4.4777
[09/25 23:35:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 23:35:22 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/25 23:35:28 visual_prompt]: Epoch 3 / 100: avg data time: 3.83e-02, avg batch time: 0.4886, average train loss: 4.5277
[09/25 23:35:30 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1690, average loss: 4.4326
[09/25 23:35:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 13.00	
[09/25 23:35:30 visual_prompt]: Best epoch 3: best metric: 0.065
[09/25 23:35:30 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/25 23:35:36 visual_prompt]: Epoch 4 / 100: avg data time: 3.79e-02, avg batch time: 0.4865, average train loss: 4.7470
[09/25 23:35:37 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1690, average loss: 4.7523
[09/25 23:35:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.50	top5: 15.00	
[09/25 23:35:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/25 23:35:44 visual_prompt]: Epoch 5 / 100: avg data time: 4.11e-02, avg batch time: 0.4906, average train loss: 5.4456
[09/25 23:35:45 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1690, average loss: 5.8313
[09/25 23:35:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 29.50	
[09/25 23:35:45 visual_prompt]: Best epoch 5: best metric: 0.095
[09/25 23:35:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/25 23:35:52 visual_prompt]: Epoch 6 / 100: avg data time: 4.95e-02, avg batch time: 0.4979, average train loss: 9.3813
[09/25 23:35:54 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1690, average loss: 9.8127
[09/25 23:35:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 22.50	
[09/25 23:35:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/25 23:36:00 visual_prompt]: Epoch 7 / 100: avg data time: 5.31e-02, avg batch time: 0.5010, average train loss: 21.8404
[09/25 23:36:02 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 23.5005
[09/25 23:36:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 17.00	
[09/25 23:36:02 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/25 23:36:08 visual_prompt]: Epoch 8 / 100: avg data time: 5.03e-02, avg batch time: 0.4995, average train loss: 39.8211
[09/25 23:36:10 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 60.3641
[09/25 23:36:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/25 23:36:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/25 23:36:16 visual_prompt]: Epoch 9 / 100: avg data time: 5.30e-02, avg batch time: 0.4994, average train loss: 114.5062
[09/25 23:36:18 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1692, average loss: 129.1668
[09/25 23:36:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.00	
[09/25 23:36:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/25 23:36:25 visual_prompt]: Epoch 10 / 100: avg data time: 4.76e-02, avg batch time: 0.4956, average train loss: 160.8978
[09/25 23:36:26 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1689, average loss: 127.0660
[09/25 23:36:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.50	
[09/25 23:36:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/25 23:36:33 visual_prompt]: Epoch 11 / 100: avg data time: 5.28e-02, avg batch time: 0.5004, average train loss: 137.9089
[09/25 23:36:34 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1688, average loss: 114.3772
[09/25 23:36:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 16.00	
[09/25 23:36:34 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/25 23:36:41 visual_prompt]: Epoch 12 / 100: avg data time: 5.18e-02, avg batch time: 0.4998, average train loss: 139.4623
[09/25 23:36:42 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1690, average loss: 116.3204
[09/25 23:36:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/25 23:36:42 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/25 23:36:49 visual_prompt]: Epoch 13 / 100: avg data time: 4.94e-02, avg batch time: 0.4971, average train loss: 112.7360
[09/25 23:36:50 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1687, average loss: 114.0705
[09/25 23:36:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 23:36:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/25 23:36:57 visual_prompt]: Epoch 14 / 100: avg data time: 5.13e-02, avg batch time: 0.4987, average train loss: 121.5818
[09/25 23:36:58 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 120.4798
[09/25 23:36:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 23:36:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/25 23:37:05 visual_prompt]: Epoch 15 / 100: avg data time: 5.73e-02, avg batch time: 0.5053, average train loss: 110.2576
[09/25 23:37:07 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1690, average loss: 90.0871
[09/25 23:37:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 10.50	
[09/25 23:37:07 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/25 23:37:13 visual_prompt]: Epoch 16 / 100: avg data time: 3.94e-02, avg batch time: 0.4870, average train loss: 83.3910
[09/25 23:37:15 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1689, average loss: 88.1319
[09/25 23:37:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.00	
[09/25 23:37:15 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/25 23:37:21 visual_prompt]: Epoch 17 / 100: avg data time: 5.53e-02, avg batch time: 0.5029, average train loss: 86.9042
[09/25 23:37:23 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1687, average loss: 89.2631
[09/25 23:37:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.50	
[09/25 23:37:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/25 23:37:29 visual_prompt]: Epoch 18 / 100: avg data time: 4.32e-02, avg batch time: 0.4905, average train loss: 84.5897
[09/25 23:37:31 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1691, average loss: 76.8656
[09/25 23:37:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/25 23:37:31 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/25 23:37:38 visual_prompt]: Epoch 19 / 100: avg data time: 5.68e-02, avg batch time: 0.5037, average train loss: 83.0790
[09/25 23:37:39 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1692, average loss: 62.1525
[09/25 23:37:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/25 23:37:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/25 23:37:46 visual_prompt]: Epoch 20 / 100: avg data time: 4.09e-02, avg batch time: 0.4898, average train loss: 70.9846
[09/25 23:37:47 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1691, average loss: 58.3778
[09/25 23:37:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/25 23:37:47 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/25 23:37:54 visual_prompt]: Epoch 21 / 100: avg data time: 4.57e-02, avg batch time: 0.4950, average train loss: 70.6049
[09/25 23:37:55 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1689, average loss: 53.7634
[09/25 23:37:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/25 23:37:55 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/25 23:38:02 visual_prompt]: Epoch 22 / 100: avg data time: 3.93e-02, avg batch time: 0.4889, average train loss: 59.3380
[09/25 23:38:03 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 46.7903
[09/25 23:38:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.00	
[09/25 23:38:03 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/25 23:38:10 visual_prompt]: Epoch 23 / 100: avg data time: 4.05e-02, avg batch time: 0.4890, average train loss: 46.4258
[09/25 23:38:11 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1690, average loss: 48.2705
[09/25 23:38:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 13.00	
[09/25 23:38:11 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/25 23:38:18 visual_prompt]: Epoch 24 / 100: avg data time: 5.52e-02, avg batch time: 0.5030, average train loss: 48.3651
[09/25 23:38:19 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1691, average loss: 42.7935
[09/25 23:38:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 17.50	
[09/25 23:38:19 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/25 23:38:26 visual_prompt]: Epoch 25 / 100: avg data time: 5.44e-02, avg batch time: 0.5029, average train loss: 41.2840
[09/25 23:38:27 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1688, average loss: 36.5296
[09/25 23:38:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 16.50	
[09/25 23:38:27 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/25 23:38:34 visual_prompt]: Epoch 26 / 100: avg data time: 4.23e-02, avg batch time: 0.4918, average train loss: 35.1643
[09/25 23:38:35 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 40.9867
[09/25 23:38:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.50	
[09/25 23:38:35 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/25 23:38:42 visual_prompt]: Epoch 27 / 100: avg data time: 5.64e-02, avg batch time: 0.5043, average train loss: 42.7919
[09/25 23:38:44 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1690, average loss: 39.4992
[09/25 23:38:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 15.00	
[09/25 23:38:44 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/25 23:38:50 visual_prompt]: Epoch 28 / 100: avg data time: 5.09e-02, avg batch time: 0.4991, average train loss: 40.5616
[09/25 23:38:52 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1689, average loss: 41.1006
[09/25 23:38:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 15.00	
[09/25 23:38:52 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/25 23:38:59 visual_prompt]: Epoch 29 / 100: avg data time: 6.04e-02, avg batch time: 0.5094, average train loss: 31.7901
[09/25 23:39:00 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1690, average loss: 23.0932
[09/25 23:39:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 18.50	
[09/25 23:39:00 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/25 23:39:07 visual_prompt]: Epoch 30 / 100: avg data time: 5.41e-02, avg batch time: 0.5037, average train loss: 26.0163
[09/25 23:39:08 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 27.7523
[09/25 23:39:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 19.00	
[09/25 23:39:08 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/25 23:39:15 visual_prompt]: Epoch 31 / 100: avg data time: 5.75e-02, avg batch time: 0.5053, average train loss: 24.2293
[09/25 23:39:16 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1688, average loss: 26.3863
[09/25 23:39:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.50	top5: 17.50	
[09/25 23:39:16 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/25 23:39:23 visual_prompt]: Epoch 32 / 100: avg data time: 5.06e-02, avg batch time: 0.4988, average train loss: 26.8322
[09/25 23:39:25 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 27.6400
[09/25 23:39:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 17.50	
[09/25 23:39:25 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/25 23:39:31 visual_prompt]: Epoch 33 / 100: avg data time: 5.49e-02, avg batch time: 0.5033, average train loss: 24.4476
[09/25 23:39:33 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1693, average loss: 21.8487
[09/25 23:39:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 17.00	
[09/25 23:39:33 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/25 23:39:39 visual_prompt]: Epoch 34 / 100: avg data time: 4.82e-02, avg batch time: 0.4957, average train loss: 20.5717
[09/25 23:39:41 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1688, average loss: 15.4235
[09/25 23:39:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 15.00	
[09/25 23:39:41 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/25 23:39:48 visual_prompt]: Epoch 35 / 100: avg data time: 5.73e-02, avg batch time: 0.5048, average train loss: 13.2636
[09/25 23:39:49 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 12.5460
[09/25 23:39:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.50	top5: 15.00	
[09/25 23:39:49 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/25 23:39:56 visual_prompt]: Epoch 36 / 100: avg data time: 4.81e-02, avg batch time: 0.4972, average train loss: 10.2502
[09/25 23:39:57 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1695, average loss: 10.8017
[09/25 23:39:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.00	top5: 17.50	
[09/25 23:39:57 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/25 23:40:04 visual_prompt]: Epoch 37 / 100: avg data time: 3.81e-02, avg batch time: 0.4879, average train loss: 10.7401
[09/25 23:40:05 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1690, average loss: 9.3914
[09/25 23:40:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 22.00	
[09/25 23:40:05 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/25 23:40:12 visual_prompt]: Epoch 38 / 100: avg data time: 3.74e-02, avg batch time: 0.4864, average train loss: 8.4725
[09/25 23:40:13 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 7.9976
[09/25 23:40:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.50	top5: 21.50	
[09/25 23:40:13 visual_prompt]: Best epoch 38: best metric: 0.115
[09/25 23:40:13 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/25 23:40:20 visual_prompt]: Epoch 39 / 100: avg data time: 5.34e-02, avg batch time: 0.5012, average train loss: 7.8477
[09/25 23:40:21 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1692, average loss: 7.2028
[09/25 23:40:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.00	top5: 25.50	
[09/25 23:40:21 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/25 23:40:28 visual_prompt]: Epoch 40 / 100: avg data time: 3.77e-02, avg batch time: 0.4872, average train loss: 6.6546
[09/25 23:40:29 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1696, average loss: 7.3403
[09/25 23:40:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.00	top5: 27.00	
[09/25 23:40:29 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/25 23:40:36 visual_prompt]: Epoch 41 / 100: avg data time: 5.37e-02, avg batch time: 0.5018, average train loss: 6.7148
[09/25 23:40:37 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1688, average loss: 6.9067
[09/25 23:40:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.00	top5: 26.50	
[09/25 23:40:37 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/25 23:40:44 visual_prompt]: Epoch 42 / 100: avg data time: 4.06e-02, avg batch time: 0.4890, average train loss: 6.9474
[09/25 23:40:46 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1693, average loss: 7.7590
[09/25 23:40:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 25.50	
[09/25 23:40:46 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/25 23:40:52 visual_prompt]: Epoch 43 / 100: avg data time: 3.98e-02, avg batch time: 0.4886, average train loss: 6.0147
[09/25 23:40:54 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 6.9071
[09/25 23:40:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 24.50	
[09/25 23:40:54 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/25 23:41:00 visual_prompt]: Epoch 44 / 100: avg data time: 4.61e-02, avg batch time: 0.4944, average train loss: 5.9213
[09/25 23:41:02 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1692, average loss: 7.7071
[09/25 23:41:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.50	top5: 23.00	
[09/25 23:41:02 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/25 23:41:08 visual_prompt]: Epoch 45 / 100: avg data time: 3.93e-02, avg batch time: 0.4895, average train loss: 5.3530
[09/25 23:41:10 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1693, average loss: 6.1233
[09/25 23:41:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.00	top5: 32.00	
[09/25 23:41:10 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/25 23:41:16 visual_prompt]: Epoch 46 / 100: avg data time: 4.74e-02, avg batch time: 0.4972, average train loss: 4.8300
[09/25 23:41:18 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1699, average loss: 6.5003
[09/25 23:41:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 14.00	top5: 32.00	
[09/25 23:41:18 visual_prompt]: Best epoch 46: best metric: 0.140
[09/25 23:41:18 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/25 23:41:25 visual_prompt]: Epoch 47 / 100: avg data time: 4.01e-02, avg batch time: 0.4902, average train loss: 5.1604
[09/25 23:41:26 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 7.1314
[09/25 23:41:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.50	top5: 29.00	
[09/25 23:41:26 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/25 23:41:33 visual_prompt]: Epoch 48 / 100: avg data time: 4.99e-02, avg batch time: 0.5003, average train loss: 4.7075
[09/25 23:41:34 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 5.9565
[09/25 23:41:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.50	top5: 28.00	
[09/25 23:41:34 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/25 23:41:41 visual_prompt]: Epoch 49 / 100: avg data time: 5.19e-02, avg batch time: 0.5004, average train loss: 4.4014
[09/25 23:41:42 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1692, average loss: 6.6144
[09/25 23:41:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.50	top5: 31.00	
[09/25 23:41:42 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/25 23:41:49 visual_prompt]: Epoch 50 / 100: avg data time: 4.91e-02, avg batch time: 0.4973, average train loss: 4.1483
[09/25 23:41:50 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1695, average loss: 5.9691
[09/25 23:41:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.50	top5: 29.00	
[09/25 23:41:50 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/25 23:41:57 visual_prompt]: Epoch 51 / 100: avg data time: 4.55e-02, avg batch time: 0.4953, average train loss: 4.6805
[09/25 23:41:58 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 5.7396
[09/25 23:41:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.00	top5: 31.50	
[09/25 23:41:58 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/25 23:42:05 visual_prompt]: Epoch 52 / 100: avg data time: 4.46e-02, avg batch time: 0.4933, average train loss: 4.5032
[09/25 23:42:06 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 5.8658
[09/25 23:42:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.00	top5: 32.00	
[09/25 23:42:06 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/25 23:42:13 visual_prompt]: Epoch 53 / 100: avg data time: 4.16e-02, avg batch time: 0.4917, average train loss: 4.2467
[09/25 23:42:15 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 6.1611
[09/25 23:42:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.00	top5: 30.00	
[09/25 23:42:15 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/25 23:42:21 visual_prompt]: Epoch 54 / 100: avg data time: 4.32e-02, avg batch time: 0.4926, average train loss: 4.0298
[09/25 23:42:23 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1691, average loss: 5.3989
[09/25 23:42:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 17.50	top5: 30.50	
[09/25 23:42:23 visual_prompt]: Best epoch 54: best metric: 0.175
[09/25 23:42:23 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/25 23:42:29 visual_prompt]: Epoch 55 / 100: avg data time: 3.93e-02, avg batch time: 0.4875, average train loss: 3.9510
[09/25 23:42:31 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 6.4028
[09/25 23:42:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.50	top5: 33.00	
[09/25 23:42:31 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/25 23:42:37 visual_prompt]: Epoch 56 / 100: avg data time: 4.33e-02, avg batch time: 0.4928, average train loss: 3.9108
[09/25 23:42:39 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1690, average loss: 6.4666
[09/25 23:42:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 14.50	top5: 30.50	
[09/25 23:42:39 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/25 23:42:45 visual_prompt]: Epoch 57 / 100: avg data time: 3.82e-02, avg batch time: 0.4892, average train loss: 4.0047
[09/25 23:42:47 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1697, average loss: 6.2645
[09/25 23:42:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.50	top5: 37.00	
[09/25 23:42:47 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/25 23:42:53 visual_prompt]: Epoch 58 / 100: avg data time: 4.09e-02, avg batch time: 0.4894, average train loss: 3.5102
[09/25 23:42:55 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1692, average loss: 5.3682
[09/25 23:42:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.00	top5: 37.50	
[09/25 23:42:55 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/25 23:43:01 visual_prompt]: Epoch 59 / 100: avg data time: 4.06e-02, avg batch time: 0.4899, average train loss: 3.2292
[09/25 23:43:03 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1697, average loss: 4.8969
[09/25 23:43:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 18.50	top5: 44.00	
[09/25 23:43:03 visual_prompt]: Best epoch 59: best metric: 0.185
[09/25 23:43:03 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/25 23:43:09 visual_prompt]: Epoch 60 / 100: avg data time: 3.90e-02, avg batch time: 0.4904, average train loss: 3.4614
[09/25 23:43:11 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 4.6936
[09/25 23:43:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.00	top5: 39.00	
[09/25 23:43:11 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/25 23:43:17 visual_prompt]: Epoch 61 / 100: avg data time: 4.79e-02, avg batch time: 0.4966, average train loss: 3.6795
[09/25 23:43:19 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1695, average loss: 5.4631
[09/25 23:43:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.50	top5: 39.00	
[09/25 23:43:19 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/25 23:43:26 visual_prompt]: Epoch 62 / 100: avg data time: 5.29e-02, avg batch time: 0.5022, average train loss: 3.4531
[09/25 23:43:27 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1690, average loss: 4.2206
[09/25 23:43:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 18.00	top5: 41.00	
[09/25 23:43:27 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/25 23:43:34 visual_prompt]: Epoch 63 / 100: avg data time: 4.74e-02, avg batch time: 0.4957, average train loss: 3.1998
[09/25 23:43:35 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1686, average loss: 5.0140
[09/25 23:43:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.50	top5: 41.00	
[09/25 23:43:35 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/25 23:43:42 visual_prompt]: Epoch 64 / 100: avg data time: 5.31e-02, avg batch time: 0.5018, average train loss: 3.1086
[09/25 23:43:43 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1691, average loss: 4.3231
[09/25 23:43:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 21.00	top5: 44.50	
[09/25 23:43:43 visual_prompt]: Best epoch 64: best metric: 0.210
[09/25 23:43:43 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/25 23:43:50 visual_prompt]: Epoch 65 / 100: avg data time: 5.49e-02, avg batch time: 0.5030, average train loss: 2.9451
[09/25 23:43:51 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1695, average loss: 4.5375
[09/25 23:43:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 18.00	top5: 41.50	
[09/25 23:43:51 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/25 23:43:58 visual_prompt]: Epoch 66 / 100: avg data time: 4.33e-02, avg batch time: 0.4932, average train loss: 3.1027
[09/25 23:43:59 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 4.9103
[09/25 23:43:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.50	top5: 40.00	
[09/25 23:43:59 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/25 23:44:06 visual_prompt]: Epoch 67 / 100: avg data time: 5.19e-02, avg batch time: 0.5026, average train loss: 3.0130
[09/25 23:44:08 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1696, average loss: 4.4689
[09/25 23:44:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 19.50	top5: 37.00	
[09/25 23:44:08 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/25 23:44:14 visual_prompt]: Epoch 68 / 100: avg data time: 4.76e-02, avg batch time: 0.4979, average train loss: 2.8398
[09/25 23:44:16 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1695, average loss: 4.4095
[09/25 23:44:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 18.00	top5: 44.50	
[09/25 23:44:16 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/25 23:44:22 visual_prompt]: Epoch 69 / 100: avg data time: 5.10e-02, avg batch time: 0.4990, average train loss: 2.8753
[09/25 23:44:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 4.6891
[09/25 23:44:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 18.50	top5: 44.50	
[09/25 23:44:24 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/25 23:44:30 visual_prompt]: Epoch 70 / 100: avg data time: 4.00e-02, avg batch time: 0.4887, average train loss: 2.7213
[09/25 23:44:32 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 3.6923
[09/25 23:44:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 21.50	top5: 52.00	
[09/25 23:44:32 visual_prompt]: Best epoch 70: best metric: 0.215
[09/25 23:44:32 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/25 23:44:38 visual_prompt]: Epoch 71 / 100: avg data time: 4.91e-02, avg batch time: 0.4987, average train loss: 2.7540
[09/25 23:44:40 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1696, average loss: 3.8446
[09/25 23:44:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 20.00	top5: 48.00	
[09/25 23:44:40 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/25 23:44:47 visual_prompt]: Epoch 72 / 100: avg data time: 4.84e-02, avg batch time: 0.4965, average train loss: 2.5907
[09/25 23:44:48 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1696, average loss: 4.5311
[09/25 23:44:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.50	top5: 41.50	
[09/25 23:44:48 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/25 23:44:55 visual_prompt]: Epoch 73 / 100: avg data time: 3.94e-02, avg batch time: 0.4877, average train loss: 2.5871
[09/25 23:44:56 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1696, average loss: 3.8865
[09/25 23:44:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 46.50	
[09/25 23:44:56 visual_prompt]: Best epoch 73: best metric: 0.220
[09/25 23:44:56 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/25 23:45:03 visual_prompt]: Epoch 74 / 100: avg data time: 4.74e-02, avg batch time: 0.4960, average train loss: 2.5063
[09/25 23:45:04 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1694, average loss: 3.7658
[09/25 23:45:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 50.50	
[09/25 23:45:04 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/25 23:45:11 visual_prompt]: Epoch 75 / 100: avg data time: 4.08e-02, avg batch time: 0.4907, average train loss: 2.3970
[09/25 23:45:12 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 3.6843
[09/25 23:45:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 51.00	
[09/25 23:45:12 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/25 23:45:19 visual_prompt]: Epoch 76 / 100: avg data time: 3.89e-02, avg batch time: 0.4889, average train loss: 2.3325
[09/25 23:45:20 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 4.3393
[09/25 23:45:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 18.00	top5: 43.00	
[09/25 23:45:20 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/25 23:45:27 visual_prompt]: Epoch 77 / 100: avg data time: 4.90e-02, avg batch time: 0.4993, average train loss: 2.4023
[09/25 23:45:28 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1692, average loss: 3.9942
[09/25 23:45:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.50	top5: 46.00	
[09/25 23:45:28 visual_prompt]: Best epoch 77: best metric: 0.225
[09/25 23:45:28 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/25 23:45:35 visual_prompt]: Epoch 78 / 100: avg data time: 4.13e-02, avg batch time: 0.4908, average train loss: 2.3898
[09/25 23:45:36 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.1695, average loss: 3.7450
[09/25 23:45:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 49.00	
[09/25 23:45:36 visual_prompt]: Best epoch 78: best metric: 0.245
[09/25 23:45:36 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/25 23:45:43 visual_prompt]: Epoch 79 / 100: avg data time: 4.43e-02, avg batch time: 0.4946, average train loss: 2.3112
[09/25 23:45:44 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1692, average loss: 3.2772
[09/25 23:45:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 28.00	top5: 51.00	
[09/25 23:45:44 visual_prompt]: Best epoch 79: best metric: 0.280
[09/25 23:45:44 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/25 23:45:51 visual_prompt]: Epoch 80 / 100: avg data time: 5.19e-02, avg batch time: 0.5003, average train loss: 2.4025
[09/25 23:45:53 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 3.6968
[09/25 23:45:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.00	top5: 50.50	
[09/25 23:45:53 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/25 23:45:59 visual_prompt]: Epoch 81 / 100: avg data time: 5.05e-02, avg batch time: 0.4999, average train loss: 2.2768
[09/25 23:46:01 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 3.9376
[09/25 23:46:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 48.00	
[09/25 23:46:01 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/25 23:46:08 visual_prompt]: Epoch 82 / 100: avg data time: 5.58e-02, avg batch time: 0.5045, average train loss: 2.2031
[09/25 23:46:09 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1691, average loss: 3.3858
[09/25 23:46:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.00	top5: 51.50	
[09/25 23:46:09 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/25 23:46:16 visual_prompt]: Epoch 83 / 100: avg data time: 3.71e-02, avg batch time: 0.4877, average train loss: 2.1317
[09/25 23:46:17 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1696, average loss: 3.7414
[09/25 23:46:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.50	top5: 50.50	
[09/25 23:46:17 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/25 23:46:24 visual_prompt]: Epoch 84 / 100: avg data time: 6.12e-02, avg batch time: 0.5093, average train loss: 2.1684
[09/25 23:46:25 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1699, average loss: 3.6666
[09/25 23:46:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.00	top5: 50.50	
[09/25 23:46:25 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/25 23:46:32 visual_prompt]: Epoch 85 / 100: avg data time: 5.29e-02, avg batch time: 0.5011, average train loss: 2.1068
[09/25 23:46:33 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1691, average loss: 3.3821
[09/25 23:46:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.50	top5: 52.00	
[09/25 23:46:33 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/25 23:46:40 visual_prompt]: Epoch 86 / 100: avg data time: 4.11e-02, avg batch time: 0.4925, average train loss: 2.0729
[09/25 23:46:42 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1695, average loss: 3.8612
[09/25 23:46:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.00	top5: 50.50	
[09/25 23:46:42 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/25 23:46:48 visual_prompt]: Epoch 87 / 100: avg data time: 4.61e-02, avg batch time: 0.4949, average train loss: 2.0771
[09/25 23:46:50 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1693, average loss: 3.5022
[09/25 23:46:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.00	top5: 51.50	
[09/25 23:46:50 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/25 23:46:56 visual_prompt]: Epoch 88 / 100: avg data time: 4.45e-02, avg batch time: 0.4933, average train loss: 2.0172
[09/25 23:46:58 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 3.7157
[09/25 23:46:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 48.50	
[09/25 23:46:58 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/25 23:47:04 visual_prompt]: Epoch 89 / 100: avg data time: 4.08e-02, avg batch time: 0.4888, average train loss: 2.0394
[09/25 23:47:06 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1698, average loss: 3.5459
[09/25 23:47:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.50	top5: 50.50	
[09/25 23:47:06 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/25 23:47:12 visual_prompt]: Epoch 90 / 100: avg data time: 4.86e-02, avg batch time: 0.4976, average train loss: 2.0015
[09/25 23:47:14 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1697, average loss: 3.5573
[09/25 23:47:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.00	top5: 50.50	
[09/25 23:47:14 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/25 23:47:20 visual_prompt]: Epoch 91 / 100: avg data time: 4.31e-02, avg batch time: 0.4917, average train loss: 2.0707
[09/25 23:47:22 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1691, average loss: 3.4905
[09/25 23:47:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.50	top5: 52.00	
[09/25 23:47:22 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/25 23:47:29 visual_prompt]: Epoch 92 / 100: avg data time: 4.86e-02, avg batch time: 0.4985, average train loss: 2.0217
[09/25 23:47:30 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1696, average loss: 3.6328
[09/25 23:47:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.00	top5: 50.00	
[09/25 23:47:30 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/25 23:47:37 visual_prompt]: Epoch 93 / 100: avg data time: 4.62e-02, avg batch time: 0.4964, average train loss: 1.9364
[09/25 23:47:38 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1693, average loss: 3.4674
[09/25 23:47:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.00	top5: 51.50	
[09/25 23:47:38 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/25 23:47:45 visual_prompt]: Epoch 94 / 100: avg data time: 5.05e-02, avg batch time: 0.5001, average train loss: 1.9722
[09/25 23:47:46 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1696, average loss: 3.5500
[09/25 23:47:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.50	top5: 52.00	
[09/25 23:47:46 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/25 23:47:53 visual_prompt]: Epoch 95 / 100: avg data time: 4.96e-02, avg batch time: 0.4989, average train loss: 1.9348
[09/25 23:47:55 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 3.6029
[09/25 23:47:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.00	top5: 51.50	
[09/25 23:47:55 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/25 23:48:01 visual_prompt]: Epoch 96 / 100: avg data time: 5.13e-02, avg batch time: 0.4995, average train loss: 1.9395
[09/25 23:48:03 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1695, average loss: 3.5663
[09/25 23:48:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.00	top5: 50.50	
[09/25 23:48:03 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/25 23:48:09 visual_prompt]: Epoch 97 / 100: avg data time: 5.06e-02, avg batch time: 0.4994, average train loss: 1.9668
[09/25 23:48:11 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1691, average loss: 3.5459
[09/25 23:48:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.50	top5: 51.00	
[09/25 23:48:11 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/25 23:48:18 visual_prompt]: Epoch 98 / 100: avg data time: 5.75e-02, avg batch time: 0.5067, average train loss: 1.9487
[09/25 23:48:19 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 3.5355
[09/25 23:48:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.50	top5: 51.50	
[09/25 23:48:19 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/25 23:48:26 visual_prompt]: Epoch 99 / 100: avg data time: 4.27e-02, avg batch time: 0.4930, average train loss: 1.9974
[09/25 23:48:27 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1689, average loss: 3.5472
[09/25 23:48:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.50	top5: 52.00	
[09/25 23:48:27 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/25 23:48:34 visual_prompt]: Epoch 100 / 100: avg data time: 3.95e-02, avg batch time: 0.4898, average train loss: 1.9309
[09/25 23:48:35 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 3.5507
[09/25 23:48:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.50	top5: 51.50	
[09/25 23:48:35 visual_prompt]: Rank of current process: 0. World size: 1
[09/25 23:48:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/25 23:48:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/25 23:48:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/25 23:48:35 visual_prompt]: Training with config:
[09/25 23:48:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/25 23:48:35 visual_prompt]: Loading training data...
[09/25 23:48:35 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 23:48:37 visual_prompt]: Number of images: 800
[09/25 23:48:37 visual_prompt]: Number of classes: 47 / 47
[09/25 23:48:37 visual_prompt]: Loading validation data...
[09/25 23:48:37 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/25 23:48:38 visual_prompt]: Number of images: 200
[09/25 23:48:38 visual_prompt]: Number of classes: 47 / 47
[09/25 23:48:38 visual_prompt]: Constructing models...
[09/25 23:48:40 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/25 23:48:40 visual_prompt]: tuned percent:0.576
[09/25 23:48:40 visual_prompt]: Device used for model: 0
[09/25 23:48:40 visual_prompt]: Setting up Evaluator...
[09/25 23:48:40 visual_prompt]: Setting up Trainer...
[09/25 23:48:40 visual_prompt]: 	Setting up the optimizer...
[09/25 23:48:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/25 23:48:47 visual_prompt]: Epoch 1 / 100: avg data time: 4.90e-02, avg batch time: 0.4961, average train loss: 3.9357
[09/25 23:48:49 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1688, average loss: 3.9045
[09/25 23:48:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/25 23:48:49 visual_prompt]: Best epoch 1: best metric: 0.040
[09/25 23:48:49 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/25 23:48:55 visual_prompt]: Epoch 2 / 100: avg data time: 6.10e-02, avg batch time: 0.5079, average train loss: 3.9550
[09/25 23:48:57 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1689, average loss: 3.9292
[09/25 23:48:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/25 23:48:57 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/25 23:49:04 visual_prompt]: Epoch 3 / 100: avg data time: 5.35e-02, avg batch time: 0.5010, average train loss: 4.0332
[09/25 23:49:05 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1687, average loss: 4.2720
[09/25 23:49:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 23:49:05 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/25 23:49:12 visual_prompt]: Epoch 4 / 100: avg data time: 4.60e-02, avg batch time: 0.4946, average train loss: 4.1815
[09/25 23:49:13 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1688, average loss: 4.2614
[09/25 23:49:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/25 23:49:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/25 23:49:20 visual_prompt]: Epoch 5 / 100: avg data time: 4.60e-02, avg batch time: 0.4948, average train loss: 4.6858
[09/25 23:49:21 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1691, average loss: 5.6825
[09/25 23:49:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 15.00	
[09/25 23:49:21 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/25 23:49:28 visual_prompt]: Epoch 6 / 100: avg data time: 5.28e-02, avg batch time: 0.4998, average train loss: 6.0676
[09/25 23:49:29 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 5.8083
[09/25 23:49:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/25 23:49:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/25 23:49:36 visual_prompt]: Epoch 7 / 100: avg data time: 5.96e-02, avg batch time: 0.5068, average train loss: 8.3093
[09/25 23:49:38 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1692, average loss: 8.0819
[09/25 23:49:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.50	
[09/25 23:49:38 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/25 23:49:44 visual_prompt]: Epoch 8 / 100: avg data time: 4.84e-02, avg batch time: 0.4957, average train loss: 16.1703
[09/25 23:49:46 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1687, average loss: 9.7784
[09/25 23:49:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/25 23:49:46 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/25 23:49:52 visual_prompt]: Epoch 9 / 100: avg data time: 4.09e-02, avg batch time: 0.4907, average train loss: 14.4315
[09/25 23:49:54 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1689, average loss: 12.2447
[09/25 23:49:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 15.00	
[09/25 23:49:54 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/25 23:50:01 visual_prompt]: Epoch 10 / 100: avg data time: 5.41e-02, avg batch time: 0.5033, average train loss: 17.8788
[09/25 23:50:02 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.1688, average loss: 14.5844
[09/25 23:50:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 13.50	
[09/25 23:50:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/25 23:50:09 visual_prompt]: Epoch 11 / 100: avg data time: 4.70e-02, avg batch time: 0.4955, average train loss: 18.8225
[09/25 23:50:10 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1692, average loss: 15.4869
[09/25 23:50:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/25 23:50:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/25 23:50:17 visual_prompt]: Epoch 12 / 100: avg data time: 5.65e-02, avg batch time: 0.5044, average train loss: 19.9240
[09/25 23:50:18 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1692, average loss: 15.3562
[09/25 23:50:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 15.50	
[09/25 23:50:18 visual_prompt]: Best epoch 12: best metric: 0.050
[09/25 23:50:18 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/25 23:50:25 visual_prompt]: Epoch 13 / 100: avg data time: 4.36e-02, avg batch time: 0.4926, average train loss: 19.8991
[09/25 23:50:27 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1691, average loss: 21.6005
[09/25 23:50:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.50	
[09/25 23:50:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/25 23:50:33 visual_prompt]: Epoch 14 / 100: avg data time: 5.27e-02, avg batch time: 0.5004, average train loss: 17.8143
[09/25 23:50:35 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1689, average loss: 13.1313
[09/25 23:50:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.00	
[09/25 23:50:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/25 23:50:42 visual_prompt]: Epoch 15 / 100: avg data time: 5.52e-02, avg batch time: 0.5026, average train loss: 18.1919
[09/25 23:50:43 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1690, average loss: 16.9140
[09/25 23:50:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 23:50:43 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/25 23:50:50 visual_prompt]: Epoch 16 / 100: avg data time: 3.95e-02, avg batch time: 0.4889, average train loss: 26.4739
[09/25 23:50:51 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1689, average loss: 27.3168
[09/25 23:50:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 23:50:51 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/25 23:50:58 visual_prompt]: Epoch 17 / 100: avg data time: 5.97e-02, avg batch time: 0.5078, average train loss: 39.0900
[09/25 23:50:59 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1690, average loss: 25.9543
[09/25 23:50:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/25 23:50:59 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/25 23:51:06 visual_prompt]: Epoch 18 / 100: avg data time: 4.67e-02, avg batch time: 0.4942, average train loss: 32.5006
[09/25 23:51:07 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1690, average loss: 38.0029
[09/25 23:51:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 23:51:07 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/25 23:51:14 visual_prompt]: Epoch 19 / 100: avg data time: 5.80e-02, avg batch time: 0.5067, average train loss: 28.3615
[09/25 23:51:16 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1691, average loss: 26.4676
[09/25 23:51:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.00	
[09/25 23:51:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/25 23:51:22 visual_prompt]: Epoch 20 / 100: avg data time: 3.79e-02, avg batch time: 0.4884, average train loss: 30.9411
[09/25 23:51:24 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1688, average loss: 26.1587
[09/25 23:51:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 14.00	
[09/25 23:51:24 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/25 23:51:30 visual_prompt]: Epoch 21 / 100: avg data time: 5.25e-02, avg batch time: 0.5016, average train loss: 27.0076
[09/25 23:51:32 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1690, average loss: 33.7212
[09/25 23:51:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.50	
[09/25 23:51:32 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/25 23:51:39 visual_prompt]: Epoch 22 / 100: avg data time: 5.51e-02, avg batch time: 0.5026, average train loss: 25.3183
[09/25 23:51:40 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1695, average loss: 25.9141
[09/25 23:51:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 23:51:40 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/25 23:51:47 visual_prompt]: Epoch 23 / 100: avg data time: 5.27e-02, avg batch time: 0.5019, average train loss: 28.7818
[09/25 23:51:48 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1690, average loss: 28.2599
[09/25 23:51:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 12.50	
[09/25 23:51:48 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/25 23:51:55 visual_prompt]: Epoch 24 / 100: avg data time: 5.09e-02, avg batch time: 0.4997, average train loss: 29.8061
[09/25 23:51:57 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1690, average loss: 36.4967
[09/25 23:51:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.50	
[09/25 23:51:57 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/25 23:52:03 visual_prompt]: Epoch 25 / 100: avg data time: 5.41e-02, avg batch time: 0.5032, average train loss: 36.0832
[09/25 23:52:05 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1687, average loss: 36.5118
[09/25 23:52:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 12.00	
[09/25 23:52:05 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/25 23:52:12 visual_prompt]: Epoch 26 / 100: avg data time: 5.58e-02, avg batch time: 0.5034, average train loss: 37.2753
[09/25 23:52:13 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1690, average loss: 27.1815
[09/25 23:52:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.00	
[09/25 23:52:13 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/25 23:52:20 visual_prompt]: Epoch 27 / 100: avg data time: 4.03e-02, avg batch time: 0.4880, average train loss: 26.8333
[09/25 23:52:21 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1689, average loss: 23.7157
[09/25 23:52:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 14.00	
[09/25 23:52:21 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/25 23:52:28 visual_prompt]: Epoch 28 / 100: avg data time: 4.77e-02, avg batch time: 0.4960, average train loss: 26.5074
[09/25 23:52:29 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 20.8283
[09/25 23:52:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.00	
[09/25 23:52:29 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/25 23:52:36 visual_prompt]: Epoch 29 / 100: avg data time: 5.09e-02, avg batch time: 0.4995, average train loss: 22.9735
[09/25 23:52:37 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1688, average loss: 80.7100
[09/25 23:52:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.50	
[09/25 23:52:37 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/25 23:52:44 visual_prompt]: Epoch 30 / 100: avg data time: 4.81e-02, avg batch time: 0.4975, average train loss: 35.1549
[09/25 23:52:46 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1690, average loss: 25.9014
[09/25 23:52:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 12.00	
[09/25 23:52:46 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/25 23:52:52 visual_prompt]: Epoch 31 / 100: avg data time: 4.67e-02, avg batch time: 0.4945, average train loss: 31.7646
[09/25 23:52:54 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1694, average loss: 26.8070
[09/25 23:52:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.00	
[09/25 23:52:54 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/25 23:53:00 visual_prompt]: Epoch 32 / 100: avg data time: 4.20e-02, avg batch time: 0.4920, average train loss: 27.7011
[09/25 23:53:02 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 21.3835
[09/25 23:53:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/25 23:53:02 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/25 23:53:08 visual_prompt]: Epoch 33 / 100: avg data time: 4.54e-02, avg batch time: 0.4960, average train loss: 25.6315
[09/25 23:53:10 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1688, average loss: 20.4739
[09/25 23:53:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/25 23:53:10 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/25 23:53:17 visual_prompt]: Epoch 34 / 100: avg data time: 5.11e-02, avg batch time: 0.5007, average train loss: 22.3692
[09/25 23:53:18 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 23.0705
[09/25 23:53:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 13.00	
[09/25 23:53:18 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/25 23:53:24 visual_prompt]: Epoch 35 / 100: avg data time: 3.56e-02, avg batch time: 0.4848, average train loss: 19.8752
[09/25 23:53:26 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1690, average loss: 15.4005
[09/25 23:53:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/25 23:53:26 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/25 23:53:33 visual_prompt]: Epoch 36 / 100: avg data time: 3.93e-02, avg batch time: 0.4899, average train loss: 25.2892
[09/25 23:53:34 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1690, average loss: 18.6493
[09/25 23:53:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/25 23:53:34 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/25 23:53:41 visual_prompt]: Epoch 37 / 100: avg data time: 5.33e-02, avg batch time: 0.5011, average train loss: 20.3687
[09/25 23:53:42 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1692, average loss: 18.2748
[09/25 23:53:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 23:53:42 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/25 23:53:49 visual_prompt]: Epoch 38 / 100: avg data time: 5.67e-02, avg batch time: 0.5048, average train loss: 19.4660
[09/25 23:53:50 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1690, average loss: 13.0455
[09/25 23:53:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.00	
[09/25 23:53:50 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/25 23:53:57 visual_prompt]: Epoch 39 / 100: avg data time: 4.98e-02, avg batch time: 0.4982, average train loss: 18.0932
[09/25 23:53:58 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1687, average loss: 15.5435
[09/25 23:53:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/25 23:53:58 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/25 23:54:05 visual_prompt]: Epoch 40 / 100: avg data time: 5.59e-02, avg batch time: 0.5035, average train loss: 17.7154
[09/25 23:54:07 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1690, average loss: 13.4221
[09/25 23:54:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/25 23:54:07 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/25 23:54:13 visual_prompt]: Epoch 41 / 100: avg data time: 4.58e-02, avg batch time: 0.4944, average train loss: 16.2055
[09/25 23:54:15 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1692, average loss: 47.0396
[09/25 23:54:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/25 23:54:15 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/25 23:54:21 visual_prompt]: Epoch 42 / 100: avg data time: 3.87e-02, avg batch time: 0.4886, average train loss: 22.2826
[09/25 23:54:23 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 10.9220
[09/25 23:54:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 23:54:23 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/25 23:54:29 visual_prompt]: Epoch 43 / 100: avg data time: 3.84e-02, avg batch time: 0.4881, average train loss: 14.3481
[09/25 23:54:31 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1691, average loss: 12.0409
[09/25 23:54:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 7.50	
[09/25 23:54:31 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/25 23:54:37 visual_prompt]: Epoch 44 / 100: avg data time: 4.03e-02, avg batch time: 0.4887, average train loss: 14.1138
[09/25 23:54:39 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 13.3625
[09/25 23:54:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.50	
[09/25 23:54:39 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/25 23:54:45 visual_prompt]: Epoch 45 / 100: avg data time: 3.91e-02, avg batch time: 0.4898, average train loss: 13.9779
[09/25 23:54:47 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 9.6720
[09/25 23:54:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/25 23:54:47 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/25 23:54:53 visual_prompt]: Epoch 46 / 100: avg data time: 4.96e-02, avg batch time: 0.4978, average train loss: 13.2952
[09/25 23:54:55 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.1691, average loss: 16.0784
[09/25 23:54:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/25 23:54:55 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/25 23:55:02 visual_prompt]: Epoch 47 / 100: avg data time: 4.51e-02, avg batch time: 0.4956, average train loss: 13.6503
[09/25 23:55:03 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 12.0516
[09/25 23:55:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/25 23:55:03 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/25 23:55:10 visual_prompt]: Epoch 48 / 100: avg data time: 5.17e-02, avg batch time: 0.5016, average train loss: 13.9853
[09/25 23:55:11 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1690, average loss: 12.8845
[09/25 23:55:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 23:55:11 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/25 23:55:18 visual_prompt]: Epoch 49 / 100: avg data time: 4.72e-02, avg batch time: 0.4959, average train loss: 11.6798
[09/25 23:55:19 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 8.6611
[09/25 23:55:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 10.00	
[09/25 23:55:19 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/25 23:55:26 visual_prompt]: Epoch 50 / 100: avg data time: 4.97e-02, avg batch time: 0.4989, average train loss: 9.8983
[09/25 23:55:27 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1686, average loss: 9.5009
[09/25 23:55:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.50	
[09/25 23:55:27 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/25 23:55:34 visual_prompt]: Epoch 51 / 100: avg data time: 4.39e-02, avg batch time: 0.4946, average train loss: 9.4592
[09/25 23:55:36 visual_prompt]: Inference (val):avg data time: 4.62e-05, avg batch time: 0.1695, average loss: 7.0006
[09/25 23:55:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.00	
[09/25 23:55:36 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/25 23:55:42 visual_prompt]: Epoch 52 / 100: avg data time: 4.31e-02, avg batch time: 0.4931, average train loss: 6.2498
[09/25 23:55:44 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 5.1754
[09/25 23:55:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 14.00	
[09/25 23:55:44 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/25 23:55:50 visual_prompt]: Epoch 53 / 100: avg data time: 4.64e-02, avg batch time: 0.4952, average train loss: 6.2223
[09/25 23:55:52 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1693, average loss: 12.8414
[09/25 23:55:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/25 23:55:52 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/25 23:55:59 visual_prompt]: Epoch 54 / 100: avg data time: 5.40e-02, avg batch time: 0.5028, average train loss: 10.4395
[09/25 23:56:00 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1688, average loss: 9.8766
[09/25 23:56:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.50	
[09/25 23:56:00 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/25 23:56:07 visual_prompt]: Epoch 55 / 100: avg data time: 4.49e-02, avg batch time: 0.4939, average train loss: 12.8311
[09/25 23:56:08 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1688, average loss: 12.2121
[09/25 23:56:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 7.00	
[09/25 23:56:08 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/25 23:56:15 visual_prompt]: Epoch 56 / 100: avg data time: 4.61e-02, avg batch time: 0.4946, average train loss: 13.7793
[09/25 23:56:16 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1690, average loss: 8.2332
[09/25 23:56:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 23:56:16 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/25 23:56:23 visual_prompt]: Epoch 57 / 100: avg data time: 4.23e-02, avg batch time: 0.4915, average train loss: 11.2642
[09/25 23:56:24 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 10.4705
[09/25 23:56:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.00	
[09/25 23:56:24 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/25 23:56:31 visual_prompt]: Epoch 58 / 100: avg data time: 4.04e-02, avg batch time: 0.4895, average train loss: 10.5597
[09/25 23:56:32 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1692, average loss: 8.5547
[09/25 23:56:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/25 23:56:32 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/25 23:56:39 visual_prompt]: Epoch 59 / 100: avg data time: 3.95e-02, avg batch time: 0.4902, average train loss: 8.1368
[09/25 23:56:40 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1693, average loss: 7.2768
[09/25 23:56:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 14.50	
[09/25 23:56:40 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/25 23:56:47 visual_prompt]: Epoch 60 / 100: avg data time: 4.16e-02, avg batch time: 0.4902, average train loss: 6.9291
[09/25 23:56:48 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 7.1069
[09/25 23:56:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.00	
[09/25 23:56:48 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/25 23:56:55 visual_prompt]: Epoch 61 / 100: avg data time: 4.92e-02, avg batch time: 0.4977, average train loss: 5.6067
[09/25 23:56:56 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1689, average loss: 5.9050
[09/25 23:56:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 23:56:56 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/25 23:57:03 visual_prompt]: Epoch 62 / 100: avg data time: 3.88e-02, avg batch time: 0.4878, average train loss: 5.2831
[09/25 23:57:04 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1691, average loss: 5.0105
[09/25 23:57:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/25 23:57:04 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/25 23:57:11 visual_prompt]: Epoch 63 / 100: avg data time: 5.06e-02, avg batch time: 0.4986, average train loss: 5.0877
[09/25 23:57:13 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1689, average loss: 4.6749
[09/25 23:57:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 8.50	
[09/25 23:57:13 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/25 23:57:19 visual_prompt]: Epoch 64 / 100: avg data time: 5.13e-02, avg batch time: 0.5010, average train loss: 4.4692
[09/25 23:57:21 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 4.5149
[09/25 23:57:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 8.50	
[09/25 23:57:21 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/25 23:57:27 visual_prompt]: Epoch 65 / 100: avg data time: 4.46e-02, avg batch time: 0.4943, average train loss: 4.1355
[09/25 23:57:29 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.1693, average loss: 4.1976
[09/25 23:57:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/25 23:57:29 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/25 23:57:35 visual_prompt]: Epoch 66 / 100: avg data time: 3.92e-02, avg batch time: 0.4887, average train loss: 4.3736
[09/25 23:57:37 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 4.2860
[09/25 23:57:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/25 23:57:37 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/25 23:57:44 visual_prompt]: Epoch 67 / 100: avg data time: 4.48e-02, avg batch time: 0.4941, average train loss: 6.2412
[09/25 23:57:45 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 6.0952
[09/25 23:57:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/25 23:57:45 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/25 23:57:52 visual_prompt]: Epoch 68 / 100: avg data time: 4.64e-02, avg batch time: 0.4956, average train loss: 5.7100
[09/25 23:57:53 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1694, average loss: 4.7202
[09/25 23:57:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/25 23:57:53 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/25 23:58:00 visual_prompt]: Epoch 69 / 100: avg data time: 4.70e-02, avg batch time: 0.4970, average train loss: 5.0482
[09/25 23:58:01 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1697, average loss: 4.6670
[09/25 23:58:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.50	
[09/25 23:58:01 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/25 23:58:08 visual_prompt]: Epoch 70 / 100: avg data time: 4.53e-02, avg batch time: 0.4947, average train loss: 5.0914
[09/25 23:58:09 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1689, average loss: 4.4275
[09/25 23:58:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.50	
[09/25 23:58:09 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/25 23:58:16 visual_prompt]: Epoch 71 / 100: avg data time: 3.57e-02, avg batch time: 0.4873, average train loss: 4.5959
[09/25 23:58:17 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1686, average loss: 4.3287
[09/25 23:58:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/25 23:58:17 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/25 23:58:24 visual_prompt]: Epoch 72 / 100: avg data time: 4.68e-02, avg batch time: 0.4955, average train loss: 4.2228
[09/25 23:58:25 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1692, average loss: 4.1419
[09/25 23:58:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/25 23:58:25 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/25 23:58:32 visual_prompt]: Epoch 73 / 100: avg data time: 5.03e-02, avg batch time: 0.4987, average train loss: 4.3324
[09/25 23:58:33 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1690, average loss: 4.0909
[09/25 23:58:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.00	
[09/25 23:58:33 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/25 23:58:40 visual_prompt]: Epoch 74 / 100: avg data time: 4.03e-02, avg batch time: 0.4901, average train loss: 4.1197
[09/25 23:58:41 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1691, average loss: 4.1898
[09/25 23:58:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/25 23:58:41 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/25 23:58:48 visual_prompt]: Epoch 75 / 100: avg data time: 4.77e-02, avg batch time: 0.4969, average train loss: 4.1519
[09/25 23:58:49 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 4.1245
[09/25 23:58:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.00	
[09/25 23:58:49 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/25 23:58:56 visual_prompt]: Epoch 76 / 100: avg data time: 3.70e-02, avg batch time: 0.4851, average train loss: 4.0576
[09/25 23:58:57 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 3.9582
[09/25 23:58:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/25 23:58:57 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/25 23:59:04 visual_prompt]: Epoch 77 / 100: avg data time: 4.60e-02, avg batch time: 0.4939, average train loss: 4.0065
[09/25 23:59:06 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1688, average loss: 3.9700
[09/25 23:59:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 13.00	
[09/25 23:59:06 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/25 23:59:12 visual_prompt]: Epoch 78 / 100: avg data time: 4.45e-02, avg batch time: 0.4940, average train loss: 4.0259
[09/25 23:59:14 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1689, average loss: 3.9775
[09/25 23:59:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.00	
[09/25 23:59:14 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/25 23:59:20 visual_prompt]: Epoch 79 / 100: avg data time: 4.19e-02, avg batch time: 0.4906, average train loss: 3.9200
[09/25 23:59:22 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 4.0472
[09/25 23:59:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/25 23:59:22 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/25 23:59:28 visual_prompt]: Epoch 80 / 100: avg data time: 3.90e-02, avg batch time: 0.4895, average train loss: 3.9429
[09/25 23:59:30 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1693, average loss: 3.9272
[09/25 23:59:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/25 23:59:30 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/25 23:59:36 visual_prompt]: Epoch 81 / 100: avg data time: 4.00e-02, avg batch time: 0.4893, average train loss: 3.9141
[09/25 23:59:38 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1691, average loss: 3.9169
[09/25 23:59:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.00	top5: 10.50	
[09/25 23:59:38 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/25 23:59:44 visual_prompt]: Epoch 82 / 100: avg data time: 4.14e-02, avg batch time: 0.4924, average train loss: 3.9117
[09/25 23:59:46 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 3.9218
[09/25 23:59:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/25 23:59:46 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/25 23:59:52 visual_prompt]: Epoch 83 / 100: avg data time: 4.36e-02, avg batch time: 0.4929, average train loss: 3.9238
[09/25 23:59:54 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 3.8799
[09/25 23:59:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/25 23:59:54 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 00:00:00 visual_prompt]: Epoch 84 / 100: avg data time: 4.00e-02, avg batch time: 0.4914, average train loss: 3.9279
[09/26 00:00:02 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1692, average loss: 4.0105
[09/26 00:00:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.50	
[09/26 00:00:02 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 00:00:08 visual_prompt]: Epoch 85 / 100: avg data time: 4.35e-02, avg batch time: 0.4916, average train loss: 3.9098
[09/26 00:00:10 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 3.9654
[09/26 00:00:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 00:00:10 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 00:00:17 visual_prompt]: Epoch 86 / 100: avg data time: 4.83e-02, avg batch time: 0.4966, average train loss: 3.9189
[09/26 00:00:18 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1688, average loss: 3.9234
[09/26 00:00:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 00:00:18 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 00:00:25 visual_prompt]: Epoch 87 / 100: avg data time: 4.50e-02, avg batch time: 0.4935, average train loss: 3.8668
[09/26 00:00:26 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 3.9050
[09/26 00:00:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/26 00:00:26 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 00:00:33 visual_prompt]: Epoch 88 / 100: avg data time: 4.27e-02, avg batch time: 0.4918, average train loss: 3.8647
[09/26 00:00:34 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1692, average loss: 3.9305
[09/26 00:00:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/26 00:00:34 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 00:00:41 visual_prompt]: Epoch 89 / 100: avg data time: 3.63e-02, avg batch time: 0.4857, average train loss: 3.8595
[09/26 00:00:42 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1692, average loss: 3.9147
[09/26 00:00:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/26 00:00:42 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 00:00:49 visual_prompt]: Epoch 90 / 100: avg data time: 5.04e-02, avg batch time: 0.4988, average train loss: 3.8440
[09/26 00:00:50 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1690, average loss: 3.9089
[09/26 00:00:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 00:00:50 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 00:00:57 visual_prompt]: Epoch 91 / 100: avg data time: 3.97e-02, avg batch time: 0.4893, average train loss: 3.8434
[09/26 00:00:58 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 3.9117
[09/26 00:00:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.00	
[09/26 00:00:58 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 00:01:05 visual_prompt]: Epoch 92 / 100: avg data time: 4.59e-02, avg batch time: 0.4944, average train loss: 3.8473
[09/26 00:01:06 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.1693, average loss: 3.9073
[09/26 00:01:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.50	
[09/26 00:01:06 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 00:01:13 visual_prompt]: Epoch 93 / 100: avg data time: 4.42e-02, avg batch time: 0.4936, average train loss: 3.8406
[09/26 00:01:14 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1693, average loss: 3.9040
[09/26 00:01:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/26 00:01:14 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 00:01:21 visual_prompt]: Epoch 94 / 100: avg data time: 4.43e-02, avg batch time: 0.4921, average train loss: 3.8344
[09/26 00:01:22 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1690, average loss: 3.9031
[09/26 00:01:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 00:01:22 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 00:01:29 visual_prompt]: Epoch 95 / 100: avg data time: 5.20e-02, avg batch time: 0.5004, average train loss: 3.8271
[09/26 00:01:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 3.8991
[09/26 00:01:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.50	
[09/26 00:01:31 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 00:01:37 visual_prompt]: Epoch 96 / 100: avg data time: 4.59e-02, avg batch time: 0.4937, average train loss: 3.8238
[09/26 00:01:39 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 3.8979
[09/26 00:01:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.50	
[09/26 00:01:39 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 00:01:45 visual_prompt]: Epoch 97 / 100: avg data time: 4.99e-02, avg batch time: 0.4988, average train loss: 3.8003
[09/26 00:01:47 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1690, average loss: 3.8783
[09/26 00:01:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.00	
[09/26 00:01:47 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 00:01:53 visual_prompt]: Epoch 98 / 100: avg data time: 3.91e-02, avg batch time: 0.4878, average train loss: 3.7745
[09/26 00:01:55 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 3.8646
[09/26 00:01:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.50	
[09/26 00:01:55 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 00:02:01 visual_prompt]: Epoch 99 / 100: avg data time: 4.59e-02, avg batch time: 0.4950, average train loss: 3.7578
[09/26 00:02:03 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1692, average loss: 3.8411
[09/26 00:02:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 00:02:03 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 00:02:09 visual_prompt]: Epoch 100 / 100: avg data time: 4.10e-02, avg batch time: 0.4896, average train loss: 3.7188
[09/26 00:02:11 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1696, average loss: 3.8512
[09/26 00:02:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/26 00:02:11 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 00:02:11 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 00:02:11 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 00:02:11 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 00:02:11 visual_prompt]: Training with config:
[09/26 00:02:11 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 00:02:11 visual_prompt]: Loading training data...
[09/26 00:02:11 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 00:02:13 visual_prompt]: Number of images: 800
[09/26 00:02:13 visual_prompt]: Number of classes: 47 / 47
[09/26 00:02:13 visual_prompt]: Loading validation data...
[09/26 00:02:13 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 00:02:14 visual_prompt]: Number of images: 200
[09/26 00:02:14 visual_prompt]: Number of classes: 47 / 47
[09/26 00:02:14 visual_prompt]: Constructing models...
[09/26 00:02:16 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 00:02:16 visual_prompt]: tuned percent:0.576
[09/26 00:02:16 visual_prompt]: Device used for model: 0
[09/26 00:02:16 visual_prompt]: Setting up Evaluator...
[09/26 00:02:16 visual_prompt]: Setting up Trainer...
[09/26 00:02:16 visual_prompt]: 	Setting up the optimizer...
[09/26 00:02:16 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 00:02:23 visual_prompt]: Epoch 1 / 100: avg data time: 4.75e-02, avg batch time: 0.4953, average train loss: 3.9323
[09/26 00:02:24 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1691, average loss: 3.9045
[09/26 00:02:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 00:02:24 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 00:02:24 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 00:02:31 visual_prompt]: Epoch 2 / 100: avg data time: 5.35e-02, avg batch time: 0.5005, average train loss: 3.9968
[09/26 00:02:32 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1688, average loss: 4.0853
[09/26 00:02:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.50	
[09/26 00:02:32 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 00:02:39 visual_prompt]: Epoch 3 / 100: avg data time: 4.20e-02, avg batch time: 0.4904, average train loss: 3.9413
[09/26 00:02:40 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1687, average loss: 3.9963
[09/26 00:02:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 19.00	
[09/26 00:02:40 visual_prompt]: Best epoch 3: best metric: 0.050
[09/26 00:02:40 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 00:02:47 visual_prompt]: Epoch 4 / 100: avg data time: 4.51e-02, avg batch time: 0.4930, average train loss: 4.2368
[09/26 00:02:49 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1689, average loss: 4.2808
[09/26 00:02:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 00:02:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 00:02:55 visual_prompt]: Epoch 5 / 100: avg data time: 5.03e-02, avg batch time: 0.4980, average train loss: 4.1747
[09/26 00:02:57 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1687, average loss: 4.1026
[09/26 00:02:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 14.50	
[09/26 00:02:57 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 00:03:03 visual_prompt]: Epoch 6 / 100: avg data time: 4.64e-02, avg batch time: 0.4939, average train loss: 4.0412
[09/26 00:03:05 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1689, average loss: 4.4306
[09/26 00:03:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.50	
[09/26 00:03:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 00:03:12 visual_prompt]: Epoch 7 / 100: avg data time: 4.64e-02, avg batch time: 0.4943, average train loss: 4.8684
[09/26 00:03:13 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1689, average loss: 5.1434
[09/26 00:03:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/26 00:03:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 00:03:20 visual_prompt]: Epoch 8 / 100: avg data time: 5.20e-02, avg batch time: 0.4992, average train loss: 5.7401
[09/26 00:03:21 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 6.8431
[09/26 00:03:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 7.00	
[09/26 00:03:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 00:03:28 visual_prompt]: Epoch 9 / 100: avg data time: 4.59e-02, avg batch time: 0.4953, average train loss: 5.8808
[09/26 00:03:29 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1689, average loss: 4.9659
[09/26 00:03:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/26 00:03:29 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 00:03:36 visual_prompt]: Epoch 10 / 100: avg data time: 5.17e-02, avg batch time: 0.4998, average train loss: 6.2395
[09/26 00:03:38 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1691, average loss: 5.6778
[09/26 00:03:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 00:03:38 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 00:03:44 visual_prompt]: Epoch 11 / 100: avg data time: 4.67e-02, avg batch time: 0.4953, average train loss: 14.8479
[09/26 00:03:46 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1686, average loss: 42.3367
[09/26 00:03:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.00	
[09/26 00:03:46 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 00:03:52 visual_prompt]: Epoch 12 / 100: avg data time: 5.20e-02, avg batch time: 0.5005, average train loss: 35.1889
[09/26 00:03:54 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 33.8969
[09/26 00:03:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.00	
[09/26 00:03:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 00:04:00 visual_prompt]: Epoch 13 / 100: avg data time: 4.35e-02, avg batch time: 0.4918, average train loss: 38.3999
[09/26 00:04:02 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1689, average loss: 48.4048
[09/26 00:04:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 00:04:02 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 00:04:09 visual_prompt]: Epoch 14 / 100: avg data time: 4.85e-02, avg batch time: 0.4972, average train loss: 45.2657
[09/26 00:04:10 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1692, average loss: 48.6933
[09/26 00:04:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/26 00:04:10 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 00:04:17 visual_prompt]: Epoch 15 / 100: avg data time: 4.08e-02, avg batch time: 0.4907, average train loss: 49.5167
[09/26 00:04:18 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1690, average loss: 54.6164
[09/26 00:04:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 00:04:18 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 00:04:25 visual_prompt]: Epoch 16 / 100: avg data time: 3.98e-02, avg batch time: 0.4896, average train loss: 53.5260
[09/26 00:04:26 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1690, average loss: 45.8295
[09/26 00:04:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.00	
[09/26 00:04:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 00:04:33 visual_prompt]: Epoch 17 / 100: avg data time: 5.68e-02, avg batch time: 0.5038, average train loss: 45.3777
[09/26 00:04:34 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1690, average loss: 46.2163
[09/26 00:04:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/26 00:04:34 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 00:04:41 visual_prompt]: Epoch 18 / 100: avg data time: 5.58e-02, avg batch time: 0.5034, average train loss: 44.6884
[09/26 00:04:43 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 34.5693
[09/26 00:04:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/26 00:04:43 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 00:04:50 visual_prompt]: Epoch 19 / 100: avg data time: 5.59e-02, avg batch time: 0.5048, average train loss: 34.5394
[09/26 00:04:51 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1689, average loss: 30.2720
[09/26 00:04:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/26 00:04:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 00:04:58 visual_prompt]: Epoch 20 / 100: avg data time: 5.32e-02, avg batch time: 0.5004, average train loss: 39.4716
[09/26 00:04:59 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.1691, average loss: 53.6649
[09/26 00:04:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 00:04:59 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 00:05:06 visual_prompt]: Epoch 21 / 100: avg data time: 5.40e-02, avg batch time: 0.5027, average train loss: 54.6941
[09/26 00:05:07 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1689, average loss: 62.8551
[09/26 00:05:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/26 00:05:07 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 00:05:14 visual_prompt]: Epoch 22 / 100: avg data time: 4.74e-02, avg batch time: 0.4952, average train loss: 52.5062
[09/26 00:05:15 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1690, average loss: 43.8313
[09/26 00:05:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/26 00:05:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 00:05:22 visual_prompt]: Epoch 23 / 100: avg data time: 5.21e-02, avg batch time: 0.5004, average train loss: 41.9414
[09/26 00:05:24 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 40.4952
[09/26 00:05:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 00:05:24 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 00:05:30 visual_prompt]: Epoch 24 / 100: avg data time: 4.76e-02, avg batch time: 0.4963, average train loss: 36.9019
[09/26 00:05:32 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1687, average loss: 41.8353
[09/26 00:05:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 6.50	
[09/26 00:05:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 00:05:39 visual_prompt]: Epoch 25 / 100: avg data time: 5.35e-02, avg batch time: 0.5014, average train loss: 39.9918
[09/26 00:05:40 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1688, average loss: 33.7244
[09/26 00:05:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 14.00	
[09/26 00:05:40 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 00:05:47 visual_prompt]: Epoch 26 / 100: avg data time: 5.45e-02, avg batch time: 0.5033, average train loss: 29.4595
[09/26 00:05:48 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1689, average loss: 30.3624
[09/26 00:05:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 6.50	
[09/26 00:05:48 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 00:05:55 visual_prompt]: Epoch 27 / 100: avg data time: 5.67e-02, avg batch time: 0.5046, average train loss: 29.4698
[09/26 00:05:57 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1691, average loss: 27.0516
[09/26 00:05:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 14.50	
[09/26 00:05:57 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 00:06:04 visual_prompt]: Epoch 28 / 100: avg data time: 5.86e-02, avg batch time: 0.5068, average train loss: 29.5229
[09/26 00:06:05 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1696, average loss: 29.7082
[09/26 00:06:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 14.00	
[09/26 00:06:05 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 00:06:12 visual_prompt]: Epoch 29 / 100: avg data time: 4.37e-02, avg batch time: 0.4929, average train loss: 34.3775
[09/26 00:06:13 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1692, average loss: 30.9615
[09/26 00:06:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 15.50	
[09/26 00:06:13 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 00:06:20 visual_prompt]: Epoch 30 / 100: avg data time: 5.23e-02, avg batch time: 0.5009, average train loss: 36.5514
[09/26 00:06:21 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1685, average loss: 39.6631
[09/26 00:06:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/26 00:06:21 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 00:06:28 visual_prompt]: Epoch 31 / 100: avg data time: 5.36e-02, avg batch time: 0.5007, average train loss: 37.1417
[09/26 00:06:29 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1686, average loss: 38.1327
[09/26 00:06:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/26 00:06:29 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 00:06:36 visual_prompt]: Epoch 32 / 100: avg data time: 4.42e-02, avg batch time: 0.4925, average train loss: 39.8578
[09/26 00:06:37 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1692, average loss: 32.1702
[09/26 00:06:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/26 00:06:37 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 00:06:44 visual_prompt]: Epoch 33 / 100: avg data time: 5.61e-02, avg batch time: 0.5063, average train loss: 33.9290
[09/26 00:06:46 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 33.4405
[09/26 00:06:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 6.00	
[09/26 00:06:46 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 00:06:53 visual_prompt]: Epoch 34 / 100: avg data time: 5.64e-02, avg batch time: 0.5042, average train loss: 31.3035
[09/26 00:06:54 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 25.5305
[09/26 00:06:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/26 00:06:54 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 00:07:01 visual_prompt]: Epoch 35 / 100: avg data time: 4.85e-02, avg batch time: 0.4964, average train loss: 24.4471
[09/26 00:07:02 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1687, average loss: 23.5106
[09/26 00:07:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 00:07:02 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 00:07:09 visual_prompt]: Epoch 36 / 100: avg data time: 5.78e-02, avg batch time: 0.5059, average train loss: 30.4984
[09/26 00:07:10 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1693, average loss: 30.8825
[09/26 00:07:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/26 00:07:10 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 00:07:17 visual_prompt]: Epoch 37 / 100: avg data time: 4.39e-02, avg batch time: 0.4933, average train loss: 27.9982
[09/26 00:07:19 visual_prompt]: Inference (val):avg data time: 4.72e-05, avg batch time: 0.1688, average loss: 23.6693
[09/26 00:07:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/26 00:07:19 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 00:07:25 visual_prompt]: Epoch 38 / 100: avg data time: 5.12e-02, avg batch time: 0.4990, average train loss: 22.4929
[09/26 00:07:27 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 18.6380
[09/26 00:07:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/26 00:07:27 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 00:07:34 visual_prompt]: Epoch 39 / 100: avg data time: 5.28e-02, avg batch time: 0.5018, average train loss: 18.3205
[09/26 00:07:35 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1688, average loss: 15.7424
[09/26 00:07:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/26 00:07:35 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 00:07:42 visual_prompt]: Epoch 40 / 100: avg data time: 5.43e-02, avg batch time: 0.5029, average train loss: 15.4185
[09/26 00:07:43 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1691, average loss: 12.1988
[09/26 00:07:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 00:07:43 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 00:07:50 visual_prompt]: Epoch 41 / 100: avg data time: 4.28e-02, avg batch time: 0.4921, average train loss: 13.4777
[09/26 00:07:51 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1687, average loss: 12.5633
[09/26 00:07:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/26 00:07:51 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 00:07:58 visual_prompt]: Epoch 42 / 100: avg data time: 5.19e-02, avg batch time: 0.5008, average train loss: 15.5664
[09/26 00:08:00 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1689, average loss: 14.0389
[09/26 00:08:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/26 00:08:00 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 00:08:06 visual_prompt]: Epoch 43 / 100: avg data time: 4.01e-02, avg batch time: 0.4902, average train loss: 17.6468
[09/26 00:08:08 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1692, average loss: 13.1072
[09/26 00:08:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 00:08:08 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 00:08:15 visual_prompt]: Epoch 44 / 100: avg data time: 5.45e-02, avg batch time: 0.5036, average train loss: 15.9575
[09/26 00:08:16 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 15.8947
[09/26 00:08:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 00:08:16 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 00:08:23 visual_prompt]: Epoch 45 / 100: avg data time: 5.76e-02, avg batch time: 0.5052, average train loss: 19.1189
[09/26 00:08:24 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 14.0298
[09/26 00:08:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 00:08:24 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 00:08:31 visual_prompt]: Epoch 46 / 100: avg data time: 5.23e-02, avg batch time: 0.5007, average train loss: 16.0879
[09/26 00:08:33 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1688, average loss: 12.7636
[09/26 00:08:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/26 00:08:33 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 00:08:39 visual_prompt]: Epoch 47 / 100: avg data time: 5.61e-02, avg batch time: 0.5036, average train loss: 12.2256
[09/26 00:08:41 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1692, average loss: 14.2868
[09/26 00:08:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.50	
[09/26 00:08:41 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 00:08:48 visual_prompt]: Epoch 48 / 100: avg data time: 5.32e-02, avg batch time: 0.5013, average train loss: 12.6711
[09/26 00:08:49 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1690, average loss: 10.4113
[09/26 00:08:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/26 00:08:49 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 00:08:56 visual_prompt]: Epoch 49 / 100: avg data time: 4.85e-02, avg batch time: 0.4968, average train loss: 13.6759
[09/26 00:08:57 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1689, average loss: 8.5293
[09/26 00:08:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.00	
[09/26 00:08:57 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 00:09:04 visual_prompt]: Epoch 50 / 100: avg data time: 5.60e-02, avg batch time: 0.5038, average train loss: 9.1457
[09/26 00:09:05 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1688, average loss: 8.4780
[09/26 00:09:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/26 00:09:05 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 00:09:12 visual_prompt]: Epoch 51 / 100: avg data time: 4.61e-02, avg batch time: 0.4960, average train loss: 9.7339
[09/26 00:09:13 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 8.5648
[09/26 00:09:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.50	
[09/26 00:09:13 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 00:09:20 visual_prompt]: Epoch 52 / 100: avg data time: 4.86e-02, avg batch time: 0.4975, average train loss: 8.4743
[09/26 00:09:22 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1692, average loss: 12.1352
[09/26 00:09:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 5.50	
[09/26 00:09:22 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 00:09:28 visual_prompt]: Epoch 53 / 100: avg data time: 5.22e-02, avg batch time: 0.5008, average train loss: 9.9363
[09/26 00:09:30 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 8.5789
[09/26 00:09:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 00:09:30 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 00:09:36 visual_prompt]: Epoch 54 / 100: avg data time: 5.27e-02, avg batch time: 0.5018, average train loss: 9.6016
[09/26 00:09:38 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 9.6493
[09/26 00:09:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.50	
[09/26 00:09:38 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 00:09:45 visual_prompt]: Epoch 55 / 100: avg data time: 5.42e-02, avg batch time: 0.5019, average train loss: 8.1065
[09/26 00:09:46 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 7.4838
[09/26 00:09:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/26 00:09:46 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 00:09:53 visual_prompt]: Epoch 56 / 100: avg data time: 4.81e-02, avg batch time: 0.4964, average train loss: 6.1643
[09/26 00:09:54 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 5.1069
[09/26 00:09:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.50	
[09/26 00:09:54 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 00:10:01 visual_prompt]: Epoch 57 / 100: avg data time: 5.52e-02, avg batch time: 0.5049, average train loss: 4.6204
[09/26 00:10:03 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 4.6048
[09/26 00:10:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 14.00	
[09/26 00:10:03 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 00:10:09 visual_prompt]: Epoch 58 / 100: avg data time: 4.77e-02, avg batch time: 0.4953, average train loss: 4.7271
[09/26 00:10:11 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1689, average loss: 4.9898
[09/26 00:10:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.50	
[09/26 00:10:11 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 00:10:17 visual_prompt]: Epoch 59 / 100: avg data time: 5.10e-02, avg batch time: 0.4994, average train loss: 4.5805
[09/26 00:10:19 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1691, average loss: 4.7356
[09/26 00:10:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 00:10:19 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 00:10:26 visual_prompt]: Epoch 60 / 100: avg data time: 5.18e-02, avg batch time: 0.4995, average train loss: 4.5436
[09/26 00:10:27 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1689, average loss: 4.4719
[09/26 00:10:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/26 00:10:27 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 00:10:34 visual_prompt]: Epoch 61 / 100: avg data time: 5.11e-02, avg batch time: 0.5002, average train loss: 4.3887
[09/26 00:10:35 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1690, average loss: 4.5091
[09/26 00:10:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.50	
[09/26 00:10:35 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 00:10:42 visual_prompt]: Epoch 62 / 100: avg data time: 5.25e-02, avg batch time: 0.5011, average train loss: 4.2349
[09/26 00:10:43 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1689, average loss: 4.2212
[09/26 00:10:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.50	
[09/26 00:10:43 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 00:10:50 visual_prompt]: Epoch 63 / 100: avg data time: 5.61e-02, avg batch time: 0.5042, average train loss: 4.8195
[09/26 00:10:52 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1690, average loss: 4.5427
[09/26 00:10:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 7.00	
[09/26 00:10:52 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 00:10:58 visual_prompt]: Epoch 64 / 100: avg data time: 5.06e-02, avg batch time: 0.4989, average train loss: 4.2678
[09/26 00:11:00 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 4.3759
[09/26 00:11:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.00	
[09/26 00:11:00 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 00:11:07 visual_prompt]: Epoch 65 / 100: avg data time: 3.98e-02, avg batch time: 0.4962, average train loss: 4.2292
[09/26 00:11:08 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 4.2825
[09/26 00:11:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 00:11:08 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 00:11:15 visual_prompt]: Epoch 66 / 100: avg data time: 5.66e-02, avg batch time: 0.5045, average train loss: 4.1260
[09/26 00:11:16 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1689, average loss: 4.2785
[09/26 00:11:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 00:11:16 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 00:11:23 visual_prompt]: Epoch 67 / 100: avg data time: 5.42e-02, avg batch time: 0.5028, average train loss: 4.0622
[09/26 00:11:24 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1691, average loss: 4.2249
[09/26 00:11:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 10.50	
[09/26 00:11:24 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 00:11:31 visual_prompt]: Epoch 68 / 100: avg data time: 5.68e-02, avg batch time: 0.5059, average train loss: 4.0462
[09/26 00:11:33 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1692, average loss: 4.1506
[09/26 00:11:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 14.50	
[09/26 00:11:33 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 00:11:39 visual_prompt]: Epoch 69 / 100: avg data time: 3.93e-02, avg batch time: 0.4875, average train loss: 3.9805
[09/26 00:11:41 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 4.2134
[09/26 00:11:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.00	
[09/26 00:11:41 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 00:11:47 visual_prompt]: Epoch 70 / 100: avg data time: 5.13e-02, avg batch time: 0.5011, average train loss: 3.8826
[09/26 00:11:49 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 3.6471
[09/26 00:11:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.00	top5: 22.00	
[09/26 00:11:49 visual_prompt]: Best epoch 70: best metric: 0.070
[09/26 00:11:49 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 00:11:56 visual_prompt]: Epoch 71 / 100: avg data time: 4.78e-02, avg batch time: 0.4960, average train loss: 3.6093
[09/26 00:11:57 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1692, average loss: 3.7273
[09/26 00:11:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.50	top5: 27.00	
[09/26 00:11:57 visual_prompt]: Best epoch 71: best metric: 0.075
[09/26 00:11:57 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 00:12:04 visual_prompt]: Epoch 72 / 100: avg data time: 5.39e-02, avg batch time: 0.5031, average train loss: 3.3613
[09/26 00:12:05 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 3.2837
[09/26 00:12:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.00	top5: 40.50	
[09/26 00:12:05 visual_prompt]: Best epoch 72: best metric: 0.130
[09/26 00:12:05 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 00:12:12 visual_prompt]: Epoch 73 / 100: avg data time: 5.69e-02, avg batch time: 0.5072, average train loss: 2.9728
[09/26 00:12:14 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1689, average loss: 3.5407
[09/26 00:12:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.00	top5: 28.00	
[09/26 00:12:14 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 00:12:20 visual_prompt]: Epoch 74 / 100: avg data time: 4.19e-02, avg batch time: 0.4919, average train loss: 2.9006
[09/26 00:12:22 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 3.0128
[09/26 00:12:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 55.00	
[09/26 00:12:22 visual_prompt]: Best epoch 74: best metric: 0.220
[09/26 00:12:22 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 00:12:28 visual_prompt]: Epoch 75 / 100: avg data time: 5.39e-02, avg batch time: 0.5019, average train loss: 2.2825
[09/26 00:12:30 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1691, average loss: 3.2723
[09/26 00:12:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.00	top5: 58.00	
[09/26 00:12:30 visual_prompt]: Best epoch 75: best metric: 0.270
[09/26 00:12:30 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 00:12:37 visual_prompt]: Epoch 76 / 100: avg data time: 5.13e-02, avg batch time: 0.4994, average train loss: 2.1658
[09/26 00:12:38 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 2.6843
[09/26 00:12:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 32.00	top5: 63.50	
[09/26 00:12:38 visual_prompt]: Best epoch 76: best metric: 0.320
[09/26 00:12:38 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 00:12:45 visual_prompt]: Epoch 77 / 100: avg data time: 5.09e-02, avg batch time: 0.5001, average train loss: 1.6230
[09/26 00:12:46 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1691, average loss: 2.6900
[09/26 00:12:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.00	top5: 67.00	
[09/26 00:12:46 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 00:12:53 visual_prompt]: Epoch 78 / 100: avg data time: 5.85e-02, avg batch time: 0.5074, average train loss: 0.9868
[09/26 00:12:54 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 2.3512
[09/26 00:12:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 72.50	
[09/26 00:12:54 visual_prompt]: Best epoch 78: best metric: 0.430
[09/26 00:12:54 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 00:13:01 visual_prompt]: Epoch 79 / 100: avg data time: 5.22e-02, avg batch time: 0.5006, average train loss: 0.6640
[09/26 00:13:02 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1694, average loss: 2.2561
[09/26 00:13:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 78.50	
[09/26 00:13:02 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 00:13:09 visual_prompt]: Epoch 80 / 100: avg data time: 3.90e-02, avg batch time: 0.4888, average train loss: 0.4096
[09/26 00:13:11 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1693, average loss: 2.0136
[09/26 00:13:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 81.00	
[09/26 00:13:11 visual_prompt]: Best epoch 80: best metric: 0.485
[09/26 00:13:11 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 00:13:17 visual_prompt]: Epoch 81 / 100: avg data time: 4.34e-02, avg batch time: 0.4926, average train loss: 0.2448
[09/26 00:13:19 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 1.9425
[09/26 00:13:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 81.50	
[09/26 00:13:19 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 00:13:25 visual_prompt]: Epoch 82 / 100: avg data time: 4.25e-02, avg batch time: 0.4936, average train loss: 0.1516
[09/26 00:13:27 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1696, average loss: 1.9542
[09/26 00:13:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 84.50	
[09/26 00:13:27 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 00:13:34 visual_prompt]: Epoch 83 / 100: avg data time: 5.66e-02, avg batch time: 0.5046, average train loss: 0.0978
[09/26 00:13:35 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1696, average loss: 1.6926
[09/26 00:13:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 88.00	
[09/26 00:13:35 visual_prompt]: Best epoch 83: best metric: 0.520
[09/26 00:13:35 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 00:13:42 visual_prompt]: Epoch 84 / 100: avg data time: 5.88e-02, avg batch time: 0.5066, average train loss: 0.0653
[09/26 00:13:43 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1691, average loss: 1.6817
[09/26 00:13:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.00	
[09/26 00:13:43 visual_prompt]: Best epoch 84: best metric: 0.535
[09/26 00:13:43 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 00:13:50 visual_prompt]: Epoch 85 / 100: avg data time: 5.78e-02, avg batch time: 0.5063, average train loss: 0.0556
[09/26 00:13:52 visual_prompt]: Inference (val):avg data time: 4.98e-05, avg batch time: 0.1690, average loss: 1.6353
[09/26 00:13:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 89.00	
[09/26 00:13:52 visual_prompt]: Best epoch 85: best metric: 0.555
[09/26 00:13:52 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 00:13:58 visual_prompt]: Epoch 86 / 100: avg data time: 5.31e-02, avg batch time: 0.5014, average train loss: 0.0519
[09/26 00:14:00 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1694, average loss: 1.6212
[09/26 00:14:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 88.50	
[09/26 00:14:00 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 00:14:06 visual_prompt]: Epoch 87 / 100: avg data time: 3.96e-02, avg batch time: 0.4879, average train loss: 0.0506
[09/26 00:14:08 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1695, average loss: 1.6464
[09/26 00:14:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 87.50	
[09/26 00:14:08 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 00:14:15 visual_prompt]: Epoch 88 / 100: avg data time: 4.55e-02, avg batch time: 0.4947, average train loss: 0.0479
[09/26 00:14:16 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1694, average loss: 1.6527
[09/26 00:14:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 86.00	
[09/26 00:14:16 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 00:14:23 visual_prompt]: Epoch 89 / 100: avg data time: 4.40e-02, avg batch time: 0.4936, average train loss: 0.0469
[09/26 00:14:24 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1696, average loss: 1.6195
[09/26 00:14:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 89.00	
[09/26 00:14:24 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 00:14:31 visual_prompt]: Epoch 90 / 100: avg data time: 5.61e-02, avg batch time: 0.5041, average train loss: 0.0445
[09/26 00:14:32 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 1.6446
[09/26 00:14:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 88.00	
[09/26 00:14:32 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 00:14:39 visual_prompt]: Epoch 91 / 100: avg data time: 5.10e-02, avg batch time: 0.4989, average train loss: 0.0421
[09/26 00:14:41 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1695, average loss: 1.6410
[09/26 00:14:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 86.00	
[09/26 00:14:41 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 00:14:47 visual_prompt]: Epoch 92 / 100: avg data time: 6.02e-02, avg batch time: 0.5081, average train loss: 0.0409
[09/26 00:14:49 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1689, average loss: 1.6271
[09/26 00:14:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 87.50	
[09/26 00:14:49 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 00:14:55 visual_prompt]: Epoch 93 / 100: avg data time: 4.09e-02, avg batch time: 0.4890, average train loss: 0.0383
[09/26 00:14:57 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1694, average loss: 1.6420
[09/26 00:14:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 85.50	
[09/26 00:14:57 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 00:15:04 visual_prompt]: Epoch 94 / 100: avg data time: 5.50e-02, avg batch time: 0.5042, average train loss: 0.0366
[09/26 00:15:05 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1690, average loss: 1.6426
[09/26 00:15:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 86.00	
[09/26 00:15:05 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 00:15:12 visual_prompt]: Epoch 95 / 100: avg data time: 5.24e-02, avg batch time: 0.5004, average train loss: 0.0361
[09/26 00:15:13 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1691, average loss: 1.6434
[09/26 00:15:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 86.50	
[09/26 00:15:13 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 00:15:20 visual_prompt]: Epoch 96 / 100: avg data time: 6.49e-02, avg batch time: 0.5136, average train loss: 0.0348
[09/26 00:15:22 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1692, average loss: 1.6394
[09/26 00:15:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 86.50	
[09/26 00:15:22 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 00:15:28 visual_prompt]: Epoch 97 / 100: avg data time: 4.60e-02, avg batch time: 0.4956, average train loss: 0.0338
[09/26 00:15:30 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1692, average loss: 1.6422
[09/26 00:15:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 86.00	
[09/26 00:15:30 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 00:15:37 visual_prompt]: Epoch 98 / 100: avg data time: 5.24e-02, avg batch time: 0.5011, average train loss: 0.0338
[09/26 00:15:38 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 1.6444
[09/26 00:15:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 86.00	
[09/26 00:15:38 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 00:15:45 visual_prompt]: Epoch 99 / 100: avg data time: 3.62e-02, avg batch time: 0.4882, average train loss: 0.0329
[09/26 00:15:46 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1689, average loss: 1.6456
[09/26 00:15:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 86.00	
[09/26 00:15:46 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 00:15:53 visual_prompt]: Epoch 100 / 100: avg data time: 5.04e-02, avg batch time: 0.4986, average train loss: 0.0329
[09/26 00:15:54 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1689, average loss: 1.6454
[09/26 00:15:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 86.00	
[09/26 00:15:54 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 00:15:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 00:15:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 00:15:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 00:15:54 visual_prompt]: Training with config:
[09/26 00:15:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 00:15:54 visual_prompt]: Loading training data...
[09/26 00:15:54 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 00:15:56 visual_prompt]: Number of images: 800
[09/26 00:15:56 visual_prompt]: Number of classes: 47 / 47
[09/26 00:15:56 visual_prompt]: Loading validation data...
[09/26 00:15:56 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 00:15:57 visual_prompt]: Number of images: 200
[09/26 00:15:57 visual_prompt]: Number of classes: 47 / 47
[09/26 00:15:57 visual_prompt]: Constructing models...
[09/26 00:16:00 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 00:16:00 visual_prompt]: tuned percent:0.576
[09/26 00:16:00 visual_prompt]: Device used for model: 0
[09/26 00:16:00 visual_prompt]: Setting up Evaluator...
[09/26 00:16:00 visual_prompt]: Setting up Trainer...
[09/26 00:16:00 visual_prompt]: 	Setting up the optimizer...
[09/26 00:16:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 00:16:06 visual_prompt]: Epoch 1 / 100: avg data time: 5.65e-02, avg batch time: 0.5024, average train loss: 3.9319
[09/26 00:16:08 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1687, average loss: 3.9045
[09/26 00:16:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 00:16:08 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 00:16:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 00:16:15 visual_prompt]: Epoch 2 / 100: avg data time: 5.26e-02, avg batch time: 0.5012, average train loss: 3.9552
[09/26 00:16:16 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1689, average loss: 4.1609
[09/26 00:16:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/26 00:16:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 00:16:23 visual_prompt]: Epoch 3 / 100: avg data time: 4.59e-02, avg batch time: 0.4934, average train loss: 3.9777
[09/26 00:16:24 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 3.9209
[09/26 00:16:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 13.50	
[09/26 00:16:24 visual_prompt]: Best epoch 3: best metric: 0.050
[09/26 00:16:24 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 00:16:31 visual_prompt]: Epoch 4 / 100: avg data time: 4.76e-02, avg batch time: 0.4971, average train loss: 4.0279
[09/26 00:16:33 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1687, average loss: 4.4370
[09/26 00:16:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 22.00	
[09/26 00:16:33 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 00:16:39 visual_prompt]: Epoch 5 / 100: avg data time: 5.56e-02, avg batch time: 0.5026, average train loss: 4.0047
[09/26 00:16:41 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1690, average loss: 6.3492
[09/26 00:16:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 14.50	
[09/26 00:16:41 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 00:16:48 visual_prompt]: Epoch 6 / 100: avg data time: 4.13e-02, avg batch time: 0.4908, average train loss: 5.5491
[09/26 00:16:49 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1690, average loss: 5.8781
[09/26 00:16:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.50	top5: 12.00	
[09/26 00:16:49 visual_prompt]: Best epoch 6: best metric: 0.055
[09/26 00:16:49 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 00:16:56 visual_prompt]: Epoch 7 / 100: avg data time: 5.19e-02, avg batch time: 0.4998, average train loss: 5.4379
[09/26 00:16:57 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1686, average loss: 7.8510
[09/26 00:16:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 13.50	
[09/26 00:16:57 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 00:17:04 visual_prompt]: Epoch 8 / 100: avg data time: 5.36e-02, avg batch time: 0.5029, average train loss: 7.7414
[09/26 00:17:05 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 6.9306
[09/26 00:17:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 17.50	
[09/26 00:17:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 00:17:12 visual_prompt]: Epoch 9 / 100: avg data time: 5.05e-02, avg batch time: 0.4988, average train loss: 6.5808
[09/26 00:17:14 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1694, average loss: 4.6541
[09/26 00:17:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.00	top5: 21.00	
[09/26 00:17:14 visual_prompt]: Best epoch 9: best metric: 0.090
[09/26 00:17:14 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 00:17:20 visual_prompt]: Epoch 10 / 100: avg data time: 3.91e-02, avg batch time: 0.4865, average train loss: 5.1357
[09/26 00:17:22 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1686, average loss: 5.5659
[09/26 00:17:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 29.00	
[09/26 00:17:22 visual_prompt]: Best epoch 10: best metric: 0.095
[09/26 00:17:22 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 00:17:28 visual_prompt]: Epoch 11 / 100: avg data time: 5.23e-02, avg batch time: 0.5014, average train loss: 6.4148
[09/26 00:17:30 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1687, average loss: 11.3911
[09/26 00:17:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.00	
[09/26 00:17:30 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 00:17:37 visual_prompt]: Epoch 12 / 100: avg data time: 5.18e-02, avg batch time: 0.4994, average train loss: 12.8147
[09/26 00:17:38 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 14.0073
[09/26 00:17:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 17.50	
[09/26 00:17:38 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 00:17:45 visual_prompt]: Epoch 13 / 100: avg data time: 5.16e-02, avg batch time: 0.5003, average train loss: 19.6794
[09/26 00:17:46 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 19.8929
[09/26 00:17:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 14.50	
[09/26 00:17:46 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 00:17:53 visual_prompt]: Epoch 14 / 100: avg data time: 5.20e-02, avg batch time: 0.4996, average train loss: 18.1762
[09/26 00:17:54 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1689, average loss: 21.2530
[09/26 00:17:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.00	
[09/26 00:17:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 00:18:01 visual_prompt]: Epoch 15 / 100: avg data time: 4.37e-02, avg batch time: 0.4950, average train loss: 37.3005
[09/26 00:18:03 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1690, average loss: 28.5520
[09/26 00:18:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 00:18:03 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 00:18:09 visual_prompt]: Epoch 16 / 100: avg data time: 4.89e-02, avg batch time: 0.4972, average train loss: 35.0005
[09/26 00:18:11 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 30.0482
[09/26 00:18:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.50	
[09/26 00:18:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 00:18:18 visual_prompt]: Epoch 17 / 100: avg data time: 4.49e-02, avg batch time: 0.4941, average train loss: 45.4932
[09/26 00:18:19 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1689, average loss: 38.3165
[09/26 00:18:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.50	
[09/26 00:18:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 00:18:26 visual_prompt]: Epoch 18 / 100: avg data time: 5.20e-02, avg batch time: 0.5003, average train loss: 39.0001
[09/26 00:18:27 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1686, average loss: 36.0579
[09/26 00:18:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.50	
[09/26 00:18:27 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 00:18:34 visual_prompt]: Epoch 19 / 100: avg data time: 3.93e-02, avg batch time: 0.4892, average train loss: 35.0287
[09/26 00:18:35 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1692, average loss: 31.9610
[09/26 00:18:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 17.00	
[09/26 00:18:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 00:18:42 visual_prompt]: Epoch 20 / 100: avg data time: 4.73e-02, avg batch time: 0.4956, average train loss: 34.0254
[09/26 00:18:44 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1685, average loss: 31.8658
[09/26 00:18:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/26 00:18:44 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 00:18:50 visual_prompt]: Epoch 21 / 100: avg data time: 4.78e-02, avg batch time: 0.4954, average train loss: 30.9197
[09/26 00:18:52 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1687, average loss: 26.9813
[09/26 00:18:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/26 00:18:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 00:18:59 visual_prompt]: Epoch 22 / 100: avg data time: 5.32e-02, avg batch time: 0.5009, average train loss: 29.1374
[09/26 00:19:00 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1688, average loss: 24.7781
[09/26 00:19:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.00	
[09/26 00:19:00 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 00:19:07 visual_prompt]: Epoch 23 / 100: avg data time: 5.18e-02, avg batch time: 0.4992, average train loss: 21.6014
[09/26 00:19:08 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 18.3944
[09/26 00:19:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/26 00:19:08 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 00:19:15 visual_prompt]: Epoch 24 / 100: avg data time: 5.13e-02, avg batch time: 0.4991, average train loss: 17.0465
[09/26 00:19:16 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1687, average loss: 11.6938
[09/26 00:19:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/26 00:19:16 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 00:19:23 visual_prompt]: Epoch 25 / 100: avg data time: 4.56e-02, avg batch time: 0.4937, average train loss: 14.3104
[09/26 00:19:24 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1689, average loss: 13.1926
[09/26 00:19:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 6.00	
[09/26 00:19:24 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 00:19:31 visual_prompt]: Epoch 26 / 100: avg data time: 4.39e-02, avg batch time: 0.4938, average train loss: 12.6476
[09/26 00:19:33 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1688, average loss: 8.9813
[09/26 00:19:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.50	
[09/26 00:19:33 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 00:19:39 visual_prompt]: Epoch 27 / 100: avg data time: 5.23e-02, avg batch time: 0.5007, average train loss: 9.7859
[09/26 00:19:41 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 6.7585
[09/26 00:19:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 12.00	
[09/26 00:19:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 00:19:47 visual_prompt]: Epoch 28 / 100: avg data time: 3.92e-02, avg batch time: 0.4880, average train loss: 6.0893
[09/26 00:19:49 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1689, average loss: 5.0997
[09/26 00:19:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 14.00	
[09/26 00:19:49 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 00:19:56 visual_prompt]: Epoch 29 / 100: avg data time: 4.18e-02, avg batch time: 0.4901, average train loss: 4.9452
[09/26 00:19:57 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 4.4009
[09/26 00:19:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 22.00	
[09/26 00:19:57 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 00:20:04 visual_prompt]: Epoch 30 / 100: avg data time: 3.77e-02, avg batch time: 0.4864, average train loss: 4.1827
[09/26 00:20:05 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 4.0872
[09/26 00:20:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.50	top5: 17.50	
[09/26 00:20:05 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 00:20:12 visual_prompt]: Epoch 31 / 100: avg data time: 4.70e-02, avg batch time: 0.4967, average train loss: 3.8611
[09/26 00:20:13 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 4.0446
[09/26 00:20:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 28.00	
[09/26 00:20:13 visual_prompt]: Best epoch 31: best metric: 0.110
[09/26 00:20:13 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 00:20:20 visual_prompt]: Epoch 32 / 100: avg data time: 5.47e-02, avg batch time: 0.5027, average train loss: 3.6161
[09/26 00:20:21 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1690, average loss: 3.6506
[09/26 00:20:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 14.50	top5: 36.00	
[09/26 00:20:21 visual_prompt]: Best epoch 32: best metric: 0.145
[09/26 00:20:21 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 00:20:28 visual_prompt]: Epoch 33 / 100: avg data time: 5.36e-02, avg batch time: 0.5026, average train loss: 3.0996
[09/26 00:20:29 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1695, average loss: 3.3388
[09/26 00:20:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.50	top5: 49.00	
[09/26 00:20:29 visual_prompt]: Best epoch 33: best metric: 0.165
[09/26 00:20:29 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 00:20:36 visual_prompt]: Epoch 34 / 100: avg data time: 5.13e-02, avg batch time: 0.5008, average train loss: 2.8252
[09/26 00:20:38 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1690, average loss: 3.3638
[09/26 00:20:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.00	top5: 44.00	
[09/26 00:20:38 visual_prompt]: Best epoch 34: best metric: 0.230
[09/26 00:20:38 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 00:20:44 visual_prompt]: Epoch 35 / 100: avg data time: 4.64e-02, avg batch time: 0.4951, average train loss: 2.5297
[09/26 00:20:46 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1691, average loss: 3.6238
[09/26 00:20:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.00	top5: 45.00	
[09/26 00:20:46 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 00:20:53 visual_prompt]: Epoch 36 / 100: avg data time: 5.07e-02, avg batch time: 0.5010, average train loss: 2.3507
[09/26 00:20:54 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1693, average loss: 2.8335
[09/26 00:20:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.00	top5: 62.50	
[09/26 00:20:54 visual_prompt]: Best epoch 36: best metric: 0.260
[09/26 00:20:54 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 00:21:01 visual_prompt]: Epoch 37 / 100: avg data time: 5.75e-02, avg batch time: 0.5068, average train loss: 1.8899
[09/26 00:21:02 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 3.1119
[09/26 00:21:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.00	top5: 67.00	
[09/26 00:21:02 visual_prompt]: Best epoch 37: best metric: 0.300
[09/26 00:21:02 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 00:21:09 visual_prompt]: Epoch 38 / 100: avg data time: 5.44e-02, avg batch time: 0.5026, average train loss: 1.2989
[09/26 00:21:10 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1691, average loss: 3.5756
[09/26 00:21:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.50	top5: 63.50	
[09/26 00:21:10 visual_prompt]: Best epoch 38: best metric: 0.305
[09/26 00:21:10 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 00:21:17 visual_prompt]: Epoch 39 / 100: avg data time: 5.56e-02, avg batch time: 0.5048, average train loss: 1.2118
[09/26 00:21:19 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1691, average loss: 2.9809
[09/26 00:21:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.00	top5: 69.00	
[09/26 00:21:19 visual_prompt]: Best epoch 39: best metric: 0.370
[09/26 00:21:19 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 00:21:25 visual_prompt]: Epoch 40 / 100: avg data time: 5.32e-02, avg batch time: 0.5014, average train loss: 0.9746
[09/26 00:21:27 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1693, average loss: 2.7170
[09/26 00:21:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.50	top5: 76.00	
[09/26 00:21:27 visual_prompt]: Best epoch 40: best metric: 0.375
[09/26 00:21:27 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 00:21:33 visual_prompt]: Epoch 41 / 100: avg data time: 4.74e-02, avg batch time: 0.4960, average train loss: 0.7532
[09/26 00:21:35 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 3.3091
[09/26 00:21:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 35.50	top5: 68.50	
[09/26 00:21:35 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 00:21:42 visual_prompt]: Epoch 42 / 100: avg data time: 5.56e-02, avg batch time: 0.5042, average train loss: 0.8210
[09/26 00:21:43 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1690, average loss: 3.7135
[09/26 00:21:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.50	top5: 70.00	
[09/26 00:21:43 visual_prompt]: Best epoch 42: best metric: 0.405
[09/26 00:21:43 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 00:21:50 visual_prompt]: Epoch 43 / 100: avg data time: 4.17e-02, avg batch time: 0.4925, average train loss: 0.5931
[09/26 00:21:51 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1692, average loss: 3.4972
[09/26 00:21:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 77.50	
[09/26 00:21:51 visual_prompt]: Best epoch 43: best metric: 0.480
[09/26 00:21:51 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 00:21:58 visual_prompt]: Epoch 44 / 100: avg data time: 5.27e-02, avg batch time: 0.5022, average train loss: 0.4562
[09/26 00:21:59 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1697, average loss: 3.6093
[09/26 00:21:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 76.00	
[09/26 00:21:59 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 00:22:06 visual_prompt]: Epoch 45 / 100: avg data time: 5.50e-02, avg batch time: 0.5037, average train loss: 0.3340
[09/26 00:22:08 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1692, average loss: 3.6234
[09/26 00:22:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 78.00	
[09/26 00:22:08 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 00:22:14 visual_prompt]: Epoch 46 / 100: avg data time: 4.98e-02, avg batch time: 0.4988, average train loss: 0.2709
[09/26 00:22:16 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 3.4660
[09/26 00:22:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 78.50	
[09/26 00:22:16 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 00:22:23 visual_prompt]: Epoch 47 / 100: avg data time: 5.04e-02, avg batch time: 0.4993, average train loss: 0.1595
[09/26 00:22:24 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1685, average loss: 3.5876
[09/26 00:22:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 78.50	
[09/26 00:22:24 visual_prompt]: Best epoch 47: best metric: 0.490
[09/26 00:22:24 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 00:22:31 visual_prompt]: Epoch 48 / 100: avg data time: 4.93e-02, avg batch time: 0.4986, average train loss: 0.1397
[09/26 00:22:32 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1696, average loss: 3.8480
[09/26 00:22:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.50	
[09/26 00:22:32 visual_prompt]: Best epoch 48: best metric: 0.510
[09/26 00:22:32 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 00:22:39 visual_prompt]: Epoch 49 / 100: avg data time: 5.43e-02, avg batch time: 0.5038, average train loss: 0.2045
[09/26 00:22:40 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 3.3137
[09/26 00:22:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 00:22:40 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 00:22:47 visual_prompt]: Epoch 50 / 100: avg data time: 5.25e-02, avg batch time: 0.5020, average train loss: 0.1410
[09/26 00:22:48 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 3.4060
[09/26 00:22:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 80.00	
[09/26 00:22:48 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 00:22:55 visual_prompt]: Epoch 51 / 100: avg data time: 3.98e-02, avg batch time: 0.4901, average train loss: 0.1440
[09/26 00:22:56 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1690, average loss: 3.3155
[09/26 00:22:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 79.50	
[09/26 00:22:56 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 00:23:03 visual_prompt]: Epoch 52 / 100: avg data time: 5.26e-02, avg batch time: 0.5029, average train loss: 0.1160
[09/26 00:23:05 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1695, average loss: 3.2917
[09/26 00:23:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 78.00	
[09/26 00:23:05 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 00:23:12 visual_prompt]: Epoch 53 / 100: avg data time: 5.61e-02, avg batch time: 0.5047, average train loss: 0.1444
[09/26 00:23:13 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1690, average loss: 3.5329
[09/26 00:23:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.00	top5: 77.50	
[09/26 00:23:13 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 00:23:20 visual_prompt]: Epoch 54 / 100: avg data time: 5.68e-02, avg batch time: 0.5061, average train loss: 0.1241
[09/26 00:23:21 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 3.3416
[09/26 00:23:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.50	
[09/26 00:23:21 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 00:23:28 visual_prompt]: Epoch 55 / 100: avg data time: 5.24e-02, avg batch time: 0.5021, average train loss: 0.1087
[09/26 00:23:29 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 3.2227
[09/26 00:23:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 75.00	
[09/26 00:23:29 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 00:23:36 visual_prompt]: Epoch 56 / 100: avg data time: 5.53e-02, avg batch time: 0.5036, average train loss: 0.0719
[09/26 00:23:38 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1695, average loss: 2.9962
[09/26 00:23:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 78.00	
[09/26 00:23:38 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 00:23:44 visual_prompt]: Epoch 57 / 100: avg data time: 4.85e-02, avg batch time: 0.4988, average train loss: 0.0833
[09/26 00:23:46 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1697, average loss: 3.2328
[09/26 00:23:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 76.00	
[09/26 00:23:46 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 00:23:53 visual_prompt]: Epoch 58 / 100: avg data time: 4.19e-02, avg batch time: 0.4905, average train loss: 0.0482
[09/26 00:23:54 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 2.7305
[09/26 00:23:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 80.00	
[09/26 00:23:54 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 00:24:01 visual_prompt]: Epoch 59 / 100: avg data time: 4.40e-02, avg batch time: 0.4940, average train loss: 0.0256
[09/26 00:24:02 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1692, average loss: 2.8740
[09/26 00:24:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 81.00	
[09/26 00:24:02 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 00:24:09 visual_prompt]: Epoch 60 / 100: avg data time: 5.41e-02, avg batch time: 0.5028, average train loss: 0.0179
[09/26 00:24:10 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 2.6755
[09/26 00:24:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 81.00	
[09/26 00:24:10 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 00:24:17 visual_prompt]: Epoch 61 / 100: avg data time: 4.18e-02, avg batch time: 0.4911, average train loss: 0.0161
[09/26 00:24:18 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1693, average loss: 2.4995
[09/26 00:24:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 80.50	
[09/26 00:24:18 visual_prompt]: Best epoch 61: best metric: 0.530
[09/26 00:24:18 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 00:24:25 visual_prompt]: Epoch 62 / 100: avg data time: 4.43e-02, avg batch time: 0.4957, average train loss: 0.0085
[09/26 00:24:26 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1690, average loss: 2.5472
[09/26 00:24:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 78.50	
[09/26 00:24:26 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 00:24:33 visual_prompt]: Epoch 63 / 100: avg data time: 5.60e-02, avg batch time: 0.5060, average train loss: 0.0047
[09/26 00:24:35 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1694, average loss: 2.4072
[09/26 00:24:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 78.00	
[09/26 00:24:35 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 00:24:41 visual_prompt]: Epoch 64 / 100: avg data time: 4.40e-02, avg batch time: 0.4936, average train loss: 0.0035
[09/26 00:24:43 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 2.2769
[09/26 00:24:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 81.00	
[09/26 00:24:43 visual_prompt]: Best epoch 64: best metric: 0.540
[09/26 00:24:43 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 00:24:50 visual_prompt]: Epoch 65 / 100: avg data time: 4.21e-02, avg batch time: 0.4928, average train loss: 0.0031
[09/26 00:24:51 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 2.1810
[09/26 00:24:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.00	
[09/26 00:24:51 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 00:24:58 visual_prompt]: Epoch 66 / 100: avg data time: 4.96e-02, avg batch time: 0.4981, average train loss: 0.0032
[09/26 00:24:59 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1692, average loss: 2.1140
[09/26 00:24:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 84.50	
[09/26 00:24:59 visual_prompt]: Best epoch 66: best metric: 0.550
[09/26 00:24:59 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 00:25:06 visual_prompt]: Epoch 67 / 100: avg data time: 5.26e-02, avg batch time: 0.5015, average train loss: 0.0036
[09/26 00:25:07 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1688, average loss: 2.0590
[09/26 00:25:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 84.50	
[09/26 00:25:07 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 00:25:14 visual_prompt]: Epoch 68 / 100: avg data time: 4.12e-02, avg batch time: 0.4915, average train loss: 0.0038
[09/26 00:25:15 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 2.0146
[09/26 00:25:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 83.50	
[09/26 00:25:15 visual_prompt]: Best epoch 68: best metric: 0.555
[09/26 00:25:15 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 00:25:22 visual_prompt]: Epoch 69 / 100: avg data time: 5.49e-02, avg batch time: 0.5046, average train loss: 0.0039
[09/26 00:25:24 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1688, average loss: 1.9779
[09/26 00:25:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.00	
[09/26 00:25:24 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 00:25:31 visual_prompt]: Epoch 70 / 100: avg data time: 5.82e-02, avg batch time: 0.5063, average train loss: 0.0040
[09/26 00:25:32 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.9358
[09/26 00:25:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 83.50	
[09/26 00:25:32 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 00:25:39 visual_prompt]: Epoch 71 / 100: avg data time: 5.42e-02, avg batch time: 0.5026, average train loss: 0.0042
[09/26 00:25:40 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1693, average loss: 1.9000
[09/26 00:25:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 00:25:40 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 00:25:47 visual_prompt]: Epoch 72 / 100: avg data time: 5.33e-02, avg batch time: 0.5024, average train loss: 0.0045
[09/26 00:25:48 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1691, average loss: 1.8692
[09/26 00:25:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 84.50	
[09/26 00:25:48 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 00:25:55 visual_prompt]: Epoch 73 / 100: avg data time: 4.26e-02, avg batch time: 0.4930, average train loss: 0.0044
[09/26 00:25:56 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1691, average loss: 1.8468
[09/26 00:25:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.50	
[09/26 00:25:56 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 00:26:03 visual_prompt]: Epoch 74 / 100: avg data time: 4.69e-02, avg batch time: 0.4969, average train loss: 0.0046
[09/26 00:26:05 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1696, average loss: 1.8186
[09/26 00:26:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.00	
[09/26 00:26:05 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 00:26:11 visual_prompt]: Epoch 75 / 100: avg data time: 4.62e-02, avg batch time: 0.4970, average train loss: 0.0047
[09/26 00:26:13 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1690, average loss: 1.7979
[09/26 00:26:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.00	
[09/26 00:26:13 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 00:26:19 visual_prompt]: Epoch 76 / 100: avg data time: 5.45e-02, avg batch time: 0.5031, average train loss: 0.0048
[09/26 00:26:21 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1693, average loss: 1.7899
[09/26 00:26:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.50	
[09/26 00:26:21 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 00:26:28 visual_prompt]: Epoch 77 / 100: avg data time: 5.53e-02, avg batch time: 0.5032, average train loss: 0.0050
[09/26 00:26:29 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1695, average loss: 1.7751
[09/26 00:26:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 84.50	
[09/26 00:26:29 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 00:26:36 visual_prompt]: Epoch 78 / 100: avg data time: 5.53e-02, avg batch time: 0.5034, average train loss: 0.0049
[09/26 00:26:37 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 1.7661
[09/26 00:26:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 85.00	
[09/26 00:26:37 visual_prompt]: Best epoch 78: best metric: 0.560
[09/26 00:26:37 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 00:26:44 visual_prompt]: Epoch 79 / 100: avg data time: 4.45e-02, avg batch time: 0.4928, average train loss: 0.0051
[09/26 00:26:46 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 1.7505
[09/26 00:26:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 84.50	
[09/26 00:26:46 visual_prompt]: Best epoch 79: best metric: 0.570
[09/26 00:26:46 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 00:26:52 visual_prompt]: Epoch 80 / 100: avg data time: 5.18e-02, avg batch time: 0.5020, average train loss: 0.0051
[09/26 00:26:54 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 1.7522
[09/26 00:26:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.00	
[09/26 00:26:54 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 00:27:01 visual_prompt]: Epoch 81 / 100: avg data time: 5.77e-02, avg batch time: 0.5059, average train loss: 0.0051
[09/26 00:27:02 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 1.7534
[09/26 00:27:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.50	
[09/26 00:27:02 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 00:27:09 visual_prompt]: Epoch 82 / 100: avg data time: 5.12e-02, avg batch time: 0.4995, average train loss: 0.0050
[09/26 00:27:10 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 1.7424
[09/26 00:27:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.00	
[09/26 00:27:10 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 00:27:17 visual_prompt]: Epoch 83 / 100: avg data time: 5.23e-02, avg batch time: 0.5005, average train loss: 0.0053
[09/26 00:27:19 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1695, average loss: 1.7376
[09/26 00:27:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 86.50	
[09/26 00:27:19 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 00:27:25 visual_prompt]: Epoch 84 / 100: avg data time: 5.07e-02, avg batch time: 0.4986, average train loss: 0.0052
[09/26 00:27:27 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.1692, average loss: 1.7321
[09/26 00:27:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 86.50	
[09/26 00:27:27 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 00:27:33 visual_prompt]: Epoch 85 / 100: avg data time: 4.53e-02, avg batch time: 0.4957, average train loss: 0.0052
[09/26 00:27:35 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1692, average loss: 1.7212
[09/26 00:27:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.50	
[09/26 00:27:35 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 00:27:42 visual_prompt]: Epoch 86 / 100: avg data time: 6.34e-02, avg batch time: 0.5114, average train loss: 0.0050
[09/26 00:27:43 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 1.7180
[09/26 00:27:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.00	
[09/26 00:27:43 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 00:27:50 visual_prompt]: Epoch 87 / 100: avg data time: 5.58e-02, avg batch time: 0.5039, average train loss: 0.0051
[09/26 00:27:51 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1693, average loss: 1.7133
[09/26 00:27:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 00:27:51 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 00:27:58 visual_prompt]: Epoch 88 / 100: avg data time: 4.96e-02, avg batch time: 0.4977, average train loss: 0.0051
[09/26 00:28:00 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1691, average loss: 1.7134
[09/26 00:28:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.50	
[09/26 00:28:00 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 00:28:06 visual_prompt]: Epoch 89 / 100: avg data time: 4.47e-02, avg batch time: 0.4947, average train loss: 0.0051
[09/26 00:28:08 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1690, average loss: 1.7116
[09/26 00:28:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 00:28:08 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 00:28:15 visual_prompt]: Epoch 90 / 100: avg data time: 5.16e-02, avg batch time: 0.5008, average train loss: 0.0051
[09/26 00:28:16 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1694, average loss: 1.7127
[09/26 00:28:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 87.50	
[09/26 00:28:16 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 00:28:23 visual_prompt]: Epoch 91 / 100: avg data time: 5.16e-02, avg batch time: 0.5010, average train loss: 0.0051
[09/26 00:28:24 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1687, average loss: 1.7129
[09/26 00:28:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 00:28:24 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 00:28:31 visual_prompt]: Epoch 92 / 100: avg data time: 5.43e-02, avg batch time: 0.5023, average train loss: 0.0051
[09/26 00:28:32 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1694, average loss: 1.7098
[09/26 00:28:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.50	
[09/26 00:28:32 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 00:28:39 visual_prompt]: Epoch 93 / 100: avg data time: 5.98e-02, avg batch time: 0.5079, average train loss: 0.0050
[09/26 00:28:41 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1692, average loss: 1.7102
[09/26 00:28:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 00:28:41 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 00:28:48 visual_prompt]: Epoch 94 / 100: avg data time: 5.55e-02, avg batch time: 0.5044, average train loss: 0.0051
[09/26 00:28:49 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1692, average loss: 1.7104
[09/26 00:28:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 87.50	
[09/26 00:28:49 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 00:28:56 visual_prompt]: Epoch 95 / 100: avg data time: 5.21e-02, avg batch time: 0.5004, average train loss: 0.0050
[09/26 00:28:57 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1690, average loss: 1.7096
[09/26 00:28:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 00:28:57 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 00:29:04 visual_prompt]: Epoch 96 / 100: avg data time: 5.48e-02, avg batch time: 0.5042, average train loss: 0.0050
[09/26 00:29:06 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1694, average loss: 1.7092
[09/26 00:29:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.50	
[09/26 00:29:06 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 00:29:12 visual_prompt]: Epoch 97 / 100: avg data time: 4.62e-02, avg batch time: 0.4956, average train loss: 0.0051
[09/26 00:29:14 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1691, average loss: 1.7090
[09/26 00:29:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.50	
[09/26 00:29:14 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 00:29:21 visual_prompt]: Epoch 98 / 100: avg data time: 5.72e-02, avg batch time: 0.5060, average train loss: 0.0050
[09/26 00:29:22 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 1.7090
[09/26 00:29:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.50	
[09/26 00:29:22 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 00:29:29 visual_prompt]: Epoch 99 / 100: avg data time: 5.30e-02, avg batch time: 0.5013, average train loss: 0.0050
[09/26 00:29:30 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1692, average loss: 1.7090
[09/26 00:29:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.50	
[09/26 00:29:30 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 00:29:37 visual_prompt]: Epoch 100 / 100: avg data time: 5.15e-02, avg batch time: 0.4998, average train loss: 0.0050
[09/26 00:29:39 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1691, average loss: 1.7090
[09/26 00:29:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.50	
[09/26 00:29:39 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 00:29:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 00:29:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 00:29:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 00:29:39 visual_prompt]: Training with config:
[09/26 00:29:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 00:29:39 visual_prompt]: Loading training data...
[09/26 00:29:39 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 00:29:41 visual_prompt]: Number of images: 800
[09/26 00:29:41 visual_prompt]: Number of classes: 47 / 47
[09/26 00:29:41 visual_prompt]: Loading validation data...
[09/26 00:29:41 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 00:29:41 visual_prompt]: Number of images: 200
[09/26 00:29:41 visual_prompt]: Number of classes: 47 / 47
[09/26 00:29:41 visual_prompt]: Constructing models...
[09/26 00:29:44 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 00:29:44 visual_prompt]: tuned percent:0.576
[09/26 00:29:44 visual_prompt]: Device used for model: 0
[09/26 00:29:44 visual_prompt]: Setting up Evaluator...
[09/26 00:29:44 visual_prompt]: Setting up Trainer...
[09/26 00:29:44 visual_prompt]: 	Setting up the optimizer...
[09/26 00:29:44 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 00:29:51 visual_prompt]: Epoch 1 / 100: avg data time: 5.37e-02, avg batch time: 0.4995, average train loss: 3.9261
[09/26 00:29:52 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1683, average loss: 3.9045
[09/26 00:29:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 00:29:52 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 00:29:52 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 00:29:59 visual_prompt]: Epoch 2 / 100: avg data time: 4.57e-02, avg batch time: 0.4963, average train loss: 4.0186
[09/26 00:30:00 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1688, average loss: 4.0207
[09/26 00:30:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.00	
[09/26 00:30:00 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 00:30:07 visual_prompt]: Epoch 3 / 100: avg data time: 5.28e-02, avg batch time: 0.4996, average train loss: 3.9909
[09/26 00:30:08 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1687, average loss: 3.8895
[09/26 00:30:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 20.00	
[09/26 00:30:08 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 00:30:15 visual_prompt]: Epoch 4 / 100: avg data time: 6.14e-02, avg batch time: 0.5084, average train loss: 4.0528
[09/26 00:30:17 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1686, average loss: 4.3011
[09/26 00:30:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.00	
[09/26 00:30:17 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 00:30:23 visual_prompt]: Epoch 5 / 100: avg data time: 4.42e-02, avg batch time: 0.4923, average train loss: 4.0499
[09/26 00:30:25 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1689, average loss: 4.1990
[09/26 00:30:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 23.00	
[09/26 00:30:25 visual_prompt]: Best epoch 5: best metric: 0.065
[09/26 00:30:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 00:30:32 visual_prompt]: Epoch 6 / 100: avg data time: 5.14e-02, avg batch time: 0.4996, average train loss: 3.9663
[09/26 00:30:33 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 4.9516
[09/26 00:30:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.00	top5: 44.00	
[09/26 00:30:33 visual_prompt]: Best epoch 6: best metric: 0.120
[09/26 00:30:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 00:30:40 visual_prompt]: Epoch 7 / 100: avg data time: 4.89e-02, avg batch time: 0.4986, average train loss: 3.0901
[09/26 00:30:41 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 3.4937
[09/26 00:30:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 29.00	top5: 65.50	
[09/26 00:30:41 visual_prompt]: Best epoch 7: best metric: 0.290
[09/26 00:30:41 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 00:30:48 visual_prompt]: Epoch 8 / 100: avg data time: 5.23e-02, avg batch time: 0.5018, average train loss: 1.7655
[09/26 00:30:49 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 2.6075
[09/26 00:30:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.50	top5: 75.50	
[09/26 00:30:49 visual_prompt]: Best epoch 8: best metric: 0.405
[09/26 00:30:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 00:30:56 visual_prompt]: Epoch 9 / 100: avg data time: 5.72e-02, avg batch time: 0.5052, average train loss: 1.0087
[09/26 00:30:58 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 2.1688
[09/26 00:30:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.50	
[09/26 00:30:58 visual_prompt]: Best epoch 9: best metric: 0.540
[09/26 00:30:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 00:31:04 visual_prompt]: Epoch 10 / 100: avg data time: 4.83e-02, avg batch time: 0.4973, average train loss: 0.4378
[09/26 00:31:06 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1691, average loss: 2.7492
[09/26 00:31:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 80.50	
[09/26 00:31:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 00:31:13 visual_prompt]: Epoch 11 / 100: avg data time: 4.98e-02, avg batch time: 0.4985, average train loss: 0.3387
[09/26 00:31:14 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 2.5024
[09/26 00:31:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 80.00	
[09/26 00:31:14 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 00:31:21 visual_prompt]: Epoch 12 / 100: avg data time: 4.91e-02, avg batch time: 0.4988, average train loss: 0.4704
[09/26 00:31:22 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.1690, average loss: 2.8178
[09/26 00:31:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 87.00	
[09/26 00:31:22 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 00:31:29 visual_prompt]: Epoch 13 / 100: avg data time: 3.82e-02, avg batch time: 0.4867, average train loss: 0.2317
[09/26 00:31:30 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1689, average loss: 2.5007
[09/26 00:31:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 85.50	
[09/26 00:31:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 00:31:37 visual_prompt]: Epoch 14 / 100: avg data time: 3.98e-02, avg batch time: 0.4887, average train loss: 0.2055
[09/26 00:31:38 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1693, average loss: 2.8397
[09/26 00:31:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 00:31:38 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 00:31:45 visual_prompt]: Epoch 15 / 100: avg data time: 5.13e-02, avg batch time: 0.5003, average train loss: 0.1738
[09/26 00:31:47 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 2.6459
[09/26 00:31:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 81.50	
[09/26 00:31:47 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 00:31:53 visual_prompt]: Epoch 16 / 100: avg data time: 5.61e-02, avg batch time: 0.5053, average train loss: 0.1049
[09/26 00:31:55 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1695, average loss: 3.4839
[09/26 00:31:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.00	
[09/26 00:31:55 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 00:32:02 visual_prompt]: Epoch 17 / 100: avg data time: 5.36e-02, avg batch time: 0.5036, average train loss: 0.0785
[09/26 00:32:03 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1693, average loss: 3.4226
[09/26 00:32:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 85.50	
[09/26 00:32:03 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 00:32:10 visual_prompt]: Epoch 18 / 100: avg data time: 5.11e-02, avg batch time: 0.5001, average train loss: 0.0912
[09/26 00:32:11 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 2.8123
[09/26 00:32:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 85.00	
[09/26 00:32:11 visual_prompt]: Best epoch 18: best metric: 0.560
[09/26 00:32:11 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 00:32:18 visual_prompt]: Epoch 19 / 100: avg data time: 5.73e-02, avg batch time: 0.5066, average train loss: 0.0629
[09/26 00:32:20 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 3.1801
[09/26 00:32:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 85.00	
[09/26 00:32:20 visual_prompt]: Best epoch 19: best metric: 0.580
[09/26 00:32:20 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 00:32:26 visual_prompt]: Epoch 20 / 100: avg data time: 5.49e-02, avg batch time: 0.5038, average train loss: 0.0828
[09/26 00:32:28 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1691, average loss: 3.3978
[09/26 00:32:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 84.00	
[09/26 00:32:28 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 00:32:34 visual_prompt]: Epoch 21 / 100: avg data time: 4.10e-02, avg batch time: 0.4917, average train loss: 0.0544
[09/26 00:32:36 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1693, average loss: 3.2059
[09/26 00:32:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 83.50	
[09/26 00:32:36 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 00:32:43 visual_prompt]: Epoch 22 / 100: avg data time: 4.12e-02, avg batch time: 0.4899, average train loss: 0.0186
[09/26 00:32:44 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1684, average loss: 3.3941
[09/26 00:32:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 83.50	
[09/26 00:32:44 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 00:32:51 visual_prompt]: Epoch 23 / 100: avg data time: 4.99e-02, avg batch time: 0.4989, average train loss: 0.0093
[09/26 00:32:52 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 3.4120
[09/26 00:32:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 84.00	
[09/26 00:32:52 visual_prompt]: Best epoch 23: best metric: 0.585
[09/26 00:32:52 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 00:32:59 visual_prompt]: Epoch 24 / 100: avg data time: 4.23e-02, avg batch time: 0.4921, average train loss: 0.0074
[09/26 00:33:00 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1688, average loss: 3.2631
[09/26 00:33:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 83.00	
[09/26 00:33:00 visual_prompt]: Best epoch 24: best metric: 0.590
[09/26 00:33:00 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 00:33:07 visual_prompt]: Epoch 25 / 100: avg data time: 5.44e-02, avg batch time: 0.5039, average train loss: 0.0147
[09/26 00:33:09 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1687, average loss: 3.1057
[09/26 00:33:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 85.00	
[09/26 00:33:09 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 00:33:15 visual_prompt]: Epoch 26 / 100: avg data time: 5.63e-02, avg batch time: 0.5051, average train loss: 0.0019
[09/26 00:33:17 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1694, average loss: 3.0731
[09/26 00:33:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 86.00	
[09/26 00:33:17 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 00:33:24 visual_prompt]: Epoch 27 / 100: avg data time: 5.95e-02, avg batch time: 0.5076, average train loss: 0.0003
[09/26 00:33:25 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 3.1118
[09/26 00:33:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 85.00	
[09/26 00:33:25 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 00:33:32 visual_prompt]: Epoch 28 / 100: avg data time: 6.59e-02, avg batch time: 0.5145, average train loss: 0.0002
[09/26 00:33:33 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1692, average loss: 3.1455
[09/26 00:33:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 85.00	
[09/26 00:33:33 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 00:33:40 visual_prompt]: Epoch 29 / 100: avg data time: 4.26e-02, avg batch time: 0.4920, average train loss: 0.0002
[09/26 00:33:42 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 3.1708
[09/26 00:33:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 85.00	
[09/26 00:33:42 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 00:33:48 visual_prompt]: Epoch 30 / 100: avg data time: 5.28e-02, avg batch time: 0.5014, average train loss: 0.0389
[09/26 00:33:50 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 2.3781
[09/26 00:33:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 84.50	
[09/26 00:33:50 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 00:33:57 visual_prompt]: Epoch 31 / 100: avg data time: 4.74e-02, avg batch time: 0.4971, average train loss: 0.0166
[09/26 00:33:58 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1694, average loss: 2.7262
[09/26 00:33:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 84.00	
[09/26 00:33:58 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 00:34:05 visual_prompt]: Epoch 32 / 100: avg data time: 5.65e-02, avg batch time: 0.5054, average train loss: 0.0044
[09/26 00:34:07 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1694, average loss: 2.7700
[09/26 00:34:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 84.00	
[09/26 00:34:07 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 00:34:13 visual_prompt]: Epoch 33 / 100: avg data time: 4.76e-02, avg batch time: 0.4959, average train loss: 0.0014
[09/26 00:34:15 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1690, average loss: 2.8396
[09/26 00:34:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 85.00	
[09/26 00:34:15 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 00:34:22 visual_prompt]: Epoch 34 / 100: avg data time: 5.79e-02, avg batch time: 0.5064, average train loss: 0.0010
[09/26 00:34:23 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 2.8471
[09/26 00:34:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 84.50	
[09/26 00:34:23 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 00:34:30 visual_prompt]: Epoch 35 / 100: avg data time: 4.41e-02, avg batch time: 0.4934, average train loss: 0.0006
[09/26 00:34:31 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1691, average loss: 2.8274
[09/26 00:34:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 84.50	
[09/26 00:34:31 visual_prompt]: Best epoch 35: best metric: 0.595
[09/26 00:34:31 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 00:34:38 visual_prompt]: Epoch 36 / 100: avg data time: 5.60e-02, avg batch time: 0.5040, average train loss: 0.0005
[09/26 00:34:39 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1691, average loss: 2.8233
[09/26 00:34:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 84.50	
[09/26 00:34:39 visual_prompt]: Best epoch 36: best metric: 0.600
[09/26 00:34:39 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 00:34:46 visual_prompt]: Epoch 37 / 100: avg data time: 4.42e-02, avg batch time: 0.4946, average train loss: 0.0004
[09/26 00:34:48 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 2.8239
[09/26 00:34:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 86.00	
[09/26 00:34:48 visual_prompt]: Best epoch 37: best metric: 0.605
[09/26 00:34:48 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 00:34:54 visual_prompt]: Epoch 38 / 100: avg data time: 5.38e-02, avg batch time: 0.5038, average train loss: 0.0004
[09/26 00:34:56 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.1691, average loss: 2.8275
[09/26 00:34:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.50	
[09/26 00:34:56 visual_prompt]: Best epoch 38: best metric: 0.610
[09/26 00:34:56 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 00:35:03 visual_prompt]: Epoch 39 / 100: avg data time: 5.01e-02, avg batch time: 0.5013, average train loss: 0.0003
[09/26 00:35:04 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1692, average loss: 2.8335
[09/26 00:35:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.50	
[09/26 00:35:04 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 00:35:11 visual_prompt]: Epoch 40 / 100: avg data time: 5.04e-02, avg batch time: 0.4988, average train loss: 0.0003
[09/26 00:35:12 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 2.8380
[09/26 00:35:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.50	
[09/26 00:35:12 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 00:35:19 visual_prompt]: Epoch 41 / 100: avg data time: 4.30e-02, avg batch time: 0.4915, average train loss: 0.0003
[09/26 00:35:20 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1693, average loss: 2.8472
[09/26 00:35:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.50	
[09/26 00:35:20 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 00:35:27 visual_prompt]: Epoch 42 / 100: avg data time: 4.06e-02, avg batch time: 0.4899, average train loss: 0.0003
[09/26 00:35:29 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 2.8516
[09/26 00:35:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.50	
[09/26 00:35:29 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 00:35:35 visual_prompt]: Epoch 43 / 100: avg data time: 5.54e-02, avg batch time: 0.5037, average train loss: 0.0003
[09/26 00:35:37 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1685, average loss: 2.8568
[09/26 00:35:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:35:37 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 00:35:44 visual_prompt]: Epoch 44 / 100: avg data time: 4.98e-02, avg batch time: 0.4993, average train loss: 0.0003
[09/26 00:35:45 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 2.8598
[09/26 00:35:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:35:45 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 00:35:52 visual_prompt]: Epoch 45 / 100: avg data time: 5.15e-02, avg batch time: 0.4995, average train loss: 0.0003
[09/26 00:35:53 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1693, average loss: 2.8630
[09/26 00:35:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:35:53 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 00:36:00 visual_prompt]: Epoch 46 / 100: avg data time: 5.32e-02, avg batch time: 0.5024, average train loss: 0.0002
[09/26 00:36:02 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1694, average loss: 2.8671
[09/26 00:36:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:36:02 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 00:36:08 visual_prompt]: Epoch 47 / 100: avg data time: 5.57e-02, avg batch time: 0.5038, average train loss: 0.0003
[09/26 00:36:10 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 2.8772
[09/26 00:36:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:36:10 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 00:36:17 visual_prompt]: Epoch 48 / 100: avg data time: 5.23e-02, avg batch time: 0.5017, average train loss: 0.0002
[09/26 00:36:18 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1691, average loss: 2.8905
[09/26 00:36:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:36:18 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 00:36:25 visual_prompt]: Epoch 49 / 100: avg data time: 5.26e-02, avg batch time: 0.5010, average train loss: 0.0002
[09/26 00:36:26 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1693, average loss: 2.8969
[09/26 00:36:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 85.00	
[09/26 00:36:26 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 00:36:33 visual_prompt]: Epoch 50 / 100: avg data time: 5.20e-02, avg batch time: 0.5002, average train loss: 0.0002
[09/26 00:36:35 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1693, average loss: 2.9023
[09/26 00:36:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 85.00	
[09/26 00:36:35 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 00:36:41 visual_prompt]: Epoch 51 / 100: avg data time: 5.09e-02, avg batch time: 0.5020, average train loss: 0.0002
[09/26 00:36:43 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1688, average loss: 2.9058
[09/26 00:36:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:36:43 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 00:36:50 visual_prompt]: Epoch 52 / 100: avg data time: 5.58e-02, avg batch time: 0.5045, average train loss: 0.0002
[09/26 00:36:51 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 2.9087
[09/26 00:36:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:36:51 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 00:36:58 visual_prompt]: Epoch 53 / 100: avg data time: 5.62e-02, avg batch time: 0.5040, average train loss: 0.0002
[09/26 00:36:59 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1695, average loss: 2.9105
[09/26 00:36:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:36:59 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 00:37:06 visual_prompt]: Epoch 54 / 100: avg data time: 5.40e-02, avg batch time: 0.5022, average train loss: 0.0002
[09/26 00:37:08 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1684, average loss: 2.9121
[09/26 00:37:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:37:08 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 00:37:14 visual_prompt]: Epoch 55 / 100: avg data time: 5.67e-02, avg batch time: 0.5046, average train loss: 0.0002
[09/26 00:37:16 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1691, average loss: 2.9152
[09/26 00:37:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:37:16 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 00:37:23 visual_prompt]: Epoch 56 / 100: avg data time: 4.13e-02, avg batch time: 0.4913, average train loss: 0.0002
[09/26 00:37:24 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1689, average loss: 2.9187
[09/26 00:37:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:37:24 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 00:37:31 visual_prompt]: Epoch 57 / 100: avg data time: 4.63e-02, avg batch time: 0.4959, average train loss: 0.0002
[09/26 00:37:32 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1690, average loss: 2.9220
[09/26 00:37:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:37:32 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 00:37:39 visual_prompt]: Epoch 58 / 100: avg data time: 5.94e-02, avg batch time: 0.5073, average train loss: 0.0002
[09/26 00:37:41 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 2.9243
[09/26 00:37:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:37:41 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 00:37:47 visual_prompt]: Epoch 59 / 100: avg data time: 4.18e-02, avg batch time: 0.4928, average train loss: 0.0002
[09/26 00:37:49 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1692, average loss: 2.9263
[09/26 00:37:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:37:49 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 00:37:55 visual_prompt]: Epoch 60 / 100: avg data time: 4.26e-02, avg batch time: 0.4913, average train loss: 0.0002
[09/26 00:37:57 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 2.9289
[09/26 00:37:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:37:57 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 00:38:04 visual_prompt]: Epoch 61 / 100: avg data time: 5.15e-02, avg batch time: 0.5010, average train loss: 0.0001
[09/26 00:38:05 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1691, average loss: 2.9318
[09/26 00:38:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:38:05 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 00:38:12 visual_prompt]: Epoch 62 / 100: avg data time: 4.35e-02, avg batch time: 0.4926, average train loss: 0.0002
[09/26 00:38:13 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1690, average loss: 2.9343
[09/26 00:38:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:38:13 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 00:38:20 visual_prompt]: Epoch 63 / 100: avg data time: 5.31e-02, avg batch time: 0.5017, average train loss: 0.0002
[09/26 00:38:22 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1694, average loss: 2.9361
[09/26 00:38:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:38:22 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 00:38:28 visual_prompt]: Epoch 64 / 100: avg data time: 5.97e-02, avg batch time: 0.5079, average train loss: 0.0002
[09/26 00:38:30 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 2.9384
[09/26 00:38:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:38:30 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 00:38:37 visual_prompt]: Epoch 65 / 100: avg data time: 5.19e-02, avg batch time: 0.5001, average train loss: 0.0002
[09/26 00:38:38 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1691, average loss: 2.9409
[09/26 00:38:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:38:38 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 00:38:45 visual_prompt]: Epoch 66 / 100: avg data time: 4.80e-02, avg batch time: 0.4971, average train loss: 0.0002
[09/26 00:38:46 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 2.9424
[09/26 00:38:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:38:46 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 00:38:53 visual_prompt]: Epoch 67 / 100: avg data time: 5.06e-02, avg batch time: 0.4986, average train loss: 0.0002
[09/26 00:38:55 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1688, average loss: 2.9437
[09/26 00:38:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:38:55 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 00:39:02 visual_prompt]: Epoch 68 / 100: avg data time: 5.61e-02, avg batch time: 0.5049, average train loss: 0.0001
[09/26 00:39:03 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1697, average loss: 2.9452
[09/26 00:39:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 00:39:03 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 00:39:10 visual_prompt]: Epoch 69 / 100: avg data time: 4.69e-02, avg batch time: 0.4949, average train loss: 0.0001
[09/26 00:39:11 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1695, average loss: 2.9461
[09/26 00:39:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:39:11 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 00:39:18 visual_prompt]: Epoch 70 / 100: avg data time: 5.95e-02, avg batch time: 0.5075, average train loss: 0.0001
[09/26 00:39:20 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1693, average loss: 2.9472
[09/26 00:39:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:39:20 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 00:39:27 visual_prompt]: Epoch 71 / 100: avg data time: 5.09e-02, avg batch time: 0.4988, average train loss: 0.0002
[09/26 00:39:28 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1690, average loss: 2.9484
[09/26 00:39:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:39:28 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 00:39:35 visual_prompt]: Epoch 72 / 100: avg data time: 5.24e-02, avg batch time: 0.5005, average train loss: 0.0001
[09/26 00:39:36 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1693, average loss: 2.9495
[09/26 00:39:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:39:36 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 00:39:43 visual_prompt]: Epoch 73 / 100: avg data time: 5.49e-02, avg batch time: 0.5037, average train loss: 0.0001
[09/26 00:39:44 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1694, average loss: 2.9503
[09/26 00:39:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:39:44 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 00:39:51 visual_prompt]: Epoch 74 / 100: avg data time: 4.16e-02, avg batch time: 0.4909, average train loss: 0.0001
[09/26 00:39:53 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 2.9512
[09/26 00:39:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:39:53 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 00:39:59 visual_prompt]: Epoch 75 / 100: avg data time: 5.54e-02, avg batch time: 0.5047, average train loss: 0.0001
[09/26 00:40:01 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 2.9522
[09/26 00:40:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:40:01 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 00:40:08 visual_prompt]: Epoch 76 / 100: avg data time: 4.68e-02, avg batch time: 0.4956, average train loss: 0.0001
[09/26 00:40:09 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1687, average loss: 2.9524
[09/26 00:40:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:40:09 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 00:40:16 visual_prompt]: Epoch 77 / 100: avg data time: 5.44e-02, avg batch time: 0.5038, average train loss: 0.0001
[09/26 00:40:17 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1692, average loss: 2.9531
[09/26 00:40:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:40:17 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 00:40:24 visual_prompt]: Epoch 78 / 100: avg data time: 5.63e-02, avg batch time: 0.5059, average train loss: 0.0001
[09/26 00:40:26 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 2.9536
[09/26 00:40:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:40:26 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 00:40:32 visual_prompt]: Epoch 79 / 100: avg data time: 5.08e-02, avg batch time: 0.5002, average train loss: 0.0002
[09/26 00:40:34 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1690, average loss: 2.9539
[09/26 00:40:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:40:34 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 00:40:41 visual_prompt]: Epoch 80 / 100: avg data time: 5.55e-02, avg batch time: 0.5049, average train loss: 0.0001
[09/26 00:40:42 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1694, average loss: 2.9542
[09/26 00:40:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:40:42 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 00:40:49 visual_prompt]: Epoch 81 / 100: avg data time: 4.64e-02, avg batch time: 0.4970, average train loss: 0.0001
[09/26 00:40:50 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1695, average loss: 2.9543
[09/26 00:40:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:40:50 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 00:40:57 visual_prompt]: Epoch 82 / 100: avg data time: 5.02e-02, avg batch time: 0.4994, average train loss: 0.0001
[09/26 00:40:59 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1691, average loss: 2.9546
[09/26 00:40:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:40:59 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 00:41:05 visual_prompt]: Epoch 83 / 100: avg data time: 4.40e-02, avg batch time: 0.4940, average train loss: 0.0001
[09/26 00:41:07 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1690, average loss: 2.9551
[09/26 00:41:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:41:07 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 00:41:13 visual_prompt]: Epoch 84 / 100: avg data time: 4.08e-02, avg batch time: 0.4894, average train loss: 0.0002
[09/26 00:41:15 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1689, average loss: 2.9548
[09/26 00:41:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:41:15 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 00:41:21 visual_prompt]: Epoch 85 / 100: avg data time: 4.06e-02, avg batch time: 0.4889, average train loss: 0.0001
[09/26 00:41:23 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 2.9549
[09/26 00:41:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:41:23 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 00:41:30 visual_prompt]: Epoch 86 / 100: avg data time: 5.23e-02, avg batch time: 0.5016, average train loss: 0.0001
[09/26 00:41:31 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1692, average loss: 2.9551
[09/26 00:41:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:41:31 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 00:41:38 visual_prompt]: Epoch 87 / 100: avg data time: 5.15e-02, avg batch time: 0.5013, average train loss: 0.0001
[09/26 00:41:39 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1693, average loss: 2.9555
[09/26 00:41:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:41:39 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 00:41:46 visual_prompt]: Epoch 88 / 100: avg data time: 4.78e-02, avg batch time: 0.4973, average train loss: 0.0001
[09/26 00:41:47 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.1688, average loss: 2.9557
[09/26 00:41:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:41:47 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 00:41:54 visual_prompt]: Epoch 89 / 100: avg data time: 5.54e-02, avg batch time: 0.5050, average train loss: 0.0001
[09/26 00:41:56 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1690, average loss: 2.9559
[09/26 00:41:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:41:56 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 00:42:02 visual_prompt]: Epoch 90 / 100: avg data time: 4.36e-02, avg batch time: 0.4920, average train loss: 0.0001
[09/26 00:42:04 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1697, average loss: 2.9560
[09/26 00:42:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:42:04 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 00:42:11 visual_prompt]: Epoch 91 / 100: avg data time: 5.33e-02, avg batch time: 0.5015, average train loss: 0.0001
[09/26 00:42:12 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1695, average loss: 2.9561
[09/26 00:42:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:42:12 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 00:42:19 visual_prompt]: Epoch 92 / 100: avg data time: 5.60e-02, avg batch time: 0.5050, average train loss: 0.0001
[09/26 00:42:21 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 2.9562
[09/26 00:42:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:42:21 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 00:42:27 visual_prompt]: Epoch 93 / 100: avg data time: 5.64e-02, avg batch time: 0.5051, average train loss: 0.0001
[09/26 00:42:29 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1690, average loss: 2.9562
[09/26 00:42:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:42:29 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 00:42:36 visual_prompt]: Epoch 94 / 100: avg data time: 4.28e-02, avg batch time: 0.4927, average train loss: 0.0001
[09/26 00:42:37 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1697, average loss: 2.9563
[09/26 00:42:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:42:37 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 00:42:44 visual_prompt]: Epoch 95 / 100: avg data time: 4.98e-02, avg batch time: 0.4996, average train loss: 0.0001
[09/26 00:42:45 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 2.9563
[09/26 00:42:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:42:45 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 00:42:52 visual_prompt]: Epoch 96 / 100: avg data time: 4.04e-02, avg batch time: 0.4906, average train loss: 0.0001
[09/26 00:42:53 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1697, average loss: 2.9563
[09/26 00:42:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:42:53 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 00:43:00 visual_prompt]: Epoch 97 / 100: avg data time: 4.73e-02, avg batch time: 0.4964, average train loss: 0.0001
[09/26 00:43:02 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1688, average loss: 2.9564
[09/26 00:43:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:43:02 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 00:43:08 visual_prompt]: Epoch 98 / 100: avg data time: 4.40e-02, avg batch time: 0.4947, average train loss: 0.0001
[09/26 00:43:10 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1691, average loss: 2.9564
[09/26 00:43:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:43:10 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 00:43:17 visual_prompt]: Epoch 99 / 100: avg data time: 5.34e-02, avg batch time: 0.5018, average train loss: 0.0001
[09/26 00:43:18 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 2.9564
[09/26 00:43:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:43:18 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 00:43:25 visual_prompt]: Epoch 100 / 100: avg data time: 4.89e-02, avg batch time: 0.4968, average train loss: 0.0001
[09/26 00:43:26 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1690, average loss: 2.9564
[09/26 00:43:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 00:43:26 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 00:43:26 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 00:43:26 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 00:43:26 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 00:43:26 visual_prompt]: Training with config:
[09/26 00:43:26 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 00:43:26 visual_prompt]: Loading training data...
[09/26 00:43:26 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 00:43:28 visual_prompt]: Number of images: 800
[09/26 00:43:28 visual_prompt]: Number of classes: 47 / 47
[09/26 00:43:28 visual_prompt]: Loading validation data...
[09/26 00:43:28 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 00:43:29 visual_prompt]: Number of images: 200
[09/26 00:43:29 visual_prompt]: Number of classes: 47 / 47
[09/26 00:43:29 visual_prompt]: Constructing models...
[09/26 00:43:31 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 00:43:31 visual_prompt]: tuned percent:0.576
[09/26 00:43:31 visual_prompt]: Device used for model: 0
[09/26 00:43:31 visual_prompt]: Setting up Evaluator...
[09/26 00:43:31 visual_prompt]: Setting up Trainer...
[09/26 00:43:31 visual_prompt]: 	Setting up the optimizer...
[09/26 00:43:31 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 00:43:38 visual_prompt]: Epoch 1 / 100: avg data time: 5.60e-02, avg batch time: 0.5027, average train loss: 3.9273
[09/26 00:43:40 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1687, average loss: 3.9045
[09/26 00:43:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 00:43:40 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 00:43:40 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 00:43:46 visual_prompt]: Epoch 2 / 100: avg data time: 5.05e-02, avg batch time: 0.4972, average train loss: 3.9175
[09/26 00:43:48 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1687, average loss: 3.9621
[09/26 00:43:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.50	
[09/26 00:43:48 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 00:43:55 visual_prompt]: Epoch 3 / 100: avg data time: 5.66e-02, avg batch time: 0.5039, average train loss: 3.8675
[09/26 00:43:56 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1689, average loss: 4.0823
[09/26 00:43:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 7.50	
[09/26 00:43:56 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 00:44:03 visual_prompt]: Epoch 4 / 100: avg data time: 4.62e-02, avg batch time: 0.4930, average train loss: 3.9739
[09/26 00:44:04 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1688, average loss: 4.0194
[09/26 00:44:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/26 00:44:04 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 00:44:11 visual_prompt]: Epoch 5 / 100: avg data time: 4.55e-02, avg batch time: 0.4939, average train loss: 3.9392
[09/26 00:44:13 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 4.0023
[09/26 00:44:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 7.00	
[09/26 00:44:13 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 00:44:19 visual_prompt]: Epoch 6 / 100: avg data time: 4.83e-02, avg batch time: 0.4956, average train loss: 4.0455
[09/26 00:44:21 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1685, average loss: 4.3171
[09/26 00:44:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/26 00:44:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 00:44:28 visual_prompt]: Epoch 7 / 100: avg data time: 5.68e-02, avg batch time: 0.5034, average train loss: 4.1197
[09/26 00:44:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1681, average loss: 4.0213
[09/26 00:44:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.00	
[09/26 00:44:29 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 00:44:36 visual_prompt]: Epoch 8 / 100: avg data time: 5.26e-02, avg batch time: 0.5015, average train loss: 4.2396
[09/26 00:44:37 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1685, average loss: 5.7303
[09/26 00:44:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/26 00:44:37 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 00:44:44 visual_prompt]: Epoch 9 / 100: avg data time: 5.03e-02, avg batch time: 0.4982, average train loss: 5.1136
[09/26 00:44:45 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1689, average loss: 4.8208
[09/26 00:44:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 00:44:45 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 00:44:52 visual_prompt]: Epoch 10 / 100: avg data time: 4.03e-02, avg batch time: 0.4899, average train loss: 5.6060
[09/26 00:44:54 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1679, average loss: 4.8047
[09/26 00:44:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.00	
[09/26 00:44:54 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 00:45:00 visual_prompt]: Epoch 11 / 100: avg data time: 4.47e-02, avg batch time: 0.4934, average train loss: 5.5797
[09/26 00:45:02 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1685, average loss: 4.8652
[09/26 00:45:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.50	
[09/26 00:45:02 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 00:45:08 visual_prompt]: Epoch 12 / 100: avg data time: 4.45e-02, avg batch time: 0.4940, average train loss: 5.5709
[09/26 00:45:10 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1689, average loss: 4.6606
[09/26 00:45:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/26 00:45:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 00:45:17 visual_prompt]: Epoch 13 / 100: avg data time: 4.39e-02, avg batch time: 0.4934, average train loss: 5.1312
[09/26 00:45:18 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1688, average loss: 7.4271
[09/26 00:45:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.00	
[09/26 00:45:18 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 00:45:25 visual_prompt]: Epoch 14 / 100: avg data time: 4.90e-02, avg batch time: 0.4975, average train loss: 5.9023
[09/26 00:45:26 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 5.2147
[09/26 00:45:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 00:45:26 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 00:45:33 visual_prompt]: Epoch 15 / 100: avg data time: 4.77e-02, avg batch time: 0.4977, average train loss: 5.4477
[09/26 00:45:34 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 5.9592
[09/26 00:45:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/26 00:45:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 00:45:41 visual_prompt]: Epoch 16 / 100: avg data time: 4.37e-02, avg batch time: 0.4923, average train loss: 5.9780
[09/26 00:45:42 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1690, average loss: 7.3728
[09/26 00:45:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 00:45:42 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 00:45:49 visual_prompt]: Epoch 17 / 100: avg data time: 5.57e-02, avg batch time: 0.5030, average train loss: 5.6995
[09/26 00:45:51 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1689, average loss: 4.8971
[09/26 00:45:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/26 00:45:51 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 00:45:57 visual_prompt]: Epoch 18 / 100: avg data time: 4.28e-02, avg batch time: 0.4921, average train loss: 5.5631
[09/26 00:45:59 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 5.9644
[09/26 00:45:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 6.00	
[09/26 00:45:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 00:46:06 visual_prompt]: Epoch 19 / 100: avg data time: 4.48e-02, avg batch time: 0.4946, average train loss: 5.5334
[09/26 00:46:07 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1692, average loss: 4.8004
[09/26 00:46:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.50	
[09/26 00:46:07 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 00:46:14 visual_prompt]: Epoch 20 / 100: avg data time: 5.88e-02, avg batch time: 0.5063, average train loss: 5.8291
[09/26 00:46:15 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1689, average loss: 5.3949
[09/26 00:46:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 00:46:15 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 00:46:22 visual_prompt]: Epoch 21 / 100: avg data time: 5.14e-02, avg batch time: 0.4988, average train loss: 5.2550
[09/26 00:46:24 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 5.1153
[09/26 00:46:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.50	
[09/26 00:46:24 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 00:46:30 visual_prompt]: Epoch 22 / 100: avg data time: 5.25e-02, avg batch time: 0.5000, average train loss: 5.7087
[09/26 00:46:32 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1689, average loss: 5.6528
[09/26 00:46:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 15.00	
[09/26 00:46:32 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 00:46:38 visual_prompt]: Epoch 23 / 100: avg data time: 4.73e-02, avg batch time: 0.4950, average train loss: 6.9783
[09/26 00:46:40 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1694, average loss: 27.0053
[09/26 00:46:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.50	
[09/26 00:46:40 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 00:46:47 visual_prompt]: Epoch 24 / 100: avg data time: 4.68e-02, avg batch time: 0.4957, average train loss: 11.9011
[09/26 00:46:48 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1687, average loss: 9.6561
[09/26 00:46:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 00:46:48 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 00:46:55 visual_prompt]: Epoch 25 / 100: avg data time: 5.27e-02, avg batch time: 0.5002, average train loss: 9.1204
[09/26 00:46:56 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1688, average loss: 7.7422
[09/26 00:46:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/26 00:46:56 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 00:47:03 visual_prompt]: Epoch 26 / 100: avg data time: 5.42e-02, avg batch time: 0.5025, average train loss: 8.3710
[09/26 00:47:05 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1688, average loss: 7.5212
[09/26 00:47:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.50	
[09/26 00:47:05 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 00:47:11 visual_prompt]: Epoch 27 / 100: avg data time: 5.27e-02, avg batch time: 0.5014, average train loss: 17.9201
[09/26 00:47:13 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1686, average loss: 7.3669
[09/26 00:47:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 00:47:13 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 00:47:19 visual_prompt]: Epoch 28 / 100: avg data time: 4.57e-02, avg batch time: 0.4944, average train loss: 6.9919
[09/26 00:47:21 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 5.7603
[09/26 00:47:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/26 00:47:21 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 00:47:28 visual_prompt]: Epoch 29 / 100: avg data time: 5.45e-02, avg batch time: 0.5039, average train loss: 6.8067
[09/26 00:47:29 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1690, average loss: 6.8212
[09/26 00:47:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/26 00:47:29 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 00:47:36 visual_prompt]: Epoch 30 / 100: avg data time: 5.62e-02, avg batch time: 0.5047, average train loss: 7.1574
[09/26 00:47:37 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1692, average loss: 5.5839
[09/26 00:47:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/26 00:47:37 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 00:47:44 visual_prompt]: Epoch 31 / 100: avg data time: 4.73e-02, avg batch time: 0.4961, average train loss: 6.0540
[09/26 00:47:46 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1685, average loss: 5.8991
[09/26 00:47:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.50	
[09/26 00:47:46 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 00:47:52 visual_prompt]: Epoch 32 / 100: avg data time: 4.34e-02, avg batch time: 0.4920, average train loss: 6.2115
[09/26 00:47:54 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 5.8200
[09/26 00:47:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/26 00:47:54 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 00:48:00 visual_prompt]: Epoch 33 / 100: avg data time: 4.41e-02, avg batch time: 0.4926, average train loss: 5.2741
[09/26 00:48:02 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1689, average loss: 4.7759
[09/26 00:48:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.50	
[09/26 00:48:02 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 00:48:09 visual_prompt]: Epoch 34 / 100: avg data time: 5.81e-02, avg batch time: 0.5055, average train loss: 4.4472
[09/26 00:48:10 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1691, average loss: 4.7002
[09/26 00:48:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 00:48:10 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 00:48:17 visual_prompt]: Epoch 35 / 100: avg data time: 4.84e-02, avg batch time: 0.4984, average train loss: 5.2814
[09/26 00:48:18 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1700, average loss: 6.7751
[09/26 00:48:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.50	
[09/26 00:48:18 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 00:48:25 visual_prompt]: Epoch 36 / 100: avg data time: 4.15e-02, avg batch time: 0.4919, average train loss: 8.8036
[09/26 00:48:26 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1688, average loss: 6.6809
[09/26 00:48:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 00:48:26 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 00:48:34 visual_prompt]: Epoch 37 / 100: avg data time: 4.41e-02, avg batch time: 0.5629, average train loss: 7.5203
[09/26 00:48:35 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1685, average loss: 6.0138
[09/26 00:48:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 6.00	
[09/26 00:48:35 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 00:48:42 visual_prompt]: Epoch 38 / 100: avg data time: 5.30e-02, avg batch time: 0.5009, average train loss: 6.5731
[09/26 00:48:44 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1689, average loss: 5.6306
[09/26 00:48:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 00:48:44 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 00:48:51 visual_prompt]: Epoch 39 / 100: avg data time: 5.93e-02, avg batch time: 0.5063, average train loss: 6.5300
[09/26 00:48:52 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1686, average loss: 5.8214
[09/26 00:48:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.00	
[09/26 00:48:52 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 00:48:59 visual_prompt]: Epoch 40 / 100: avg data time: 5.61e-02, avg batch time: 0.5032, average train loss: 6.3443
[09/26 00:49:00 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1689, average loss: 5.8424
[09/26 00:49:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.50	
[09/26 00:49:00 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 00:49:07 visual_prompt]: Epoch 41 / 100: avg data time: 4.35e-02, avg batch time: 0.4919, average train loss: 5.6266
[09/26 00:49:08 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1688, average loss: 5.1252
[09/26 00:49:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/26 00:49:08 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 00:49:15 visual_prompt]: Epoch 42 / 100: avg data time: 5.15e-02, avg batch time: 0.4988, average train loss: 5.1759
[09/26 00:49:17 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1687, average loss: 4.4841
[09/26 00:49:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 15.00	
[09/26 00:49:17 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 00:49:23 visual_prompt]: Epoch 43 / 100: avg data time: 4.29e-02, avg batch time: 0.4928, average train loss: 4.5461
[09/26 00:49:25 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1685, average loss: 4.2017
[09/26 00:49:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 14.00	
[09/26 00:49:25 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 00:49:31 visual_prompt]: Epoch 44 / 100: avg data time: 4.44e-02, avg batch time: 0.4943, average train loss: 4.3293
[09/26 00:49:33 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1685, average loss: 4.3242
[09/26 00:49:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 00:49:33 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 00:49:40 visual_prompt]: Epoch 45 / 100: avg data time: 4.87e-02, avg batch time: 0.4977, average train loss: 4.2054
[09/26 00:49:41 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1686, average loss: 4.0730
[09/26 00:49:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.50	
[09/26 00:49:41 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 00:49:48 visual_prompt]: Epoch 46 / 100: avg data time: 5.35e-02, avg batch time: 0.5022, average train loss: 4.2420
[09/26 00:49:50 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1688, average loss: 4.1734
[09/26 00:49:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 13.50	
[09/26 00:49:50 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 00:49:56 visual_prompt]: Epoch 47 / 100: avg data time: 4.81e-02, avg batch time: 0.4970, average train loss: 4.2432
[09/26 00:49:58 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 4.3784
[09/26 00:49:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 00:49:58 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 00:50:04 visual_prompt]: Epoch 48 / 100: avg data time: 4.56e-02, avg batch time: 0.4940, average train loss: 4.4736
[09/26 00:50:06 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1688, average loss: 4.1867
[09/26 00:50:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.00	
[09/26 00:50:06 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 00:50:13 visual_prompt]: Epoch 49 / 100: avg data time: 4.12e-02, avg batch time: 0.4930, average train loss: 4.2651
[09/26 00:50:14 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 4.1683
[09/26 00:50:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 00:50:14 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 00:50:21 visual_prompt]: Epoch 50 / 100: avg data time: 5.42e-02, avg batch time: 0.5020, average train loss: 4.2499
[09/26 00:50:22 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1686, average loss: 4.2384
[09/26 00:50:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 12.00	
[09/26 00:50:22 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 00:50:29 visual_prompt]: Epoch 51 / 100: avg data time: 5.12e-02, avg batch time: 0.5015, average train loss: 4.1488
[09/26 00:50:31 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1687, average loss: 4.3448
[09/26 00:50:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 12.00	
[09/26 00:50:31 visual_prompt]: Best epoch 51: best metric: 0.045
[09/26 00:50:31 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 00:50:38 visual_prompt]: Epoch 52 / 100: avg data time: 5.88e-02, avg batch time: 0.5061, average train loss: 4.5191
[09/26 00:50:39 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 4.3179
[09/26 00:50:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 00:50:39 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 00:50:46 visual_prompt]: Epoch 53 / 100: avg data time: 5.47e-02, avg batch time: 0.5035, average train loss: 4.2882
[09/26 00:50:47 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1690, average loss: 4.7208
[09/26 00:50:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/26 00:50:47 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 00:50:54 visual_prompt]: Epoch 54 / 100: avg data time: 5.43e-02, avg batch time: 0.5026, average train loss: 4.2498
[09/26 00:50:56 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1686, average loss: 4.2453
[09/26 00:50:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 7.00	
[09/26 00:50:56 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 00:51:02 visual_prompt]: Epoch 55 / 100: avg data time: 4.10e-02, avg batch time: 0.4912, average train loss: 4.1207
[09/26 00:51:04 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1690, average loss: 4.2358
[09/26 00:51:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.50	
[09/26 00:51:04 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 00:51:11 visual_prompt]: Epoch 56 / 100: avg data time: 5.56e-02, avg batch time: 0.5037, average train loss: 4.0348
[09/26 00:51:12 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1685, average loss: 4.1138
[09/26 00:51:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/26 00:51:12 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 00:51:19 visual_prompt]: Epoch 57 / 100: avg data time: 5.57e-02, avg batch time: 0.5042, average train loss: 4.0710
[09/26 00:51:20 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1683, average loss: 4.0932
[09/26 00:51:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/26 00:51:20 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 00:51:27 visual_prompt]: Epoch 58 / 100: avg data time: 5.35e-02, avg batch time: 0.5018, average train loss: 4.2639
[09/26 00:51:29 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1686, average loss: 4.1196
[09/26 00:51:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 00:51:29 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 00:51:35 visual_prompt]: Epoch 59 / 100: avg data time: 4.90e-02, avg batch time: 0.4967, average train loss: 4.1137
[09/26 00:51:37 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 4.1431
[09/26 00:51:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 9.50	
[09/26 00:51:37 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 00:51:44 visual_prompt]: Epoch 60 / 100: avg data time: 4.98e-02, avg batch time: 0.4981, average train loss: 4.0982
[09/26 00:51:45 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1681, average loss: 4.3749
[09/26 00:51:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/26 00:51:45 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 00:51:52 visual_prompt]: Epoch 61 / 100: avg data time: 5.64e-02, avg batch time: 0.5047, average train loss: 4.3116
[09/26 00:51:53 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1687, average loss: 4.0522
[09/26 00:51:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.00	
[09/26 00:51:53 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 00:52:00 visual_prompt]: Epoch 62 / 100: avg data time: 4.69e-02, avg batch time: 0.4976, average train loss: 4.2515
[09/26 00:52:01 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1689, average loss: 4.0297
[09/26 00:52:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.50	
[09/26 00:52:01 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 00:52:08 visual_prompt]: Epoch 63 / 100: avg data time: 5.70e-02, avg batch time: 0.5051, average train loss: 4.0502
[09/26 00:52:10 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1687, average loss: 4.2631
[09/26 00:52:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 10.00	
[09/26 00:52:10 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 00:52:17 visual_prompt]: Epoch 64 / 100: avg data time: 5.59e-02, avg batch time: 0.5038, average train loss: 4.1272
[09/26 00:52:18 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1680, average loss: 3.9598
[09/26 00:52:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 00:52:18 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 00:52:25 visual_prompt]: Epoch 65 / 100: avg data time: 5.11e-02, avg batch time: 0.5001, average train loss: 3.9745
[09/26 00:52:26 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1688, average loss: 4.0863
[09/26 00:52:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 7.00	
[09/26 00:52:26 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 00:52:33 visual_prompt]: Epoch 66 / 100: avg data time: 5.25e-02, avg batch time: 0.5010, average train loss: 4.0211
[09/26 00:52:35 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1685, average loss: 4.1589
[09/26 00:52:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 00:52:35 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 00:52:41 visual_prompt]: Epoch 67 / 100: avg data time: 5.21e-02, avg batch time: 0.5011, average train loss: 3.9814
[09/26 00:52:43 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1689, average loss: 3.9559
[09/26 00:52:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 00:52:43 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 00:52:50 visual_prompt]: Epoch 68 / 100: avg data time: 5.64e-02, avg batch time: 0.5040, average train loss: 3.9421
[09/26 00:52:51 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1691, average loss: 4.1241
[09/26 00:52:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 00:52:51 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 00:52:58 visual_prompt]: Epoch 69 / 100: avg data time: 6.57e-02, avg batch time: 0.5137, average train loss: 3.9926
[09/26 00:53:00 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1687, average loss: 4.0280
[09/26 00:53:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 00:53:00 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 00:53:06 visual_prompt]: Epoch 70 / 100: avg data time: 4.52e-02, avg batch time: 0.4956, average train loss: 3.9488
[09/26 00:53:08 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1686, average loss: 4.0303
[09/26 00:53:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 00:53:08 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 00:53:14 visual_prompt]: Epoch 71 / 100: avg data time: 4.34e-02, avg batch time: 0.4909, average train loss: 3.9294
[09/26 00:53:16 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 3.9717
[09/26 00:53:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 12.50	
[09/26 00:53:16 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 00:53:23 visual_prompt]: Epoch 72 / 100: avg data time: 5.78e-02, avg batch time: 0.5068, average train loss: 3.9217
[09/26 00:53:24 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1687, average loss: 3.9687
[09/26 00:53:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 8.50	
[09/26 00:53:24 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 00:53:31 visual_prompt]: Epoch 73 / 100: avg data time: 4.68e-02, avg batch time: 0.4964, average train loss: 3.9166
[09/26 00:53:32 visual_prompt]: Inference (val):avg data time: 4.84e-05, avg batch time: 0.1693, average loss: 3.9582
[09/26 00:53:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 00:53:32 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 00:53:39 visual_prompt]: Epoch 74 / 100: avg data time: 5.06e-02, avg batch time: 0.4993, average train loss: 3.9027
[09/26 00:53:41 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 3.9639
[09/26 00:53:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 7.00	
[09/26 00:53:41 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 00:53:47 visual_prompt]: Epoch 75 / 100: avg data time: 5.31e-02, avg batch time: 0.5007, average train loss: 3.9407
[09/26 00:53:49 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 3.9332
[09/26 00:53:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.50	
[09/26 00:53:49 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 00:53:56 visual_prompt]: Epoch 76 / 100: avg data time: 5.16e-02, avg batch time: 0.4997, average train loss: 3.9104
[09/26 00:53:57 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1693, average loss: 3.9789
[09/26 00:53:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/26 00:53:57 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 00:54:04 visual_prompt]: Epoch 77 / 100: avg data time: 5.69e-02, avg batch time: 0.5058, average train loss: 3.9034
[09/26 00:54:05 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1686, average loss: 3.9655
[09/26 00:54:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/26 00:54:05 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 00:54:12 visual_prompt]: Epoch 78 / 100: avg data time: 5.45e-02, avg batch time: 0.5027, average train loss: 3.9160
[09/26 00:54:14 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1687, average loss: 3.9009
[09/26 00:54:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/26 00:54:14 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 00:54:20 visual_prompt]: Epoch 79 / 100: avg data time: 5.86e-02, avg batch time: 0.5055, average train loss: 3.8953
[09/26 00:54:22 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1683, average loss: 3.8964
[09/26 00:54:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 00:54:22 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 00:54:29 visual_prompt]: Epoch 80 / 100: avg data time: 5.92e-02, avg batch time: 0.5085, average train loss: 3.8721
[09/26 00:54:30 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1691, average loss: 3.9318
[09/26 00:54:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 00:54:30 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 00:54:37 visual_prompt]: Epoch 81 / 100: avg data time: 4.58e-02, avg batch time: 0.4948, average train loss: 3.8769
[09/26 00:54:39 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1688, average loss: 3.9297
[09/26 00:54:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/26 00:54:39 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 00:54:45 visual_prompt]: Epoch 82 / 100: avg data time: 5.28e-02, avg batch time: 0.5013, average train loss: 3.8769
[09/26 00:54:47 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 3.9054
[09/26 00:54:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 00:54:47 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 00:54:54 visual_prompt]: Epoch 83 / 100: avg data time: 5.28e-02, avg batch time: 0.5028, average train loss: 3.8624
[09/26 00:54:55 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1690, average loss: 3.9010
[09/26 00:54:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/26 00:54:55 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 00:55:02 visual_prompt]: Epoch 84 / 100: avg data time: 4.53e-02, avg batch time: 0.4945, average train loss: 3.8784
[09/26 00:55:03 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1691, average loss: 3.9068
[09/26 00:55:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.50	
[09/26 00:55:03 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 00:55:10 visual_prompt]: Epoch 85 / 100: avg data time: 5.87e-02, avg batch time: 0.5070, average train loss: 3.8849
[09/26 00:55:12 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1692, average loss: 3.9040
[09/26 00:55:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.00	
[09/26 00:55:12 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 00:55:18 visual_prompt]: Epoch 86 / 100: avg data time: 4.43e-02, avg batch time: 0.4929, average train loss: 3.8462
[09/26 00:55:20 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 3.9620
[09/26 00:55:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 00:55:20 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 00:55:27 visual_prompt]: Epoch 87 / 100: avg data time: 5.99e-02, avg batch time: 0.5083, average train loss: 3.8582
[09/26 00:55:28 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1691, average loss: 3.9247
[09/26 00:55:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 00:55:28 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 00:55:35 visual_prompt]: Epoch 88 / 100: avg data time: 4.56e-02, avg batch time: 0.4944, average train loss: 3.8633
[09/26 00:55:36 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1687, average loss: 3.9157
[09/26 00:55:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/26 00:55:36 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 00:55:43 visual_prompt]: Epoch 89 / 100: avg data time: 5.61e-02, avg batch time: 0.5035, average train loss: 3.8473
[09/26 00:55:45 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1688, average loss: 3.9211
[09/26 00:55:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 8.00	
[09/26 00:55:45 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 00:55:51 visual_prompt]: Epoch 90 / 100: avg data time: 5.38e-02, avg batch time: 0.5027, average train loss: 3.8448
[09/26 00:55:53 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1689, average loss: 3.8969
[09/26 00:55:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 00:55:53 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 00:56:00 visual_prompt]: Epoch 91 / 100: avg data time: 5.35e-02, avg batch time: 0.5020, average train loss: 3.8298
[09/26 00:56:01 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1688, average loss: 3.9021
[09/26 00:56:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/26 00:56:01 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 00:56:08 visual_prompt]: Epoch 92 / 100: avg data time: 4.57e-02, avg batch time: 0.4937, average train loss: 3.8276
[09/26 00:56:09 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1692, average loss: 3.8773
[09/26 00:56:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.50	
[09/26 00:56:09 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 00:56:16 visual_prompt]: Epoch 93 / 100: avg data time: 5.67e-02, avg batch time: 0.5053, average train loss: 3.7969
[09/26 00:56:18 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1690, average loss: 3.8735
[09/26 00:56:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/26 00:56:18 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 00:56:25 visual_prompt]: Epoch 94 / 100: avg data time: 5.37e-02, avg batch time: 0.5013, average train loss: 3.7338
[09/26 00:56:26 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1688, average loss: 3.7827
[09/26 00:56:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 16.50	
[09/26 00:56:26 visual_prompt]: Best epoch 94: best metric: 0.065
[09/26 00:56:26 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 00:56:33 visual_prompt]: Epoch 95 / 100: avg data time: 5.66e-02, avg batch time: 0.5060, average train loss: 3.5138
[09/26 00:56:34 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1691, average loss: 3.5782
[09/26 00:56:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 31.50	
[09/26 00:56:34 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 00:56:41 visual_prompt]: Epoch 96 / 100: avg data time: 4.90e-02, avg batch time: 0.5001, average train loss: 3.2284
[09/26 00:56:43 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1692, average loss: 3.5550
[09/26 00:56:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.50	top5: 32.00	
[09/26 00:56:43 visual_prompt]: Best epoch 96: best metric: 0.075
[09/26 00:56:43 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 00:56:49 visual_prompt]: Epoch 97 / 100: avg data time: 5.94e-02, avg batch time: 0.5074, average train loss: 2.9679
[09/26 00:56:51 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 3.1702
[09/26 00:56:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 21.00	top5: 52.00	
[09/26 00:56:51 visual_prompt]: Best epoch 97: best metric: 0.210
[09/26 00:56:51 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 00:56:58 visual_prompt]: Epoch 98 / 100: avg data time: 5.67e-02, avg batch time: 0.5048, average train loss: 2.6965
[09/26 00:56:59 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 3.0628
[09/26 00:56:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.50	top5: 56.00	
[09/26 00:56:59 visual_prompt]: Best epoch 98: best metric: 0.235
[09/26 00:56:59 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 00:57:06 visual_prompt]: Epoch 99 / 100: avg data time: 5.03e-02, avg batch time: 0.4985, average train loss: 2.5661
[09/26 00:57:07 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 3.0075
[09/26 00:57:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.00	top5: 57.50	
[09/26 00:57:07 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 00:57:14 visual_prompt]: Epoch 100 / 100: avg data time: 5.53e-02, avg batch time: 0.5036, average train loss: 2.4792
[09/26 00:57:16 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1683, average loss: 3.0065
[09/26 00:57:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.00	top5: 56.50	
[09/26 00:57:16 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 00:57:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 00:57:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 00:57:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 00:57:16 visual_prompt]: Training with config:
[09/26 00:57:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 00:57:16 visual_prompt]: Loading training data...
[09/26 00:57:16 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 00:57:18 visual_prompt]: Number of images: 800
[09/26 00:57:18 visual_prompt]: Number of classes: 47 / 47
[09/26 00:57:18 visual_prompt]: Loading validation data...
[09/26 00:57:18 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 00:57:19 visual_prompt]: Number of images: 200
[09/26 00:57:19 visual_prompt]: Number of classes: 47 / 47
[09/26 00:57:19 visual_prompt]: Constructing models...
[09/26 00:57:21 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 00:57:21 visual_prompt]: tuned percent:0.576
[09/26 00:57:21 visual_prompt]: Device used for model: 0
[09/26 00:57:21 visual_prompt]: Setting up Evaluator...
[09/26 00:57:21 visual_prompt]: Setting up Trainer...
[09/26 00:57:21 visual_prompt]: 	Setting up the optimizer...
[09/26 00:57:21 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 00:57:28 visual_prompt]: Epoch 1 / 100: avg data time: 5.05e-02, avg batch time: 0.4994, average train loss: 3.9260
[09/26 00:57:29 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1691, average loss: 3.9045
[09/26 00:57:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 00:57:29 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 00:57:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 00:57:36 visual_prompt]: Epoch 2 / 100: avg data time: 5.31e-02, avg batch time: 0.4998, average train loss: 3.9138
[09/26 00:57:37 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 3.9073
[09/26 00:57:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 17.00	
[09/26 00:57:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 00:57:44 visual_prompt]: Epoch 3 / 100: avg data time: 5.17e-02, avg batch time: 0.5005, average train loss: 3.8506
[09/26 00:57:46 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1688, average loss: 3.8420
[09/26 00:57:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 17.50	
[09/26 00:57:46 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 00:57:52 visual_prompt]: Epoch 4 / 100: avg data time: 3.99e-02, avg batch time: 0.4897, average train loss: 3.8614
[09/26 00:57:54 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1688, average loss: 4.0861
[09/26 00:57:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/26 00:57:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 00:58:01 visual_prompt]: Epoch 5 / 100: avg data time: 5.54e-02, avg batch time: 0.5026, average train loss: 3.9957
[09/26 00:58:02 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 3.9121
[09/26 00:58:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 20.50	
[09/26 00:58:02 visual_prompt]: Best epoch 5: best metric: 0.050
[09/26 00:58:02 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 00:58:09 visual_prompt]: Epoch 6 / 100: avg data time: 5.47e-02, avg batch time: 0.5027, average train loss: 3.8923
[09/26 00:58:10 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1686, average loss: 3.6663
[09/26 00:58:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.50	top5: 30.50	
[09/26 00:58:10 visual_prompt]: Best epoch 6: best metric: 0.115
[09/26 00:58:10 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 00:58:17 visual_prompt]: Epoch 7 / 100: avg data time: 5.03e-02, avg batch time: 0.4992, average train loss: 4.0045
[09/26 00:58:19 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1692, average loss: 4.0580
[09/26 00:58:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/26 00:58:19 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 00:58:25 visual_prompt]: Epoch 8 / 100: avg data time: 4.36e-02, avg batch time: 0.4932, average train loss: 4.3804
[09/26 00:58:27 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1685, average loss: 4.5718
[09/26 00:58:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/26 00:58:27 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 00:58:33 visual_prompt]: Epoch 9 / 100: avg data time: 4.39e-02, avg batch time: 0.4920, average train loss: 5.1864
[09/26 00:58:35 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 4.7776
[09/26 00:58:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/26 00:58:35 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 00:58:42 visual_prompt]: Epoch 10 / 100: avg data time: 4.59e-02, avg batch time: 0.4958, average train loss: 5.5173
[09/26 00:58:43 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 4.9245
[09/26 00:58:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.50	
[09/26 00:58:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 00:58:50 visual_prompt]: Epoch 11 / 100: avg data time: 4.53e-02, avg batch time: 0.4949, average train loss: 5.6861
[09/26 00:58:51 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1694, average loss: 5.1135
[09/26 00:58:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.50	
[09/26 00:58:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 00:58:58 visual_prompt]: Epoch 12 / 100: avg data time: 4.23e-02, avg batch time: 0.4914, average train loss: 5.7036
[09/26 00:58:59 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1687, average loss: 5.4266
[09/26 00:58:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 00:58:59 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 00:59:06 visual_prompt]: Epoch 13 / 100: avg data time: 4.06e-02, avg batch time: 0.4904, average train loss: 5.3984
[09/26 00:59:07 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1692, average loss: 5.1146
[09/26 00:59:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/26 00:59:07 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 00:59:14 visual_prompt]: Epoch 14 / 100: avg data time: 4.12e-02, avg batch time: 0.4899, average train loss: 4.9505
[09/26 00:59:15 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1695, average loss: 4.4362
[09/26 00:59:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.50	
[09/26 00:59:15 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 00:59:22 visual_prompt]: Epoch 15 / 100: avg data time: 4.90e-02, avg batch time: 0.5011, average train loss: 4.7793
[09/26 00:59:24 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 4.5981
[09/26 00:59:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.50	
[09/26 00:59:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 00:59:30 visual_prompt]: Epoch 16 / 100: avg data time: 4.06e-02, avg batch time: 0.4911, average train loss: 4.4837
[09/26 00:59:32 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1695, average loss: 4.3157
[09/26 00:59:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.50	
[09/26 00:59:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 00:59:39 visual_prompt]: Epoch 17 / 100: avg data time: 4.87e-02, avg batch time: 0.4974, average train loss: 4.5007
[09/26 00:59:40 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 4.8535
[09/26 00:59:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/26 00:59:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 00:59:47 visual_prompt]: Epoch 18 / 100: avg data time: 4.16e-02, avg batch time: 0.4913, average train loss: 4.9093
[09/26 00:59:48 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1691, average loss: 4.5765
[09/26 00:59:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/26 00:59:48 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 00:59:55 visual_prompt]: Epoch 19 / 100: avg data time: 5.81e-02, avg batch time: 0.5064, average train loss: 4.8215
[09/26 00:59:56 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 4.9019
[09/26 00:59:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.00	
[09/26 00:59:56 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 01:00:03 visual_prompt]: Epoch 20 / 100: avg data time: 4.29e-02, avg batch time: 0.4924, average train loss: 5.1925
[09/26 01:00:04 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 4.8872
[09/26 01:00:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 11.50	
[09/26 01:00:04 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 01:00:11 visual_prompt]: Epoch 21 / 100: avg data time: 4.81e-02, avg batch time: 0.4956, average train loss: 4.7894
[09/26 01:00:13 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 4.6678
[09/26 01:00:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/26 01:00:13 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 01:00:19 visual_prompt]: Epoch 22 / 100: avg data time: 3.75e-02, avg batch time: 0.4885, average train loss: 4.6938
[09/26 01:00:21 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 5.4123
[09/26 01:00:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.50	
[09/26 01:00:21 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 01:00:27 visual_prompt]: Epoch 23 / 100: avg data time: 4.23e-02, avg batch time: 0.4902, average train loss: 4.7893
[09/26 01:00:29 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 4.4756
[09/26 01:00:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/26 01:00:29 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 01:00:35 visual_prompt]: Epoch 24 / 100: avg data time: 5.26e-02, avg batch time: 0.5009, average train loss: 4.4264
[09/26 01:00:37 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 4.2816
[09/26 01:00:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.50	
[09/26 01:00:37 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 01:00:44 visual_prompt]: Epoch 25 / 100: avg data time: 4.43e-02, avg batch time: 0.4936, average train loss: 4.3178
[09/26 01:00:45 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 4.2483
[09/26 01:00:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 01:00:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 01:00:52 visual_prompt]: Epoch 26 / 100: avg data time: 5.56e-02, avg batch time: 0.5053, average train loss: 4.2866
[09/26 01:00:53 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 4.0178
[09/26 01:00:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.50	
[09/26 01:00:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 01:01:00 visual_prompt]: Epoch 27 / 100: avg data time: 4.95e-02, avg batch time: 0.4975, average train loss: 4.6525
[09/26 01:01:02 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1697, average loss: 5.1203
[09/26 01:01:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 01:01:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 01:01:09 visual_prompt]: Epoch 28 / 100: avg data time: 5.47e-02, avg batch time: 0.5030, average train loss: 5.0421
[09/26 01:01:10 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 4.7520
[09/26 01:01:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 16.00	
[09/26 01:01:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 01:01:17 visual_prompt]: Epoch 29 / 100: avg data time: 5.17e-02, avg batch time: 0.5001, average train loss: 4.8789
[09/26 01:01:18 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1692, average loss: 5.3724
[09/26 01:01:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 01:01:18 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 01:01:25 visual_prompt]: Epoch 30 / 100: avg data time: 4.82e-02, avg batch time: 0.4976, average train loss: 4.5916
[09/26 01:01:26 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1689, average loss: 4.1369
[09/26 01:01:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.00	
[09/26 01:01:26 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 01:01:33 visual_prompt]: Epoch 31 / 100: avg data time: 4.45e-02, avg batch time: 0.4933, average train loss: 4.3912
[09/26 01:01:34 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1693, average loss: 4.1058
[09/26 01:01:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 01:01:34 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 01:01:41 visual_prompt]: Epoch 32 / 100: avg data time: 4.65e-02, avg batch time: 0.4943, average train loss: 4.2638
[09/26 01:01:43 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1693, average loss: 4.2187
[09/26 01:01:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 11.00	
[09/26 01:01:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 01:01:49 visual_prompt]: Epoch 33 / 100: avg data time: 4.10e-02, avg batch time: 0.4894, average train loss: 4.1452
[09/26 01:01:51 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 4.1678
[09/26 01:01:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.00	
[09/26 01:01:51 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 01:01:57 visual_prompt]: Epoch 34 / 100: avg data time: 4.05e-02, avg batch time: 0.4910, average train loss: 4.1032
[09/26 01:01:59 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1692, average loss: 4.1743
[09/26 01:01:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 01:01:59 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 01:02:05 visual_prompt]: Epoch 35 / 100: avg data time: 5.54e-02, avg batch time: 0.5041, average train loss: 4.0467
[09/26 01:02:07 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1691, average loss: 4.0763
[09/26 01:02:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/26 01:02:07 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 01:02:14 visual_prompt]: Epoch 36 / 100: avg data time: 5.34e-02, avg batch time: 0.5007, average train loss: 4.0457
[09/26 01:02:15 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 4.1998
[09/26 01:02:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/26 01:02:15 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 01:02:22 visual_prompt]: Epoch 37 / 100: avg data time: 4.10e-02, avg batch time: 0.4894, average train loss: 4.0539
[09/26 01:02:23 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 4.0522
[09/26 01:02:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 9.50	
[09/26 01:02:23 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 01:02:30 visual_prompt]: Epoch 38 / 100: avg data time: 4.25e-02, avg batch time: 0.4920, average train loss: 4.0116
[09/26 01:02:31 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1691, average loss: 4.1486
[09/26 01:02:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.50	
[09/26 01:02:31 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 01:02:38 visual_prompt]: Epoch 39 / 100: avg data time: 4.96e-02, avg batch time: 0.4984, average train loss: 4.0209
[09/26 01:02:39 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1687, average loss: 4.2177
[09/26 01:02:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.50	
[09/26 01:02:39 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 01:02:46 visual_prompt]: Epoch 40 / 100: avg data time: 5.33e-02, avg batch time: 0.5020, average train loss: 4.0243
[09/26 01:02:48 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1694, average loss: 4.0909
[09/26 01:02:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/26 01:02:48 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 01:02:54 visual_prompt]: Epoch 41 / 100: avg data time: 4.08e-02, avg batch time: 0.4901, average train loss: 4.0280
[09/26 01:02:56 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1685, average loss: 4.1685
[09/26 01:02:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/26 01:02:56 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 01:03:03 visual_prompt]: Epoch 42 / 100: avg data time: 5.24e-02, avg batch time: 0.4999, average train loss: 4.0148
[09/26 01:03:04 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1687, average loss: 4.1946
[09/26 01:03:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 01:03:04 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 01:03:11 visual_prompt]: Epoch 43 / 100: avg data time: 4.32e-02, avg batch time: 0.4933, average train loss: 4.0142
[09/26 01:03:12 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 4.1113
[09/26 01:03:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/26 01:03:12 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 01:03:19 visual_prompt]: Epoch 44 / 100: avg data time: 5.40e-02, avg batch time: 0.5019, average train loss: 4.0838
[09/26 01:03:20 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1692, average loss: 4.2068
[09/26 01:03:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.50	
[09/26 01:03:20 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 01:03:27 visual_prompt]: Epoch 45 / 100: avg data time: 4.87e-02, avg batch time: 0.5008, average train loss: 4.0915
[09/26 01:03:29 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1691, average loss: 4.0865
[09/26 01:03:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.00	
[09/26 01:03:29 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 01:03:35 visual_prompt]: Epoch 46 / 100: avg data time: 4.43e-02, avg batch time: 0.4937, average train loss: 4.0005
[09/26 01:03:37 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1697, average loss: 4.0028
[09/26 01:03:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.00	
[09/26 01:03:37 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 01:03:44 visual_prompt]: Epoch 47 / 100: avg data time: 5.21e-02, avg batch time: 0.5004, average train loss: 3.9692
[09/26 01:03:45 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 4.0080
[09/26 01:03:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 16.00	
[09/26 01:03:45 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 01:03:52 visual_prompt]: Epoch 48 / 100: avg data time: 4.26e-02, avg batch time: 0.4924, average train loss: 3.9620
[09/26 01:03:53 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1693, average loss: 3.9562
[09/26 01:03:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.00	
[09/26 01:03:53 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 01:04:00 visual_prompt]: Epoch 49 / 100: avg data time: 4.11e-02, avg batch time: 0.4910, average train loss: 3.9073
[09/26 01:04:01 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 4.0146
[09/26 01:04:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 11.00	
[09/26 01:04:01 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 01:04:08 visual_prompt]: Epoch 50 / 100: avg data time: 4.12e-02, avg batch time: 0.4923, average train loss: 3.9382
[09/26 01:04:09 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 4.1349
[09/26 01:04:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/26 01:04:09 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 01:04:16 visual_prompt]: Epoch 51 / 100: avg data time: 4.36e-02, avg batch time: 0.4927, average train loss: 4.0330
[09/26 01:04:18 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1694, average loss: 3.9999
[09/26 01:04:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.00	
[09/26 01:04:18 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 01:04:24 visual_prompt]: Epoch 52 / 100: avg data time: 4.93e-02, avg batch time: 0.4978, average train loss: 4.1293
[09/26 01:04:26 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 4.1821
[09/26 01:04:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/26 01:04:26 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 01:04:32 visual_prompt]: Epoch 53 / 100: avg data time: 5.03e-02, avg batch time: 0.4991, average train loss: 4.0368
[09/26 01:04:34 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 3.9462
[09/26 01:04:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 14.50	
[09/26 01:04:34 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 01:04:41 visual_prompt]: Epoch 54 / 100: avg data time: 4.69e-02, avg batch time: 0.4954, average train loss: 3.7730
[09/26 01:04:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1697, average loss: 4.0911
[09/26 01:04:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 14.50	
[09/26 01:04:42 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 01:04:49 visual_prompt]: Epoch 55 / 100: avg data time: 6.15e-02, avg batch time: 0.5107, average train loss: 3.3903
[09/26 01:04:50 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1697, average loss: 3.6326
[09/26 01:04:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.00	top5: 38.50	
[09/26 01:04:50 visual_prompt]: Best epoch 55: best metric: 0.150
[09/26 01:04:50 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 01:04:57 visual_prompt]: Epoch 56 / 100: avg data time: 4.08e-02, avg batch time: 0.4897, average train loss: 2.8191
[09/26 01:04:58 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1694, average loss: 3.0611
[09/26 01:04:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.50	top5: 52.00	
[09/26 01:04:58 visual_prompt]: Best epoch 56: best metric: 0.235
[09/26 01:04:58 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 01:05:05 visual_prompt]: Epoch 57 / 100: avg data time: 4.36e-02, avg batch time: 0.4923, average train loss: 2.1393
[09/26 01:05:07 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 3.2268
[09/26 01:05:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 21.00	top5: 49.00	
[09/26 01:05:07 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 01:05:13 visual_prompt]: Epoch 58 / 100: avg data time: 4.40e-02, avg batch time: 0.4952, average train loss: 2.0967
[09/26 01:05:15 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1693, average loss: 2.8186
[09/26 01:05:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.50	top5: 67.00	
[09/26 01:05:15 visual_prompt]: Best epoch 58: best metric: 0.305
[09/26 01:05:15 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 01:05:21 visual_prompt]: Epoch 59 / 100: avg data time: 4.63e-02, avg batch time: 0.4955, average train loss: 1.4055
[09/26 01:05:23 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1696, average loss: 2.4180
[09/26 01:05:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 76.50	
[09/26 01:05:23 visual_prompt]: Best epoch 59: best metric: 0.425
[09/26 01:05:23 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 01:05:30 visual_prompt]: Epoch 60 / 100: avg data time: 5.00e-02, avg batch time: 0.5007, average train loss: 0.9512
[09/26 01:05:31 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1698, average loss: 2.1182
[09/26 01:05:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 78.00	
[09/26 01:05:31 visual_prompt]: Best epoch 60: best metric: 0.450
[09/26 01:05:31 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 01:05:38 visual_prompt]: Epoch 61 / 100: avg data time: 4.99e-02, avg batch time: 0.4988, average train loss: 0.5559
[09/26 01:05:39 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1699, average loss: 1.9799
[09/26 01:05:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.50	
[09/26 01:05:39 visual_prompt]: Best epoch 61: best metric: 0.480
[09/26 01:05:39 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 01:05:46 visual_prompt]: Epoch 62 / 100: avg data time: 5.29e-02, avg batch time: 0.5019, average train loss: 0.3283
[09/26 01:05:48 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1692, average loss: 1.9846
[09/26 01:05:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 81.50	
[09/26 01:05:48 visual_prompt]: Best epoch 62: best metric: 0.530
[09/26 01:05:48 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 01:05:54 visual_prompt]: Epoch 63 / 100: avg data time: 5.12e-02, avg batch time: 0.5001, average train loss: 0.2454
[09/26 01:05:56 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1696, average loss: 1.8769
[09/26 01:05:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 82.50	
[09/26 01:05:56 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 01:06:03 visual_prompt]: Epoch 64 / 100: avg data time: 5.30e-02, avg batch time: 0.5023, average train loss: 0.1875
[09/26 01:06:04 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1694, average loss: 1.7216
[09/26 01:06:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 83.50	
[09/26 01:06:04 visual_prompt]: Best epoch 64: best metric: 0.570
[09/26 01:06:04 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 01:06:11 visual_prompt]: Epoch 65 / 100: avg data time: 5.29e-02, avg batch time: 0.5017, average train loss: 0.1626
[09/26 01:06:12 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 1.6694
[09/26 01:06:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 01:06:12 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 01:06:19 visual_prompt]: Epoch 66 / 100: avg data time: 5.99e-02, avg batch time: 0.5091, average train loss: 0.1134
[09/26 01:06:21 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1697, average loss: 1.5755
[09/26 01:06:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 01:06:21 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 01:06:27 visual_prompt]: Epoch 67 / 100: avg data time: 5.37e-02, avg batch time: 0.5021, average train loss: 0.0921
[09/26 01:06:29 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 1.5876
[09/26 01:06:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 85.50	
[09/26 01:06:29 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 01:06:36 visual_prompt]: Epoch 68 / 100: avg data time: 4.57e-02, avg batch time: 0.4947, average train loss: 0.0840
[09/26 01:06:37 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1695, average loss: 1.5728
[09/26 01:06:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.50	
[09/26 01:06:37 visual_prompt]: Best epoch 68: best metric: 0.580
[09/26 01:06:37 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 01:06:44 visual_prompt]: Epoch 69 / 100: avg data time: 4.01e-02, avg batch time: 0.4912, average train loss: 0.0823
[09/26 01:06:45 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 1.5621
[09/26 01:06:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 88.00	
[09/26 01:06:45 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 01:06:52 visual_prompt]: Epoch 70 / 100: avg data time: 5.08e-02, avg batch time: 0.5009, average train loss: 0.0790
[09/26 01:06:53 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 1.5200
[09/26 01:06:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 87.50	
[09/26 01:06:53 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 01:07:00 visual_prompt]: Epoch 71 / 100: avg data time: 4.92e-02, avg batch time: 0.4978, average train loss: 0.0860
[09/26 01:07:02 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1691, average loss: 1.5715
[09/26 01:07:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 87.00	
[09/26 01:07:02 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 01:07:08 visual_prompt]: Epoch 72 / 100: avg data time: 5.41e-02, avg batch time: 0.5026, average train loss: 0.1195
[09/26 01:07:10 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 1.6293
[09/26 01:07:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 01:07:10 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 01:07:17 visual_prompt]: Epoch 73 / 100: avg data time: 4.83e-02, avg batch time: 0.5006, average train loss: 0.1310
[09/26 01:07:18 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1695, average loss: 1.6110
[09/26 01:07:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.00	
[09/26 01:07:18 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 01:07:25 visual_prompt]: Epoch 74 / 100: avg data time: 4.94e-02, avg batch time: 0.4990, average train loss: 0.1260
[09/26 01:07:26 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 1.7146
[09/26 01:07:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 83.50	
[09/26 01:07:26 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 01:07:33 visual_prompt]: Epoch 75 / 100: avg data time: 5.29e-02, avg batch time: 0.5027, average train loss: 0.1035
[09/26 01:07:35 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1691, average loss: 1.5022
[09/26 01:07:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 92.00	
[09/26 01:07:35 visual_prompt]: Best epoch 75: best metric: 0.590
[09/26 01:07:35 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 01:07:42 visual_prompt]: Epoch 76 / 100: avg data time: 5.19e-02, avg batch time: 0.5018, average train loss: 0.0606
[09/26 01:07:43 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 1.4390
[09/26 01:07:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 64.00	top5: 90.00	
[09/26 01:07:43 visual_prompt]: Best epoch 76: best metric: 0.640
[09/26 01:07:43 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 01:07:50 visual_prompt]: Epoch 77 / 100: avg data time: 4.45e-02, avg batch time: 0.4932, average train loss: 0.0418
[09/26 01:07:51 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 1.4304
[09/26 01:07:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 90.50	
[09/26 01:07:51 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 01:07:58 visual_prompt]: Epoch 78 / 100: avg data time: 5.33e-02, avg batch time: 0.5035, average train loss: 0.0352
[09/26 01:07:59 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1695, average loss: 1.3935
[09/26 01:07:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 64.50	top5: 91.00	
[09/26 01:07:59 visual_prompt]: Best epoch 78: best metric: 0.645
[09/26 01:07:59 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 01:08:06 visual_prompt]: Epoch 79 / 100: avg data time: 4.37e-02, avg batch time: 0.4941, average train loss: 0.0306
[09/26 01:08:08 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1697, average loss: 1.4294
[09/26 01:08:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.50	top5: 90.00	
[09/26 01:08:08 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 01:08:14 visual_prompt]: Epoch 80 / 100: avg data time: 4.46e-02, avg batch time: 0.4959, average train loss: 0.0295
[09/26 01:08:16 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1697, average loss: 1.4123
[09/26 01:08:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 89.50	
[09/26 01:08:16 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 01:08:22 visual_prompt]: Epoch 81 / 100: avg data time: 4.45e-02, avg batch time: 0.4937, average train loss: 0.0286
[09/26 01:08:24 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1694, average loss: 1.4321
[09/26 01:08:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 91.00	
[09/26 01:08:24 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 01:08:31 visual_prompt]: Epoch 82 / 100: avg data time: 5.29e-02, avg batch time: 0.5028, average train loss: 0.0283
[09/26 01:08:32 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1695, average loss: 1.4500
[09/26 01:08:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 64.00	top5: 90.00	
[09/26 01:08:32 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 01:08:39 visual_prompt]: Epoch 83 / 100: avg data time: 4.83e-02, avg batch time: 0.4991, average train loss: 0.0297
[09/26 01:08:40 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1691, average loss: 1.4981
[09/26 01:08:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 88.50	
[09/26 01:08:40 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 01:08:47 visual_prompt]: Epoch 84 / 100: avg data time: 4.21e-02, avg batch time: 0.4931, average train loss: 0.0345
[09/26 01:08:49 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1695, average loss: 1.5111
[09/26 01:08:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 90.50	
[09/26 01:08:49 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 01:08:55 visual_prompt]: Epoch 85 / 100: avg data time: 5.87e-02, avg batch time: 0.5077, average train loss: 0.0335
[09/26 01:08:57 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1696, average loss: 1.4617
[09/26 01:08:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 90.00	
[09/26 01:08:57 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 01:09:04 visual_prompt]: Epoch 86 / 100: avg data time: 5.50e-02, avg batch time: 0.5033, average train loss: 0.0268
[09/26 01:09:05 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1694, average loss: 1.4532
[09/26 01:09:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 64.00	top5: 90.50	
[09/26 01:09:05 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 01:09:12 visual_prompt]: Epoch 87 / 100: avg data time: 5.43e-02, avg batch time: 0.5036, average train loss: 0.0229
[09/26 01:09:14 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 1.5016
[09/26 01:09:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 90.50	
[09/26 01:09:14 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 01:09:20 visual_prompt]: Epoch 88 / 100: avg data time: 4.26e-02, avg batch time: 0.4917, average train loss: 0.0201
[09/26 01:09:22 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1695, average loss: 1.4736
[09/26 01:09:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 90.50	
[09/26 01:09:22 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 01:09:28 visual_prompt]: Epoch 89 / 100: avg data time: 4.97e-02, avg batch time: 0.4989, average train loss: 0.0183
[09/26 01:09:30 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1695, average loss: 1.5051
[09/26 01:09:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 91.50	
[09/26 01:09:30 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 01:09:37 visual_prompt]: Epoch 90 / 100: avg data time: 6.55e-02, avg batch time: 0.5139, average train loss: 0.0173
[09/26 01:09:38 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1694, average loss: 1.4990
[09/26 01:09:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 91.00	
[09/26 01:09:38 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 01:09:45 visual_prompt]: Epoch 91 / 100: avg data time: 4.69e-02, avg batch time: 0.4972, average train loss: 0.0169
[09/26 01:09:46 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1694, average loss: 1.5129
[09/26 01:09:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 91.50	
[09/26 01:09:46 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 01:09:53 visual_prompt]: Epoch 92 / 100: avg data time: 4.35e-02, avg batch time: 0.4941, average train loss: 0.0164
[09/26 01:09:54 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1692, average loss: 1.5037
[09/26 01:09:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 91.50	
[09/26 01:09:54 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 01:10:01 visual_prompt]: Epoch 93 / 100: avg data time: 5.04e-02, avg batch time: 0.4998, average train loss: 0.0162
[09/26 01:10:03 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1696, average loss: 1.5162
[09/26 01:10:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 91.50	
[09/26 01:10:03 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 01:10:09 visual_prompt]: Epoch 94 / 100: avg data time: 4.52e-02, avg batch time: 0.4956, average train loss: 0.0159
[09/26 01:10:11 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1690, average loss: 1.5110
[09/26 01:10:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 91.00	
[09/26 01:10:11 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 01:10:18 visual_prompt]: Epoch 95 / 100: avg data time: 5.00e-02, avg batch time: 0.4989, average train loss: 0.0157
[09/26 01:10:19 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1693, average loss: 1.5129
[09/26 01:10:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 91.50	
[09/26 01:10:19 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 01:10:26 visual_prompt]: Epoch 96 / 100: avg data time: 5.09e-02, avg batch time: 0.5001, average train loss: 0.0154
[09/26 01:10:28 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 1.5129
[09/26 01:10:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 90.50	
[09/26 01:10:28 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 01:10:34 visual_prompt]: Epoch 97 / 100: avg data time: 5.38e-02, avg batch time: 0.5029, average train loss: 0.0155
[09/26 01:10:36 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1694, average loss: 1.5104
[09/26 01:10:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 91.50	
[09/26 01:10:36 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 01:10:43 visual_prompt]: Epoch 98 / 100: avg data time: 5.23e-02, avg batch time: 0.5009, average train loss: 0.0153
[09/26 01:10:44 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1696, average loss: 1.5090
[09/26 01:10:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 91.50	
[09/26 01:10:44 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 01:10:51 visual_prompt]: Epoch 99 / 100: avg data time: 4.41e-02, avg batch time: 0.4934, average train loss: 0.0154
[09/26 01:10:52 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1691, average loss: 1.5092
[09/26 01:10:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 91.50	
[09/26 01:10:52 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 01:10:59 visual_prompt]: Epoch 100 / 100: avg data time: 4.40e-02, avg batch time: 0.4948, average train loss: 0.0154
[09/26 01:11:01 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1695, average loss: 1.5096
[09/26 01:11:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 91.50	
[09/26 01:11:01 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 01:11:01 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 01:11:01 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 01:11:01 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 01:11:01 visual_prompt]: Training with config:
[09/26 01:11:01 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 01:11:01 visual_prompt]: Loading training data...
[09/26 01:11:01 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 01:11:03 visual_prompt]: Number of images: 800
[09/26 01:11:03 visual_prompt]: Number of classes: 47 / 47
[09/26 01:11:03 visual_prompt]: Loading validation data...
[09/26 01:11:03 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 01:11:03 visual_prompt]: Number of images: 200
[09/26 01:11:03 visual_prompt]: Number of classes: 47 / 47
[09/26 01:11:03 visual_prompt]: Constructing models...
[09/26 01:11:06 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 01:11:06 visual_prompt]: tuned percent:0.576
[09/26 01:11:06 visual_prompt]: Device used for model: 0
[09/26 01:11:06 visual_prompt]: Setting up Evaluator...
[09/26 01:11:06 visual_prompt]: Setting up Trainer...
[09/26 01:11:06 visual_prompt]: 	Setting up the optimizer...
[09/26 01:11:06 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 01:11:13 visual_prompt]: Epoch 1 / 100: avg data time: 5.84e-02, avg batch time: 0.5053, average train loss: 3.9282
[09/26 01:11:14 visual_prompt]: Inference (val):avg data time: 6.04e-05, avg batch time: 0.1695, average loss: 3.9045
[09/26 01:11:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 01:11:14 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 01:11:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 01:11:21 visual_prompt]: Epoch 2 / 100: avg data time: 5.63e-02, avg batch time: 0.5034, average train loss: 3.9188
[09/26 01:11:23 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1689, average loss: 3.9838
[09/26 01:11:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/26 01:11:23 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 01:11:30 visual_prompt]: Epoch 3 / 100: avg data time: 6.04e-02, avg batch time: 0.5072, average train loss: 3.9176
[09/26 01:11:31 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1690, average loss: 4.0285
[09/26 01:11:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/26 01:11:31 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 01:11:38 visual_prompt]: Epoch 4 / 100: avg data time: 5.94e-02, avg batch time: 0.5075, average train loss: 4.0499
[09/26 01:11:39 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1694, average loss: 4.1937
[09/26 01:11:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 9.50	
[09/26 01:11:39 visual_prompt]: Best epoch 4: best metric: 0.050
[09/26 01:11:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 01:11:46 visual_prompt]: Epoch 5 / 100: avg data time: 6.09e-02, avg batch time: 0.5081, average train loss: 3.9662
[09/26 01:11:48 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1694, average loss: 4.0222
[09/26 01:11:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 12.00	
[09/26 01:11:48 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 01:11:55 visual_prompt]: Epoch 6 / 100: avg data time: 5.84e-02, avg batch time: 0.5071, average train loss: 3.9039
[09/26 01:11:56 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1693, average loss: 3.8889
[09/26 01:11:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 25.50	
[09/26 01:11:56 visual_prompt]: Best epoch 6: best metric: 0.095
[09/26 01:11:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 01:12:03 visual_prompt]: Epoch 7 / 100: avg data time: 4.95e-02, avg batch time: 0.4987, average train loss: 3.2660
[09/26 01:12:04 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1697, average loss: 3.3357
[09/26 01:12:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 17.00	top5: 47.00	
[09/26 01:12:04 visual_prompt]: Best epoch 7: best metric: 0.170
[09/26 01:12:04 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 01:12:11 visual_prompt]: Epoch 8 / 100: avg data time: 5.29e-02, avg batch time: 0.5007, average train loss: 2.1931
[09/26 01:12:12 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 2.3227
[09/26 01:12:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.00	top5: 72.50	
[09/26 01:12:12 visual_prompt]: Best epoch 8: best metric: 0.380
[09/26 01:12:12 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 01:12:19 visual_prompt]: Epoch 9 / 100: avg data time: 4.44e-02, avg batch time: 0.4932, average train loss: 1.3545
[09/26 01:12:21 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1696, average loss: 1.9117
[09/26 01:12:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 85.00	
[09/26 01:12:21 visual_prompt]: Best epoch 9: best metric: 0.500
[09/26 01:12:21 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 01:12:27 visual_prompt]: Epoch 10 / 100: avg data time: 5.93e-02, avg batch time: 0.5069, average train loss: 0.6676
[09/26 01:12:29 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1687, average loss: 2.4827
[09/26 01:12:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 81.00	
[09/26 01:12:29 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 01:12:36 visual_prompt]: Epoch 11 / 100: avg data time: 5.55e-02, avg batch time: 0.5048, average train loss: 0.4775
[09/26 01:12:37 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1685, average loss: 2.2196
[09/26 01:12:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 80.00	
[09/26 01:12:37 visual_prompt]: Best epoch 11: best metric: 0.530
[09/26 01:12:37 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 01:12:44 visual_prompt]: Epoch 12 / 100: avg data time: 5.86e-02, avg batch time: 0.5075, average train loss: 0.2977
[09/26 01:12:46 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1695, average loss: 2.7865
[09/26 01:12:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 79.50	
[09/26 01:12:46 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 01:12:53 visual_prompt]: Epoch 13 / 100: avg data time: 5.62e-02, avg batch time: 0.5050, average train loss: 0.2204
[09/26 01:12:54 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1692, average loss: 3.5204
[09/26 01:12:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 75.50	
[09/26 01:12:54 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 01:13:01 visual_prompt]: Epoch 14 / 100: avg data time: 4.20e-02, avg batch time: 0.4908, average train loss: 0.2115
[09/26 01:13:02 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1697, average loss: 3.1983
[09/26 01:13:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 78.50	
[09/26 01:13:02 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 01:13:09 visual_prompt]: Epoch 15 / 100: avg data time: 5.60e-02, avg batch time: 0.5048, average train loss: 0.2218
[09/26 01:13:10 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1686, average loss: 2.7634
[09/26 01:13:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 83.00	
[09/26 01:13:10 visual_prompt]: Best epoch 15: best metric: 0.545
[09/26 01:13:10 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 01:13:17 visual_prompt]: Epoch 16 / 100: avg data time: 4.77e-02, avg batch time: 0.4970, average train loss: 0.1138
[09/26 01:13:19 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 2.4895
[09/26 01:13:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 82.00	
[09/26 01:13:19 visual_prompt]: Best epoch 16: best metric: 0.570
[09/26 01:13:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 01:13:26 visual_prompt]: Epoch 17 / 100: avg data time: 6.06e-02, avg batch time: 0.5089, average train loss: 0.0817
[09/26 01:13:27 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1694, average loss: 2.6729
[09/26 01:13:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 81.00	
[09/26 01:13:27 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 01:13:34 visual_prompt]: Epoch 18 / 100: avg data time: 5.59e-02, avg batch time: 0.5052, average train loss: 0.0637
[09/26 01:13:35 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1691, average loss: 2.6955
[09/26 01:13:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 81.00	
[09/26 01:13:35 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 01:13:42 visual_prompt]: Epoch 19 / 100: avg data time: 6.58e-02, avg batch time: 0.5137, average train loss: 0.0703
[09/26 01:13:44 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1694, average loss: 2.6586
[09/26 01:13:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 80.00	
[09/26 01:13:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 01:13:50 visual_prompt]: Epoch 20 / 100: avg data time: 4.92e-02, avg batch time: 0.4981, average train loss: 0.0641
[09/26 01:13:52 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1696, average loss: 2.5900
[09/26 01:13:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 82.00	
[09/26 01:13:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 01:13:59 visual_prompt]: Epoch 21 / 100: avg data time: 6.00e-02, avg batch time: 0.5088, average train loss: 0.0375
[09/26 01:14:00 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 2.4318
[09/26 01:14:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 81.50	
[09/26 01:14:00 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 01:14:07 visual_prompt]: Epoch 22 / 100: avg data time: 4.98e-02, avg batch time: 0.4982, average train loss: 0.0770
[09/26 01:14:09 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 2.4294
[09/26 01:14:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 83.50	
[09/26 01:14:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 01:14:15 visual_prompt]: Epoch 23 / 100: avg data time: 5.01e-02, avg batch time: 0.4997, average train loss: 0.0197
[09/26 01:14:17 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1689, average loss: 2.2736
[09/26 01:14:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 84.50	
[09/26 01:14:17 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 01:14:24 visual_prompt]: Epoch 24 / 100: avg data time: 5.43e-02, avg batch time: 0.5025, average train loss: 0.0228
[09/26 01:14:25 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1696, average loss: 2.3135
[09/26 01:14:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 82.00	
[09/26 01:14:25 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 01:14:32 visual_prompt]: Epoch 25 / 100: avg data time: 4.35e-02, avg batch time: 0.4925, average train loss: 0.0509
[09/26 01:14:33 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1690, average loss: 2.1963
[09/26 01:14:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 01:14:33 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 01:14:40 visual_prompt]: Epoch 26 / 100: avg data time: 4.36e-02, avg batch time: 0.4944, average train loss: 0.0451
[09/26 01:14:41 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 2.0128
[09/26 01:14:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 82.00	
[09/26 01:14:41 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 01:14:48 visual_prompt]: Epoch 27 / 100: avg data time: 4.02e-02, avg batch time: 0.4913, average train loss: 0.0534
[09/26 01:14:50 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 2.1841
[09/26 01:14:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.50	
[09/26 01:14:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 01:14:56 visual_prompt]: Epoch 28 / 100: avg data time: 4.01e-02, avg batch time: 0.4900, average train loss: 0.0833
[09/26 01:14:58 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1696, average loss: 2.1470
[09/26 01:14:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 82.50	
[09/26 01:14:58 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 01:15:05 visual_prompt]: Epoch 29 / 100: avg data time: 5.05e-02, avg batch time: 0.4992, average train loss: 0.1077
[09/26 01:15:06 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 2.3053
[09/26 01:15:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 81.00	
[09/26 01:15:06 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 01:15:13 visual_prompt]: Epoch 30 / 100: avg data time: 6.78e-02, avg batch time: 0.5162, average train loss: 0.1014
[09/26 01:15:14 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 2.4196
[09/26 01:15:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.50	
[09/26 01:15:14 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 01:15:21 visual_prompt]: Epoch 31 / 100: avg data time: 5.80e-02, avg batch time: 0.5083, average train loss: 0.1024
[09/26 01:15:23 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1699, average loss: 2.3174
[09/26 01:15:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 82.50	
[09/26 01:15:23 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 01:15:30 visual_prompt]: Epoch 32 / 100: avg data time: 4.55e-02, avg batch time: 0.4941, average train loss: 0.0558
[09/26 01:15:31 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 2.1617
[09/26 01:15:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 83.50	
[09/26 01:15:31 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 01:15:38 visual_prompt]: Epoch 33 / 100: avg data time: 5.00e-02, avg batch time: 0.4993, average train loss: 0.0603
[09/26 01:15:39 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 2.1338
[09/26 01:15:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 85.00	
[09/26 01:15:39 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 01:15:46 visual_prompt]: Epoch 34 / 100: avg data time: 5.00e-02, avg batch time: 0.4992, average train loss: 0.0317
[09/26 01:15:48 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1693, average loss: 2.2474
[09/26 01:15:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.00	
[09/26 01:15:48 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 01:15:55 visual_prompt]: Epoch 35 / 100: avg data time: 5.97e-02, avg batch time: 0.5081, average train loss: 0.0232
[09/26 01:15:56 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 2.1510
[09/26 01:15:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.00	
[09/26 01:15:56 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 01:16:03 visual_prompt]: Epoch 36 / 100: avg data time: 5.39e-02, avg batch time: 0.5022, average train loss: 0.0168
[09/26 01:16:04 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1696, average loss: 2.0758
[09/26 01:16:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 82.50	
[09/26 01:16:04 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 01:16:11 visual_prompt]: Epoch 37 / 100: avg data time: 4.33e-02, avg batch time: 0.4941, average train loss: 0.0202
[09/26 01:16:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1700, average loss: 2.0304
[09/26 01:16:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 83.50	
[09/26 01:16:13 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 01:16:19 visual_prompt]: Epoch 38 / 100: avg data time: 5.15e-02, avg batch time: 0.4998, average train loss: 0.0204
[09/26 01:16:21 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1694, average loss: 1.8147
[09/26 01:16:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 87.00	
[09/26 01:16:21 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 01:16:27 visual_prompt]: Epoch 39 / 100: avg data time: 4.05e-02, avg batch time: 0.4930, average train loss: 0.0700
[09/26 01:16:29 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 1.9521
[09/26 01:16:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.00	
[09/26 01:16:29 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 01:16:36 visual_prompt]: Epoch 40 / 100: avg data time: 5.13e-02, avg batch time: 0.5006, average train loss: 0.0302
[09/26 01:16:37 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1696, average loss: 2.0044
[09/26 01:16:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 84.00	
[09/26 01:16:37 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 01:16:44 visual_prompt]: Epoch 41 / 100: avg data time: 4.75e-02, avg batch time: 0.4970, average train loss: 0.0287
[09/26 01:16:45 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1691, average loss: 1.7145
[09/26 01:16:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 88.00	
[09/26 01:16:45 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 01:16:52 visual_prompt]: Epoch 42 / 100: avg data time: 4.96e-02, avg batch time: 0.4978, average train loss: 0.0131
[09/26 01:16:54 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1697, average loss: 1.7374
[09/26 01:16:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 88.00	
[09/26 01:16:54 visual_prompt]: Best epoch 42: best metric: 0.590
[09/26 01:16:54 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 01:17:00 visual_prompt]: Epoch 43 / 100: avg data time: 4.43e-02, avg batch time: 0.4927, average train loss: 0.0188
[09/26 01:17:02 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1695, average loss: 1.6973
[09/26 01:17:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 87.00	
[09/26 01:17:02 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 01:17:09 visual_prompt]: Epoch 44 / 100: avg data time: 5.25e-02, avg batch time: 0.5014, average train loss: 0.0164
[09/26 01:17:10 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1697, average loss: 1.8228
[09/26 01:17:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 87.00	
[09/26 01:17:10 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 01:17:17 visual_prompt]: Epoch 45 / 100: avg data time: 5.52e-02, avg batch time: 0.5046, average train loss: 0.0080
[09/26 01:17:19 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 1.6881
[09/26 01:17:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 88.00	
[09/26 01:17:19 visual_prompt]: Best epoch 45: best metric: 0.605
[09/26 01:17:19 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 01:17:25 visual_prompt]: Epoch 46 / 100: avg data time: 5.25e-02, avg batch time: 0.5014, average train loss: 0.0059
[09/26 01:17:27 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1694, average loss: 1.6481
[09/26 01:17:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 88.00	
[09/26 01:17:27 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 01:17:34 visual_prompt]: Epoch 47 / 100: avg data time: 5.62e-02, avg batch time: 0.5045, average train loss: 0.0054
[09/26 01:17:35 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1692, average loss: 1.6237
[09/26 01:17:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 88.50	
[09/26 01:17:35 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 01:17:42 visual_prompt]: Epoch 48 / 100: avg data time: 3.97e-02, avg batch time: 0.4901, average train loss: 0.0054
[09/26 01:17:43 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 1.6094
[09/26 01:17:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 88.50	
[09/26 01:17:43 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 01:17:50 visual_prompt]: Epoch 49 / 100: avg data time: 5.32e-02, avg batch time: 0.5021, average train loss: 0.0055
[09/26 01:17:52 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.1694, average loss: 1.5876
[09/26 01:17:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 88.50	
[09/26 01:17:52 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 01:17:59 visual_prompt]: Epoch 50 / 100: avg data time: 5.68e-02, avg batch time: 0.5050, average train loss: 0.0057
[09/26 01:18:00 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1687, average loss: 1.5688
[09/26 01:18:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 88.00	
[09/26 01:18:00 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 01:18:07 visual_prompt]: Epoch 51 / 100: avg data time: 6.10e-02, avg batch time: 0.5096, average train loss: 0.0059
[09/26 01:18:09 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1696, average loss: 1.5566
[09/26 01:18:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 88.00	
[09/26 01:18:09 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 01:18:16 visual_prompt]: Epoch 52 / 100: avg data time: 5.85e-02, avg batch time: 0.5081, average train loss: 0.0060
[09/26 01:18:17 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1688, average loss: 1.5447
[09/26 01:18:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 88.50	
[09/26 01:18:17 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 01:18:24 visual_prompt]: Epoch 53 / 100: avg data time: 5.66e-02, avg batch time: 0.5052, average train loss: 0.0060
[09/26 01:18:25 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1696, average loss: 1.5340
[09/26 01:18:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 89.00	
[09/26 01:18:25 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 01:18:32 visual_prompt]: Epoch 54 / 100: avg data time: 5.41e-02, avg batch time: 0.5023, average train loss: 0.0059
[09/26 01:18:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1696, average loss: 1.5185
[09/26 01:18:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 88.50	
[09/26 01:18:34 visual_prompt]: Best epoch 54: best metric: 0.610
[09/26 01:18:34 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 01:18:40 visual_prompt]: Epoch 55 / 100: avg data time: 5.73e-02, avg batch time: 0.5056, average train loss: 0.0060
[09/26 01:18:42 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 1.5276
[09/26 01:18:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 89.00	
[09/26 01:18:42 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 01:18:49 visual_prompt]: Epoch 56 / 100: avg data time: 4.35e-02, avg batch time: 0.4926, average train loss: 0.0060
[09/26 01:18:50 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 1.5277
[09/26 01:18:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 89.50	
[09/26 01:18:50 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 01:18:57 visual_prompt]: Epoch 57 / 100: avg data time: 5.58e-02, avg batch time: 0.5044, average train loss: 0.0058
[09/26 01:18:58 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 1.5294
[09/26 01:18:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 89.00	
[09/26 01:18:58 visual_prompt]: Best epoch 57: best metric: 0.615
[09/26 01:18:58 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 01:19:05 visual_prompt]: Epoch 58 / 100: avg data time: 5.36e-02, avg batch time: 0.5016, average train loss: 0.0059
[09/26 01:19:07 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1692, average loss: 1.5038
[09/26 01:19:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 88.00	
[09/26 01:19:07 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 01:19:13 visual_prompt]: Epoch 59 / 100: avg data time: 5.53e-02, avg batch time: 0.5042, average train loss: 0.0057
[09/26 01:19:15 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 1.5052
[09/26 01:19:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 89.00	
[09/26 01:19:15 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 01:19:22 visual_prompt]: Epoch 60 / 100: avg data time: 4.71e-02, avg batch time: 0.4973, average train loss: 0.0055
[09/26 01:19:23 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1697, average loss: 1.5100
[09/26 01:19:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 88.00	
[09/26 01:19:23 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 01:19:30 visual_prompt]: Epoch 61 / 100: avg data time: 4.65e-02, avg batch time: 0.4947, average train loss: 0.0055
[09/26 01:19:31 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 1.5066
[09/26 01:19:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 89.00	
[09/26 01:19:31 visual_prompt]: Best epoch 61: best metric: 0.625
[09/26 01:19:31 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 01:19:38 visual_prompt]: Epoch 62 / 100: avg data time: 5.45e-02, avg batch time: 0.5039, average train loss: 0.0052
[09/26 01:19:40 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 1.5047
[09/26 01:19:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 88.00	
[09/26 01:19:40 visual_prompt]: Best epoch 62: best metric: 0.630
[09/26 01:19:40 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 01:19:46 visual_prompt]: Epoch 63 / 100: avg data time: 5.99e-02, avg batch time: 0.5083, average train loss: 0.0051
[09/26 01:19:48 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1690, average loss: 1.5080
[09/26 01:19:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 87.00	
[09/26 01:19:48 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 01:19:55 visual_prompt]: Epoch 64 / 100: avg data time: 6.00e-02, avg batch time: 0.5094, average train loss: 0.0050
[09/26 01:19:56 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1693, average loss: 1.5092
[09/26 01:19:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 89.00	
[09/26 01:19:56 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 01:20:03 visual_prompt]: Epoch 65 / 100: avg data time: 5.87e-02, avg batch time: 0.5068, average train loss: 0.0050
[09/26 01:20:05 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 1.5152
[09/26 01:20:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 88.50	
[09/26 01:20:05 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 01:20:11 visual_prompt]: Epoch 66 / 100: avg data time: 4.84e-02, avg batch time: 0.4989, average train loss: 0.0048
[09/26 01:20:13 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1695, average loss: 1.5234
[09/26 01:20:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 88.50	
[09/26 01:20:13 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 01:20:20 visual_prompt]: Epoch 67 / 100: avg data time: 6.61e-02, avg batch time: 0.5143, average train loss: 0.0047
[09/26 01:20:22 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 1.5355
[09/26 01:20:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 88.00	
[09/26 01:20:22 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 01:20:28 visual_prompt]: Epoch 68 / 100: avg data time: 5.34e-02, avg batch time: 0.5027, average train loss: 0.0046
[09/26 01:20:30 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1695, average loss: 1.5280
[09/26 01:20:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 88.50	
[09/26 01:20:30 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 01:20:37 visual_prompt]: Epoch 69 / 100: avg data time: 5.89e-02, avg batch time: 0.5098, average train loss: 0.0045
[09/26 01:20:38 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 1.5422
[09/26 01:20:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 87.50	
[09/26 01:20:38 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 01:20:45 visual_prompt]: Epoch 70 / 100: avg data time: 6.17e-02, avg batch time: 0.5105, average train loss: 0.0045
[09/26 01:20:47 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1692, average loss: 1.5297
[09/26 01:20:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 89.00	
[09/26 01:20:47 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 01:20:54 visual_prompt]: Epoch 71 / 100: avg data time: 5.65e-02, avg batch time: 0.5046, average train loss: 0.0045
[09/26 01:20:55 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1696, average loss: 1.5507
[09/26 01:20:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 88.50	
[09/26 01:20:55 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 01:21:02 visual_prompt]: Epoch 72 / 100: avg data time: 4.89e-02, avg batch time: 0.4970, average train loss: 0.0043
[09/26 01:21:03 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1693, average loss: 1.5638
[09/26 01:21:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 88.50	
[09/26 01:21:03 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 01:21:10 visual_prompt]: Epoch 73 / 100: avg data time: 6.86e-02, avg batch time: 0.5165, average train loss: 0.0042
[09/26 01:21:12 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1689, average loss: 1.5535
[09/26 01:21:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 89.00	
[09/26 01:21:12 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 01:21:19 visual_prompt]: Epoch 74 / 100: avg data time: 6.63e-02, avg batch time: 0.5142, average train loss: 0.0041
[09/26 01:21:20 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1693, average loss: 1.5544
[09/26 01:21:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 89.00	
[09/26 01:21:20 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 01:21:27 visual_prompt]: Epoch 75 / 100: avg data time: 5.97e-02, avg batch time: 0.5077, average train loss: 0.0040
[09/26 01:21:29 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 1.5415
[09/26 01:21:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 89.50	
[09/26 01:21:29 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 01:21:36 visual_prompt]: Epoch 76 / 100: avg data time: 6.41e-02, avg batch time: 0.5129, average train loss: 0.0039
[09/26 01:21:37 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1695, average loss: 1.5577
[09/26 01:21:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 89.00	
[09/26 01:21:37 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 01:21:44 visual_prompt]: Epoch 77 / 100: avg data time: 6.58e-02, avg batch time: 0.5137, average train loss: 0.0038
[09/26 01:21:46 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 1.5531
[09/26 01:21:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 89.00	
[09/26 01:21:46 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 01:21:52 visual_prompt]: Epoch 78 / 100: avg data time: 5.84e-02, avg batch time: 0.5064, average train loss: 0.0039
[09/26 01:21:54 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1695, average loss: 1.5705
[09/26 01:21:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 90.00	
[09/26 01:21:54 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 01:22:01 visual_prompt]: Epoch 79 / 100: avg data time: 5.83e-02, avg batch time: 0.5070, average train loss: 0.0038
[09/26 01:22:02 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1696, average loss: 1.5584
[09/26 01:22:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 89.50	
[09/26 01:22:02 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 01:22:09 visual_prompt]: Epoch 80 / 100: avg data time: 6.27e-02, avg batch time: 0.5126, average train loss: 0.0037
[09/26 01:22:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1692, average loss: 1.5494
[09/26 01:22:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 89.00	
[09/26 01:22:11 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 01:22:17 visual_prompt]: Epoch 81 / 100: avg data time: 4.78e-02, avg batch time: 0.4981, average train loss: 0.0036
[09/26 01:22:19 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1693, average loss: 1.5489
[09/26 01:22:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 90.00	
[09/26 01:22:19 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 01:22:26 visual_prompt]: Epoch 82 / 100: avg data time: 4.85e-02, avg batch time: 0.4974, average train loss: 0.0037
[09/26 01:22:27 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1690, average loss: 1.5520
[09/26 01:22:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 90.00	
[09/26 01:22:27 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 01:22:34 visual_prompt]: Epoch 83 / 100: avg data time: 5.25e-02, avg batch time: 0.5026, average train loss: 0.0035
[09/26 01:22:35 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 1.5626
[09/26 01:22:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 89.00	
[09/26 01:22:35 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 01:22:42 visual_prompt]: Epoch 84 / 100: avg data time: 6.03e-02, avg batch time: 0.5088, average train loss: 0.0035
[09/26 01:22:44 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1697, average loss: 1.5601
[09/26 01:22:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 90.00	
[09/26 01:22:44 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 01:22:51 visual_prompt]: Epoch 85 / 100: avg data time: 5.42e-02, avg batch time: 0.5024, average train loss: 0.0035
[09/26 01:22:52 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1699, average loss: 1.5611
[09/26 01:22:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 90.00	
[09/26 01:22:52 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 01:22:59 visual_prompt]: Epoch 86 / 100: avg data time: 5.69e-02, avg batch time: 0.5051, average train loss: 0.0034
[09/26 01:23:00 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1688, average loss: 1.5713
[09/26 01:23:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 90.00	
[09/26 01:23:00 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 01:23:07 visual_prompt]: Epoch 87 / 100: avg data time: 6.35e-02, avg batch time: 0.5122, average train loss: 0.0035
[09/26 01:23:09 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.1695, average loss: 1.5705
[09/26 01:23:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 89.50	
[09/26 01:23:09 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 01:23:16 visual_prompt]: Epoch 88 / 100: avg data time: 6.24e-02, avg batch time: 0.5105, average train loss: 0.0034
[09/26 01:23:17 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1694, average loss: 1.5717
[09/26 01:23:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 89.00	
[09/26 01:23:17 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 01:23:24 visual_prompt]: Epoch 89 / 100: avg data time: 6.00e-02, avg batch time: 0.5089, average train loss: 0.0034
[09/26 01:23:25 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 1.5665
[09/26 01:23:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 90.00	
[09/26 01:23:25 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 01:23:32 visual_prompt]: Epoch 90 / 100: avg data time: 6.05e-02, avg batch time: 0.5095, average train loss: 0.0033
[09/26 01:23:34 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 1.5664
[09/26 01:23:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 89.50	
[09/26 01:23:34 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 01:23:41 visual_prompt]: Epoch 91 / 100: avg data time: 6.54e-02, avg batch time: 0.5136, average train loss: 0.0033
[09/26 01:23:42 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1696, average loss: 1.5705
[09/26 01:23:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 89.00	
[09/26 01:23:42 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 01:23:49 visual_prompt]: Epoch 92 / 100: avg data time: 5.52e-02, avg batch time: 0.5223, average train loss: 0.0033
[09/26 01:23:51 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1694, average loss: 1.5730
[09/26 01:23:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 89.00	
[09/26 01:23:51 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 01:23:58 visual_prompt]: Epoch 93 / 100: avg data time: 6.52e-02, avg batch time: 0.5138, average train loss: 0.0033
[09/26 01:23:59 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 1.5745
[09/26 01:23:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 89.00	
[09/26 01:23:59 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 01:24:06 visual_prompt]: Epoch 94 / 100: avg data time: 5.08e-02, avg batch time: 0.4992, average train loss: 0.0033
[09/26 01:24:08 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 1.5727
[09/26 01:24:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 89.00	
[09/26 01:24:08 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 01:24:14 visual_prompt]: Epoch 95 / 100: avg data time: 4.55e-02, avg batch time: 0.4941, average train loss: 0.0033
[09/26 01:24:16 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 1.5746
[09/26 01:24:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 89.00	
[09/26 01:24:16 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 01:24:22 visual_prompt]: Epoch 96 / 100: avg data time: 4.21e-02, avg batch time: 0.4927, average train loss: 0.0033
[09/26 01:24:24 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1691, average loss: 1.5747
[09/26 01:24:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 89.00	
[09/26 01:24:24 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 01:24:31 visual_prompt]: Epoch 97 / 100: avg data time: 5.25e-02, avg batch time: 0.5007, average train loss: 0.0033
[09/26 01:24:32 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 1.5748
[09/26 01:24:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 89.00	
[09/26 01:24:32 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 01:24:39 visual_prompt]: Epoch 98 / 100: avg data time: 5.21e-02, avg batch time: 0.5014, average train loss: 0.0032
[09/26 01:24:40 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1691, average loss: 1.5741
[09/26 01:24:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 89.00	
[09/26 01:24:40 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 01:24:47 visual_prompt]: Epoch 99 / 100: avg data time: 4.18e-02, avg batch time: 0.4909, average train loss: 0.0032
[09/26 01:24:48 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 1.5741
[09/26 01:24:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 89.00	
[09/26 01:24:48 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 01:24:55 visual_prompt]: Epoch 100 / 100: avg data time: 4.98e-02, avg batch time: 0.4985, average train loss: 0.0033
[09/26 01:24:56 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1696, average loss: 1.5740
[09/26 01:24:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 89.00	
[09/26 01:24:57 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 01:24:57 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 01:24:57 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 01:24:57 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 01:24:57 visual_prompt]: Training with config:
[09/26 01:24:57 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 01:24:57 visual_prompt]: Loading training data...
[09/26 01:24:57 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 01:24:59 visual_prompt]: Number of images: 800
[09/26 01:24:59 visual_prompt]: Number of classes: 47 / 47
[09/26 01:24:59 visual_prompt]: Loading validation data...
[09/26 01:24:59 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 01:24:59 visual_prompt]: Number of images: 200
[09/26 01:24:59 visual_prompt]: Number of classes: 47 / 47
[09/26 01:24:59 visual_prompt]: Constructing models...
[09/26 01:25:02 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 01:25:02 visual_prompt]: tuned percent:0.576
[09/26 01:25:02 visual_prompt]: Device used for model: 0
[09/26 01:25:02 visual_prompt]: Setting up Evaluator...
[09/26 01:25:02 visual_prompt]: Setting up Trainer...
[09/26 01:25:02 visual_prompt]: 	Setting up the optimizer...
[09/26 01:25:02 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 01:25:09 visual_prompt]: Epoch 1 / 100: avg data time: 5.31e-02, avg batch time: 0.5000, average train loss: 3.9370
[09/26 01:25:10 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1685, average loss: 3.9045
[09/26 01:25:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 01:25:10 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 01:25:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 01:25:17 visual_prompt]: Epoch 2 / 100: avg data time: 5.92e-02, avg batch time: 0.5065, average train loss: 3.9424
[09/26 01:25:18 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1689, average loss: 3.9536
[09/26 01:25:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 11.00	
[09/26 01:25:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 01:25:25 visual_prompt]: Epoch 3 / 100: avg data time: 6.07e-02, avg batch time: 0.5087, average train loss: 3.8984
[09/26 01:25:27 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1689, average loss: 3.9237
[09/26 01:25:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 19.00	
[09/26 01:25:27 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 01:25:34 visual_prompt]: Epoch 4 / 100: avg data time: 6.21e-02, avg batch time: 0.5091, average train loss: 3.8486
[09/26 01:25:35 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1693, average loss: 4.0876
[09/26 01:25:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 14.00	
[09/26 01:25:35 visual_prompt]: Best epoch 4: best metric: 0.045
[09/26 01:25:35 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 01:25:42 visual_prompt]: Epoch 5 / 100: avg data time: 5.74e-02, avg batch time: 0.5045, average train loss: 3.8439
[09/26 01:25:44 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1690, average loss: 3.6905
[09/26 01:25:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 23.50	
[09/26 01:25:44 visual_prompt]: Best epoch 5: best metric: 0.050
[09/26 01:25:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 01:25:50 visual_prompt]: Epoch 6 / 100: avg data time: 5.01e-02, avg batch time: 0.4983, average train loss: 3.5234
[09/26 01:25:52 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1690, average loss: 4.2795
[09/26 01:25:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.50	top5: 28.00	
[09/26 01:25:52 visual_prompt]: Best epoch 6: best metric: 0.085
[09/26 01:25:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 01:25:59 visual_prompt]: Epoch 7 / 100: avg data time: 5.20e-02, avg batch time: 0.4989, average train loss: 3.5247
[09/26 01:26:00 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1690, average loss: 3.3128
[09/26 01:26:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.50	top5: 41.50	
[09/26 01:26:00 visual_prompt]: Best epoch 7: best metric: 0.115
[09/26 01:26:00 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 01:26:07 visual_prompt]: Epoch 8 / 100: avg data time: 6.41e-02, avg batch time: 0.5114, average train loss: 2.9703
[09/26 01:26:09 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1690, average loss: 3.0746
[09/26 01:26:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.00	top5: 50.50	
[09/26 01:26:09 visual_prompt]: Best epoch 8: best metric: 0.250
[09/26 01:26:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 01:26:16 visual_prompt]: Epoch 9 / 100: avg data time: 4.93e-02, avg batch time: 0.4991, average train loss: 2.3021
[09/26 01:26:17 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1692, average loss: 2.7421
[09/26 01:26:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 29.00	top5: 63.00	
[09/26 01:26:17 visual_prompt]: Best epoch 9: best metric: 0.290
[09/26 01:26:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 01:26:24 visual_prompt]: Epoch 10 / 100: avg data time: 5.10e-02, avg batch time: 0.4990, average train loss: 1.7167
[09/26 01:26:25 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 3.5127
[09/26 01:26:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 28.00	top5: 68.50	
[09/26 01:26:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 01:26:32 visual_prompt]: Epoch 11 / 100: avg data time: 4.82e-02, avg batch time: 0.4960, average train loss: 1.3655
[09/26 01:26:33 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1693, average loss: 2.7128
[09/26 01:26:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 79.00	
[09/26 01:26:33 visual_prompt]: Best epoch 11: best metric: 0.465
[09/26 01:26:33 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 01:26:40 visual_prompt]: Epoch 12 / 100: avg data time: 5.25e-02, avg batch time: 0.5000, average train loss: 0.9760
[09/26 01:26:42 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1696, average loss: 3.2827
[09/26 01:26:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 81.00	
[09/26 01:26:42 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 01:26:49 visual_prompt]: Epoch 13 / 100: avg data time: 4.85e-02, avg batch time: 0.4970, average train loss: 0.5799
[09/26 01:26:50 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1694, average loss: 3.3076
[09/26 01:26:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 73.50	
[09/26 01:26:50 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 01:26:57 visual_prompt]: Epoch 14 / 100: avg data time: 5.58e-02, avg batch time: 0.5043, average train loss: 0.4384
[09/26 01:26:58 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1695, average loss: 3.7625
[09/26 01:26:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 80.50	
[09/26 01:26:58 visual_prompt]: Best epoch 14: best metric: 0.470
[09/26 01:26:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 01:27:05 visual_prompt]: Epoch 15 / 100: avg data time: 4.41e-02, avg batch time: 0.4930, average train loss: 0.3772
[09/26 01:27:07 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1695, average loss: 3.4306
[09/26 01:27:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 79.50	
[09/26 01:27:07 visual_prompt]: Best epoch 15: best metric: 0.510
[09/26 01:27:07 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 01:27:13 visual_prompt]: Epoch 16 / 100: avg data time: 5.66e-02, avg batch time: 0.5049, average train loss: 0.1835
[09/26 01:27:15 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1689, average loss: 4.0110
[09/26 01:27:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 81.00	
[09/26 01:27:15 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 01:27:22 visual_prompt]: Epoch 17 / 100: avg data time: 5.73e-02, avg batch time: 0.5054, average train loss: 0.1423
[09/26 01:27:23 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 3.7660
[09/26 01:27:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 83.00	
[09/26 01:27:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 01:27:30 visual_prompt]: Epoch 18 / 100: avg data time: 6.00e-02, avg batch time: 0.5091, average train loss: 0.0981
[09/26 01:27:32 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1696, average loss: 3.6515
[09/26 01:27:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 83.00	
[09/26 01:27:32 visual_prompt]: Best epoch 18: best metric: 0.565
[09/26 01:27:32 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 01:27:38 visual_prompt]: Epoch 19 / 100: avg data time: 4.01e-02, avg batch time: 0.4912, average train loss: 0.0481
[09/26 01:27:40 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 3.6025
[09/26 01:27:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 84.00	
[09/26 01:27:40 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 01:27:47 visual_prompt]: Epoch 20 / 100: avg data time: 5.39e-02, avg batch time: 0.5025, average train loss: 0.0518
[09/26 01:27:48 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1695, average loss: 3.6882
[09/26 01:27:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 79.00	
[09/26 01:27:48 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 01:27:55 visual_prompt]: Epoch 21 / 100: avg data time: 5.97e-02, avg batch time: 0.5082, average train loss: 0.0638
[09/26 01:27:56 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1691, average loss: 3.4813
[09/26 01:27:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 85.00	
[09/26 01:27:56 visual_prompt]: Best epoch 21: best metric: 0.590
[09/26 01:27:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 01:28:03 visual_prompt]: Epoch 22 / 100: avg data time: 5.78e-02, avg batch time: 0.5068, average train loss: 0.0128
[09/26 01:28:05 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 3.6152
[09/26 01:28:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 84.00	
[09/26 01:28:05 visual_prompt]: Best epoch 22: best metric: 0.600
[09/26 01:28:05 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 01:28:11 visual_prompt]: Epoch 23 / 100: avg data time: 4.70e-02, avg batch time: 0.4965, average train loss: 0.0049
[09/26 01:28:13 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1693, average loss: 3.6260
[09/26 01:28:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 84.00	
[09/26 01:28:13 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 01:28:20 visual_prompt]: Epoch 24 / 100: avg data time: 5.14e-02, avg batch time: 0.5002, average train loss: 0.0043
[09/26 01:28:21 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1696, average loss: 3.4544
[09/26 01:28:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.50	
[09/26 01:28:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 01:28:28 visual_prompt]: Epoch 25 / 100: avg data time: 5.64e-02, avg batch time: 0.5047, average train loss: 0.0024
[09/26 01:28:30 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1689, average loss: 3.4164
[09/26 01:28:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.50	
[09/26 01:28:30 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 01:28:36 visual_prompt]: Epoch 26 / 100: avg data time: 4.35e-02, avg batch time: 0.4958, average train loss: 0.0003
[09/26 01:28:38 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1690, average loss: 3.4231
[09/26 01:28:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 87.50	
[09/26 01:28:38 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 01:28:45 visual_prompt]: Epoch 27 / 100: avg data time: 5.93e-02, avg batch time: 0.5078, average train loss: 0.0004
[09/26 01:28:46 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1695, average loss: 3.4330
[09/26 01:28:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:28:46 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 01:28:53 visual_prompt]: Epoch 28 / 100: avg data time: 5.82e-02, avg batch time: 0.5068, average train loss: 0.0004
[09/26 01:28:55 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1691, average loss: 3.4431
[09/26 01:28:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:28:55 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 01:29:02 visual_prompt]: Epoch 29 / 100: avg data time: 4.43e-02, avg batch time: 0.4971, average train loss: 0.0002
[09/26 01:29:03 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 3.4515
[09/26 01:29:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:29:03 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 01:29:10 visual_prompt]: Epoch 30 / 100: avg data time: 4.02e-02, avg batch time: 0.4888, average train loss: 0.0002
[09/26 01:29:11 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1697, average loss: 3.4613
[09/26 01:29:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:29:11 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 01:29:18 visual_prompt]: Epoch 31 / 100: avg data time: 6.37e-02, avg batch time: 0.5116, average train loss: 0.0001
[09/26 01:29:20 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1690, average loss: 3.4657
[09/26 01:29:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:29:20 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 01:29:27 visual_prompt]: Epoch 32 / 100: avg data time: 5.04e-02, avg batch time: 0.4998, average train loss: 0.0001
[09/26 01:29:28 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 3.4686
[09/26 01:29:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:29:28 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 01:29:35 visual_prompt]: Epoch 33 / 100: avg data time: 4.59e-02, avg batch time: 0.4940, average train loss: 0.0001
[09/26 01:29:37 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 3.4708
[09/26 01:29:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:29:37 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 01:29:43 visual_prompt]: Epoch 34 / 100: avg data time: 4.30e-02, avg batch time: 0.4934, average train loss: 0.0001
[09/26 01:29:45 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1694, average loss: 3.4709
[09/26 01:29:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:29:45 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 01:29:52 visual_prompt]: Epoch 35 / 100: avg data time: 4.58e-02, avg batch time: 0.4946, average train loss: 0.0001
[09/26 01:29:53 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1691, average loss: 3.4740
[09/26 01:29:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:29:53 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 01:30:00 visual_prompt]: Epoch 36 / 100: avg data time: 6.13e-02, avg batch time: 0.5101, average train loss: 0.0001
[09/26 01:30:01 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1691, average loss: 3.4767
[09/26 01:30:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:30:01 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 01:30:08 visual_prompt]: Epoch 37 / 100: avg data time: 6.40e-02, avg batch time: 0.5122, average train loss: 0.0001
[09/26 01:30:10 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1693, average loss: 3.4800
[09/26 01:30:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:30:10 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 01:30:17 visual_prompt]: Epoch 38 / 100: avg data time: 5.89e-02, avg batch time: 0.5077, average train loss: 0.0001
[09/26 01:30:18 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 3.4789
[09/26 01:30:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:30:18 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 01:30:25 visual_prompt]: Epoch 39 / 100: avg data time: 5.31e-02, avg batch time: 0.5043, average train loss: 0.0001
[09/26 01:30:27 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1693, average loss: 3.4799
[09/26 01:30:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:30:27 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 01:30:33 visual_prompt]: Epoch 40 / 100: avg data time: 5.06e-02, avg batch time: 0.5009, average train loss: 0.0001
[09/26 01:30:35 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1692, average loss: 3.4819
[09/26 01:30:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:30:35 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 01:30:42 visual_prompt]: Epoch 41 / 100: avg data time: 4.79e-02, avg batch time: 0.4967, average train loss: 0.0001
[09/26 01:30:43 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1688, average loss: 3.4838
[09/26 01:30:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:30:43 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 01:30:50 visual_prompt]: Epoch 42 / 100: avg data time: 5.85e-02, avg batch time: 0.5066, average train loss: 0.0001
[09/26 01:30:52 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1694, average loss: 3.4842
[09/26 01:30:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:30:52 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 01:30:58 visual_prompt]: Epoch 43 / 100: avg data time: 4.87e-02, avg batch time: 0.4988, average train loss: 0.0001
[09/26 01:31:00 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1694, average loss: 3.4850
[09/26 01:31:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.50	
[09/26 01:31:00 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 01:31:07 visual_prompt]: Epoch 44 / 100: avg data time: 4.59e-02, avg batch time: 0.4970, average train loss: 0.0001
[09/26 01:31:08 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1695, average loss: 3.4860
[09/26 01:31:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:31:08 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 01:31:15 visual_prompt]: Epoch 45 / 100: avg data time: 5.75e-02, avg batch time: 0.5065, average train loss: 0.0001
[09/26 01:31:16 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1692, average loss: 3.4871
[09/26 01:31:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:31:16 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 01:31:23 visual_prompt]: Epoch 46 / 100: avg data time: 5.94e-02, avg batch time: 0.5103, average train loss: 0.0001
[09/26 01:31:25 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1693, average loss: 3.4882
[09/26 01:31:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:31:25 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 01:31:32 visual_prompt]: Epoch 47 / 100: avg data time: 5.47e-02, avg batch time: 0.5035, average train loss: 0.0001
[09/26 01:31:33 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1689, average loss: 3.4894
[09/26 01:31:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 01:31:33 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 01:31:40 visual_prompt]: Epoch 48 / 100: avg data time: 6.45e-02, avg batch time: 0.5126, average train loss: 0.0001
[09/26 01:31:42 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1692, average loss: 3.4908
[09/26 01:31:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:31:42 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 01:31:49 visual_prompt]: Epoch 49 / 100: avg data time: 6.95e-02, avg batch time: 0.5176, average train loss: 0.0001
[09/26 01:31:50 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 3.4924
[09/26 01:31:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:31:50 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 01:31:57 visual_prompt]: Epoch 50 / 100: avg data time: 6.09e-02, avg batch time: 0.5093, average train loss: 0.0001
[09/26 01:31:58 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1694, average loss: 3.4938
[09/26 01:31:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:31:58 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 01:32:05 visual_prompt]: Epoch 51 / 100: avg data time: 4.33e-02, avg batch time: 0.4932, average train loss: 0.0001
[09/26 01:32:07 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1692, average loss: 3.4953
[09/26 01:32:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:32:07 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 01:32:13 visual_prompt]: Epoch 52 / 100: avg data time: 6.22e-02, avg batch time: 0.5111, average train loss: 0.0001
[09/26 01:32:15 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1697, average loss: 3.4963
[09/26 01:32:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:32:15 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 01:32:22 visual_prompt]: Epoch 53 / 100: avg data time: 5.48e-02, avg batch time: 0.5030, average train loss: 0.0001
[09/26 01:32:23 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1695, average loss: 3.4972
[09/26 01:32:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:32:23 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 01:32:30 visual_prompt]: Epoch 54 / 100: avg data time: 6.75e-02, avg batch time: 0.5156, average train loss: 0.0001
[09/26 01:32:32 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1692, average loss: 3.4980
[09/26 01:32:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:32:32 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 01:32:38 visual_prompt]: Epoch 55 / 100: avg data time: 4.73e-02, avg batch time: 0.4969, average train loss: 0.0001
[09/26 01:32:40 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1693, average loss: 3.4992
[09/26 01:32:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:32:40 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 01:32:47 visual_prompt]: Epoch 56 / 100: avg data time: 6.43e-02, avg batch time: 0.5126, average train loss: 0.0001
[09/26 01:32:48 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1693, average loss: 3.5002
[09/26 01:32:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:32:48 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 01:32:55 visual_prompt]: Epoch 57 / 100: avg data time: 6.61e-02, avg batch time: 0.5141, average train loss: 0.0001
[09/26 01:32:57 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1690, average loss: 3.5008
[09/26 01:32:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:32:57 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 01:33:04 visual_prompt]: Epoch 58 / 100: avg data time: 5.10e-02, avg batch time: 0.4998, average train loss: 0.0001
[09/26 01:33:05 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 3.5018
[09/26 01:33:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:33:05 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 01:33:12 visual_prompt]: Epoch 59 / 100: avg data time: 5.77e-02, avg batch time: 0.5069, average train loss: 0.0001
[09/26 01:33:13 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1697, average loss: 3.5027
[09/26 01:33:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:33:13 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 01:33:20 visual_prompt]: Epoch 60 / 100: avg data time: 5.69e-02, avg batch time: 0.5049, average train loss: 0.0001
[09/26 01:33:22 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 3.5033
[09/26 01:33:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:33:22 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 01:33:29 visual_prompt]: Epoch 61 / 100: avg data time: 5.13e-02, avg batch time: 0.5001, average train loss: 0.0001
[09/26 01:33:30 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1695, average loss: 3.5038
[09/26 01:33:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:33:30 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 01:33:37 visual_prompt]: Epoch 62 / 100: avg data time: 5.19e-02, avg batch time: 0.4999, average train loss: 0.0001
[09/26 01:33:38 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1694, average loss: 3.5044
[09/26 01:33:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:33:38 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 01:33:45 visual_prompt]: Epoch 63 / 100: avg data time: 5.74e-02, avg batch time: 0.5054, average train loss: 0.0001
[09/26 01:33:47 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 3.5048
[09/26 01:33:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:33:47 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 01:33:54 visual_prompt]: Epoch 64 / 100: avg data time: 5.11e-02, avg batch time: 0.4997, average train loss: 0.0001
[09/26 01:33:55 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1698, average loss: 3.5053
[09/26 01:33:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:33:55 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 01:34:02 visual_prompt]: Epoch 65 / 100: avg data time: 6.00e-02, avg batch time: 0.5089, average train loss: 0.0001
[09/26 01:34:03 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1695, average loss: 3.5058
[09/26 01:34:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:34:03 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 01:34:10 visual_prompt]: Epoch 66 / 100: avg data time: 5.98e-02, avg batch time: 0.5087, average train loss: 0.0001
[09/26 01:34:12 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 3.5063
[09/26 01:34:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:34:12 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 01:34:19 visual_prompt]: Epoch 67 / 100: avg data time: 5.50e-02, avg batch time: 0.5038, average train loss: 0.0001
[09/26 01:34:20 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1696, average loss: 3.5068
[09/26 01:34:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:34:20 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 01:34:27 visual_prompt]: Epoch 68 / 100: avg data time: 6.16e-02, avg batch time: 0.5108, average train loss: 0.0001
[09/26 01:34:29 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1700, average loss: 3.5073
[09/26 01:34:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:34:29 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 01:34:36 visual_prompt]: Epoch 69 / 100: avg data time: 6.17e-02, avg batch time: 0.5094, average train loss: 0.0001
[09/26 01:34:37 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1694, average loss: 3.5083
[09/26 01:34:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:34:37 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 01:34:44 visual_prompt]: Epoch 70 / 100: avg data time: 5.56e-02, avg batch time: 0.5039, average train loss: 0.0001
[09/26 01:34:46 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 3.5094
[09/26 01:34:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:34:46 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 01:34:53 visual_prompt]: Epoch 71 / 100: avg data time: 5.30e-02, avg batch time: 0.5026, average train loss: 0.0001
[09/26 01:34:54 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 3.5101
[09/26 01:34:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:34:54 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 01:35:01 visual_prompt]: Epoch 72 / 100: avg data time: 4.75e-02, avg batch time: 0.4963, average train loss: 0.0001
[09/26 01:35:02 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1695, average loss: 3.5106
[09/26 01:35:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:35:02 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 01:35:09 visual_prompt]: Epoch 73 / 100: avg data time: 4.30e-02, avg batch time: 0.4945, average train loss: 0.0001
[09/26 01:35:11 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1690, average loss: 3.5110
[09/26 01:35:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:35:11 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 01:35:18 visual_prompt]: Epoch 74 / 100: avg data time: 5.83e-02, avg batch time: 0.5064, average train loss: 0.0001
[09/26 01:35:19 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1691, average loss: 3.5104
[09/26 01:35:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:35:19 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 01:35:26 visual_prompt]: Epoch 75 / 100: avg data time: 6.06e-02, avg batch time: 0.5087, average train loss: 0.0001
[09/26 01:35:27 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1695, average loss: 3.5103
[09/26 01:35:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:35:27 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 01:35:34 visual_prompt]: Epoch 76 / 100: avg data time: 4.56e-02, avg batch time: 0.4966, average train loss: 0.0001
[09/26 01:35:36 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 3.5107
[09/26 01:35:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:35:36 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 01:35:42 visual_prompt]: Epoch 77 / 100: avg data time: 4.21e-02, avg batch time: 0.4931, average train loss: 0.0001
[09/26 01:35:44 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1696, average loss: 3.5111
[09/26 01:35:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:35:44 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 01:35:51 visual_prompt]: Epoch 78 / 100: avg data time: 6.14e-02, avg batch time: 0.5094, average train loss: 0.0001
[09/26 01:35:52 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 3.5114
[09/26 01:35:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:35:52 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 01:35:59 visual_prompt]: Epoch 79 / 100: avg data time: 5.19e-02, avg batch time: 0.5002, average train loss: 0.0001
[09/26 01:36:01 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1697, average loss: 3.5116
[09/26 01:36:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:36:01 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 01:36:07 visual_prompt]: Epoch 80 / 100: avg data time: 5.50e-02, avg batch time: 0.5051, average train loss: 0.0001
[09/26 01:36:09 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1697, average loss: 3.5117
[09/26 01:36:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:36:09 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 01:36:16 visual_prompt]: Epoch 81 / 100: avg data time: 4.90e-02, avg batch time: 0.4977, average train loss: 0.0001
[09/26 01:36:17 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 3.5119
[09/26 01:36:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:36:17 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 01:36:24 visual_prompt]: Epoch 82 / 100: avg data time: 6.13e-02, avg batch time: 0.5096, average train loss: 0.0001
[09/26 01:36:26 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 3.5120
[09/26 01:36:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:36:26 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 01:36:32 visual_prompt]: Epoch 83 / 100: avg data time: 4.30e-02, avg batch time: 0.4917, average train loss: 0.0001
[09/26 01:36:34 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1696, average loss: 3.5122
[09/26 01:36:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:36:34 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 01:36:41 visual_prompt]: Epoch 84 / 100: avg data time: 4.47e-02, avg batch time: 0.4939, average train loss: 0.0001
[09/26 01:36:42 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1699, average loss: 3.5123
[09/26 01:36:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:36:42 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 01:36:49 visual_prompt]: Epoch 85 / 100: avg data time: 5.74e-02, avg batch time: 0.5056, average train loss: 0.0001
[09/26 01:36:50 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1692, average loss: 3.5124
[09/26 01:36:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:36:50 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 01:36:57 visual_prompt]: Epoch 86 / 100: avg data time: 5.79e-02, avg batch time: 0.5059, average train loss: 0.0001
[09/26 01:36:59 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1693, average loss: 3.5125
[09/26 01:36:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 88.00	
[09/26 01:36:59 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 01:37:06 visual_prompt]: Epoch 87 / 100: avg data time: 5.24e-02, avg batch time: 0.5026, average train loss: 0.0001
[09/26 01:37:07 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1688, average loss: 3.5126
[09/26 01:37:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:37:07 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 01:37:14 visual_prompt]: Epoch 88 / 100: avg data time: 4.74e-02, avg batch time: 0.4961, average train loss: 0.0001
[09/26 01:37:15 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 3.5126
[09/26 01:37:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:37:15 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 01:37:22 visual_prompt]: Epoch 89 / 100: avg data time: 6.07e-02, avg batch time: 0.5092, average train loss: 0.0001
[09/26 01:37:24 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1695, average loss: 3.5127
[09/26 01:37:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:37:24 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 01:37:31 visual_prompt]: Epoch 90 / 100: avg data time: 4.69e-02, avg batch time: 0.4955, average train loss: 0.0001
[09/26 01:37:32 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1696, average loss: 3.5128
[09/26 01:37:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:37:32 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 01:37:39 visual_prompt]: Epoch 91 / 100: avg data time: 5.07e-02, avg batch time: 0.4995, average train loss: 0.0001
[09/26 01:37:40 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1689, average loss: 3.5129
[09/26 01:37:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:37:40 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 01:37:47 visual_prompt]: Epoch 92 / 100: avg data time: 5.82e-02, avg batch time: 0.5063, average train loss: 0.0001
[09/26 01:37:49 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1692, average loss: 3.5129
[09/26 01:37:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:37:49 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 01:37:55 visual_prompt]: Epoch 93 / 100: avg data time: 5.84e-02, avg batch time: 0.5073, average train loss: 0.0001
[09/26 01:37:57 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1696, average loss: 3.5130
[09/26 01:37:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:37:57 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 01:38:04 visual_prompt]: Epoch 94 / 100: avg data time: 5.18e-02, avg batch time: 0.5009, average train loss: 0.0001
[09/26 01:38:05 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1692, average loss: 3.5130
[09/26 01:38:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:38:05 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 01:38:12 visual_prompt]: Epoch 95 / 100: avg data time: 4.75e-02, avg batch time: 0.4961, average train loss: 0.0001
[09/26 01:38:13 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1695, average loss: 3.5130
[09/26 01:38:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:38:13 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 01:38:20 visual_prompt]: Epoch 96 / 100: avg data time: 6.74e-02, avg batch time: 0.5158, average train loss: 0.0001
[09/26 01:38:22 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1692, average loss: 3.5130
[09/26 01:38:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:38:22 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 01:38:29 visual_prompt]: Epoch 97 / 100: avg data time: 4.79e-02, avg batch time: 0.4981, average train loss: 0.0001
[09/26 01:38:30 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1691, average loss: 3.5130
[09/26 01:38:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:38:30 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 01:38:37 visual_prompt]: Epoch 98 / 100: avg data time: 5.18e-02, avg batch time: 0.5001, average train loss: 0.0001
[09/26 01:38:39 visual_prompt]: Inference (val):avg data time: 1.99e-05, avg batch time: 0.1693, average loss: 3.5130
[09/26 01:38:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:38:39 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 01:38:46 visual_prompt]: Epoch 99 / 100: avg data time: 6.33e-02, avg batch time: 0.5123, average train loss: 0.0001
[09/26 01:38:47 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1694, average loss: 3.5130
[09/26 01:38:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:38:47 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 01:38:54 visual_prompt]: Epoch 100 / 100: avg data time: 6.30e-02, avg batch time: 0.5122, average train loss: 0.0001
[09/26 01:38:56 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 3.5130
[09/26 01:38:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 01:38:56 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 01:38:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 01:38:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 01:38:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 01:38:56 visual_prompt]: Training with config:
[09/26 01:38:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 01:38:56 visual_prompt]: Loading training data...
[09/26 01:38:56 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 01:38:58 visual_prompt]: Number of images: 800
[09/26 01:38:58 visual_prompt]: Number of classes: 47 / 47
[09/26 01:38:58 visual_prompt]: Loading validation data...
[09/26 01:38:58 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 01:38:59 visual_prompt]: Number of images: 200
[09/26 01:38:59 visual_prompt]: Number of classes: 47 / 47
[09/26 01:38:59 visual_prompt]: Constructing models...
[09/26 01:39:01 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 01:39:01 visual_prompt]: tuned percent:0.576
[09/26 01:39:01 visual_prompt]: Device used for model: 0
[09/26 01:39:01 visual_prompt]: Setting up Evaluator...
[09/26 01:39:01 visual_prompt]: Setting up Trainer...
[09/26 01:39:01 visual_prompt]: 	Setting up the optimizer...
[09/26 01:39:01 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 01:39:08 visual_prompt]: Epoch 1 / 100: avg data time: 4.33e-02, avg batch time: 0.4902, average train loss: 3.9324
[09/26 01:39:09 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1688, average loss: 3.9045
[09/26 01:39:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 01:39:09 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 01:39:09 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 01:39:16 visual_prompt]: Epoch 2 / 100: avg data time: 4.35e-02, avg batch time: 0.4917, average train loss: 3.8888
[09/26 01:39:17 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.1691, average loss: 4.0625
[09/26 01:39:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 12.00	
[09/26 01:39:17 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 01:39:24 visual_prompt]: Epoch 3 / 100: avg data time: 4.74e-02, avg batch time: 0.4952, average train loss: 3.8673
[09/26 01:39:26 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1691, average loss: 3.8575
[09/26 01:39:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 14.00	
[09/26 01:39:26 visual_prompt]: Best epoch 3: best metric: 0.050
[09/26 01:39:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 01:39:32 visual_prompt]: Epoch 4 / 100: avg data time: 4.73e-02, avg batch time: 0.4958, average train loss: 3.8725
[09/26 01:39:34 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1689, average loss: 3.9097
[09/26 01:39:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 13.00	
[09/26 01:39:34 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 01:39:41 visual_prompt]: Epoch 5 / 100: avg data time: 4.42e-02, avg batch time: 0.4920, average train loss: 3.9106
[09/26 01:39:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 4.0671
[09/26 01:39:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 6.50	
[09/26 01:39:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 01:39:49 visual_prompt]: Epoch 6 / 100: avg data time: 4.76e-02, avg batch time: 0.4950, average train loss: 3.9473
[09/26 01:39:50 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1689, average loss: 3.9445
[09/26 01:39:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 01:39:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 01:39:57 visual_prompt]: Epoch 7 / 100: avg data time: 4.46e-02, avg batch time: 0.4933, average train loss: 3.9314
[09/26 01:39:58 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1691, average loss: 3.9726
[09/26 01:39:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.00	
[09/26 01:39:58 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 01:40:05 visual_prompt]: Epoch 8 / 100: avg data time: 5.74e-02, avg batch time: 0.5052, average train loss: 3.9735
[09/26 01:40:07 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1688, average loss: 4.0483
[09/26 01:40:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.50	
[09/26 01:40:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 01:40:14 visual_prompt]: Epoch 9 / 100: avg data time: 5.39e-02, avg batch time: 0.5007, average train loss: 4.0242
[09/26 01:40:15 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1690, average loss: 3.9741
[09/26 01:40:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/26 01:40:15 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 01:40:22 visual_prompt]: Epoch 10 / 100: avg data time: 4.87e-02, avg batch time: 0.4979, average train loss: 3.9977
[09/26 01:40:24 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1688, average loss: 4.0222
[09/26 01:40:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.00	
[09/26 01:40:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 01:40:30 visual_prompt]: Epoch 11 / 100: avg data time: 4.74e-02, avg batch time: 0.4960, average train loss: 4.0170
[09/26 01:40:32 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1690, average loss: 4.0577
[09/26 01:40:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 4.50	
[09/26 01:40:32 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 01:40:39 visual_prompt]: Epoch 12 / 100: avg data time: 6.24e-02, avg batch time: 0.5107, average train loss: 3.9711
[09/26 01:40:40 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 4.1323
[09/26 01:40:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 01:40:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 01:40:47 visual_prompt]: Epoch 13 / 100: avg data time: 4.80e-02, avg batch time: 0.4957, average train loss: 4.0935
[09/26 01:40:49 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1688, average loss: 4.0026
[09/26 01:40:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 01:40:49 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 01:40:55 visual_prompt]: Epoch 14 / 100: avg data time: 4.24e-02, avg batch time: 0.4910, average train loss: 3.9935
[09/26 01:40:57 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1688, average loss: 4.0478
[09/26 01:40:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.50	
[09/26 01:40:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 01:41:04 visual_prompt]: Epoch 15 / 100: avg data time: 6.77e-02, avg batch time: 0.5150, average train loss: 4.0068
[09/26 01:41:06 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1690, average loss: 4.0418
[09/26 01:41:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/26 01:41:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 01:41:13 visual_prompt]: Epoch 16 / 100: avg data time: 5.91e-02, avg batch time: 0.5073, average train loss: 4.0670
[09/26 01:41:14 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 4.1661
[09/26 01:41:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 7.00	
[09/26 01:41:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 01:41:21 visual_prompt]: Epoch 17 / 100: avg data time: 5.49e-02, avg batch time: 0.5055, average train loss: 4.2461
[09/26 01:41:22 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1690, average loss: 4.0265
[09/26 01:41:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/26 01:41:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 01:41:29 visual_prompt]: Epoch 18 / 100: avg data time: 6.61e-02, avg batch time: 0.5131, average train loss: 4.5780
[09/26 01:41:31 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1696, average loss: 4.1908
[09/26 01:41:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 14.50	
[09/26 01:41:31 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 01:41:38 visual_prompt]: Epoch 19 / 100: avg data time: 4.80e-02, avg batch time: 0.4955, average train loss: 4.1526
[09/26 01:41:39 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1689, average loss: 4.1490
[09/26 01:41:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/26 01:41:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 01:41:46 visual_prompt]: Epoch 20 / 100: avg data time: 5.38e-02, avg batch time: 0.5027, average train loss: 4.1317
[09/26 01:41:47 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 4.1878
[09/26 01:41:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/26 01:41:47 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 01:41:54 visual_prompt]: Epoch 21 / 100: avg data time: 6.36e-02, avg batch time: 0.5122, average train loss: 4.2398
[09/26 01:41:56 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1695, average loss: 4.1515
[09/26 01:41:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 01:41:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 01:42:03 visual_prompt]: Epoch 22 / 100: avg data time: 6.43e-02, avg batch time: 0.5116, average train loss: 4.0800
[09/26 01:42:05 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 4.3282
[09/26 01:42:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.00	
[09/26 01:42:05 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 01:42:11 visual_prompt]: Epoch 23 / 100: avg data time: 5.18e-02, avg batch time: 0.4999, average train loss: 4.1418
[09/26 01:42:13 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1686, average loss: 4.1845
[09/26 01:42:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.00	
[09/26 01:42:13 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 01:42:20 visual_prompt]: Epoch 24 / 100: avg data time: 5.96e-02, avg batch time: 0.5078, average train loss: 4.0497
[09/26 01:42:21 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 4.0798
[09/26 01:42:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/26 01:42:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 01:42:28 visual_prompt]: Epoch 25 / 100: avg data time: 6.54e-02, avg batch time: 0.5126, average train loss: 4.1018
[09/26 01:42:30 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1693, average loss: 4.2100
[09/26 01:42:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.50	
[09/26 01:42:30 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 01:42:37 visual_prompt]: Epoch 26 / 100: avg data time: 5.93e-02, avg batch time: 0.5097, average train loss: 4.0887
[09/26 01:42:38 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1693, average loss: 4.0593
[09/26 01:42:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/26 01:42:38 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 01:42:45 visual_prompt]: Epoch 27 / 100: avg data time: 5.56e-02, avg batch time: 0.5047, average train loss: 4.0965
[09/26 01:42:46 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 3.9945
[09/26 01:42:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.00	
[09/26 01:42:46 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 01:42:53 visual_prompt]: Epoch 28 / 100: avg data time: 6.17e-02, avg batch time: 0.5092, average train loss: 4.0923
[09/26 01:42:55 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.1697, average loss: 4.0814
[09/26 01:42:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/26 01:42:55 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 01:43:02 visual_prompt]: Epoch 29 / 100: avg data time: 5.98e-02, avg batch time: 0.5089, average train loss: 4.0955
[09/26 01:43:03 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1694, average loss: 4.3180
[09/26 01:43:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 01:43:03 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 01:43:10 visual_prompt]: Epoch 30 / 100: avg data time: 4.64e-02, avg batch time: 0.4945, average train loss: 4.0415
[09/26 01:43:11 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1693, average loss: 3.9861
[09/26 01:43:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 01:43:11 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 01:43:18 visual_prompt]: Epoch 31 / 100: avg data time: 6.22e-02, avg batch time: 0.5098, average train loss: 4.0631
[09/26 01:43:20 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1695, average loss: 4.0494
[09/26 01:43:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 8.00	
[09/26 01:43:20 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 01:43:27 visual_prompt]: Epoch 32 / 100: avg data time: 5.74e-02, avg batch time: 0.5068, average train loss: 4.0680
[09/26 01:43:28 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 4.2433
[09/26 01:43:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 01:43:28 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 01:43:35 visual_prompt]: Epoch 33 / 100: avg data time: 6.05e-02, avg batch time: 0.5079, average train loss: 4.0408
[09/26 01:43:37 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1693, average loss: 4.0095
[09/26 01:43:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 01:43:37 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 01:43:44 visual_prompt]: Epoch 34 / 100: avg data time: 6.31e-02, avg batch time: 0.5113, average train loss: 4.0451
[09/26 01:43:45 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1690, average loss: 3.9914
[09/26 01:43:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 01:43:45 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 01:43:52 visual_prompt]: Epoch 35 / 100: avg data time: 6.06e-02, avg batch time: 0.5085, average train loss: 4.0025
[09/26 01:43:53 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1692, average loss: 3.9760
[09/26 01:43:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.00	
[09/26 01:43:53 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 01:44:00 visual_prompt]: Epoch 36 / 100: avg data time: 5.95e-02, avg batch time: 0.5074, average train loss: 3.9624
[09/26 01:44:02 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1689, average loss: 4.0264
[09/26 01:44:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 5.50	
[09/26 01:44:02 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 01:44:09 visual_prompt]: Epoch 37 / 100: avg data time: 6.08e-02, avg batch time: 0.5079, average train loss: 3.9881
[09/26 01:44:10 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1693, average loss: 4.0008
[09/26 01:44:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.50	
[09/26 01:44:10 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 01:44:17 visual_prompt]: Epoch 38 / 100: avg data time: 5.56e-02, avg batch time: 0.5036, average train loss: 4.0408
[09/26 01:44:18 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1693, average loss: 4.0542
[09/26 01:44:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/26 01:44:18 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 01:44:25 visual_prompt]: Epoch 39 / 100: avg data time: 4.84e-02, avg batch time: 0.4966, average train loss: 3.9561
[09/26 01:44:27 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1693, average loss: 3.9769
[09/26 01:44:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.50	
[09/26 01:44:27 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 01:44:34 visual_prompt]: Epoch 40 / 100: avg data time: 6.36e-02, avg batch time: 0.5109, average train loss: 3.9943
[09/26 01:44:35 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1690, average loss: 4.0076
[09/26 01:44:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.00	
[09/26 01:44:35 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 01:44:42 visual_prompt]: Epoch 41 / 100: avg data time: 5.56e-02, avg batch time: 0.5039, average train loss: 3.9909
[09/26 01:44:43 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1692, average loss: 4.1999
[09/26 01:44:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/26 01:44:43 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 01:44:50 visual_prompt]: Epoch 42 / 100: avg data time: 5.64e-02, avg batch time: 0.5047, average train loss: 4.0128
[09/26 01:44:52 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1697, average loss: 4.0267
[09/26 01:44:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/26 01:44:52 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 01:44:59 visual_prompt]: Epoch 43 / 100: avg data time: 5.18e-02, avg batch time: 0.5005, average train loss: 3.9808
[09/26 01:45:00 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1695, average loss: 3.9781
[09/26 01:45:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.50	
[09/26 01:45:00 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 01:45:07 visual_prompt]: Epoch 44 / 100: avg data time: 5.85e-02, avg batch time: 0.5065, average train loss: 3.9364
[09/26 01:45:08 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 4.0059
[09/26 01:45:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 12.00	
[09/26 01:45:08 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 01:45:15 visual_prompt]: Epoch 45 / 100: avg data time: 5.40e-02, avg batch time: 0.5023, average train loss: 3.9866
[09/26 01:45:17 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 3.9503
[09/26 01:45:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 01:45:17 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 01:45:23 visual_prompt]: Epoch 46 / 100: avg data time: 5.61e-02, avg batch time: 0.5035, average train loss: 4.0151
[09/26 01:45:25 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 3.9833
[09/26 01:45:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.00	
[09/26 01:45:25 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 01:45:32 visual_prompt]: Epoch 47 / 100: avg data time: 5.77e-02, avg batch time: 0.5057, average train loss: 3.9635
[09/26 01:45:33 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1694, average loss: 3.9361
[09/26 01:45:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 01:45:33 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 01:45:40 visual_prompt]: Epoch 48 / 100: avg data time: 6.13e-02, avg batch time: 0.5092, average train loss: 3.9659
[09/26 01:45:42 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 3.9791
[09/26 01:45:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.50	
[09/26 01:45:42 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 01:45:49 visual_prompt]: Epoch 49 / 100: avg data time: 6.39e-02, avg batch time: 0.5122, average train loss: 3.9517
[09/26 01:45:50 visual_prompt]: Inference (val):avg data time: 4.77e-05, avg batch time: 0.1693, average loss: 3.9466
[09/26 01:45:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 01:45:50 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 01:45:57 visual_prompt]: Epoch 50 / 100: avg data time: 5.01e-02, avg batch time: 0.4986, average train loss: 3.9324
[09/26 01:45:58 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1694, average loss: 4.1057
[09/26 01:45:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/26 01:45:58 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 01:46:05 visual_prompt]: Epoch 51 / 100: avg data time: 5.83e-02, avg batch time: 0.5066, average train loss: 3.9581
[09/26 01:46:07 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1693, average loss: 4.0673
[09/26 01:46:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.50	
[09/26 01:46:07 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 01:46:14 visual_prompt]: Epoch 52 / 100: avg data time: 6.22e-02, avg batch time: 0.5093, average train loss: 3.9224
[09/26 01:46:15 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1695, average loss: 3.9492
[09/26 01:46:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/26 01:46:15 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 01:46:22 visual_prompt]: Epoch 53 / 100: avg data time: 4.59e-02, avg batch time: 0.4948, average train loss: 3.9276
[09/26 01:46:24 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 3.9589
[09/26 01:46:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 12.50	
[09/26 01:46:24 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 01:46:30 visual_prompt]: Epoch 54 / 100: avg data time: 5.32e-02, avg batch time: 0.5023, average train loss: 3.9146
[09/26 01:46:32 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1695, average loss: 3.9674
[09/26 01:46:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 01:46:32 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 01:46:39 visual_prompt]: Epoch 55 / 100: avg data time: 5.43e-02, avg batch time: 0.5019, average train loss: 3.9220
[09/26 01:46:40 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1693, average loss: 3.9734
[09/26 01:46:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/26 01:46:40 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 01:46:47 visual_prompt]: Epoch 56 / 100: avg data time: 5.07e-02, avg batch time: 0.4993, average train loss: 3.9038
[09/26 01:46:49 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1687, average loss: 3.9372
[09/26 01:46:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.00	
[09/26 01:46:49 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 01:46:55 visual_prompt]: Epoch 57 / 100: avg data time: 4.73e-02, avg batch time: 0.4954, average train loss: 3.9262
[09/26 01:46:57 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1689, average loss: 3.9596
[09/26 01:46:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 6.00	
[09/26 01:46:57 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 01:47:04 visual_prompt]: Epoch 58 / 100: avg data time: 4.04e-02, avg batch time: 0.4925, average train loss: 3.9312
[09/26 01:47:05 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1695, average loss: 3.9524
[09/26 01:47:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.50	
[09/26 01:47:05 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 01:47:12 visual_prompt]: Epoch 59 / 100: avg data time: 5.62e-02, avg batch time: 0.5037, average train loss: 3.9491
[09/26 01:47:14 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 3.8741
[09/26 01:47:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 14.00	
[09/26 01:47:14 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 01:47:20 visual_prompt]: Epoch 60 / 100: avg data time: 4.66e-02, avg batch time: 0.4957, average train loss: 3.9305
[09/26 01:47:22 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 4.0308
[09/26 01:47:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 8.50	
[09/26 01:47:22 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 01:47:29 visual_prompt]: Epoch 61 / 100: avg data time: 5.79e-02, avg batch time: 0.5049, average train loss: 3.9362
[09/26 01:47:30 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1692, average loss: 3.9274
[09/26 01:47:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 01:47:30 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 01:47:37 visual_prompt]: Epoch 62 / 100: avg data time: 5.59e-02, avg batch time: 0.5057, average train loss: 3.9299
[09/26 01:47:39 visual_prompt]: Inference (val):avg data time: 4.71e-05, avg batch time: 0.1692, average loss: 3.9491
[09/26 01:47:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 01:47:39 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 01:47:45 visual_prompt]: Epoch 63 / 100: avg data time: 5.26e-02, avg batch time: 0.5016, average train loss: 3.9104
[09/26 01:47:47 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 4.0630
[09/26 01:47:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/26 01:47:47 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 01:47:54 visual_prompt]: Epoch 64 / 100: avg data time: 4.45e-02, avg batch time: 0.4936, average train loss: 3.9109
[09/26 01:47:55 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1691, average loss: 4.0878
[09/26 01:47:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.00	
[09/26 01:47:55 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 01:48:02 visual_prompt]: Epoch 65 / 100: avg data time: 4.64e-02, avg batch time: 0.4946, average train loss: 3.9080
[09/26 01:48:03 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1689, average loss: 3.9419
[09/26 01:48:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 01:48:03 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 01:48:10 visual_prompt]: Epoch 66 / 100: avg data time: 5.43e-02, avg batch time: 0.5025, average train loss: 3.9043
[09/26 01:48:12 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 3.9112
[09/26 01:48:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.00	
[09/26 01:48:12 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 01:48:18 visual_prompt]: Epoch 67 / 100: avg data time: 4.75e-02, avg batch time: 0.4954, average train loss: 3.8978
[09/26 01:48:20 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.1689, average loss: 3.9424
[09/26 01:48:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 01:48:20 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 01:48:27 visual_prompt]: Epoch 68 / 100: avg data time: 5.67e-02, avg batch time: 0.5050, average train loss: 3.9144
[09/26 01:48:28 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1694, average loss: 3.9482
[09/26 01:48:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.50	
[09/26 01:48:28 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 01:48:35 visual_prompt]: Epoch 69 / 100: avg data time: 5.81e-02, avg batch time: 0.5075, average train loss: 3.8994
[09/26 01:48:37 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 3.9531
[09/26 01:48:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/26 01:48:37 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 01:48:44 visual_prompt]: Epoch 70 / 100: avg data time: 5.97e-02, avg batch time: 0.5085, average train loss: 3.8878
[09/26 01:48:45 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 3.9390
[09/26 01:48:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 10.50	
[09/26 01:48:45 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 01:48:52 visual_prompt]: Epoch 71 / 100: avg data time: 4.37e-02, avg batch time: 0.4932, average train loss: 3.8948
[09/26 01:48:53 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 3.8963
[09/26 01:48:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.50	
[09/26 01:48:53 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 01:49:00 visual_prompt]: Epoch 72 / 100: avg data time: 5.50e-02, avg batch time: 0.5045, average train loss: 3.8628
[09/26 01:49:02 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 3.9451
[09/26 01:49:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/26 01:49:02 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 01:49:09 visual_prompt]: Epoch 73 / 100: avg data time: 5.13e-02, avg batch time: 0.5003, average train loss: 3.8651
[09/26 01:49:10 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1691, average loss: 3.9174
[09/26 01:49:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.50	
[09/26 01:49:10 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 01:49:17 visual_prompt]: Epoch 74 / 100: avg data time: 5.50e-02, avg batch time: 0.5025, average train loss: 3.8622
[09/26 01:49:19 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 3.8884
[09/26 01:49:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 12.50	
[09/26 01:49:19 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 01:49:26 visual_prompt]: Epoch 75 / 100: avg data time: 5.32e-02, avg batch time: 0.5018, average train loss: 3.8635
[09/26 01:49:27 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 3.8859
[09/26 01:49:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.50	
[09/26 01:49:27 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 01:49:34 visual_prompt]: Epoch 76 / 100: avg data time: 5.26e-02, avg batch time: 0.5012, average train loss: 3.8461
[09/26 01:49:36 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 3.8928
[09/26 01:49:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 01:49:36 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 01:49:42 visual_prompt]: Epoch 77 / 100: avg data time: 4.92e-02, avg batch time: 0.4980, average train loss: 3.8758
[09/26 01:49:44 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1689, average loss: 3.9229
[09/26 01:49:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 01:49:44 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 01:49:51 visual_prompt]: Epoch 78 / 100: avg data time: 6.75e-02, avg batch time: 0.5151, average train loss: 3.8599
[09/26 01:49:52 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 3.9053
[09/26 01:49:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 01:49:52 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 01:49:59 visual_prompt]: Epoch 79 / 100: avg data time: 5.43e-02, avg batch time: 0.5022, average train loss: 3.8469
[09/26 01:50:01 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 3.8958
[09/26 01:50:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 01:50:01 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 01:50:08 visual_prompt]: Epoch 80 / 100: avg data time: 5.03e-02, avg batch time: 0.4998, average train loss: 3.8481
[09/26 01:50:09 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 3.9070
[09/26 01:50:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/26 01:50:09 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 01:50:16 visual_prompt]: Epoch 81 / 100: avg data time: 4.68e-02, avg batch time: 0.4959, average train loss: 3.8550
[09/26 01:50:17 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 3.9014
[09/26 01:50:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 01:50:17 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 01:50:24 visual_prompt]: Epoch 82 / 100: avg data time: 4.40e-02, avg batch time: 0.4925, average train loss: 3.8276
[09/26 01:50:25 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 3.9006
[09/26 01:50:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 13.00	
[09/26 01:50:25 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 01:50:32 visual_prompt]: Epoch 83 / 100: avg data time: 5.20e-02, avg batch time: 0.5010, average train loss: 3.8362
[09/26 01:50:34 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1695, average loss: 3.9200
[09/26 01:50:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.00	
[09/26 01:50:34 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 01:50:40 visual_prompt]: Epoch 84 / 100: avg data time: 5.27e-02, avg batch time: 0.5020, average train loss: 3.8357
[09/26 01:50:42 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1691, average loss: 3.8393
[09/26 01:50:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 16.00	
[09/26 01:50:42 visual_prompt]: Best epoch 84: best metric: 0.060
[09/26 01:50:42 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 01:50:49 visual_prompt]: Epoch 85 / 100: avg data time: 5.84e-02, avg batch time: 0.5073, average train loss: 3.7758
[09/26 01:50:50 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1693, average loss: 3.8029
[09/26 01:50:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 18.50	
[09/26 01:50:50 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 01:50:57 visual_prompt]: Epoch 86 / 100: avg data time: 4.52e-02, avg batch time: 0.4941, average train loss: 3.5709
[09/26 01:50:58 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1693, average loss: 3.4761
[09/26 01:50:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.00	top5: 41.50	
[09/26 01:50:58 visual_prompt]: Best epoch 86: best metric: 0.150
[09/26 01:50:58 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 01:51:05 visual_prompt]: Epoch 87 / 100: avg data time: 5.02e-02, avg batch time: 0.4998, average train loss: 3.0584
[09/26 01:51:07 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 3.0716
[09/26 01:51:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.50	top5: 50.50	
[09/26 01:51:07 visual_prompt]: Best epoch 87: best metric: 0.225
[09/26 01:51:07 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 01:51:13 visual_prompt]: Epoch 88 / 100: avg data time: 4.67e-02, avg batch time: 0.4967, average train loss: 2.3829
[09/26 01:51:15 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1695, average loss: 2.5687
[09/26 01:51:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 33.50	top5: 69.00	
[09/26 01:51:15 visual_prompt]: Best epoch 88: best metric: 0.335
[09/26 01:51:15 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 01:51:21 visual_prompt]: Epoch 89 / 100: avg data time: 4.14e-02, avg batch time: 0.4917, average train loss: 1.7975
[09/26 01:51:23 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1694, average loss: 2.2418
[09/26 01:51:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.00	top5: 74.50	
[09/26 01:51:23 visual_prompt]: Best epoch 89: best metric: 0.380
[09/26 01:51:23 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 01:51:30 visual_prompt]: Epoch 90 / 100: avg data time: 4.88e-02, avg batch time: 0.4971, average train loss: 1.4212
[09/26 01:51:31 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1693, average loss: 2.0527
[09/26 01:51:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 81.50	
[09/26 01:51:31 visual_prompt]: Best epoch 90: best metric: 0.425
[09/26 01:51:31 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 01:51:38 visual_prompt]: Epoch 91 / 100: avg data time: 6.18e-02, avg batch time: 0.5101, average train loss: 1.1528
[09/26 01:51:40 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1695, average loss: 1.9408
[09/26 01:51:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 84.50	
[09/26 01:51:40 visual_prompt]: Best epoch 91: best metric: 0.490
[09/26 01:51:40 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 01:51:47 visual_prompt]: Epoch 92 / 100: avg data time: 5.80e-02, avg batch time: 0.5061, average train loss: 0.9750
[09/26 01:51:48 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1695, average loss: 1.9100
[09/26 01:51:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 84.00	
[09/26 01:51:48 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 01:51:55 visual_prompt]: Epoch 93 / 100: avg data time: 5.87e-02, avg batch time: 0.5069, average train loss: 0.8556
[09/26 01:51:56 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1697, average loss: 1.8255
[09/26 01:51:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 84.00	
[09/26 01:51:56 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 01:52:03 visual_prompt]: Epoch 94 / 100: avg data time: 5.61e-02, avg batch time: 0.5043, average train loss: 0.7368
[09/26 01:52:05 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1695, average loss: 1.7775
[09/26 01:52:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 84.50	
[09/26 01:52:05 visual_prompt]: Best epoch 94: best metric: 0.495
[09/26 01:52:05 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 01:52:11 visual_prompt]: Epoch 95 / 100: avg data time: 5.97e-02, avg batch time: 0.5095, average train loss: 0.6492
[09/26 01:52:13 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 1.7743
[09/26 01:52:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 87.50	
[09/26 01:52:13 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 01:52:20 visual_prompt]: Epoch 96 / 100: avg data time: 4.35e-02, avg batch time: 0.4928, average train loss: 0.5691
[09/26 01:52:21 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 1.7395
[09/26 01:52:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 86.00	
[09/26 01:52:21 visual_prompt]: Best epoch 96: best metric: 0.500
[09/26 01:52:21 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 01:52:28 visual_prompt]: Epoch 97 / 100: avg data time: 4.82e-02, avg batch time: 0.4979, average train loss: 0.5212
[09/26 01:52:29 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1694, average loss: 1.7267
[09/26 01:52:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 87.00	
[09/26 01:52:29 visual_prompt]: Best epoch 97: best metric: 0.515
[09/26 01:52:29 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 01:52:36 visual_prompt]: Epoch 98 / 100: avg data time: 6.05e-02, avg batch time: 0.5096, average train loss: 0.4875
[09/26 01:52:38 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 1.7126
[09/26 01:52:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 87.00	
[09/26 01:52:38 visual_prompt]: Best epoch 98: best metric: 0.520
[09/26 01:52:38 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 01:52:44 visual_prompt]: Epoch 99 / 100: avg data time: 4.36e-02, avg batch time: 0.4936, average train loss: 0.4647
[09/26 01:52:46 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1691, average loss: 1.7160
[09/26 01:52:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 87.00	
[09/26 01:52:46 visual_prompt]: Best epoch 99: best metric: 0.525
[09/26 01:52:46 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 01:52:53 visual_prompt]: Epoch 100 / 100: avg data time: 6.15e-02, avg batch time: 0.5104, average train loss: 0.4555
[09/26 01:52:54 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1691, average loss: 1.7141
[09/26 01:52:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 87.50	
[09/26 01:52:54 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 01:52:54 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 01:52:54 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 01:52:54 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 01:52:54 visual_prompt]: Training with config:
[09/26 01:52:54 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 01:52:54 visual_prompt]: Loading training data...
[09/26 01:52:54 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 01:52:56 visual_prompt]: Number of images: 800
[09/26 01:52:56 visual_prompt]: Number of classes: 47 / 47
[09/26 01:52:56 visual_prompt]: Loading validation data...
[09/26 01:52:56 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 01:52:57 visual_prompt]: Number of images: 200
[09/26 01:52:57 visual_prompt]: Number of classes: 47 / 47
[09/26 01:52:57 visual_prompt]: Constructing models...
[09/26 01:52:59 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 01:52:59 visual_prompt]: tuned percent:0.576
[09/26 01:52:59 visual_prompt]: Device used for model: 0
[09/26 01:52:59 visual_prompt]: Setting up Evaluator...
[09/26 01:52:59 visual_prompt]: Setting up Trainer...
[09/26 01:52:59 visual_prompt]: 	Setting up the optimizer...
[09/26 01:52:59 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 01:53:06 visual_prompt]: Epoch 1 / 100: avg data time: 5.86e-02, avg batch time: 0.5054, average train loss: 3.9241
[09/26 01:53:08 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1687, average loss: 3.9045
[09/26 01:53:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 01:53:08 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 01:53:08 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 01:53:15 visual_prompt]: Epoch 2 / 100: avg data time: 6.45e-02, avg batch time: 0.5113, average train loss: 3.8924
[09/26 01:53:16 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1687, average loss: 3.9343
[09/26 01:53:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 01:53:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 01:53:23 visual_prompt]: Epoch 3 / 100: avg data time: 6.50e-02, avg batch time: 0.5129, average train loss: 3.8718
[09/26 01:53:25 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1687, average loss: 3.8909
[09/26 01:53:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.50	
[09/26 01:53:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 01:53:32 visual_prompt]: Epoch 4 / 100: avg data time: 6.71e-02, avg batch time: 0.5146, average train loss: 3.8551
[09/26 01:53:33 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1687, average loss: 3.8138
[09/26 01:53:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 13.50	
[09/26 01:53:33 visual_prompt]: Best epoch 4: best metric: 0.045
[09/26 01:53:33 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 01:53:40 visual_prompt]: Epoch 5 / 100: avg data time: 5.03e-02, avg batch time: 0.4978, average train loss: 3.7658
[09/26 01:53:42 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1685, average loss: 3.8057
[09/26 01:53:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 15.00	
[09/26 01:53:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 01:53:48 visual_prompt]: Epoch 6 / 100: avg data time: 5.86e-02, avg batch time: 0.5067, average train loss: 3.9783
[09/26 01:53:50 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1689, average loss: 3.9190
[09/26 01:53:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 19.50	
[09/26 01:53:50 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 01:53:57 visual_prompt]: Epoch 7 / 100: avg data time: 6.13e-02, avg batch time: 0.5085, average train loss: 3.7721
[09/26 01:53:59 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1686, average loss: 3.9427
[09/26 01:53:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 21.00	
[09/26 01:53:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 01:54:05 visual_prompt]: Epoch 8 / 100: avg data time: 5.68e-02, avg batch time: 0.5049, average train loss: 3.6032
[09/26 01:54:07 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1692, average loss: 3.6264
[09/26 01:54:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 26.00	
[09/26 01:54:07 visual_prompt]: Best epoch 8: best metric: 0.065
[09/26 01:54:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 01:54:14 visual_prompt]: Epoch 9 / 100: avg data time: 7.27e-02, avg batch time: 0.5199, average train loss: 3.6121
[09/26 01:54:15 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1687, average loss: 3.7432
[09/26 01:54:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 19.00	
[09/26 01:54:15 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 01:54:22 visual_prompt]: Epoch 10 / 100: avg data time: 4.06e-02, avg batch time: 0.4906, average train loss: 3.5765
[09/26 01:54:24 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 4.7199
[09/26 01:54:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 15.00	
[09/26 01:54:24 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 01:54:30 visual_prompt]: Epoch 11 / 100: avg data time: 6.23e-02, avg batch time: 0.5097, average train loss: 4.1390
[09/26 01:54:32 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1689, average loss: 4.1494
[09/26 01:54:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 15.00	
[09/26 01:54:32 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 01:54:39 visual_prompt]: Epoch 12 / 100: avg data time: 7.12e-02, avg batch time: 0.5187, average train loss: 4.1087
[09/26 01:54:40 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1691, average loss: 3.7898
[09/26 01:54:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.50	top5: 23.50	
[09/26 01:54:40 visual_prompt]: Best epoch 12: best metric: 0.075
[09/26 01:54:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 01:54:47 visual_prompt]: Epoch 13 / 100: avg data time: 5.45e-02, avg batch time: 0.5023, average train loss: 3.4523
[09/26 01:54:49 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1687, average loss: 4.0585
[09/26 01:54:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.00	top5: 37.50	
[09/26 01:54:49 visual_prompt]: Best epoch 13: best metric: 0.130
[09/26 01:54:49 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 01:54:56 visual_prompt]: Epoch 14 / 100: avg data time: 6.89e-02, avg batch time: 0.5163, average train loss: 3.3389
[09/26 01:54:57 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1692, average loss: 3.8280
[09/26 01:54:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.50	top5: 42.50	
[09/26 01:54:57 visual_prompt]: Best epoch 14: best metric: 0.165
[09/26 01:54:57 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 01:55:04 visual_prompt]: Epoch 15 / 100: avg data time: 5.80e-02, avg batch time: 0.5057, average train loss: 3.3163
[09/26 01:55:06 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1692, average loss: 3.4929
[09/26 01:55:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.00	top5: 42.50	
[09/26 01:55:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 01:55:12 visual_prompt]: Epoch 16 / 100: avg data time: 4.90e-02, avg batch time: 0.4976, average train loss: 2.6521
[09/26 01:55:14 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1692, average loss: 3.1175
[09/26 01:55:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 56.50	
[09/26 01:55:14 visual_prompt]: Best epoch 16: best metric: 0.245
[09/26 01:55:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 01:55:21 visual_prompt]: Epoch 17 / 100: avg data time: 6.19e-02, avg batch time: 0.5094, average train loss: 1.8564
[09/26 01:55:22 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1686, average loss: 2.7043
[09/26 01:55:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.00	top5: 68.00	
[09/26 01:55:22 visual_prompt]: Best epoch 17: best metric: 0.400
[09/26 01:55:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 01:55:29 visual_prompt]: Epoch 18 / 100: avg data time: 5.61e-02, avg batch time: 0.5037, average train loss: 1.5546
[09/26 01:55:31 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1694, average loss: 4.6596
[09/26 01:55:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.50	top5: 30.00	
[09/26 01:55:31 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 01:55:37 visual_prompt]: Epoch 19 / 100: avg data time: 6.03e-02, avg batch time: 0.5088, average train loss: 3.0082
[09/26 01:55:39 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1691, average loss: 3.8817
[09/26 01:55:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 32.00	
[09/26 01:55:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 01:55:46 visual_prompt]: Epoch 20 / 100: avg data time: 5.83e-02, avg batch time: 0.5077, average train loss: 2.6561
[09/26 01:55:48 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 3.8938
[09/26 01:55:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 19.00	top5: 50.00	
[09/26 01:55:48 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 01:55:54 visual_prompt]: Epoch 21 / 100: avg data time: 4.35e-02, avg batch time: 0.4924, average train loss: 3.3932
[09/26 01:55:56 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1691, average loss: 3.9267
[09/26 01:55:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.50	top5: 49.50	
[09/26 01:55:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 01:56:02 visual_prompt]: Epoch 22 / 100: avg data time: 4.84e-02, avg batch time: 0.4991, average train loss: 3.5373
[09/26 01:56:04 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 3.8257
[09/26 01:56:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 20.00	top5: 51.00	
[09/26 01:56:04 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 01:56:11 visual_prompt]: Epoch 23 / 100: avg data time: 6.94e-02, avg batch time: 0.5171, average train loss: 3.6288
[09/26 01:56:13 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1693, average loss: 6.0670
[09/26 01:56:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 23.00	
[09/26 01:56:13 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 01:56:20 visual_prompt]: Epoch 24 / 100: avg data time: 6.04e-02, avg batch time: 0.5098, average train loss: 4.6042
[09/26 01:56:21 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 3.8348
[09/26 01:56:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.00	top5: 42.50	
[09/26 01:56:21 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 01:56:28 visual_prompt]: Epoch 25 / 100: avg data time: 6.54e-02, avg batch time: 0.5128, average train loss: 4.3822
[09/26 01:56:30 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 4.7412
[09/26 01:56:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 22.50	
[09/26 01:56:30 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 01:56:37 visual_prompt]: Epoch 26 / 100: avg data time: 7.01e-02, avg batch time: 0.5174, average train loss: 4.6844
[09/26 01:56:38 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 4.0027
[09/26 01:56:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.50	top5: 27.00	
[09/26 01:56:38 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 01:56:45 visual_prompt]: Epoch 27 / 100: avg data time: 6.86e-02, avg batch time: 0.5158, average train loss: 4.2736
[09/26 01:56:47 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1688, average loss: 4.4725
[09/26 01:56:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.50	
[09/26 01:56:47 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 01:56:54 visual_prompt]: Epoch 28 / 100: avg data time: 5.82e-02, avg batch time: 0.5063, average train loss: 4.3570
[09/26 01:56:55 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1688, average loss: 4.2797
[09/26 01:56:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 14.00	
[09/26 01:56:55 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 01:57:02 visual_prompt]: Epoch 29 / 100: avg data time: 6.62e-02, avg batch time: 0.5130, average train loss: 4.3068
[09/26 01:57:04 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 4.3145
[09/26 01:57:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/26 01:57:04 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 01:57:11 visual_prompt]: Epoch 30 / 100: avg data time: 6.97e-02, avg batch time: 0.5158, average train loss: 4.3132
[09/26 01:57:13 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1685, average loss: 4.5742
[09/26 01:57:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 10.00	
[09/26 01:57:13 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 01:57:19 visual_prompt]: Epoch 31 / 100: avg data time: 6.40e-02, avg batch time: 0.5126, average train loss: 4.3121
[09/26 01:57:21 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 4.2468
[09/26 01:57:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 14.00	
[09/26 01:57:21 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 01:57:28 visual_prompt]: Epoch 32 / 100: avg data time: 5.50e-02, avg batch time: 0.5027, average train loss: 4.2031
[09/26 01:57:30 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1694, average loss: 4.2176
[09/26 01:57:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 01:57:30 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 01:57:37 visual_prompt]: Epoch 33 / 100: avg data time: 5.82e-02, avg batch time: 0.5068, average train loss: 4.1363
[09/26 01:57:38 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 4.0084
[09/26 01:57:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 13.50	
[09/26 01:57:38 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 01:57:45 visual_prompt]: Epoch 34 / 100: avg data time: 5.03e-02, avg batch time: 0.4980, average train loss: 4.0989
[09/26 01:57:46 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1689, average loss: 3.9398
[09/26 01:57:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/26 01:57:46 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 01:57:53 visual_prompt]: Epoch 35 / 100: avg data time: 4.08e-02, avg batch time: 0.4915, average train loss: 4.0612
[09/26 01:57:54 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1693, average loss: 3.9700
[09/26 01:57:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 17.00	
[09/26 01:57:54 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 01:58:01 visual_prompt]: Epoch 36 / 100: avg data time: 4.41e-02, avg batch time: 0.4940, average train loss: 4.0999
[09/26 01:58:03 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 4.0323
[09/26 01:58:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 01:58:03 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 01:58:10 visual_prompt]: Epoch 37 / 100: avg data time: 5.67e-02, avg batch time: 0.5053, average train loss: 4.0154
[09/26 01:58:11 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 4.0408
[09/26 01:58:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 13.50	
[09/26 01:58:11 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 01:58:18 visual_prompt]: Epoch 38 / 100: avg data time: 5.30e-02, avg batch time: 0.5008, average train loss: 3.9742
[09/26 01:58:20 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 3.9300
[09/26 01:58:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 14.50	
[09/26 01:58:20 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 01:58:26 visual_prompt]: Epoch 39 / 100: avg data time: 4.39e-02, avg batch time: 0.4927, average train loss: 3.7713
[09/26 01:58:28 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 3.7771
[09/26 01:58:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 22.50	
[09/26 01:58:28 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 01:58:35 visual_prompt]: Epoch 40 / 100: avg data time: 5.32e-02, avg batch time: 0.5020, average train loss: 3.4892
[09/26 01:58:36 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1692, average loss: 3.8660
[09/26 01:58:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.00	top5: 22.50	
[09/26 01:58:36 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 01:58:43 visual_prompt]: Epoch 41 / 100: avg data time: 6.08e-02, avg batch time: 0.5085, average train loss: 3.6596
[09/26 01:58:44 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 3.6256
[09/26 01:58:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 28.50	
[09/26 01:58:44 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 01:58:51 visual_prompt]: Epoch 42 / 100: avg data time: 6.54e-02, avg batch time: 0.5132, average train loss: 3.1753
[09/26 01:58:53 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 3.3363
[09/26 01:58:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 18.00	top5: 41.50	
[09/26 01:58:53 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 01:59:00 visual_prompt]: Epoch 43 / 100: avg data time: 5.30e-02, avg batch time: 0.5027, average train loss: 2.6700
[09/26 01:59:01 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 3.3079
[09/26 01:59:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 19.00	top5: 47.50	
[09/26 01:59:01 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 01:59:08 visual_prompt]: Epoch 44 / 100: avg data time: 4.37e-02, avg batch time: 0.4918, average train loss: 2.9700
[09/26 01:59:10 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1690, average loss: 3.1740
[09/26 01:59:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.50	top5: 44.00	
[09/26 01:59:10 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 01:59:16 visual_prompt]: Epoch 45 / 100: avg data time: 4.49e-02, avg batch time: 0.4943, average train loss: 2.0631
[09/26 01:59:18 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1690, average loss: 3.0959
[09/26 01:59:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.00	top5: 54.00	
[09/26 01:59:18 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 01:59:25 visual_prompt]: Epoch 46 / 100: avg data time: 5.81e-02, avg batch time: 0.5061, average train loss: 1.5951
[09/26 01:59:26 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 3.3273
[09/26 01:59:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 28.50	top5: 67.00	
[09/26 01:59:26 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 01:59:33 visual_prompt]: Epoch 47 / 100: avg data time: 4.50e-02, avg batch time: 0.4949, average train loss: 1.6001
[09/26 01:59:35 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 2.7468
[09/26 01:59:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 34.00	top5: 69.50	
[09/26 01:59:35 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 01:59:41 visual_prompt]: Epoch 48 / 100: avg data time: 4.61e-02, avg batch time: 0.4948, average train loss: 1.2044
[09/26 01:59:43 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 2.1819
[09/26 01:59:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 75.00	
[09/26 01:59:43 visual_prompt]: Best epoch 48: best metric: 0.455
[09/26 01:59:43 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 01:59:49 visual_prompt]: Epoch 49 / 100: avg data time: 4.29e-02, avg batch time: 0.4921, average train loss: 0.9237
[09/26 01:59:51 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1695, average loss: 2.6553
[09/26 01:59:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.50	top5: 71.00	
[09/26 01:59:51 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 01:59:58 visual_prompt]: Epoch 50 / 100: avg data time: 5.56e-02, avg batch time: 0.5036, average train loss: 0.7466
[09/26 01:59:59 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1689, average loss: 2.2481
[09/26 01:59:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 78.50	
[09/26 01:59:59 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 02:00:06 visual_prompt]: Epoch 51 / 100: avg data time: 5.08e-02, avg batch time: 0.4999, average train loss: 0.4870
[09/26 02:00:07 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1690, average loss: 2.0089
[09/26 02:00:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 81.00	
[09/26 02:00:07 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 02:00:14 visual_prompt]: Epoch 52 / 100: avg data time: 5.17e-02, avg batch time: 0.5000, average train loss: 0.2855
[09/26 02:00:16 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1694, average loss: 1.9345
[09/26 02:00:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 82.50	
[09/26 02:00:16 visual_prompt]: Best epoch 52: best metric: 0.475
[09/26 02:00:16 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 02:00:22 visual_prompt]: Epoch 53 / 100: avg data time: 4.89e-02, avg batch time: 0.4988, average train loss: 0.2040
[09/26 02:00:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1695, average loss: 1.8371
[09/26 02:00:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 81.00	
[09/26 02:00:24 visual_prompt]: Best epoch 53: best metric: 0.500
[09/26 02:00:24 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 02:00:31 visual_prompt]: Epoch 54 / 100: avg data time: 4.77e-02, avg batch time: 0.4963, average train loss: 0.1502
[09/26 02:00:32 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1694, average loss: 1.8158
[09/26 02:00:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 84.00	
[09/26 02:00:32 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 02:00:39 visual_prompt]: Epoch 55 / 100: avg data time: 4.55e-02, avg batch time: 0.4955, average train loss: 0.1467
[09/26 02:00:40 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1693, average loss: 1.6966
[09/26 02:00:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 84.50	
[09/26 02:00:40 visual_prompt]: Best epoch 55: best metric: 0.520
[09/26 02:00:40 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 02:00:47 visual_prompt]: Epoch 56 / 100: avg data time: 4.85e-02, avg batch time: 0.4976, average train loss: 0.1458
[09/26 02:00:49 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1695, average loss: 1.6843
[09/26 02:00:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 84.50	
[09/26 02:00:49 visual_prompt]: Best epoch 56: best metric: 0.565
[09/26 02:00:49 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 02:00:55 visual_prompt]: Epoch 57 / 100: avg data time: 4.82e-02, avg batch time: 0.4973, average train loss: 0.1199
[09/26 02:00:57 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1688, average loss: 1.7284
[09/26 02:00:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 86.00	
[09/26 02:00:57 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 02:01:03 visual_prompt]: Epoch 58 / 100: avg data time: 4.25e-02, avg batch time: 0.4922, average train loss: 0.1047
[09/26 02:01:05 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1695, average loss: 1.5941
[09/26 02:01:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.50	
[09/26 02:01:05 visual_prompt]: Best epoch 58: best metric: 0.570
[09/26 02:01:05 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 02:01:12 visual_prompt]: Epoch 59 / 100: avg data time: 4.56e-02, avg batch time: 0.4935, average train loss: 0.0923
[09/26 02:01:13 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 1.6480
[09/26 02:01:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.00	
[09/26 02:01:13 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 02:01:20 visual_prompt]: Epoch 60 / 100: avg data time: 5.03e-02, avg batch time: 0.4986, average train loss: 0.0881
[09/26 02:01:22 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1698, average loss: 1.6680
[09/26 02:01:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 85.00	
[09/26 02:01:22 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 02:01:29 visual_prompt]: Epoch 61 / 100: avg data time: 6.38e-02, avg batch time: 0.5123, average train loss: 0.1055
[09/26 02:01:30 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1694, average loss: 1.5500
[09/26 02:01:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 85.50	
[09/26 02:01:30 visual_prompt]: Best epoch 61: best metric: 0.585
[09/26 02:01:30 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 02:01:37 visual_prompt]: Epoch 62 / 100: avg data time: 4.97e-02, avg batch time: 0.4989, average train loss: 0.0819
[09/26 02:01:38 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1691, average loss: 1.6199
[09/26 02:01:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 86.00	
[09/26 02:01:38 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 02:01:45 visual_prompt]: Epoch 63 / 100: avg data time: 5.23e-02, avg batch time: 0.5013, average train loss: 0.0723
[09/26 02:01:47 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 1.6026
[09/26 02:01:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.00	
[09/26 02:01:47 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 02:01:54 visual_prompt]: Epoch 64 / 100: avg data time: 6.15e-02, avg batch time: 0.5102, average train loss: 0.0907
[09/26 02:01:55 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1686, average loss: 1.5444
[09/26 02:01:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 87.00	
[09/26 02:01:55 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 02:02:02 visual_prompt]: Epoch 65 / 100: avg data time: 4.56e-02, avg batch time: 0.4970, average train loss: 0.1092
[09/26 02:02:03 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 1.6902
[09/26 02:02:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 87.00	
[09/26 02:02:03 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 02:02:10 visual_prompt]: Epoch 66 / 100: avg data time: 4.65e-02, avg batch time: 0.4946, average train loss: 0.1232
[09/26 02:02:12 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1694, average loss: 1.6460
[09/26 02:02:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 85.50	
[09/26 02:02:12 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 02:02:19 visual_prompt]: Epoch 67 / 100: avg data time: 4.77e-02, avg batch time: 0.4962, average train loss: 0.1135
[09/26 02:02:20 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1694, average loss: 1.5707
[09/26 02:02:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 85.50	
[09/26 02:02:20 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 02:02:27 visual_prompt]: Epoch 68 / 100: avg data time: 5.74e-02, avg batch time: 0.5054, average train loss: 0.0737
[09/26 02:02:28 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1688, average loss: 1.5377
[09/26 02:02:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 02:02:28 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 02:02:35 visual_prompt]: Epoch 69 / 100: avg data time: 6.51e-02, avg batch time: 0.5132, average train loss: 0.0497
[09/26 02:02:37 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1692, average loss: 1.5063
[09/26 02:02:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 87.50	
[09/26 02:02:37 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 02:02:44 visual_prompt]: Epoch 70 / 100: avg data time: 5.53e-02, avg batch time: 0.5046, average train loss: 0.0462
[09/26 02:02:45 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 1.4896
[09/26 02:02:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 88.00	
[09/26 02:02:45 visual_prompt]: Best epoch 70: best metric: 0.600
[09/26 02:02:45 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 02:02:52 visual_prompt]: Epoch 71 / 100: avg data time: 5.66e-02, avg batch time: 0.5062, average train loss: 0.0399
[09/26 02:02:54 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1685, average loss: 1.5835
[09/26 02:02:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 87.00	
[09/26 02:02:54 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 02:03:00 visual_prompt]: Epoch 72 / 100: avg data time: 5.62e-02, avg batch time: 0.5051, average train loss: 0.0306
[09/26 02:03:02 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1688, average loss: 1.5682
[09/26 02:03:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 02:03:02 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 02:03:09 visual_prompt]: Epoch 73 / 100: avg data time: 5.24e-02, avg batch time: 0.5005, average train loss: 0.0305
[09/26 02:03:10 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1690, average loss: 1.6057
[09/26 02:03:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.50	
[09/26 02:03:10 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 02:03:17 visual_prompt]: Epoch 74 / 100: avg data time: 6.25e-02, avg batch time: 0.5113, average train loss: 0.0315
[09/26 02:03:19 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1695, average loss: 1.7352
[09/26 02:03:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 85.00	
[09/26 02:03:19 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 02:03:26 visual_prompt]: Epoch 75 / 100: avg data time: 6.43e-02, avg batch time: 0.5132, average train loss: 0.0658
[09/26 02:03:28 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1689, average loss: 1.7063
[09/26 02:03:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 83.50	
[09/26 02:03:28 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 02:03:34 visual_prompt]: Epoch 76 / 100: avg data time: 6.68e-02, avg batch time: 0.5146, average train loss: 0.0819
[09/26 02:03:36 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 1.6997
[09/26 02:03:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 02:03:36 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 02:03:43 visual_prompt]: Epoch 77 / 100: avg data time: 7.48e-02, avg batch time: 0.5223, average train loss: 0.1703
[09/26 02:03:45 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 1.8046
[09/26 02:03:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 85.50	
[09/26 02:03:45 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 02:03:51 visual_prompt]: Epoch 78 / 100: avg data time: 5.73e-02, avg batch time: 0.5058, average train loss: 0.1482
[09/26 02:03:53 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1692, average loss: 1.6319
[09/26 02:03:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 84.50	
[09/26 02:03:53 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 02:04:00 visual_prompt]: Epoch 79 / 100: avg data time: 4.94e-02, avg batch time: 0.4975, average train loss: 0.1069
[09/26 02:04:02 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1690, average loss: 1.6656
[09/26 02:04:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.00	
[09/26 02:04:02 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 02:04:09 visual_prompt]: Epoch 80 / 100: avg data time: 6.63e-02, avg batch time: 0.5138, average train loss: 0.0561
[09/26 02:04:10 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1690, average loss: 1.6615
[09/26 02:04:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 86.50	
[09/26 02:04:10 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 02:04:17 visual_prompt]: Epoch 81 / 100: avg data time: 6.35e-02, avg batch time: 0.5117, average train loss: 0.0353
[09/26 02:04:19 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 1.6411
[09/26 02:04:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 88.00	
[09/26 02:04:19 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 02:04:26 visual_prompt]: Epoch 82 / 100: avg data time: 5.71e-02, avg batch time: 0.5057, average train loss: 0.0269
[09/26 02:04:27 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1692, average loss: 1.6247
[09/26 02:04:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 88.50	
[09/26 02:04:27 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 02:04:34 visual_prompt]: Epoch 83 / 100: avg data time: 4.22e-02, avg batch time: 0.4910, average train loss: 0.0215
[09/26 02:04:35 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 1.6682
[09/26 02:04:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 86.50	
[09/26 02:04:35 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 02:04:42 visual_prompt]: Epoch 84 / 100: avg data time: 4.94e-02, avg batch time: 0.4977, average train loss: 0.0190
[09/26 02:04:44 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1690, average loss: 1.6472
[09/26 02:04:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 88.00	
[09/26 02:04:44 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 02:04:51 visual_prompt]: Epoch 85 / 100: avg data time: 6.29e-02, avg batch time: 0.5105, average train loss: 0.0176
[09/26 02:04:52 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 1.6396
[09/26 02:04:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 87.00	
[09/26 02:04:52 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 02:04:59 visual_prompt]: Epoch 86 / 100: avg data time: 5.29e-02, avg batch time: 0.5007, average train loss: 0.0171
[09/26 02:05:00 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1693, average loss: 1.6558
[09/26 02:05:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 87.50	
[09/26 02:05:00 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 02:05:07 visual_prompt]: Epoch 87 / 100: avg data time: 5.36e-02, avg batch time: 0.5021, average train loss: 0.0170
[09/26 02:05:09 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1690, average loss: 1.6584
[09/26 02:05:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 88.00	
[09/26 02:05:09 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 02:05:15 visual_prompt]: Epoch 88 / 100: avg data time: 4.93e-02, avg batch time: 0.4995, average train loss: 0.0170
[09/26 02:05:17 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1691, average loss: 1.6525
[09/26 02:05:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 88.00	
[09/26 02:05:17 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 02:05:24 visual_prompt]: Epoch 89 / 100: avg data time: 6.22e-02, avg batch time: 0.5118, average train loss: 0.0167
[09/26 02:05:25 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1688, average loss: 1.6532
[09/26 02:05:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 88.00	
[09/26 02:05:25 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 02:05:32 visual_prompt]: Epoch 90 / 100: avg data time: 6.18e-02, avg batch time: 0.5104, average train loss: 0.0164
[09/26 02:05:34 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 1.6618
[09/26 02:05:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 88.00	
[09/26 02:05:34 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 02:05:41 visual_prompt]: Epoch 91 / 100: avg data time: 6.46e-02, avg batch time: 0.5127, average train loss: 0.0165
[09/26 02:05:42 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1686, average loss: 1.6646
[09/26 02:05:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 87.50	
[09/26 02:05:42 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 02:05:49 visual_prompt]: Epoch 92 / 100: avg data time: 6.98e-02, avg batch time: 0.5174, average train loss: 0.0162
[09/26 02:05:51 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1694, average loss: 1.6620
[09/26 02:05:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.50	
[09/26 02:05:51 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 02:05:58 visual_prompt]: Epoch 93 / 100: avg data time: 6.55e-02, avg batch time: 0.5132, average train loss: 0.0162
[09/26 02:05:59 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 1.6628
[09/26 02:05:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 02:05:59 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 02:06:06 visual_prompt]: Epoch 94 / 100: avg data time: 5.40e-02, avg batch time: 0.5031, average train loss: 0.0161
[09/26 02:06:08 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1690, average loss: 1.6651
[09/26 02:06:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 02:06:08 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 02:06:14 visual_prompt]: Epoch 95 / 100: avg data time: 6.54e-02, avg batch time: 0.5140, average train loss: 0.0163
[09/26 02:06:16 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1688, average loss: 1.6657
[09/26 02:06:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 02:06:16 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 02:06:23 visual_prompt]: Epoch 96 / 100: avg data time: 6.31e-02, avg batch time: 0.5126, average train loss: 0.0161
[09/26 02:06:24 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 1.6680
[09/26 02:06:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 02:06:24 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 02:06:31 visual_prompt]: Epoch 97 / 100: avg data time: 6.39e-02, avg batch time: 0.5116, average train loss: 0.0162
[09/26 02:06:33 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1691, average loss: 1.6680
[09/26 02:06:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 02:06:33 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 02:06:40 visual_prompt]: Epoch 98 / 100: avg data time: 4.84e-02, avg batch time: 0.4979, average train loss: 0.0160
[09/26 02:06:41 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1691, average loss: 1.6679
[09/26 02:06:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.50	
[09/26 02:06:41 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 02:06:48 visual_prompt]: Epoch 99 / 100: avg data time: 4.67e-02, avg batch time: 0.4951, average train loss: 0.0159
[09/26 02:06:49 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 1.6679
[09/26 02:06:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.50	
[09/26 02:06:49 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 02:06:56 visual_prompt]: Epoch 100 / 100: avg data time: 5.53e-02, avg batch time: 0.5043, average train loss: 0.0164
[09/26 02:06:57 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 1.6680
[09/26 02:06:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 02:06:58 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 02:06:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 02:06:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 02:06:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 02:06:58 visual_prompt]: Training with config:
[09/26 02:06:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 02:06:58 visual_prompt]: Loading training data...
[09/26 02:06:58 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 02:07:00 visual_prompt]: Number of images: 800
[09/26 02:07:00 visual_prompt]: Number of classes: 47 / 47
[09/26 02:07:00 visual_prompt]: Loading validation data...
[09/26 02:07:00 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 02:07:00 visual_prompt]: Number of images: 200
[09/26 02:07:00 visual_prompt]: Number of classes: 47 / 47
[09/26 02:07:00 visual_prompt]: Constructing models...
[09/26 02:07:03 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 02:07:03 visual_prompt]: tuned percent:0.576
[09/26 02:07:03 visual_prompt]: Device used for model: 0
[09/26 02:07:03 visual_prompt]: Setting up Evaluator...
[09/26 02:07:03 visual_prompt]: Setting up Trainer...
[09/26 02:07:03 visual_prompt]: 	Setting up the optimizer...
[09/26 02:07:03 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 02:07:09 visual_prompt]: Epoch 1 / 100: avg data time: 5.24e-02, avg batch time: 0.4988, average train loss: 3.9254
[09/26 02:07:11 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1686, average loss: 3.9045
[09/26 02:07:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 02:07:11 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 02:07:11 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 02:07:18 visual_prompt]: Epoch 2 / 100: avg data time: 5.76e-02, avg batch time: 0.5051, average train loss: 3.8801
[09/26 02:07:19 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1686, average loss: 3.9557
[09/26 02:07:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 02:07:19 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 02:07:26 visual_prompt]: Epoch 3 / 100: avg data time: 4.96e-02, avg batch time: 0.4972, average train loss: 3.8417
[09/26 02:07:28 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1689, average loss: 3.8709
[09/26 02:07:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 15.50	
[09/26 02:07:28 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 02:07:34 visual_prompt]: Epoch 4 / 100: avg data time: 4.49e-02, avg batch time: 0.4929, average train loss: 3.8631
[09/26 02:07:36 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1692, average loss: 3.8179
[09/26 02:07:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 19.50	
[09/26 02:07:36 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 02:07:43 visual_prompt]: Epoch 5 / 100: avg data time: 4.59e-02, avg batch time: 0.4959, average train loss: 3.7601
[09/26 02:07:44 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1688, average loss: 3.8612
[09/26 02:07:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 19.00	
[09/26 02:07:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 02:07:51 visual_prompt]: Epoch 6 / 100: avg data time: 6.06e-02, avg batch time: 0.5082, average train loss: 3.4783
[09/26 02:07:52 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1689, average loss: 3.7759
[09/26 02:07:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.50	top5: 27.00	
[09/26 02:07:52 visual_prompt]: Best epoch 6: best metric: 0.085
[09/26 02:07:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 02:07:59 visual_prompt]: Epoch 7 / 100: avg data time: 4.85e-02, avg batch time: 0.4969, average train loss: 3.5640
[09/26 02:08:01 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 3.4876
[09/26 02:08:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.00	top5: 37.00	
[09/26 02:08:01 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 02:08:08 visual_prompt]: Epoch 8 / 100: avg data time: 6.49e-02, avg batch time: 0.5124, average train loss: 3.2051
[09/26 02:08:09 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1685, average loss: 3.2664
[09/26 02:08:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 21.50	top5: 42.50	
[09/26 02:08:09 visual_prompt]: Best epoch 8: best metric: 0.215
[09/26 02:08:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 02:08:16 visual_prompt]: Epoch 9 / 100: avg data time: 6.27e-02, avg batch time: 0.5113, average train loss: 2.4697
[09/26 02:08:18 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1686, average loss: 3.2297
[09/26 02:08:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.50	top5: 55.50	
[09/26 02:08:18 visual_prompt]: Best epoch 9: best metric: 0.255
[09/26 02:08:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 02:08:25 visual_prompt]: Epoch 10 / 100: avg data time: 4.06e-02, avg batch time: 0.4894, average train loss: 1.8175
[09/26 02:08:26 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1690, average loss: 2.7737
[09/26 02:08:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.50	top5: 69.00	
[09/26 02:08:26 visual_prompt]: Best epoch 10: best metric: 0.375
[09/26 02:08:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 02:08:33 visual_prompt]: Epoch 11 / 100: avg data time: 5.09e-02, avg batch time: 0.4990, average train loss: 1.1472
[09/26 02:08:35 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1693, average loss: 2.3111
[09/26 02:08:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 77.50	
[09/26 02:08:35 visual_prompt]: Best epoch 11: best metric: 0.435
[09/26 02:08:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 02:08:41 visual_prompt]: Epoch 12 / 100: avg data time: 5.40e-02, avg batch time: 0.5029, average train loss: 0.5497
[09/26 02:08:43 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1691, average loss: 2.5370
[09/26 02:08:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 82.00	
[09/26 02:08:43 visual_prompt]: Best epoch 12: best metric: 0.475
[09/26 02:08:43 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 02:08:50 visual_prompt]: Epoch 13 / 100: avg data time: 5.67e-02, avg batch time: 0.5051, average train loss: 0.4307
[09/26 02:08:51 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1690, average loss: 2.7984
[09/26 02:08:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 81.00	
[09/26 02:08:51 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 02:08:58 visual_prompt]: Epoch 14 / 100: avg data time: 5.09e-02, avg batch time: 0.4996, average train loss: 0.3927
[09/26 02:08:59 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1687, average loss: 2.6462
[09/26 02:08:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 77.50	
[09/26 02:08:59 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 02:09:06 visual_prompt]: Epoch 15 / 100: avg data time: 6.08e-02, avg batch time: 0.5093, average train loss: 0.1547
[09/26 02:09:08 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 2.7710
[09/26 02:09:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 82.00	
[09/26 02:09:08 visual_prompt]: Best epoch 15: best metric: 0.505
[09/26 02:09:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 02:09:15 visual_prompt]: Epoch 16 / 100: avg data time: 6.92e-02, avg batch time: 0.5168, average train loss: 0.0673
[09/26 02:09:16 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1691, average loss: 2.5658
[09/26 02:09:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 02:09:16 visual_prompt]: Best epoch 16: best metric: 0.545
[09/26 02:09:16 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 02:09:23 visual_prompt]: Epoch 17 / 100: avg data time: 4.58e-02, avg batch time: 0.4936, average train loss: 0.0481
[09/26 02:09:25 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1693, average loss: 2.6312
[09/26 02:09:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 82.00	
[09/26 02:09:25 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 02:09:31 visual_prompt]: Epoch 18 / 100: avg data time: 5.90e-02, avg batch time: 0.5073, average train loss: 0.0213
[09/26 02:09:33 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1691, average loss: 2.3800
[09/26 02:09:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 85.50	
[09/26 02:09:33 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 02:09:40 visual_prompt]: Epoch 19 / 100: avg data time: 5.91e-02, avg batch time: 0.5085, average train loss: 0.0080
[09/26 02:09:41 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1692, average loss: 2.2624
[09/26 02:09:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 86.00	
[09/26 02:09:41 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 02:09:48 visual_prompt]: Epoch 20 / 100: avg data time: 4.87e-02, avg batch time: 0.4968, average train loss: 0.0054
[09/26 02:09:49 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 2.0870
[09/26 02:09:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 02:09:49 visual_prompt]: Best epoch 20: best metric: 0.560
[09/26 02:09:49 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 02:09:56 visual_prompt]: Epoch 21 / 100: avg data time: 5.51e-02, avg batch time: 0.5030, average train loss: 0.0036
[09/26 02:09:58 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1689, average loss: 2.0601
[09/26 02:09:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.50	
[09/26 02:09:58 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 02:10:05 visual_prompt]: Epoch 22 / 100: avg data time: 6.88e-02, avg batch time: 0.5167, average train loss: 0.0031
[09/26 02:10:06 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1688, average loss: 2.0295
[09/26 02:10:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 86.50	
[09/26 02:10:06 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 02:10:13 visual_prompt]: Epoch 23 / 100: avg data time: 4.56e-02, avg batch time: 0.4933, average train loss: 0.0028
[09/26 02:10:14 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 1.9794
[09/26 02:10:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.00	
[09/26 02:10:14 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 02:10:21 visual_prompt]: Epoch 24 / 100: avg data time: 5.52e-02, avg batch time: 0.5045, average train loss: 0.0026
[09/26 02:10:23 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 1.9220
[09/26 02:10:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 87.00	
[09/26 02:10:23 visual_prompt]: Best epoch 24: best metric: 0.570
[09/26 02:10:23 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 02:10:30 visual_prompt]: Epoch 25 / 100: avg data time: 5.64e-02, avg batch time: 0.5068, average train loss: 0.0028
[09/26 02:10:31 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 1.8839
[09/26 02:10:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 87.50	
[09/26 02:10:31 visual_prompt]: Best epoch 25: best metric: 0.580
[09/26 02:10:31 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 02:10:38 visual_prompt]: Epoch 26 / 100: avg data time: 6.24e-02, avg batch time: 0.5103, average train loss: 0.0029
[09/26 02:10:39 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1694, average loss: 1.8658
[09/26 02:10:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 87.50	
[09/26 02:10:39 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 02:10:46 visual_prompt]: Epoch 27 / 100: avg data time: 6.21e-02, avg batch time: 0.5098, average train loss: 0.0031
[09/26 02:10:48 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 1.8237
[09/26 02:10:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.00	
[09/26 02:10:48 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 02:10:55 visual_prompt]: Epoch 28 / 100: avg data time: 6.21e-02, avg batch time: 0.5106, average train loss: 0.0033
[09/26 02:10:56 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1691, average loss: 1.7945
[09/26 02:10:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 87.50	
[09/26 02:10:56 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 02:11:03 visual_prompt]: Epoch 29 / 100: avg data time: 5.48e-02, avg batch time: 0.5027, average train loss: 0.0036
[09/26 02:11:04 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 1.7680
[09/26 02:11:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 87.50	
[09/26 02:11:04 visual_prompt]: Best epoch 29: best metric: 0.585
[09/26 02:11:04 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 02:11:11 visual_prompt]: Epoch 30 / 100: avg data time: 5.65e-02, avg batch time: 0.5047, average train loss: 0.0038
[09/26 02:11:13 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1694, average loss: 1.7730
[09/26 02:11:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 86.50	
[09/26 02:11:13 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 02:11:20 visual_prompt]: Epoch 31 / 100: avg data time: 6.32e-02, avg batch time: 0.5111, average train loss: 0.0042
[09/26 02:11:21 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1693, average loss: 1.7237
[09/26 02:11:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 87.00	
[09/26 02:11:21 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 02:11:28 visual_prompt]: Epoch 32 / 100: avg data time: 6.49e-02, avg batch time: 0.5127, average train loss: 0.0044
[09/26 02:11:29 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1693, average loss: 1.7035
[09/26 02:11:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 86.50	
[09/26 02:11:30 visual_prompt]: Best epoch 32: best metric: 0.590
[09/26 02:11:30 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 02:11:36 visual_prompt]: Epoch 33 / 100: avg data time: 5.90e-02, avg batch time: 0.5077, average train loss: 0.0046
[09/26 02:11:38 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1694, average loss: 1.6804
[09/26 02:11:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 87.00	
[09/26 02:11:38 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 02:11:45 visual_prompt]: Epoch 34 / 100: avg data time: 4.46e-02, avg batch time: 0.4937, average train loss: 0.0049
[09/26 02:11:46 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1690, average loss: 1.6743
[09/26 02:11:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.00	
[09/26 02:11:46 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 02:11:53 visual_prompt]: Epoch 35 / 100: avg data time: 4.35e-02, avg batch time: 0.4928, average train loss: 0.0050
[09/26 02:11:54 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 1.6504
[09/26 02:11:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 86.00	
[09/26 02:11:54 visual_prompt]: Best epoch 35: best metric: 0.595
[09/26 02:11:54 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 02:12:01 visual_prompt]: Epoch 36 / 100: avg data time: 5.85e-02, avg batch time: 0.5071, average train loss: 0.0052
[09/26 02:12:03 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1690, average loss: 1.6339
[09/26 02:12:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.50	
[09/26 02:12:03 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 02:12:09 visual_prompt]: Epoch 37 / 100: avg data time: 5.80e-02, avg batch time: 0.5066, average train loss: 0.0051
[09/26 02:12:11 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 1.6316
[09/26 02:12:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.50	
[09/26 02:12:11 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 02:12:18 visual_prompt]: Epoch 38 / 100: avg data time: 5.40e-02, avg batch time: 0.5033, average train loss: 0.0053
[09/26 02:12:19 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1692, average loss: 1.6190
[09/26 02:12:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 87.00	
[09/26 02:12:19 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 02:12:26 visual_prompt]: Epoch 39 / 100: avg data time: 6.64e-02, avg batch time: 0.5141, average train loss: 0.0055
[09/26 02:12:28 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1695, average loss: 1.5907
[09/26 02:12:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 87.50	
[09/26 02:12:28 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 02:12:35 visual_prompt]: Epoch 40 / 100: avg data time: 6.00e-02, avg batch time: 0.5079, average train loss: 0.0058
[09/26 02:12:36 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1693, average loss: 1.5779
[09/26 02:12:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 87.50	
[09/26 02:12:36 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 02:12:43 visual_prompt]: Epoch 41 / 100: avg data time: 5.67e-02, avg batch time: 0.5044, average train loss: 0.0058
[09/26 02:12:44 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1694, average loss: 1.5957
[09/26 02:12:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 88.00	
[09/26 02:12:44 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 02:12:51 visual_prompt]: Epoch 42 / 100: avg data time: 4.98e-02, avg batch time: 0.5015, average train loss: 0.0057
[09/26 02:12:53 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1692, average loss: 1.6251
[09/26 02:12:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 87.00	
[09/26 02:12:53 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 02:12:59 visual_prompt]: Epoch 43 / 100: avg data time: 4.81e-02, avg batch time: 0.4974, average train loss: 0.0058
[09/26 02:13:01 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 1.6041
[09/26 02:13:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 87.00	
[09/26 02:13:01 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 02:13:08 visual_prompt]: Epoch 44 / 100: avg data time: 6.02e-02, avg batch time: 0.5080, average train loss: 0.0062
[09/26 02:13:09 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1690, average loss: 1.5930
[09/26 02:13:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 88.00	
[09/26 02:13:09 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 02:13:16 visual_prompt]: Epoch 45 / 100: avg data time: 5.59e-02, avg batch time: 0.5046, average train loss: 0.0066
[09/26 02:13:18 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 1.6020
[09/26 02:13:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 87.50	
[09/26 02:13:18 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 02:13:24 visual_prompt]: Epoch 46 / 100: avg data time: 5.72e-02, avg batch time: 0.5050, average train loss: 0.0063
[09/26 02:13:26 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 1.5860
[09/26 02:13:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 87.50	
[09/26 02:13:26 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 02:13:33 visual_prompt]: Epoch 47 / 100: avg data time: 4.05e-02, avg batch time: 0.4902, average train loss: 0.0058
[09/26 02:13:34 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1688, average loss: 1.5865
[09/26 02:13:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 88.00	
[09/26 02:13:34 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 02:13:41 visual_prompt]: Epoch 48 / 100: avg data time: 4.78e-02, avg batch time: 0.4969, average train loss: 0.0056
[09/26 02:13:42 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.1692, average loss: 1.5772
[09/26 02:13:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 87.50	
[09/26 02:13:42 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 02:13:49 visual_prompt]: Epoch 49 / 100: avg data time: 6.08e-02, avg batch time: 0.5084, average train loss: 0.0053
[09/26 02:13:51 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 1.5928
[09/26 02:13:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 02:13:51 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 02:13:58 visual_prompt]: Epoch 50 / 100: avg data time: 5.72e-02, avg batch time: 0.5058, average train loss: 0.0057
[09/26 02:13:59 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1690, average loss: 1.6111
[09/26 02:13:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 87.50	
[09/26 02:13:59 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 02:14:06 visual_prompt]: Epoch 51 / 100: avg data time: 5.94e-02, avg batch time: 0.5081, average train loss: 0.0056
[09/26 02:14:07 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1689, average loss: 1.5867
[09/26 02:14:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 88.50	
[09/26 02:14:07 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 02:14:14 visual_prompt]: Epoch 52 / 100: avg data time: 5.27e-02, avg batch time: 0.5017, average train loss: 0.0054
[09/26 02:14:16 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1694, average loss: 1.6039
[09/26 02:14:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.50	
[09/26 02:14:16 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 02:14:22 visual_prompt]: Epoch 53 / 100: avg data time: 5.29e-02, avg batch time: 0.5022, average train loss: 0.0055
[09/26 02:14:24 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 1.5882
[09/26 02:14:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.50	
[09/26 02:14:24 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 02:14:31 visual_prompt]: Epoch 54 / 100: avg data time: 4.84e-02, avg batch time: 0.4983, average train loss: 0.0052
[09/26 02:14:32 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 1.5788
[09/26 02:14:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 87.50	
[09/26 02:14:32 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 02:14:39 visual_prompt]: Epoch 55 / 100: avg data time: 5.64e-02, avg batch time: 0.5048, average train loss: 0.0052
[09/26 02:14:41 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 1.6080
[09/26 02:14:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 87.50	
[09/26 02:14:41 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 02:14:47 visual_prompt]: Epoch 56 / 100: avg data time: 5.83e-02, avg batch time: 0.5090, average train loss: 0.0053
[09/26 02:14:49 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1690, average loss: 1.6011
[09/26 02:14:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.50	
[09/26 02:14:49 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 02:14:56 visual_prompt]: Epoch 57 / 100: avg data time: 5.41e-02, avg batch time: 0.5036, average train loss: 0.0055
[09/26 02:14:57 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1692, average loss: 1.6131
[09/26 02:14:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 02:14:57 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 02:15:04 visual_prompt]: Epoch 58 / 100: avg data time: 5.48e-02, avg batch time: 0.5055, average train loss: 0.0053
[09/26 02:15:06 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 1.5890
[09/26 02:15:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 87.00	
[09/26 02:15:06 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 02:15:12 visual_prompt]: Epoch 59 / 100: avg data time: 4.68e-02, avg batch time: 0.4962, average train loss: 0.0051
[09/26 02:15:14 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 1.6143
[09/26 02:15:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.00	
[09/26 02:15:14 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 02:15:21 visual_prompt]: Epoch 60 / 100: avg data time: 4.62e-02, avg batch time: 0.4949, average train loss: 0.0050
[09/26 02:15:22 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1691, average loss: 1.6100
[09/26 02:15:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 02:15:22 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 02:15:29 visual_prompt]: Epoch 61 / 100: avg data time: 5.19e-02, avg batch time: 0.5004, average train loss: 0.0049
[09/26 02:15:30 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 1.5956
[09/26 02:15:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.50	
[09/26 02:15:30 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 02:15:37 visual_prompt]: Epoch 62 / 100: avg data time: 4.84e-02, avg batch time: 0.4966, average train loss: 0.0049
[09/26 02:15:39 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1692, average loss: 1.5887
[09/26 02:15:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 87.00	
[09/26 02:15:39 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 02:15:45 visual_prompt]: Epoch 63 / 100: avg data time: 4.51e-02, avg batch time: 0.4945, average train loss: 0.0048
[09/26 02:15:47 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1693, average loss: 1.6014
[09/26 02:15:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 85.50	
[09/26 02:15:47 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 02:15:54 visual_prompt]: Epoch 64 / 100: avg data time: 4.86e-02, avg batch time: 0.4975, average train loss: 0.0049
[09/26 02:15:55 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1697, average loss: 1.5857
[09/26 02:15:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.50	
[09/26 02:15:55 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 02:16:02 visual_prompt]: Epoch 65 / 100: avg data time: 6.08e-02, avg batch time: 0.5095, average train loss: 0.0048
[09/26 02:16:04 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 1.6141
[09/26 02:16:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 85.50	
[09/26 02:16:04 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 02:16:10 visual_prompt]: Epoch 66 / 100: avg data time: 4.10e-02, avg batch time: 0.4900, average train loss: 0.0051
[09/26 02:16:12 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 1.5759
[09/26 02:16:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 02:16:12 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 02:16:19 visual_prompt]: Epoch 67 / 100: avg data time: 4.39e-02, avg batch time: 0.4941, average train loss: 0.0051
[09/26 02:16:20 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1690, average loss: 1.5932
[09/26 02:16:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 02:16:20 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 02:16:27 visual_prompt]: Epoch 68 / 100: avg data time: 5.19e-02, avg batch time: 0.4999, average train loss: 0.0048
[09/26 02:16:28 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1696, average loss: 1.5658
[09/26 02:16:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 87.00	
[09/26 02:16:28 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 02:16:35 visual_prompt]: Epoch 69 / 100: avg data time: 5.36e-02, avg batch time: 0.5016, average train loss: 0.0046
[09/26 02:16:37 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 1.5864
[09/26 02:16:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 86.50	
[09/26 02:16:37 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 02:16:44 visual_prompt]: Epoch 70 / 100: avg data time: 4.68e-02, avg batch time: 0.4964, average train loss: 0.0045
[09/26 02:16:45 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1694, average loss: 1.6096
[09/26 02:16:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.00	
[09/26 02:16:45 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 02:16:52 visual_prompt]: Epoch 71 / 100: avg data time: 3.81e-02, avg batch time: 0.4880, average train loss: 0.0044
[09/26 02:16:53 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1693, average loss: 1.6046
[09/26 02:16:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 02:16:53 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 02:17:00 visual_prompt]: Epoch 72 / 100: avg data time: 5.56e-02, avg batch time: 0.5045, average train loss: 0.0042
[09/26 02:17:02 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 1.5847
[09/26 02:17:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 87.50	
[09/26 02:17:02 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 02:17:08 visual_prompt]: Epoch 73 / 100: avg data time: 4.73e-02, avg batch time: 0.4978, average train loss: 0.0044
[09/26 02:17:10 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1690, average loss: 1.6084
[09/26 02:17:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.50	
[09/26 02:17:10 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 02:17:17 visual_prompt]: Epoch 74 / 100: avg data time: 5.88e-02, avg batch time: 0.5071, average train loss: 0.0043
[09/26 02:17:18 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1690, average loss: 1.5962
[09/26 02:17:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.50	
[09/26 02:17:18 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 02:17:25 visual_prompt]: Epoch 75 / 100: avg data time: 6.65e-02, avg batch time: 0.5142, average train loss: 0.0044
[09/26 02:17:27 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1690, average loss: 1.6259
[09/26 02:17:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 86.50	
[09/26 02:17:27 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 02:17:34 visual_prompt]: Epoch 76 / 100: avg data time: 4.61e-02, avg batch time: 0.4953, average train loss: 0.0044
[09/26 02:17:35 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1694, average loss: 1.6102
[09/26 02:17:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.00	
[09/26 02:17:35 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 02:17:42 visual_prompt]: Epoch 77 / 100: avg data time: 6.53e-02, avg batch time: 0.5132, average train loss: 0.0042
[09/26 02:17:44 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1693, average loss: 1.6065
[09/26 02:17:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.50	
[09/26 02:17:44 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 02:17:50 visual_prompt]: Epoch 78 / 100: avg data time: 5.01e-02, avg batch time: 0.4985, average train loss: 0.0042
[09/26 02:17:52 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1691, average loss: 1.6213
[09/26 02:17:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 85.50	
[09/26 02:17:52 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 02:17:59 visual_prompt]: Epoch 79 / 100: avg data time: 5.90e-02, avg batch time: 0.5066, average train loss: 0.0041
[09/26 02:18:00 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 1.6148
[09/26 02:18:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 02:18:00 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 02:18:07 visual_prompt]: Epoch 80 / 100: avg data time: 5.13e-02, avg batch time: 0.4998, average train loss: 0.0041
[09/26 02:18:09 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1693, average loss: 1.6206
[09/26 02:18:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 02:18:09 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 02:18:15 visual_prompt]: Epoch 81 / 100: avg data time: 5.42e-02, avg batch time: 0.5020, average train loss: 0.0040
[09/26 02:18:17 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 1.6258
[09/26 02:18:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.00	
[09/26 02:18:17 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 02:18:24 visual_prompt]: Epoch 82 / 100: avg data time: 5.11e-02, avg batch time: 0.5003, average train loss: 0.0040
[09/26 02:18:25 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 1.6170
[09/26 02:18:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.50	
[09/26 02:18:25 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 02:18:32 visual_prompt]: Epoch 83 / 100: avg data time: 5.38e-02, avg batch time: 0.5029, average train loss: 0.0040
[09/26 02:18:34 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 1.6123
[09/26 02:18:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.00	
[09/26 02:18:34 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 02:18:40 visual_prompt]: Epoch 84 / 100: avg data time: 4.67e-02, avg batch time: 0.4970, average train loss: 0.0039
[09/26 02:18:42 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1690, average loss: 1.6176
[09/26 02:18:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.00	
[09/26 02:18:42 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 02:18:49 visual_prompt]: Epoch 85 / 100: avg data time: 5.50e-02, avg batch time: 0.5031, average train loss: 0.0039
[09/26 02:18:50 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1695, average loss: 1.6243
[09/26 02:18:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 02:18:50 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 02:18:57 visual_prompt]: Epoch 86 / 100: avg data time: 5.30e-02, avg batch time: 0.5007, average train loss: 0.0038
[09/26 02:18:59 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1689, average loss: 1.6212
[09/26 02:18:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 02:18:59 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 02:19:05 visual_prompt]: Epoch 87 / 100: avg data time: 6.18e-02, avg batch time: 0.5102, average train loss: 0.0039
[09/26 02:19:07 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1690, average loss: 1.6186
[09/26 02:19:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.00	
[09/26 02:19:07 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 02:19:14 visual_prompt]: Epoch 88 / 100: avg data time: 5.35e-02, avg batch time: 0.5018, average train loss: 0.0039
[09/26 02:19:15 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1692, average loss: 1.6183
[09/26 02:19:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.50	
[09/26 02:19:15 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 02:19:22 visual_prompt]: Epoch 89 / 100: avg data time: 5.90e-02, avg batch time: 0.5071, average train loss: 0.0038
[09/26 02:19:24 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1688, average loss: 1.6210
[09/26 02:19:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.50	
[09/26 02:19:24 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 02:19:30 visual_prompt]: Epoch 90 / 100: avg data time: 4.62e-02, avg batch time: 0.4954, average train loss: 0.0039
[09/26 02:19:32 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1690, average loss: 1.6177
[09/26 02:19:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.00	
[09/26 02:19:32 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 02:19:39 visual_prompt]: Epoch 91 / 100: avg data time: 6.04e-02, avg batch time: 0.5096, average train loss: 0.0038
[09/26 02:19:40 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1690, average loss: 1.6191
[09/26 02:19:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 85.50	
[09/26 02:19:40 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 02:19:47 visual_prompt]: Epoch 92 / 100: avg data time: 5.89e-02, avg batch time: 0.5068, average train loss: 0.0038
[09/26 02:19:49 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1689, average loss: 1.6222
[09/26 02:19:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.50	
[09/26 02:19:49 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 02:19:55 visual_prompt]: Epoch 93 / 100: avg data time: 5.87e-02, avg batch time: 0.5066, average train loss: 0.0038
[09/26 02:19:57 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 1.6198
[09/26 02:19:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 02:19:57 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 02:20:04 visual_prompt]: Epoch 94 / 100: avg data time: 6.23e-02, avg batch time: 0.5107, average train loss: 0.0037
[09/26 02:20:05 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 1.6190
[09/26 02:20:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 86.00	
[09/26 02:20:05 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 02:20:12 visual_prompt]: Epoch 95 / 100: avg data time: 6.89e-02, avg batch time: 0.5168, average train loss: 0.0037
[09/26 02:20:14 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1690, average loss: 1.6196
[09/26 02:20:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 86.00	
[09/26 02:20:14 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 02:20:21 visual_prompt]: Epoch 96 / 100: avg data time: 5.78e-02, avg batch time: 0.5059, average train loss: 0.0038
[09/26 02:20:22 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1691, average loss: 1.6196
[09/26 02:20:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.00	
[09/26 02:20:22 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 02:20:29 visual_prompt]: Epoch 97 / 100: avg data time: 5.16e-02, avg batch time: 0.5011, average train loss: 0.0038
[09/26 02:20:31 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 1.6198
[09/26 02:20:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 02:20:31 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 02:20:38 visual_prompt]: Epoch 98 / 100: avg data time: 7.03e-02, avg batch time: 0.5181, average train loss: 0.0037
[09/26 02:20:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1694, average loss: 1.6197
[09/26 02:20:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 02:20:39 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 02:20:46 visual_prompt]: Epoch 99 / 100: avg data time: 6.81e-02, avg batch time: 0.5157, average train loss: 0.0037
[09/26 02:20:48 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 1.6196
[09/26 02:20:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 02:20:48 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 02:20:55 visual_prompt]: Epoch 100 / 100: avg data time: 6.21e-02, avg batch time: 0.5107, average train loss: 0.0038
[09/26 02:20:56 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1690, average loss: 1.6195
[09/26 02:20:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 02:20:56 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 02:20:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 02:20:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 02:20:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 02:20:56 visual_prompt]: Training with config:
[09/26 02:20:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 02:20:56 visual_prompt]: Loading training data...
[09/26 02:20:56 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 02:20:58 visual_prompt]: Number of images: 800
[09/26 02:20:58 visual_prompt]: Number of classes: 47 / 47
[09/26 02:20:58 visual_prompt]: Loading validation data...
[09/26 02:20:58 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 02:20:59 visual_prompt]: Number of images: 200
[09/26 02:20:59 visual_prompt]: Number of classes: 47 / 47
[09/26 02:20:59 visual_prompt]: Constructing models...
[09/26 02:21:01 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 02:21:01 visual_prompt]: tuned percent:0.576
[09/26 02:21:01 visual_prompt]: Device used for model: 0
[09/26 02:21:01 visual_prompt]: Setting up Evaluator...
[09/26 02:21:01 visual_prompt]: Setting up Trainer...
[09/26 02:21:01 visual_prompt]: 	Setting up the optimizer...
[09/26 02:21:01 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 02:21:08 visual_prompt]: Epoch 1 / 100: avg data time: 5.51e-02, avg batch time: 0.5019, average train loss: 3.9299
[09/26 02:21:10 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1689, average loss: 3.9045
[09/26 02:21:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 02:21:10 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 02:21:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 02:21:16 visual_prompt]: Epoch 2 / 100: avg data time: 4.28e-02, avg batch time: 0.4892, average train loss: 3.8604
[09/26 02:21:18 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1686, average loss: 3.9518
[09/26 02:21:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/26 02:21:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 02:21:24 visual_prompt]: Epoch 3 / 100: avg data time: 4.39e-02, avg batch time: 0.4926, average train loss: 3.8236
[09/26 02:21:26 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1688, average loss: 3.8671
[09/26 02:21:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 13.50	
[09/26 02:21:26 visual_prompt]: Best epoch 3: best metric: 0.050
[09/26 02:21:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 02:21:33 visual_prompt]: Epoch 4 / 100: avg data time: 5.23e-02, avg batch time: 0.5008, average train loss: 3.9296
[09/26 02:21:34 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1688, average loss: 3.9520
[09/26 02:21:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 02:21:34 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 02:21:41 visual_prompt]: Epoch 5 / 100: avg data time: 4.26e-02, avg batch time: 0.4911, average train loss: 3.8764
[09/26 02:21:42 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1687, average loss: 3.7803
[09/26 02:21:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 19.50	
[09/26 02:21:42 visual_prompt]: Best epoch 5: best metric: 0.065
[09/26 02:21:42 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 02:21:49 visual_prompt]: Epoch 6 / 100: avg data time: 5.77e-02, avg batch time: 0.5047, average train loss: 3.6365
[09/26 02:21:51 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1691, average loss: 3.9581
[09/26 02:21:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 15.50	
[09/26 02:21:51 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 02:21:57 visual_prompt]: Epoch 7 / 100: avg data time: 5.43e-02, avg batch time: 0.5023, average train loss: 3.3926
[09/26 02:21:59 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1690, average loss: 3.4241
[09/26 02:21:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.50	top5: 43.00	
[09/26 02:21:59 visual_prompt]: Best epoch 7: best metric: 0.135
[09/26 02:21:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 02:22:06 visual_prompt]: Epoch 8 / 100: avg data time: 4.43e-02, avg batch time: 0.4935, average train loss: 2.6391
[09/26 02:22:07 visual_prompt]: Inference (val):avg data time: 4.58e-05, avg batch time: 0.1688, average loss: 2.7632
[09/26 02:22:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 31.00	top5: 61.50	
[09/26 02:22:07 visual_prompt]: Best epoch 8: best metric: 0.310
[09/26 02:22:07 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 02:22:14 visual_prompt]: Epoch 9 / 100: avg data time: 4.22e-02, avg batch time: 0.4915, average train loss: 1.4787
[09/26 02:22:15 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 3.0236
[09/26 02:22:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 35.00	top5: 71.00	
[09/26 02:22:15 visual_prompt]: Best epoch 9: best metric: 0.350
[09/26 02:22:15 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 02:22:22 visual_prompt]: Epoch 10 / 100: avg data time: 4.33e-02, avg batch time: 0.4928, average train loss: 1.2801
[09/26 02:22:23 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1687, average loss: 1.9170
[09/26 02:22:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.00	
[09/26 02:22:23 visual_prompt]: Best epoch 10: best metric: 0.505
[09/26 02:22:23 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 02:22:30 visual_prompt]: Epoch 11 / 100: avg data time: 4.49e-02, avg batch time: 0.4940, average train loss: 0.4560
[09/26 02:22:32 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 2.0588
[09/26 02:22:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 82.50	
[09/26 02:22:32 visual_prompt]: Best epoch 11: best metric: 0.525
[09/26 02:22:32 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 02:22:38 visual_prompt]: Epoch 12 / 100: avg data time: 4.55e-02, avg batch time: 0.4941, average train loss: 0.2158
[09/26 02:22:40 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 1.9901
[09/26 02:22:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 87.00	
[09/26 02:22:40 visual_prompt]: Best epoch 12: best metric: 0.540
[09/26 02:22:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 02:22:47 visual_prompt]: Epoch 13 / 100: avg data time: 6.03e-02, avg batch time: 0.5102, average train loss: 0.1146
[09/26 02:22:48 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1695, average loss: 2.3258
[09/26 02:22:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 81.50	
[09/26 02:22:48 visual_prompt]: Best epoch 13: best metric: 0.570
[09/26 02:22:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 02:22:55 visual_prompt]: Epoch 14 / 100: avg data time: 4.99e-02, avg batch time: 0.4988, average train loss: 0.0796
[09/26 02:22:56 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 2.0714
[09/26 02:22:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 84.50	
[09/26 02:22:56 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 02:23:03 visual_prompt]: Epoch 15 / 100: avg data time: 4.54e-02, avg batch time: 0.4945, average train loss: 0.0315
[09/26 02:23:05 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 2.1833
[09/26 02:23:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 86.00	
[09/26 02:23:05 visual_prompt]: Best epoch 15: best metric: 0.595
[09/26 02:23:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 02:23:11 visual_prompt]: Epoch 16 / 100: avg data time: 5.57e-02, avg batch time: 0.5037, average train loss: 0.0103
[09/26 02:23:13 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1689, average loss: 2.1673
[09/26 02:23:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 85.00	
[09/26 02:23:13 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 02:23:20 visual_prompt]: Epoch 17 / 100: avg data time: 5.75e-02, avg batch time: 0.5073, average train loss: 0.0038
[09/26 02:23:21 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1692, average loss: 2.1909
[09/26 02:23:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 86.50	
[09/26 02:23:21 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 02:23:28 visual_prompt]: Epoch 18 / 100: avg data time: 5.49e-02, avg batch time: 0.5033, average train loss: 0.0022
[09/26 02:23:30 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1694, average loss: 2.2072
[09/26 02:23:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 86.50	
[09/26 02:23:30 visual_prompt]: Best epoch 18: best metric: 0.600
[09/26 02:23:30 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 02:23:36 visual_prompt]: Epoch 19 / 100: avg data time: 4.77e-02, avg batch time: 0.4971, average train loss: 0.0016
[09/26 02:23:38 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1690, average loss: 2.1977
[09/26 02:23:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 86.50	
[09/26 02:23:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 02:23:44 visual_prompt]: Epoch 20 / 100: avg data time: 4.33e-02, avg batch time: 0.4948, average train loss: 0.0015
[09/26 02:23:46 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 2.2118
[09/26 02:23:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 87.00	
[09/26 02:23:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 02:23:53 visual_prompt]: Epoch 21 / 100: avg data time: 5.71e-02, avg batch time: 0.5061, average train loss: 0.0012
[09/26 02:23:54 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 2.2239
[09/26 02:23:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 86.50	
[09/26 02:23:54 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 02:24:01 visual_prompt]: Epoch 22 / 100: avg data time: 5.08e-02, avg batch time: 0.5024, average train loss: 0.0010
[09/26 02:24:03 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1689, average loss: 2.2346
[09/26 02:24:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 86.50	
[09/26 02:24:03 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 02:24:09 visual_prompt]: Epoch 23 / 100: avg data time: 4.85e-02, avg batch time: 0.4965, average train loss: 0.0009
[09/26 02:24:11 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1691, average loss: 2.2382
[09/26 02:24:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 86.50	
[09/26 02:24:11 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 02:24:18 visual_prompt]: Epoch 24 / 100: avg data time: 6.04e-02, avg batch time: 0.5092, average train loss: 0.0009
[09/26 02:24:19 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 2.2455
[09/26 02:24:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 87.00	
[09/26 02:24:19 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 02:24:26 visual_prompt]: Epoch 25 / 100: avg data time: 5.59e-02, avg batch time: 0.5057, average train loss: 0.0010
[09/26 02:24:28 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1694, average loss: 2.2436
[09/26 02:24:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 87.50	
[09/26 02:24:28 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 02:24:34 visual_prompt]: Epoch 26 / 100: avg data time: 6.16e-02, avg batch time: 0.5096, average train loss: 0.0008
[09/26 02:24:36 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1693, average loss: 2.2416
[09/26 02:24:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 87.50	
[09/26 02:24:36 visual_prompt]: Best epoch 26: best metric: 0.605
[09/26 02:24:36 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 02:24:43 visual_prompt]: Epoch 27 / 100: avg data time: 5.43e-02, avg batch time: 0.5023, average train loss: 0.0008
[09/26 02:24:44 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1692, average loss: 2.2501
[09/26 02:24:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 87.50	
[09/26 02:24:44 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 02:24:51 visual_prompt]: Epoch 28 / 100: avg data time: 4.76e-02, avg batch time: 0.4977, average train loss: 0.0008
[09/26 02:24:53 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 2.2520
[09/26 02:24:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 87.50	
[09/26 02:24:53 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 02:24:59 visual_prompt]: Epoch 29 / 100: avg data time: 5.55e-02, avg batch time: 0.5032, average train loss: 0.0007
[09/26 02:25:01 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1689, average loss: 2.2484
[09/26 02:25:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 87.50	
[09/26 02:25:01 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 02:25:08 visual_prompt]: Epoch 30 / 100: avg data time: 5.29e-02, avg batch time: 0.5020, average train loss: 0.0007
[09/26 02:25:09 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1692, average loss: 2.2480
[09/26 02:25:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 87.50	
[09/26 02:25:09 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 02:25:16 visual_prompt]: Epoch 31 / 100: avg data time: 5.21e-02, avg batch time: 0.5001, average train loss: 0.0007
[09/26 02:25:18 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1687, average loss: 2.2397
[09/26 02:25:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 87.50	
[09/26 02:25:18 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 02:25:24 visual_prompt]: Epoch 32 / 100: avg data time: 4.70e-02, avg batch time: 0.4953, average train loss: 0.0007
[09/26 02:25:26 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1687, average loss: 2.2484
[09/26 02:25:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 87.50	
[09/26 02:25:26 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 02:25:33 visual_prompt]: Epoch 33 / 100: avg data time: 5.62e-02, avg batch time: 0.5041, average train loss: 0.0006
[09/26 02:25:34 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1690, average loss: 2.2523
[09/26 02:25:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 87.50	
[09/26 02:25:34 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 02:25:41 visual_prompt]: Epoch 34 / 100: avg data time: 6.00e-02, avg batch time: 0.5077, average train loss: 0.0006
[09/26 02:25:43 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1696, average loss: 2.2585
[09/26 02:25:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:25:43 visual_prompt]: Best epoch 34: best metric: 0.610
[09/26 02:25:43 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 02:25:49 visual_prompt]: Epoch 35 / 100: avg data time: 6.02e-02, avg batch time: 0.5086, average train loss: 0.0006
[09/26 02:25:51 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1692, average loss: 2.2655
[09/26 02:25:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:25:51 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 02:25:58 visual_prompt]: Epoch 36 / 100: avg data time: 5.59e-02, avg batch time: 0.5051, average train loss: 0.0006
[09/26 02:25:59 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 2.2727
[09/26 02:25:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:25:59 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 02:26:06 visual_prompt]: Epoch 37 / 100: avg data time: 5.55e-02, avg batch time: 0.5044, average train loss: 0.0005
[09/26 02:26:08 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1690, average loss: 2.2758
[09/26 02:26:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:26:08 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 02:26:14 visual_prompt]: Epoch 38 / 100: avg data time: 5.84e-02, avg batch time: 0.5062, average train loss: 0.0006
[09/26 02:26:16 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1689, average loss: 2.2766
[09/26 02:26:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:26:16 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 02:26:23 visual_prompt]: Epoch 39 / 100: avg data time: 5.27e-02, avg batch time: 0.5020, average train loss: 0.0006
[09/26 02:26:24 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1693, average loss: 2.2763
[09/26 02:26:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:26:24 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 02:26:31 visual_prompt]: Epoch 40 / 100: avg data time: 4.16e-02, avg batch time: 0.4909, average train loss: 0.0005
[09/26 02:26:32 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 2.2761
[09/26 02:26:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:26:32 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 02:26:39 visual_prompt]: Epoch 41 / 100: avg data time: 5.62e-02, avg batch time: 0.5053, average train loss: 0.0005
[09/26 02:26:41 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 2.2800
[09/26 02:26:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 87.50	
[09/26 02:26:41 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 02:26:48 visual_prompt]: Epoch 42 / 100: avg data time: 5.75e-02, avg batch time: 0.5069, average train loss: 0.0005
[09/26 02:26:49 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1694, average loss: 2.2821
[09/26 02:26:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:26:49 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 02:26:56 visual_prompt]: Epoch 43 / 100: avg data time: 4.36e-02, avg batch time: 0.4938, average train loss: 0.0005
[09/26 02:26:57 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1690, average loss: 2.2826
[09/26 02:26:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:26:57 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 02:27:04 visual_prompt]: Epoch 44 / 100: avg data time: 5.54e-02, avg batch time: 0.5041, average train loss: 0.0005
[09/26 02:27:06 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1695, average loss: 2.2844
[09/26 02:27:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:27:06 visual_prompt]: Best epoch 44: best metric: 0.615
[09/26 02:27:06 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 02:27:12 visual_prompt]: Epoch 45 / 100: avg data time: 4.84e-02, avg batch time: 0.4968, average train loss: 0.0005
[09/26 02:27:14 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 2.2874
[09/26 02:27:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 87.50	
[09/26 02:27:14 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 02:27:21 visual_prompt]: Epoch 46 / 100: avg data time: 4.67e-02, avg batch time: 0.4966, average train loss: 0.0005
[09/26 02:27:22 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1695, average loss: 2.2858
[09/26 02:27:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 87.50	
[09/26 02:27:22 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 02:27:29 visual_prompt]: Epoch 47 / 100: avg data time: 5.19e-02, avg batch time: 0.5001, average train loss: 0.0005
[09/26 02:27:30 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1692, average loss: 2.2874
[09/26 02:27:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:27:30 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 02:27:37 visual_prompt]: Epoch 48 / 100: avg data time: 5.70e-02, avg batch time: 0.5050, average train loss: 0.0005
[09/26 02:27:39 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1691, average loss: 2.2903
[09/26 02:27:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:27:39 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 02:27:45 visual_prompt]: Epoch 49 / 100: avg data time: 5.26e-02, avg batch time: 0.5020, average train loss: 0.0005
[09/26 02:27:47 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1699, average loss: 2.2913
[09/26 02:27:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:27:47 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 02:27:54 visual_prompt]: Epoch 50 / 100: avg data time: 5.65e-02, avg batch time: 0.5051, average train loss: 0.0004
[09/26 02:27:55 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1690, average loss: 2.2929
[09/26 02:27:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:27:55 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 02:28:02 visual_prompt]: Epoch 51 / 100: avg data time: 5.62e-02, avg batch time: 0.5046, average train loss: 0.0004
[09/26 02:28:04 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1695, average loss: 2.2946
[09/26 02:28:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:28:04 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 02:28:11 visual_prompt]: Epoch 52 / 100: avg data time: 5.73e-02, avg batch time: 0.5060, average train loss: 0.0005
[09/26 02:28:12 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 2.2958
[09/26 02:28:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:28:12 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 02:28:19 visual_prompt]: Epoch 53 / 100: avg data time: 5.55e-02, avg batch time: 0.5044, average train loss: 0.0004
[09/26 02:28:20 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1698, average loss: 2.2959
[09/26 02:28:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:28:20 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 02:28:27 visual_prompt]: Epoch 54 / 100: avg data time: 4.33e-02, avg batch time: 0.4936, average train loss: 0.0004
[09/26 02:28:28 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 2.2987
[09/26 02:28:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:28:28 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 02:28:35 visual_prompt]: Epoch 55 / 100: avg data time: 5.46e-02, avg batch time: 0.5039, average train loss: 0.0007
[09/26 02:28:37 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 2.2999
[09/26 02:28:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 87.50	
[09/26 02:28:37 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 02:28:43 visual_prompt]: Epoch 56 / 100: avg data time: 4.34e-02, avg batch time: 0.4932, average train loss: 0.0004
[09/26 02:28:45 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1696, average loss: 2.2976
[09/26 02:28:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 88.00	
[09/26 02:28:45 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 02:28:52 visual_prompt]: Epoch 57 / 100: avg data time: 5.65e-02, avg batch time: 0.5051, average train loss: 0.0005
[09/26 02:28:53 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 2.2933
[09/26 02:28:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 87.50	
[09/26 02:28:53 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 02:29:00 visual_prompt]: Epoch 58 / 100: avg data time: 4.98e-02, avg batch time: 0.4988, average train loss: 0.0004
[09/26 02:29:01 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 2.2910
[09/26 02:29:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 87.50	
[09/26 02:29:01 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 02:29:08 visual_prompt]: Epoch 59 / 100: avg data time: 4.71e-02, avg batch time: 0.4967, average train loss: 0.0004
[09/26 02:29:09 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1692, average loss: 2.2890
[09/26 02:29:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:29:09 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 02:29:16 visual_prompt]: Epoch 60 / 100: avg data time: 4.25e-02, avg batch time: 0.4927, average train loss: 0.0004
[09/26 02:29:17 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1694, average loss: 2.2906
[09/26 02:29:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:29:17 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 02:29:24 visual_prompt]: Epoch 61 / 100: avg data time: 4.09e-02, avg batch time: 0.4913, average train loss: 0.0004
[09/26 02:29:26 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 2.2935
[09/26 02:29:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:29:26 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 02:29:32 visual_prompt]: Epoch 62 / 100: avg data time: 4.38e-02, avg batch time: 0.4948, average train loss: 0.0004
[09/26 02:29:34 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1688, average loss: 2.2965
[09/26 02:29:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:29:34 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 02:29:40 visual_prompt]: Epoch 63 / 100: avg data time: 3.83e-02, avg batch time: 0.4885, average train loss: 0.0004
[09/26 02:29:42 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.1692, average loss: 2.2994
[09/26 02:29:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:29:42 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 02:29:49 visual_prompt]: Epoch 64 / 100: avg data time: 5.70e-02, avg batch time: 0.5052, average train loss: 0.0004
[09/26 02:29:50 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1690, average loss: 2.3011
[09/26 02:29:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:29:50 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 02:29:57 visual_prompt]: Epoch 65 / 100: avg data time: 4.80e-02, avg batch time: 0.4969, average train loss: 0.0004
[09/26 02:29:58 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.1692, average loss: 2.3013
[09/26 02:29:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:29:58 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 02:30:05 visual_prompt]: Epoch 66 / 100: avg data time: 4.30e-02, avg batch time: 0.4931, average train loss: 0.0004
[09/26 02:30:07 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 2.3018
[09/26 02:30:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 02:30:07 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 02:30:13 visual_prompt]: Epoch 67 / 100: avg data time: 3.78e-02, avg batch time: 0.4889, average train loss: 0.0004
[09/26 02:30:15 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 2.3035
[09/26 02:30:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:30:15 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 02:30:21 visual_prompt]: Epoch 68 / 100: avg data time: 4.12e-02, avg batch time: 0.4907, average train loss: 0.0003
[09/26 02:30:23 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1690, average loss: 2.3051
[09/26 02:30:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:30:23 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 02:30:30 visual_prompt]: Epoch 69 / 100: avg data time: 5.08e-02, avg batch time: 0.4991, average train loss: 0.0004
[09/26 02:30:31 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 2.3061
[09/26 02:30:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:30:31 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 02:30:38 visual_prompt]: Epoch 70 / 100: avg data time: 5.32e-02, avg batch time: 0.5022, average train loss: 0.0004
[09/26 02:30:39 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1692, average loss: 2.3061
[09/26 02:30:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:30:39 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 02:30:46 visual_prompt]: Epoch 71 / 100: avg data time: 6.07e-02, avg batch time: 0.5089, average train loss: 0.0004
[09/26 02:30:48 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 2.3066
[09/26 02:30:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:30:48 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 02:30:55 visual_prompt]: Epoch 72 / 100: avg data time: 4.28e-02, avg batch time: 0.4923, average train loss: 0.0004
[09/26 02:30:56 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 2.3070
[09/26 02:30:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:30:56 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 02:31:03 visual_prompt]: Epoch 73 / 100: avg data time: 5.59e-02, avg batch time: 0.5050, average train loss: 0.0004
[09/26 02:31:04 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1690, average loss: 2.3062
[09/26 02:31:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:31:04 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 02:31:11 visual_prompt]: Epoch 74 / 100: avg data time: 4.66e-02, avg batch time: 0.4949, average train loss: 0.0004
[09/26 02:31:12 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 2.3058
[09/26 02:31:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 87.50	
[09/26 02:31:12 visual_prompt]: Best epoch 74: best metric: 0.620
[09/26 02:31:12 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 02:31:19 visual_prompt]: Epoch 75 / 100: avg data time: 5.45e-02, avg batch time: 0.5028, average train loss: 0.0004
[09/26 02:31:21 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1689, average loss: 2.3062
[09/26 02:31:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 87.50	
[09/26 02:31:21 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 02:31:28 visual_prompt]: Epoch 76 / 100: avg data time: 4.47e-02, avg batch time: 0.4933, average train loss: 0.0004
[09/26 02:31:29 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1688, average loss: 2.3059
[09/26 02:31:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 87.50	
[09/26 02:31:29 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 02:31:36 visual_prompt]: Epoch 77 / 100: avg data time: 5.03e-02, avg batch time: 0.5001, average train loss: 0.0004
[09/26 02:31:37 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1691, average loss: 2.3056
[09/26 02:31:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 87.50	
[09/26 02:31:37 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 02:31:44 visual_prompt]: Epoch 78 / 100: avg data time: 5.23e-02, avg batch time: 0.5009, average train loss: 0.0004
[09/26 02:31:46 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1693, average loss: 2.3063
[09/26 02:31:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 87.50	
[09/26 02:31:46 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 02:31:53 visual_prompt]: Epoch 79 / 100: avg data time: 5.62e-02, avg batch time: 0.5049, average train loss: 0.0004
[09/26 02:31:54 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1689, average loss: 2.3073
[09/26 02:31:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 87.50	
[09/26 02:31:54 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 02:32:01 visual_prompt]: Epoch 80 / 100: avg data time: 6.09e-02, avg batch time: 0.5096, average train loss: 0.0004
[09/26 02:32:02 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1688, average loss: 2.3082
[09/26 02:32:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 87.50	
[09/26 02:32:02 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 02:32:09 visual_prompt]: Epoch 81 / 100: avg data time: 5.48e-02, avg batch time: 0.5031, average train loss: 0.0003
[09/26 02:32:11 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 2.3084
[09/26 02:32:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 87.50	
[09/26 02:32:11 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 02:32:18 visual_prompt]: Epoch 82 / 100: avg data time: 6.44e-02, avg batch time: 0.5124, average train loss: 0.0004
[09/26 02:32:19 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 2.3084
[09/26 02:32:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:32:19 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 02:32:26 visual_prompt]: Epoch 83 / 100: avg data time: 6.37e-02, avg batch time: 0.5118, average train loss: 0.0004
[09/26 02:32:28 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.1691, average loss: 2.3086
[09/26 02:32:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:32:28 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 02:32:34 visual_prompt]: Epoch 84 / 100: avg data time: 4.16e-02, avg batch time: 0.4916, average train loss: 0.0004
[09/26 02:32:36 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1693, average loss: 2.3089
[09/26 02:32:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:32:36 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 02:32:43 visual_prompt]: Epoch 85 / 100: avg data time: 6.10e-02, avg batch time: 0.5097, average train loss: 0.0004
[09/26 02:32:44 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1693, average loss: 2.3090
[09/26 02:32:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:32:44 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 02:32:51 visual_prompt]: Epoch 86 / 100: avg data time: 5.77e-02, avg batch time: 0.5057, average train loss: 0.0004
[09/26 02:32:53 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1690, average loss: 2.3092
[09/26 02:32:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 87.50	
[09/26 02:32:53 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 02:33:00 visual_prompt]: Epoch 87 / 100: avg data time: 5.69e-02, avg batch time: 0.5053, average train loss: 0.0004
[09/26 02:33:01 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 2.3094
[09/26 02:33:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:33:01 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 02:33:08 visual_prompt]: Epoch 88 / 100: avg data time: 5.37e-02, avg batch time: 0.5033, average train loss: 0.0004
[09/26 02:33:09 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1691, average loss: 2.3096
[09/26 02:33:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:33:09 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 02:33:16 visual_prompt]: Epoch 89 / 100: avg data time: 4.26e-02, avg batch time: 0.4927, average train loss: 0.0004
[09/26 02:33:17 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 2.3096
[09/26 02:33:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:33:17 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 02:33:24 visual_prompt]: Epoch 90 / 100: avg data time: 5.49e-02, avg batch time: 0.5047, average train loss: 0.0003
[09/26 02:33:26 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1694, average loss: 2.3096
[09/26 02:33:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:33:26 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 02:33:33 visual_prompt]: Epoch 91 / 100: avg data time: 5.61e-02, avg batch time: 0.5052, average train loss: 0.0004
[09/26 02:33:34 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 2.3097
[09/26 02:33:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:33:34 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 02:33:41 visual_prompt]: Epoch 92 / 100: avg data time: 4.41e-02, avg batch time: 0.4926, average train loss: 0.0004
[09/26 02:33:42 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1689, average loss: 2.3098
[09/26 02:33:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:33:42 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 02:33:49 visual_prompt]: Epoch 93 / 100: avg data time: 5.64e-02, avg batch time: 0.5068, average train loss: 0.0004
[09/26 02:33:50 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 2.3097
[09/26 02:33:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:33:50 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 02:33:57 visual_prompt]: Epoch 94 / 100: avg data time: 5.35e-02, avg batch time: 0.5014, average train loss: 0.0004
[09/26 02:33:59 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1689, average loss: 2.3096
[09/26 02:33:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:33:59 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 02:34:06 visual_prompt]: Epoch 95 / 100: avg data time: 5.59e-02, avg batch time: 0.5044, average train loss: 0.0003
[09/26 02:34:07 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1691, average loss: 2.3095
[09/26 02:34:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:34:07 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 02:34:14 visual_prompt]: Epoch 96 / 100: avg data time: 5.33e-02, avg batch time: 0.5033, average train loss: 0.0004
[09/26 02:34:15 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 2.3095
[09/26 02:34:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:34:15 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 02:34:22 visual_prompt]: Epoch 97 / 100: avg data time: 5.02e-02, avg batch time: 0.5004, average train loss: 0.0004
[09/26 02:34:24 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 2.3095
[09/26 02:34:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:34:24 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 02:34:30 visual_prompt]: Epoch 98 / 100: avg data time: 3.95e-02, avg batch time: 0.4920, average train loss: 0.0004
[09/26 02:34:32 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1690, average loss: 2.3095
[09/26 02:34:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:34:32 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 02:34:38 visual_prompt]: Epoch 99 / 100: avg data time: 4.30e-02, avg batch time: 0.4921, average train loss: 0.0004
[09/26 02:34:40 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1689, average loss: 2.3095
[09/26 02:34:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:34:40 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 02:34:47 visual_prompt]: Epoch 100 / 100: avg data time: 4.43e-02, avg batch time: 0.4951, average train loss: 0.0004
[09/26 02:34:48 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1695, average loss: 2.3095
[09/26 02:34:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 02:34:48 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 02:34:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 02:34:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 02:34:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 02:34:48 visual_prompt]: Training with config:
[09/26 02:34:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 02:34:48 visual_prompt]: Loading training data...
[09/26 02:34:48 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 02:34:50 visual_prompt]: Number of images: 800
[09/26 02:34:50 visual_prompt]: Number of classes: 47 / 47
[09/26 02:34:50 visual_prompt]: Loading validation data...
[09/26 02:34:50 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 02:34:51 visual_prompt]: Number of images: 200
[09/26 02:34:51 visual_prompt]: Number of classes: 47 / 47
[09/26 02:34:51 visual_prompt]: Constructing models...
[09/26 02:34:53 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 02:34:53 visual_prompt]: tuned percent:0.576
[09/26 02:34:53 visual_prompt]: Device used for model: 0
[09/26 02:34:53 visual_prompt]: Setting up Evaluator...
[09/26 02:34:53 visual_prompt]: Setting up Trainer...
[09/26 02:34:53 visual_prompt]: 	Setting up the optimizer...
[09/26 02:34:53 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 02:35:00 visual_prompt]: Epoch 1 / 100: avg data time: 5.54e-02, avg batch time: 0.5022, average train loss: 3.9270
[09/26 02:35:02 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1690, average loss: 3.9045
[09/26 02:35:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 02:35:02 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 02:35:02 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 02:35:08 visual_prompt]: Epoch 2 / 100: avg data time: 5.63e-02, avg batch time: 0.5033, average train loss: 3.8807
[09/26 02:35:10 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1689, average loss: 3.9102
[09/26 02:35:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 02:35:10 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 02:35:17 visual_prompt]: Epoch 3 / 100: avg data time: 5.71e-02, avg batch time: 0.5037, average train loss: 3.8164
[09/26 02:35:18 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1690, average loss: 4.2190
[09/26 02:35:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 18.00	
[09/26 02:35:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 02:35:25 visual_prompt]: Epoch 4 / 100: avg data time: 4.98e-02, avg batch time: 0.4966, average train loss: 3.8097
[09/26 02:35:26 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1689, average loss: 3.7279
[09/26 02:35:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 18.00	
[09/26 02:35:26 visual_prompt]: Best epoch 4: best metric: 0.045
[09/26 02:35:26 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 02:35:33 visual_prompt]: Epoch 5 / 100: avg data time: 5.56e-02, avg batch time: 0.5039, average train loss: 3.8248
[09/26 02:35:35 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1687, average loss: 3.9466
[09/26 02:35:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 02:35:35 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 02:35:42 visual_prompt]: Epoch 6 / 100: avg data time: 4.83e-02, avg batch time: 0.4979, average train loss: 3.8663
[09/26 02:35:43 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1690, average loss: 3.9230
[09/26 02:35:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.50	
[09/26 02:35:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 02:35:50 visual_prompt]: Epoch 7 / 100: avg data time: 5.65e-02, avg batch time: 0.5069, average train loss: 3.8184
[09/26 02:35:51 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1685, average loss: 3.8859
[09/26 02:35:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 14.50	
[09/26 02:35:51 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 02:35:58 visual_prompt]: Epoch 8 / 100: avg data time: 5.48e-02, avg batch time: 0.5031, average train loss: 3.8831
[09/26 02:36:00 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1690, average loss: 3.9199
[09/26 02:36:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.50	
[09/26 02:36:00 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 02:36:07 visual_prompt]: Epoch 9 / 100: avg data time: 5.82e-02, avg batch time: 0.5062, average train loss: 3.9125
[09/26 02:36:08 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1689, average loss: 3.9320
[09/26 02:36:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 02:36:08 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 02:36:15 visual_prompt]: Epoch 10 / 100: avg data time: 4.94e-02, avg batch time: 0.4973, average train loss: 3.9382
[09/26 02:36:16 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 3.9646
[09/26 02:36:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/26 02:36:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 02:36:23 visual_prompt]: Epoch 11 / 100: avg data time: 4.40e-02, avg batch time: 0.4931, average train loss: 3.9217
[09/26 02:36:25 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1687, average loss: 3.9451
[09/26 02:36:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/26 02:36:25 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 02:36:31 visual_prompt]: Epoch 12 / 100: avg data time: 4.72e-02, avg batch time: 0.4967, average train loss: 3.9357
[09/26 02:36:33 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 3.9299
[09/26 02:36:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 02:36:33 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 02:36:40 visual_prompt]: Epoch 13 / 100: avg data time: 5.90e-02, avg batch time: 0.5073, average train loss: 3.9093
[09/26 02:36:41 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 3.9095
[09/26 02:36:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.00	
[09/26 02:36:41 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 02:36:48 visual_prompt]: Epoch 14 / 100: avg data time: 4.62e-02, avg batch time: 0.4946, average train loss: 3.9010
[09/26 02:36:49 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1692, average loss: 3.9401
[09/26 02:36:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 9.00	
[09/26 02:36:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 02:36:56 visual_prompt]: Epoch 15 / 100: avg data time: 5.81e-02, avg batch time: 0.5052, average train loss: 3.9315
[09/26 02:36:57 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1691, average loss: 3.9675
[09/26 02:36:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 8.50	
[09/26 02:36:57 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 02:37:04 visual_prompt]: Epoch 16 / 100: avg data time: 5.66e-02, avg batch time: 0.5057, average train loss: 3.9260
[09/26 02:37:06 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1692, average loss: 3.9508
[09/26 02:37:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 02:37:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 02:37:13 visual_prompt]: Epoch 17 / 100: avg data time: 5.65e-02, avg batch time: 0.5042, average train loss: 3.9011
[09/26 02:37:14 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 3.9543
[09/26 02:37:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 7.00	
[09/26 02:37:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 02:37:21 visual_prompt]: Epoch 18 / 100: avg data time: 4.44e-02, avg batch time: 0.4928, average train loss: 3.8838
[09/26 02:37:22 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1687, average loss: 3.9835
[09/26 02:37:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.00	
[09/26 02:37:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 02:37:29 visual_prompt]: Epoch 19 / 100: avg data time: 4.64e-02, avg batch time: 0.4958, average train loss: 3.9127
[09/26 02:37:31 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1687, average loss: 3.9200
[09/26 02:37:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 8.00	
[09/26 02:37:31 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 02:37:37 visual_prompt]: Epoch 20 / 100: avg data time: 4.23e-02, avg batch time: 0.4912, average train loss: 3.8901
[09/26 02:37:39 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1690, average loss: 3.9196
[09/26 02:37:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.50	
[09/26 02:37:39 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 02:37:45 visual_prompt]: Epoch 21 / 100: avg data time: 4.34e-02, avg batch time: 0.4948, average train loss: 3.8934
[09/26 02:37:47 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1687, average loss: 3.9358
[09/26 02:37:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 8.00	
[09/26 02:37:47 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 02:37:54 visual_prompt]: Epoch 22 / 100: avg data time: 5.27e-02, avg batch time: 0.5002, average train loss: 3.9072
[09/26 02:37:55 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1690, average loss: 3.8921
[09/26 02:37:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.00	
[09/26 02:37:55 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 02:38:02 visual_prompt]: Epoch 23 / 100: avg data time: 4.75e-02, avg batch time: 0.4964, average train loss: 3.9005
[09/26 02:38:03 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1687, average loss: 3.9203
[09/26 02:38:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 10.00	
[09/26 02:38:03 visual_prompt]: Best epoch 23: best metric: 0.050
[09/26 02:38:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 02:38:10 visual_prompt]: Epoch 24 / 100: avg data time: 4.80e-02, avg batch time: 0.4958, average train loss: 3.9075
[09/26 02:38:12 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1689, average loss: 3.9611
[09/26 02:38:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 02:38:12 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 02:38:18 visual_prompt]: Epoch 25 / 100: avg data time: 4.10e-02, avg batch time: 0.4924, average train loss: 3.8962
[09/26 02:38:20 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1690, average loss: 3.9377
[09/26 02:38:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 11.00	
[09/26 02:38:20 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 02:38:26 visual_prompt]: Epoch 26 / 100: avg data time: 4.36e-02, avg batch time: 0.4954, average train loss: 3.8938
[09/26 02:38:28 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1689, average loss: 3.9424
[09/26 02:38:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 12.50	
[09/26 02:38:28 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 02:38:34 visual_prompt]: Epoch 27 / 100: avg data time: 4.30e-02, avg batch time: 0.4918, average train loss: 3.9008
[09/26 02:38:36 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1691, average loss: 3.9040
[09/26 02:38:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/26 02:38:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 02:38:43 visual_prompt]: Epoch 28 / 100: avg data time: 4.83e-02, avg batch time: 0.4961, average train loss: 3.8865
[09/26 02:38:44 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1688, average loss: 3.9380
[09/26 02:38:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/26 02:38:44 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 02:38:51 visual_prompt]: Epoch 29 / 100: avg data time: 5.36e-02, avg batch time: 0.5037, average train loss: 3.9268
[09/26 02:38:53 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 3.9878
[09/26 02:38:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 5.00	
[09/26 02:38:53 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 02:38:59 visual_prompt]: Epoch 30 / 100: avg data time: 5.42e-02, avg batch time: 0.5026, average train loss: 3.9079
[09/26 02:39:01 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1684, average loss: 3.9147
[09/26 02:39:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 13.50	
[09/26 02:39:01 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 02:39:08 visual_prompt]: Epoch 31 / 100: avg data time: 6.90e-02, avg batch time: 0.5169, average train loss: 3.8880
[09/26 02:39:09 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1690, average loss: 3.8966
[09/26 02:39:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/26 02:39:09 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 02:39:16 visual_prompt]: Epoch 32 / 100: avg data time: 4.78e-02, avg batch time: 0.4992, average train loss: 3.8941
[09/26 02:39:18 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1689, average loss: 3.9377
[09/26 02:39:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 11.00	
[09/26 02:39:18 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 02:39:25 visual_prompt]: Epoch 33 / 100: avg data time: 5.54e-02, avg batch time: 0.5031, average train loss: 3.8714
[09/26 02:39:26 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1686, average loss: 3.9094
[09/26 02:39:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 8.00	
[09/26 02:39:26 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 02:39:33 visual_prompt]: Epoch 34 / 100: avg data time: 5.28e-02, avg batch time: 0.5001, average train loss: 3.8882
[09/26 02:39:34 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1690, average loss: 3.9242
[09/26 02:39:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.50	
[09/26 02:39:34 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 02:39:41 visual_prompt]: Epoch 35 / 100: avg data time: 4.57e-02, avg batch time: 0.4935, average train loss: 3.9091
[09/26 02:39:43 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1693, average loss: 3.9535
[09/26 02:39:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 5.50	
[09/26 02:39:43 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 02:39:50 visual_prompt]: Epoch 36 / 100: avg data time: 4.48e-02, avg batch time: 0.4926, average train loss: 3.9033
[09/26 02:39:51 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 3.9430
[09/26 02:39:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/26 02:39:51 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 02:39:58 visual_prompt]: Epoch 37 / 100: avg data time: 5.41e-02, avg batch time: 0.5029, average train loss: 3.9352
[09/26 02:39:59 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1694, average loss: 3.9204
[09/26 02:39:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.00	
[09/26 02:39:59 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 02:40:06 visual_prompt]: Epoch 38 / 100: avg data time: 4.29e-02, avg batch time: 0.4922, average train loss: 3.8992
[09/26 02:40:08 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 3.9584
[09/26 02:40:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 6.00	
[09/26 02:40:08 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 02:40:15 visual_prompt]: Epoch 39 / 100: avg data time: 5.99e-02, avg batch time: 0.5075, average train loss: 3.8816
[09/26 02:40:16 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1689, average loss: 3.9592
[09/26 02:40:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 02:40:16 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 02:40:23 visual_prompt]: Epoch 40 / 100: avg data time: 6.27e-02, avg batch time: 0.5101, average train loss: 3.8993
[09/26 02:40:24 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1690, average loss: 3.9064
[09/26 02:40:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 02:40:24 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 02:40:31 visual_prompt]: Epoch 41 / 100: avg data time: 4.45e-02, avg batch time: 0.4949, average train loss: 3.8747
[09/26 02:40:33 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1688, average loss: 3.9266
[09/26 02:40:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.00	
[09/26 02:40:33 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 02:40:39 visual_prompt]: Epoch 42 / 100: avg data time: 5.49e-02, avg batch time: 0.5029, average train loss: 3.8762
[09/26 02:40:41 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1687, average loss: 3.9117
[09/26 02:40:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/26 02:40:41 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 02:40:48 visual_prompt]: Epoch 43 / 100: avg data time: 5.68e-02, avg batch time: 0.5044, average train loss: 3.8807
[09/26 02:40:49 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 3.9248
[09/26 02:40:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/26 02:40:49 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 02:40:56 visual_prompt]: Epoch 44 / 100: avg data time: 4.34e-02, avg batch time: 0.4934, average train loss: 3.8732
[09/26 02:40:58 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1688, average loss: 3.9849
[09/26 02:40:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/26 02:40:58 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 02:41:04 visual_prompt]: Epoch 45 / 100: avg data time: 4.33e-02, avg batch time: 0.4911, average train loss: 3.8883
[09/26 02:41:06 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1693, average loss: 3.9104
[09/26 02:41:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 10.00	
[09/26 02:41:06 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 02:41:13 visual_prompt]: Epoch 46 / 100: avg data time: 5.71e-02, avg batch time: 0.5048, average train loss: 3.8849
[09/26 02:41:14 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 3.9218
[09/26 02:41:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/26 02:41:14 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 02:41:21 visual_prompt]: Epoch 47 / 100: avg data time: 5.16e-02, avg batch time: 0.5004, average train loss: 3.8763
[09/26 02:41:22 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1688, average loss: 3.8854
[09/26 02:41:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/26 02:41:22 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 02:41:29 visual_prompt]: Epoch 48 / 100: avg data time: 4.83e-02, avg batch time: 0.4959, average train loss: 3.8748
[09/26 02:41:31 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1691, average loss: 3.9269
[09/26 02:41:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/26 02:41:31 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 02:41:37 visual_prompt]: Epoch 49 / 100: avg data time: 6.03e-02, avg batch time: 0.5087, average train loss: 3.8944
[09/26 02:41:39 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1692, average loss: 3.9188
[09/26 02:41:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 02:41:39 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 02:41:46 visual_prompt]: Epoch 50 / 100: avg data time: 5.30e-02, avg batch time: 0.5014, average train loss: 3.8806
[09/26 02:41:47 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 3.9052
[09/26 02:41:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 12.50	
[09/26 02:41:47 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 02:41:54 visual_prompt]: Epoch 51 / 100: avg data time: 4.95e-02, avg batch time: 0.4970, average train loss: 3.8624
[09/26 02:41:55 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1691, average loss: 3.8994
[09/26 02:41:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.50	
[09/26 02:41:55 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 02:42:02 visual_prompt]: Epoch 52 / 100: avg data time: 4.17e-02, avg batch time: 0.4900, average train loss: 3.8859
[09/26 02:42:04 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.1690, average loss: 3.9419
[09/26 02:42:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.00	
[09/26 02:42:04 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 02:42:10 visual_prompt]: Epoch 53 / 100: avg data time: 5.97e-02, avg batch time: 0.5079, average train loss: 3.8753
[09/26 02:42:12 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 3.9249
[09/26 02:42:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 14.00	
[09/26 02:42:12 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 02:42:19 visual_prompt]: Epoch 54 / 100: avg data time: 4.83e-02, avg batch time: 0.4967, average train loss: 3.8717
[09/26 02:42:20 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 3.9213
[09/26 02:42:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.00	
[09/26 02:42:20 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 02:42:27 visual_prompt]: Epoch 55 / 100: avg data time: 4.75e-02, avg batch time: 0.4968, average train loss: 3.8750
[09/26 02:42:29 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1691, average loss: 3.9338
[09/26 02:42:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.50	
[09/26 02:42:29 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 02:42:35 visual_prompt]: Epoch 56 / 100: avg data time: 4.59e-02, avg batch time: 0.4951, average train loss: 3.8671
[09/26 02:42:37 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1695, average loss: 3.9161
[09/26 02:42:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 7.50	
[09/26 02:42:37 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 02:42:43 visual_prompt]: Epoch 57 / 100: avg data time: 5.06e-02, avg batch time: 0.4995, average train loss: 3.8712
[09/26 02:42:45 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 3.8883
[09/26 02:42:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 02:42:45 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 02:42:52 visual_prompt]: Epoch 58 / 100: avg data time: 5.13e-02, avg batch time: 0.4994, average train loss: 3.8643
[09/26 02:42:53 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1689, average loss: 3.8871
[09/26 02:42:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/26 02:42:53 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 02:43:00 visual_prompt]: Epoch 59 / 100: avg data time: 6.03e-02, avg batch time: 0.5084, average train loss: 3.8491
[09/26 02:43:02 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1692, average loss: 3.9495
[09/26 02:43:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 0.50	top5: 8.00	
[09/26 02:43:02 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 02:43:08 visual_prompt]: Epoch 60 / 100: avg data time: 4.78e-02, avg batch time: 0.4968, average train loss: 3.8767
[09/26 02:43:10 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1690, average loss: 3.9255
[09/26 02:43:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 7.00	
[09/26 02:43:10 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 02:43:17 visual_prompt]: Epoch 61 / 100: avg data time: 4.81e-02, avg batch time: 0.4966, average train loss: 3.8640
[09/26 02:43:18 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1689, average loss: 3.8970
[09/26 02:43:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/26 02:43:18 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 02:43:25 visual_prompt]: Epoch 62 / 100: avg data time: 5.28e-02, avg batch time: 0.5026, average train loss: 3.8561
[09/26 02:43:27 visual_prompt]: Inference (val):avg data time: 4.86e-05, avg batch time: 0.1698, average loss: 3.9376
[09/26 02:43:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.50	
[09/26 02:43:27 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 02:43:33 visual_prompt]: Epoch 63 / 100: avg data time: 3.91e-02, avg batch time: 0.4867, average train loss: 3.8652
[09/26 02:43:35 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1687, average loss: 3.9083
[09/26 02:43:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 02:43:35 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 02:43:42 visual_prompt]: Epoch 64 / 100: avg data time: 5.19e-02, avg batch time: 0.5009, average train loss: 3.8491
[09/26 02:43:43 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1688, average loss: 3.9165
[09/26 02:43:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 5.50	
[09/26 02:43:43 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 02:43:50 visual_prompt]: Epoch 65 / 100: avg data time: 6.05e-02, avg batch time: 0.5077, average train loss: 3.8537
[09/26 02:43:52 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1692, average loss: 3.9662
[09/26 02:43:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 6.00	
[09/26 02:43:52 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 02:43:59 visual_prompt]: Epoch 66 / 100: avg data time: 5.50e-02, avg batch time: 0.5034, average train loss: 3.8646
[09/26 02:44:00 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1691, average loss: 3.9128
[09/26 02:44:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 02:44:00 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 02:44:07 visual_prompt]: Epoch 67 / 100: avg data time: 5.86e-02, avg batch time: 0.5060, average train loss: 3.8548
[09/26 02:44:08 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 3.9010
[09/26 02:44:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 9.50	
[09/26 02:44:08 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 02:44:15 visual_prompt]: Epoch 68 / 100: avg data time: 4.11e-02, avg batch time: 0.4911, average train loss: 3.8535
[09/26 02:44:17 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 3.9271
[09/26 02:44:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 02:44:17 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 02:44:23 visual_prompt]: Epoch 69 / 100: avg data time: 3.91e-02, avg batch time: 0.4891, average train loss: 3.8400
[09/26 02:44:25 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1688, average loss: 3.9234
[09/26 02:44:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 02:44:25 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 02:44:31 visual_prompt]: Epoch 70 / 100: avg data time: 4.96e-02, avg batch time: 0.4979, average train loss: 3.8390
[09/26 02:44:33 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 3.9159
[09/26 02:44:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 02:44:33 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 02:44:40 visual_prompt]: Epoch 71 / 100: avg data time: 5.54e-02, avg batch time: 0.5044, average train loss: 3.8478
[09/26 02:44:41 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1690, average loss: 3.9291
[09/26 02:44:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.00	
[09/26 02:44:41 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 02:44:48 visual_prompt]: Epoch 72 / 100: avg data time: 5.23e-02, avg batch time: 0.5012, average train loss: 3.8427
[09/26 02:44:50 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1690, average loss: 3.8893
[09/26 02:44:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 9.00	
[09/26 02:44:50 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 02:44:56 visual_prompt]: Epoch 73 / 100: avg data time: 5.42e-02, avg batch time: 0.5032, average train loss: 3.8378
[09/26 02:44:58 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1690, average loss: 3.8934
[09/26 02:44:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.50	
[09/26 02:44:58 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 02:45:05 visual_prompt]: Epoch 74 / 100: avg data time: 5.16e-02, avg batch time: 0.5003, average train loss: 3.8432
[09/26 02:45:06 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1695, average loss: 3.8771
[09/26 02:45:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 11.50	
[09/26 02:45:06 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 02:45:13 visual_prompt]: Epoch 75 / 100: avg data time: 5.46e-02, avg batch time: 0.5041, average train loss: 3.8126
[09/26 02:45:15 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1690, average loss: 3.9094
[09/26 02:45:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 02:45:15 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 02:45:21 visual_prompt]: Epoch 76 / 100: avg data time: 4.31e-02, avg batch time: 0.4921, average train loss: 3.8164
[09/26 02:45:23 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 3.8624
[09/26 02:45:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 8.50	
[09/26 02:45:23 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 02:45:30 visual_prompt]: Epoch 77 / 100: avg data time: 5.75e-02, avg batch time: 0.5052, average train loss: 3.7990
[09/26 02:45:31 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 3.9473
[09/26 02:45:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.00	
[09/26 02:45:31 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 02:45:38 visual_prompt]: Epoch 78 / 100: avg data time: 5.34e-02, avg batch time: 0.5010, average train loss: 3.8385
[09/26 02:45:39 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1688, average loss: 3.8913
[09/26 02:45:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.00	
[09/26 02:45:39 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 02:45:46 visual_prompt]: Epoch 79 / 100: avg data time: 4.89e-02, avg batch time: 0.4980, average train loss: 3.8030
[09/26 02:45:48 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1690, average loss: 3.8993
[09/26 02:45:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 9.00	
[09/26 02:45:48 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 02:45:54 visual_prompt]: Epoch 80 / 100: avg data time: 4.89e-02, avg batch time: 0.4976, average train loss: 3.8293
[09/26 02:45:56 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1690, average loss: 3.8701
[09/26 02:45:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 10.50	
[09/26 02:45:56 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 02:46:03 visual_prompt]: Epoch 81 / 100: avg data time: 5.27e-02, avg batch time: 0.5011, average train loss: 3.8020
[09/26 02:46:04 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1689, average loss: 3.8533
[09/26 02:46:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.50	
[09/26 02:46:04 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 02:46:11 visual_prompt]: Epoch 82 / 100: avg data time: 5.51e-02, avg batch time: 0.5042, average train loss: 3.8013
[09/26 02:46:12 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1691, average loss: 3.9607
[09/26 02:46:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 11.50	
[09/26 02:46:12 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 02:46:19 visual_prompt]: Epoch 83 / 100: avg data time: 5.06e-02, avg batch time: 0.4998, average train loss: 3.7590
[09/26 02:46:21 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1693, average loss: 3.7871
[09/26 02:46:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.50	top5: 20.00	
[09/26 02:46:21 visual_prompt]: Best epoch 83: best metric: 0.055
[09/26 02:46:21 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 02:46:27 visual_prompt]: Epoch 84 / 100: avg data time: 4.45e-02, avg batch time: 0.4935, average train loss: 3.6976
[09/26 02:46:29 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1690, average loss: 3.7818
[09/26 02:46:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 18.00	
[09/26 02:46:29 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 02:46:36 visual_prompt]: Epoch 85 / 100: avg data time: 4.38e-02, avg batch time: 0.4926, average train loss: 3.5553
[09/26 02:46:37 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1693, average loss: 3.5906
[09/26 02:46:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.00	top5: 30.50	
[09/26 02:46:37 visual_prompt]: Best epoch 85: best metric: 0.100
[09/26 02:46:37 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 02:46:44 visual_prompt]: Epoch 86 / 100: avg data time: 4.27e-02, avg batch time: 0.4911, average train loss: 3.2944
[09/26 02:46:45 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1695, average loss: 3.3498
[09/26 02:46:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.00	top5: 36.00	
[09/26 02:46:45 visual_prompt]: Best epoch 86: best metric: 0.150
[09/26 02:46:45 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 02:46:52 visual_prompt]: Epoch 87 / 100: avg data time: 5.71e-02, avg batch time: 0.5051, average train loss: 3.0276
[09/26 02:46:54 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1690, average loss: 3.1829
[09/26 02:46:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 17.50	top5: 46.50	
[09/26 02:46:54 visual_prompt]: Best epoch 87: best metric: 0.175
[09/26 02:46:54 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 02:47:00 visual_prompt]: Epoch 88 / 100: avg data time: 5.95e-02, avg batch time: 0.5074, average train loss: 2.8726
[09/26 02:47:02 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.1695, average loss: 3.0295
[09/26 02:47:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.00	top5: 53.50	
[09/26 02:47:02 visual_prompt]: Best epoch 88: best metric: 0.250
[09/26 02:47:02 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 02:47:09 visual_prompt]: Epoch 89 / 100: avg data time: 5.67e-02, avg batch time: 0.5061, average train loss: 2.5289
[09/26 02:47:10 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1695, average loss: 2.8431
[09/26 02:47:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.50	top5: 60.00	
[09/26 02:47:10 visual_prompt]: Best epoch 89: best metric: 0.255
[09/26 02:47:10 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 02:47:17 visual_prompt]: Epoch 90 / 100: avg data time: 5.91e-02, avg batch time: 0.5079, average train loss: 2.2411
[09/26 02:47:19 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 2.6538
[09/26 02:47:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 32.00	top5: 62.50	
[09/26 02:47:19 visual_prompt]: Best epoch 90: best metric: 0.320
[09/26 02:47:19 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 02:47:26 visual_prompt]: Epoch 91 / 100: avg data time: 6.05e-02, avg batch time: 0.5086, average train loss: 2.0017
[09/26 02:47:27 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1695, average loss: 2.4889
[09/26 02:47:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 32.00	top5: 65.00	
[09/26 02:47:27 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 02:47:34 visual_prompt]: Epoch 92 / 100: avg data time: 5.56e-02, avg batch time: 0.5041, average train loss: 1.8024
[09/26 02:47:36 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1693, average loss: 2.4683
[09/26 02:47:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.50	top5: 66.00	
[09/26 02:47:36 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 02:47:43 visual_prompt]: Epoch 93 / 100: avg data time: 6.00e-02, avg batch time: 0.5079, average train loss: 1.6552
[09/26 02:47:44 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 2.3062
[09/26 02:47:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.00	top5: 71.50	
[09/26 02:47:44 visual_prompt]: Best epoch 93: best metric: 0.370
[09/26 02:47:44 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 02:47:51 visual_prompt]: Epoch 94 / 100: avg data time: 6.14e-02, avg batch time: 0.5097, average train loss: 1.5346
[09/26 02:47:52 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1693, average loss: 2.2651
[09/26 02:47:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.00	top5: 72.00	
[09/26 02:47:52 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 02:47:59 visual_prompt]: Epoch 95 / 100: avg data time: 5.03e-02, avg batch time: 0.4997, average train loss: 1.4174
[09/26 02:48:01 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 2.2285
[09/26 02:48:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.00	top5: 69.50	
[09/26 02:48:01 visual_prompt]: Best epoch 95: best metric: 0.410
[09/26 02:48:01 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 02:48:08 visual_prompt]: Epoch 96 / 100: avg data time: 5.22e-02, avg batch time: 0.5007, average train loss: 1.3373
[09/26 02:48:09 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 2.2002
[09/26 02:48:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.50	top5: 71.50	
[09/26 02:48:09 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 02:48:16 visual_prompt]: Epoch 97 / 100: avg data time: 4.40e-02, avg batch time: 0.4954, average train loss: 1.2653
[09/26 02:48:17 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1690, average loss: 2.2024
[09/26 02:48:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.50	top5: 70.50	
[09/26 02:48:17 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 02:48:24 visual_prompt]: Epoch 98 / 100: avg data time: 5.86e-02, avg batch time: 0.5066, average train loss: 1.2216
[09/26 02:48:26 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 2.1923
[09/26 02:48:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.50	top5: 70.50	
[09/26 02:48:26 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 02:48:32 visual_prompt]: Epoch 99 / 100: avg data time: 5.53e-02, avg batch time: 0.5037, average train loss: 1.2027
[09/26 02:48:34 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1689, average loss: 2.1777
[09/26 02:48:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 39.00	top5: 71.00	
[09/26 02:48:34 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 02:48:41 visual_prompt]: Epoch 100 / 100: avg data time: 5.46e-02, avg batch time: 0.5038, average train loss: 1.1842
[09/26 02:48:42 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 2.1746
[09/26 02:48:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 39.00	top5: 71.50	
[09/26 02:48:42 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 02:48:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 02:48:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 02:48:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 02:48:42 visual_prompt]: Training with config:
[09/26 02:48:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 02:48:42 visual_prompt]: Loading training data...
[09/26 02:48:42 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 02:48:44 visual_prompt]: Number of images: 800
[09/26 02:48:44 visual_prompt]: Number of classes: 47 / 47
[09/26 02:48:44 visual_prompt]: Loading validation data...
[09/26 02:48:44 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 02:48:45 visual_prompt]: Number of images: 200
[09/26 02:48:45 visual_prompt]: Number of classes: 47 / 47
[09/26 02:48:45 visual_prompt]: Constructing models...
[09/26 02:48:48 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 02:48:48 visual_prompt]: tuned percent:0.576
[09/26 02:48:48 visual_prompt]: Device used for model: 0
[09/26 02:48:48 visual_prompt]: Setting up Evaluator...
[09/26 02:48:48 visual_prompt]: Setting up Trainer...
[09/26 02:48:48 visual_prompt]: 	Setting up the optimizer...
[09/26 02:48:48 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 02:48:54 visual_prompt]: Epoch 1 / 100: avg data time: 4.86e-02, avg batch time: 0.4968, average train loss: 3.9322
[09/26 02:48:56 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1690, average loss: 3.9045
[09/26 02:48:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 02:48:56 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 02:48:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 02:49:03 visual_prompt]: Epoch 2 / 100: avg data time: 5.22e-02, avg batch time: 0.4988, average train loss: 3.8637
[09/26 02:49:04 visual_prompt]: Inference (val):avg data time: 4.98e-05, avg batch time: 0.1691, average loss: 3.8926
[09/26 02:49:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 10.50	
[09/26 02:49:04 visual_prompt]: Best epoch 2: best metric: 0.045
[09/26 02:49:04 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 02:49:11 visual_prompt]: Epoch 3 / 100: avg data time: 6.25e-02, avg batch time: 0.5097, average train loss: 3.8039
[09/26 02:49:13 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1690, average loss: 3.8252
[09/26 02:49:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 19.00	
[09/26 02:49:13 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 02:49:19 visual_prompt]: Epoch 4 / 100: avg data time: 5.53e-02, avg batch time: 0.5050, average train loss: 3.6387
[09/26 02:49:21 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1693, average loss: 3.5734
[09/26 02:49:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.00	top5: 27.00	
[09/26 02:49:21 visual_prompt]: Best epoch 4: best metric: 0.090
[09/26 02:49:21 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 02:49:28 visual_prompt]: Epoch 5 / 100: avg data time: 4.89e-02, avg batch time: 0.4988, average train loss: 3.4407
[09/26 02:49:29 visual_prompt]: Inference (val):avg data time: 5.79e-05, avg batch time: 0.1696, average loss: 3.5307
[09/26 02:49:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.00	top5: 34.50	
[09/26 02:49:29 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 02:49:36 visual_prompt]: Epoch 6 / 100: avg data time: 5.18e-02, avg batch time: 0.5010, average train loss: 3.0071
[09/26 02:49:38 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1696, average loss: 3.8300
[09/26 02:49:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 34.50	
[09/26 02:49:38 visual_prompt]: Best epoch 6: best metric: 0.110
[09/26 02:49:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 02:49:44 visual_prompt]: Epoch 7 / 100: avg data time: 4.40e-02, avg batch time: 0.4939, average train loss: 2.5511
[09/26 02:49:46 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 2.5939
[09/26 02:49:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 32.50	top5: 61.00	
[09/26 02:49:46 visual_prompt]: Best epoch 7: best metric: 0.325
[09/26 02:49:46 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 02:49:53 visual_prompt]: Epoch 8 / 100: avg data time: 5.55e-02, avg batch time: 0.5043, average train loss: 1.6316
[09/26 02:49:54 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1693, average loss: 2.1270
[09/26 02:49:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 72.50	
[09/26 02:49:54 visual_prompt]: Best epoch 8: best metric: 0.415
[09/26 02:49:54 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 02:50:01 visual_prompt]: Epoch 9 / 100: avg data time: 4.87e-02, avg batch time: 0.4982, average train loss: 0.8590
[09/26 02:50:02 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1693, average loss: 1.7009
[09/26 02:50:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 02:50:02 visual_prompt]: Best epoch 9: best metric: 0.475
[09/26 02:50:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 02:50:09 visual_prompt]: Epoch 10 / 100: avg data time: 4.38e-02, avg batch time: 0.4941, average train loss: 0.3859
[09/26 02:50:11 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1694, average loss: 1.8444
[09/26 02:50:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 81.50	
[09/26 02:50:11 visual_prompt]: Best epoch 10: best metric: 0.535
[09/26 02:50:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 02:50:17 visual_prompt]: Epoch 11 / 100: avg data time: 5.15e-02, avg batch time: 0.5000, average train loss: 0.2139
[09/26 02:50:19 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1694, average loss: 1.7032
[09/26 02:50:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 86.00	
[09/26 02:50:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 02:50:26 visual_prompt]: Epoch 12 / 100: avg data time: 4.46e-02, avg batch time: 0.4951, average train loss: 0.1191
[09/26 02:50:27 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1695, average loss: 1.6791
[09/26 02:50:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 85.50	
[09/26 02:50:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 02:50:34 visual_prompt]: Epoch 13 / 100: avg data time: 4.58e-02, avg batch time: 0.4944, average train loss: 0.0723
[09/26 02:50:35 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1698, average loss: 1.5845
[09/26 02:50:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 85.00	
[09/26 02:50:35 visual_prompt]: Best epoch 13: best metric: 0.570
[09/26 02:50:35 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 02:50:42 visual_prompt]: Epoch 14 / 100: avg data time: 4.19e-02, avg batch time: 0.4918, average train loss: 0.0624
[09/26 02:50:43 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1698, average loss: 1.5679
[09/26 02:50:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 84.50	
[09/26 02:50:43 visual_prompt]: Best epoch 14: best metric: 0.580
[09/26 02:50:43 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 02:50:50 visual_prompt]: Epoch 15 / 100: avg data time: 5.04e-02, avg batch time: 0.4996, average train loss: 0.0589
[09/26 02:50:52 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 1.5561
[09/26 02:50:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 87.00	
[09/26 02:50:52 visual_prompt]: Best epoch 15: best metric: 0.585
[09/26 02:50:52 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 02:50:58 visual_prompt]: Epoch 16 / 100: avg data time: 4.72e-02, avg batch time: 0.4978, average train loss: 0.0577
[09/26 02:51:00 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1700, average loss: 1.5541
[09/26 02:51:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 02:51:00 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 02:51:07 visual_prompt]: Epoch 17 / 100: avg data time: 4.96e-02, avg batch time: 0.5000, average train loss: 0.0581
[09/26 02:51:08 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1697, average loss: 1.5542
[09/26 02:51:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 85.00	
[09/26 02:51:08 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 02:51:15 visual_prompt]: Epoch 18 / 100: avg data time: 4.47e-02, avg batch time: 0.4966, average train loss: 0.0588
[09/26 02:51:16 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1695, average loss: 1.5460
[09/26 02:51:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.00	
[09/26 02:51:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 02:51:23 visual_prompt]: Epoch 19 / 100: avg data time: 4.99e-02, avg batch time: 0.4999, average train loss: 0.0689
[09/26 02:51:25 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1694, average loss: 1.6611
[09/26 02:51:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 02:51:25 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 02:51:31 visual_prompt]: Epoch 20 / 100: avg data time: 4.46e-02, avg batch time: 0.4939, average train loss: 0.2771
[09/26 02:51:33 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1700, average loss: 1.9661
[09/26 02:51:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 83.50	
[09/26 02:51:33 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 02:51:40 visual_prompt]: Epoch 21 / 100: avg data time: 5.08e-02, avg batch time: 0.5017, average train loss: 1.1565
[09/26 02:51:41 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1695, average loss: 2.1316
[09/26 02:51:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 76.00	
[09/26 02:51:41 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 02:51:48 visual_prompt]: Epoch 22 / 100: avg data time: 5.01e-02, avg batch time: 0.4990, average train loss: 0.8724
[09/26 02:51:50 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1698, average loss: 1.6301
[09/26 02:51:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 87.50	
[09/26 02:51:50 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 02:51:56 visual_prompt]: Epoch 23 / 100: avg data time: 6.01e-02, avg batch time: 0.5086, average train loss: 0.2692
[09/26 02:51:58 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1697, average loss: 1.5454
[09/26 02:51:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 88.50	
[09/26 02:51:58 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 02:52:05 visual_prompt]: Epoch 24 / 100: avg data time: 5.55e-02, avg batch time: 0.5041, average train loss: 0.1161
[09/26 02:52:06 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1693, average loss: 1.5301
[09/26 02:52:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 89.50	
[09/26 02:52:06 visual_prompt]: Best epoch 24: best metric: 0.600
[09/26 02:52:06 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 02:52:13 visual_prompt]: Epoch 25 / 100: avg data time: 4.19e-02, avg batch time: 0.4921, average train loss: 0.0749
[09/26 02:52:14 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1697, average loss: 1.4933
[09/26 02:52:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 88.00	
[09/26 02:52:14 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 02:52:21 visual_prompt]: Epoch 26 / 100: avg data time: 5.91e-02, avg batch time: 0.5089, average train loss: 0.0589
[09/26 02:52:23 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1695, average loss: 1.4263
[09/26 02:52:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 88.50	
[09/26 02:52:23 visual_prompt]: Best epoch 26: best metric: 0.605
[09/26 02:52:23 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 02:52:30 visual_prompt]: Epoch 27 / 100: avg data time: 6.06e-02, avg batch time: 0.5090, average train loss: 0.0504
[09/26 02:52:31 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1697, average loss: 1.4314
[09/26 02:52:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 89.50	
[09/26 02:52:31 visual_prompt]: Best epoch 27: best metric: 0.630
[09/26 02:52:31 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 02:52:38 visual_prompt]: Epoch 28 / 100: avg data time: 5.17e-02, avg batch time: 0.5020, average train loss: 0.0508
[09/26 02:52:40 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.4381
[09/26 02:52:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 88.00	
[09/26 02:52:40 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 02:52:47 visual_prompt]: Epoch 29 / 100: avg data time: 5.03e-02, avg batch time: 0.4987, average train loss: 0.0533
[09/26 02:52:48 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1695, average loss: 1.4284
[09/26 02:52:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 90.00	
[09/26 02:52:48 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 02:52:55 visual_prompt]: Epoch 30 / 100: avg data time: 4.42e-02, avg batch time: 0.4933, average train loss: 0.0523
[09/26 02:52:56 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1693, average loss: 1.4375
[09/26 02:52:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 89.50	
[09/26 02:52:56 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 02:53:03 visual_prompt]: Epoch 31 / 100: avg data time: 4.76e-02, avg batch time: 0.4975, average train loss: 0.0506
[09/26 02:53:05 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1698, average loss: 1.4458
[09/26 02:53:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 89.00	
[09/26 02:53:05 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 02:53:11 visual_prompt]: Epoch 32 / 100: avg data time: 5.72e-02, avg batch time: 0.5075, average train loss: 0.0513
[09/26 02:53:13 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1698, average loss: 1.5542
[09/26 02:53:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 89.00	
[09/26 02:53:13 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 02:53:20 visual_prompt]: Epoch 33 / 100: avg data time: 4.14e-02, avg batch time: 0.4910, average train loss: 0.0847
[09/26 02:53:21 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1699, average loss: 1.5537
[09/26 02:53:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 87.00	
[09/26 02:53:21 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 02:53:28 visual_prompt]: Epoch 34 / 100: avg data time: 5.08e-02, avg batch time: 0.4999, average train loss: 0.5003
[09/26 02:53:30 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1698, average loss: 3.5926
[09/26 02:53:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.50	top5: 24.00	
[09/26 02:53:30 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 02:53:36 visual_prompt]: Epoch 35 / 100: avg data time: 5.42e-02, avg batch time: 0.5030, average train loss: 1.8984
[09/26 02:53:38 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1697, average loss: 1.7640
[09/26 02:53:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 81.50	
[09/26 02:53:38 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 02:53:45 visual_prompt]: Epoch 36 / 100: avg data time: 4.03e-02, avg batch time: 0.4901, average train loss: 0.4108
[09/26 02:53:46 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1698, average loss: 1.5258
[09/26 02:53:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 87.50	
[09/26 02:53:46 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 02:53:53 visual_prompt]: Epoch 37 / 100: avg data time: 4.53e-02, avg batch time: 0.4941, average train loss: 0.1489
[09/26 02:53:54 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1699, average loss: 1.4066
[09/26 02:53:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 93.00	
[09/26 02:53:54 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 02:54:01 visual_prompt]: Epoch 38 / 100: avg data time: 4.98e-02, avg batch time: 0.4987, average train loss: 0.0777
[09/26 02:54:03 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1695, average loss: 1.4684
[09/26 02:54:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 02:54:03 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 02:54:09 visual_prompt]: Epoch 39 / 100: avg data time: 4.80e-02, avg batch time: 0.4981, average train loss: 0.0581
[09/26 02:54:11 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1697, average loss: 1.3845
[09/26 02:54:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 91.50	
[09/26 02:54:11 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 02:54:18 visual_prompt]: Epoch 40 / 100: avg data time: 4.51e-02, avg batch time: 0.4953, average train loss: 0.0495
[09/26 02:54:19 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1697, average loss: 1.3830
[09/26 02:54:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.50	top5: 91.00	
[09/26 02:54:19 visual_prompt]: Best epoch 40: best metric: 0.635
[09/26 02:54:19 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 02:54:26 visual_prompt]: Epoch 41 / 100: avg data time: 4.22e-02, avg batch time: 0.4926, average train loss: 0.0480
[09/26 02:54:27 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1696, average loss: 1.3947
[09/26 02:54:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 92.00	
[09/26 02:54:27 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 02:54:34 visual_prompt]: Epoch 42 / 100: avg data time: 4.53e-02, avg batch time: 0.4960, average train loss: 0.0460
[09/26 02:54:36 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1697, average loss: 1.3998
[09/26 02:54:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 92.00	
[09/26 02:54:36 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 02:54:42 visual_prompt]: Epoch 43 / 100: avg data time: 4.19e-02, avg batch time: 0.4933, average train loss: 0.0446
[09/26 02:54:44 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1700, average loss: 1.3613
[09/26 02:54:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 92.00	
[09/26 02:54:44 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 02:54:51 visual_prompt]: Epoch 44 / 100: avg data time: 6.20e-02, avg batch time: 0.5114, average train loss: 0.0429
[09/26 02:54:52 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1699, average loss: 1.4049
[09/26 02:54:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 91.00	
[09/26 02:54:52 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 02:54:59 visual_prompt]: Epoch 45 / 100: avg data time: 5.75e-02, avg batch time: 0.5079, average train loss: 0.0450
[09/26 02:55:01 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1697, average loss: 1.4011
[09/26 02:55:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 90.50	
[09/26 02:55:01 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 02:55:07 visual_prompt]: Epoch 46 / 100: avg data time: 4.72e-02, avg batch time: 0.4970, average train loss: 0.0452
[09/26 02:55:09 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.1697, average loss: 1.3962
[09/26 02:55:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 65.50	top5: 91.50	
[09/26 02:55:09 visual_prompt]: Best epoch 46: best metric: 0.655
[09/26 02:55:09 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 02:55:16 visual_prompt]: Epoch 47 / 100: avg data time: 5.48e-02, avg batch time: 0.5043, average train loss: 0.0488
[09/26 02:55:17 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1698, average loss: 1.4186
[09/26 02:55:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 93.00	
[09/26 02:55:17 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 02:55:24 visual_prompt]: Epoch 48 / 100: avg data time: 5.80e-02, avg batch time: 0.5066, average train loss: 0.0772
[09/26 02:55:26 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1699, average loss: 1.5816
[09/26 02:55:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 89.00	
[09/26 02:55:26 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 02:55:32 visual_prompt]: Epoch 49 / 100: avg data time: 6.06e-02, avg batch time: 0.5090, average train loss: 0.1071
[09/26 02:55:34 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1699, average loss: 1.4438
[09/26 02:55:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 91.50	
[09/26 02:55:34 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 02:55:41 visual_prompt]: Epoch 50 / 100: avg data time: 5.43e-02, avg batch time: 0.5041, average train loss: 0.1075
[09/26 02:55:42 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1698, average loss: 1.5428
[09/26 02:55:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 87.50	
[09/26 02:55:42 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 02:55:49 visual_prompt]: Epoch 51 / 100: avg data time: 4.45e-02, avg batch time: 0.4952, average train loss: 0.1183
[09/26 02:55:51 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1699, average loss: 1.5450
[09/26 02:55:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 90.50	
[09/26 02:55:51 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 02:55:57 visual_prompt]: Epoch 52 / 100: avg data time: 5.39e-02, avg batch time: 0.5030, average train loss: 0.0763
[09/26 02:55:59 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1697, average loss: 1.4660
[09/26 02:55:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 91.50	
[09/26 02:55:59 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 02:56:06 visual_prompt]: Epoch 53 / 100: avg data time: 5.29e-02, avg batch time: 0.5039, average train loss: 0.0457
[09/26 02:56:07 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1696, average loss: 1.4632
[09/26 02:56:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 90.50	
[09/26 02:56:07 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 02:56:14 visual_prompt]: Epoch 54 / 100: avg data time: 4.44e-02, avg batch time: 0.4946, average train loss: 0.0347
[09/26 02:56:15 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1697, average loss: 1.4716
[09/26 02:56:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 92.00	
[09/26 02:56:15 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 02:56:22 visual_prompt]: Epoch 55 / 100: avg data time: 6.05e-02, avg batch time: 0.5093, average train loss: 0.0275
[09/26 02:56:24 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 1.4304
[09/26 02:56:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 90.50	
[09/26 02:56:24 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 02:56:31 visual_prompt]: Epoch 56 / 100: avg data time: 4.87e-02, avg batch time: 0.4987, average train loss: 0.0244
[09/26 02:56:32 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1698, average loss: 1.4669
[09/26 02:56:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 02:56:32 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 02:56:39 visual_prompt]: Epoch 57 / 100: avg data time: 5.33e-02, avg batch time: 0.5028, average train loss: 0.0231
[09/26 02:56:40 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1696, average loss: 1.4565
[09/26 02:56:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 91.00	
[09/26 02:56:40 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 02:56:47 visual_prompt]: Epoch 58 / 100: avg data time: 5.68e-02, avg batch time: 0.5061, average train loss: 0.0224
[09/26 02:56:49 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.4577
[09/26 02:56:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 90.50	
[09/26 02:56:49 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 02:56:56 visual_prompt]: Epoch 59 / 100: avg data time: 4.46e-02, avg batch time: 0.4933, average train loss: 0.0224
[09/26 02:56:57 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1697, average loss: 1.4406
[09/26 02:56:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 91.00	
[09/26 02:56:57 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 02:57:04 visual_prompt]: Epoch 60 / 100: avg data time: 5.94e-02, avg batch time: 0.5089, average train loss: 0.0223
[09/26 02:57:06 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 1.4551
[09/26 02:57:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 93.00	
[09/26 02:57:06 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 02:57:12 visual_prompt]: Epoch 61 / 100: avg data time: 4.75e-02, avg batch time: 0.4969, average train loss: 0.0219
[09/26 02:57:14 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1697, average loss: 1.4632
[09/26 02:57:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 90.00	
[09/26 02:57:14 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 02:57:21 visual_prompt]: Epoch 62 / 100: avg data time: 4.45e-02, avg batch time: 0.4946, average train loss: 0.0223
[09/26 02:57:22 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 1.4877
[09/26 02:57:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 91.50	
[09/26 02:57:22 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 02:57:29 visual_prompt]: Epoch 63 / 100: avg data time: 4.92e-02, avg batch time: 0.4989, average train loss: 0.0217
[09/26 02:57:30 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1697, average loss: 1.4833
[09/26 02:57:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 91.00	
[09/26 02:57:30 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 02:57:37 visual_prompt]: Epoch 64 / 100: avg data time: 5.90e-02, avg batch time: 0.5075, average train loss: 0.0203
[09/26 02:57:39 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1696, average loss: 1.5142
[09/26 02:57:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 92.00	
[09/26 02:57:39 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 02:57:46 visual_prompt]: Epoch 65 / 100: avg data time: 4.82e-02, avg batch time: 0.4972, average train loss: 0.0212
[09/26 02:57:47 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1695, average loss: 1.5106
[09/26 02:57:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 90.50	
[09/26 02:57:47 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 02:57:54 visual_prompt]: Epoch 66 / 100: avg data time: 5.42e-02, avg batch time: 0.5027, average train loss: 0.0232
[09/26 02:57:56 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1696, average loss: 1.6104
[09/26 02:57:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 91.00	
[09/26 02:57:56 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 02:58:02 visual_prompt]: Epoch 67 / 100: avg data time: 4.54e-02, avg batch time: 0.4962, average train loss: 0.2108
[09/26 02:58:04 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1697, average loss: 2.0127
[09/26 02:58:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 86.00	
[09/26 02:58:04 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 02:58:11 visual_prompt]: Epoch 68 / 100: avg data time: 6.24e-02, avg batch time: 0.5116, average train loss: 1.6855
[09/26 02:58:12 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1699, average loss: 1.7264
[09/26 02:58:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 87.00	
[09/26 02:58:12 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 02:58:19 visual_prompt]: Epoch 69 / 100: avg data time: 5.79e-02, avg batch time: 0.5075, average train loss: 0.5661
[09/26 02:58:21 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 1.4606
[09/26 02:58:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.50	
[09/26 02:58:21 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 02:58:27 visual_prompt]: Epoch 70 / 100: avg data time: 5.73e-02, avg batch time: 0.5072, average train loss: 0.2513
[09/26 02:58:29 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1697, average loss: 1.4299
[09/26 02:58:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 91.50	
[09/26 02:58:29 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 02:58:36 visual_prompt]: Epoch 71 / 100: avg data time: 5.87e-02, avg batch time: 0.5079, average train loss: 0.1238
[09/26 02:58:37 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1697, average loss: 1.4084
[09/26 02:58:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 91.50	
[09/26 02:58:37 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 02:58:44 visual_prompt]: Epoch 72 / 100: avg data time: 5.90e-02, avg batch time: 0.5080, average train loss: 0.0758
[09/26 02:58:46 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1699, average loss: 1.3667
[09/26 02:58:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 91.50	
[09/26 02:58:46 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 02:58:53 visual_prompt]: Epoch 73 / 100: avg data time: 5.60e-02, avg batch time: 0.5044, average train loss: 0.0535
[09/26 02:58:54 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1698, average loss: 1.3923
[09/26 02:58:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.50	
[09/26 02:58:54 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 02:59:01 visual_prompt]: Epoch 74 / 100: avg data time: 5.86e-02, avg batch time: 0.5097, average train loss: 0.0435
[09/26 02:59:02 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1692, average loss: 1.3885
[09/26 02:59:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 91.50	
[09/26 02:59:02 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 02:59:09 visual_prompt]: Epoch 75 / 100: avg data time: 4.41e-02, avg batch time: 0.4959, average train loss: 0.0393
[09/26 02:59:11 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1697, average loss: 1.3823
[09/26 02:59:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 91.00	
[09/26 02:59:11 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 02:59:17 visual_prompt]: Epoch 76 / 100: avg data time: 5.70e-02, avg batch time: 0.5072, average train loss: 0.0359
[09/26 02:59:19 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1694, average loss: 1.3843
[09/26 02:59:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 90.50	
[09/26 02:59:19 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 02:59:26 visual_prompt]: Epoch 77 / 100: avg data time: 4.49e-02, avg batch time: 0.4961, average train loss: 0.0338
[09/26 02:59:27 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1697, average loss: 1.3876
[09/26 02:59:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 90.00	
[09/26 02:59:27 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 02:59:34 visual_prompt]: Epoch 78 / 100: avg data time: 5.54e-02, avg batch time: 0.5055, average train loss: 0.0320
[09/26 02:59:36 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1693, average loss: 1.4155
[09/26 02:59:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 02:59:36 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 02:59:42 visual_prompt]: Epoch 79 / 100: avg data time: 5.93e-02, avg batch time: 0.5094, average train loss: 0.0306
[09/26 02:59:44 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1695, average loss: 1.4027
[09/26 02:59:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 90.50	
[09/26 02:59:44 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 02:59:51 visual_prompt]: Epoch 80 / 100: avg data time: 6.14e-02, avg batch time: 0.5107, average train loss: 0.0298
[09/26 02:59:52 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1694, average loss: 1.4017
[09/26 02:59:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 90.50	
[09/26 02:59:52 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 02:59:59 visual_prompt]: Epoch 81 / 100: avg data time: 4.93e-02, avg batch time: 0.4979, average train loss: 0.0284
[09/26 03:00:01 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1697, average loss: 1.4038
[09/26 03:00:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 91.50	
[09/26 03:00:01 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 03:00:08 visual_prompt]: Epoch 82 / 100: avg data time: 6.32e-02, avg batch time: 0.5121, average train loss: 0.0278
[09/26 03:00:09 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1693, average loss: 1.4151
[09/26 03:00:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 90.50	
[09/26 03:00:09 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 03:00:16 visual_prompt]: Epoch 83 / 100: avg data time: 4.55e-02, avg batch time: 0.4954, average train loss: 0.0269
[09/26 03:00:17 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1695, average loss: 1.4140
[09/26 03:00:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:00:17 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 03:00:24 visual_prompt]: Epoch 84 / 100: avg data time: 5.31e-02, avg batch time: 0.5017, average train loss: 0.0260
[09/26 03:00:26 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1699, average loss: 1.4141
[09/26 03:00:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 91.50	
[09/26 03:00:26 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 03:00:33 visual_prompt]: Epoch 85 / 100: avg data time: 5.71e-02, avg batch time: 0.5061, average train loss: 0.0256
[09/26 03:00:34 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1698, average loss: 1.4144
[09/26 03:00:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:00:34 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 03:00:41 visual_prompt]: Epoch 86 / 100: avg data time: 4.24e-02, avg batch time: 0.4916, average train loss: 0.0252
[09/26 03:00:42 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1699, average loss: 1.4130
[09/26 03:00:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 91.00	
[09/26 03:00:42 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 03:00:49 visual_prompt]: Epoch 87 / 100: avg data time: 5.93e-02, avg batch time: 0.5076, average train loss: 0.0248
[09/26 03:00:51 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 1.4084
[09/26 03:00:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:00:51 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 03:00:57 visual_prompt]: Epoch 88 / 100: avg data time: 4.48e-02, avg batch time: 0.4964, average train loss: 0.0248
[09/26 03:00:59 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 1.4098
[09/26 03:00:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:00:59 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 03:01:06 visual_prompt]: Epoch 89 / 100: avg data time: 5.94e-02, avg batch time: 0.5075, average train loss: 0.0241
[09/26 03:01:07 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1697, average loss: 1.4132
[09/26 03:01:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:01:07 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 03:01:14 visual_prompt]: Epoch 90 / 100: avg data time: 4.99e-02, avg batch time: 0.5004, average train loss: 0.0239
[09/26 03:01:16 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1696, average loss: 1.4151
[09/26 03:01:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:01:16 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 03:01:22 visual_prompt]: Epoch 91 / 100: avg data time: 5.84e-02, avg batch time: 0.5074, average train loss: 0.0237
[09/26 03:01:24 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1698, average loss: 1.4177
[09/26 03:01:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 91.00	
[09/26 03:01:24 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 03:01:31 visual_prompt]: Epoch 92 / 100: avg data time: 4.23e-02, avg batch time: 0.4932, average train loss: 0.0234
[09/26 03:01:32 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1697, average loss: 1.4170
[09/26 03:01:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 91.00	
[09/26 03:01:32 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 03:01:39 visual_prompt]: Epoch 93 / 100: avg data time: 5.19e-02, avg batch time: 0.5005, average train loss: 0.0233
[09/26 03:01:41 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1694, average loss: 1.4172
[09/26 03:01:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:01:41 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 03:01:47 visual_prompt]: Epoch 94 / 100: avg data time: 5.22e-02, avg batch time: 0.5018, average train loss: 0.0232
[09/26 03:01:49 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1697, average loss: 1.4199
[09/26 03:01:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:01:49 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 03:01:56 visual_prompt]: Epoch 95 / 100: avg data time: 4.52e-02, avg batch time: 0.4962, average train loss: 0.0235
[09/26 03:01:57 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1699, average loss: 1.4194
[09/26 03:01:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:01:57 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 03:02:04 visual_prompt]: Epoch 96 / 100: avg data time: 6.21e-02, avg batch time: 0.5108, average train loss: 0.0230
[09/26 03:02:05 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1696, average loss: 1.4176
[09/26 03:02:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:02:05 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 03:02:12 visual_prompt]: Epoch 97 / 100: avg data time: 5.57e-02, avg batch time: 0.5054, average train loss: 0.0230
[09/26 03:02:14 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1697, average loss: 1.4172
[09/26 03:02:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:02:14 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 03:02:21 visual_prompt]: Epoch 98 / 100: avg data time: 6.00e-02, avg batch time: 0.5101, average train loss: 0.0232
[09/26 03:02:22 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1696, average loss: 1.4180
[09/26 03:02:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:02:22 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 03:02:29 visual_prompt]: Epoch 99 / 100: avg data time: 4.46e-02, avg batch time: 0.4960, average train loss: 0.0229
[09/26 03:02:31 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1697, average loss: 1.4184
[09/26 03:02:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:02:31 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 03:02:37 visual_prompt]: Epoch 100 / 100: avg data time: 4.80e-02, avg batch time: 0.4991, average train loss: 0.0229
[09/26 03:02:39 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1690, average loss: 1.4184
[09/26 03:02:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 91.00	
[09/26 03:02:39 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 03:02:39 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 03:02:39 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 03:02:39 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 03:02:39 visual_prompt]: Training with config:
[09/26 03:02:39 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 03:02:39 visual_prompt]: Loading training data...
[09/26 03:02:39 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 03:02:41 visual_prompt]: Number of images: 800
[09/26 03:02:41 visual_prompt]: Number of classes: 47 / 47
[09/26 03:02:41 visual_prompt]: Loading validation data...
[09/26 03:02:41 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 03:02:41 visual_prompt]: Number of images: 200
[09/26 03:02:41 visual_prompt]: Number of classes: 47 / 47
[09/26 03:02:41 visual_prompt]: Constructing models...
[09/26 03:02:44 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 03:02:44 visual_prompt]: tuned percent:0.576
[09/26 03:02:44 visual_prompt]: Device used for model: 0
[09/26 03:02:44 visual_prompt]: Setting up Evaluator...
[09/26 03:02:44 visual_prompt]: Setting up Trainer...
[09/26 03:02:44 visual_prompt]: 	Setting up the optimizer...
[09/26 03:02:44 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 03:02:51 visual_prompt]: Epoch 1 / 100: avg data time: 4.83e-02, avg batch time: 0.4956, average train loss: 3.9342
[09/26 03:02:52 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1692, average loss: 3.9045
[09/26 03:02:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 03:02:52 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 03:02:52 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 03:02:59 visual_prompt]: Epoch 2 / 100: avg data time: 5.07e-02, avg batch time: 0.5005, average train loss: 3.8743
[09/26 03:03:01 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1690, average loss: 3.9276
[09/26 03:03:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 8.00	
[09/26 03:03:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 03:03:07 visual_prompt]: Epoch 3 / 100: avg data time: 5.26e-02, avg batch time: 0.5015, average train loss: 3.8094
[09/26 03:03:09 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 3.8468
[09/26 03:03:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 20.00	
[09/26 03:03:09 visual_prompt]: Best epoch 3: best metric: 0.065
[09/26 03:03:09 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 03:03:16 visual_prompt]: Epoch 4 / 100: avg data time: 4.60e-02, avg batch time: 0.4952, average train loss: 3.7656
[09/26 03:03:17 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 3.7368
[09/26 03:03:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 16.50	
[09/26 03:03:17 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 03:03:24 visual_prompt]: Epoch 5 / 100: avg data time: 4.59e-02, avg batch time: 0.4934, average train loss: 3.7338
[09/26 03:03:25 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 3.8189
[09/26 03:03:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 14.50	
[09/26 03:03:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 03:03:32 visual_prompt]: Epoch 6 / 100: avg data time: 6.15e-02, avg batch time: 0.5100, average train loss: 3.6484
[09/26 03:03:34 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 3.6782
[09/26 03:03:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 18.00	
[09/26 03:03:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 03:03:40 visual_prompt]: Epoch 7 / 100: avg data time: 4.26e-02, avg batch time: 0.4910, average train loss: 3.3451
[09/26 03:03:42 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1694, average loss: 3.4588
[09/26 03:03:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.50	top5: 33.50	
[09/26 03:03:42 visual_prompt]: Best epoch 7: best metric: 0.115
[09/26 03:03:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 03:03:49 visual_prompt]: Epoch 8 / 100: avg data time: 6.00e-02, avg batch time: 0.5082, average train loss: 3.2072
[09/26 03:03:50 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1690, average loss: 3.5067
[09/26 03:03:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.50	top5: 34.50	
[09/26 03:03:50 visual_prompt]: Best epoch 8: best metric: 0.125
[09/26 03:03:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 03:03:57 visual_prompt]: Epoch 9 / 100: avg data time: 5.61e-02, avg batch time: 0.5056, average train loss: 2.4844
[09/26 03:03:59 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1695, average loss: 2.9556
[09/26 03:03:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.00	top5: 60.00	
[09/26 03:03:59 visual_prompt]: Best epoch 9: best metric: 0.250
[09/26 03:03:59 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 03:04:06 visual_prompt]: Epoch 10 / 100: avg data time: 5.74e-02, avg batch time: 0.5052, average train loss: 1.6221
[09/26 03:04:07 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1699, average loss: 2.3433
[09/26 03:04:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 39.00	top5: 71.00	
[09/26 03:04:07 visual_prompt]: Best epoch 10: best metric: 0.390
[09/26 03:04:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 03:04:14 visual_prompt]: Epoch 11 / 100: avg data time: 5.02e-02, avg batch time: 0.4986, average train loss: 0.9144
[09/26 03:04:15 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1693, average loss: 2.2210
[09/26 03:04:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 80.00	
[09/26 03:04:15 visual_prompt]: Best epoch 11: best metric: 0.425
[09/26 03:04:15 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 03:04:22 visual_prompt]: Epoch 12 / 100: avg data time: 5.67e-02, avg batch time: 0.5047, average train loss: 0.5085
[09/26 03:04:24 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 1.9778
[09/26 03:04:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 84.00	
[09/26 03:04:24 visual_prompt]: Best epoch 12: best metric: 0.490
[09/26 03:04:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 03:04:31 visual_prompt]: Epoch 13 / 100: avg data time: 5.78e-02, avg batch time: 0.5068, average train loss: 0.2082
[09/26 03:04:32 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1690, average loss: 1.7388
[09/26 03:04:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 83.00	
[09/26 03:04:32 visual_prompt]: Best epoch 13: best metric: 0.555
[09/26 03:04:32 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 03:04:39 visual_prompt]: Epoch 14 / 100: avg data time: 5.87e-02, avg batch time: 0.5066, average train loss: 0.0663
[09/26 03:04:41 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 1.8095
[09/26 03:04:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 84.50	
[09/26 03:04:41 visual_prompt]: Best epoch 14: best metric: 0.560
[09/26 03:04:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 03:04:47 visual_prompt]: Epoch 15 / 100: avg data time: 5.08e-02, avg batch time: 0.4988, average train loss: 0.0269
[09/26 03:04:49 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 1.9176
[09/26 03:04:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 82.50	
[09/26 03:04:49 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 03:04:56 visual_prompt]: Epoch 16 / 100: avg data time: 4.46e-02, avg batch time: 0.4929, average train loss: 0.0159
[09/26 03:04:57 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1693, average loss: 1.8203
[09/26 03:04:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 85.00	
[09/26 03:04:57 visual_prompt]: Best epoch 16: best metric: 0.575
[09/26 03:04:57 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 03:05:04 visual_prompt]: Epoch 17 / 100: avg data time: 5.81e-02, avg batch time: 0.5076, average train loss: 0.0116
[09/26 03:05:05 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1697, average loss: 1.7912
[09/26 03:05:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.00	
[09/26 03:05:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 03:05:12 visual_prompt]: Epoch 18 / 100: avg data time: 4.87e-02, avg batch time: 0.4988, average train loss: 0.0097
[09/26 03:05:14 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1700, average loss: 1.7955
[09/26 03:05:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 85.00	
[09/26 03:05:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 03:05:21 visual_prompt]: Epoch 19 / 100: avg data time: 5.84e-02, avg batch time: 0.5065, average train loss: 0.0081
[09/26 03:05:22 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 1.8201
[09/26 03:05:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 84.00	
[09/26 03:05:22 visual_prompt]: Best epoch 19: best metric: 0.585
[09/26 03:05:22 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 03:05:29 visual_prompt]: Epoch 20 / 100: avg data time: 4.64e-02, avg batch time: 0.4961, average train loss: 0.0072
[09/26 03:05:30 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1696, average loss: 1.7976
[09/26 03:05:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 85.50	
[09/26 03:05:30 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 03:05:37 visual_prompt]: Epoch 21 / 100: avg data time: 5.67e-02, avg batch time: 0.5053, average train loss: 0.0069
[09/26 03:05:39 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1692, average loss: 1.7829
[09/26 03:05:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.50	
[09/26 03:05:39 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 03:05:46 visual_prompt]: Epoch 22 / 100: avg data time: 5.78e-02, avg batch time: 0.5062, average train loss: 0.0064
[09/26 03:05:47 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1699, average loss: 1.7806
[09/26 03:05:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.50	
[09/26 03:05:47 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 03:05:54 visual_prompt]: Epoch 23 / 100: avg data time: 5.57e-02, avg batch time: 0.5035, average train loss: 0.0065
[09/26 03:05:56 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1694, average loss: 1.7735
[09/26 03:05:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 03:05:56 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 03:06:02 visual_prompt]: Epoch 24 / 100: avg data time: 6.10e-02, avg batch time: 0.5087, average train loss: 0.0062
[09/26 03:06:04 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1696, average loss: 1.7678
[09/26 03:06:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 03:06:04 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 03:06:11 visual_prompt]: Epoch 25 / 100: avg data time: 4.49e-02, avg batch time: 0.4936, average train loss: 0.0067
[09/26 03:06:12 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1693, average loss: 1.7772
[09/26 03:06:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 03:06:12 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 03:06:19 visual_prompt]: Epoch 26 / 100: avg data time: 5.65e-02, avg batch time: 0.5054, average train loss: 0.0066
[09/26 03:06:21 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1693, average loss: 1.7480
[09/26 03:06:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 87.00	
[09/26 03:06:21 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 03:06:27 visual_prompt]: Epoch 27 / 100: avg data time: 4.49e-02, avg batch time: 0.4963, average train loss: 0.0064
[09/26 03:06:29 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1697, average loss: 1.7593
[09/26 03:06:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.00	
[09/26 03:06:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 03:06:35 visual_prompt]: Epoch 28 / 100: avg data time: 4.60e-02, avg batch time: 0.4958, average train loss: 0.0060
[09/26 03:06:37 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1697, average loss: 1.7786
[09/26 03:06:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 03:06:37 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 03:06:44 visual_prompt]: Epoch 29 / 100: avg data time: 4.38e-02, avg batch time: 0.4929, average train loss: 0.0060
[09/26 03:06:45 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1694, average loss: 1.7747
[09/26 03:06:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 03:06:45 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 03:06:52 visual_prompt]: Epoch 30 / 100: avg data time: 4.53e-02, avg batch time: 0.4940, average train loss: 0.0060
[09/26 03:06:53 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1698, average loss: 1.7779
[09/26 03:06:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 03:06:53 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 03:07:00 visual_prompt]: Epoch 31 / 100: avg data time: 4.75e-02, avg batch time: 0.4976, average train loss: 0.0056
[09/26 03:07:02 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1693, average loss: 1.7733
[09/26 03:07:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 86.00	
[09/26 03:07:02 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 03:07:08 visual_prompt]: Epoch 32 / 100: avg data time: 5.70e-02, avg batch time: 0.5056, average train loss: 0.0060
[09/26 03:07:10 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1698, average loss: 1.7654
[09/26 03:07:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.50	
[09/26 03:07:10 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 03:07:17 visual_prompt]: Epoch 33 / 100: avg data time: 6.04e-02, avg batch time: 0.5099, average train loss: 0.0057
[09/26 03:07:18 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1698, average loss: 1.7647
[09/26 03:07:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.00	
[09/26 03:07:18 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 03:07:25 visual_prompt]: Epoch 34 / 100: avg data time: 5.95e-02, avg batch time: 0.5084, average train loss: 0.0056
[09/26 03:07:27 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1695, average loss: 1.8040
[09/26 03:07:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 86.00	
[09/26 03:07:27 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 03:07:34 visual_prompt]: Epoch 35 / 100: avg data time: 5.25e-02, avg batch time: 0.5023, average train loss: 0.0058
[09/26 03:07:35 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1699, average loss: 1.7857
[09/26 03:07:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 86.00	
[09/26 03:07:35 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 03:07:42 visual_prompt]: Epoch 36 / 100: avg data time: 5.07e-02, avg batch time: 0.4993, average train loss: 0.0055
[09/26 03:07:44 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1698, average loss: 1.7582
[09/26 03:07:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 03:07:44 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 03:07:51 visual_prompt]: Epoch 37 / 100: avg data time: 5.82e-02, avg batch time: 0.5070, average train loss: 0.0055
[09/26 03:07:52 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1696, average loss: 1.7497
[09/26 03:07:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 86.00	
[09/26 03:07:52 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 03:07:59 visual_prompt]: Epoch 38 / 100: avg data time: 5.53e-02, avg batch time: 0.5059, average train loss: 0.0058
[09/26 03:08:00 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1698, average loss: 1.7517
[09/26 03:08:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 03:08:00 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 03:08:07 visual_prompt]: Epoch 39 / 100: avg data time: 5.81e-02, avg batch time: 0.5077, average train loss: 0.0055
[09/26 03:08:09 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1697, average loss: 1.7631
[09/26 03:08:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.50	
[09/26 03:08:09 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 03:08:16 visual_prompt]: Epoch 40 / 100: avg data time: 5.77e-02, avg batch time: 0.5067, average train loss: 0.0055
[09/26 03:08:17 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 1.7694
[09/26 03:08:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 03:08:17 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 03:08:24 visual_prompt]: Epoch 41 / 100: avg data time: 4.78e-02, avg batch time: 0.4972, average train loss: 0.0053
[09/26 03:08:26 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1695, average loss: 1.7577
[09/26 03:08:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.00	
[09/26 03:08:26 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 03:08:33 visual_prompt]: Epoch 42 / 100: avg data time: 6.51e-02, avg batch time: 0.5136, average train loss: 0.0052
[09/26 03:08:34 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1694, average loss: 1.7459
[09/26 03:08:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.50	
[09/26 03:08:34 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 03:08:41 visual_prompt]: Epoch 43 / 100: avg data time: 5.88e-02, avg batch time: 0.5079, average train loss: 0.0052
[09/26 03:08:42 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1697, average loss: 1.7554
[09/26 03:08:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.50	
[09/26 03:08:42 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 03:08:49 visual_prompt]: Epoch 44 / 100: avg data time: 5.68e-02, avg batch time: 0.5054, average train loss: 0.0052
[09/26 03:08:51 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1695, average loss: 1.7532
[09/26 03:08:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 03:08:51 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 03:08:58 visual_prompt]: Epoch 45 / 100: avg data time: 5.73e-02, avg batch time: 0.5068, average train loss: 0.0052
[09/26 03:08:59 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1694, average loss: 1.7524
[09/26 03:08:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 86.50	
[09/26 03:08:59 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 03:09:06 visual_prompt]: Epoch 46 / 100: avg data time: 5.79e-02, avg batch time: 0.5086, average train loss: 0.0052
[09/26 03:09:08 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 1.7552
[09/26 03:09:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.50	
[09/26 03:09:08 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 03:09:14 visual_prompt]: Epoch 47 / 100: avg data time: 5.85e-02, avg batch time: 0.5081, average train loss: 0.0052
[09/26 03:09:16 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 1.7392
[09/26 03:09:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 03:09:16 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 03:09:23 visual_prompt]: Epoch 48 / 100: avg data time: 5.79e-02, avg batch time: 0.5060, average train loss: 0.0052
[09/26 03:09:24 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1690, average loss: 1.7546
[09/26 03:09:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.00	
[09/26 03:09:24 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 03:09:31 visual_prompt]: Epoch 49 / 100: avg data time: 5.27e-02, avg batch time: 0.5007, average train loss: 0.0051
[09/26 03:09:33 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1699, average loss: 1.7606
[09/26 03:09:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.00	
[09/26 03:09:33 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 03:09:40 visual_prompt]: Epoch 50 / 100: avg data time: 5.66e-02, avg batch time: 0.5047, average train loss: 0.0050
[09/26 03:09:41 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1699, average loss: 1.7556
[09/26 03:09:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.00	
[09/26 03:09:41 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 03:09:48 visual_prompt]: Epoch 51 / 100: avg data time: 6.20e-02, avg batch time: 0.5100, average train loss: 0.0052
[09/26 03:09:50 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1692, average loss: 1.7622
[09/26 03:09:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 86.00	
[09/26 03:09:50 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 03:09:56 visual_prompt]: Epoch 52 / 100: avg data time: 4.78e-02, avg batch time: 0.4972, average train loss: 0.0049
[09/26 03:09:58 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1697, average loss: 1.7620
[09/26 03:09:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.00	
[09/26 03:09:58 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 03:10:05 visual_prompt]: Epoch 53 / 100: avg data time: 5.15e-02, avg batch time: 0.5006, average train loss: 0.0051
[09/26 03:10:06 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 1.7570
[09/26 03:10:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 03:10:06 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 03:10:13 visual_prompt]: Epoch 54 / 100: avg data time: 5.18e-02, avg batch time: 0.5021, average train loss: 0.0048
[09/26 03:10:15 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 1.7585
[09/26 03:10:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 03:10:15 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 03:10:21 visual_prompt]: Epoch 55 / 100: avg data time: 5.12e-02, avg batch time: 0.4997, average train loss: 0.0049
[09/26 03:10:23 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 1.7451
[09/26 03:10:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.50	
[09/26 03:10:23 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 03:10:30 visual_prompt]: Epoch 56 / 100: avg data time: 5.38e-02, avg batch time: 0.5020, average train loss: 0.0049
[09/26 03:10:31 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1696, average loss: 1.7505
[09/26 03:10:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 03:10:31 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 03:10:38 visual_prompt]: Epoch 57 / 100: avg data time: 5.43e-02, avg batch time: 0.5034, average train loss: 0.0048
[09/26 03:10:40 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1697, average loss: 1.7437
[09/26 03:10:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 86.00	
[09/26 03:10:40 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 03:10:46 visual_prompt]: Epoch 58 / 100: avg data time: 4.98e-02, avg batch time: 0.4984, average train loss: 0.0049
[09/26 03:10:48 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 1.7301
[09/26 03:10:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 86.00	
[09/26 03:10:48 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 03:10:55 visual_prompt]: Epoch 59 / 100: avg data time: 5.48e-02, avg batch time: 0.5036, average train loss: 0.0048
[09/26 03:10:56 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1696, average loss: 1.7386
[09/26 03:10:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.50	
[09/26 03:10:56 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 03:11:03 visual_prompt]: Epoch 60 / 100: avg data time: 5.26e-02, avg batch time: 0.5021, average train loss: 0.0047
[09/26 03:11:04 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1695, average loss: 1.7539
[09/26 03:11:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:11:04 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 03:11:11 visual_prompt]: Epoch 61 / 100: avg data time: 5.60e-02, avg batch time: 0.5058, average train loss: 0.0046
[09/26 03:11:13 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1693, average loss: 1.7620
[09/26 03:11:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 03:11:13 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 03:11:20 visual_prompt]: Epoch 62 / 100: avg data time: 5.92e-02, avg batch time: 0.5092, average train loss: 0.0047
[09/26 03:11:21 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 1.7554
[09/26 03:11:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:11:21 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 03:11:28 visual_prompt]: Epoch 63 / 100: avg data time: 4.97e-02, avg batch time: 0.4992, average train loss: 0.0046
[09/26 03:11:29 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1695, average loss: 1.7375
[09/26 03:11:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.50	
[09/26 03:11:29 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 03:11:36 visual_prompt]: Epoch 64 / 100: avg data time: 6.19e-02, avg batch time: 0.5105, average train loss: 0.0047
[09/26 03:11:38 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1698, average loss: 1.7423
[09/26 03:11:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.50	
[09/26 03:11:38 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 03:11:45 visual_prompt]: Epoch 65 / 100: avg data time: 4.48e-02, avg batch time: 0.4954, average train loss: 0.0045
[09/26 03:11:46 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1697, average loss: 1.7328
[09/26 03:11:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 85.00	
[09/26 03:11:46 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 03:11:53 visual_prompt]: Epoch 66 / 100: avg data time: 5.41e-02, avg batch time: 0.5025, average train loss: 0.0045
[09/26 03:11:54 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1697, average loss: 1.7367
[09/26 03:11:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.00	
[09/26 03:11:54 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 03:12:01 visual_prompt]: Epoch 67 / 100: avg data time: 5.73e-02, avg batch time: 0.5056, average train loss: 0.0045
[09/26 03:12:03 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 1.7420
[09/26 03:12:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.00	
[09/26 03:12:03 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 03:12:10 visual_prompt]: Epoch 68 / 100: avg data time: 4.45e-02, avg batch time: 0.4953, average train loss: 0.0046
[09/26 03:12:11 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1699, average loss: 1.7502
[09/26 03:12:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.00	
[09/26 03:12:11 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 03:12:18 visual_prompt]: Epoch 69 / 100: avg data time: 5.58e-02, avg batch time: 0.5057, average train loss: 0.0045
[09/26 03:12:19 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1696, average loss: 1.7321
[09/26 03:12:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 03:12:19 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 03:12:26 visual_prompt]: Epoch 70 / 100: avg data time: 5.60e-02, avg batch time: 0.5060, average train loss: 0.0045
[09/26 03:12:28 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 1.7372
[09/26 03:12:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 03:12:28 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 03:12:34 visual_prompt]: Epoch 71 / 100: avg data time: 4.56e-02, avg batch time: 0.4944, average train loss: 0.0043
[09/26 03:12:36 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1696, average loss: 1.7387
[09/26 03:12:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.50	
[09/26 03:12:36 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 03:12:43 visual_prompt]: Epoch 72 / 100: avg data time: 4.04e-02, avg batch time: 0.4918, average train loss: 0.0043
[09/26 03:12:44 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1694, average loss: 1.7316
[09/26 03:12:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.00	
[09/26 03:12:44 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 03:12:51 visual_prompt]: Epoch 73 / 100: avg data time: 4.52e-02, avg batch time: 0.4944, average train loss: 0.0046
[09/26 03:12:52 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1699, average loss: 1.7353
[09/26 03:12:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.00	
[09/26 03:12:52 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 03:12:59 visual_prompt]: Epoch 74 / 100: avg data time: 4.36e-02, avg batch time: 0.4938, average train loss: 0.0043
[09/26 03:13:00 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1698, average loss: 1.7434
[09/26 03:13:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.00	
[09/26 03:13:00 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 03:13:07 visual_prompt]: Epoch 75 / 100: avg data time: 5.35e-02, avg batch time: 0.5045, average train loss: 0.0042
[09/26 03:13:09 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1702, average loss: 1.7540
[09/26 03:13:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.00	
[09/26 03:13:09 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 03:13:16 visual_prompt]: Epoch 76 / 100: avg data time: 5.63e-02, avg batch time: 0.5056, average train loss: 0.0044
[09/26 03:13:17 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1698, average loss: 1.7510
[09/26 03:13:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.00	
[09/26 03:13:17 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 03:13:24 visual_prompt]: Epoch 77 / 100: avg data time: 6.47e-02, avg batch time: 0.5129, average train loss: 0.0042
[09/26 03:13:26 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1694, average loss: 1.7506
[09/26 03:13:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:13:26 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 03:13:33 visual_prompt]: Epoch 78 / 100: avg data time: 6.35e-02, avg batch time: 0.5139, average train loss: 0.0043
[09/26 03:13:34 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1695, average loss: 1.7474
[09/26 03:13:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.50	
[09/26 03:13:34 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 03:13:41 visual_prompt]: Epoch 79 / 100: avg data time: 5.92e-02, avg batch time: 0.5076, average train loss: 0.0042
[09/26 03:13:42 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1699, average loss: 1.7453
[09/26 03:13:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.50	
[09/26 03:13:42 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 03:13:49 visual_prompt]: Epoch 80 / 100: avg data time: 4.31e-02, avg batch time: 0.4944, average train loss: 0.0044
[09/26 03:13:51 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1697, average loss: 1.7471
[09/26 03:13:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 03:13:51 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 03:13:57 visual_prompt]: Epoch 81 / 100: avg data time: 4.42e-02, avg batch time: 0.4958, average train loss: 0.0041
[09/26 03:13:59 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1695, average loss: 1.7514
[09/26 03:13:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 84.50	
[09/26 03:13:59 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 03:14:06 visual_prompt]: Epoch 82 / 100: avg data time: 6.10e-02, avg batch time: 0.5105, average train loss: 0.0044
[09/26 03:14:07 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 1.7515
[09/26 03:14:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 84.50	
[09/26 03:14:07 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 03:14:14 visual_prompt]: Epoch 83 / 100: avg data time: 5.56e-02, avg batch time: 0.5057, average train loss: 0.0044
[09/26 03:14:16 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1693, average loss: 1.7547
[09/26 03:14:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 84.50	
[09/26 03:14:16 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 03:14:23 visual_prompt]: Epoch 84 / 100: avg data time: 5.02e-02, avg batch time: 0.4995, average train loss: 0.0042
[09/26 03:14:24 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 1.7543
[09/26 03:14:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 03:14:24 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 03:14:31 visual_prompt]: Epoch 85 / 100: avg data time: 4.82e-02, avg batch time: 0.4974, average train loss: 0.0043
[09/26 03:14:32 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 1.7495
[09/26 03:14:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 03:14:32 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 03:14:39 visual_prompt]: Epoch 86 / 100: avg data time: 4.83e-02, avg batch time: 0.5013, average train loss: 0.0043
[09/26 03:14:41 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 1.7447
[09/26 03:14:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 86.00	
[09/26 03:14:41 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 03:14:48 visual_prompt]: Epoch 87 / 100: avg data time: 5.58e-02, avg batch time: 0.5054, average train loss: 0.0043
[09/26 03:14:49 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 1.7420
[09/26 03:14:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:14:49 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 03:14:56 visual_prompt]: Epoch 88 / 100: avg data time: 3.97e-02, avg batch time: 0.4888, average train loss: 0.0044
[09/26 03:14:57 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.1695, average loss: 1.7386
[09/26 03:14:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.50	
[09/26 03:14:57 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 03:15:04 visual_prompt]: Epoch 89 / 100: avg data time: 4.48e-02, avg batch time: 0.4957, average train loss: 0.0043
[09/26 03:15:05 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 1.7384
[09/26 03:15:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.50	
[09/26 03:15:05 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 03:15:12 visual_prompt]: Epoch 90 / 100: avg data time: 5.90e-02, avg batch time: 0.5074, average train loss: 0.0043
[09/26 03:15:14 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1697, average loss: 1.7400
[09/26 03:15:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.50	
[09/26 03:15:14 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 03:15:21 visual_prompt]: Epoch 91 / 100: avg data time: 5.22e-02, avg batch time: 0.5002, average train loss: 0.0043
[09/26 03:15:22 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 1.7392
[09/26 03:15:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:15:22 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 03:15:29 visual_prompt]: Epoch 92 / 100: avg data time: 4.67e-02, avg batch time: 0.4969, average train loss: 0.0043
[09/26 03:15:31 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1696, average loss: 1.7395
[09/26 03:15:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:15:31 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 03:15:38 visual_prompt]: Epoch 93 / 100: avg data time: 5.94e-02, avg batch time: 0.5086, average train loss: 0.0043
[09/26 03:15:39 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 1.7397
[09/26 03:15:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:15:39 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 03:15:46 visual_prompt]: Epoch 94 / 100: avg data time: 5.40e-02, avg batch time: 0.5038, average train loss: 0.0043
[09/26 03:15:47 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1695, average loss: 1.7402
[09/26 03:15:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:15:47 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 03:15:54 visual_prompt]: Epoch 95 / 100: avg data time: 6.04e-02, avg batch time: 0.5092, average train loss: 0.0044
[09/26 03:15:56 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1698, average loss: 1.7405
[09/26 03:15:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:15:56 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 03:16:03 visual_prompt]: Epoch 96 / 100: avg data time: 4.32e-02, avg batch time: 0.4933, average train loss: 0.0043
[09/26 03:16:04 visual_prompt]: Inference (val):avg data time: 4.74e-05, avg batch time: 0.1697, average loss: 1.7406
[09/26 03:16:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:16:04 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 03:16:11 visual_prompt]: Epoch 97 / 100: avg data time: 5.47e-02, avg batch time: 0.5043, average train loss: 0.0042
[09/26 03:16:13 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1697, average loss: 1.7408
[09/26 03:16:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:16:13 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 03:16:19 visual_prompt]: Epoch 98 / 100: avg data time: 5.61e-02, avg batch time: 0.5048, average train loss: 0.0043
[09/26 03:16:21 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1697, average loss: 1.7409
[09/26 03:16:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:16:21 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 03:16:28 visual_prompt]: Epoch 99 / 100: avg data time: 4.55e-02, avg batch time: 0.4941, average train loss: 0.0043
[09/26 03:16:29 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1694, average loss: 1.7410
[09/26 03:16:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:16:29 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 03:16:36 visual_prompt]: Epoch 100 / 100: avg data time: 6.07e-02, avg batch time: 0.5090, average train loss: 0.0042
[09/26 03:16:38 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1695, average loss: 1.7410
[09/26 03:16:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.50	
[09/26 03:16:38 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 03:16:38 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 03:16:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 03:16:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 03:16:38 visual_prompt]: Training with config:
[09/26 03:16:38 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 03:16:38 visual_prompt]: Loading training data...
[09/26 03:16:38 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 03:16:40 visual_prompt]: Number of images: 800
[09/26 03:16:40 visual_prompt]: Number of classes: 47 / 47
[09/26 03:16:40 visual_prompt]: Loading validation data...
[09/26 03:16:40 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 03:16:41 visual_prompt]: Number of images: 200
[09/26 03:16:41 visual_prompt]: Number of classes: 47 / 47
[09/26 03:16:41 visual_prompt]: Constructing models...
[09/26 03:16:43 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 03:16:43 visual_prompt]: tuned percent:0.576
[09/26 03:16:43 visual_prompt]: Device used for model: 0
[09/26 03:16:43 visual_prompt]: Setting up Evaluator...
[09/26 03:16:43 visual_prompt]: Setting up Trainer...
[09/26 03:16:43 visual_prompt]: 	Setting up the optimizer...
[09/26 03:16:43 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 03:16:50 visual_prompt]: Epoch 1 / 100: avg data time: 4.34e-02, avg batch time: 0.4927, average train loss: 3.9329
[09/26 03:16:51 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1688, average loss: 3.9045
[09/26 03:16:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 03:16:51 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 03:16:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 03:16:58 visual_prompt]: Epoch 2 / 100: avg data time: 6.11e-02, avg batch time: 0.5089, average train loss: 3.8520
[09/26 03:17:00 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 3.9059
[09/26 03:17:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 12.50	
[09/26 03:17:00 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 03:17:07 visual_prompt]: Epoch 3 / 100: avg data time: 6.49e-02, avg batch time: 0.5124, average train loss: 3.7568
[09/26 03:17:08 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1687, average loss: 3.9904
[09/26 03:17:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 22.00	
[09/26 03:17:08 visual_prompt]: Best epoch 3: best metric: 0.060
[09/26 03:17:08 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 03:17:15 visual_prompt]: Epoch 4 / 100: avg data time: 4.64e-02, avg batch time: 0.4944, average train loss: 3.6813
[09/26 03:17:16 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 3.7441
[09/26 03:17:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 22.00	
[09/26 03:17:16 visual_prompt]: Best epoch 4: best metric: 0.065
[09/26 03:17:16 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 03:17:23 visual_prompt]: Epoch 5 / 100: avg data time: 6.11e-02, avg batch time: 0.5087, average train loss: 3.2861
[09/26 03:17:25 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1688, average loss: 3.3887
[09/26 03:17:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.00	top5: 34.00	
[09/26 03:17:25 visual_prompt]: Best epoch 5: best metric: 0.120
[09/26 03:17:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 03:17:32 visual_prompt]: Epoch 6 / 100: avg data time: 4.32e-02, avg batch time: 0.4926, average train loss: 2.8895
[09/26 03:17:33 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1688, average loss: 2.8695
[09/26 03:17:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.00	top5: 53.00	
[09/26 03:17:33 visual_prompt]: Best epoch 6: best metric: 0.270
[09/26 03:17:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 03:17:40 visual_prompt]: Epoch 7 / 100: avg data time: 5.33e-02, avg batch time: 0.5023, average train loss: 2.2760
[09/26 03:17:41 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 2.6671
[09/26 03:17:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.00	top5: 64.00	
[09/26 03:17:41 visual_prompt]: Best epoch 7: best metric: 0.300
[09/26 03:17:41 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 03:17:48 visual_prompt]: Epoch 8 / 100: avg data time: 6.26e-02, avg batch time: 0.5109, average train loss: 1.6022
[09/26 03:17:50 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 2.3324
[09/26 03:17:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.50	top5: 73.50	
[09/26 03:17:50 visual_prompt]: Best epoch 8: best metric: 0.385
[09/26 03:17:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 03:17:57 visual_prompt]: Epoch 9 / 100: avg data time: 5.04e-02, avg batch time: 0.4991, average train loss: 0.9272
[09/26 03:17:58 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1697, average loss: 1.8564
[09/26 03:17:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 83.00	
[09/26 03:17:58 visual_prompt]: Best epoch 9: best metric: 0.460
[09/26 03:17:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 03:18:05 visual_prompt]: Epoch 10 / 100: avg data time: 4.33e-02, avg batch time: 0.4924, average train loss: 0.4113
[09/26 03:18:06 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1693, average loss: 1.9778
[09/26 03:18:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 81.00	
[09/26 03:18:06 visual_prompt]: Best epoch 10: best metric: 0.505
[09/26 03:18:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 03:18:13 visual_prompt]: Epoch 11 / 100: avg data time: 5.80e-02, avg batch time: 0.5066, average train loss: 0.1693
[09/26 03:18:15 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 1.9038
[09/26 03:18:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 83.00	
[09/26 03:18:15 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 03:18:21 visual_prompt]: Epoch 12 / 100: avg data time: 5.25e-02, avg batch time: 0.5015, average train loss: 0.0721
[09/26 03:18:23 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1694, average loss: 1.9402
[09/26 03:18:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 85.00	
[09/26 03:18:23 visual_prompt]: Best epoch 12: best metric: 0.510
[09/26 03:18:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 03:18:30 visual_prompt]: Epoch 13 / 100: avg data time: 5.32e-02, avg batch time: 0.5019, average train loss: 0.0244
[09/26 03:18:31 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 1.9001
[09/26 03:18:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 83.50	
[09/26 03:18:31 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 03:18:38 visual_prompt]: Epoch 14 / 100: avg data time: 5.31e-02, avg batch time: 0.5025, average train loss: 0.0131
[09/26 03:18:40 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1697, average loss: 1.9114
[09/26 03:18:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 83.50	
[09/26 03:18:40 visual_prompt]: Best epoch 14: best metric: 0.520
[09/26 03:18:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 03:18:46 visual_prompt]: Epoch 15 / 100: avg data time: 5.37e-02, avg batch time: 0.5028, average train loss: 0.0085
[09/26 03:18:48 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 1.9170
[09/26 03:18:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 85.50	
[09/26 03:18:48 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 03:18:55 visual_prompt]: Epoch 16 / 100: avg data time: 4.58e-02, avg batch time: 0.4942, average train loss: 0.0064
[09/26 03:18:56 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1697, average loss: 1.9357
[09/26 03:18:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 84.00	
[09/26 03:18:56 visual_prompt]: Best epoch 16: best metric: 0.525
[09/26 03:18:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 03:19:03 visual_prompt]: Epoch 17 / 100: avg data time: 4.39e-02, avg batch time: 0.4936, average train loss: 0.0054
[09/26 03:19:04 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 1.9449
[09/26 03:19:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 84.50	
[09/26 03:19:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 03:19:11 visual_prompt]: Epoch 18 / 100: avg data time: 5.60e-02, avg batch time: 0.5054, average train loss: 0.0046
[09/26 03:19:13 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1695, average loss: 1.9452
[09/26 03:19:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 85.00	
[09/26 03:19:13 visual_prompt]: Best epoch 18: best metric: 0.530
[09/26 03:19:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 03:19:19 visual_prompt]: Epoch 19 / 100: avg data time: 5.61e-02, avg batch time: 0.5043, average train loss: 0.0042
[09/26 03:19:21 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1692, average loss: 1.9472
[09/26 03:19:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 86.00	
[09/26 03:19:21 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 03:19:28 visual_prompt]: Epoch 20 / 100: avg data time: 5.43e-02, avg batch time: 0.5031, average train loss: 0.0038
[09/26 03:19:29 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.1690, average loss: 1.9459
[09/26 03:19:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 86.50	
[09/26 03:19:29 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 03:19:36 visual_prompt]: Epoch 21 / 100: avg data time: 4.51e-02, avg batch time: 0.4958, average train loss: 0.0036
[09/26 03:19:38 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1696, average loss: 1.9510
[09/26 03:19:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 85.50	
[09/26 03:19:38 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 03:19:44 visual_prompt]: Epoch 22 / 100: avg data time: 4.69e-02, avg batch time: 0.4978, average train loss: 0.0033
[09/26 03:19:46 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 1.9608
[09/26 03:19:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 85.00	
[09/26 03:19:46 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 03:19:53 visual_prompt]: Epoch 23 / 100: avg data time: 5.23e-02, avg batch time: 0.5007, average train loss: 0.0031
[09/26 03:19:54 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1693, average loss: 1.9624
[09/26 03:19:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 85.50	
[09/26 03:19:54 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 03:20:01 visual_prompt]: Epoch 24 / 100: avg data time: 5.80e-02, avg batch time: 0.5078, average train loss: 0.0029
[09/26 03:20:03 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1696, average loss: 1.9721
[09/26 03:20:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 85.50	
[09/26 03:20:03 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 03:20:10 visual_prompt]: Epoch 25 / 100: avg data time: 5.01e-02, avg batch time: 0.5000, average train loss: 0.0027
[09/26 03:20:11 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 1.9771
[09/26 03:20:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 85.50	
[09/26 03:20:11 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 03:20:18 visual_prompt]: Epoch 26 / 100: avg data time: 4.96e-02, avg batch time: 0.4983, average train loss: 0.0027
[09/26 03:20:19 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1697, average loss: 1.9739
[09/26 03:20:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 86.50	
[09/26 03:20:19 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 03:20:26 visual_prompt]: Epoch 27 / 100: avg data time: 4.62e-02, avg batch time: 0.4949, average train loss: 0.0025
[09/26 03:20:28 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1698, average loss: 1.9784
[09/26 03:20:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 85.50	
[09/26 03:20:28 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 03:20:34 visual_prompt]: Epoch 28 / 100: avg data time: 4.76e-02, avg batch time: 0.4983, average train loss: 0.0024
[09/26 03:20:36 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.1697, average loss: 1.9710
[09/26 03:20:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 87.50	
[09/26 03:20:36 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 03:20:43 visual_prompt]: Epoch 29 / 100: avg data time: 4.99e-02, avg batch time: 0.4995, average train loss: 0.0023
[09/26 03:20:44 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1696, average loss: 1.9706
[09/26 03:20:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 87.00	
[09/26 03:20:44 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 03:20:51 visual_prompt]: Epoch 30 / 100: avg data time: 5.65e-02, avg batch time: 0.5052, average train loss: 0.0021
[09/26 03:20:53 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 1.9774
[09/26 03:20:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 87.00	
[09/26 03:20:53 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 03:20:59 visual_prompt]: Epoch 31 / 100: avg data time: 5.54e-02, avg batch time: 0.5053, average train loss: 0.0021
[09/26 03:21:01 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1697, average loss: 1.9896
[09/26 03:21:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.00	
[09/26 03:21:01 visual_prompt]: Best epoch 31: best metric: 0.535
[09/26 03:21:01 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 03:21:08 visual_prompt]: Epoch 32 / 100: avg data time: 5.90e-02, avg batch time: 0.5070, average train loss: 0.0020
[09/26 03:21:09 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1690, average loss: 1.9943
[09/26 03:21:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.00	
[09/26 03:21:09 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 03:21:16 visual_prompt]: Epoch 33 / 100: avg data time: 5.59e-02, avg batch time: 0.5054, average train loss: 0.0019
[09/26 03:21:18 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1694, average loss: 1.9954
[09/26 03:21:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.00	
[09/26 03:21:18 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 03:21:25 visual_prompt]: Epoch 34 / 100: avg data time: 5.25e-02, avg batch time: 0.5010, average train loss: 0.0018
[09/26 03:21:26 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1700, average loss: 1.9901
[09/26 03:21:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 86.50	
[09/26 03:21:26 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 03:21:33 visual_prompt]: Epoch 35 / 100: avg data time: 5.87e-02, avg batch time: 0.5068, average train loss: 0.0018
[09/26 03:21:35 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1692, average loss: 1.9959
[09/26 03:21:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 87.00	
[09/26 03:21:35 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 03:21:41 visual_prompt]: Epoch 36 / 100: avg data time: 5.95e-02, avg batch time: 0.5098, average train loss: 0.0018
[09/26 03:21:43 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1695, average loss: 1.9946
[09/26 03:21:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 87.50	
[09/26 03:21:43 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 03:21:50 visual_prompt]: Epoch 37 / 100: avg data time: 5.96e-02, avg batch time: 0.5085, average train loss: 0.0017
[09/26 03:21:51 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1692, average loss: 2.0024
[09/26 03:21:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 88.00	
[09/26 03:21:51 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 03:21:58 visual_prompt]: Epoch 38 / 100: avg data time: 5.41e-02, avg batch time: 0.5027, average train loss: 0.0016
[09/26 03:22:00 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1693, average loss: 2.0074
[09/26 03:22:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 87.50	
[09/26 03:22:00 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 03:22:07 visual_prompt]: Epoch 39 / 100: avg data time: 4.64e-02, avg batch time: 0.4954, average train loss: 0.0016
[09/26 03:22:08 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1693, average loss: 2.0082
[09/26 03:22:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 86.50	
[09/26 03:22:08 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 03:22:15 visual_prompt]: Epoch 40 / 100: avg data time: 5.31e-02, avg batch time: 0.5016, average train loss: 0.0015
[09/26 03:22:16 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1696, average loss: 2.0106
[09/26 03:22:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 87.00	
[09/26 03:22:16 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 03:22:23 visual_prompt]: Epoch 41 / 100: avg data time: 5.11e-02, avg batch time: 0.5010, average train loss: 0.0015
[09/26 03:22:25 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 2.0143
[09/26 03:22:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 87.50	
[09/26 03:22:25 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 03:22:32 visual_prompt]: Epoch 42 / 100: avg data time: 4.29e-02, avg batch time: 0.4957, average train loss: 0.0014
[09/26 03:22:33 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1695, average loss: 2.0148
[09/26 03:22:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 87.50	
[09/26 03:22:33 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 03:22:40 visual_prompt]: Epoch 43 / 100: avg data time: 4.44e-02, avg batch time: 0.4935, average train loss: 0.0014
[09/26 03:22:42 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1698, average loss: 2.0155
[09/26 03:22:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 87.00	
[09/26 03:22:42 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 03:22:48 visual_prompt]: Epoch 44 / 100: avg data time: 5.51e-02, avg batch time: 0.5049, average train loss: 0.0014
[09/26 03:22:50 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1693, average loss: 2.0152
[09/26 03:22:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 87.00	
[09/26 03:22:50 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 03:22:57 visual_prompt]: Epoch 45 / 100: avg data time: 3.88e-02, avg batch time: 0.4916, average train loss: 0.0014
[09/26 03:22:58 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 2.0203
[09/26 03:22:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 88.00	
[09/26 03:22:58 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 03:23:05 visual_prompt]: Epoch 46 / 100: avg data time: 4.22e-02, avg batch time: 0.4925, average train loss: 0.0014
[09/26 03:23:06 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1696, average loss: 2.0234
[09/26 03:23:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 88.00	
[09/26 03:23:06 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 03:23:13 visual_prompt]: Epoch 47 / 100: avg data time: 5.02e-02, avg batch time: 0.4997, average train loss: 0.0013
[09/26 03:23:15 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1696, average loss: 2.0262
[09/26 03:23:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 88.00	
[09/26 03:23:15 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 03:23:21 visual_prompt]: Epoch 48 / 100: avg data time: 4.70e-02, avg batch time: 0.4961, average train loss: 0.0014
[09/26 03:23:23 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1698, average loss: 2.0262
[09/26 03:23:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 87.50	
[09/26 03:23:23 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 03:23:30 visual_prompt]: Epoch 49 / 100: avg data time: 5.04e-02, avg batch time: 0.5008, average train loss: 0.0013
[09/26 03:23:31 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1694, average loss: 2.0220
[09/26 03:23:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:23:31 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 03:23:38 visual_prompt]: Epoch 50 / 100: avg data time: 5.51e-02, avg batch time: 0.5035, average train loss: 0.0013
[09/26 03:23:39 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1699, average loss: 2.0236
[09/26 03:23:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:23:39 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 03:23:46 visual_prompt]: Epoch 51 / 100: avg data time: 5.42e-02, avg batch time: 0.5026, average train loss: 0.0013
[09/26 03:23:48 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 2.0221
[09/26 03:23:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:23:48 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 03:23:55 visual_prompt]: Epoch 52 / 100: avg data time: 4.55e-02, avg batch time: 0.4944, average train loss: 0.0012
[09/26 03:23:56 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 2.0228
[09/26 03:23:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:23:56 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 03:24:03 visual_prompt]: Epoch 53 / 100: avg data time: 4.20e-02, avg batch time: 0.4927, average train loss: 0.0013
[09/26 03:24:04 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.1694, average loss: 2.0244
[09/26 03:24:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:24:04 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 03:24:11 visual_prompt]: Epoch 54 / 100: avg data time: 4.87e-02, avg batch time: 0.4975, average train loss: 0.0012
[09/26 03:24:13 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1692, average loss: 2.0284
[09/26 03:24:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:24:13 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 03:24:19 visual_prompt]: Epoch 55 / 100: avg data time: 5.14e-02, avg batch time: 0.5022, average train loss: 0.0012
[09/26 03:24:21 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1697, average loss: 2.0319
[09/26 03:24:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:24:21 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 03:24:28 visual_prompt]: Epoch 56 / 100: avg data time: 4.16e-02, avg batch time: 0.4921, average train loss: 0.0011
[09/26 03:24:29 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1694, average loss: 2.0349
[09/26 03:24:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:24:29 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 03:24:36 visual_prompt]: Epoch 57 / 100: avg data time: 5.44e-02, avg batch time: 0.5044, average train loss: 0.0011
[09/26 03:24:38 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.1693, average loss: 2.0356
[09/26 03:24:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:24:38 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 03:24:44 visual_prompt]: Epoch 58 / 100: avg data time: 5.18e-02, avg batch time: 0.5005, average train loss: 0.0011
[09/26 03:24:46 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1693, average loss: 2.0338
[09/26 03:24:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:24:46 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 03:24:53 visual_prompt]: Epoch 59 / 100: avg data time: 4.34e-02, avg batch time: 0.4924, average train loss: 0.0011
[09/26 03:24:54 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1697, average loss: 2.0344
[09/26 03:24:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:24:54 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 03:25:01 visual_prompt]: Epoch 60 / 100: avg data time: 4.53e-02, avg batch time: 0.4958, average train loss: 0.0011
[09/26 03:25:02 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1694, average loss: 2.0369
[09/26 03:25:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:25:02 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 03:25:09 visual_prompt]: Epoch 61 / 100: avg data time: 5.85e-02, avg batch time: 0.5080, average train loss: 0.0011
[09/26 03:25:11 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1695, average loss: 2.0371
[09/26 03:25:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:25:11 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 03:25:17 visual_prompt]: Epoch 62 / 100: avg data time: 4.46e-02, avg batch time: 0.4940, average train loss: 0.0011
[09/26 03:25:19 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 2.0361
[09/26 03:25:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:25:19 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 03:25:26 visual_prompt]: Epoch 63 / 100: avg data time: 5.31e-02, avg batch time: 0.5026, average train loss: 0.0011
[09/26 03:25:27 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 2.0360
[09/26 03:25:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:25:27 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 03:25:34 visual_prompt]: Epoch 64 / 100: avg data time: 5.80e-02, avg batch time: 0.5070, average train loss: 0.0010
[09/26 03:25:36 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1693, average loss: 2.0376
[09/26 03:25:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:25:36 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 03:25:42 visual_prompt]: Epoch 65 / 100: avg data time: 5.53e-02, avg batch time: 0.5056, average train loss: 0.0011
[09/26 03:25:44 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 2.0400
[09/26 03:25:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:25:44 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 03:25:51 visual_prompt]: Epoch 66 / 100: avg data time: 5.63e-02, avg batch time: 0.5063, average train loss: 0.0011
[09/26 03:25:52 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 2.0409
[09/26 03:25:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:25:52 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 03:25:59 visual_prompt]: Epoch 67 / 100: avg data time: 6.06e-02, avg batch time: 0.5088, average train loss: 0.0010
[09/26 03:26:01 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1691, average loss: 2.0417
[09/26 03:26:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:26:01 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 03:26:07 visual_prompt]: Epoch 68 / 100: avg data time: 4.35e-02, avg batch time: 0.4934, average train loss: 0.0010
[09/26 03:26:09 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1693, average loss: 2.0428
[09/26 03:26:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:26:09 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 03:26:16 visual_prompt]: Epoch 69 / 100: avg data time: 5.45e-02, avg batch time: 0.5038, average train loss: 0.0010
[09/26 03:26:17 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1697, average loss: 2.0441
[09/26 03:26:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:26:17 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 03:26:24 visual_prompt]: Epoch 70 / 100: avg data time: 5.67e-02, avg batch time: 0.5055, average train loss: 0.0010
[09/26 03:26:26 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1693, average loss: 2.0438
[09/26 03:26:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:26:26 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 03:26:32 visual_prompt]: Epoch 71 / 100: avg data time: 4.97e-02, avg batch time: 0.4983, average train loss: 0.0010
[09/26 03:26:34 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1691, average loss: 2.0434
[09/26 03:26:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:26:34 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 03:26:41 visual_prompt]: Epoch 72 / 100: avg data time: 5.47e-02, avg batch time: 0.5031, average train loss: 0.0010
[09/26 03:26:42 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1691, average loss: 2.0439
[09/26 03:26:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:26:42 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 03:26:49 visual_prompt]: Epoch 73 / 100: avg data time: 5.50e-02, avg batch time: 0.5053, average train loss: 0.0010
[09/26 03:26:50 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 2.0446
[09/26 03:26:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:26:50 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 03:26:57 visual_prompt]: Epoch 74 / 100: avg data time: 6.08e-02, avg batch time: 0.5097, average train loss: 0.0010
[09/26 03:26:59 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1692, average loss: 2.0447
[09/26 03:26:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:26:59 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 03:27:06 visual_prompt]: Epoch 75 / 100: avg data time: 5.57e-02, avg batch time: 0.5044, average train loss: 0.0010
[09/26 03:27:07 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 2.0457
[09/26 03:27:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:27:07 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 03:27:14 visual_prompt]: Epoch 76 / 100: avg data time: 5.56e-02, avg batch time: 0.5038, average train loss: 0.0009
[09/26 03:27:16 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1688, average loss: 2.0459
[09/26 03:27:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:27:16 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 03:27:22 visual_prompt]: Epoch 77 / 100: avg data time: 5.52e-02, avg batch time: 0.5039, average train loss: 0.0009
[09/26 03:27:24 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1697, average loss: 2.0470
[09/26 03:27:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:27:24 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 03:27:31 visual_prompt]: Epoch 78 / 100: avg data time: 5.63e-02, avg batch time: 0.5045, average train loss: 0.0010
[09/26 03:27:32 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1695, average loss: 2.0477
[09/26 03:27:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:27:32 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 03:27:39 visual_prompt]: Epoch 79 / 100: avg data time: 5.93e-02, avg batch time: 0.5087, average train loss: 0.0010
[09/26 03:27:41 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1695, average loss: 2.0485
[09/26 03:27:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:27:41 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 03:27:48 visual_prompt]: Epoch 80 / 100: avg data time: 5.94e-02, avg batch time: 0.5079, average train loss: 0.0010
[09/26 03:27:49 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1694, average loss: 2.0490
[09/26 03:27:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:27:49 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 03:27:56 visual_prompt]: Epoch 81 / 100: avg data time: 4.21e-02, avg batch time: 0.4913, average train loss: 0.0010
[09/26 03:27:58 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 2.0487
[09/26 03:27:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:27:58 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 03:28:04 visual_prompt]: Epoch 82 / 100: avg data time: 4.58e-02, avg batch time: 0.4947, average train loss: 0.0010
[09/26 03:28:06 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 2.0488
[09/26 03:28:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:28:06 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 03:28:13 visual_prompt]: Epoch 83 / 100: avg data time: 5.60e-02, avg batch time: 0.5048, average train loss: 0.0009
[09/26 03:28:14 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1696, average loss: 2.0492
[09/26 03:28:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:28:14 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 03:28:21 visual_prompt]: Epoch 84 / 100: avg data time: 5.44e-02, avg batch time: 0.5047, average train loss: 0.0010
[09/26 03:28:22 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1694, average loss: 2.0498
[09/26 03:28:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:28:22 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 03:28:29 visual_prompt]: Epoch 85 / 100: avg data time: 4.36e-02, avg batch time: 0.4944, average train loss: 0.0010
[09/26 03:28:31 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1695, average loss: 2.0497
[09/26 03:28:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:28:31 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 03:28:38 visual_prompt]: Epoch 86 / 100: avg data time: 5.61e-02, avg batch time: 0.5045, average train loss: 0.0010
[09/26 03:28:39 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 2.0499
[09/26 03:28:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:28:39 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 03:28:46 visual_prompt]: Epoch 87 / 100: avg data time: 5.07e-02, avg batch time: 0.4995, average train loss: 0.0010
[09/26 03:28:47 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 2.0499
[09/26 03:28:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:28:47 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 03:28:54 visual_prompt]: Epoch 88 / 100: avg data time: 5.22e-02, avg batch time: 0.5006, average train loss: 0.0010
[09/26 03:28:56 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1696, average loss: 2.0498
[09/26 03:28:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:28:56 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 03:29:02 visual_prompt]: Epoch 89 / 100: avg data time: 4.34e-02, avg batch time: 0.4938, average train loss: 0.0009
[09/26 03:29:04 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1693, average loss: 2.0498
[09/26 03:29:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:29:04 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 03:29:11 visual_prompt]: Epoch 90 / 100: avg data time: 4.50e-02, avg batch time: 0.4938, average train loss: 0.0009
[09/26 03:29:12 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1694, average loss: 2.0498
[09/26 03:29:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:29:12 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 03:29:19 visual_prompt]: Epoch 91 / 100: avg data time: 4.80e-02, avg batch time: 0.4982, average train loss: 0.0009
[09/26 03:29:20 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1693, average loss: 2.0499
[09/26 03:29:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:29:20 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 03:29:27 visual_prompt]: Epoch 92 / 100: avg data time: 4.17e-02, avg batch time: 0.4923, average train loss: 0.0009
[09/26 03:29:29 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1698, average loss: 2.0499
[09/26 03:29:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:29:29 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 03:29:35 visual_prompt]: Epoch 93 / 100: avg data time: 5.24e-02, avg batch time: 0.5020, average train loss: 0.0010
[09/26 03:29:37 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 2.0498
[09/26 03:29:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:29:37 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 03:29:44 visual_prompt]: Epoch 94 / 100: avg data time: 4.13e-02, avg batch time: 0.4903, average train loss: 0.0010
[09/26 03:29:45 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1693, average loss: 2.0498
[09/26 03:29:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:29:45 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 03:29:52 visual_prompt]: Epoch 95 / 100: avg data time: 4.49e-02, avg batch time: 0.4945, average train loss: 0.0010
[09/26 03:29:53 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1697, average loss: 2.0497
[09/26 03:29:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:29:53 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 03:30:00 visual_prompt]: Epoch 96 / 100: avg data time: 5.68e-02, avg batch time: 0.5051, average train loss: 0.0009
[09/26 03:30:02 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1694, average loss: 2.0498
[09/26 03:30:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:30:02 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 03:30:08 visual_prompt]: Epoch 97 / 100: avg data time: 5.15e-02, avg batch time: 0.5002, average train loss: 0.0010
[09/26 03:30:10 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1693, average loss: 2.0498
[09/26 03:30:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:30:10 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 03:30:17 visual_prompt]: Epoch 98 / 100: avg data time: 4.88e-02, avg batch time: 0.5003, average train loss: 0.0010
[09/26 03:30:18 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1690, average loss: 2.0498
[09/26 03:30:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:30:18 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 03:30:25 visual_prompt]: Epoch 99 / 100: avg data time: 4.43e-02, avg batch time: 0.4959, average train loss: 0.0010
[09/26 03:30:26 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1697, average loss: 2.0498
[09/26 03:30:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:30:26 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 03:30:33 visual_prompt]: Epoch 100 / 100: avg data time: 5.19e-02, avg batch time: 0.5018, average train loss: 0.0010
[09/26 03:30:35 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1691, average loss: 2.0498
[09/26 03:30:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:30:35 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 03:30:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 03:30:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 03:30:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 03:30:35 visual_prompt]: Training with config:
[09/26 03:30:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 03:30:35 visual_prompt]: Loading training data...
[09/26 03:30:35 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 03:30:37 visual_prompt]: Number of images: 800
[09/26 03:30:37 visual_prompt]: Number of classes: 47 / 47
[09/26 03:30:37 visual_prompt]: Loading validation data...
[09/26 03:30:37 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 03:30:37 visual_prompt]: Number of images: 200
[09/26 03:30:37 visual_prompt]: Number of classes: 47 / 47
[09/26 03:30:37 visual_prompt]: Constructing models...
[09/26 03:30:40 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 03:30:40 visual_prompt]: tuned percent:0.576
[09/26 03:30:40 visual_prompt]: Device used for model: 0
[09/26 03:30:40 visual_prompt]: Setting up Evaluator...
[09/26 03:30:40 visual_prompt]: Setting up Trainer...
[09/26 03:30:40 visual_prompt]: 	Setting up the optimizer...
[09/26 03:30:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 03:30:47 visual_prompt]: Epoch 1 / 100: avg data time: 5.27e-02, avg batch time: 0.4997, average train loss: 3.9268
[09/26 03:30:48 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1690, average loss: 3.9045
[09/26 03:30:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 03:30:48 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 03:30:48 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 03:30:55 visual_prompt]: Epoch 2 / 100: avg data time: 5.33e-02, avg batch time: 0.5002, average train loss: 3.8731
[09/26 03:30:57 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1692, average loss: 3.9064
[09/26 03:30:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 8.50	
[09/26 03:30:57 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 03:31:04 visual_prompt]: Epoch 3 / 100: avg data time: 5.76e-02, avg batch time: 0.5053, average train loss: 3.8294
[09/26 03:31:05 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1692, average loss: 3.8882
[09/26 03:31:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 03:31:05 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 03:31:12 visual_prompt]: Epoch 4 / 100: avg data time: 5.64e-02, avg batch time: 0.5048, average train loss: 3.7804
[09/26 03:31:13 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1691, average loss: 3.9021
[09/26 03:31:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 16.50	
[09/26 03:31:13 visual_prompt]: Best epoch 4: best metric: 0.045
[09/26 03:31:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 03:31:20 visual_prompt]: Epoch 5 / 100: avg data time: 6.07e-02, avg batch time: 0.5084, average train loss: 3.7642
[09/26 03:31:22 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1693, average loss: 3.8786
[09/26 03:31:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 13.00	
[09/26 03:31:22 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 03:31:29 visual_prompt]: Epoch 6 / 100: avg data time: 6.20e-02, avg batch time: 0.5105, average train loss: 3.7773
[09/26 03:31:30 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1690, average loss: 3.7888
[09/26 03:31:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.00	top5: 19.50	
[09/26 03:31:30 visual_prompt]: Best epoch 6: best metric: 0.070
[09/26 03:31:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 03:31:37 visual_prompt]: Epoch 7 / 100: avg data time: 4.17e-02, avg batch time: 0.4939, average train loss: 3.5742
[09/26 03:31:39 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1688, average loss: 3.8757
[09/26 03:31:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 8.50	
[09/26 03:31:39 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 03:31:45 visual_prompt]: Epoch 8 / 100: avg data time: 5.64e-02, avg batch time: 0.5043, average train loss: 3.6850
[09/26 03:31:47 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 4.1654
[09/26 03:31:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/26 03:31:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 03:31:54 visual_prompt]: Epoch 9 / 100: avg data time: 5.48e-02, avg batch time: 0.5026, average train loss: 3.7868
[09/26 03:31:55 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 3.8613
[09/26 03:31:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 19.00	
[09/26 03:31:55 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 03:32:02 visual_prompt]: Epoch 10 / 100: avg data time: 4.30e-02, avg batch time: 0.4912, average train loss: 3.6389
[09/26 03:32:04 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1694, average loss: 3.5495
[09/26 03:32:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.50	top5: 32.50	
[09/26 03:32:04 visual_prompt]: Best epoch 10: best metric: 0.115
[09/26 03:32:04 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 03:32:10 visual_prompt]: Epoch 11 / 100: avg data time: 4.43e-02, avg batch time: 0.4924, average train loss: 3.0459
[09/26 03:32:12 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 3.1486
[09/26 03:32:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 45.50	
[09/26 03:32:12 visual_prompt]: Best epoch 11: best metric: 0.220
[09/26 03:32:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 03:32:18 visual_prompt]: Epoch 12 / 100: avg data time: 4.53e-02, avg batch time: 0.4944, average train loss: 2.4584
[09/26 03:32:20 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1696, average loss: 3.5109
[09/26 03:32:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 32.50	
[09/26 03:32:20 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 03:32:27 visual_prompt]: Epoch 13 / 100: avg data time: 4.61e-02, avg batch time: 0.4971, average train loss: 2.2486
[09/26 03:32:28 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 2.4944
[09/26 03:32:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.00	top5: 67.00	
[09/26 03:32:28 visual_prompt]: Best epoch 13: best metric: 0.380
[09/26 03:32:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 03:32:35 visual_prompt]: Epoch 14 / 100: avg data time: 4.97e-02, avg batch time: 0.5001, average train loss: 1.6961
[09/26 03:32:36 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1700, average loss: 2.3272
[09/26 03:32:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.00	top5: 72.50	
[09/26 03:32:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 03:32:43 visual_prompt]: Epoch 15 / 100: avg data time: 4.53e-02, avg batch time: 0.4945, average train loss: 1.3799
[09/26 03:32:45 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 2.0975
[09/26 03:32:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 79.00	
[09/26 03:32:45 visual_prompt]: Best epoch 15: best metric: 0.460
[09/26 03:32:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 03:32:51 visual_prompt]: Epoch 16 / 100: avg data time: 4.58e-02, avg batch time: 0.4948, average train loss: 1.3850
[09/26 03:32:53 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1697, average loss: 2.5779
[09/26 03:32:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.50	top5: 64.00	
[09/26 03:32:53 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 03:33:00 visual_prompt]: Epoch 17 / 100: avg data time: 5.26e-02, avg batch time: 0.5019, average train loss: 1.5025
[09/26 03:33:01 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1688, average loss: 2.5139
[09/26 03:33:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 34.00	top5: 77.00	
[09/26 03:33:01 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 03:33:08 visual_prompt]: Epoch 18 / 100: avg data time: 4.79e-02, avg batch time: 0.4989, average train loss: 1.4289
[09/26 03:33:10 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1694, average loss: 2.1371
[09/26 03:33:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 39.00	top5: 79.50	
[09/26 03:33:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 03:33:16 visual_prompt]: Epoch 19 / 100: avg data time: 5.03e-02, avg batch time: 0.5005, average train loss: 1.4278
[09/26 03:33:18 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1693, average loss: 2.6571
[09/26 03:33:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.50	top5: 66.50	
[09/26 03:33:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 03:33:25 visual_prompt]: Epoch 20 / 100: avg data time: 4.51e-02, avg batch time: 0.4933, average train loss: 1.3735
[09/26 03:33:26 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1696, average loss: 2.0041
[09/26 03:33:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 82.00	
[09/26 03:33:26 visual_prompt]: Best epoch 20: best metric: 0.490
[09/26 03:33:26 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 03:33:33 visual_prompt]: Epoch 21 / 100: avg data time: 4.70e-02, avg batch time: 0.4978, average train loss: 1.2191
[09/26 03:33:34 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 2.3484
[09/26 03:33:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 79.00	
[09/26 03:33:34 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 03:33:41 visual_prompt]: Epoch 22 / 100: avg data time: 4.37e-02, avg batch time: 0.4921, average train loss: 1.9737
[09/26 03:33:43 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1692, average loss: 2.2325
[09/26 03:33:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 76.00	
[09/26 03:33:43 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 03:33:49 visual_prompt]: Epoch 23 / 100: avg data time: 5.26e-02, avg batch time: 0.5017, average train loss: 1.4996
[09/26 03:33:51 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1695, average loss: 1.8509
[09/26 03:33:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 85.50	
[09/26 03:33:51 visual_prompt]: Best epoch 23: best metric: 0.520
[09/26 03:33:51 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 03:33:58 visual_prompt]: Epoch 24 / 100: avg data time: 5.13e-02, avg batch time: 0.5004, average train loss: 2.2912
[09/26 03:33:59 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1692, average loss: 3.7473
[09/26 03:33:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 27.00	
[09/26 03:33:59 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 03:34:06 visual_prompt]: Epoch 25 / 100: avg data time: 4.97e-02, avg batch time: 0.5000, average train loss: 3.3734
[09/26 03:34:08 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1694, average loss: 3.0671
[09/26 03:34:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 50.00	
[09/26 03:34:08 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 03:34:15 visual_prompt]: Epoch 26 / 100: avg data time: 5.66e-02, avg batch time: 0.5069, average train loss: 2.4646
[09/26 03:34:16 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1690, average loss: 2.7621
[09/26 03:34:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 28.50	top5: 59.00	
[09/26 03:34:16 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 03:34:23 visual_prompt]: Epoch 27 / 100: avg data time: 5.51e-02, avg batch time: 0.5040, average train loss: 2.2652
[09/26 03:34:25 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 2.5853
[09/26 03:34:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 28.50	top5: 69.50	
[09/26 03:34:25 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 03:34:31 visual_prompt]: Epoch 28 / 100: avg data time: 4.46e-02, avg batch time: 0.4952, average train loss: 1.6552
[09/26 03:34:33 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1695, average loss: 1.8793
[09/26 03:34:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 85.00	
[09/26 03:34:33 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 03:34:40 visual_prompt]: Epoch 29 / 100: avg data time: 5.69e-02, avg batch time: 0.5055, average train loss: 1.4875
[09/26 03:34:41 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 2.8408
[09/26 03:34:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 29.00	top5: 67.00	
[09/26 03:34:41 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 03:34:48 visual_prompt]: Epoch 30 / 100: avg data time: 4.74e-02, avg batch time: 0.4990, average train loss: 2.0569
[09/26 03:34:49 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 2.2956
[09/26 03:34:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 73.50	
[09/26 03:34:49 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 03:34:56 visual_prompt]: Epoch 31 / 100: avg data time: 5.89e-02, avg batch time: 0.5084, average train loss: 1.9736
[09/26 03:34:58 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1688, average loss: 2.1822
[09/26 03:34:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.00	top5: 75.50	
[09/26 03:34:58 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 03:35:05 visual_prompt]: Epoch 32 / 100: avg data time: 4.97e-02, avg batch time: 0.4995, average train loss: 1.4477
[09/26 03:35:06 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1692, average loss: 2.0345
[09/26 03:35:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 80.00	
[09/26 03:35:06 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 03:35:13 visual_prompt]: Epoch 33 / 100: avg data time: 5.59e-02, avg batch time: 0.5069, average train loss: 1.2452
[09/26 03:35:14 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 1.9099
[09/26 03:35:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 85.50	
[09/26 03:35:14 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 03:35:21 visual_prompt]: Epoch 34 / 100: avg data time: 5.24e-02, avg batch time: 0.5006, average train loss: 1.2776
[09/26 03:35:23 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 1.9440
[09/26 03:35:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 85.00	
[09/26 03:35:23 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 03:35:30 visual_prompt]: Epoch 35 / 100: avg data time: 5.76e-02, avg batch time: 0.5069, average train loss: 2.6068
[09/26 03:35:31 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 3.4984
[09/26 03:35:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.00	top5: 41.50	
[09/26 03:35:31 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 03:35:38 visual_prompt]: Epoch 36 / 100: avg data time: 4.81e-02, avg batch time: 0.4984, average train loss: 3.0502
[09/26 03:35:39 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1693, average loss: 2.7132
[09/26 03:35:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.50	top5: 63.00	
[09/26 03:35:39 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 03:35:46 visual_prompt]: Epoch 37 / 100: avg data time: 5.10e-02, avg batch time: 0.5005, average train loss: 1.9413
[09/26 03:35:48 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1693, average loss: 2.0386
[09/26 03:35:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.00	top5: 82.50	
[09/26 03:35:48 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 03:35:55 visual_prompt]: Epoch 38 / 100: avg data time: 5.61e-02, avg batch time: 0.5056, average train loss: 1.3750
[09/26 03:35:56 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1693, average loss: 1.9230
[09/26 03:35:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 86.00	
[09/26 03:35:56 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 03:36:03 visual_prompt]: Epoch 39 / 100: avg data time: 6.01e-02, avg batch time: 0.5082, average train loss: 1.0982
[09/26 03:36:04 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1694, average loss: 1.8757
[09/26 03:36:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 83.50	
[09/26 03:36:04 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 03:36:11 visual_prompt]: Epoch 40 / 100: avg data time: 5.04e-02, avg batch time: 0.4989, average train loss: 1.1276
[09/26 03:36:13 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 2.3774
[09/26 03:36:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 77.50	
[09/26 03:36:13 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 03:36:20 visual_prompt]: Epoch 41 / 100: avg data time: 4.56e-02, avg batch time: 0.4968, average train loss: 1.2597
[09/26 03:36:21 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1697, average loss: 1.9338
[09/26 03:36:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 85.00	
[09/26 03:36:21 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 03:36:28 visual_prompt]: Epoch 42 / 100: avg data time: 5.60e-02, avg batch time: 0.5046, average train loss: 1.0371
[09/26 03:36:29 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1692, average loss: 1.6562
[09/26 03:36:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 90.50	
[09/26 03:36:29 visual_prompt]: Best epoch 42: best metric: 0.560
[09/26 03:36:29 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 03:36:36 visual_prompt]: Epoch 43 / 100: avg data time: 5.91e-02, avg batch time: 0.5081, average train loss: 0.8398
[09/26 03:36:38 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1696, average loss: 1.6032
[09/26 03:36:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 90.50	
[09/26 03:36:38 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 03:36:45 visual_prompt]: Epoch 44 / 100: avg data time: 5.72e-02, avg batch time: 0.5052, average train loss: 0.8332
[09/26 03:36:46 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1698, average loss: 1.9172
[09/26 03:36:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 86.00	
[09/26 03:36:46 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 03:36:53 visual_prompt]: Epoch 45 / 100: avg data time: 5.55e-02, avg batch time: 0.5057, average train loss: 0.8930
[09/26 03:36:55 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1694, average loss: 2.0748
[09/26 03:36:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 82.00	
[09/26 03:36:55 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 03:37:01 visual_prompt]: Epoch 46 / 100: avg data time: 5.53e-02, avg batch time: 0.5042, average train loss: 1.1556
[09/26 03:37:03 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1696, average loss: 1.8466
[09/26 03:37:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 85.50	
[09/26 03:37:03 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 03:37:10 visual_prompt]: Epoch 47 / 100: avg data time: 5.49e-02, avg batch time: 0.5040, average train loss: 0.8237
[09/26 03:37:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1697, average loss: 1.5518
[09/26 03:37:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 91.50	
[09/26 03:37:11 visual_prompt]: Best epoch 47: best metric: 0.600
[09/26 03:37:11 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 03:37:18 visual_prompt]: Epoch 48 / 100: avg data time: 6.21e-02, avg batch time: 0.5114, average train loss: 0.9506
[09/26 03:37:20 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 2.1350
[09/26 03:37:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 83.50	
[09/26 03:37:20 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 03:37:27 visual_prompt]: Epoch 49 / 100: avg data time: 5.63e-02, avg batch time: 0.5051, average train loss: 1.0798
[09/26 03:37:28 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 1.6397
[09/26 03:37:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 89.50	
[09/26 03:37:28 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 03:37:35 visual_prompt]: Epoch 50 / 100: avg data time: 5.52e-02, avg batch time: 0.5042, average train loss: 1.0061
[09/26 03:37:37 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1692, average loss: 2.7590
[09/26 03:37:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.00	top5: 62.00	
[09/26 03:37:37 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 03:37:43 visual_prompt]: Epoch 51 / 100: avg data time: 5.68e-02, avg batch time: 0.5055, average train loss: 1.3464
[09/26 03:37:45 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1695, average loss: 2.3251
[09/26 03:37:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 84.50	
[09/26 03:37:45 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 03:37:52 visual_prompt]: Epoch 52 / 100: avg data time: 5.66e-02, avg batch time: 0.5045, average train loss: 3.0639
[09/26 03:37:53 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1695, average loss: 2.7862
[09/26 03:37:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 66.50	
[09/26 03:37:53 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 03:38:00 visual_prompt]: Epoch 53 / 100: avg data time: 5.15e-02, avg batch time: 0.5004, average train loss: 1.5132
[09/26 03:38:02 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 1.9545
[09/26 03:38:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 03:38:02 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 03:38:09 visual_prompt]: Epoch 54 / 100: avg data time: 5.11e-02, avg batch time: 0.5000, average train loss: 1.0755
[09/26 03:38:10 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1690, average loss: 1.6377
[09/26 03:38:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 90.00	
[09/26 03:38:10 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 03:38:17 visual_prompt]: Epoch 55 / 100: avg data time: 5.37e-02, avg batch time: 0.5027, average train loss: 0.9260
[09/26 03:38:18 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1699, average loss: 1.6606
[09/26 03:38:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.50	
[09/26 03:38:18 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 03:38:25 visual_prompt]: Epoch 56 / 100: avg data time: 5.55e-02, avg batch time: 0.5034, average train loss: 1.1965
[09/26 03:38:27 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1696, average loss: 1.7104
[09/26 03:38:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 88.50	
[09/26 03:38:27 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 03:38:34 visual_prompt]: Epoch 57 / 100: avg data time: 6.05e-02, avg batch time: 0.5084, average train loss: 0.9355
[09/26 03:38:35 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1695, average loss: 1.6064
[09/26 03:38:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 89.00	
[09/26 03:38:35 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 03:38:42 visual_prompt]: Epoch 58 / 100: avg data time: 4.38e-02, avg batch time: 0.4922, average train loss: 0.7637
[09/26 03:38:44 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1695, average loss: 1.5936
[09/26 03:38:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 91.00	
[09/26 03:38:44 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 03:38:51 visual_prompt]: Epoch 59 / 100: avg data time: 5.46e-02, avg batch time: 0.5044, average train loss: 0.6827
[09/26 03:38:52 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1694, average loss: 1.5602
[09/26 03:38:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 92.00	
[09/26 03:38:52 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 03:38:59 visual_prompt]: Epoch 60 / 100: avg data time: 5.70e-02, avg batch time: 0.5062, average train loss: 0.6624
[09/26 03:39:00 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1693, average loss: 1.6107
[09/26 03:39:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 87.50	
[09/26 03:39:01 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 03:39:07 visual_prompt]: Epoch 61 / 100: avg data time: 4.52e-02, avg batch time: 0.4947, average train loss: 0.6388
[09/26 03:39:09 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1695, average loss: 1.5772
[09/26 03:39:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 87.50	
[09/26 03:39:09 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 03:39:16 visual_prompt]: Epoch 62 / 100: avg data time: 4.37e-02, avg batch time: 0.4949, average train loss: 0.6047
[09/26 03:39:17 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1697, average loss: 1.5758
[09/26 03:39:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 87.50	
[09/26 03:39:17 visual_prompt]: Best epoch 62: best metric: 0.610
[09/26 03:39:17 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 03:39:24 visual_prompt]: Epoch 63 / 100: avg data time: 4.59e-02, avg batch time: 0.4949, average train loss: 0.5652
[09/26 03:39:25 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1698, average loss: 1.6357
[09/26 03:39:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 90.00	
[09/26 03:39:25 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 03:39:32 visual_prompt]: Epoch 64 / 100: avg data time: 5.85e-02, avg batch time: 0.5070, average train loss: 0.5316
[09/26 03:39:34 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1691, average loss: 1.5396
[09/26 03:39:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 89.50	
[09/26 03:39:34 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 03:39:41 visual_prompt]: Epoch 65 / 100: avg data time: 4.98e-02, avg batch time: 0.5023, average train loss: 0.5113
[09/26 03:39:42 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1695, average loss: 1.5481
[09/26 03:39:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 90.00	
[09/26 03:39:42 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 03:39:49 visual_prompt]: Epoch 66 / 100: avg data time: 4.31e-02, avg batch time: 0.4940, average train loss: 0.4895
[09/26 03:39:50 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 1.6127
[09/26 03:39:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 88.50	
[09/26 03:39:50 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 03:39:57 visual_prompt]: Epoch 67 / 100: avg data time: 5.27e-02, avg batch time: 0.5011, average train loss: 0.4677
[09/26 03:39:59 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1696, average loss: 1.5858
[09/26 03:39:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 91.00	
[09/26 03:39:59 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 03:40:06 visual_prompt]: Epoch 68 / 100: avg data time: 5.41e-02, avg batch time: 0.5036, average train loss: 0.4827
[09/26 03:40:07 visual_prompt]: Inference (val):avg data time: 4.82e-05, avg batch time: 0.1701, average loss: 1.4878
[09/26 03:40:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 91.00	
[09/26 03:40:07 visual_prompt]: Best epoch 68: best metric: 0.620
[09/26 03:40:07 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 03:40:14 visual_prompt]: Epoch 69 / 100: avg data time: 4.19e-02, avg batch time: 0.4913, average train loss: 0.4691
[09/26 03:40:15 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1694, average loss: 1.5038
[09/26 03:40:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 91.50	
[09/26 03:40:15 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 03:40:22 visual_prompt]: Epoch 70 / 100: avg data time: 5.33e-02, avg batch time: 0.5023, average train loss: 0.3946
[09/26 03:40:24 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 1.5646
[09/26 03:40:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 90.00	
[09/26 03:40:24 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 03:40:31 visual_prompt]: Epoch 71 / 100: avg data time: 6.09e-02, avg batch time: 0.5094, average train loss: 0.3381
[09/26 03:40:32 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 1.5682
[09/26 03:40:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 86.50	
[09/26 03:40:32 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 03:40:39 visual_prompt]: Epoch 72 / 100: avg data time: 5.25e-02, avg batch time: 0.5009, average train loss: 0.3096
[09/26 03:40:40 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1696, average loss: 1.4843
[09/26 03:40:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 92.00	
[09/26 03:40:40 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 03:40:47 visual_prompt]: Epoch 73 / 100: avg data time: 5.46e-02, avg batch time: 0.5030, average train loss: 0.2747
[09/26 03:40:49 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1693, average loss: 1.5471
[09/26 03:40:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 89.00	
[09/26 03:40:49 visual_prompt]: Best epoch 73: best metric: 0.625
[09/26 03:40:49 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 03:40:56 visual_prompt]: Epoch 74 / 100: avg data time: 5.84e-02, avg batch time: 0.5069, average train loss: 0.6685
[09/26 03:40:57 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1695, average loss: 1.6881
[09/26 03:40:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 88.00	
[09/26 03:40:57 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 03:41:04 visual_prompt]: Epoch 75 / 100: avg data time: 5.24e-02, avg batch time: 0.5019, average train loss: 0.5131
[09/26 03:41:05 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1696, average loss: 1.5602
[09/26 03:41:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 90.00	
[09/26 03:41:05 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 03:41:12 visual_prompt]: Epoch 76 / 100: avg data time: 5.24e-02, avg batch time: 0.5009, average train loss: 0.3451
[09/26 03:41:14 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1692, average loss: 1.5752
[09/26 03:41:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 89.00	
[09/26 03:41:14 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 03:41:21 visual_prompt]: Epoch 77 / 100: avg data time: 5.74e-02, avg batch time: 0.5057, average train loss: 0.2897
[09/26 03:41:22 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1691, average loss: 1.5388
[09/26 03:41:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 90.50	
[09/26 03:41:22 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 03:41:29 visual_prompt]: Epoch 78 / 100: avg data time: 4.75e-02, avg batch time: 0.4976, average train loss: 0.2491
[09/26 03:41:30 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1695, average loss: 1.5168
[09/26 03:41:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 92.00	
[09/26 03:41:30 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 03:41:37 visual_prompt]: Epoch 79 / 100: avg data time: 6.29e-02, avg batch time: 0.5117, average train loss: 0.2212
[09/26 03:41:39 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 1.5150
[09/26 03:41:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 91.00	
[09/26 03:41:39 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 03:41:46 visual_prompt]: Epoch 80 / 100: avg data time: 5.76e-02, avg batch time: 0.5055, average train loss: 0.1832
[09/26 03:41:47 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1695, average loss: 1.5068
[09/26 03:41:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 89.50	
[09/26 03:41:47 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 03:41:54 visual_prompt]: Epoch 81 / 100: avg data time: 5.31e-02, avg batch time: 0.5039, average train loss: 0.1659
[09/26 03:41:56 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1694, average loss: 1.4939
[09/26 03:41:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 91.00	
[09/26 03:41:56 visual_prompt]: Best epoch 81: best metric: 0.630
[09/26 03:41:56 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 03:42:03 visual_prompt]: Epoch 82 / 100: avg data time: 4.97e-02, avg batch time: 0.4987, average train loss: 0.1466
[09/26 03:42:04 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1696, average loss: 1.5070
[09/26 03:42:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 90.00	
[09/26 03:42:04 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 03:42:11 visual_prompt]: Epoch 83 / 100: avg data time: 6.16e-02, avg batch time: 0.5110, average train loss: 0.1386
[09/26 03:42:13 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 1.5305
[09/26 03:42:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.50	top5: 90.00	
[09/26 03:42:13 visual_prompt]: Best epoch 83: best metric: 0.635
[09/26 03:42:13 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 03:42:19 visual_prompt]: Epoch 84 / 100: avg data time: 6.25e-02, avg batch time: 0.5111, average train loss: 0.1311
[09/26 03:42:21 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 1.5185
[09/26 03:42:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 64.00	top5: 88.00	
[09/26 03:42:21 visual_prompt]: Best epoch 84: best metric: 0.640
[09/26 03:42:21 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 03:42:28 visual_prompt]: Epoch 85 / 100: avg data time: 5.77e-02, avg batch time: 0.5058, average train loss: 0.1255
[09/26 03:42:30 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1696, average loss: 1.5625
[09/26 03:42:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 90.00	
[09/26 03:42:30 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 03:42:36 visual_prompt]: Epoch 86 / 100: avg data time: 6.05e-02, avg batch time: 0.5086, average train loss: 0.1238
[09/26 03:42:38 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 1.5768
[09/26 03:42:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 88.50	
[09/26 03:42:38 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 03:42:45 visual_prompt]: Epoch 87 / 100: avg data time: 4.78e-02, avg batch time: 0.4970, average train loss: 0.1252
[09/26 03:42:46 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 1.5601
[09/26 03:42:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.50	top5: 89.50	
[09/26 03:42:46 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 03:42:53 visual_prompt]: Epoch 88 / 100: avg data time: 4.37e-02, avg batch time: 0.4932, average train loss: 0.1205
[09/26 03:42:54 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1700, average loss: 1.5573
[09/26 03:42:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 90.00	
[09/26 03:42:54 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 03:43:01 visual_prompt]: Epoch 89 / 100: avg data time: 5.50e-02, avg batch time: 0.5032, average train loss: 0.1168
[09/26 03:43:03 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1697, average loss: 1.5825
[09/26 03:43:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 88.00	
[09/26 03:43:03 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 03:43:10 visual_prompt]: Epoch 90 / 100: avg data time: 5.68e-02, avg batch time: 0.5051, average train loss: 0.1136
[09/26 03:43:11 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1695, average loss: 1.5706
[09/26 03:43:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 87.00	
[09/26 03:43:11 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 03:43:18 visual_prompt]: Epoch 91 / 100: avg data time: 5.54e-02, avg batch time: 0.5055, average train loss: 0.1112
[09/26 03:43:20 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 1.5983
[09/26 03:43:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 88.50	
[09/26 03:43:20 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 03:43:26 visual_prompt]: Epoch 92 / 100: avg data time: 4.89e-02, avg batch time: 0.4977, average train loss: 0.1110
[09/26 03:43:28 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 1.5857
[09/26 03:43:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 88.50	
[09/26 03:43:28 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 03:43:35 visual_prompt]: Epoch 93 / 100: avg data time: 4.82e-02, avg batch time: 0.5004, average train loss: 0.1099
[09/26 03:43:36 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1697, average loss: 1.5871
[09/26 03:43:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 89.00	
[09/26 03:43:36 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 03:43:43 visual_prompt]: Epoch 94 / 100: avg data time: 6.16e-02, avg batch time: 0.5108, average train loss: 0.1082
[09/26 03:43:45 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 1.5992
[09/26 03:43:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 88.50	
[09/26 03:43:45 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 03:43:52 visual_prompt]: Epoch 95 / 100: avg data time: 5.83e-02, avg batch time: 0.5066, average train loss: 0.1074
[09/26 03:43:53 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 1.6040
[09/26 03:43:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 88.00	
[09/26 03:43:53 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 03:44:00 visual_prompt]: Epoch 96 / 100: avg data time: 4.86e-02, avg batch time: 0.4977, average train loss: 0.1070
[09/26 03:44:01 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1699, average loss: 1.6093
[09/26 03:44:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 88.00	
[09/26 03:44:01 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 03:44:08 visual_prompt]: Epoch 97 / 100: avg data time: 5.45e-02, avg batch time: 0.5041, average train loss: 0.1067
[09/26 03:44:10 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1695, average loss: 1.6152
[09/26 03:44:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 88.00	
[09/26 03:44:10 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 03:44:17 visual_prompt]: Epoch 98 / 100: avg data time: 5.96e-02, avg batch time: 0.5078, average train loss: 0.1063
[09/26 03:44:18 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1690, average loss: 1.6166
[09/26 03:44:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 88.50	
[09/26 03:44:18 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 03:44:25 visual_prompt]: Epoch 99 / 100: avg data time: 6.01e-02, avg batch time: 0.5095, average train loss: 0.1058
[09/26 03:44:27 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 1.6152
[09/26 03:44:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 88.50	
[09/26 03:44:27 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 03:44:33 visual_prompt]: Epoch 100 / 100: avg data time: 4.08e-02, avg batch time: 0.4916, average train loss: 0.1058
[09/26 03:44:35 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 1.6151
[09/26 03:44:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 88.50	
[09/26 03:44:35 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 03:44:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 03:44:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 03:44:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 03:44:35 visual_prompt]: Training with config:
[09/26 03:44:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 03:44:35 visual_prompt]: Loading training data...
[09/26 03:44:35 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 03:44:37 visual_prompt]: Number of images: 800
[09/26 03:44:37 visual_prompt]: Number of classes: 47 / 47
[09/26 03:44:37 visual_prompt]: Loading validation data...
[09/26 03:44:37 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 03:44:37 visual_prompt]: Number of images: 200
[09/26 03:44:37 visual_prompt]: Number of classes: 47 / 47
[09/26 03:44:37 visual_prompt]: Constructing models...
[09/26 03:44:40 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 03:44:40 visual_prompt]: tuned percent:0.576
[09/26 03:44:40 visual_prompt]: Device used for model: 0
[09/26 03:44:40 visual_prompt]: Setting up Evaluator...
[09/26 03:44:40 visual_prompt]: Setting up Trainer...
[09/26 03:44:40 visual_prompt]: 	Setting up the optimizer...
[09/26 03:44:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 03:44:47 visual_prompt]: Epoch 1 / 100: avg data time: 5.26e-02, avg batch time: 0.5011, average train loss: 3.9256
[09/26 03:44:48 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1688, average loss: 3.9045
[09/26 03:44:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 03:44:48 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 03:44:48 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 03:44:55 visual_prompt]: Epoch 2 / 100: avg data time: 5.02e-02, avg batch time: 0.4977, average train loss: 3.8734
[09/26 03:44:57 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1689, average loss: 3.9066
[09/26 03:44:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 8.50	
[09/26 03:44:57 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 03:45:03 visual_prompt]: Epoch 3 / 100: avg data time: 5.18e-02, avg batch time: 0.4995, average train loss: 3.8292
[09/26 03:45:05 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 3.8597
[09/26 03:45:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 12.00	
[09/26 03:45:05 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 03:45:12 visual_prompt]: Epoch 4 / 100: avg data time: 4.51e-02, avg batch time: 0.4934, average train loss: 3.7164
[09/26 03:45:13 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1688, average loss: 3.6770
[09/26 03:45:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.00	top5: 27.00	
[09/26 03:45:13 visual_prompt]: Best epoch 4: best metric: 0.090
[09/26 03:45:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 03:45:20 visual_prompt]: Epoch 5 / 100: avg data time: 5.51e-02, avg batch time: 0.5028, average train loss: 3.6483
[09/26 03:45:22 visual_prompt]: Inference (val):avg data time: 4.43e-05, avg batch time: 0.1687, average loss: 3.7070
[09/26 03:45:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 22.00	
[09/26 03:45:22 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 03:45:29 visual_prompt]: Epoch 6 / 100: avg data time: 5.79e-02, avg batch time: 0.5054, average train loss: 3.4545
[09/26 03:45:30 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 3.7057
[09/26 03:45:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.00	top5: 29.50	
[09/26 03:45:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 03:45:37 visual_prompt]: Epoch 7 / 100: avg data time: 4.38e-02, avg batch time: 0.4935, average train loss: 3.1129
[09/26 03:45:38 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 3.4197
[09/26 03:45:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 27.50	
[09/26 03:45:38 visual_prompt]: Best epoch 7: best metric: 0.110
[09/26 03:45:38 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 03:45:45 visual_prompt]: Epoch 8 / 100: avg data time: 4.41e-02, avg batch time: 0.4933, average train loss: 2.7356
[09/26 03:45:46 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 3.7881
[09/26 03:45:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 14.00	top5: 31.00	
[09/26 03:45:46 visual_prompt]: Best epoch 8: best metric: 0.140
[09/26 03:45:46 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 03:45:53 visual_prompt]: Epoch 9 / 100: avg data time: 5.96e-02, avg batch time: 0.5071, average train loss: 1.9968
[09/26 03:45:55 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1690, average loss: 2.5366
[09/26 03:45:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 32.00	top5: 67.00	
[09/26 03:45:55 visual_prompt]: Best epoch 9: best metric: 0.320
[09/26 03:45:55 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 03:46:02 visual_prompt]: Epoch 10 / 100: avg data time: 5.58e-02, avg batch time: 0.5034, average train loss: 1.2112
[09/26 03:46:03 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 1.9212
[09/26 03:46:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 83.00	
[09/26 03:46:03 visual_prompt]: Best epoch 10: best metric: 0.480
[09/26 03:46:03 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 03:46:10 visual_prompt]: Epoch 11 / 100: avg data time: 6.26e-02, avg batch time: 0.5103, average train loss: 0.6217
[09/26 03:46:12 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 1.8388
[09/26 03:46:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 81.50	
[09/26 03:46:12 visual_prompt]: Best epoch 11: best metric: 0.485
[09/26 03:46:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 03:46:18 visual_prompt]: Epoch 12 / 100: avg data time: 5.52e-02, avg batch time: 0.5031, average train loss: 0.3342
[09/26 03:46:20 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 1.6214
[09/26 03:46:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 86.50	
[09/26 03:46:20 visual_prompt]: Best epoch 12: best metric: 0.510
[09/26 03:46:20 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 03:46:27 visual_prompt]: Epoch 13 / 100: avg data time: 6.44e-02, avg batch time: 0.5120, average train loss: 0.1625
[09/26 03:46:28 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1688, average loss: 1.5902
[09/26 03:46:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.00	
[09/26 03:46:28 visual_prompt]: Best epoch 13: best metric: 0.580
[09/26 03:46:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 03:46:35 visual_prompt]: Epoch 14 / 100: avg data time: 4.74e-02, avg batch time: 0.4968, average train loss: 0.0932
[09/26 03:46:36 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 1.5833
[09/26 03:46:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 85.50	
[09/26 03:46:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 03:46:43 visual_prompt]: Epoch 15 / 100: avg data time: 4.43e-02, avg batch time: 0.4946, average train loss: 0.0661
[09/26 03:46:45 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 1.6082
[09/26 03:46:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 84.50	
[09/26 03:46:45 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 03:46:52 visual_prompt]: Epoch 16 / 100: avg data time: 5.67e-02, avg batch time: 0.5051, average train loss: 0.0556
[09/26 03:46:53 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1688, average loss: 1.6115
[09/26 03:46:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.50	
[09/26 03:46:53 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 03:47:00 visual_prompt]: Epoch 17 / 100: avg data time: 5.60e-02, avg batch time: 0.5039, average train loss: 0.0467
[09/26 03:47:02 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1690, average loss: 1.6352
[09/26 03:47:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 85.50	
[09/26 03:47:02 visual_prompt]: Best epoch 17: best metric: 0.590
[09/26 03:47:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 03:47:08 visual_prompt]: Epoch 18 / 100: avg data time: 5.97e-02, avg batch time: 0.5080, average train loss: 0.0425
[09/26 03:47:10 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1691, average loss: 1.6140
[09/26 03:47:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 84.50	
[09/26 03:47:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 03:47:17 visual_prompt]: Epoch 19 / 100: avg data time: 5.79e-02, avg batch time: 0.5079, average train loss: 0.0405
[09/26 03:47:18 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 1.6028
[09/26 03:47:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 84.50	
[09/26 03:47:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 03:47:25 visual_prompt]: Epoch 20 / 100: avg data time: 4.59e-02, avg batch time: 0.4954, average train loss: 0.0412
[09/26 03:47:26 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 1.6410
[09/26 03:47:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 86.50	
[09/26 03:47:26 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 03:47:33 visual_prompt]: Epoch 21 / 100: avg data time: 5.52e-02, avg batch time: 0.5040, average train loss: 0.0422
[09/26 03:47:35 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1695, average loss: 1.6362
[09/26 03:47:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 87.00	
[09/26 03:47:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 03:47:42 visual_prompt]: Epoch 22 / 100: avg data time: 5.73e-02, avg batch time: 0.5059, average train loss: 0.0395
[09/26 03:47:43 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1691, average loss: 1.6984
[09/26 03:47:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.00	
[09/26 03:47:43 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 03:47:50 visual_prompt]: Epoch 23 / 100: avg data time: 5.45e-02, avg batch time: 0.5032, average train loss: 0.0405
[09/26 03:47:51 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 1.6452
[09/26 03:47:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 85.00	
[09/26 03:47:51 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 03:47:58 visual_prompt]: Epoch 24 / 100: avg data time: 5.90e-02, avg batch time: 0.5076, average train loss: 0.0420
[09/26 03:48:00 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1690, average loss: 1.6876
[09/26 03:48:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 88.00	
[09/26 03:48:00 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 03:48:07 visual_prompt]: Epoch 25 / 100: avg data time: 4.91e-02, avg batch time: 0.4987, average train loss: 0.0748
[09/26 03:48:08 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 1.7744
[09/26 03:48:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 84.00	
[09/26 03:48:08 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 03:48:15 visual_prompt]: Epoch 26 / 100: avg data time: 6.09e-02, avg batch time: 0.5101, average train loss: 0.2067
[09/26 03:48:16 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1692, average loss: 1.8259
[09/26 03:48:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 84.00	
[09/26 03:48:16 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 03:48:23 visual_prompt]: Epoch 27 / 100: avg data time: 6.22e-02, avg batch time: 0.5102, average train loss: 0.3729
[09/26 03:48:25 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1690, average loss: 1.6686
[09/26 03:48:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 87.50	
[09/26 03:48:25 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 03:48:32 visual_prompt]: Epoch 28 / 100: avg data time: 6.15e-02, avg batch time: 0.5106, average train loss: 0.2267
[09/26 03:48:33 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1696, average loss: 1.6408
[09/26 03:48:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 85.00	
[09/26 03:48:33 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 03:48:40 visual_prompt]: Epoch 29 / 100: avg data time: 5.28e-02, avg batch time: 0.5007, average train loss: 0.1409
[09/26 03:48:42 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1697, average loss: 1.5689
[09/26 03:48:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 86.50	
[09/26 03:48:42 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 03:48:49 visual_prompt]: Epoch 30 / 100: avg data time: 5.45e-02, avg batch time: 0.5043, average train loss: 0.0749
[09/26 03:48:50 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 1.5827
[09/26 03:48:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 85.50	
[09/26 03:48:50 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 03:48:57 visual_prompt]: Epoch 31 / 100: avg data time: 4.32e-02, avg batch time: 0.4936, average train loss: 0.0453
[09/26 03:48:58 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1689, average loss: 1.5846
[09/26 03:48:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 87.50	
[09/26 03:48:58 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 03:49:05 visual_prompt]: Epoch 32 / 100: avg data time: 6.01e-02, avg batch time: 0.5079, average train loss: 0.0327
[09/26 03:49:07 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 1.5710
[09/26 03:49:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 87.00	
[09/26 03:49:07 visual_prompt]: Best epoch 32: best metric: 0.600
[09/26 03:49:07 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 03:49:14 visual_prompt]: Epoch 33 / 100: avg data time: 5.12e-02, avg batch time: 0.4993, average train loss: 0.0275
[09/26 03:49:15 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1695, average loss: 1.6061
[09/26 03:49:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.50	
[09/26 03:49:15 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 03:49:22 visual_prompt]: Epoch 34 / 100: avg data time: 4.76e-02, avg batch time: 0.4988, average train loss: 0.0249
[09/26 03:49:24 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1695, average loss: 1.6183
[09/26 03:49:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.50	
[09/26 03:49:24 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 03:49:30 visual_prompt]: Epoch 35 / 100: avg data time: 5.60e-02, avg batch time: 0.5054, average train loss: 0.0230
[09/26 03:49:32 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1696, average loss: 1.6325
[09/26 03:49:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 03:49:32 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 03:49:39 visual_prompt]: Epoch 36 / 100: avg data time: 4.67e-02, avg batch time: 0.4957, average train loss: 0.0225
[09/26 03:49:40 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1692, average loss: 1.6752
[09/26 03:49:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 87.00	
[09/26 03:49:40 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 03:49:47 visual_prompt]: Epoch 37 / 100: avg data time: 6.25e-02, avg batch time: 0.5107, average train loss: 0.0223
[09/26 03:49:49 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 1.6579
[09/26 03:49:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 86.50	
[09/26 03:49:49 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 03:49:56 visual_prompt]: Epoch 38 / 100: avg data time: 6.47e-02, avg batch time: 0.5124, average train loss: 0.0218
[09/26 03:49:57 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 1.6954
[09/26 03:49:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 03:49:57 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 03:50:04 visual_prompt]: Epoch 39 / 100: avg data time: 5.64e-02, avg batch time: 0.5070, average train loss: 0.0217
[09/26 03:50:06 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1691, average loss: 1.7121
[09/26 03:50:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 86.00	
[09/26 03:50:06 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 03:50:12 visual_prompt]: Epoch 40 / 100: avg data time: 4.64e-02, avg batch time: 0.4965, average train loss: 0.0209
[09/26 03:50:14 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1690, average loss: 1.6888
[09/26 03:50:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 85.50	
[09/26 03:50:14 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 03:50:21 visual_prompt]: Epoch 41 / 100: avg data time: 6.09e-02, avg batch time: 0.5097, average train loss: 0.0204
[09/26 03:50:22 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1696, average loss: 1.7261
[09/26 03:50:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 03:50:22 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 03:50:29 visual_prompt]: Epoch 42 / 100: avg data time: 6.02e-02, avg batch time: 0.5081, average train loss: 0.0212
[09/26 03:50:31 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1697, average loss: 1.8157
[09/26 03:50:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 86.00	
[09/26 03:50:31 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 03:50:38 visual_prompt]: Epoch 43 / 100: avg data time: 5.57e-02, avg batch time: 0.5056, average train loss: 0.0250
[09/26 03:50:39 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1693, average loss: 1.7470
[09/26 03:50:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 84.00	
[09/26 03:50:39 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 03:50:46 visual_prompt]: Epoch 44 / 100: avg data time: 6.38e-02, avg batch time: 0.5115, average train loss: 0.1458
[09/26 03:50:48 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1691, average loss: 2.2102
[09/26 03:50:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.00	top5: 76.50	
[09/26 03:50:48 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 03:50:55 visual_prompt]: Epoch 45 / 100: avg data time: 5.19e-02, avg batch time: 0.5005, average train loss: 0.7350
[09/26 03:50:56 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1693, average loss: 1.6695
[09/26 03:50:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 85.50	
[09/26 03:50:56 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 03:51:03 visual_prompt]: Epoch 46 / 100: avg data time: 4.92e-02, avg batch time: 0.5087, average train loss: 0.3910
[09/26 03:51:05 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 1.5757
[09/26 03:51:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 86.50	
[09/26 03:51:05 visual_prompt]: Best epoch 46: best metric: 0.610
[09/26 03:51:05 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 03:51:11 visual_prompt]: Epoch 47 / 100: avg data time: 5.10e-02, avg batch time: 0.4992, average train loss: 0.1638
[09/26 03:51:13 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1694, average loss: 1.3796
[09/26 03:51:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 65.00	top5: 89.50	
[09/26 03:51:13 visual_prompt]: Best epoch 47: best metric: 0.650
[09/26 03:51:13 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 03:51:20 visual_prompt]: Epoch 48 / 100: avg data time: 5.02e-02, avg batch time: 0.4988, average train loss: 0.0817
[09/26 03:51:21 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1692, average loss: 1.4864
[09/26 03:51:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 89.00	
[09/26 03:51:21 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 03:51:28 visual_prompt]: Epoch 49 / 100: avg data time: 5.92e-02, avg batch time: 0.5082, average train loss: 0.0514
[09/26 03:51:30 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1695, average loss: 1.4540
[09/26 03:51:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 87.50	
[09/26 03:51:30 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 03:51:36 visual_prompt]: Epoch 50 / 100: avg data time: 4.07e-02, avg batch time: 0.4897, average train loss: 0.0351
[09/26 03:51:38 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 1.4547
[09/26 03:51:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 88.00	
[09/26 03:51:38 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 03:51:45 visual_prompt]: Epoch 51 / 100: avg data time: 5.62e-02, avg batch time: 0.5045, average train loss: 0.0278
[09/26 03:51:47 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1689, average loss: 1.4772
[09/26 03:51:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 87.50	
[09/26 03:51:47 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 03:51:54 visual_prompt]: Epoch 52 / 100: avg data time: 6.37e-02, avg batch time: 0.5130, average train loss: 0.0250
[09/26 03:51:55 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 1.4756
[09/26 03:51:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.00	top5: 87.50	
[09/26 03:51:55 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 03:52:02 visual_prompt]: Epoch 53 / 100: avg data time: 5.50e-02, avg batch time: 0.5054, average train loss: 0.0236
[09/26 03:52:04 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1692, average loss: 1.4899
[09/26 03:52:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 65.00	top5: 87.00	
[09/26 03:52:04 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 03:52:10 visual_prompt]: Epoch 54 / 100: avg data time: 4.54e-02, avg batch time: 0.4961, average train loss: 0.0230
[09/26 03:52:12 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 1.4951
[09/26 03:52:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.50	top5: 87.50	
[09/26 03:52:12 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 03:52:19 visual_prompt]: Epoch 55 / 100: avg data time: 4.32e-02, avg batch time: 0.4926, average train loss: 0.0223
[09/26 03:52:20 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1691, average loss: 1.5077
[09/26 03:52:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 87.00	
[09/26 03:52:20 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 03:52:27 visual_prompt]: Epoch 56 / 100: avg data time: 4.74e-02, avg batch time: 0.4970, average train loss: 0.0215
[09/26 03:52:28 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 1.5198
[09/26 03:52:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 03:52:28 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 03:52:35 visual_prompt]: Epoch 57 / 100: avg data time: 4.49e-02, avg batch time: 0.4949, average train loss: 0.0212
[09/26 03:52:37 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1694, average loss: 1.5021
[09/26 03:52:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.50	top5: 87.00	
[09/26 03:52:37 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 03:52:43 visual_prompt]: Epoch 58 / 100: avg data time: 5.42e-02, avg batch time: 0.5031, average train loss: 0.0209
[09/26 03:52:45 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 1.5065
[09/26 03:52:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 67.00	top5: 86.50	
[09/26 03:52:45 visual_prompt]: Best epoch 58: best metric: 0.670
[09/26 03:52:45 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 03:52:52 visual_prompt]: Epoch 59 / 100: avg data time: 5.85e-02, avg batch time: 0.5065, average train loss: 0.0205
[09/26 03:52:53 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1689, average loss: 1.5578
[09/26 03:52:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.00	
[09/26 03:52:53 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 03:53:00 visual_prompt]: Epoch 60 / 100: avg data time: 4.93e-02, avg batch time: 0.4977, average train loss: 0.0203
[09/26 03:53:02 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1693, average loss: 1.5319
[09/26 03:53:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.50	top5: 86.50	
[09/26 03:53:02 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 03:53:09 visual_prompt]: Epoch 61 / 100: avg data time: 5.72e-02, avg batch time: 0.5054, average train loss: 0.0204
[09/26 03:53:10 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1690, average loss: 1.5560
[09/26 03:53:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.50	
[09/26 03:53:10 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 03:53:17 visual_prompt]: Epoch 62 / 100: avg data time: 4.48e-02, avg batch time: 0.4935, average train loss: 0.0199
[09/26 03:53:18 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 1.5396
[09/26 03:53:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 88.50	
[09/26 03:53:18 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 03:53:25 visual_prompt]: Epoch 63 / 100: avg data time: 6.61e-02, avg batch time: 0.5141, average train loss: 0.0187
[09/26 03:53:27 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 1.5553
[09/26 03:53:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 88.50	
[09/26 03:53:27 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 03:53:34 visual_prompt]: Epoch 64 / 100: avg data time: 5.03e-02, avg batch time: 0.5003, average train loss: 0.0192
[09/26 03:53:35 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 1.5956
[09/26 03:53:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 87.50	
[09/26 03:53:35 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 03:53:42 visual_prompt]: Epoch 65 / 100: avg data time: 6.32e-02, avg batch time: 0.5141, average train loss: 0.0181
[09/26 03:53:44 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1694, average loss: 1.5777
[09/26 03:53:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 88.00	
[09/26 03:53:44 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 03:53:50 visual_prompt]: Epoch 66 / 100: avg data time: 4.66e-02, avg batch time: 0.4953, average train loss: 0.0175
[09/26 03:53:52 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1694, average loss: 1.6130
[09/26 03:53:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 86.00	
[09/26 03:53:52 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 03:53:59 visual_prompt]: Epoch 67 / 100: avg data time: 4.93e-02, avg batch time: 0.4981, average train loss: 0.0174
[09/26 03:54:00 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 1.6060
[09/26 03:54:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 89.00	
[09/26 03:54:00 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 03:54:07 visual_prompt]: Epoch 68 / 100: avg data time: 4.54e-02, avg batch time: 0.4953, average train loss: 0.0168
[09/26 03:54:08 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1692, average loss: 1.6412
[09/26 03:54:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 87.00	
[09/26 03:54:08 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 03:54:15 visual_prompt]: Epoch 69 / 100: avg data time: 4.55e-02, avg batch time: 0.4956, average train loss: 0.0167
[09/26 03:54:17 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 1.6313
[09/26 03:54:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 87.50	
[09/26 03:54:17 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 03:54:24 visual_prompt]: Epoch 70 / 100: avg data time: 5.80e-02, avg batch time: 0.5073, average train loss: 0.0164
[09/26 03:54:25 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1695, average loss: 1.6288
[09/26 03:54:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 87.50	
[09/26 03:54:25 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 03:54:32 visual_prompt]: Epoch 71 / 100: avg data time: 5.57e-02, avg batch time: 0.5051, average train loss: 0.0163
[09/26 03:54:34 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 1.6429
[09/26 03:54:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 03:54:34 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 03:54:41 visual_prompt]: Epoch 72 / 100: avg data time: 5.22e-02, avg batch time: 0.5018, average train loss: 0.0160
[09/26 03:54:42 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1693, average loss: 1.6615
[09/26 03:54:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 87.50	
[09/26 03:54:42 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 03:54:49 visual_prompt]: Epoch 73 / 100: avg data time: 5.02e-02, avg batch time: 0.4998, average train loss: 0.0161
[09/26 03:54:51 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 1.6523
[09/26 03:54:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 88.00	
[09/26 03:54:51 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 03:54:57 visual_prompt]: Epoch 74 / 100: avg data time: 4.50e-02, avg batch time: 0.4959, average train loss: 0.0156
[09/26 03:54:59 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 1.6811
[09/26 03:54:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.00	
[09/26 03:54:59 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 03:55:06 visual_prompt]: Epoch 75 / 100: avg data time: 4.81e-02, avg batch time: 0.4979, average train loss: 0.0155
[09/26 03:55:07 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1694, average loss: 1.6720
[09/26 03:55:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 87.50	
[09/26 03:55:07 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 03:55:14 visual_prompt]: Epoch 76 / 100: avg data time: 5.16e-02, avg batch time: 0.5014, average train loss: 0.0153
[09/26 03:55:15 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1696, average loss: 1.6826
[09/26 03:55:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 87.00	
[09/26 03:55:15 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 03:55:22 visual_prompt]: Epoch 77 / 100: avg data time: 4.85e-02, avg batch time: 0.4980, average train loss: 0.0154
[09/26 03:55:24 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1692, average loss: 1.6782
[09/26 03:55:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 87.00	
[09/26 03:55:24 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 03:55:31 visual_prompt]: Epoch 78 / 100: avg data time: 5.26e-02, avg batch time: 0.5027, average train loss: 0.0155
[09/26 03:55:32 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.1694, average loss: 1.6860
[09/26 03:55:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 03:55:32 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 03:55:39 visual_prompt]: Epoch 79 / 100: avg data time: 6.06e-02, avg batch time: 0.5093, average train loss: 0.0153
[09/26 03:55:41 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 1.6760
[09/26 03:55:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 87.50	
[09/26 03:55:41 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 03:55:47 visual_prompt]: Epoch 80 / 100: avg data time: 5.92e-02, avg batch time: 0.5073, average train loss: 0.0151
[09/26 03:55:49 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 1.6959
[09/26 03:55:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 03:55:49 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 03:55:56 visual_prompt]: Epoch 81 / 100: avg data time: 5.44e-02, avg batch time: 0.5035, average train loss: 0.0149
[09/26 03:55:57 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1697, average loss: 1.6830
[09/26 03:55:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 87.00	
[09/26 03:55:57 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 03:56:04 visual_prompt]: Epoch 82 / 100: avg data time: 5.55e-02, avg batch time: 0.5034, average train loss: 0.0149
[09/26 03:56:06 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 1.7242
[09/26 03:56:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.00	
[09/26 03:56:06 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 03:56:12 visual_prompt]: Epoch 83 / 100: avg data time: 4.57e-02, avg batch time: 0.4964, average train loss: 0.0149
[09/26 03:56:14 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1688, average loss: 1.7112
[09/26 03:56:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 85.50	
[09/26 03:56:14 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 03:56:21 visual_prompt]: Epoch 84 / 100: avg data time: 6.64e-02, avg batch time: 0.5157, average train loss: 0.0146
[09/26 03:56:23 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 1.7306
[09/26 03:56:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 03:56:23 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 03:56:29 visual_prompt]: Epoch 85 / 100: avg data time: 5.35e-02, avg batch time: 0.5015, average train loss: 0.0148
[09/26 03:56:31 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 1.7237
[09/26 03:56:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.50	
[09/26 03:56:31 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 03:56:38 visual_prompt]: Epoch 86 / 100: avg data time: 5.00e-02, avg batch time: 0.4979, average train loss: 0.0147
[09/26 03:56:39 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1692, average loss: 1.7261
[09/26 03:56:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 03:56:39 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 03:56:46 visual_prompt]: Epoch 87 / 100: avg data time: 5.06e-02, avg batch time: 0.4991, average train loss: 0.0147
[09/26 03:56:48 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1694, average loss: 1.7202
[09/26 03:56:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 86.00	
[09/26 03:56:48 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 03:56:55 visual_prompt]: Epoch 88 / 100: avg data time: 5.81e-02, avg batch time: 0.5082, average train loss: 0.0144
[09/26 03:56:56 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1691, average loss: 1.7309
[09/26 03:56:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 85.50	
[09/26 03:56:56 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 03:57:03 visual_prompt]: Epoch 89 / 100: avg data time: 6.02e-02, avg batch time: 0.5081, average train loss: 0.0144
[09/26 03:57:04 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 1.7331
[09/26 03:57:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.50	
[09/26 03:57:04 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 03:57:11 visual_prompt]: Epoch 90 / 100: avg data time: 5.69e-02, avg batch time: 0.5053, average train loss: 0.0143
[09/26 03:57:13 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 1.7275
[09/26 03:57:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 86.00	
[09/26 03:57:13 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 03:57:20 visual_prompt]: Epoch 91 / 100: avg data time: 6.23e-02, avg batch time: 0.5104, average train loss: 0.0145
[09/26 03:57:21 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1690, average loss: 1.7331
[09/26 03:57:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.00	
[09/26 03:57:21 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 03:57:28 visual_prompt]: Epoch 92 / 100: avg data time: 6.27e-02, avg batch time: 0.5112, average train loss: 0.0145
[09/26 03:57:30 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1697, average loss: 1.7356
[09/26 03:57:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 86.50	
[09/26 03:57:30 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 03:57:37 visual_prompt]: Epoch 93 / 100: avg data time: 5.16e-02, avg batch time: 0.5004, average train loss: 0.0144
[09/26 03:57:38 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1694, average loss: 1.7368
[09/26 03:57:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 86.50	
[09/26 03:57:38 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 03:57:45 visual_prompt]: Epoch 94 / 100: avg data time: 5.10e-02, avg batch time: 0.4995, average train loss: 0.0143
[09/26 03:57:46 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1696, average loss: 1.7373
[09/26 03:57:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 85.50	
[09/26 03:57:46 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 03:57:53 visual_prompt]: Epoch 95 / 100: avg data time: 6.13e-02, avg batch time: 0.5093, average train loss: 0.0142
[09/26 03:57:55 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1692, average loss: 1.7366
[09/26 03:57:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 85.50	
[09/26 03:57:55 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 03:58:02 visual_prompt]: Epoch 96 / 100: avg data time: 5.75e-02, avg batch time: 0.5055, average train loss: 0.0142
[09/26 03:58:03 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 1.7385
[09/26 03:58:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 86.00	
[09/26 03:58:03 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 03:58:10 visual_prompt]: Epoch 97 / 100: avg data time: 6.30e-02, avg batch time: 0.5118, average train loss: 0.0142
[09/26 03:58:12 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1689, average loss: 1.7393
[09/26 03:58:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 03:58:12 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 03:58:19 visual_prompt]: Epoch 98 / 100: avg data time: 5.77e-02, avg batch time: 0.5073, average train loss: 0.0143
[09/26 03:58:20 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 1.7394
[09/26 03:58:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 86.00	
[09/26 03:58:20 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 03:58:27 visual_prompt]: Epoch 99 / 100: avg data time: 5.44e-02, avg batch time: 0.5028, average train loss: 0.0142
[09/26 03:58:29 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1693, average loss: 1.7393
[09/26 03:58:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 03:58:29 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 03:58:36 visual_prompt]: Epoch 100 / 100: avg data time: 6.33e-02, avg batch time: 0.5125, average train loss: 0.0143
[09/26 03:58:37 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1694, average loss: 1.7392
[09/26 03:58:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 03:58:37 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 03:58:37 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 03:58:37 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 03:58:37 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 03:58:37 visual_prompt]: Training with config:
[09/26 03:58:37 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 03:58:37 visual_prompt]: Loading training data...
[09/26 03:58:37 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 03:58:39 visual_prompt]: Number of images: 800
[09/26 03:58:39 visual_prompt]: Number of classes: 47 / 47
[09/26 03:58:39 visual_prompt]: Loading validation data...
[09/26 03:58:39 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 03:58:40 visual_prompt]: Number of images: 200
[09/26 03:58:40 visual_prompt]: Number of classes: 47 / 47
[09/26 03:58:40 visual_prompt]: Constructing models...
[09/26 03:58:42 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 03:58:42 visual_prompt]: tuned percent:0.576
[09/26 03:58:42 visual_prompt]: Device used for model: 0
[09/26 03:58:42 visual_prompt]: Setting up Evaluator...
[09/26 03:58:42 visual_prompt]: Setting up Trainer...
[09/26 03:58:42 visual_prompt]: 	Setting up the optimizer...
[09/26 03:58:42 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 03:58:49 visual_prompt]: Epoch 1 / 100: avg data time: 5.34e-02, avg batch time: 0.5009, average train loss: 3.9268
[09/26 03:58:51 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1685, average loss: 3.9045
[09/26 03:58:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 03:58:51 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 03:58:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 03:58:57 visual_prompt]: Epoch 2 / 100: avg data time: 4.80e-02, avg batch time: 0.4973, average train loss: 3.8626
[09/26 03:58:59 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1684, average loss: 3.8907
[09/26 03:58:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 13.00	
[09/26 03:58:59 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 03:59:06 visual_prompt]: Epoch 3 / 100: avg data time: 5.95e-02, avg batch time: 0.5063, average train loss: 3.7733
[09/26 03:59:07 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1690, average loss: 3.8069
[09/26 03:59:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 20.00	
[09/26 03:59:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 03:59:14 visual_prompt]: Epoch 4 / 100: avg data time: 5.50e-02, avg batch time: 0.5022, average train loss: 3.6165
[09/26 03:59:15 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1688, average loss: 4.9036
[09/26 03:59:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.50	
[09/26 03:59:15 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 03:59:22 visual_prompt]: Epoch 5 / 100: avg data time: 6.63e-02, avg batch time: 0.5136, average train loss: 3.7694
[09/26 03:59:24 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 3.5762
[09/26 03:59:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.50	top5: 22.50	
[09/26 03:59:24 visual_prompt]: Best epoch 5: best metric: 0.075
[09/26 03:59:24 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 03:59:31 visual_prompt]: Epoch 6 / 100: avg data time: 5.32e-02, avg batch time: 0.5012, average train loss: 3.3006
[09/26 03:59:32 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1687, average loss: 3.3009
[09/26 03:59:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.50	top5: 36.00	
[09/26 03:59:32 visual_prompt]: Best epoch 6: best metric: 0.135
[09/26 03:59:32 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 03:59:39 visual_prompt]: Epoch 7 / 100: avg data time: 6.48e-02, avg batch time: 0.5129, average train loss: 2.9210
[09/26 03:59:41 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1694, average loss: 3.0569
[09/26 03:59:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 17.00	top5: 50.00	
[09/26 03:59:41 visual_prompt]: Best epoch 7: best metric: 0.170
[09/26 03:59:41 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 03:59:48 visual_prompt]: Epoch 8 / 100: avg data time: 6.25e-02, avg batch time: 0.5098, average train loss: 2.3028
[09/26 03:59:49 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1687, average loss: 2.9665
[09/26 03:59:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 25.50	top5: 57.50	
[09/26 03:59:49 visual_prompt]: Best epoch 8: best metric: 0.255
[09/26 03:59:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 03:59:56 visual_prompt]: Epoch 9 / 100: avg data time: 5.35e-02, avg batch time: 0.5018, average train loss: 1.6728
[09/26 03:59:58 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1693, average loss: 2.5026
[09/26 03:59:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 34.50	top5: 66.50	
[09/26 03:59:58 visual_prompt]: Best epoch 9: best metric: 0.345
[09/26 03:59:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 04:00:04 visual_prompt]: Epoch 10 / 100: avg data time: 4.23e-02, avg batch time: 0.4925, average train loss: 1.0819
[09/26 04:00:06 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1689, average loss: 2.1870
[09/26 04:00:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 39.00	top5: 73.50	
[09/26 04:00:06 visual_prompt]: Best epoch 10: best metric: 0.390
[09/26 04:00:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 04:00:13 visual_prompt]: Epoch 11 / 100: avg data time: 5.56e-02, avg batch time: 0.5030, average train loss: 0.5808
[09/26 04:00:14 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1693, average loss: 1.9676
[09/26 04:00:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 81.00	
[09/26 04:00:14 visual_prompt]: Best epoch 11: best metric: 0.470
[09/26 04:00:14 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 04:00:21 visual_prompt]: Epoch 12 / 100: avg data time: 5.71e-02, avg batch time: 0.5050, average train loss: 0.2876
[09/26 04:00:23 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.1689, average loss: 1.9315
[09/26 04:00:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 82.50	
[09/26 04:00:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 04:00:30 visual_prompt]: Epoch 13 / 100: avg data time: 6.15e-02, avg batch time: 0.5104, average train loss: 0.1188
[09/26 04:00:31 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 1.9988
[09/26 04:00:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 80.50	
[09/26 04:00:31 visual_prompt]: Best epoch 13: best metric: 0.510
[09/26 04:00:31 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 04:00:38 visual_prompt]: Epoch 14 / 100: avg data time: 5.66e-02, avg batch time: 0.5051, average train loss: 0.0664
[09/26 04:00:40 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1693, average loss: 1.9641
[09/26 04:00:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 84.00	
[09/26 04:00:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 04:00:46 visual_prompt]: Epoch 15 / 100: avg data time: 4.43e-02, avg batch time: 0.4927, average train loss: 0.0407
[09/26 04:00:48 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1691, average loss: 2.0231
[09/26 04:00:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 82.50	
[09/26 04:00:48 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 04:00:55 visual_prompt]: Epoch 16 / 100: avg data time: 5.31e-02, avg batch time: 0.5016, average train loss: 0.0270
[09/26 04:00:56 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 2.0016
[09/26 04:00:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 83.50	
[09/26 04:00:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 04:01:03 visual_prompt]: Epoch 17 / 100: avg data time: 5.67e-02, avg batch time: 0.5048, average train loss: 0.0178
[09/26 04:01:05 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.1693, average loss: 1.9944
[09/26 04:01:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 82.00	
[09/26 04:01:05 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 04:01:12 visual_prompt]: Epoch 18 / 100: avg data time: 5.61e-02, avg batch time: 0.5042, average train loss: 0.0148
[09/26 04:01:13 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1692, average loss: 2.0029
[09/26 04:01:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.50	
[09/26 04:01:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 04:01:20 visual_prompt]: Epoch 19 / 100: avg data time: 4.49e-02, avg batch time: 0.4929, average train loss: 0.0130
[09/26 04:01:22 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1690, average loss: 2.0358
[09/26 04:01:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 04:01:22 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 04:01:28 visual_prompt]: Epoch 20 / 100: avg data time: 5.57e-02, avg batch time: 0.5051, average train loss: 0.0111
[09/26 04:01:30 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1691, average loss: 2.0243
[09/26 04:01:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 04:01:30 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 04:01:37 visual_prompt]: Epoch 21 / 100: avg data time: 6.11e-02, avg batch time: 0.5084, average train loss: 0.0098
[09/26 04:01:38 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1691, average loss: 2.0208
[09/26 04:01:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 04:01:38 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 04:01:45 visual_prompt]: Epoch 22 / 100: avg data time: 6.11e-02, avg batch time: 0.5094, average train loss: 0.0093
[09/26 04:01:47 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1691, average loss: 2.0323
[09/26 04:01:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 85.00	
[09/26 04:01:47 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 04:01:54 visual_prompt]: Epoch 23 / 100: avg data time: 4.62e-02, avg batch time: 0.4951, average train loss: 0.0085
[09/26 04:01:55 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1693, average loss: 2.0352
[09/26 04:01:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 85.00	
[09/26 04:01:55 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 04:02:02 visual_prompt]: Epoch 24 / 100: avg data time: 5.97e-02, avg batch time: 0.5074, average train loss: 0.0086
[09/26 04:02:04 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1690, average loss: 2.0272
[09/26 04:02:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 85.50	
[09/26 04:02:04 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 04:02:11 visual_prompt]: Epoch 25 / 100: avg data time: 5.73e-02, avg batch time: 0.5059, average train loss: 0.0082
[09/26 04:02:12 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1689, average loss: 2.0289
[09/26 04:02:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 85.50	
[09/26 04:02:12 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 04:02:19 visual_prompt]: Epoch 26 / 100: avg data time: 4.97e-02, avg batch time: 0.4992, average train loss: 0.0076
[09/26 04:02:20 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1692, average loss: 2.0400
[09/26 04:02:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 85.00	
[09/26 04:02:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 04:02:27 visual_prompt]: Epoch 27 / 100: avg data time: 5.79e-02, avg batch time: 0.5070, average train loss: 0.0072
[09/26 04:02:29 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1689, average loss: 2.0424
[09/26 04:02:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 85.00	
[09/26 04:02:29 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 04:02:36 visual_prompt]: Epoch 28 / 100: avg data time: 4.45e-02, avg batch time: 0.4935, average train loss: 0.0070
[09/26 04:02:37 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1692, average loss: 2.0438
[09/26 04:02:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 85.00	
[09/26 04:02:37 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 04:02:44 visual_prompt]: Epoch 29 / 100: avg data time: 5.81e-02, avg batch time: 0.5058, average train loss: 0.0065
[09/26 04:02:45 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1694, average loss: 2.0487
[09/26 04:02:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 85.00	
[09/26 04:02:45 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 04:02:52 visual_prompt]: Epoch 30 / 100: avg data time: 5.31e-02, avg batch time: 0.5027, average train loss: 0.0067
[09/26 04:02:54 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 2.0543
[09/26 04:02:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.50	
[09/26 04:02:54 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 04:03:01 visual_prompt]: Epoch 31 / 100: avg data time: 5.86e-02, avg batch time: 0.5082, average train loss: 0.0063
[09/26 04:03:02 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 2.0519
[09/26 04:03:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.50	
[09/26 04:03:02 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 04:03:09 visual_prompt]: Epoch 32 / 100: avg data time: 5.54e-02, avg batch time: 0.5045, average train loss: 0.0059
[09/26 04:03:11 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1690, average loss: 2.0514
[09/26 04:03:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.50	
[09/26 04:03:11 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 04:03:17 visual_prompt]: Epoch 33 / 100: avg data time: 5.72e-02, avg batch time: 0.5062, average train loss: 0.0062
[09/26 04:03:19 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 2.0523
[09/26 04:03:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.50	
[09/26 04:03:19 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 04:03:26 visual_prompt]: Epoch 34 / 100: avg data time: 6.06e-02, avg batch time: 0.5085, average train loss: 0.0058
[09/26 04:03:27 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1687, average loss: 2.0592
[09/26 04:03:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 85.00	
[09/26 04:03:27 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 04:03:34 visual_prompt]: Epoch 35 / 100: avg data time: 5.48e-02, avg batch time: 0.5035, average train loss: 0.0058
[09/26 04:03:36 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 2.0627
[09/26 04:03:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.00	
[09/26 04:03:36 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 04:03:43 visual_prompt]: Epoch 36 / 100: avg data time: 5.67e-02, avg batch time: 0.5045, average train loss: 0.0058
[09/26 04:03:44 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1687, average loss: 2.0611
[09/26 04:03:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 84.50	
[09/26 04:03:44 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 04:03:51 visual_prompt]: Epoch 37 / 100: avg data time: 4.34e-02, avg batch time: 0.4936, average train loss: 0.0055
[09/26 04:03:52 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 2.0649
[09/26 04:03:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.00	
[09/26 04:03:52 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 04:03:59 visual_prompt]: Epoch 38 / 100: avg data time: 5.60e-02, avg batch time: 0.5040, average train loss: 0.0055
[09/26 04:04:01 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 2.0730
[09/26 04:04:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 84.50	
[09/26 04:04:01 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 04:04:07 visual_prompt]: Epoch 39 / 100: avg data time: 5.22e-02, avg batch time: 0.5015, average train loss: 0.0054
[09/26 04:04:09 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1690, average loss: 2.0729
[09/26 04:04:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 84.50	
[09/26 04:04:09 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 04:04:16 visual_prompt]: Epoch 40 / 100: avg data time: 5.81e-02, avg batch time: 0.5069, average train loss: 0.0052
[09/26 04:04:17 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1690, average loss: 2.0663
[09/26 04:04:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 84.50	
[09/26 04:04:17 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 04:04:24 visual_prompt]: Epoch 41 / 100: avg data time: 5.74e-02, avg batch time: 0.5062, average train loss: 0.0053
[09/26 04:04:26 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1688, average loss: 2.0576
[09/26 04:04:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 84.00	
[09/26 04:04:26 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 04:04:33 visual_prompt]: Epoch 42 / 100: avg data time: 5.16e-02, avg batch time: 0.4998, average train loss: 0.0051
[09/26 04:04:34 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 2.0607
[09/26 04:04:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:04:34 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 04:04:41 visual_prompt]: Epoch 43 / 100: avg data time: 5.39e-02, avg batch time: 0.5024, average train loss: 0.0052
[09/26 04:04:42 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 2.0664
[09/26 04:04:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:04:42 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 04:04:49 visual_prompt]: Epoch 44 / 100: avg data time: 5.67e-02, avg batch time: 0.5057, average train loss: 0.0049
[09/26 04:04:51 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 2.0672
[09/26 04:04:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 84.00	
[09/26 04:04:51 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 04:04:58 visual_prompt]: Epoch 45 / 100: avg data time: 5.04e-02, avg batch time: 0.4995, average train loss: 0.0048
[09/26 04:04:59 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 2.0632
[09/26 04:04:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 84.50	
[09/26 04:04:59 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 04:05:06 visual_prompt]: Epoch 46 / 100: avg data time: 4.57e-02, avg batch time: 0.4954, average train loss: 0.0049
[09/26 04:05:07 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 2.0599
[09/26 04:05:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 84.50	
[09/26 04:05:07 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 04:05:14 visual_prompt]: Epoch 47 / 100: avg data time: 5.54e-02, avg batch time: 0.5033, average train loss: 0.0048
[09/26 04:05:16 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 2.0551
[09/26 04:05:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.50	
[09/26 04:05:16 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 04:05:23 visual_prompt]: Epoch 48 / 100: avg data time: 5.82e-02, avg batch time: 0.5062, average train loss: 0.0047
[09/26 04:05:24 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 2.0589
[09/26 04:05:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.00	
[09/26 04:05:24 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 04:05:31 visual_prompt]: Epoch 49 / 100: avg data time: 6.51e-02, avg batch time: 0.5128, average train loss: 0.0048
[09/26 04:05:33 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1690, average loss: 2.0597
[09/26 04:05:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.50	
[09/26 04:05:33 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 04:05:40 visual_prompt]: Epoch 50 / 100: avg data time: 5.91e-02, avg batch time: 0.5079, average train loss: 0.0047
[09/26 04:05:41 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1693, average loss: 2.0741
[09/26 04:05:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.00	
[09/26 04:05:41 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 04:05:48 visual_prompt]: Epoch 51 / 100: avg data time: 5.22e-02, avg batch time: 0.5002, average train loss: 0.0047
[09/26 04:05:49 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1695, average loss: 2.0725
[09/26 04:05:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.00	
[09/26 04:05:49 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 04:05:56 visual_prompt]: Epoch 52 / 100: avg data time: 5.62e-02, avg batch time: 0.5059, average train loss: 0.0045
[09/26 04:05:58 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1692, average loss: 2.0665
[09/26 04:05:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 83.00	
[09/26 04:05:58 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 04:06:05 visual_prompt]: Epoch 53 / 100: avg data time: 6.03e-02, avg batch time: 0.5081, average train loss: 0.0048
[09/26 04:06:06 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1689, average loss: 2.0628
[09/26 04:06:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 83.50	
[09/26 04:06:06 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 04:06:13 visual_prompt]: Epoch 54 / 100: avg data time: 5.49e-02, avg batch time: 0.5028, average train loss: 0.0043
[09/26 04:06:15 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1687, average loss: 2.0602
[09/26 04:06:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.50	
[09/26 04:06:15 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 04:06:22 visual_prompt]: Epoch 55 / 100: avg data time: 5.16e-02, avg batch time: 0.5004, average train loss: 0.0046
[09/26 04:06:23 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1692, average loss: 2.0589
[09/26 04:06:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.00	
[09/26 04:06:23 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 04:06:30 visual_prompt]: Epoch 56 / 100: avg data time: 4.54e-02, avg batch time: 0.4958, average train loss: 0.0045
[09/26 04:06:31 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1691, average loss: 2.0649
[09/26 04:06:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.50	
[09/26 04:06:31 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 04:06:38 visual_prompt]: Epoch 57 / 100: avg data time: 4.26e-02, avg batch time: 0.4920, average train loss: 0.0045
[09/26 04:06:40 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1690, average loss: 2.0620
[09/26 04:06:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.50	
[09/26 04:06:40 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 04:06:47 visual_prompt]: Epoch 58 / 100: avg data time: 5.58e-02, avg batch time: 0.5045, average train loss: 0.0044
[09/26 04:06:48 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1691, average loss: 2.0625
[09/26 04:06:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 83.50	
[09/26 04:06:48 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 04:06:55 visual_prompt]: Epoch 59 / 100: avg data time: 5.61e-02, avg batch time: 0.5040, average train loss: 0.0044
[09/26 04:06:57 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 2.0654
[09/26 04:06:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 83.50	
[09/26 04:06:57 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 04:07:03 visual_prompt]: Epoch 60 / 100: avg data time: 5.66e-02, avg batch time: 0.5046, average train loss: 0.0044
[09/26 04:07:05 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1689, average loss: 2.0607
[09/26 04:07:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 84.00	
[09/26 04:07:05 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 04:07:12 visual_prompt]: Epoch 61 / 100: avg data time: 5.49e-02, avg batch time: 0.5035, average train loss: 0.0043
[09/26 04:07:13 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1691, average loss: 2.0607
[09/26 04:07:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.00	
[09/26 04:07:13 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 04:07:20 visual_prompt]: Epoch 62 / 100: avg data time: 5.46e-02, avg batch time: 0.5060, average train loss: 0.0044
[09/26 04:07:22 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1695, average loss: 2.0554
[09/26 04:07:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.00	
[09/26 04:07:22 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 04:07:29 visual_prompt]: Epoch 63 / 100: avg data time: 6.00e-02, avg batch time: 0.5080, average train loss: 0.0041
[09/26 04:07:30 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 2.0565
[09/26 04:07:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.00	
[09/26 04:07:30 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 04:07:37 visual_prompt]: Epoch 64 / 100: avg data time: 5.15e-02, avg batch time: 0.5004, average train loss: 0.0042
[09/26 04:07:39 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1693, average loss: 2.0592
[09/26 04:07:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.00	
[09/26 04:07:39 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 04:07:45 visual_prompt]: Epoch 65 / 100: avg data time: 6.11e-02, avg batch time: 0.5092, average train loss: 0.0043
[09/26 04:07:47 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1690, average loss: 2.0624
[09/26 04:07:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.50	
[09/26 04:07:47 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 04:07:54 visual_prompt]: Epoch 66 / 100: avg data time: 6.08e-02, avg batch time: 0.5088, average train loss: 0.0042
[09/26 04:07:56 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 2.0654
[09/26 04:07:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 83.50	
[09/26 04:07:56 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 04:08:02 visual_prompt]: Epoch 67 / 100: avg data time: 4.63e-02, avg batch time: 0.4949, average train loss: 0.0043
[09/26 04:08:04 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1689, average loss: 2.0652
[09/26 04:08:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.00	
[09/26 04:08:04 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 04:08:10 visual_prompt]: Epoch 68 / 100: avg data time: 4.38e-02, avg batch time: 0.4933, average train loss: 0.0042
[09/26 04:08:12 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1687, average loss: 2.0678
[09/26 04:08:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.50	
[09/26 04:08:12 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 04:08:19 visual_prompt]: Epoch 69 / 100: avg data time: 4.69e-02, avg batch time: 0.4972, average train loss: 0.0043
[09/26 04:08:20 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1691, average loss: 2.0666
[09/26 04:08:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 82.50	
[09/26 04:08:20 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 04:08:27 visual_prompt]: Epoch 70 / 100: avg data time: 4.66e-02, avg batch time: 0.4958, average train loss: 0.0042
[09/26 04:08:29 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.1694, average loss: 2.0691
[09/26 04:08:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.00	
[09/26 04:08:29 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 04:08:35 visual_prompt]: Epoch 71 / 100: avg data time: 4.35e-02, avg batch time: 0.4951, average train loss: 0.0041
[09/26 04:08:37 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 2.0699
[09/26 04:08:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.00	
[09/26 04:08:37 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 04:08:44 visual_prompt]: Epoch 72 / 100: avg data time: 5.19e-02, avg batch time: 0.4998, average train loss: 0.0041
[09/26 04:08:45 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1694, average loss: 2.0674
[09/26 04:08:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.00	
[09/26 04:08:45 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 04:08:52 visual_prompt]: Epoch 73 / 100: avg data time: 6.56e-02, avg batch time: 0.5135, average train loss: 0.0039
[09/26 04:08:54 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1690, average loss: 2.0624
[09/26 04:08:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.50	
[09/26 04:08:54 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 04:09:00 visual_prompt]: Epoch 74 / 100: avg data time: 4.97e-02, avg batch time: 0.4983, average train loss: 0.0039
[09/26 04:09:02 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1695, average loss: 2.0626
[09/26 04:09:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.50	
[09/26 04:09:02 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 04:09:09 visual_prompt]: Epoch 75 / 100: avg data time: 4.36e-02, avg batch time: 0.4928, average train loss: 0.0040
[09/26 04:09:10 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1691, average loss: 2.0639
[09/26 04:09:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.50	
[09/26 04:09:10 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 04:09:17 visual_prompt]: Epoch 76 / 100: avg data time: 4.29e-02, avg batch time: 0.4917, average train loss: 0.0040
[09/26 04:09:18 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1689, average loss: 2.0639
[09/26 04:09:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 83.00	
[09/26 04:09:18 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 04:09:25 visual_prompt]: Epoch 77 / 100: avg data time: 4.72e-02, avg batch time: 0.4973, average train loss: 0.0039
[09/26 04:09:27 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1694, average loss: 2.0637
[09/26 04:09:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.00	
[09/26 04:09:27 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 04:09:34 visual_prompt]: Epoch 78 / 100: avg data time: 5.54e-02, avg batch time: 0.5042, average train loss: 0.0040
[09/26 04:09:35 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 2.0605
[09/26 04:09:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:09:35 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 04:09:42 visual_prompt]: Epoch 79 / 100: avg data time: 5.73e-02, avg batch time: 0.5062, average train loss: 0.0041
[09/26 04:09:44 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1690, average loss: 2.0597
[09/26 04:09:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 84.00	
[09/26 04:09:44 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 04:09:51 visual_prompt]: Epoch 80 / 100: avg data time: 5.96e-02, avg batch time: 0.5085, average train loss: 0.0041
[09/26 04:09:52 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 2.0595
[09/26 04:09:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.00	
[09/26 04:09:52 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 04:09:59 visual_prompt]: Epoch 81 / 100: avg data time: 4.55e-02, avg batch time: 0.4952, average train loss: 0.0041
[09/26 04:10:00 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1688, average loss: 2.0611
[09/26 04:10:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:10:00 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 04:10:07 visual_prompt]: Epoch 82 / 100: avg data time: 5.87e-02, avg batch time: 0.5072, average train loss: 0.0040
[09/26 04:10:09 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1690, average loss: 2.0599
[09/26 04:10:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:10:09 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 04:10:15 visual_prompt]: Epoch 83 / 100: avg data time: 4.27e-02, avg batch time: 0.4904, average train loss: 0.0041
[09/26 04:10:17 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 2.0600
[09/26 04:10:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.00	
[09/26 04:10:17 visual_prompt]: Best epoch 83: best metric: 0.515
[09/26 04:10:17 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 04:10:24 visual_prompt]: Epoch 84 / 100: avg data time: 5.91e-02, avg batch time: 0.5083, average train loss: 0.0041
[09/26 04:10:26 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1690, average loss: 2.0602
[09/26 04:10:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.00	
[09/26 04:10:26 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 04:10:32 visual_prompt]: Epoch 85 / 100: avg data time: 5.54e-02, avg batch time: 0.5032, average train loss: 0.0041
[09/26 04:10:34 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1691, average loss: 2.0600
[09/26 04:10:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:10:34 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 04:10:41 visual_prompt]: Epoch 86 / 100: avg data time: 4.66e-02, avg batch time: 0.4958, average train loss: 0.0040
[09/26 04:10:42 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 2.0597
[09/26 04:10:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:10:42 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 04:10:49 visual_prompt]: Epoch 87 / 100: avg data time: 6.13e-02, avg batch time: 0.5096, average train loss: 0.0040
[09/26 04:10:51 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1692, average loss: 2.0589
[09/26 04:10:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:10:51 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 04:10:58 visual_prompt]: Epoch 88 / 100: avg data time: 5.34e-02, avg batch time: 0.5028, average train loss: 0.0041
[09/26 04:10:59 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 2.0589
[09/26 04:10:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:10:59 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 04:11:06 visual_prompt]: Epoch 89 / 100: avg data time: 5.52e-02, avg batch time: 0.5038, average train loss: 0.0039
[09/26 04:11:07 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1692, average loss: 2.0589
[09/26 04:11:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:11:07 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 04:11:14 visual_prompt]: Epoch 90 / 100: avg data time: 4.53e-02, avg batch time: 0.4933, average train loss: 0.0040
[09/26 04:11:16 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.1697, average loss: 2.0575
[09/26 04:11:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:11:16 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 04:11:23 visual_prompt]: Epoch 91 / 100: avg data time: 6.40e-02, avg batch time: 0.5122, average train loss: 0.0041
[09/26 04:11:24 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1693, average loss: 2.0577
[09/26 04:11:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:11:24 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 04:11:31 visual_prompt]: Epoch 92 / 100: avg data time: 5.83e-02, avg batch time: 0.5069, average train loss: 0.0040
[09/26 04:11:33 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1694, average loss: 2.0577
[09/26 04:11:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:11:33 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 04:11:40 visual_prompt]: Epoch 93 / 100: avg data time: 5.90e-02, avg batch time: 0.5092, average train loss: 0.0039
[09/26 04:11:41 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1693, average loss: 2.0576
[09/26 04:11:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.50	
[09/26 04:11:41 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 04:11:48 visual_prompt]: Epoch 94 / 100: avg data time: 5.47e-02, avg batch time: 0.5027, average train loss: 0.0039
[09/26 04:11:49 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.1693, average loss: 2.0578
[09/26 04:11:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.50	
[09/26 04:11:49 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 04:11:56 visual_prompt]: Epoch 95 / 100: avg data time: 5.41e-02, avg batch time: 0.5032, average train loss: 0.0039
[09/26 04:11:58 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1692, average loss: 2.0574
[09/26 04:11:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.50	
[09/26 04:11:58 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 04:12:05 visual_prompt]: Epoch 96 / 100: avg data time: 5.65e-02, avg batch time: 0.5043, average train loss: 0.0040
[09/26 04:12:06 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1693, average loss: 2.0573
[09/26 04:12:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.50	
[09/26 04:12:06 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 04:12:13 visual_prompt]: Epoch 97 / 100: avg data time: 5.52e-02, avg batch time: 0.5039, average train loss: 0.0042
[09/26 04:12:14 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1690, average loss: 2.0573
[09/26 04:12:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.50	
[09/26 04:12:14 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 04:12:21 visual_prompt]: Epoch 98 / 100: avg data time: 5.82e-02, avg batch time: 0.5068, average train loss: 0.0040
[09/26 04:12:23 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1691, average loss: 2.0573
[09/26 04:12:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.50	
[09/26 04:12:23 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 04:12:30 visual_prompt]: Epoch 99 / 100: avg data time: 5.39e-02, avg batch time: 0.5031, average train loss: 0.0038
[09/26 04:12:31 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1693, average loss: 2.0573
[09/26 04:12:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.50	
[09/26 04:12:31 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 04:12:38 visual_prompt]: Epoch 100 / 100: avg data time: 4.30e-02, avg batch time: 0.4917, average train loss: 0.0041
[09/26 04:12:40 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 2.0573
[09/26 04:12:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.50	
[09/26 04:12:40 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 04:12:40 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 04:12:40 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 04:12:40 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 04:12:40 visual_prompt]: Training with config:
[09/26 04:12:40 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 04:12:40 visual_prompt]: Loading training data...
[09/26 04:12:40 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 04:12:42 visual_prompt]: Number of images: 800
[09/26 04:12:42 visual_prompt]: Number of classes: 47 / 47
[09/26 04:12:42 visual_prompt]: Loading validation data...
[09/26 04:12:42 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 04:12:42 visual_prompt]: Number of images: 200
[09/26 04:12:42 visual_prompt]: Number of classes: 47 / 47
[09/26 04:12:42 visual_prompt]: Constructing models...
[09/26 04:12:45 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 04:12:45 visual_prompt]: tuned percent:0.576
[09/26 04:12:45 visual_prompt]: Device used for model: 0
[09/26 04:12:45 visual_prompt]: Setting up Evaluator...
[09/26 04:12:45 visual_prompt]: Setting up Trainer...
[09/26 04:12:45 visual_prompt]: 	Setting up the optimizer...
[09/26 04:12:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 04:12:52 visual_prompt]: Epoch 1 / 100: avg data time: 5.72e-02, avg batch time: 0.5047, average train loss: 3.9308
[09/26 04:12:53 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1684, average loss: 3.9045
[09/26 04:12:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 04:12:53 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 04:12:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 04:13:00 visual_prompt]: Epoch 2 / 100: avg data time: 4.98e-02, avg batch time: 0.4966, average train loss: 3.8927
[09/26 04:13:01 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1687, average loss: 3.9108
[09/26 04:13:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/26 04:13:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 04:13:08 visual_prompt]: Epoch 3 / 100: avg data time: 5.44e-02, avg batch time: 0.5024, average train loss: 3.8188
[09/26 04:13:10 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1686, average loss: 3.8835
[09/26 04:13:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.00	
[09/26 04:13:10 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 04:13:17 visual_prompt]: Epoch 4 / 100: avg data time: 5.35e-02, avg batch time: 0.5012, average train loss: 3.7362
[09/26 04:13:18 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1690, average loss: 3.7345
[09/26 04:13:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 19.00	
[09/26 04:13:18 visual_prompt]: Best epoch 4: best metric: 0.045
[09/26 04:13:18 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 04:13:25 visual_prompt]: Epoch 5 / 100: avg data time: 5.91e-02, avg batch time: 0.5074, average train loss: 3.7105
[09/26 04:13:27 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1692, average loss: 3.8452
[09/26 04:13:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 15.00	
[09/26 04:13:27 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 04:13:34 visual_prompt]: Epoch 6 / 100: avg data time: 6.01e-02, avg batch time: 0.5071, average train loss: 3.7825
[09/26 04:13:35 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1689, average loss: 3.7080
[09/26 04:13:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 23.50	
[09/26 04:13:35 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 04:13:42 visual_prompt]: Epoch 7 / 100: avg data time: 4.42e-02, avg batch time: 0.4932, average train loss: 3.4568
[09/26 04:13:44 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1691, average loss: 3.4908
[09/26 04:13:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 33.00	
[09/26 04:13:44 visual_prompt]: Best epoch 7: best metric: 0.095
[09/26 04:13:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 04:13:50 visual_prompt]: Epoch 8 / 100: avg data time: 5.20e-02, avg batch time: 0.5002, average train loss: 3.1400
[09/26 04:13:52 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.1691, average loss: 3.3096
[09/26 04:13:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.50	top5: 40.00	
[09/26 04:13:52 visual_prompt]: Best epoch 8: best metric: 0.105
[09/26 04:13:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 04:13:59 visual_prompt]: Epoch 9 / 100: avg data time: 4.75e-02, avg batch time: 0.4967, average train loss: 2.6180
[09/26 04:14:00 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1694, average loss: 2.7080
[09/26 04:14:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 28.00	top5: 58.50	
[09/26 04:14:00 visual_prompt]: Best epoch 9: best metric: 0.280
[09/26 04:14:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 04:14:07 visual_prompt]: Epoch 10 / 100: avg data time: 5.92e-02, avg batch time: 0.5065, average train loss: 1.9641
[09/26 04:14:09 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1690, average loss: 2.3932
[09/26 04:14:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.50	top5: 71.00	
[09/26 04:14:09 visual_prompt]: Best epoch 10: best metric: 0.385
[09/26 04:14:09 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 04:14:16 visual_prompt]: Epoch 11 / 100: avg data time: 5.49e-02, avg batch time: 0.5042, average train loss: 1.3129
[09/26 04:14:17 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1691, average loss: 2.1036
[09/26 04:14:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 77.00	
[09/26 04:14:17 visual_prompt]: Best epoch 11: best metric: 0.445
[09/26 04:14:17 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 04:14:24 visual_prompt]: Epoch 12 / 100: avg data time: 6.36e-02, avg batch time: 0.5116, average train loss: 0.6797
[09/26 04:14:26 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1688, average loss: 1.8566
[09/26 04:14:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 81.50	
[09/26 04:14:26 visual_prompt]: Best epoch 12: best metric: 0.505
[09/26 04:14:26 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 04:14:32 visual_prompt]: Epoch 13 / 100: avg data time: 4.21e-02, avg batch time: 0.4908, average train loss: 0.3282
[09/26 04:14:34 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 1.9000
[09/26 04:14:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 84.00	
[09/26 04:14:34 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 04:14:41 visual_prompt]: Epoch 14 / 100: avg data time: 4.83e-02, avg batch time: 0.4963, average train loss: 0.1554
[09/26 04:14:42 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1693, average loss: 1.8896
[09/26 04:14:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 04:14:42 visual_prompt]: Best epoch 14: best metric: 0.535
[09/26 04:14:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 04:14:49 visual_prompt]: Epoch 15 / 100: avg data time: 4.28e-02, avg batch time: 0.4932, average train loss: 0.0680
[09/26 04:14:50 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1687, average loss: 1.9373
[09/26 04:14:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 86.50	
[09/26 04:14:50 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 04:14:57 visual_prompt]: Epoch 16 / 100: avg data time: 5.96e-02, avg batch time: 0.5070, average train loss: 0.0363
[09/26 04:14:59 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 1.8584
[09/26 04:14:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 87.00	
[09/26 04:14:59 visual_prompt]: Best epoch 16: best metric: 0.550
[09/26 04:14:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 04:15:06 visual_prompt]: Epoch 17 / 100: avg data time: 4.72e-02, avg batch time: 0.4965, average train loss: 0.0235
[09/26 04:15:07 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 1.9203
[09/26 04:15:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 04:15:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 04:15:14 visual_prompt]: Epoch 18 / 100: avg data time: 5.64e-02, avg batch time: 0.5053, average train loss: 0.0177
[09/26 04:15:16 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1692, average loss: 1.9674
[09/26 04:15:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 85.00	
[09/26 04:15:16 visual_prompt]: Best epoch 18: best metric: 0.560
[09/26 04:15:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 04:15:23 visual_prompt]: Epoch 19 / 100: avg data time: 5.29e-02, avg batch time: 0.5019, average train loss: 0.0135
[09/26 04:15:24 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 1.9838
[09/26 04:15:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 04:15:24 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 04:15:31 visual_prompt]: Epoch 20 / 100: avg data time: 5.62e-02, avg batch time: 0.5039, average train loss: 0.0107
[09/26 04:15:33 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1691, average loss: 1.9972
[09/26 04:15:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 04:15:33 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 04:15:39 visual_prompt]: Epoch 21 / 100: avg data time: 5.20e-02, avg batch time: 0.5000, average train loss: 0.0102
[09/26 04:15:41 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 2.0179
[09/26 04:15:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.00	
[09/26 04:15:41 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 04:15:48 visual_prompt]: Epoch 22 / 100: avg data time: 4.48e-02, avg batch time: 0.4949, average train loss: 0.0089
[09/26 04:15:49 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.1687, average loss: 2.0158
[09/26 04:15:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.50	
[09/26 04:15:49 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 04:15:56 visual_prompt]: Epoch 23 / 100: avg data time: 5.85e-02, avg batch time: 0.5067, average train loss: 0.0083
[09/26 04:15:58 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 2.0253
[09/26 04:15:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 04:15:58 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 04:16:04 visual_prompt]: Epoch 24 / 100: avg data time: 4.81e-02, avg batch time: 0.4974, average train loss: 0.0075
[09/26 04:16:06 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1692, average loss: 2.0445
[09/26 04:16:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 86.00	
[09/26 04:16:06 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 04:16:13 visual_prompt]: Epoch 25 / 100: avg data time: 5.45e-02, avg batch time: 0.5030, average train loss: 0.0069
[09/26 04:16:14 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1687, average loss: 2.0670
[09/26 04:16:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.50	
[09/26 04:16:14 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 04:16:21 visual_prompt]: Epoch 26 / 100: avg data time: 4.28e-02, avg batch time: 0.4933, average train loss: 0.0063
[09/26 04:16:23 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 2.0682
[09/26 04:16:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.50	
[09/26 04:16:23 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 04:16:29 visual_prompt]: Epoch 27 / 100: avg data time: 5.82e-02, avg batch time: 0.5064, average train loss: 0.0057
[09/26 04:16:31 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1694, average loss: 2.0758
[09/26 04:16:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 84.50	
[09/26 04:16:31 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 04:16:38 visual_prompt]: Epoch 28 / 100: avg data time: 4.34e-02, avg batch time: 0.4910, average train loss: 0.0059
[09/26 04:16:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1690, average loss: 2.0764
[09/26 04:16:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.00	
[09/26 04:16:39 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 04:16:46 visual_prompt]: Epoch 29 / 100: avg data time: 4.49e-02, avg batch time: 0.4951, average train loss: 0.0050
[09/26 04:16:48 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 2.0898
[09/26 04:16:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 04:16:48 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 04:16:54 visual_prompt]: Epoch 30 / 100: avg data time: 5.52e-02, avg batch time: 0.5040, average train loss: 0.0050
[09/26 04:16:56 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1690, average loss: 2.1027
[09/26 04:16:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 85.00	
[09/26 04:16:56 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 04:17:03 visual_prompt]: Epoch 31 / 100: avg data time: 5.19e-02, avg batch time: 0.5003, average train loss: 0.0046
[09/26 04:17:04 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1691, average loss: 2.1086
[09/26 04:17:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 84.50	
[09/26 04:17:04 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 04:17:11 visual_prompt]: Epoch 32 / 100: avg data time: 5.20e-02, avg batch time: 0.4997, average train loss: 0.0043
[09/26 04:17:13 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1693, average loss: 2.1090
[09/26 04:17:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.00	
[09/26 04:17:13 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 04:17:19 visual_prompt]: Epoch 33 / 100: avg data time: 5.68e-02, avg batch time: 0.5049, average train loss: 0.0045
[09/26 04:17:21 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 2.1105
[09/26 04:17:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 84.50	
[09/26 04:17:21 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 04:17:28 visual_prompt]: Epoch 34 / 100: avg data time: 6.03e-02, avg batch time: 0.5089, average train loss: 0.0041
[09/26 04:17:29 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1690, average loss: 2.1214
[09/26 04:17:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 85.00	
[09/26 04:17:29 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 04:17:36 visual_prompt]: Epoch 35 / 100: avg data time: 5.91e-02, avg batch time: 0.5078, average train loss: 0.0039
[09/26 04:17:38 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 2.1253
[09/26 04:17:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 85.50	
[09/26 04:17:38 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 04:17:45 visual_prompt]: Epoch 36 / 100: avg data time: 5.67e-02, avg batch time: 0.5055, average train loss: 0.0039
[09/26 04:17:46 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1692, average loss: 2.1346
[09/26 04:17:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 04:17:46 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 04:17:53 visual_prompt]: Epoch 37 / 100: avg data time: 4.23e-02, avg batch time: 0.4928, average train loss: 0.0040
[09/26 04:17:55 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 2.1431
[09/26 04:17:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.00	
[09/26 04:17:55 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 04:18:01 visual_prompt]: Epoch 38 / 100: avg data time: 4.42e-02, avg batch time: 0.4946, average train loss: 0.0039
[09/26 04:18:03 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 2.1510
[09/26 04:18:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 04:18:03 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 04:18:09 visual_prompt]: Epoch 39 / 100: avg data time: 4.48e-02, avg batch time: 0.4941, average train loss: 0.0036
[09/26 04:18:11 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 2.1620
[09/26 04:18:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 04:18:11 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 04:18:18 visual_prompt]: Epoch 40 / 100: avg data time: 4.52e-02, avg batch time: 0.4944, average train loss: 0.0034
[09/26 04:18:19 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1692, average loss: 2.1626
[09/26 04:18:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 85.00	
[09/26 04:18:19 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 04:18:26 visual_prompt]: Epoch 41 / 100: avg data time: 4.48e-02, avg batch time: 0.4953, average train loss: 0.0033
[09/26 04:18:27 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1695, average loss: 2.1713
[09/26 04:18:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.00	
[09/26 04:18:27 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 04:18:34 visual_prompt]: Epoch 42 / 100: avg data time: 4.48e-02, avg batch time: 0.4954, average train loss: 0.0031
[09/26 04:18:36 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 2.1778
[09/26 04:18:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.00	
[09/26 04:18:36 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 04:18:42 visual_prompt]: Epoch 43 / 100: avg data time: 4.53e-02, avg batch time: 0.4960, average train loss: 0.0033
[09/26 04:18:44 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 2.1802
[09/26 04:18:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:18:44 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 04:18:51 visual_prompt]: Epoch 44 / 100: avg data time: 6.34e-02, avg batch time: 0.5112, average train loss: 0.0030
[09/26 04:18:52 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 2.1900
[09/26 04:18:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 84.00	
[09/26 04:18:52 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 04:18:59 visual_prompt]: Epoch 45 / 100: avg data time: 5.92e-02, avg batch time: 0.5077, average train loss: 0.0030
[09/26 04:19:01 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1695, average loss: 2.1861
[09/26 04:19:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 84.00	
[09/26 04:19:01 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 04:19:07 visual_prompt]: Epoch 46 / 100: avg data time: 4.72e-02, avg batch time: 0.4978, average train loss: 0.0029
[09/26 04:19:09 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1686, average loss: 2.1848
[09/26 04:19:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.00	
[09/26 04:19:09 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 04:19:16 visual_prompt]: Epoch 47 / 100: avg data time: 5.80e-02, avg batch time: 0.5069, average train loss: 0.0027
[09/26 04:19:17 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1695, average loss: 2.1896
[09/26 04:19:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.00	
[09/26 04:19:17 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 04:19:24 visual_prompt]: Epoch 48 / 100: avg data time: 5.40e-02, avg batch time: 0.5022, average train loss: 0.0028
[09/26 04:19:26 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1691, average loss: 2.1918
[09/26 04:19:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 04:19:26 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 04:19:32 visual_prompt]: Epoch 49 / 100: avg data time: 5.56e-02, avg batch time: 0.5044, average train loss: 0.0027
[09/26 04:19:34 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1694, average loss: 2.1926
[09/26 04:19:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 04:19:34 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 04:19:41 visual_prompt]: Epoch 50 / 100: avg data time: 5.32e-02, avg batch time: 0.5021, average train loss: 0.0026
[09/26 04:19:42 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1700, average loss: 2.1967
[09/26 04:19:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.50	
[09/26 04:19:42 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 04:19:49 visual_prompt]: Epoch 51 / 100: avg data time: 6.23e-02, avg batch time: 0.5104, average train loss: 0.0026
[09/26 04:19:51 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1697, average loss: 2.2081
[09/26 04:19:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.00	
[09/26 04:19:51 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 04:19:58 visual_prompt]: Epoch 52 / 100: avg data time: 5.11e-02, avg batch time: 0.5004, average train loss: 0.0025
[09/26 04:19:59 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1693, average loss: 2.2118
[09/26 04:19:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 84.00	
[09/26 04:19:59 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 04:20:06 visual_prompt]: Epoch 53 / 100: avg data time: 5.47e-02, avg batch time: 0.5036, average train loss: 0.0023
[09/26 04:20:08 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 2.2097
[09/26 04:20:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 83.50	
[09/26 04:20:08 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 04:20:14 visual_prompt]: Epoch 54 / 100: avg data time: 4.34e-02, avg batch time: 0.4931, average train loss: 0.0024
[09/26 04:20:16 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1692, average loss: 2.2069
[09/26 04:20:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 83.50	
[09/26 04:20:16 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 04:20:23 visual_prompt]: Epoch 55 / 100: avg data time: 4.45e-02, avg batch time: 0.4938, average train loss: 0.0022
[09/26 04:20:24 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1691, average loss: 2.2055
[09/26 04:20:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 84.00	
[09/26 04:20:24 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 04:20:31 visual_prompt]: Epoch 56 / 100: avg data time: 4.96e-02, avg batch time: 0.4991, average train loss: 0.0023
[09/26 04:20:32 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 2.2074
[09/26 04:20:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 84.00	
[09/26 04:20:32 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 04:20:39 visual_prompt]: Epoch 57 / 100: avg data time: 5.81e-02, avg batch time: 0.5062, average train loss: 0.0023
[09/26 04:20:41 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1689, average loss: 2.2128
[09/26 04:20:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 84.00	
[09/26 04:20:41 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 04:20:48 visual_prompt]: Epoch 58 / 100: avg data time: 6.20e-02, avg batch time: 0.5096, average train loss: 0.0022
[09/26 04:20:49 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1689, average loss: 2.2172
[09/26 04:20:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 83.50	
[09/26 04:20:49 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 04:20:56 visual_prompt]: Epoch 59 / 100: avg data time: 5.29e-02, avg batch time: 0.5018, average train loss: 0.0023
[09/26 04:20:58 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1693, average loss: 2.2198
[09/26 04:20:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 83.50	
[09/26 04:20:58 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 04:21:05 visual_prompt]: Epoch 60 / 100: avg data time: 6.04e-02, avg batch time: 0.5091, average train loss: 0.0023
[09/26 04:21:06 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1694, average loss: 2.2240
[09/26 04:21:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.00	
[09/26 04:21:06 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 04:21:13 visual_prompt]: Epoch 61 / 100: avg data time: 5.90e-02, avg batch time: 0.5075, average train loss: 0.0021
[09/26 04:21:15 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 2.2251
[09/26 04:21:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 83.00	
[09/26 04:21:15 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 04:21:21 visual_prompt]: Epoch 62 / 100: avg data time: 4.78e-02, avg batch time: 0.4982, average train loss: 0.0021
[09/26 04:21:23 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1693, average loss: 2.2226
[09/26 04:21:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.00	
[09/26 04:21:23 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 04:21:30 visual_prompt]: Epoch 63 / 100: avg data time: 5.14e-02, avg batch time: 0.5007, average train loss: 0.0022
[09/26 04:21:31 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1693, average loss: 2.2256
[09/26 04:21:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.00	
[09/26 04:21:31 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 04:21:38 visual_prompt]: Epoch 64 / 100: avg data time: 5.34e-02, avg batch time: 0.5011, average train loss: 0.0021
[09/26 04:21:40 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 2.2278
[09/26 04:21:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.00	
[09/26 04:21:40 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 04:21:47 visual_prompt]: Epoch 65 / 100: avg data time: 5.70e-02, avg batch time: 0.5047, average train loss: 0.0020
[09/26 04:21:48 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1693, average loss: 2.2306
[09/26 04:21:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.00	
[09/26 04:21:48 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 04:21:55 visual_prompt]: Epoch 66 / 100: avg data time: 5.73e-02, avg batch time: 0.5054, average train loss: 0.0020
[09/26 04:21:56 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1693, average loss: 2.2326
[09/26 04:21:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.00	
[09/26 04:21:56 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 04:22:03 visual_prompt]: Epoch 67 / 100: avg data time: 6.41e-02, avg batch time: 0.5124, average train loss: 0.0019
[09/26 04:22:05 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1695, average loss: 2.2338
[09/26 04:22:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:22:05 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 04:22:12 visual_prompt]: Epoch 68 / 100: avg data time: 6.01e-02, avg batch time: 0.5082, average train loss: 0.0020
[09/26 04:22:13 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 2.2342
[09/26 04:22:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 83.50	
[09/26 04:22:13 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 04:22:20 visual_prompt]: Epoch 69 / 100: avg data time: 4.60e-02, avg batch time: 0.4953, average train loss: 0.0021
[09/26 04:22:22 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 2.2362
[09/26 04:22:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 83.50	
[09/26 04:22:22 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 04:22:29 visual_prompt]: Epoch 70 / 100: avg data time: 5.76e-02, avg batch time: 0.5073, average train loss: 0.0020
[09/26 04:22:30 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1692, average loss: 2.2350
[09/26 04:22:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:22:30 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 04:22:37 visual_prompt]: Epoch 71 / 100: avg data time: 5.26e-02, avg batch time: 0.5015, average train loss: 0.0020
[09/26 04:22:38 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 2.2358
[09/26 04:22:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:22:38 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 04:22:45 visual_prompt]: Epoch 72 / 100: avg data time: 4.54e-02, avg batch time: 0.4949, average train loss: 0.0019
[09/26 04:22:47 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1694, average loss: 2.2365
[09/26 04:22:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:22:47 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 04:22:54 visual_prompt]: Epoch 73 / 100: avg data time: 6.07e-02, avg batch time: 0.5099, average train loss: 0.0020
[09/26 04:22:55 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1689, average loss: 2.2376
[09/26 04:22:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:22:55 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 04:23:02 visual_prompt]: Epoch 74 / 100: avg data time: 5.07e-02, avg batch time: 0.4995, average train loss: 0.0020
[09/26 04:23:03 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1691, average loss: 2.2395
[09/26 04:23:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:23:03 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 04:23:10 visual_prompt]: Epoch 75 / 100: avg data time: 5.79e-02, avg batch time: 0.5058, average train loss: 0.0019
[09/26 04:23:12 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 2.2400
[09/26 04:23:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:23:12 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 04:23:18 visual_prompt]: Epoch 76 / 100: avg data time: 4.54e-02, avg batch time: 0.4941, average train loss: 0.0019
[09/26 04:23:20 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1692, average loss: 2.2407
[09/26 04:23:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:23:20 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 04:23:27 visual_prompt]: Epoch 77 / 100: avg data time: 5.57e-02, avg batch time: 0.5037, average train loss: 0.0018
[09/26 04:23:28 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 2.2409
[09/26 04:23:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:23:28 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 04:23:35 visual_prompt]: Epoch 78 / 100: avg data time: 6.03e-02, avg batch time: 0.5091, average train loss: 0.0018
[09/26 04:23:37 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 2.2415
[09/26 04:23:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:23:37 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 04:23:44 visual_prompt]: Epoch 79 / 100: avg data time: 6.70e-02, avg batch time: 0.5152, average train loss: 0.0019
[09/26 04:23:45 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1692, average loss: 2.2410
[09/26 04:23:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:23:45 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 04:23:52 visual_prompt]: Epoch 80 / 100: avg data time: 6.12e-02, avg batch time: 0.5103, average train loss: 0.0019
[09/26 04:23:54 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1694, average loss: 2.2411
[09/26 04:23:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:23:54 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 04:24:01 visual_prompt]: Epoch 81 / 100: avg data time: 4.46e-02, avg batch time: 0.4941, average train loss: 0.0019
[09/26 04:24:02 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1692, average loss: 2.2417
[09/26 04:24:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:24:02 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 04:24:09 visual_prompt]: Epoch 82 / 100: avg data time: 4.71e-02, avg batch time: 0.4954, average train loss: 0.0018
[09/26 04:24:10 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 2.2422
[09/26 04:24:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:24:10 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 04:24:17 visual_prompt]: Epoch 83 / 100: avg data time: 5.91e-02, avg batch time: 0.5091, average train loss: 0.0020
[09/26 04:24:19 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1689, average loss: 2.2429
[09/26 04:24:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:24:19 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 04:24:26 visual_prompt]: Epoch 84 / 100: avg data time: 5.39e-02, avg batch time: 0.5016, average train loss: 0.0018
[09/26 04:24:27 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 2.2431
[09/26 04:24:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:24:27 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 04:24:34 visual_prompt]: Epoch 85 / 100: avg data time: 5.01e-02, avg batch time: 0.4980, average train loss: 0.0019
[09/26 04:24:36 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1694, average loss: 2.2435
[09/26 04:24:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:24:36 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 04:24:43 visual_prompt]: Epoch 86 / 100: avg data time: 5.15e-02, avg batch time: 0.4999, average train loss: 0.0020
[09/26 04:24:44 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 2.2447
[09/26 04:24:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:24:44 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 04:24:51 visual_prompt]: Epoch 87 / 100: avg data time: 6.01e-02, avg batch time: 0.5076, average train loss: 0.0019
[09/26 04:24:53 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1693, average loss: 2.2445
[09/26 04:24:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:24:53 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 04:25:00 visual_prompt]: Epoch 88 / 100: avg data time: 5.78e-02, avg batch time: 0.5059, average train loss: 0.0019
[09/26 04:25:01 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1696, average loss: 2.2446
[09/26 04:25:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:25:01 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 04:25:08 visual_prompt]: Epoch 89 / 100: avg data time: 5.53e-02, avg batch time: 0.5026, average train loss: 0.0017
[09/26 04:25:09 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1690, average loss: 2.2447
[09/26 04:25:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:25:09 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 04:25:16 visual_prompt]: Epoch 90 / 100: avg data time: 4.35e-02, avg batch time: 0.4937, average train loss: 0.0017
[09/26 04:25:18 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1695, average loss: 2.2451
[09/26 04:25:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:25:18 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 04:25:24 visual_prompt]: Epoch 91 / 100: avg data time: 5.68e-02, avg batch time: 0.5059, average train loss: 0.0019
[09/26 04:25:26 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1691, average loss: 2.2455
[09/26 04:25:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:25:26 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 04:25:33 visual_prompt]: Epoch 92 / 100: avg data time: 5.68e-02, avg batch time: 0.5047, average train loss: 0.0019
[09/26 04:25:34 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1692, average loss: 2.2455
[09/26 04:25:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:25:34 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 04:25:41 visual_prompt]: Epoch 93 / 100: avg data time: 5.00e-02, avg batch time: 0.5006, average train loss: 0.0018
[09/26 04:25:43 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 2.2455
[09/26 04:25:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:25:43 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 04:25:50 visual_prompt]: Epoch 94 / 100: avg data time: 6.13e-02, avg batch time: 0.5089, average train loss: 0.0019
[09/26 04:25:51 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1687, average loss: 2.2456
[09/26 04:25:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:25:51 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 04:25:58 visual_prompt]: Epoch 95 / 100: avg data time: 5.26e-02, avg batch time: 0.5017, average train loss: 0.0018
[09/26 04:26:00 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1690, average loss: 2.2456
[09/26 04:26:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:26:00 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 04:26:06 visual_prompt]: Epoch 96 / 100: avg data time: 4.41e-02, avg batch time: 0.4941, average train loss: 0.0017
[09/26 04:26:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1691, average loss: 2.2457
[09/26 04:26:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:26:08 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 04:26:15 visual_prompt]: Epoch 97 / 100: avg data time: 5.62e-02, avg batch time: 0.5050, average train loss: 0.0018
[09/26 04:26:16 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1689, average loss: 2.2457
[09/26 04:26:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:26:16 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 04:26:23 visual_prompt]: Epoch 98 / 100: avg data time: 5.78e-02, avg batch time: 0.5065, average train loss: 0.0019
[09/26 04:26:25 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 2.2457
[09/26 04:26:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:26:25 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 04:26:32 visual_prompt]: Epoch 99 / 100: avg data time: 5.90e-02, avg batch time: 0.5075, average train loss: 0.0019
[09/26 04:26:33 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1695, average loss: 2.2457
[09/26 04:26:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:26:33 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 04:26:40 visual_prompt]: Epoch 100 / 100: avg data time: 5.53e-02, avg batch time: 0.5038, average train loss: 0.0019
[09/26 04:26:42 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1695, average loss: 2.2457
[09/26 04:26:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 84.00	
[09/26 04:26:42 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 04:26:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 04:26:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 04:26:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 04:26:42 visual_prompt]: Training with config:
[09/26 04:26:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 04:26:42 visual_prompt]: Loading training data...
[09/26 04:26:42 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 04:26:44 visual_prompt]: Number of images: 800
[09/26 04:26:44 visual_prompt]: Number of classes: 47 / 47
[09/26 04:26:44 visual_prompt]: Loading validation data...
[09/26 04:26:44 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 04:26:44 visual_prompt]: Number of images: 200
[09/26 04:26:44 visual_prompt]: Number of classes: 47 / 47
[09/26 04:26:44 visual_prompt]: Constructing models...
[09/26 04:26:47 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 04:26:47 visual_prompt]: tuned percent:0.576
[09/26 04:26:47 visual_prompt]: Device used for model: 0
[09/26 04:26:47 visual_prompt]: Setting up Evaluator...
[09/26 04:26:47 visual_prompt]: Setting up Trainer...
[09/26 04:26:47 visual_prompt]: 	Setting up the optimizer...
[09/26 04:26:47 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 04:26:54 visual_prompt]: Epoch 1 / 100: avg data time: 4.43e-02, avg batch time: 0.4943, average train loss: 3.9288
[09/26 04:26:55 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1683, average loss: 3.9045
[09/26 04:26:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 04:26:55 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 04:26:55 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 04:27:02 visual_prompt]: Epoch 2 / 100: avg data time: 4.81e-02, avg batch time: 0.4963, average train loss: 3.8892
[09/26 04:27:03 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1686, average loss: 3.8982
[09/26 04:27:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 9.50	
[09/26 04:27:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 04:27:10 visual_prompt]: Epoch 3 / 100: avg data time: 5.77e-02, avg batch time: 0.5055, average train loss: 3.8291
[09/26 04:27:12 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1689, average loss: 3.8565
[09/26 04:27:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.50	top5: 15.00	
[09/26 04:27:12 visual_prompt]: Best epoch 3: best metric: 0.055
[09/26 04:27:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 04:27:18 visual_prompt]: Epoch 4 / 100: avg data time: 6.23e-02, avg batch time: 0.5093, average train loss: 3.6793
[09/26 04:27:20 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1688, average loss: 3.8143
[09/26 04:27:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 21.00	
[09/26 04:27:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 04:27:27 visual_prompt]: Epoch 5 / 100: avg data time: 5.92e-02, avg batch time: 0.5068, average train loss: 3.4214
[09/26 04:27:28 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1688, average loss: 3.5350
[09/26 04:27:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.50	top5: 28.50	
[09/26 04:27:28 visual_prompt]: Best epoch 5: best metric: 0.125
[09/26 04:27:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 04:27:35 visual_prompt]: Epoch 6 / 100: avg data time: 6.29e-02, avg batch time: 0.5112, average train loss: 3.4996
[09/26 04:27:37 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1687, average loss: 3.5403
[09/26 04:27:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 30.00	
[09/26 04:27:37 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 04:27:44 visual_prompt]: Epoch 7 / 100: avg data time: 6.47e-02, avg batch time: 0.5138, average train loss: 2.9710
[09/26 04:27:46 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 3.0024
[09/26 04:27:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.50	top5: 53.00	
[09/26 04:27:46 visual_prompt]: Best epoch 7: best metric: 0.165
[09/26 04:27:46 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 04:27:52 visual_prompt]: Epoch 8 / 100: avg data time: 5.81e-02, avg batch time: 0.5064, average train loss: 2.3781
[09/26 04:27:54 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 2.5687
[09/26 04:27:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.50	top5: 66.00	
[09/26 04:27:54 visual_prompt]: Best epoch 8: best metric: 0.385
[09/26 04:27:54 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 04:28:01 visual_prompt]: Epoch 9 / 100: avg data time: 5.74e-02, avg batch time: 0.5049, average train loss: 1.8374
[09/26 04:28:02 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 2.4201
[09/26 04:28:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 33.50	top5: 68.50	
[09/26 04:28:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 04:28:09 visual_prompt]: Epoch 10 / 100: avg data time: 6.11e-02, avg batch time: 0.5096, average train loss: 1.5243
[09/26 04:28:11 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 2.1376
[09/26 04:28:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 76.00	
[09/26 04:28:11 visual_prompt]: Best epoch 10: best metric: 0.445
[09/26 04:28:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 04:28:17 visual_prompt]: Epoch 11 / 100: avg data time: 5.50e-02, avg batch time: 0.5035, average train loss: 1.2845
[09/26 04:28:19 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1692, average loss: 1.8862
[09/26 04:28:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 82.50	
[09/26 04:28:19 visual_prompt]: Best epoch 11: best metric: 0.475
[09/26 04:28:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 04:28:26 visual_prompt]: Epoch 12 / 100: avg data time: 5.12e-02, avg batch time: 0.5030, average train loss: 1.0087
[09/26 04:28:27 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1697, average loss: 1.7744
[09/26 04:28:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 84.50	
[09/26 04:28:27 visual_prompt]: Best epoch 12: best metric: 0.525
[09/26 04:28:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 04:28:34 visual_prompt]: Epoch 13 / 100: avg data time: 5.73e-02, avg batch time: 0.5049, average train loss: 1.6024
[09/26 04:28:36 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1688, average loss: 3.0515
[09/26 04:28:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.50	top5: 50.50	
[09/26 04:28:36 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 04:28:43 visual_prompt]: Epoch 14 / 100: avg data time: 6.06e-02, avg batch time: 0.5078, average train loss: 2.0816
[09/26 04:28:44 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1693, average loss: 2.1999
[09/26 04:28:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 81.00	
[09/26 04:28:44 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 04:28:51 visual_prompt]: Epoch 15 / 100: avg data time: 6.42e-02, avg batch time: 0.5119, average train loss: 1.3114
[09/26 04:28:53 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1694, average loss: 1.8637
[09/26 04:28:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 86.50	
[09/26 04:28:53 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 04:29:00 visual_prompt]: Epoch 16 / 100: avg data time: 6.40e-02, avg batch time: 0.5139, average train loss: 0.9624
[09/26 04:29:01 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1690, average loss: 1.7829
[09/26 04:29:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 87.00	
[09/26 04:29:01 visual_prompt]: Best epoch 16: best metric: 0.530
[09/26 04:29:01 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 04:29:08 visual_prompt]: Epoch 17 / 100: avg data time: 5.34e-02, avg batch time: 0.5022, average train loss: 0.7931
[09/26 04:29:10 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1692, average loss: 1.6626
[09/26 04:29:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 89.00	
[09/26 04:29:10 visual_prompt]: Best epoch 17: best metric: 0.550
[09/26 04:29:10 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 04:29:16 visual_prompt]: Epoch 18 / 100: avg data time: 4.97e-02, avg batch time: 0.4992, average train loss: 0.7048
[09/26 04:29:18 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1696, average loss: 1.8982
[09/26 04:29:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 87.00	
[09/26 04:29:18 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 04:29:25 visual_prompt]: Epoch 19 / 100: avg data time: 5.35e-02, avg batch time: 0.5019, average train loss: 0.7536
[09/26 04:29:26 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1692, average loss: 1.7109
[09/26 04:29:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 86.50	
[09/26 04:29:26 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 04:29:33 visual_prompt]: Epoch 20 / 100: avg data time: 6.35e-02, avg batch time: 0.5112, average train loss: 0.7517
[09/26 04:29:35 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1696, average loss: 1.6628
[09/26 04:29:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.50	top5: 90.00	
[09/26 04:29:35 visual_prompt]: Best epoch 20: best metric: 0.585
[09/26 04:29:35 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 04:29:42 visual_prompt]: Epoch 21 / 100: avg data time: 5.90e-02, avg batch time: 0.5066, average train loss: 0.6972
[09/26 04:29:43 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1691, average loss: 1.6537
[09/26 04:29:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 91.00	
[09/26 04:29:43 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 04:29:50 visual_prompt]: Epoch 22 / 100: avg data time: 5.29e-02, avg batch time: 0.5027, average train loss: 0.6706
[09/26 04:29:52 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 1.7770
[09/26 04:29:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 86.50	
[09/26 04:29:52 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 04:29:58 visual_prompt]: Epoch 23 / 100: avg data time: 5.21e-02, avg batch time: 0.5000, average train loss: 1.1672
[09/26 04:30:00 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 1.9548
[09/26 04:30:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 86.00	
[09/26 04:30:00 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 04:30:07 visual_prompt]: Epoch 24 / 100: avg data time: 5.06e-02, avg batch time: 0.4989, average train loss: 1.1386
[09/26 04:30:08 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1691, average loss: 1.9724
[09/26 04:30:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.50	top5: 85.50	
[09/26 04:30:08 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 04:30:15 visual_prompt]: Epoch 25 / 100: avg data time: 5.78e-02, avg batch time: 0.5065, average train loss: 1.2317
[09/26 04:30:17 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1694, average loss: 1.6939
[09/26 04:30:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 88.50	
[09/26 04:30:17 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 04:30:23 visual_prompt]: Epoch 26 / 100: avg data time: 4.32e-02, avg batch time: 0.4920, average train loss: 1.0837
[09/26 04:30:25 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1693, average loss: 1.9307
[09/26 04:30:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 88.00	
[09/26 04:30:25 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 04:30:32 visual_prompt]: Epoch 27 / 100: avg data time: 4.87e-02, avg batch time: 0.4984, average train loss: 0.8181
[09/26 04:30:33 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1697, average loss: 1.6633
[09/26 04:30:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 90.50	
[09/26 04:30:33 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 04:30:40 visual_prompt]: Epoch 28 / 100: avg data time: 4.12e-02, avg batch time: 0.4902, average train loss: 0.6919
[09/26 04:30:42 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 1.5844
[09/26 04:30:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 90.50	
[09/26 04:30:42 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 04:30:48 visual_prompt]: Epoch 29 / 100: avg data time: 5.54e-02, avg batch time: 0.5042, average train loss: 0.6023
[09/26 04:30:50 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1690, average loss: 1.7245
[09/26 04:30:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 86.50	
[09/26 04:30:50 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 04:30:57 visual_prompt]: Epoch 30 / 100: avg data time: 5.53e-02, avg batch time: 0.5040, average train loss: 1.0819
[09/26 04:30:58 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 3.6239
[09/26 04:30:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.50	top5: 39.50	
[09/26 04:30:58 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 04:31:05 visual_prompt]: Epoch 31 / 100: avg data time: 5.00e-02, avg batch time: 0.5029, average train loss: 2.3924
[09/26 04:31:07 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 2.5415
[09/26 04:31:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.00	top5: 71.50	
[09/26 04:31:07 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 04:31:14 visual_prompt]: Epoch 32 / 100: avg data time: 5.84e-02, avg batch time: 0.5065, average train loss: 1.2157
[09/26 04:31:15 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1688, average loss: 1.6904
[09/26 04:31:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 89.00	
[09/26 04:31:15 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 04:31:22 visual_prompt]: Epoch 33 / 100: avg data time: 5.36e-02, avg batch time: 0.5025, average train loss: 0.8755
[09/26 04:31:23 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1690, average loss: 1.5877
[09/26 04:31:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 89.50	
[09/26 04:31:23 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 04:31:30 visual_prompt]: Epoch 34 / 100: avg data time: 5.18e-02, avg batch time: 0.5014, average train loss: 0.7858
[09/26 04:31:32 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 1.5408
[09/26 04:31:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 89.00	
[09/26 04:31:32 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 04:31:39 visual_prompt]: Epoch 35 / 100: avg data time: 5.95e-02, avg batch time: 0.5080, average train loss: 0.7154
[09/26 04:31:40 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1693, average loss: 1.5119
[09/26 04:31:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 89.00	
[09/26 04:31:40 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 04:31:47 visual_prompt]: Epoch 36 / 100: avg data time: 4.98e-02, avg batch time: 0.4997, average train loss: 0.6986
[09/26 04:31:48 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 1.6783
[09/26 04:31:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 89.50	
[09/26 04:31:48 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 04:31:55 visual_prompt]: Epoch 37 / 100: avg data time: 5.28e-02, avg batch time: 0.5015, average train loss: 0.7828
[09/26 04:31:57 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1691, average loss: 1.6762
[09/26 04:31:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 89.00	
[09/26 04:31:57 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 04:32:04 visual_prompt]: Epoch 38 / 100: avg data time: 5.16e-02, avg batch time: 0.5006, average train loss: 0.8309
[09/26 04:32:05 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1694, average loss: 1.5307
[09/26 04:32:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 89.50	
[09/26 04:32:05 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 04:32:12 visual_prompt]: Epoch 39 / 100: avg data time: 4.91e-02, avg batch time: 0.4978, average train loss: 0.6164
[09/26 04:32:13 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 1.5506
[09/26 04:32:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 89.50	
[09/26 04:32:13 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 04:32:20 visual_prompt]: Epoch 40 / 100: avg data time: 4.88e-02, avg batch time: 0.4993, average train loss: 0.5654
[09/26 04:32:22 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1692, average loss: 1.5796
[09/26 04:32:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.00	
[09/26 04:32:22 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 04:32:28 visual_prompt]: Epoch 41 / 100: avg data time: 4.50e-02, avg batch time: 0.4978, average train loss: 0.5132
[09/26 04:32:30 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1693, average loss: 1.6302
[09/26 04:32:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 85.00	
[09/26 04:32:30 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 04:32:37 visual_prompt]: Epoch 42 / 100: avg data time: 5.73e-02, avg batch time: 0.5067, average train loss: 0.4481
[09/26 04:32:38 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1692, average loss: 1.7210
[09/26 04:32:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 91.00	
[09/26 04:32:38 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 04:32:45 visual_prompt]: Epoch 43 / 100: avg data time: 4.84e-02, avg batch time: 0.4985, average train loss: 0.4672
[09/26 04:32:47 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 1.6090
[09/26 04:32:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 92.00	
[09/26 04:32:47 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 04:32:53 visual_prompt]: Epoch 44 / 100: avg data time: 5.48e-02, avg batch time: 0.5031, average train loss: 0.4568
[09/26 04:32:55 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 1.5726
[09/26 04:32:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 92.50	
[09/26 04:32:55 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 04:33:02 visual_prompt]: Epoch 45 / 100: avg data time: 5.59e-02, avg batch time: 0.5049, average train loss: 0.8399
[09/26 04:33:03 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1687, average loss: 2.1418
[09/26 04:33:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 80.50	
[09/26 04:33:03 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 04:33:10 visual_prompt]: Epoch 46 / 100: avg data time: 6.51e-02, avg batch time: 0.5139, average train loss: 1.2583
[09/26 04:33:12 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1692, average loss: 1.7841
[09/26 04:33:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 85.00	
[09/26 04:33:12 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 04:33:19 visual_prompt]: Epoch 47 / 100: avg data time: 5.47e-02, avg batch time: 0.5029, average train loss: 0.7813
[09/26 04:33:20 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1690, average loss: 1.7101
[09/26 04:33:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 87.00	
[09/26 04:33:20 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 04:33:27 visual_prompt]: Epoch 48 / 100: avg data time: 5.94e-02, avg batch time: 0.5090, average train loss: 0.5467
[09/26 04:33:29 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1692, average loss: 1.5408
[09/26 04:33:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 89.00	
[09/26 04:33:29 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 04:33:36 visual_prompt]: Epoch 49 / 100: avg data time: 5.75e-02, avg batch time: 0.5077, average train loss: 1.2742
[09/26 04:33:37 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 1.7593
[09/26 04:33:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 88.50	
[09/26 04:33:37 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 04:33:44 visual_prompt]: Epoch 50 / 100: avg data time: 5.82e-02, avg batch time: 0.5063, average train loss: 0.6711
[09/26 04:33:46 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1688, average loss: 1.6140
[09/26 04:33:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 87.50	
[09/26 04:33:46 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 04:33:52 visual_prompt]: Epoch 51 / 100: avg data time: 4.56e-02, avg batch time: 0.4942, average train loss: 0.5197
[09/26 04:33:54 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1694, average loss: 1.5770
[09/26 04:33:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 87.50	
[09/26 04:33:54 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 04:34:01 visual_prompt]: Epoch 52 / 100: avg data time: 4.67e-02, avg batch time: 0.4948, average train loss: 0.3979
[09/26 04:34:02 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 1.5540
[09/26 04:34:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.50	top5: 87.00	
[09/26 04:34:02 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 04:34:09 visual_prompt]: Epoch 53 / 100: avg data time: 6.02e-02, avg batch time: 0.5085, average train loss: 0.3700
[09/26 04:34:11 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1687, average loss: 1.5902
[09/26 04:34:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 84.50	
[09/26 04:34:11 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 04:34:17 visual_prompt]: Epoch 54 / 100: avg data time: 5.48e-02, avg batch time: 0.5030, average train loss: 0.3798
[09/26 04:34:19 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 1.5566
[09/26 04:34:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 58.00	top5: 86.50	
[09/26 04:34:19 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 04:34:26 visual_prompt]: Epoch 55 / 100: avg data time: 5.76e-02, avg batch time: 0.5067, average train loss: 0.4068
[09/26 04:34:27 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1691, average loss: 1.6723
[09/26 04:34:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 87.50	
[09/26 04:34:27 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 04:34:34 visual_prompt]: Epoch 56 / 100: avg data time: 4.62e-02, avg batch time: 0.4959, average train loss: 0.4751
[09/26 04:34:36 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1696, average loss: 1.6434
[09/26 04:34:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 87.50	
[09/26 04:34:36 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 04:34:42 visual_prompt]: Epoch 57 / 100: avg data time: 4.96e-02, avg batch time: 0.4987, average train loss: 0.4412
[09/26 04:34:44 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1692, average loss: 1.6534
[09/26 04:34:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 86.00	
[09/26 04:34:44 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 04:34:51 visual_prompt]: Epoch 58 / 100: avg data time: 5.01e-02, avg batch time: 0.4990, average train loss: 1.0224
[09/26 04:34:52 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1695, average loss: 1.9124
[09/26 04:34:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 04:34:52 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 04:34:59 visual_prompt]: Epoch 59 / 100: avg data time: 4.90e-02, avg batch time: 0.4986, average train loss: 0.8847
[09/26 04:35:00 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 1.6997
[09/26 04:35:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 85.50	
[09/26 04:35:00 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 04:35:07 visual_prompt]: Epoch 60 / 100: avg data time: 6.60e-02, avg batch time: 0.5139, average train loss: 0.5210
[09/26 04:35:09 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1695, average loss: 1.6158
[09/26 04:35:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 04:35:09 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 04:35:16 visual_prompt]: Epoch 61 / 100: avg data time: 4.34e-02, avg batch time: 0.4925, average train loss: 0.3584
[09/26 04:35:17 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 1.5768
[09/26 04:35:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 04:35:17 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 04:35:24 visual_prompt]: Epoch 62 / 100: avg data time: 5.95e-02, avg batch time: 0.5081, average train loss: 0.2905
[09/26 04:35:26 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1694, average loss: 1.5614
[09/26 04:35:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.00	top5: 86.50	
[09/26 04:35:26 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 04:35:32 visual_prompt]: Epoch 63 / 100: avg data time: 6.00e-02, avg batch time: 0.5079, average train loss: 0.2632
[09/26 04:35:34 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1690, average loss: 1.5237
[09/26 04:35:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 56.50	top5: 87.50	
[09/26 04:35:34 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 04:35:41 visual_prompt]: Epoch 64 / 100: avg data time: 4.40e-02, avg batch time: 0.4927, average train loss: 0.2329
[09/26 04:35:42 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1692, average loss: 1.6259
[09/26 04:35:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 85.00	
[09/26 04:35:42 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 04:35:49 visual_prompt]: Epoch 65 / 100: avg data time: 4.78e-02, avg batch time: 0.4982, average train loss: 0.2439
[09/26 04:35:50 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1688, average loss: 1.5912
[09/26 04:35:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 86.00	
[09/26 04:35:50 visual_prompt]: Best epoch 65: best metric: 0.595
[09/26 04:35:50 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 04:35:57 visual_prompt]: Epoch 66 / 100: avg data time: 5.89e-02, avg batch time: 0.5068, average train loss: 0.2592
[09/26 04:35:59 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1695, average loss: 1.6243
[09/26 04:35:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 57.00	top5: 89.50	
[09/26 04:35:59 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 04:36:06 visual_prompt]: Epoch 67 / 100: avg data time: 5.39e-02, avg batch time: 0.5030, average train loss: 0.7370
[09/26 04:36:07 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1697, average loss: 1.6884
[09/26 04:36:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 85.50	
[09/26 04:36:07 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 04:36:14 visual_prompt]: Epoch 68 / 100: avg data time: 6.39e-02, avg batch time: 0.5120, average train loss: 0.4709
[09/26 04:36:16 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1692, average loss: 1.5209
[09/26 04:36:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 90.00	
[09/26 04:36:16 visual_prompt]: Best epoch 68: best metric: 0.605
[09/26 04:36:16 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 04:36:23 visual_prompt]: Epoch 69 / 100: avg data time: 5.84e-02, avg batch time: 0.5079, average train loss: 0.3293
[09/26 04:36:24 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 1.5928
[09/26 04:36:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 04:36:24 visual_prompt]: Best epoch 69: best metric: 0.610
[09/26 04:36:24 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 04:36:31 visual_prompt]: Epoch 70 / 100: avg data time: 5.41e-02, avg batch time: 0.5029, average train loss: 0.2582
[09/26 04:36:32 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 1.5557
[09/26 04:36:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 86.50	
[09/26 04:36:32 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 04:36:39 visual_prompt]: Epoch 71 / 100: avg data time: 5.86e-02, avg batch time: 0.5066, average train loss: 0.2017
[09/26 04:36:41 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1689, average loss: 1.7371
[09/26 04:36:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 86.00	
[09/26 04:36:41 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 04:36:48 visual_prompt]: Epoch 72 / 100: avg data time: 5.37e-02, avg batch time: 0.5037, average train loss: 0.2876
[09/26 04:36:50 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1689, average loss: 1.5512
[09/26 04:36:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 88.50	
[09/26 04:36:50 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 04:36:56 visual_prompt]: Epoch 73 / 100: avg data time: 6.22e-02, avg batch time: 0.5127, average train loss: 0.1932
[09/26 04:36:58 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1694, average loss: 1.5908
[09/26 04:36:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 86.50	
[09/26 04:36:58 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 04:37:05 visual_prompt]: Epoch 74 / 100: avg data time: 5.63e-02, avg batch time: 0.5049, average train loss: 0.1586
[09/26 04:37:06 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1689, average loss: 1.5743
[09/26 04:37:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 88.50	
[09/26 04:37:06 visual_prompt]: Best epoch 74: best metric: 0.615
[09/26 04:37:06 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 04:37:13 visual_prompt]: Epoch 75 / 100: avg data time: 5.72e-02, avg batch time: 0.5052, average train loss: 0.1422
[09/26 04:37:15 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 1.5619
[09/26 04:37:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.50	top5: 87.50	
[09/26 04:37:15 visual_prompt]: Best epoch 75: best metric: 0.625
[09/26 04:37:15 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 04:37:22 visual_prompt]: Epoch 76 / 100: avg data time: 5.06e-02, avg batch time: 0.4994, average train loss: 0.1302
[09/26 04:37:23 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 1.6124
[09/26 04:37:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 87.00	
[09/26 04:37:23 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 04:37:30 visual_prompt]: Epoch 77 / 100: avg data time: 5.46e-02, avg batch time: 0.5026, average train loss: 0.1254
[09/26 04:37:31 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1694, average loss: 1.6220
[09/26 04:37:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 86.00	
[09/26 04:37:31 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 04:37:38 visual_prompt]: Epoch 78 / 100: avg data time: 5.51e-02, avg batch time: 0.5030, average train loss: 0.1264
[09/26 04:37:40 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1694, average loss: 1.5858
[09/26 04:37:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 63.50	top5: 87.00	
[09/26 04:37:40 visual_prompt]: Best epoch 78: best metric: 0.635
[09/26 04:37:40 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 04:37:47 visual_prompt]: Epoch 79 / 100: avg data time: 4.66e-02, avg batch time: 0.4979, average train loss: 0.1191
[09/26 04:37:48 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1692, average loss: 1.6253
[09/26 04:37:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 86.50	
[09/26 04:37:48 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 04:37:55 visual_prompt]: Epoch 80 / 100: avg data time: 5.19e-02, avg batch time: 0.5005, average train loss: 0.1143
[09/26 04:37:56 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1689, average loss: 1.6424
[09/26 04:37:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 86.00	
[09/26 04:37:57 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 04:38:03 visual_prompt]: Epoch 81 / 100: avg data time: 5.77e-02, avg batch time: 0.5059, average train loss: 0.1167
[09/26 04:38:05 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1691, average loss: 1.5966
[09/26 04:38:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 86.00	
[09/26 04:38:05 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 04:38:12 visual_prompt]: Epoch 82 / 100: avg data time: 6.29e-02, avg batch time: 0.5114, average train loss: 0.1136
[09/26 04:38:13 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 1.6365
[09/26 04:38:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 85.00	
[09/26 04:38:13 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 04:38:20 visual_prompt]: Epoch 83 / 100: avg data time: 4.71e-02, avg batch time: 0.4966, average train loss: 0.1108
[09/26 04:38:22 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1691, average loss: 1.6484
[09/26 04:38:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.00	top5: 85.50	
[09/26 04:38:22 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 04:38:28 visual_prompt]: Epoch 84 / 100: avg data time: 4.42e-02, avg batch time: 0.4922, average train loss: 0.1082
[09/26 04:38:30 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1691, average loss: 1.6419
[09/26 04:38:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 62.00	top5: 86.50	
[09/26 04:38:30 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 04:38:37 visual_prompt]: Epoch 85 / 100: avg data time: 4.47e-02, avg batch time: 0.4943, average train loss: 0.1083
[09/26 04:38:38 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1697, average loss: 1.6743
[09/26 04:38:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.50	top5: 83.50	
[09/26 04:38:38 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 04:38:45 visual_prompt]: Epoch 86 / 100: avg data time: 5.60e-02, avg batch time: 0.5061, average train loss: 0.1064
[09/26 04:38:47 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1694, average loss: 1.6774
[09/26 04:38:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 85.50	
[09/26 04:38:47 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 04:38:54 visual_prompt]: Epoch 87 / 100: avg data time: 5.77e-02, avg batch time: 0.5077, average train loss: 0.1065
[09/26 04:38:55 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1694, average loss: 1.6751
[09/26 04:38:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 59.50	top5: 86.50	
[09/26 04:38:55 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 04:39:02 visual_prompt]: Epoch 88 / 100: avg data time: 5.03e-02, avg batch time: 0.4988, average train loss: 0.1060
[09/26 04:39:03 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 1.6689
[09/26 04:39:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 86.00	
[09/26 04:39:03 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 04:39:10 visual_prompt]: Epoch 89 / 100: avg data time: 4.75e-02, avg batch time: 0.4969, average train loss: 0.1053
[09/26 04:39:12 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1692, average loss: 1.6963
[09/26 04:39:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.50	
[09/26 04:39:12 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 04:39:18 visual_prompt]: Epoch 90 / 100: avg data time: 5.68e-02, avg batch time: 0.5053, average train loss: 0.1042
[09/26 04:39:20 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1695, average loss: 1.7021
[09/26 04:39:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 84.50	
[09/26 04:39:20 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 04:39:27 visual_prompt]: Epoch 91 / 100: avg data time: 4.44e-02, avg batch time: 0.4968, average train loss: 0.1037
[09/26 04:39:28 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1695, average loss: 1.7246
[09/26 04:39:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 84.50	
[09/26 04:39:28 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 04:39:35 visual_prompt]: Epoch 92 / 100: avg data time: 5.99e-02, avg batch time: 0.5085, average train loss: 0.1035
[09/26 04:39:37 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1696, average loss: 1.7113
[09/26 04:39:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 84.00	
[09/26 04:39:37 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 04:39:44 visual_prompt]: Epoch 93 / 100: avg data time: 5.69e-02, avg batch time: 0.5058, average train loss: 0.1030
[09/26 04:39:45 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 1.7139
[09/26 04:39:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 84.50	
[09/26 04:39:45 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 04:39:52 visual_prompt]: Epoch 94 / 100: avg data time: 5.79e-02, avg batch time: 0.5067, average train loss: 0.1024
[09/26 04:39:54 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 1.7123
[09/26 04:39:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.00	top5: 85.00	
[09/26 04:39:54 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 04:40:01 visual_prompt]: Epoch 95 / 100: avg data time: 5.88e-02, avg batch time: 0.5080, average train loss: 0.1024
[09/26 04:40:02 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1693, average loss: 1.7191
[09/26 04:40:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 60.50	top5: 85.00	
[09/26 04:40:02 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 04:40:09 visual_prompt]: Epoch 96 / 100: avg data time: 4.62e-02, avg batch time: 0.4948, average train loss: 0.1021
[09/26 04:40:10 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 1.7206
[09/26 04:40:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 85.00	
[09/26 04:40:10 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 04:40:17 visual_prompt]: Epoch 97 / 100: avg data time: 5.78e-02, avg batch time: 0.5058, average train loss: 0.1024
[09/26 04:40:19 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1694, average loss: 1.7238
[09/26 04:40:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 84.50	
[09/26 04:40:19 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 04:40:26 visual_prompt]: Epoch 98 / 100: avg data time: 5.77e-02, avg batch time: 0.5056, average train loss: 0.1018
[09/26 04:40:27 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1692, average loss: 1.7245
[09/26 04:40:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 84.50	
[09/26 04:40:27 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 04:40:34 visual_prompt]: Epoch 99 / 100: avg data time: 5.78e-02, avg batch time: 0.5058, average train loss: 0.1016
[09/26 04:40:35 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1691, average loss: 1.7251
[09/26 04:40:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 84.50	
[09/26 04:40:35 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 04:40:42 visual_prompt]: Epoch 100 / 100: avg data time: 4.67e-02, avg batch time: 0.4977, average train loss: 0.1019
[09/26 04:40:44 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1695, average loss: 1.7249
[09/26 04:40:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 61.00	top5: 84.50	
[09/26 04:40:44 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 04:40:44 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 04:40:44 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 04:40:44 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 04:40:44 visual_prompt]: Training with config:
[09/26 04:40:44 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 04:40:44 visual_prompt]: Loading training data...
[09/26 04:40:44 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 04:40:46 visual_prompt]: Number of images: 800
[09/26 04:40:46 visual_prompt]: Number of classes: 47 / 47
[09/26 04:40:46 visual_prompt]: Loading validation data...
[09/26 04:40:46 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 04:40:46 visual_prompt]: Number of images: 200
[09/26 04:40:46 visual_prompt]: Number of classes: 47 / 47
[09/26 04:40:46 visual_prompt]: Constructing models...
[09/26 04:40:49 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 04:40:49 visual_prompt]: tuned percent:0.576
[09/26 04:40:49 visual_prompt]: Device used for model: 0
[09/26 04:40:49 visual_prompt]: Setting up Evaluator...
[09/26 04:40:49 visual_prompt]: Setting up Trainer...
[09/26 04:40:49 visual_prompt]: 	Setting up the optimizer...
[09/26 04:40:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 04:40:56 visual_prompt]: Epoch 1 / 100: avg data time: 5.24e-02, avg batch time: 0.5001, average train loss: 3.9340
[09/26 04:40:57 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 3.9045
[09/26 04:40:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 04:40:57 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 04:40:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 04:41:04 visual_prompt]: Epoch 2 / 100: avg data time: 4.69e-02, avg batch time: 0.4984, average train loss: 3.8644
[09/26 04:41:05 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 3.8342
[09/26 04:41:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.00	top5: 16.50	
[09/26 04:41:05 visual_prompt]: Best epoch 2: best metric: 0.070
[09/26 04:41:05 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 04:41:12 visual_prompt]: Epoch 3 / 100: avg data time: 5.73e-02, avg batch time: 0.5045, average train loss: 3.7451
[09/26 04:41:14 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 3.7484
[09/26 04:41:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.50	top5: 22.50	
[09/26 04:41:14 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 04:41:21 visual_prompt]: Epoch 4 / 100: avg data time: 5.86e-02, avg batch time: 0.5068, average train loss: 3.4546
[09/26 04:41:22 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 3.4059
[09/26 04:41:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.00	top5: 33.00	
[09/26 04:41:22 visual_prompt]: Best epoch 4: best metric: 0.100
[09/26 04:41:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 04:41:29 visual_prompt]: Epoch 5 / 100: avg data time: 5.92e-02, avg batch time: 0.5068, average train loss: 2.9810
[09/26 04:41:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1697, average loss: 3.3521
[09/26 04:41:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.50	top5: 42.50	
[09/26 04:41:31 visual_prompt]: Best epoch 5: best metric: 0.125
[09/26 04:41:31 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 04:41:37 visual_prompt]: Epoch 6 / 100: avg data time: 5.02e-02, avg batch time: 0.4989, average train loss: 2.7648
[09/26 04:41:39 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1697, average loss: 3.0428
[09/26 04:41:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 19.00	top5: 52.50	
[09/26 04:41:39 visual_prompt]: Best epoch 6: best metric: 0.190
[09/26 04:41:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 04:41:46 visual_prompt]: Epoch 7 / 100: avg data time: 5.13e-02, avg batch time: 0.5001, average train loss: 2.2233
[09/26 04:41:47 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1691, average loss: 2.8102
[09/26 04:41:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 57.50	
[09/26 04:41:47 visual_prompt]: Best epoch 7: best metric: 0.245
[09/26 04:41:47 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 04:41:54 visual_prompt]: Epoch 8 / 100: avg data time: 5.06e-02, avg batch time: 0.4993, average train loss: 1.7097
[09/26 04:41:56 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1691, average loss: 2.3904
[09/26 04:41:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 35.50	top5: 66.50	
[09/26 04:41:56 visual_prompt]: Best epoch 8: best metric: 0.355
[09/26 04:41:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 04:42:02 visual_prompt]: Epoch 9 / 100: avg data time: 5.16e-02, avg batch time: 0.5010, average train loss: 1.2462
[09/26 04:42:04 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 2.3420
[09/26 04:42:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.50	top5: 69.00	
[09/26 04:42:04 visual_prompt]: Best epoch 9: best metric: 0.375
[09/26 04:42:04 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 04:42:11 visual_prompt]: Epoch 10 / 100: avg data time: 5.57e-02, avg batch time: 0.5043, average train loss: 0.9052
[09/26 04:42:12 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1692, average loss: 2.2147
[09/26 04:42:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.00	top5: 77.50	
[09/26 04:42:12 visual_prompt]: Best epoch 10: best metric: 0.380
[09/26 04:42:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 04:42:19 visual_prompt]: Epoch 11 / 100: avg data time: 5.96e-02, avg batch time: 0.5081, average train loss: 0.5643
[09/26 04:42:21 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1697, average loss: 2.1392
[09/26 04:42:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 39.50	top5: 77.50	
[09/26 04:42:21 visual_prompt]: Best epoch 11: best metric: 0.395
[09/26 04:42:21 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 04:42:28 visual_prompt]: Epoch 12 / 100: avg data time: 5.99e-02, avg batch time: 0.5085, average train loss: 0.3943
[09/26 04:42:29 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1695, average loss: 2.1067
[09/26 04:42:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 74.50	
[09/26 04:42:29 visual_prompt]: Best epoch 12: best metric: 0.420
[09/26 04:42:29 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 04:42:36 visual_prompt]: Epoch 13 / 100: avg data time: 5.43e-02, avg batch time: 0.5042, average train loss: 0.2752
[09/26 04:42:38 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1696, average loss: 2.0862
[09/26 04:42:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 80.50	
[09/26 04:42:38 visual_prompt]: Best epoch 13: best metric: 0.455
[09/26 04:42:38 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 04:42:44 visual_prompt]: Epoch 14 / 100: avg data time: 4.46e-02, avg batch time: 0.4943, average train loss: 0.2071
[09/26 04:42:46 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1695, average loss: 1.9982
[09/26 04:42:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 83.00	
[09/26 04:42:46 visual_prompt]: Best epoch 14: best metric: 0.460
[09/26 04:42:46 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 04:42:53 visual_prompt]: Epoch 15 / 100: avg data time: 4.55e-02, avg batch time: 0.4961, average train loss: 0.1315
[09/26 04:42:54 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1698, average loss: 1.9688
[09/26 04:42:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 83.50	
[09/26 04:42:54 visual_prompt]: Best epoch 15: best metric: 0.475
[09/26 04:42:54 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 04:43:01 visual_prompt]: Epoch 16 / 100: avg data time: 5.52e-02, avg batch time: 0.5040, average train loss: 0.0834
[09/26 04:43:03 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1700, average loss: 1.9311
[09/26 04:43:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 83.50	
[09/26 04:43:03 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 04:43:09 visual_prompt]: Epoch 17 / 100: avg data time: 5.80e-02, avg batch time: 0.5059, average train loss: 0.0514
[09/26 04:43:11 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1694, average loss: 1.9953
[09/26 04:43:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 83.50	
[09/26 04:43:11 visual_prompt]: Best epoch 17: best metric: 0.490
[09/26 04:43:11 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 04:43:18 visual_prompt]: Epoch 18 / 100: avg data time: 5.72e-02, avg batch time: 0.5053, average train loss: 0.0382
[09/26 04:43:19 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 1.9305
[09/26 04:43:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 85.50	
[09/26 04:43:19 visual_prompt]: Best epoch 18: best metric: 0.500
[09/26 04:43:19 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 04:43:26 visual_prompt]: Epoch 19 / 100: avg data time: 4.36e-02, avg batch time: 0.4928, average train loss: 0.0315
[09/26 04:43:28 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1697, average loss: 1.9531
[09/26 04:43:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 84.00	
[09/26 04:43:28 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 04:43:35 visual_prompt]: Epoch 20 / 100: avg data time: 6.29e-02, avg batch time: 0.5117, average train loss: 0.0270
[09/26 04:43:36 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1700, average loss: 1.9475
[09/26 04:43:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 84.50	
[09/26 04:43:36 visual_prompt]: Best epoch 20: best metric: 0.515
[09/26 04:43:36 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 04:43:43 visual_prompt]: Epoch 21 / 100: avg data time: 6.13e-02, avg batch time: 0.5101, average train loss: 0.0260
[09/26 04:43:45 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1696, average loss: 1.9570
[09/26 04:43:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 84.50	
[09/26 04:43:45 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 04:43:52 visual_prompt]: Epoch 22 / 100: avg data time: 5.58e-02, avg batch time: 0.5057, average train loss: 0.0250
[09/26 04:43:53 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 1.9439
[09/26 04:43:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 84.00	
[09/26 04:43:53 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 04:44:00 visual_prompt]: Epoch 23 / 100: avg data time: 5.36e-02, avg batch time: 0.5034, average train loss: 0.0247
[09/26 04:44:02 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1698, average loss: 1.9247
[09/26 04:44:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:44:02 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 04:44:09 visual_prompt]: Epoch 24 / 100: avg data time: 4.89e-02, avg batch time: 0.4987, average train loss: 0.0244
[09/26 04:44:10 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1695, average loss: 1.9402
[09/26 04:44:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 84.00	
[09/26 04:44:10 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 04:44:17 visual_prompt]: Epoch 25 / 100: avg data time: 4.02e-02, avg batch time: 0.4925, average train loss: 0.0232
[09/26 04:44:18 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1693, average loss: 1.9398
[09/26 04:44:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 84.00	
[09/26 04:44:18 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 04:44:25 visual_prompt]: Epoch 26 / 100: avg data time: 5.68e-02, avg batch time: 0.5050, average train loss: 0.0230
[09/26 04:44:27 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1695, average loss: 1.9616
[09/26 04:44:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.00	
[09/26 04:44:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 04:44:34 visual_prompt]: Epoch 27 / 100: avg data time: 6.32e-02, avg batch time: 0.5110, average train loss: 0.0226
[09/26 04:44:35 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1692, average loss: 1.9291
[09/26 04:44:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.50	
[09/26 04:44:35 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 04:44:42 visual_prompt]: Epoch 28 / 100: avg data time: 5.36e-02, avg batch time: 0.5040, average train loss: 0.0224
[09/26 04:44:44 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 1.9376
[09/26 04:44:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 84.00	
[09/26 04:44:44 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 04:44:50 visual_prompt]: Epoch 29 / 100: avg data time: 4.49e-02, avg batch time: 0.4953, average train loss: 0.0218
[09/26 04:44:52 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1697, average loss: 1.9445
[09/26 04:44:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.00	
[09/26 04:44:52 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 04:44:59 visual_prompt]: Epoch 30 / 100: avg data time: 5.68e-02, avg batch time: 0.5058, average train loss: 0.0222
[09/26 04:45:00 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1698, average loss: 1.9419
[09/26 04:45:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 82.50	
[09/26 04:45:00 visual_prompt]: Best epoch 30: best metric: 0.520
[09/26 04:45:00 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 04:45:07 visual_prompt]: Epoch 31 / 100: avg data time: 5.27e-02, avg batch time: 0.5024, average train loss: 0.0220
[09/26 04:45:09 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1696, average loss: 1.9841
[09/26 04:45:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 82.00	
[09/26 04:45:09 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 04:45:16 visual_prompt]: Epoch 32 / 100: avg data time: 5.25e-02, avg batch time: 0.5015, average train loss: 0.0221
[09/26 04:45:17 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1695, average loss: 1.9719
[09/26 04:45:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 84.00	
[09/26 04:45:17 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 04:45:24 visual_prompt]: Epoch 33 / 100: avg data time: 6.04e-02, avg batch time: 0.5096, average train loss: 0.0233
[09/26 04:45:25 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 1.9710
[09/26 04:45:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 82.50	
[09/26 04:45:26 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 04:45:32 visual_prompt]: Epoch 34 / 100: avg data time: 4.58e-02, avg batch time: 0.4961, average train loss: 0.0239
[09/26 04:45:34 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.1696, average loss: 1.9515
[09/26 04:45:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 81.50	
[09/26 04:45:34 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 04:45:41 visual_prompt]: Epoch 35 / 100: avg data time: 5.78e-02, avg batch time: 0.5083, average train loss: 0.0221
[09/26 04:45:43 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 1.9288
[09/26 04:45:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 80.50	
[09/26 04:45:43 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 04:45:49 visual_prompt]: Epoch 36 / 100: avg data time: 4.39e-02, avg batch time: 0.4929, average train loss: 0.0210
[09/26 04:45:51 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1697, average loss: 1.9870
[09/26 04:45:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 83.00	
[09/26 04:45:51 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 04:45:58 visual_prompt]: Epoch 37 / 100: avg data time: 5.35e-02, avg batch time: 0.5022, average train loss: 0.0202
[09/26 04:45:59 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 1.9875
[09/26 04:45:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 81.50	
[09/26 04:45:59 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 04:46:06 visual_prompt]: Epoch 38 / 100: avg data time: 5.42e-02, avg batch time: 0.5031, average train loss: 0.0191
[09/26 04:46:07 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1694, average loss: 2.0120
[09/26 04:46:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 82.00	
[09/26 04:46:07 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 04:46:14 visual_prompt]: Epoch 39 / 100: avg data time: 5.03e-02, avg batch time: 0.5004, average train loss: 0.0199
[09/26 04:46:16 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1697, average loss: 1.9580
[09/26 04:46:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 82.50	
[09/26 04:46:16 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 04:46:23 visual_prompt]: Epoch 40 / 100: avg data time: 5.43e-02, avg batch time: 0.5030, average train loss: 0.0204
[09/26 04:46:24 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1697, average loss: 2.0504
[09/26 04:46:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 82.00	
[09/26 04:46:24 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 04:46:31 visual_prompt]: Epoch 41 / 100: avg data time: 4.41e-02, avg batch time: 0.4931, average train loss: 0.0227
[09/26 04:46:32 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1692, average loss: 2.0575
[09/26 04:46:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 81.00	
[09/26 04:46:32 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 04:46:39 visual_prompt]: Epoch 42 / 100: avg data time: 4.56e-02, avg batch time: 0.4951, average train loss: 0.0239
[09/26 04:46:41 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1696, average loss: 2.1355
[09/26 04:46:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 80.50	
[09/26 04:46:41 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 04:46:48 visual_prompt]: Epoch 43 / 100: avg data time: 6.15e-02, avg batch time: 0.5130, average train loss: 0.0260
[09/26 04:46:49 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1696, average loss: 2.0556
[09/26 04:46:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 81.00	
[09/26 04:46:49 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 04:46:56 visual_prompt]: Epoch 44 / 100: avg data time: 4.68e-02, avg batch time: 0.4971, average train loss: 0.0270
[09/26 04:46:57 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1696, average loss: 2.0468
[09/26 04:46:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 81.50	
[09/26 04:46:57 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 04:47:04 visual_prompt]: Epoch 45 / 100: avg data time: 4.80e-02, avg batch time: 0.4981, average train loss: 0.0342
[09/26 04:47:06 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.9942
[09/26 04:47:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 83.00	
[09/26 04:47:06 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 04:47:12 visual_prompt]: Epoch 46 / 100: avg data time: 4.84e-02, avg batch time: 0.4971, average train loss: 0.1058
[09/26 04:47:14 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1696, average loss: 2.0304
[09/26 04:47:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 75.50	
[09/26 04:47:14 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 04:47:21 visual_prompt]: Epoch 47 / 100: avg data time: 5.71e-02, avg batch time: 0.5054, average train loss: 0.4927
[09/26 04:47:23 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1694, average loss: 2.2373
[09/26 04:47:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 39.50	top5: 71.00	
[09/26 04:47:23 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 04:47:29 visual_prompt]: Epoch 48 / 100: avg data time: 5.54e-02, avg batch time: 0.5040, average train loss: 0.6773
[09/26 04:47:31 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1698, average loss: 2.0568
[09/26 04:47:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 83.00	
[09/26 04:47:31 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 04:47:38 visual_prompt]: Epoch 49 / 100: avg data time: 4.69e-02, avg batch time: 0.4963, average train loss: 0.4426
[09/26 04:47:39 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1696, average loss: 1.9500
[09/26 04:47:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 81.00	
[09/26 04:47:39 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 04:47:46 visual_prompt]: Epoch 50 / 100: avg data time: 4.52e-02, avg batch time: 0.4948, average train loss: 0.2102
[09/26 04:47:47 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1696, average loss: 1.9686
[09/26 04:47:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 83.00	
[09/26 04:47:47 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 04:47:54 visual_prompt]: Epoch 51 / 100: avg data time: 5.83e-02, avg batch time: 0.5073, average train loss: 0.0965
[09/26 04:47:56 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1692, average loss: 1.8963
[09/26 04:47:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 81.00	
[09/26 04:47:56 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 04:48:03 visual_prompt]: Epoch 52 / 100: avg data time: 6.11e-02, avg batch time: 0.5110, average train loss: 0.0551
[09/26 04:48:04 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1695, average loss: 2.0017
[09/26 04:48:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 81.50	
[09/26 04:48:04 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 04:48:11 visual_prompt]: Epoch 53 / 100: avg data time: 6.03e-02, avg batch time: 0.5086, average train loss: 0.0376
[09/26 04:48:13 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1696, average loss: 1.9117
[09/26 04:48:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 82.00	
[09/26 04:48:13 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 04:48:20 visual_prompt]: Epoch 54 / 100: avg data time: 6.21e-02, avg batch time: 0.5102, average train loss: 0.0256
[09/26 04:48:21 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1697, average loss: 1.9375
[09/26 04:48:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 83.00	
[09/26 04:48:21 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 04:48:28 visual_prompt]: Epoch 55 / 100: avg data time: 5.86e-02, avg batch time: 0.5075, average train loss: 0.0216
[09/26 04:48:30 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.1699, average loss: 1.9587
[09/26 04:48:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 82.00	
[09/26 04:48:30 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 04:48:37 visual_prompt]: Epoch 56 / 100: avg data time: 6.23e-02, avg batch time: 0.5103, average train loss: 0.0190
[09/26 04:48:38 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1696, average loss: 1.9513
[09/26 04:48:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 82.50	
[09/26 04:48:38 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 04:48:45 visual_prompt]: Epoch 57 / 100: avg data time: 4.71e-02, avg batch time: 0.4969, average train loss: 0.0181
[09/26 04:48:46 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1694, average loss: 1.9484
[09/26 04:48:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 83.50	
[09/26 04:48:46 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 04:48:53 visual_prompt]: Epoch 58 / 100: avg data time: 4.79e-02, avg batch time: 0.4986, average train loss: 0.0173
[09/26 04:48:55 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1700, average loss: 1.9512
[09/26 04:48:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 82.00	
[09/26 04:48:55 visual_prompt]: Best epoch 58: best metric: 0.530
[09/26 04:48:55 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 04:49:02 visual_prompt]: Epoch 59 / 100: avg data time: 5.99e-02, avg batch time: 0.5086, average train loss: 0.0166
[09/26 04:49:03 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1695, average loss: 1.9617
[09/26 04:49:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 81.50	
[09/26 04:49:03 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 04:49:10 visual_prompt]: Epoch 60 / 100: avg data time: 5.05e-02, avg batch time: 0.5007, average train loss: 0.0165
[09/26 04:49:12 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1692, average loss: 1.9626
[09/26 04:49:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 82.00	
[09/26 04:49:12 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 04:49:19 visual_prompt]: Epoch 61 / 100: avg data time: 5.31e-02, avg batch time: 0.5033, average train loss: 0.0163
[09/26 04:49:20 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1696, average loss: 1.9660
[09/26 04:49:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 83.00	
[09/26 04:49:20 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 04:49:27 visual_prompt]: Epoch 62 / 100: avg data time: 5.47e-02, avg batch time: 0.5047, average train loss: 0.0158
[09/26 04:49:29 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1690, average loss: 1.9709
[09/26 04:49:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 83.00	
[09/26 04:49:29 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 04:49:35 visual_prompt]: Epoch 63 / 100: avg data time: 5.73e-02, avg batch time: 0.5057, average train loss: 0.0152
[09/26 04:49:37 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1695, average loss: 1.9749
[09/26 04:49:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 82.00	
[09/26 04:49:37 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 04:49:44 visual_prompt]: Epoch 64 / 100: avg data time: 5.98e-02, avg batch time: 0.5084, average train loss: 0.0155
[09/26 04:49:45 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1697, average loss: 1.9688
[09/26 04:49:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 83.00	
[09/26 04:49:45 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 04:49:52 visual_prompt]: Epoch 65 / 100: avg data time: 6.32e-02, avg batch time: 0.5121, average train loss: 0.0156
[09/26 04:49:54 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 1.9713
[09/26 04:49:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 83.00	
[09/26 04:49:54 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 04:50:01 visual_prompt]: Epoch 66 / 100: avg data time: 4.21e-02, avg batch time: 0.4928, average train loss: 0.0152
[09/26 04:50:02 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1697, average loss: 1.9743
[09/26 04:50:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.00	
[09/26 04:50:02 visual_prompt]: Best epoch 66: best metric: 0.535
[09/26 04:50:02 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 04:50:09 visual_prompt]: Epoch 67 / 100: avg data time: 5.24e-02, avg batch time: 0.5009, average train loss: 0.0153
[09/26 04:50:10 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1695, average loss: 1.9719
[09/26 04:50:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 83.00	
[09/26 04:50:10 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 04:50:17 visual_prompt]: Epoch 68 / 100: avg data time: 6.69e-02, avg batch time: 0.5151, average train loss: 0.0154
[09/26 04:50:19 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1698, average loss: 1.9838
[09/26 04:50:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 83.00	
[09/26 04:50:19 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 04:50:26 visual_prompt]: Epoch 69 / 100: avg data time: 4.99e-02, avg batch time: 0.4988, average train loss: 0.0152
[09/26 04:50:27 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1694, average loss: 1.9888
[09/26 04:50:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 83.50	
[09/26 04:50:27 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 04:50:34 visual_prompt]: Epoch 70 / 100: avg data time: 5.10e-02, avg batch time: 0.4995, average train loss: 0.0152
[09/26 04:50:35 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1693, average loss: 1.9904
[09/26 04:50:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.00	
[09/26 04:50:35 visual_prompt]: Best epoch 70: best metric: 0.540
[09/26 04:50:35 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 04:50:42 visual_prompt]: Epoch 71 / 100: avg data time: 4.83e-02, avg batch time: 0.4994, average train loss: 0.0152
[09/26 04:50:44 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1693, average loss: 1.9784
[09/26 04:50:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 82.50	
[09/26 04:50:44 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 04:50:50 visual_prompt]: Epoch 72 / 100: avg data time: 4.53e-02, avg batch time: 0.4955, average train loss: 0.0153
[09/26 04:50:52 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1692, average loss: 1.9748
[09/26 04:50:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:50:52 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 04:50:59 visual_prompt]: Epoch 73 / 100: avg data time: 5.39e-02, avg batch time: 0.5033, average train loss: 0.0152
[09/26 04:51:00 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1695, average loss: 1.9810
[09/26 04:51:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.00	
[09/26 04:51:00 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 04:51:07 visual_prompt]: Epoch 74 / 100: avg data time: 5.79e-02, avg batch time: 0.5060, average train loss: 0.0151
[09/26 04:51:09 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1699, average loss: 1.9767
[09/26 04:51:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 83.50	
[09/26 04:51:09 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 04:51:16 visual_prompt]: Epoch 75 / 100: avg data time: 5.56e-02, avg batch time: 0.5043, average train loss: 0.0153
[09/26 04:51:17 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 1.9735
[09/26 04:51:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 83.50	
[09/26 04:51:17 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 04:51:24 visual_prompt]: Epoch 76 / 100: avg data time: 5.44e-02, avg batch time: 0.5034, average train loss: 0.0150
[09/26 04:51:25 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1696, average loss: 1.9799
[09/26 04:51:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.50	
[09/26 04:51:25 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 04:51:32 visual_prompt]: Epoch 77 / 100: avg data time: 4.73e-02, avg batch time: 0.4975, average train loss: 0.0151
[09/26 04:51:34 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1701, average loss: 1.9709
[09/26 04:51:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.50	
[09/26 04:51:34 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 04:51:41 visual_prompt]: Epoch 78 / 100: avg data time: 5.55e-02, avg batch time: 0.5038, average train loss: 0.0152
[09/26 04:51:42 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1698, average loss: 1.9754
[09/26 04:51:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.50	
[09/26 04:51:42 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 04:51:49 visual_prompt]: Epoch 79 / 100: avg data time: 4.76e-02, avg batch time: 0.4982, average train loss: 0.0149
[09/26 04:51:50 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 1.9763
[09/26 04:51:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.50	
[09/26 04:51:50 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 04:51:57 visual_prompt]: Epoch 80 / 100: avg data time: 4.74e-02, avg batch time: 0.4972, average train loss: 0.0153
[09/26 04:51:59 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1696, average loss: 1.9777
[09/26 04:51:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.50	
[09/26 04:51:59 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 04:52:06 visual_prompt]: Epoch 81 / 100: avg data time: 5.58e-02, avg batch time: 0.5065, average train loss: 0.0150
[09/26 04:52:07 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 1.9864
[09/26 04:52:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 83.50	
[09/26 04:52:07 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 04:52:14 visual_prompt]: Epoch 82 / 100: avg data time: 4.66e-02, avg batch time: 0.4945, average train loss: 0.0150
[09/26 04:52:15 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 1.9892
[09/26 04:52:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:52:15 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 04:52:22 visual_prompt]: Epoch 83 / 100: avg data time: 4.60e-02, avg batch time: 0.4966, average train loss: 0.0151
[09/26 04:52:24 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1700, average loss: 1.9834
[09/26 04:52:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:52:24 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 04:52:31 visual_prompt]: Epoch 84 / 100: avg data time: 5.05e-02, avg batch time: 0.5011, average train loss: 0.0153
[09/26 04:52:32 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 1.9804
[09/26 04:52:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:52:32 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 04:52:39 visual_prompt]: Epoch 85 / 100: avg data time: 5.91e-02, avg batch time: 0.5072, average train loss: 0.0150
[09/26 04:52:41 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1694, average loss: 1.9809
[09/26 04:52:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 83.50	
[09/26 04:52:41 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 04:52:48 visual_prompt]: Epoch 86 / 100: avg data time: 5.89e-02, avg batch time: 0.5084, average train loss: 0.0150
[09/26 04:52:49 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 1.9816
[09/26 04:52:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:52:49 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 04:52:56 visual_prompt]: Epoch 87 / 100: avg data time: 5.63e-02, avg batch time: 0.5055, average train loss: 0.0151
[09/26 04:52:58 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1692, average loss: 1.9831
[09/26 04:52:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:52:58 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 04:53:04 visual_prompt]: Epoch 88 / 100: avg data time: 5.96e-02, avg batch time: 0.5100, average train loss: 0.0147
[09/26 04:53:06 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1694, average loss: 1.9841
[09/26 04:53:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:53:06 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 04:53:13 visual_prompt]: Epoch 89 / 100: avg data time: 5.75e-02, avg batch time: 0.5065, average train loss: 0.0153
[09/26 04:53:15 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1695, average loss: 1.9824
[09/26 04:53:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:53:15 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 04:53:21 visual_prompt]: Epoch 90 / 100: avg data time: 5.58e-02, avg batch time: 0.5041, average train loss: 0.0150
[09/26 04:53:23 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 1.9800
[09/26 04:53:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:53:23 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 04:53:30 visual_prompt]: Epoch 91 / 100: avg data time: 5.95e-02, avg batch time: 0.5081, average train loss: 0.0149
[09/26 04:53:31 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1696, average loss: 1.9804
[09/26 04:53:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:53:31 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 04:53:38 visual_prompt]: Epoch 92 / 100: avg data time: 6.22e-02, avg batch time: 0.5106, average train loss: 0.0151
[09/26 04:53:40 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 1.9814
[09/26 04:53:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:53:40 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 04:53:47 visual_prompt]: Epoch 93 / 100: avg data time: 4.79e-02, avg batch time: 0.4980, average train loss: 0.0149
[09/26 04:53:48 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 1.9818
[09/26 04:53:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:53:48 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 04:53:55 visual_prompt]: Epoch 94 / 100: avg data time: 5.25e-02, avg batch time: 0.5006, average train loss: 0.0148
[09/26 04:53:57 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 1.9824
[09/26 04:53:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:53:57 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 04:54:03 visual_prompt]: Epoch 95 / 100: avg data time: 5.75e-02, avg batch time: 0.5070, average train loss: 0.0147
[09/26 04:54:05 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1696, average loss: 1.9827
[09/26 04:54:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:54:05 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 04:54:12 visual_prompt]: Epoch 96 / 100: avg data time: 5.29e-02, avg batch time: 0.5015, average train loss: 0.0150
[09/26 04:54:13 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 1.9832
[09/26 04:54:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:54:13 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 04:54:20 visual_prompt]: Epoch 97 / 100: avg data time: 6.39e-02, avg batch time: 0.5123, average train loss: 0.0150
[09/26 04:54:22 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1699, average loss: 1.9833
[09/26 04:54:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:54:22 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 04:54:29 visual_prompt]: Epoch 98 / 100: avg data time: 5.95e-02, avg batch time: 0.5074, average train loss: 0.0147
[09/26 04:54:30 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1695, average loss: 1.9834
[09/26 04:54:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:54:30 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 04:54:37 visual_prompt]: Epoch 99 / 100: avg data time: 5.45e-02, avg batch time: 0.5029, average train loss: 0.0150
[09/26 04:54:39 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1697, average loss: 1.9835
[09/26 04:54:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:54:39 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 04:54:46 visual_prompt]: Epoch 100 / 100: avg data time: 5.79e-02, avg batch time: 0.5060, average train loss: 0.0147
[09/26 04:54:47 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1696, average loss: 1.9835
[09/26 04:54:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 83.50	
[09/26 04:54:47 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 04:54:47 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 04:54:47 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 04:54:47 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 04:54:47 visual_prompt]: Training with config:
[09/26 04:54:47 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 04:54:47 visual_prompt]: Loading training data...
[09/26 04:54:47 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 04:54:49 visual_prompt]: Number of images: 800
[09/26 04:54:49 visual_prompt]: Number of classes: 47 / 47
[09/26 04:54:49 visual_prompt]: Loading validation data...
[09/26 04:54:49 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 04:54:50 visual_prompt]: Number of images: 200
[09/26 04:54:50 visual_prompt]: Number of classes: 47 / 47
[09/26 04:54:50 visual_prompt]: Constructing models...
[09/26 04:54:52 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 04:54:52 visual_prompt]: tuned percent:0.576
[09/26 04:54:52 visual_prompt]: Device used for model: 0
[09/26 04:54:52 visual_prompt]: Setting up Evaluator...
[09/26 04:54:52 visual_prompt]: Setting up Trainer...
[09/26 04:54:52 visual_prompt]: 	Setting up the optimizer...
[09/26 04:54:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 04:54:59 visual_prompt]: Epoch 1 / 100: avg data time: 5.87e-02, avg batch time: 0.5059, average train loss: 3.9354
[09/26 04:55:01 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1691, average loss: 3.9045
[09/26 04:55:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 04:55:01 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 04:55:01 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 04:55:08 visual_prompt]: Epoch 2 / 100: avg data time: 5.65e-02, avg batch time: 0.5031, average train loss: 3.8890
[09/26 04:55:09 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1688, average loss: 3.8864
[09/26 04:55:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 10.50	
[09/26 04:55:09 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 04:55:16 visual_prompt]: Epoch 3 / 100: avg data time: 5.20e-02, avg batch time: 0.5001, average train loss: 3.8392
[09/26 04:55:18 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 3.9068
[09/26 04:55:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 9.50	
[09/26 04:55:18 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 04:55:24 visual_prompt]: Epoch 4 / 100: avg data time: 4.52e-02, avg batch time: 0.4967, average train loss: 3.7874
[09/26 04:55:26 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1691, average loss: 3.8409
[09/26 04:55:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 17.00	
[09/26 04:55:26 visual_prompt]: Best epoch 4: best metric: 0.045
[09/26 04:55:26 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 04:55:33 visual_prompt]: Epoch 5 / 100: avg data time: 5.11e-02, avg batch time: 0.4993, average train loss: 3.6825
[09/26 04:55:34 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1687, average loss: 3.6072
[09/26 04:55:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.50	top5: 25.00	
[09/26 04:55:34 visual_prompt]: Best epoch 5: best metric: 0.105
[09/26 04:55:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 04:55:41 visual_prompt]: Epoch 6 / 100: avg data time: 5.73e-02, avg batch time: 0.5063, average train loss: 3.3123
[09/26 04:55:43 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1694, average loss: 4.5526
[09/26 04:55:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 23.50	
[09/26 04:55:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 04:55:50 visual_prompt]: Epoch 7 / 100: avg data time: 4.81e-02, avg batch time: 0.5346, average train loss: 3.5042
[09/26 04:55:51 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 3.4974
[09/26 04:55:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 35.00	
[09/26 04:55:51 visual_prompt]: Best epoch 7: best metric: 0.110
[09/26 04:55:51 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 04:55:58 visual_prompt]: Epoch 8 / 100: avg data time: 4.61e-02, avg batch time: 0.4940, average train loss: 2.9464
[09/26 04:56:00 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1693, average loss: 3.2490
[09/26 04:56:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 18.00	top5: 46.50	
[09/26 04:56:00 visual_prompt]: Best epoch 8: best metric: 0.180
[09/26 04:56:00 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 04:56:07 visual_prompt]: Epoch 9 / 100: avg data time: 5.99e-02, avg batch time: 0.5073, average train loss: 2.3246
[09/26 04:56:08 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1690, average loss: 2.6871
[09/26 04:56:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.50	top5: 58.00	
[09/26 04:56:08 visual_prompt]: Best epoch 9: best metric: 0.305
[09/26 04:56:08 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 04:56:15 visual_prompt]: Epoch 10 / 100: avg data time: 4.66e-02, avg batch time: 0.4970, average train loss: 1.6482
[09/26 04:56:16 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1696, average loss: 2.6768
[09/26 04:56:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 28.00	top5: 67.00	
[09/26 04:56:16 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 04:56:23 visual_prompt]: Epoch 11 / 100: avg data time: 4.44e-02, avg batch time: 0.4924, average train loss: 1.1209
[09/26 04:56:24 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 2.4840
[09/26 04:56:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 36.00	top5: 69.50	
[09/26 04:56:24 visual_prompt]: Best epoch 11: best metric: 0.360
[09/26 04:56:24 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 04:56:31 visual_prompt]: Epoch 12 / 100: avg data time: 5.92e-02, avg batch time: 0.5069, average train loss: 0.6686
[09/26 04:56:33 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1696, average loss: 2.0244
[09/26 04:56:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 78.50	
[09/26 04:56:33 visual_prompt]: Best epoch 12: best metric: 0.465
[09/26 04:56:33 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 04:56:40 visual_prompt]: Epoch 13 / 100: avg data time: 4.63e-02, avg batch time: 0.4968, average train loss: 0.4074
[09/26 04:56:41 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1696, average loss: 2.1083
[09/26 04:56:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 76.50	
[09/26 04:56:41 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 04:56:48 visual_prompt]: Epoch 14 / 100: avg data time: 5.12e-02, avg batch time: 0.4997, average train loss: 0.2105
[09/26 04:56:49 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1694, average loss: 2.0828
[09/26 04:56:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 79.00	
[09/26 04:56:49 visual_prompt]: Best epoch 14: best metric: 0.495
[09/26 04:56:49 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 04:56:56 visual_prompt]: Epoch 15 / 100: avg data time: 5.63e-02, avg batch time: 0.5049, average train loss: 0.1144
[09/26 04:56:58 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1697, average loss: 2.1133
[09/26 04:56:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 80.00	
[09/26 04:56:58 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 04:57:04 visual_prompt]: Epoch 16 / 100: avg data time: 4.51e-02, avg batch time: 0.4949, average train loss: 0.0653
[09/26 04:57:06 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1698, average loss: 2.1147
[09/26 04:57:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 82.00	
[09/26 04:57:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 04:57:13 visual_prompt]: Epoch 17 / 100: avg data time: 5.79e-02, avg batch time: 0.5058, average train loss: 0.0454
[09/26 04:57:14 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1695, average loss: 2.1854
[09/26 04:57:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 80.50	
[09/26 04:57:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 04:57:21 visual_prompt]: Epoch 18 / 100: avg data time: 5.81e-02, avg batch time: 0.5067, average train loss: 0.0319
[09/26 04:57:23 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 2.2083
[09/26 04:57:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 80.00	
[09/26 04:57:23 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 04:57:30 visual_prompt]: Epoch 19 / 100: avg data time: 4.76e-02, avg batch time: 0.4972, average train loss: 0.0244
[09/26 04:57:31 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1696, average loss: 2.1623
[09/26 04:57:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.50	
[09/26 04:57:31 visual_prompt]: Best epoch 19: best metric: 0.515
[09/26 04:57:31 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 04:57:38 visual_prompt]: Epoch 20 / 100: avg data time: 5.37e-02, avg batch time: 0.5024, average train loss: 0.0192
[09/26 04:57:40 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1695, average loss: 2.1833
[09/26 04:57:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 79.00	
[09/26 04:57:40 visual_prompt]: Best epoch 20: best metric: 0.525
[09/26 04:57:40 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 04:57:46 visual_prompt]: Epoch 21 / 100: avg data time: 4.44e-02, avg batch time: 0.4946, average train loss: 0.0155
[09/26 04:57:48 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1695, average loss: 2.2067
[09/26 04:57:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.00	
[09/26 04:57:48 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 04:57:55 visual_prompt]: Epoch 22 / 100: avg data time: 4.72e-02, avg batch time: 0.4955, average train loss: 0.0144
[09/26 04:57:56 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1693, average loss: 2.2139
[09/26 04:57:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 80.50	
[09/26 04:57:56 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 04:58:03 visual_prompt]: Epoch 23 / 100: avg data time: 4.78e-02, avg batch time: 0.4966, average train loss: 0.0126
[09/26 04:58:04 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1695, average loss: 2.2142
[09/26 04:58:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 80.50	
[09/26 04:58:04 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 04:58:11 visual_prompt]: Epoch 24 / 100: avg data time: 4.71e-02, avg batch time: 0.4968, average train loss: 0.0120
[09/26 04:58:13 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1695, average loss: 2.2075
[09/26 04:58:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 80.50	
[09/26 04:58:13 visual_prompt]: Best epoch 24: best metric: 0.530
[09/26 04:58:13 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 04:58:20 visual_prompt]: Epoch 25 / 100: avg data time: 5.99e-02, avg batch time: 0.5080, average train loss: 0.0111
[09/26 04:58:21 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1697, average loss: 2.2227
[09/26 04:58:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.50	
[09/26 04:58:21 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 04:58:28 visual_prompt]: Epoch 26 / 100: avg data time: 6.23e-02, avg batch time: 0.5109, average train loss: 0.0109
[09/26 04:58:30 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1693, average loss: 2.2531
[09/26 04:58:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 80.50	
[09/26 04:58:30 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 04:58:37 visual_prompt]: Epoch 27 / 100: avg data time: 6.05e-02, avg batch time: 0.5098, average train loss: 0.0097
[09/26 04:58:38 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1696, average loss: 2.2724
[09/26 04:58:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 81.00	
[09/26 04:58:38 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 04:58:45 visual_prompt]: Epoch 28 / 100: avg data time: 4.60e-02, avg batch time: 0.4966, average train loss: 0.0096
[09/26 04:58:47 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1696, average loss: 2.2344
[09/26 04:58:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 81.50	
[09/26 04:58:47 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 04:58:54 visual_prompt]: Epoch 29 / 100: avg data time: 5.68e-02, avg batch time: 0.5050, average train loss: 0.0086
[09/26 04:58:55 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1698, average loss: 2.2499
[09/26 04:58:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 81.00	
[09/26 04:58:55 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 04:59:02 visual_prompt]: Epoch 30 / 100: avg data time: 5.26e-02, avg batch time: 0.5016, average train loss: 0.0083
[09/26 04:59:03 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1694, average loss: 2.2700
[09/26 04:59:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 81.50	
[09/26 04:59:03 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 04:59:10 visual_prompt]: Epoch 31 / 100: avg data time: 4.73e-02, avg batch time: 0.4965, average train loss: 0.0078
[09/26 04:59:12 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1695, average loss: 2.2765
[09/26 04:59:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 04:59:12 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 04:59:18 visual_prompt]: Epoch 32 / 100: avg data time: 4.59e-02, avg batch time: 0.4947, average train loss: 0.0075
[09/26 04:59:20 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1694, average loss: 2.2769
[09/26 04:59:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 82.00	
[09/26 04:59:20 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 04:59:27 visual_prompt]: Epoch 33 / 100: avg data time: 5.84e-02, avg batch time: 0.5080, average train loss: 0.0070
[09/26 04:59:28 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 2.2950
[09/26 04:59:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.50	
[09/26 04:59:28 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 04:59:35 visual_prompt]: Epoch 34 / 100: avg data time: 6.12e-02, avg batch time: 0.5093, average train loss: 0.0069
[09/26 04:59:37 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1698, average loss: 2.2829
[09/26 04:59:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 80.50	
[09/26 04:59:37 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 04:59:44 visual_prompt]: Epoch 35 / 100: avg data time: 5.82e-02, avg batch time: 0.5064, average train loss: 0.0069
[09/26 04:59:45 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1696, average loss: 2.2792
[09/26 04:59:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 80.50	
[09/26 04:59:45 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 04:59:52 visual_prompt]: Epoch 36 / 100: avg data time: 6.00e-02, avg batch time: 0.5103, average train loss: 0.0065
[09/26 04:59:54 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.1696, average loss: 2.2882
[09/26 04:59:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 81.50	
[09/26 04:59:54 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 05:00:01 visual_prompt]: Epoch 37 / 100: avg data time: 5.17e-02, avg batch time: 0.5017, average train loss: 0.0064
[09/26 05:00:02 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1695, average loss: 2.2720
[09/26 05:00:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 80.00	
[09/26 05:00:02 visual_prompt]: Best epoch 37: best metric: 0.535
[09/26 05:00:02 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 05:00:09 visual_prompt]: Epoch 38 / 100: avg data time: 6.04e-02, avg batch time: 0.5105, average train loss: 0.0062
[09/26 05:00:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 2.2993
[09/26 05:00:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 81.50	
[09/26 05:00:11 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 05:00:17 visual_prompt]: Epoch 39 / 100: avg data time: 4.80e-02, avg batch time: 0.4974, average train loss: 0.0059
[09/26 05:00:19 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 2.3165
[09/26 05:00:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 80.00	
[09/26 05:00:19 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 05:00:26 visual_prompt]: Epoch 40 / 100: avg data time: 5.16e-02, avg batch time: 0.5006, average train loss: 0.0055
[09/26 05:00:27 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1689, average loss: 2.3105
[09/26 05:00:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.00	
[09/26 05:00:27 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 05:00:34 visual_prompt]: Epoch 41 / 100: avg data time: 6.14e-02, avg batch time: 0.5101, average train loss: 0.0058
[09/26 05:00:36 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1692, average loss: 2.2985
[09/26 05:00:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 80.00	
[09/26 05:00:36 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 05:00:43 visual_prompt]: Epoch 42 / 100: avg data time: 5.76e-02, avg batch time: 0.5060, average train loss: 0.0054
[09/26 05:00:44 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1695, average loss: 2.3021
[09/26 05:00:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 80.50	
[09/26 05:00:44 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 05:00:51 visual_prompt]: Epoch 43 / 100: avg data time: 4.51e-02, avg batch time: 0.4953, average train loss: 0.0052
[09/26 05:00:53 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1692, average loss: 2.3142
[09/26 05:00:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 81.00	
[09/26 05:00:53 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 05:00:59 visual_prompt]: Epoch 44 / 100: avg data time: 5.86e-02, avg batch time: 0.5066, average train loss: 0.0054
[09/26 05:01:01 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 2.3333
[09/26 05:01:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 81.00	
[09/26 05:01:01 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 05:01:08 visual_prompt]: Epoch 45 / 100: avg data time: 4.53e-02, avg batch time: 0.4948, average train loss: 0.0050
[09/26 05:01:09 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1695, average loss: 2.3246
[09/26 05:01:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 81.00	
[09/26 05:01:09 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 05:01:16 visual_prompt]: Epoch 46 / 100: avg data time: 5.70e-02, avg batch time: 0.5064, average train loss: 0.0049
[09/26 05:01:18 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1694, average loss: 2.3185
[09/26 05:01:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 80.50	
[09/26 05:01:18 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 05:01:25 visual_prompt]: Epoch 47 / 100: avg data time: 4.38e-02, avg batch time: 0.4952, average train loss: 0.0049
[09/26 05:01:26 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1696, average loss: 2.3147
[09/26 05:01:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.50	
[09/26 05:01:26 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 05:01:33 visual_prompt]: Epoch 48 / 100: avg data time: 5.50e-02, avg batch time: 0.5033, average train loss: 0.0048
[09/26 05:01:34 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1697, average loss: 2.3254
[09/26 05:01:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 79.50	
[09/26 05:01:34 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 05:01:41 visual_prompt]: Epoch 49 / 100: avg data time: 5.18e-02, avg batch time: 0.5002, average train loss: 0.0047
[09/26 05:01:43 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1694, average loss: 2.3278
[09/26 05:01:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 79.00	
[09/26 05:01:43 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 05:01:49 visual_prompt]: Epoch 50 / 100: avg data time: 5.20e-02, avg batch time: 0.5019, average train loss: 0.0048
[09/26 05:01:51 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1696, average loss: 2.3313
[09/26 05:01:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 80.00	
[09/26 05:01:51 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 05:01:58 visual_prompt]: Epoch 51 / 100: avg data time: 5.92e-02, avg batch time: 0.5073, average train loss: 0.0048
[09/26 05:01:59 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 2.3377
[09/26 05:01:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:01:59 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 05:02:06 visual_prompt]: Epoch 52 / 100: avg data time: 4.59e-02, avg batch time: 0.4957, average train loss: 0.0048
[09/26 05:02:08 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1701, average loss: 2.3407
[09/26 05:02:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 80.00	
[09/26 05:02:08 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 05:02:15 visual_prompt]: Epoch 53 / 100: avg data time: 5.95e-02, avg batch time: 0.5088, average train loss: 0.0044
[09/26 05:02:16 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 2.3416
[09/26 05:02:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 79.50	
[09/26 05:02:16 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 05:02:23 visual_prompt]: Epoch 54 / 100: avg data time: 4.41e-02, avg batch time: 0.4938, average train loss: 0.0046
[09/26 05:02:24 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 2.3419
[09/26 05:02:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 79.50	
[09/26 05:02:24 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 05:02:31 visual_prompt]: Epoch 55 / 100: avg data time: 5.67e-02, avg batch time: 0.5058, average train loss: 0.0046
[09/26 05:02:33 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 2.3459
[09/26 05:02:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 79.50	
[09/26 05:02:33 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 05:02:40 visual_prompt]: Epoch 56 / 100: avg data time: 5.39e-02, avg batch time: 0.5038, average train loss: 0.0043
[09/26 05:02:41 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1696, average loss: 2.3357
[09/26 05:02:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 79.50	
[09/26 05:02:41 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 05:02:48 visual_prompt]: Epoch 57 / 100: avg data time: 4.75e-02, avg batch time: 0.4973, average train loss: 0.0044
[09/26 05:02:50 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1697, average loss: 2.3305
[09/26 05:02:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 80.00	
[09/26 05:02:50 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 05:02:56 visual_prompt]: Epoch 58 / 100: avg data time: 4.40e-02, avg batch time: 0.4936, average train loss: 0.0046
[09/26 05:02:58 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1696, average loss: 2.3350
[09/26 05:02:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 80.00	
[09/26 05:02:58 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 05:03:05 visual_prompt]: Epoch 59 / 100: avg data time: 5.56e-02, avg batch time: 0.5038, average train loss: 0.0045
[09/26 05:03:06 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1692, average loss: 2.3459
[09/26 05:03:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.50	
[09/26 05:03:06 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 05:03:13 visual_prompt]: Epoch 60 / 100: avg data time: 5.56e-02, avg batch time: 0.5060, average train loss: 0.0041
[09/26 05:03:15 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 2.3454
[09/26 05:03:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 81.00	
[09/26 05:03:15 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 05:03:21 visual_prompt]: Epoch 61 / 100: avg data time: 5.86e-02, avg batch time: 0.5082, average train loss: 0.0041
[09/26 05:03:23 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 2.3404
[09/26 05:03:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 81.00	
[09/26 05:03:23 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 05:03:30 visual_prompt]: Epoch 62 / 100: avg data time: 5.63e-02, avg batch time: 0.5068, average train loss: 0.0042
[09/26 05:03:31 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 2.3386
[09/26 05:03:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 81.00	
[09/26 05:03:31 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 05:03:38 visual_prompt]: Epoch 63 / 100: avg data time: 5.14e-02, avg batch time: 0.5014, average train loss: 0.0040
[09/26 05:03:40 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1691, average loss: 2.3410
[09/26 05:03:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.50	
[09/26 05:03:40 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 05:03:47 visual_prompt]: Epoch 64 / 100: avg data time: 4.65e-02, avg batch time: 0.4945, average train loss: 0.0040
[09/26 05:03:48 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1695, average loss: 2.3482
[09/26 05:03:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 80.50	
[09/26 05:03:48 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 05:03:55 visual_prompt]: Epoch 65 / 100: avg data time: 5.37e-02, avg batch time: 0.5025, average train loss: 0.0041
[09/26 05:03:56 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1694, average loss: 2.3461
[09/26 05:03:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.50	
[09/26 05:03:56 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 05:04:03 visual_prompt]: Epoch 66 / 100: avg data time: 5.15e-02, avg batch time: 0.5003, average train loss: 0.0041
[09/26 05:04:05 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1698, average loss: 2.3441
[09/26 05:04:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 80.50	
[09/26 05:04:05 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 05:04:12 visual_prompt]: Epoch 67 / 100: avg data time: 4.92e-02, avg batch time: 0.4981, average train loss: 0.0041
[09/26 05:04:13 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1699, average loss: 2.3456
[09/26 05:04:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 81.00	
[09/26 05:04:13 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 05:04:20 visual_prompt]: Epoch 68 / 100: avg data time: 5.34e-02, avg batch time: 0.5024, average train loss: 0.0040
[09/26 05:04:22 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1695, average loss: 2.3541
[09/26 05:04:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 81.00	
[09/26 05:04:22 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 05:04:28 visual_prompt]: Epoch 69 / 100: avg data time: 5.62e-02, avg batch time: 0.5050, average train loss: 0.0040
[09/26 05:04:30 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1698, average loss: 2.3594
[09/26 05:04:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 81.00	
[09/26 05:04:30 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 05:04:37 visual_prompt]: Epoch 70 / 100: avg data time: 5.13e-02, avg batch time: 0.5016, average train loss: 0.0040
[09/26 05:04:38 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 2.3592
[09/26 05:04:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 81.00	
[09/26 05:04:38 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 05:04:45 visual_prompt]: Epoch 71 / 100: avg data time: 5.91e-02, avg batch time: 0.5078, average train loss: 0.0038
[09/26 05:04:47 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1691, average loss: 2.3581
[09/26 05:04:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 81.00	
[09/26 05:04:47 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 05:04:54 visual_prompt]: Epoch 72 / 100: avg data time: 5.52e-02, avg batch time: 0.5055, average train loss: 0.0040
[09/26 05:04:55 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 2.3579
[09/26 05:04:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:04:55 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 05:05:02 visual_prompt]: Epoch 73 / 100: avg data time: 4.83e-02, avg batch time: 0.4983, average train loss: 0.0040
[09/26 05:05:04 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1690, average loss: 2.3561
[09/26 05:05:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 81.50	
[09/26 05:05:04 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 05:05:10 visual_prompt]: Epoch 74 / 100: avg data time: 5.25e-02, avg batch time: 0.5016, average train loss: 0.0039
[09/26 05:05:12 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 2.3542
[09/26 05:05:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 81.50	
[09/26 05:05:12 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 05:05:19 visual_prompt]: Epoch 75 / 100: avg data time: 4.22e-02, avg batch time: 0.4936, average train loss: 0.0039
[09/26 05:05:20 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1696, average loss: 2.3542
[09/26 05:05:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 81.50	
[09/26 05:05:20 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 05:05:27 visual_prompt]: Epoch 76 / 100: avg data time: 4.49e-02, avg batch time: 0.4933, average train loss: 0.0038
[09/26 05:05:29 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1698, average loss: 2.3552
[09/26 05:05:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 81.00	
[09/26 05:05:29 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 05:05:35 visual_prompt]: Epoch 77 / 100: avg data time: 4.87e-02, avg batch time: 0.4976, average train loss: 0.0038
[09/26 05:05:37 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1693, average loss: 2.3558
[09/26 05:05:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 81.00	
[09/26 05:05:37 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 05:05:44 visual_prompt]: Epoch 78 / 100: avg data time: 5.06e-02, avg batch time: 0.5003, average train loss: 0.0037
[09/26 05:05:45 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 2.3552
[09/26 05:05:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 81.00	
[09/26 05:05:45 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 05:05:52 visual_prompt]: Epoch 79 / 100: avg data time: 4.18e-02, avg batch time: 0.4911, average train loss: 0.0039
[09/26 05:05:53 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1694, average loss: 2.3572
[09/26 05:05:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:05:53 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 05:06:00 visual_prompt]: Epoch 80 / 100: avg data time: 5.34e-02, avg batch time: 0.5029, average train loss: 0.0038
[09/26 05:06:02 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1696, average loss: 2.3580
[09/26 05:06:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:06:02 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 05:06:09 visual_prompt]: Epoch 81 / 100: avg data time: 5.14e-02, avg batch time: 0.5005, average train loss: 0.0037
[09/26 05:06:10 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1695, average loss: 2.3579
[09/26 05:06:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:06:10 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 05:06:17 visual_prompt]: Epoch 82 / 100: avg data time: 5.23e-02, avg batch time: 0.5022, average train loss: 0.0038
[09/26 05:06:18 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1697, average loss: 2.3590
[09/26 05:06:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:06:18 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 05:06:25 visual_prompt]: Epoch 83 / 100: avg data time: 4.98e-02, avg batch time: 0.4996, average train loss: 0.0036
[09/26 05:06:27 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1697, average loss: 2.3590
[09/26 05:06:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:06:27 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 05:06:34 visual_prompt]: Epoch 84 / 100: avg data time: 4.96e-02, avg batch time: 0.4999, average train loss: 0.0037
[09/26 05:06:35 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1695, average loss: 2.3584
[09/26 05:06:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:06:35 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 05:06:42 visual_prompt]: Epoch 85 / 100: avg data time: 4.59e-02, avg batch time: 0.4943, average train loss: 0.0037
[09/26 05:06:43 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1695, average loss: 2.3589
[09/26 05:06:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:06:43 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 05:06:50 visual_prompt]: Epoch 86 / 100: avg data time: 5.02e-02, avg batch time: 0.4999, average train loss: 0.0040
[09/26 05:06:52 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1698, average loss: 2.3590
[09/26 05:06:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:06:52 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 05:06:59 visual_prompt]: Epoch 87 / 100: avg data time: 5.98e-02, avg batch time: 0.5081, average train loss: 0.0038
[09/26 05:07:00 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 2.3594
[09/26 05:07:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:07:00 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 05:07:07 visual_prompt]: Epoch 88 / 100: avg data time: 6.26e-02, avg batch time: 0.5112, average train loss: 0.0038
[09/26 05:07:08 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 2.3602
[09/26 05:07:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 05:07:08 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 05:07:15 visual_prompt]: Epoch 89 / 100: avg data time: 5.63e-02, avg batch time: 0.5062, average train loss: 0.0038
[09/26 05:07:17 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1691, average loss: 2.3603
[09/26 05:07:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:07:17 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 05:07:24 visual_prompt]: Epoch 90 / 100: avg data time: 6.12e-02, avg batch time: 0.5099, average train loss: 0.0039
[09/26 05:07:25 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1696, average loss: 2.3596
[09/26 05:07:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:07:25 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 05:07:32 visual_prompt]: Epoch 91 / 100: avg data time: 5.54e-02, avg batch time: 0.5033, average train loss: 0.0036
[09/26 05:07:34 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 2.3595
[09/26 05:07:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:07:34 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 05:07:40 visual_prompt]: Epoch 92 / 100: avg data time: 5.06e-02, avg batch time: 0.5000, average train loss: 0.0037
[09/26 05:07:42 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1694, average loss: 2.3601
[09/26 05:07:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:07:42 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 05:07:49 visual_prompt]: Epoch 93 / 100: avg data time: 5.16e-02, avg batch time: 0.5017, average train loss: 0.0036
[09/26 05:07:50 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 2.3602
[09/26 05:07:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:07:50 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 05:07:57 visual_prompt]: Epoch 94 / 100: avg data time: 5.58e-02, avg batch time: 0.5051, average train loss: 0.0039
[09/26 05:07:59 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1692, average loss: 2.3602
[09/26 05:07:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:07:59 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 05:08:06 visual_prompt]: Epoch 95 / 100: avg data time: 6.00e-02, avg batch time: 0.5098, average train loss: 0.0038
[09/26 05:08:07 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1692, average loss: 2.3600
[09/26 05:08:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:08:07 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 05:08:14 visual_prompt]: Epoch 96 / 100: avg data time: 4.72e-02, avg batch time: 0.4972, average train loss: 0.0040
[09/26 05:08:16 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1694, average loss: 2.3599
[09/26 05:08:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:08:16 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 05:08:22 visual_prompt]: Epoch 97 / 100: avg data time: 5.42e-02, avg batch time: 0.5037, average train loss: 0.0036
[09/26 05:08:24 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 2.3599
[09/26 05:08:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:08:24 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 05:08:31 visual_prompt]: Epoch 98 / 100: avg data time: 5.26e-02, avg batch time: 0.5031, average train loss: 0.0038
[09/26 05:08:32 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1696, average loss: 2.3599
[09/26 05:08:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:08:32 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 05:08:39 visual_prompt]: Epoch 99 / 100: avg data time: 5.41e-02, avg batch time: 0.5032, average train loss: 0.0038
[09/26 05:08:41 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1695, average loss: 2.3599
[09/26 05:08:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:08:41 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 05:08:48 visual_prompt]: Epoch 100 / 100: avg data time: 6.17e-02, avg batch time: 0.5107, average train loss: 0.0036
[09/26 05:08:49 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 2.3599
[09/26 05:08:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.00	
[09/26 05:08:49 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 05:08:49 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 05:08:49 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 05:08:49 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 05:08:49 visual_prompt]: Training with config:
[09/26 05:08:49 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 05:08:49 visual_prompt]: Loading training data...
[09/26 05:08:49 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 05:08:51 visual_prompt]: Number of images: 800
[09/26 05:08:51 visual_prompt]: Number of classes: 47 / 47
[09/26 05:08:51 visual_prompt]: Loading validation data...
[09/26 05:08:51 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 05:08:52 visual_prompt]: Number of images: 200
[09/26 05:08:52 visual_prompt]: Number of classes: 47 / 47
[09/26 05:08:52 visual_prompt]: Constructing models...
[09/26 05:08:54 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 05:08:54 visual_prompt]: tuned percent:0.576
[09/26 05:08:54 visual_prompt]: Device used for model: 0
[09/26 05:08:54 visual_prompt]: Setting up Evaluator...
[09/26 05:08:54 visual_prompt]: Setting up Trainer...
[09/26 05:08:54 visual_prompt]: 	Setting up the optimizer...
[09/26 05:08:54 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 05:09:01 visual_prompt]: Epoch 1 / 100: avg data time: 4.40e-02, avg batch time: 0.4918, average train loss: 3.9284
[09/26 05:09:03 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1688, average loss: 3.9045
[09/26 05:09:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 05:09:03 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 05:09:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 05:09:10 visual_prompt]: Epoch 2 / 100: avg data time: 6.04e-02, avg batch time: 0.5069, average train loss: 3.8942
[09/26 05:09:11 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1691, average loss: 3.8802
[09/26 05:09:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 10.00	
[09/26 05:09:11 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 05:09:18 visual_prompt]: Epoch 3 / 100: avg data time: 4.74e-02, avg batch time: 0.4952, average train loss: 3.8067
[09/26 05:09:19 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 3.8204
[09/26 05:09:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 16.50	
[09/26 05:09:19 visual_prompt]: Best epoch 3: best metric: 0.060
[09/26 05:09:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 05:09:26 visual_prompt]: Epoch 4 / 100: avg data time: 5.50e-02, avg batch time: 0.5028, average train loss: 3.6318
[09/26 05:09:28 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 3.6269
[09/26 05:09:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.00	top5: 21.50	
[09/26 05:09:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 05:09:34 visual_prompt]: Epoch 5 / 100: avg data time: 4.86e-02, avg batch time: 0.4971, average train loss: 3.2692
[09/26 05:09:36 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1690, average loss: 3.2715
[09/26 05:09:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.00	top5: 40.00	
[09/26 05:09:36 visual_prompt]: Best epoch 5: best metric: 0.150
[09/26 05:09:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 05:09:43 visual_prompt]: Epoch 6 / 100: avg data time: 5.43e-02, avg batch time: 0.5022, average train loss: 2.9310
[09/26 05:09:44 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1691, average loss: 3.2780
[09/26 05:09:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 19.00	top5: 45.50	
[09/26 05:09:44 visual_prompt]: Best epoch 6: best metric: 0.190
[09/26 05:09:44 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 05:09:51 visual_prompt]: Epoch 7 / 100: avg data time: 5.20e-02, avg batch time: 0.5000, average train loss: 2.3099
[09/26 05:09:53 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 2.5491
[09/26 05:09:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.00	top5: 63.50	
[09/26 05:09:53 visual_prompt]: Best epoch 7: best metric: 0.300
[09/26 05:09:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 05:09:59 visual_prompt]: Epoch 8 / 100: avg data time: 4.56e-02, avg batch time: 0.4957, average train loss: 1.8828
[09/26 05:10:01 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1693, average loss: 2.7950
[09/26 05:10:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.50	top5: 62.50	
[09/26 05:10:01 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 05:10:08 visual_prompt]: Epoch 9 / 100: avg data time: 5.52e-02, avg batch time: 0.5031, average train loss: 1.3525
[09/26 05:10:09 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 2.1875
[09/26 05:10:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 39.00	top5: 73.50	
[09/26 05:10:09 visual_prompt]: Best epoch 9: best metric: 0.390
[09/26 05:10:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 05:10:16 visual_prompt]: Epoch 10 / 100: avg data time: 5.99e-02, avg batch time: 0.5076, average train loss: 0.9510
[09/26 05:10:18 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1692, average loss: 2.1373
[09/26 05:10:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 76.00	
[09/26 05:10:18 visual_prompt]: Best epoch 10: best metric: 0.435
[09/26 05:10:18 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 05:10:24 visual_prompt]: Epoch 11 / 100: avg data time: 4.33e-02, avg batch time: 0.4935, average train loss: 0.5854
[09/26 05:10:26 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1693, average loss: 1.9613
[09/26 05:10:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 82.50	
[09/26 05:10:26 visual_prompt]: Best epoch 11: best metric: 0.455
[09/26 05:10:26 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 05:10:33 visual_prompt]: Epoch 12 / 100: avg data time: 5.61e-02, avg batch time: 0.5059, average train loss: 0.3323
[09/26 05:10:34 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 2.0025
[09/26 05:10:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 81.00	
[09/26 05:10:34 visual_prompt]: Best epoch 12: best metric: 0.475
[09/26 05:10:34 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 05:10:41 visual_prompt]: Epoch 13 / 100: avg data time: 6.54e-02, avg batch time: 0.5134, average train loss: 0.2456
[09/26 05:10:43 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1695, average loss: 2.0452
[09/26 05:10:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 83.50	
[09/26 05:10:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 05:10:49 visual_prompt]: Epoch 14 / 100: avg data time: 4.34e-02, avg batch time: 0.4918, average train loss: 0.1230
[09/26 05:10:51 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1695, average loss: 2.0836
[09/26 05:10:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 83.50	
[09/26 05:10:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 05:10:58 visual_prompt]: Epoch 15 / 100: avg data time: 5.64e-02, avg batch time: 0.5060, average train loss: 0.0771
[09/26 05:10:59 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1691, average loss: 2.0078
[09/26 05:10:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 85.00	
[09/26 05:10:59 visual_prompt]: Best epoch 15: best metric: 0.510
[09/26 05:10:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 05:11:06 visual_prompt]: Epoch 16 / 100: avg data time: 5.92e-02, avg batch time: 0.5088, average train loss: 0.0462
[09/26 05:11:08 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1693, average loss: 2.0355
[09/26 05:11:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 83.50	
[09/26 05:11:08 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 05:11:15 visual_prompt]: Epoch 17 / 100: avg data time: 5.87e-02, avg batch time: 0.5070, average train loss: 0.0303
[09/26 05:11:16 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1693, average loss: 2.0798
[09/26 05:11:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 84.00	
[09/26 05:11:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 05:11:23 visual_prompt]: Epoch 18 / 100: avg data time: 5.68e-02, avg batch time: 0.5054, average train loss: 0.0206
[09/26 05:11:25 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1693, average loss: 2.0965
[09/26 05:11:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 84.00	
[09/26 05:11:25 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 05:11:31 visual_prompt]: Epoch 19 / 100: avg data time: 5.69e-02, avg batch time: 0.5057, average train loss: 0.0157
[09/26 05:11:33 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 2.1105
[09/26 05:11:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 83.50	
[09/26 05:11:33 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 05:11:40 visual_prompt]: Epoch 20 / 100: avg data time: 5.51e-02, avg batch time: 0.5043, average train loss: 0.0139
[09/26 05:11:41 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1691, average loss: 2.1132
[09/26 05:11:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 83.50	
[09/26 05:11:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 05:11:48 visual_prompt]: Epoch 21 / 100: avg data time: 4.55e-02, avg batch time: 0.4956, average train loss: 0.0121
[09/26 05:11:49 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1697, average loss: 2.1160
[09/26 05:11:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.00	
[09/26 05:11:49 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 05:11:56 visual_prompt]: Epoch 22 / 100: avg data time: 5.01e-02, avg batch time: 0.4995, average train loss: 0.0108
[09/26 05:11:58 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1693, average loss: 2.1268
[09/26 05:11:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 83.50	
[09/26 05:11:58 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 05:12:04 visual_prompt]: Epoch 23 / 100: avg data time: 4.32e-02, avg batch time: 0.4925, average train loss: 0.0107
[09/26 05:12:06 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 2.1761
[09/26 05:12:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 84.50	
[09/26 05:12:06 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 05:12:13 visual_prompt]: Epoch 24 / 100: avg data time: 4.72e-02, avg batch time: 0.4977, average train loss: 0.0092
[09/26 05:12:14 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 2.1930
[09/26 05:12:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 84.50	
[09/26 05:12:14 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 05:12:21 visual_prompt]: Epoch 25 / 100: avg data time: 4.37e-02, avg batch time: 0.4934, average train loss: 0.0082
[09/26 05:12:22 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1697, average loss: 2.1864
[09/26 05:12:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 85.00	
[09/26 05:12:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 05:12:29 visual_prompt]: Epoch 26 / 100: avg data time: 4.55e-02, avg batch time: 0.4947, average train loss: 0.0083
[09/26 05:12:31 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1691, average loss: 2.1814
[09/26 05:12:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:12:31 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 05:12:38 visual_prompt]: Epoch 27 / 100: avg data time: 5.63e-02, avg batch time: 0.5056, average train loss: 0.0073
[09/26 05:12:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1694, average loss: 2.1793
[09/26 05:12:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 84.00	
[09/26 05:12:39 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 05:12:46 visual_prompt]: Epoch 28 / 100: avg data time: 6.50e-02, avg batch time: 0.5132, average train loss: 0.0073
[09/26 05:12:48 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1695, average loss: 2.2053
[09/26 05:12:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 84.50	
[09/26 05:12:48 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 05:12:55 visual_prompt]: Epoch 29 / 100: avg data time: 6.02e-02, avg batch time: 0.5090, average train loss: 0.0069
[09/26 05:12:56 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1694, average loss: 2.2290
[09/26 05:12:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:12:56 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 05:13:03 visual_prompt]: Epoch 30 / 100: avg data time: 5.49e-02, avg batch time: 0.5039, average train loss: 0.0064
[09/26 05:13:04 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1697, average loss: 2.2494
[09/26 05:13:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.00	
[09/26 05:13:04 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 05:13:11 visual_prompt]: Epoch 31 / 100: avg data time: 6.35e-02, avg batch time: 0.5129, average train loss: 0.0058
[09/26 05:13:13 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1696, average loss: 2.2583
[09/26 05:13:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.00	
[09/26 05:13:13 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 05:13:20 visual_prompt]: Epoch 32 / 100: avg data time: 5.59e-02, avg batch time: 0.5051, average train loss: 0.0057
[09/26 05:13:21 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1696, average loss: 2.2534
[09/26 05:13:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:13:21 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 05:13:28 visual_prompt]: Epoch 33 / 100: avg data time: 6.17e-02, avg batch time: 0.5096, average train loss: 0.0052
[09/26 05:13:30 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 2.2534
[09/26 05:13:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 85.00	
[09/26 05:13:30 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 05:13:37 visual_prompt]: Epoch 34 / 100: avg data time: 4.39e-02, avg batch time: 0.4929, average train loss: 0.0052
[09/26 05:13:38 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1699, average loss: 2.2595
[09/26 05:13:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 85.00	
[09/26 05:13:38 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 05:13:45 visual_prompt]: Epoch 35 / 100: avg data time: 4.66e-02, avg batch time: 0.4979, average train loss: 0.0048
[09/26 05:13:47 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1695, average loss: 2.2660
[09/26 05:13:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 84.50	
[09/26 05:13:47 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 05:13:53 visual_prompt]: Epoch 36 / 100: avg data time: 5.34e-02, avg batch time: 0.5033, average train loss: 0.0049
[09/26 05:13:55 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1692, average loss: 2.2441
[09/26 05:13:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 84.50	
[09/26 05:13:55 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 05:14:02 visual_prompt]: Epoch 37 / 100: avg data time: 5.37e-02, avg batch time: 0.5018, average train loss: 0.0044
[09/26 05:14:03 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 2.2422
[09/26 05:14:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:14:03 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 05:14:10 visual_prompt]: Epoch 38 / 100: avg data time: 5.21e-02, avg batch time: 0.5013, average train loss: 0.0042
[09/26 05:14:12 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 2.2560
[09/26 05:14:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:14:12 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 05:14:19 visual_prompt]: Epoch 39 / 100: avg data time: 5.95e-02, avg batch time: 0.5075, average train loss: 0.0042
[09/26 05:14:20 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1695, average loss: 2.2669
[09/26 05:14:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:14:20 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 05:14:27 visual_prompt]: Epoch 40 / 100: avg data time: 6.00e-02, avg batch time: 0.5091, average train loss: 0.0043
[09/26 05:14:29 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1699, average loss: 2.2745
[09/26 05:14:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:14:29 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 05:14:35 visual_prompt]: Epoch 41 / 100: avg data time: 5.27e-02, avg batch time: 0.5018, average train loss: 0.0042
[09/26 05:14:37 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 2.2820
[09/26 05:14:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:14:37 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 05:14:44 visual_prompt]: Epoch 42 / 100: avg data time: 6.02e-02, avg batch time: 0.5081, average train loss: 0.0041
[09/26 05:14:46 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1696, average loss: 2.2825
[09/26 05:14:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:14:46 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 05:14:52 visual_prompt]: Epoch 43 / 100: avg data time: 5.36e-02, avg batch time: 0.5026, average train loss: 0.0040
[09/26 05:14:54 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1699, average loss: 2.2848
[09/26 05:14:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 85.00	
[09/26 05:14:54 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 05:15:01 visual_prompt]: Epoch 44 / 100: avg data time: 4.88e-02, avg batch time: 0.4968, average train loss: 0.0037
[09/26 05:15:02 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1696, average loss: 2.2899
[09/26 05:15:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:15:02 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 05:15:09 visual_prompt]: Epoch 45 / 100: avg data time: 5.49e-02, avg batch time: 0.5039, average train loss: 0.0037
[09/26 05:15:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1698, average loss: 2.2860
[09/26 05:15:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 85.00	
[09/26 05:15:11 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 05:15:18 visual_prompt]: Epoch 46 / 100: avg data time: 5.08e-02, avg batch time: 0.4991, average train loss: 0.0037
[09/26 05:15:19 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1693, average loss: 2.2843
[09/26 05:15:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:15:19 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 05:15:26 visual_prompt]: Epoch 47 / 100: avg data time: 5.30e-02, avg batch time: 0.5010, average train loss: 0.0039
[09/26 05:15:28 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 2.2878
[09/26 05:15:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.00	
[09/26 05:15:28 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 05:15:35 visual_prompt]: Epoch 48 / 100: avg data time: 6.13e-02, avg batch time: 0.5100, average train loss: 0.0035
[09/26 05:15:36 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 2.2960
[09/26 05:15:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 83.50	
[09/26 05:15:36 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 05:15:43 visual_prompt]: Epoch 49 / 100: avg data time: 5.62e-02, avg batch time: 0.5039, average train loss: 0.0032
[09/26 05:15:45 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1694, average loss: 2.3004
[09/26 05:15:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.00	
[09/26 05:15:45 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 05:15:51 visual_prompt]: Epoch 50 / 100: avg data time: 4.43e-02, avg batch time: 0.4953, average train loss: 0.0035
[09/26 05:15:53 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 2.3053
[09/26 05:15:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:15:53 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 05:16:00 visual_prompt]: Epoch 51 / 100: avg data time: 5.22e-02, avg batch time: 0.5017, average train loss: 0.0035
[09/26 05:16:01 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1690, average loss: 2.3091
[09/26 05:16:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 84.50	
[09/26 05:16:01 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 05:16:08 visual_prompt]: Epoch 52 / 100: avg data time: 4.59e-02, avg batch time: 0.4965, average train loss: 0.0033
[09/26 05:16:10 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1695, average loss: 2.3100
[09/26 05:16:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 84.50	
[09/26 05:16:10 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 05:16:16 visual_prompt]: Epoch 53 / 100: avg data time: 4.49e-02, avg batch time: 0.4933, average train loss: 0.0032
[09/26 05:16:18 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 2.3114
[09/26 05:16:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 84.50	
[09/26 05:16:18 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 05:16:25 visual_prompt]: Epoch 54 / 100: avg data time: 4.37e-02, avg batch time: 0.4924, average train loss: 0.0029
[09/26 05:16:26 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1696, average loss: 2.3156
[09/26 05:16:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 84.50	
[09/26 05:16:26 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 05:16:33 visual_prompt]: Epoch 55 / 100: avg data time: 5.04e-02, avg batch time: 0.4989, average train loss: 0.0031
[09/26 05:16:34 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1694, average loss: 2.3176
[09/26 05:16:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 84.50	
[09/26 05:16:34 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 05:16:41 visual_prompt]: Epoch 56 / 100: avg data time: 5.81e-02, avg batch time: 0.5064, average train loss: 0.0031
[09/26 05:16:43 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1696, average loss: 2.3186
[09/26 05:16:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:16:43 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 05:16:50 visual_prompt]: Epoch 57 / 100: avg data time: 6.16e-02, avg batch time: 0.5098, average train loss: 0.0030
[09/26 05:16:51 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1697, average loss: 2.3224
[09/26 05:16:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:16:51 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 05:16:58 visual_prompt]: Epoch 58 / 100: avg data time: 4.36e-02, avg batch time: 0.4936, average train loss: 0.0030
[09/26 05:17:00 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1695, average loss: 2.3265
[09/26 05:17:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:17:00 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 05:17:06 visual_prompt]: Epoch 59 / 100: avg data time: 4.98e-02, avg batch time: 0.4986, average train loss: 0.0029
[09/26 05:17:08 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 2.3268
[09/26 05:17:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:17:08 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 05:17:15 visual_prompt]: Epoch 60 / 100: avg data time: 5.92e-02, avg batch time: 0.5080, average train loss: 0.0028
[09/26 05:17:16 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1704, average loss: 2.3283
[09/26 05:17:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:17:16 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 05:17:23 visual_prompt]: Epoch 61 / 100: avg data time: 4.82e-02, avg batch time: 0.4981, average train loss: 0.0029
[09/26 05:17:25 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1697, average loss: 2.3278
[09/26 05:17:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:17:25 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 05:17:32 visual_prompt]: Epoch 62 / 100: avg data time: 5.46e-02, avg batch time: 0.5032, average train loss: 0.0030
[09/26 05:17:33 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1696, average loss: 2.3292
[09/26 05:17:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:17:33 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 05:17:40 visual_prompt]: Epoch 63 / 100: avg data time: 4.76e-02, avg batch time: 0.4987, average train loss: 0.0026
[09/26 05:17:41 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1697, average loss: 2.3342
[09/26 05:17:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:17:41 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 05:17:48 visual_prompt]: Epoch 64 / 100: avg data time: 6.05e-02, avg batch time: 0.5089, average train loss: 0.0030
[09/26 05:17:50 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1697, average loss: 2.3345
[09/26 05:17:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:17:50 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 05:17:57 visual_prompt]: Epoch 65 / 100: avg data time: 5.46e-02, avg batch time: 0.5048, average train loss: 0.0029
[09/26 05:17:58 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1696, average loss: 2.3310
[09/26 05:17:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 84.50	
[09/26 05:17:58 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 05:18:05 visual_prompt]: Epoch 66 / 100: avg data time: 6.34e-02, avg batch time: 0.5115, average train loss: 0.0028
[09/26 05:18:07 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1693, average loss: 2.3323
[09/26 05:18:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 84.50	
[09/26 05:18:07 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 05:18:14 visual_prompt]: Epoch 67 / 100: avg data time: 6.18e-02, avg batch time: 0.5109, average train loss: 0.0028
[09/26 05:18:15 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1697, average loss: 2.3355
[09/26 05:18:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 84.50	
[09/26 05:18:15 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 05:18:22 visual_prompt]: Epoch 68 / 100: avg data time: 5.17e-02, avg batch time: 0.5016, average train loss: 0.0027
[09/26 05:18:24 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1693, average loss: 2.3349
[09/26 05:18:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:18:24 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 05:18:31 visual_prompt]: Epoch 69 / 100: avg data time: 4.99e-02, avg batch time: 0.5007, average train loss: 0.0027
[09/26 05:18:32 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1698, average loss: 2.3355
[09/26 05:18:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:18:32 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 05:18:39 visual_prompt]: Epoch 70 / 100: avg data time: 5.61e-02, avg batch time: 0.5047, average train loss: 0.0026
[09/26 05:18:41 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1696, average loss: 2.3383
[09/26 05:18:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:18:41 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 05:18:48 visual_prompt]: Epoch 71 / 100: avg data time: 4.90e-02, avg batch time: 0.5002, average train loss: 0.0026
[09/26 05:18:49 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1701, average loss: 2.3407
[09/26 05:18:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 84.50	
[09/26 05:18:49 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 05:18:56 visual_prompt]: Epoch 72 / 100: avg data time: 5.71e-02, avg batch time: 0.5059, average train loss: 0.0026
[09/26 05:18:58 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1692, average loss: 2.3424
[09/26 05:18:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 84.50	
[09/26 05:18:58 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 05:19:04 visual_prompt]: Epoch 73 / 100: avg data time: 4.66e-02, avg batch time: 0.4966, average train loss: 0.0027
[09/26 05:19:06 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1691, average loss: 2.3428
[09/26 05:19:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 84.50	
[09/26 05:19:06 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 05:19:13 visual_prompt]: Epoch 74 / 100: avg data time: 5.17e-02, avg batch time: 0.5000, average train loss: 0.0025
[09/26 05:19:14 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 2.3428
[09/26 05:19:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 84.50	
[09/26 05:19:14 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 05:19:21 visual_prompt]: Epoch 75 / 100: avg data time: 5.40e-02, avg batch time: 0.5033, average train loss: 0.0026
[09/26 05:19:23 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1691, average loss: 2.3424
[09/26 05:19:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 84.50	
[09/26 05:19:23 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 05:19:30 visual_prompt]: Epoch 76 / 100: avg data time: 5.45e-02, avg batch time: 0.5028, average train loss: 0.0026
[09/26 05:19:31 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1697, average loss: 2.3418
[09/26 05:19:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:19:31 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 05:19:38 visual_prompt]: Epoch 77 / 100: avg data time: 5.32e-02, avg batch time: 0.5034, average train loss: 0.0024
[09/26 05:19:39 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1695, average loss: 2.3420
[09/26 05:19:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:19:39 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 05:19:46 visual_prompt]: Epoch 78 / 100: avg data time: 5.75e-02, avg batch time: 0.5062, average train loss: 0.0027
[09/26 05:19:48 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 2.3425
[09/26 05:19:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 84.50	
[09/26 05:19:48 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 05:19:55 visual_prompt]: Epoch 79 / 100: avg data time: 5.49e-02, avg batch time: 0.5040, average train loss: 0.0026
[09/26 05:19:56 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1696, average loss: 2.3430
[09/26 05:19:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 84.50	
[09/26 05:19:56 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 05:20:03 visual_prompt]: Epoch 80 / 100: avg data time: 4.73e-02, avg batch time: 0.4959, average train loss: 0.0025
[09/26 05:20:04 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1699, average loss: 2.3443
[09/26 05:20:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 84.50	
[09/26 05:20:04 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 05:20:11 visual_prompt]: Epoch 81 / 100: avg data time: 5.10e-02, avg batch time: 0.4994, average train loss: 0.0025
[09/26 05:20:13 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1697, average loss: 2.3447
[09/26 05:20:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 84.50	
[09/26 05:20:13 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 05:20:20 visual_prompt]: Epoch 82 / 100: avg data time: 4.70e-02, avg batch time: 0.4962, average train loss: 0.0027
[09/26 05:20:21 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1693, average loss: 2.3450
[09/26 05:20:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 84.50	
[09/26 05:20:21 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 05:20:28 visual_prompt]: Epoch 83 / 100: avg data time: 6.08e-02, avg batch time: 0.5100, average train loss: 0.0025
[09/26 05:20:29 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1693, average loss: 2.3448
[09/26 05:20:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:20:30 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 05:20:36 visual_prompt]: Epoch 84 / 100: avg data time: 5.19e-02, avg batch time: 0.5035, average train loss: 0.0024
[09/26 05:20:38 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 2.3444
[09/26 05:20:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:20:38 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 05:20:45 visual_prompt]: Epoch 85 / 100: avg data time: 5.32e-02, avg batch time: 0.5022, average train loss: 0.0024
[09/26 05:20:46 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1697, average loss: 2.3444
[09/26 05:20:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:20:46 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 05:20:53 visual_prompt]: Epoch 86 / 100: avg data time: 5.52e-02, avg batch time: 0.5045, average train loss: 0.0024
[09/26 05:20:55 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1695, average loss: 2.3441
[09/26 05:20:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:20:55 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 05:21:01 visual_prompt]: Epoch 87 / 100: avg data time: 4.47e-02, avg batch time: 0.4943, average train loss: 0.0026
[09/26 05:21:03 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1696, average loss: 2.3441
[09/26 05:21:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:21:03 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 05:21:10 visual_prompt]: Epoch 88 / 100: avg data time: 5.26e-02, avg batch time: 0.5022, average train loss: 0.0024
[09/26 05:21:11 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1696, average loss: 2.3440
[09/26 05:21:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:21:11 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 05:21:18 visual_prompt]: Epoch 89 / 100: avg data time: 4.37e-02, avg batch time: 0.4932, average train loss: 0.0025
[09/26 05:21:19 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 2.3440
[09/26 05:21:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:21:19 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 05:21:26 visual_prompt]: Epoch 90 / 100: avg data time: 5.97e-02, avg batch time: 0.5085, average train loss: 0.0024
[09/26 05:21:28 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 2.3444
[09/26 05:21:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:21:28 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 05:21:35 visual_prompt]: Epoch 91 / 100: avg data time: 5.88e-02, avg batch time: 0.5082, average train loss: 0.0024
[09/26 05:21:36 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1691, average loss: 2.3445
[09/26 05:21:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:21:36 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 05:21:43 visual_prompt]: Epoch 92 / 100: avg data time: 5.07e-02, avg batch time: 0.4997, average train loss: 0.0025
[09/26 05:21:45 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1692, average loss: 2.3446
[09/26 05:21:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:21:45 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 05:21:51 visual_prompt]: Epoch 93 / 100: avg data time: 4.82e-02, avg batch time: 0.4965, average train loss: 0.0024
[09/26 05:21:53 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1696, average loss: 2.3446
[09/26 05:21:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:21:53 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 05:22:00 visual_prompt]: Epoch 94 / 100: avg data time: 5.97e-02, avg batch time: 0.5098, average train loss: 0.0023
[09/26 05:22:01 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1693, average loss: 2.3448
[09/26 05:22:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:22:01 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 05:22:08 visual_prompt]: Epoch 95 / 100: avg data time: 6.56e-02, avg batch time: 0.5135, average train loss: 0.0024
[09/26 05:22:10 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1697, average loss: 2.3448
[09/26 05:22:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:22:10 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 05:22:17 visual_prompt]: Epoch 96 / 100: avg data time: 6.13e-02, avg batch time: 0.5096, average train loss: 0.0025
[09/26 05:22:18 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 2.3449
[09/26 05:22:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:22:18 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 05:22:25 visual_prompt]: Epoch 97 / 100: avg data time: 5.55e-02, avg batch time: 0.5033, average train loss: 0.0025
[09/26 05:22:27 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1692, average loss: 2.3449
[09/26 05:22:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:22:27 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 05:22:34 visual_prompt]: Epoch 98 / 100: avg data time: 6.55e-02, avg batch time: 0.5149, average train loss: 0.0025
[09/26 05:22:35 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1695, average loss: 2.3449
[09/26 05:22:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:22:35 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 05:22:42 visual_prompt]: Epoch 99 / 100: avg data time: 5.86e-02, avg batch time: 0.5079, average train loss: 0.0024
[09/26 05:22:44 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1695, average loss: 2.3450
[09/26 05:22:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:22:44 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 05:22:51 visual_prompt]: Epoch 100 / 100: avg data time: 5.77e-02, avg batch time: 0.5079, average train loss: 0.0025
[09/26 05:22:52 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1692, average loss: 2.3450
[09/26 05:22:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 84.50	
[09/26 05:22:52 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 05:22:52 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 05:22:52 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 05:22:52 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 05:22:52 visual_prompt]: Training with config:
[09/26 05:22:52 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 05:22:52 visual_prompt]: Loading training data...
[09/26 05:22:52 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 05:22:54 visual_prompt]: Number of images: 800
[09/26 05:22:54 visual_prompt]: Number of classes: 47 / 47
[09/26 05:22:54 visual_prompt]: Loading validation data...
[09/26 05:22:54 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 05:22:55 visual_prompt]: Number of images: 200
[09/26 05:22:55 visual_prompt]: Number of classes: 47 / 47
[09/26 05:22:55 visual_prompt]: Constructing models...
[09/26 05:22:57 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 05:22:57 visual_prompt]: tuned percent:0.576
[09/26 05:22:57 visual_prompt]: Device used for model: 0
[09/26 05:22:57 visual_prompt]: Setting up Evaluator...
[09/26 05:22:57 visual_prompt]: Setting up Trainer...
[09/26 05:22:57 visual_prompt]: 	Setting up the optimizer...
[09/26 05:22:57 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 05:23:04 visual_prompt]: Epoch 1 / 100: avg data time: 4.49e-02, avg batch time: 0.4938, average train loss: 3.9267
[09/26 05:23:06 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1691, average loss: 3.9045
[09/26 05:23:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 05:23:06 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 05:23:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 05:23:12 visual_prompt]: Epoch 2 / 100: avg data time: 5.44e-02, avg batch time: 0.5018, average train loss: 3.8916
[09/26 05:23:14 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1689, average loss: 3.8843
[09/26 05:23:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.00	top5: 10.50	
[09/26 05:23:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 05:23:21 visual_prompt]: Epoch 3 / 100: avg data time: 5.32e-02, avg batch time: 0.5001, average train loss: 3.8435
[09/26 05:23:22 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1692, average loss: 3.9075
[09/26 05:23:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.00	top5: 9.00	
[09/26 05:23:22 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 05:23:29 visual_prompt]: Epoch 4 / 100: avg data time: 5.84e-02, avg batch time: 0.5056, average train loss: 3.8049
[09/26 05:23:31 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 3.8468
[09/26 05:23:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.50	top5: 18.00	
[09/26 05:23:31 visual_prompt]: Best epoch 4: best metric: 0.075
[09/26 05:23:31 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 05:23:37 visual_prompt]: Epoch 5 / 100: avg data time: 4.58e-02, avg batch time: 0.4942, average train loss: 3.6998
[09/26 05:23:39 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1693, average loss: 3.6302
[09/26 05:23:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 10.50	top5: 29.00	
[09/26 05:23:39 visual_prompt]: Best epoch 5: best metric: 0.105
[09/26 05:23:39 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 05:23:46 visual_prompt]: Epoch 6 / 100: avg data time: 5.78e-02, avg batch time: 0.5070, average train loss: 3.4417
[09/26 05:23:47 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 3.4964
[09/26 05:23:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 14.00	top5: 35.00	
[09/26 05:23:47 visual_prompt]: Best epoch 6: best metric: 0.140
[09/26 05:23:47 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 05:23:54 visual_prompt]: Epoch 7 / 100: avg data time: 5.17e-02, avg batch time: 0.5001, average train loss: 3.0563
[09/26 05:23:56 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 3.2685
[09/26 05:23:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.00	top5: 41.50	
[09/26 05:23:56 visual_prompt]: Best epoch 7: best metric: 0.160
[09/26 05:23:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 05:24:02 visual_prompt]: Epoch 8 / 100: avg data time: 4.82e-02, avg batch time: 0.4961, average train loss: 2.6255
[09/26 05:24:04 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1696, average loss: 2.9133
[09/26 05:24:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 52.00	
[09/26 05:24:04 visual_prompt]: Best epoch 8: best metric: 0.245
[09/26 05:24:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 05:24:11 visual_prompt]: Epoch 9 / 100: avg data time: 4.58e-02, avg batch time: 0.4952, average train loss: 2.0508
[09/26 05:24:12 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1695, average loss: 2.5318
[09/26 05:24:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 31.00	top5: 60.00	
[09/26 05:24:12 visual_prompt]: Best epoch 9: best metric: 0.310
[09/26 05:24:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 05:24:19 visual_prompt]: Epoch 10 / 100: avg data time: 4.97e-02, avg batch time: 0.4990, average train loss: 1.6520
[09/26 05:24:21 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1695, average loss: 2.4045
[09/26 05:24:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.50	top5: 65.00	
[09/26 05:24:21 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 05:24:27 visual_prompt]: Epoch 11 / 100: avg data time: 5.45e-02, avg batch time: 0.5043, average train loss: 1.3071
[09/26 05:24:29 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1693, average loss: 2.2099
[09/26 05:24:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.50	top5: 71.00	
[09/26 05:24:29 visual_prompt]: Best epoch 11: best metric: 0.385
[09/26 05:24:29 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 05:24:36 visual_prompt]: Epoch 12 / 100: avg data time: 4.97e-02, avg batch time: 0.4995, average train loss: 1.0842
[09/26 05:24:37 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 2.0025
[09/26 05:24:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 76.00	
[09/26 05:24:37 visual_prompt]: Best epoch 12: best metric: 0.475
[09/26 05:24:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 05:24:44 visual_prompt]: Epoch 13 / 100: avg data time: 5.34e-02, avg batch time: 0.5029, average train loss: 0.8635
[09/26 05:24:46 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1694, average loss: 1.9977
[09/26 05:24:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 77.00	
[09/26 05:24:46 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 05:24:52 visual_prompt]: Epoch 14 / 100: avg data time: 4.57e-02, avg batch time: 0.4954, average train loss: 0.6765
[09/26 05:24:54 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1695, average loss: 1.9010
[09/26 05:24:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 77.50	
[09/26 05:24:54 visual_prompt]: Best epoch 14: best metric: 0.480
[09/26 05:24:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 05:25:01 visual_prompt]: Epoch 15 / 100: avg data time: 6.41e-02, avg batch time: 0.5122, average train loss: 0.6060
[09/26 05:25:02 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 2.1311
[09/26 05:25:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 78.00	
[09/26 05:25:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 05:25:09 visual_prompt]: Epoch 16 / 100: avg data time: 5.02e-02, avg batch time: 0.4991, average train loss: 0.5926
[09/26 05:25:11 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1693, average loss: 2.0044
[09/26 05:25:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 76.00	
[09/26 05:25:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 05:25:18 visual_prompt]: Epoch 17 / 100: avg data time: 5.72e-02, avg batch time: 0.5073, average train loss: 0.5151
[09/26 05:25:19 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1696, average loss: 1.9143
[09/26 05:25:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 79.00	
[09/26 05:25:19 visual_prompt]: Best epoch 17: best metric: 0.485
[09/26 05:25:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 05:25:26 visual_prompt]: Epoch 18 / 100: avg data time: 4.84e-02, avg batch time: 0.4984, average train loss: 0.5099
[09/26 05:25:27 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1696, average loss: 1.9587
[09/26 05:25:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.00	top5: 76.00	
[09/26 05:25:27 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 05:25:34 visual_prompt]: Epoch 19 / 100: avg data time: 5.77e-02, avg batch time: 0.5058, average train loss: 0.4705
[09/26 05:25:36 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 1.9934
[09/26 05:25:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 77.50	
[09/26 05:25:36 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 05:25:43 visual_prompt]: Epoch 20 / 100: avg data time: 5.84e-02, avg batch time: 0.5082, average train loss: 0.5365
[09/26 05:25:44 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1695, average loss: 2.0772
[09/26 05:25:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 76.00	
[09/26 05:25:44 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 05:25:51 visual_prompt]: Epoch 21 / 100: avg data time: 5.57e-02, avg batch time: 0.5046, average train loss: 0.5916
[09/26 05:25:53 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.1699, average loss: 2.0629
[09/26 05:25:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 81.50	
[09/26 05:25:53 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 05:25:59 visual_prompt]: Epoch 22 / 100: avg data time: 4.72e-02, avg batch time: 0.4975, average train loss: 0.5441
[09/26 05:26:01 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1697, average loss: 2.0307
[09/26 05:26:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 80.00	
[09/26 05:26:01 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 05:26:08 visual_prompt]: Epoch 23 / 100: avg data time: 4.71e-02, avg batch time: 0.4955, average train loss: 0.5418
[09/26 05:26:09 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1698, average loss: 1.9417
[09/26 05:26:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 80.00	
[09/26 05:26:09 visual_prompt]: Best epoch 23: best metric: 0.490
[09/26 05:26:09 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 05:26:16 visual_prompt]: Epoch 24 / 100: avg data time: 4.66e-02, avg batch time: 0.4979, average train loss: 0.5053
[09/26 05:26:18 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1700, average loss: 2.0692
[09/26 05:26:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 77.50	
[09/26 05:26:18 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 05:26:25 visual_prompt]: Epoch 25 / 100: avg data time: 6.34e-02, avg batch time: 0.5126, average train loss: 0.4570
[09/26 05:26:26 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1698, average loss: 1.9763
[09/26 05:26:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 81.50	
[09/26 05:26:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 05:26:33 visual_prompt]: Epoch 26 / 100: avg data time: 4.64e-02, avg batch time: 0.4953, average train loss: 0.4280
[09/26 05:26:35 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1697, average loss: 1.9982
[09/26 05:26:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 80.00	
[09/26 05:26:35 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 05:26:42 visual_prompt]: Epoch 27 / 100: avg data time: 5.28e-02, avg batch time: 0.5009, average train loss: 0.3758
[09/26 05:26:43 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 1.9124
[09/26 05:26:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 82.50	
[09/26 05:26:43 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 05:26:50 visual_prompt]: Epoch 28 / 100: avg data time: 5.43e-02, avg batch time: 0.5031, average train loss: 0.3231
[09/26 05:26:52 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1697, average loss: 1.8199
[09/26 05:26:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 83.00	
[09/26 05:26:52 visual_prompt]: Best epoch 28: best metric: 0.515
[09/26 05:26:52 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 05:26:58 visual_prompt]: Epoch 29 / 100: avg data time: 5.84e-02, avg batch time: 0.5068, average train loss: 0.3258
[09/26 05:27:00 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1692, average loss: 1.9710
[09/26 05:27:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 80.50	
[09/26 05:27:00 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 05:27:07 visual_prompt]: Epoch 30 / 100: avg data time: 6.43e-02, avg batch time: 0.5126, average train loss: 0.4155
[09/26 05:27:09 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1696, average loss: 2.3247
[09/26 05:27:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 39.50	top5: 70.50	
[09/26 05:27:09 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 05:27:15 visual_prompt]: Epoch 31 / 100: avg data time: 5.51e-02, avg batch time: 0.5036, average train loss: 0.5969
[09/26 05:27:17 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1696, average loss: 1.8168
[09/26 05:27:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 83.50	
[09/26 05:27:17 visual_prompt]: Best epoch 31: best metric: 0.525
[09/26 05:27:17 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 05:27:24 visual_prompt]: Epoch 32 / 100: avg data time: 5.39e-02, avg batch time: 0.5024, average train loss: 0.5078
[09/26 05:27:26 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1697, average loss: 1.9793
[09/26 05:27:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 83.00	
[09/26 05:27:26 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 05:27:32 visual_prompt]: Epoch 33 / 100: avg data time: 5.65e-02, avg batch time: 0.5057, average train loss: 0.4528
[09/26 05:27:34 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1697, average loss: 1.8263
[09/26 05:27:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 81.50	
[09/26 05:27:34 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 05:27:41 visual_prompt]: Epoch 34 / 100: avg data time: 5.92e-02, avg batch time: 0.5076, average train loss: 0.3890
[09/26 05:27:43 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1696, average loss: 2.0574
[09/26 05:27:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.00	top5: 79.00	
[09/26 05:27:43 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 05:27:49 visual_prompt]: Epoch 35 / 100: avg data time: 5.27e-02, avg batch time: 0.5013, average train loss: 0.3199
[09/26 05:27:51 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1697, average loss: 1.9316
[09/26 05:27:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 79.00	
[09/26 05:27:51 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 05:27:58 visual_prompt]: Epoch 36 / 100: avg data time: 4.58e-02, avg batch time: 0.4949, average train loss: 0.2828
[09/26 05:27:59 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1697, average loss: 1.8429
[09/26 05:27:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 81.50	
[09/26 05:27:59 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 05:28:06 visual_prompt]: Epoch 37 / 100: avg data time: 5.43e-02, avg batch time: 0.5023, average train loss: 0.2625
[09/26 05:28:08 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 1.9494
[09/26 05:28:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 79.00	
[09/26 05:28:08 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 05:28:14 visual_prompt]: Epoch 38 / 100: avg data time: 5.34e-02, avg batch time: 0.5021, average train loss: 0.2995
[09/26 05:28:16 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1699, average loss: 1.9563
[09/26 05:28:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 05:28:16 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 05:28:23 visual_prompt]: Epoch 39 / 100: avg data time: 5.48e-02, avg batch time: 0.5030, average train loss: 0.3366
[09/26 05:28:24 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1696, average loss: 2.0243
[09/26 05:28:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 77.00	
[09/26 05:28:24 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 05:28:31 visual_prompt]: Epoch 40 / 100: avg data time: 5.30e-02, avg batch time: 0.5022, average train loss: 0.4170
[09/26 05:28:33 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 1.9210
[09/26 05:28:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 80.50	
[09/26 05:28:33 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 05:28:40 visual_prompt]: Epoch 41 / 100: avg data time: 6.86e-02, avg batch time: 0.5169, average train loss: 0.3776
[09/26 05:28:41 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1698, average loss: 1.9112
[09/26 05:28:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 81.50	
[09/26 05:28:41 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 05:28:48 visual_prompt]: Epoch 42 / 100: avg data time: 4.47e-02, avg batch time: 0.4954, average train loss: 0.3088
[09/26 05:28:50 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 1.8818
[09/26 05:28:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 82.00	
[09/26 05:28:50 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 05:28:56 visual_prompt]: Epoch 43 / 100: avg data time: 6.27e-02, avg batch time: 0.5126, average train loss: 0.2734
[09/26 05:28:58 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1696, average loss: 1.9520
[09/26 05:28:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 78.50	
[09/26 05:28:58 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 05:29:05 visual_prompt]: Epoch 44 / 100: avg data time: 5.75e-02, avg batch time: 0.5068, average train loss: 0.2340
[09/26 05:29:06 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.8911
[09/26 05:29:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 82.00	
[09/26 05:29:06 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 05:29:13 visual_prompt]: Epoch 45 / 100: avg data time: 5.71e-02, avg batch time: 0.5059, average train loss: 0.2109
[09/26 05:29:15 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1692, average loss: 1.9489
[09/26 05:29:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 80.50	
[09/26 05:29:15 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 05:29:22 visual_prompt]: Epoch 46 / 100: avg data time: 5.63e-02, avg batch time: 0.5050, average train loss: 0.2004
[09/26 05:29:23 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1696, average loss: 1.9232
[09/26 05:29:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 79.50	
[09/26 05:29:23 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 05:29:30 visual_prompt]: Epoch 47 / 100: avg data time: 6.18e-02, avg batch time: 0.5100, average train loss: 0.2644
[09/26 05:29:32 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1694, average loss: 2.0290
[09/26 05:29:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 80.00	
[09/26 05:29:32 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 05:29:39 visual_prompt]: Epoch 48 / 100: avg data time: 4.85e-02, avg batch time: 0.4984, average train loss: 0.2753
[09/26 05:29:40 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1698, average loss: 1.8592
[09/26 05:29:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 81.00	
[09/26 05:29:40 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 05:29:47 visual_prompt]: Epoch 49 / 100: avg data time: 5.19e-02, avg batch time: 0.5028, average train loss: 0.2700
[09/26 05:29:49 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1698, average loss: 2.0189
[09/26 05:29:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 78.50	
[09/26 05:29:49 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 05:29:55 visual_prompt]: Epoch 50 / 100: avg data time: 6.49e-02, avg batch time: 0.5128, average train loss: 0.3514
[09/26 05:29:57 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 1.9901
[09/26 05:29:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 78.50	
[09/26 05:29:57 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 05:30:04 visual_prompt]: Epoch 51 / 100: avg data time: 6.21e-02, avg batch time: 0.5105, average train loss: 0.3055
[09/26 05:30:06 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1698, average loss: 2.0240
[09/26 05:30:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:30:06 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 05:30:12 visual_prompt]: Epoch 52 / 100: avg data time: 4.47e-02, avg batch time: 0.4953, average train loss: 0.3181
[09/26 05:30:14 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1696, average loss: 1.9172
[09/26 05:30:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 80.00	
[09/26 05:30:14 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 05:30:21 visual_prompt]: Epoch 53 / 100: avg data time: 5.60e-02, avg batch time: 0.5048, average train loss: 0.2906
[09/26 05:30:22 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1693, average loss: 2.0853
[09/26 05:30:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 80.00	
[09/26 05:30:22 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 05:30:29 visual_prompt]: Epoch 54 / 100: avg data time: 5.61e-02, avg batch time: 0.5042, average train loss: 0.2291
[09/26 05:30:31 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 1.8910
[09/26 05:30:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 79.50	
[09/26 05:30:31 visual_prompt]: Best epoch 54: best metric: 0.530
[09/26 05:30:31 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 05:30:38 visual_prompt]: Epoch 55 / 100: avg data time: 6.22e-02, avg batch time: 0.5105, average train loss: 0.2107
[09/26 05:30:39 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1697, average loss: 1.9400
[09/26 05:30:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 78.50	
[09/26 05:30:39 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 05:30:46 visual_prompt]: Epoch 56 / 100: avg data time: 4.59e-02, avg batch time: 0.4975, average train loss: 0.2098
[09/26 05:30:48 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1696, average loss: 1.8893
[09/26 05:30:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.50	
[09/26 05:30:48 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 05:30:54 visual_prompt]: Epoch 57 / 100: avg data time: 4.44e-02, avg batch time: 0.4941, average train loss: 0.1691
[09/26 05:30:56 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1697, average loss: 1.9034
[09/26 05:30:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 78.00	
[09/26 05:30:56 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 05:31:03 visual_prompt]: Epoch 58 / 100: avg data time: 4.57e-02, avg batch time: 0.4962, average train loss: 0.1393
[09/26 05:31:04 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1699, average loss: 1.9683
[09/26 05:31:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 79.00	
[09/26 05:31:04 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 05:31:11 visual_prompt]: Epoch 59 / 100: avg data time: 5.65e-02, avg batch time: 0.5057, average train loss: 0.1294
[09/26 05:31:13 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1693, average loss: 1.9808
[09/26 05:31:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 78.00	
[09/26 05:31:13 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 05:31:20 visual_prompt]: Epoch 60 / 100: avg data time: 5.12e-02, avg batch time: 0.4998, average train loss: 0.1141
[09/26 05:31:21 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1696, average loss: 1.8811
[09/26 05:31:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 78.00	
[09/26 05:31:21 visual_prompt]: Best epoch 60: best metric: 0.535
[09/26 05:31:21 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 05:31:28 visual_prompt]: Epoch 61 / 100: avg data time: 5.52e-02, avg batch time: 0.5052, average train loss: 0.1038
[09/26 05:31:30 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1696, average loss: 1.8559
[09/26 05:31:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 80.50	
[09/26 05:31:30 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 05:31:37 visual_prompt]: Epoch 62 / 100: avg data time: 5.95e-02, avg batch time: 0.5079, average train loss: 0.0997
[09/26 05:31:38 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1694, average loss: 1.8758
[09/26 05:31:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 79.50	
[09/26 05:31:38 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 05:31:45 visual_prompt]: Epoch 63 / 100: avg data time: 5.63e-02, avg batch time: 0.5042, average train loss: 0.0995
[09/26 05:31:47 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 1.8607
[09/26 05:31:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 80.00	
[09/26 05:31:47 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 05:31:53 visual_prompt]: Epoch 64 / 100: avg data time: 5.97e-02, avg batch time: 0.5077, average train loss: 0.0994
[09/26 05:31:55 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1700, average loss: 1.8858
[09/26 05:31:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 81.50	
[09/26 05:31:55 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 05:32:02 visual_prompt]: Epoch 65 / 100: avg data time: 5.73e-02, avg batch time: 0.5064, average train loss: 0.1007
[09/26 05:32:04 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1700, average loss: 1.8853
[09/26 05:32:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 79.50	
[09/26 05:32:04 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 05:32:11 visual_prompt]: Epoch 66 / 100: avg data time: 5.60e-02, avg batch time: 0.5039, average train loss: 0.1011
[09/26 05:32:12 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1696, average loss: 1.8923
[09/26 05:32:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 79.00	
[09/26 05:32:12 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 05:32:19 visual_prompt]: Epoch 67 / 100: avg data time: 5.65e-02, avg batch time: 0.5052, average train loss: 0.1015
[09/26 05:32:21 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1689, average loss: 1.8945
[09/26 05:32:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 79.00	
[09/26 05:32:21 visual_prompt]: Best epoch 67: best metric: 0.545
[09/26 05:32:21 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 05:32:28 visual_prompt]: Epoch 68 / 100: avg data time: 5.82e-02, avg batch time: 0.5066, average train loss: 0.1024
[09/26 05:32:29 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1696, average loss: 1.9063
[09/26 05:32:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 80.50	
[09/26 05:32:29 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 05:32:36 visual_prompt]: Epoch 69 / 100: avg data time: 5.09e-02, avg batch time: 0.4995, average train loss: 0.1028
[09/26 05:32:37 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 1.8972
[09/26 05:32:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 78.50	
[09/26 05:32:37 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 05:32:44 visual_prompt]: Epoch 70 / 100: avg data time: 6.38e-02, avg batch time: 0.5127, average train loss: 0.1027
[09/26 05:32:46 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1698, average loss: 1.9326
[09/26 05:32:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 79.50	
[09/26 05:32:46 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 05:32:53 visual_prompt]: Epoch 71 / 100: avg data time: 6.25e-02, avg batch time: 0.5107, average train loss: 0.1034
[09/26 05:32:54 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1697, average loss: 1.9501
[09/26 05:32:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 79.00	
[09/26 05:32:54 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 05:33:01 visual_prompt]: Epoch 72 / 100: avg data time: 5.96e-02, avg batch time: 0.5079, average train loss: 0.1030
[09/26 05:33:03 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1697, average loss: 1.9275
[09/26 05:33:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 79.50	
[09/26 05:33:03 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 05:33:10 visual_prompt]: Epoch 73 / 100: avg data time: 6.59e-02, avg batch time: 0.5138, average train loss: 0.1028
[09/26 05:33:11 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 1.9228
[09/26 05:33:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 78.00	
[09/26 05:33:11 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 05:33:18 visual_prompt]: Epoch 74 / 100: avg data time: 5.85e-02, avg batch time: 0.5077, average train loss: 0.1030
[09/26 05:33:20 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1697, average loss: 1.9327
[09/26 05:33:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 80.00	
[09/26 05:33:20 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 05:33:27 visual_prompt]: Epoch 75 / 100: avg data time: 4.59e-02, avg batch time: 0.4970, average train loss: 0.1028
[09/26 05:33:28 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 1.9446
[09/26 05:33:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 81.00	
[09/26 05:33:28 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 05:33:35 visual_prompt]: Epoch 76 / 100: avg data time: 5.32e-02, avg batch time: 0.5014, average train loss: 0.1022
[09/26 05:33:36 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 1.9527
[09/26 05:33:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 77.00	
[09/26 05:33:36 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 05:33:43 visual_prompt]: Epoch 77 / 100: avg data time: 5.35e-02, avg batch time: 0.5017, average train loss: 0.1025
[09/26 05:33:45 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1698, average loss: 1.9325
[09/26 05:33:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 81.00	
[09/26 05:33:45 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 05:33:52 visual_prompt]: Epoch 78 / 100: avg data time: 4.40e-02, avg batch time: 0.4929, average train loss: 0.1022
[09/26 05:33:53 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1694, average loss: 1.9546
[09/26 05:33:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 79.00	
[09/26 05:33:53 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 05:34:00 visual_prompt]: Epoch 79 / 100: avg data time: 4.81e-02, avg batch time: 0.4966, average train loss: 0.1017
[09/26 05:34:01 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1700, average loss: 1.9504
[09/26 05:34:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 79.50	
[09/26 05:34:01 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 05:34:08 visual_prompt]: Epoch 80 / 100: avg data time: 5.38e-02, avg batch time: 0.5031, average train loss: 0.1012
[09/26 05:34:10 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1696, average loss: 1.9394
[09/26 05:34:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 78.00	
[09/26 05:34:10 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 05:34:17 visual_prompt]: Epoch 81 / 100: avg data time: 5.59e-02, avg batch time: 0.5041, average train loss: 0.1012
[09/26 05:34:18 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1700, average loss: 1.9634
[09/26 05:34:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 78.50	
[09/26 05:34:18 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 05:34:25 visual_prompt]: Epoch 82 / 100: avg data time: 4.94e-02, avg batch time: 0.4993, average train loss: 0.1008
[09/26 05:34:26 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 1.9661
[09/26 05:34:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 79.00	
[09/26 05:34:26 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 05:34:33 visual_prompt]: Epoch 83 / 100: avg data time: 5.87e-02, avg batch time: 0.5098, average train loss: 0.1007
[09/26 05:34:35 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1699, average loss: 1.9658
[09/26 05:34:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 80.50	
[09/26 05:34:35 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 05:34:42 visual_prompt]: Epoch 84 / 100: avg data time: 5.41e-02, avg batch time: 0.5031, average train loss: 0.1009
[09/26 05:34:43 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1697, average loss: 1.9637
[09/26 05:34:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 79.00	
[09/26 05:34:43 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 05:34:50 visual_prompt]: Epoch 85 / 100: avg data time: 5.81e-02, avg batch time: 0.5073, average train loss: 0.1008
[09/26 05:34:52 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1692, average loss: 1.9597
[09/26 05:34:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 78.00	
[09/26 05:34:52 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 05:34:59 visual_prompt]: Epoch 86 / 100: avg data time: 5.89e-02, avg batch time: 0.5079, average train loss: 0.1001
[09/26 05:35:00 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1698, average loss: 1.9535
[09/26 05:35:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 79.00	
[09/26 05:35:00 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 05:35:07 visual_prompt]: Epoch 87 / 100: avg data time: 5.74e-02, avg batch time: 0.5056, average train loss: 0.1000
[09/26 05:35:09 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1697, average loss: 1.9583
[09/26 05:35:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 78.00	
[09/26 05:35:09 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 05:35:16 visual_prompt]: Epoch 88 / 100: avg data time: 5.46e-02, avg batch time: 0.5046, average train loss: 0.1001
[09/26 05:35:17 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1699, average loss: 1.9648
[09/26 05:35:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 78.50	
[09/26 05:35:17 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 05:35:24 visual_prompt]: Epoch 89 / 100: avg data time: 4.36e-02, avg batch time: 0.4930, average train loss: 0.0996
[09/26 05:35:25 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1700, average loss: 1.9578
[09/26 05:35:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 77.00	
[09/26 05:35:25 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 05:35:32 visual_prompt]: Epoch 90 / 100: avg data time: 4.39e-02, avg batch time: 0.4932, average train loss: 0.0996
[09/26 05:35:34 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1696, average loss: 1.9720
[09/26 05:35:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 76.00	
[09/26 05:35:34 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 05:35:40 visual_prompt]: Epoch 91 / 100: avg data time: 5.57e-02, avg batch time: 0.5040, average train loss: 0.0991
[09/26 05:35:42 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1696, average loss: 1.9756
[09/26 05:35:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 76.50	
[09/26 05:35:42 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 05:35:49 visual_prompt]: Epoch 92 / 100: avg data time: 5.43e-02, avg batch time: 0.5025, average train loss: 0.0991
[09/26 05:35:50 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1700, average loss: 1.9756
[09/26 05:35:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 77.50	
[09/26 05:35:50 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 05:35:57 visual_prompt]: Epoch 93 / 100: avg data time: 5.97e-02, avg batch time: 0.5085, average train loss: 0.0995
[09/26 05:35:59 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.1699, average loss: 1.9750
[09/26 05:35:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 77.00	
[09/26 05:35:59 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 05:36:06 visual_prompt]: Epoch 94 / 100: avg data time: 5.84e-02, avg batch time: 0.5066, average train loss: 0.0993
[09/26 05:36:07 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1693, average loss: 1.9697
[09/26 05:36:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 77.00	
[09/26 05:36:07 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 05:36:14 visual_prompt]: Epoch 95 / 100: avg data time: 6.12e-02, avg batch time: 0.5106, average train loss: 0.0992
[09/26 05:36:16 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1693, average loss: 1.9723
[09/26 05:36:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 77.50	
[09/26 05:36:16 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 05:36:23 visual_prompt]: Epoch 96 / 100: avg data time: 4.31e-02, avg batch time: 0.4940, average train loss: 0.0991
[09/26 05:36:24 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 1.9717
[09/26 05:36:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 76.50	
[09/26 05:36:24 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 05:36:31 visual_prompt]: Epoch 97 / 100: avg data time: 5.68e-02, avg batch time: 0.5062, average train loss: 0.0990
[09/26 05:36:33 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 1.9725
[09/26 05:36:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 77.50	
[09/26 05:36:33 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 05:36:39 visual_prompt]: Epoch 98 / 100: avg data time: 4.80e-02, avg batch time: 0.4972, average train loss: 0.0989
[09/26 05:36:41 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 1.9722
[09/26 05:36:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 77.00	
[09/26 05:36:41 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 05:36:48 visual_prompt]: Epoch 99 / 100: avg data time: 5.52e-02, avg batch time: 0.5053, average train loss: 0.0993
[09/26 05:36:49 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 1.9722
[09/26 05:36:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 77.00	
[09/26 05:36:49 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 05:36:56 visual_prompt]: Epoch 100 / 100: avg data time: 5.45e-02, avg batch time: 0.5031, average train loss: 0.0990
[09/26 05:36:58 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1697, average loss: 1.9721
[09/26 05:36:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 77.00	
[09/26 05:36:58 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 05:36:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 05:36:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 05:36:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 05:36:58 visual_prompt]: Training with config:
[09/26 05:36:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 05:36:58 visual_prompt]: Loading training data...
[09/26 05:36:58 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 05:37:00 visual_prompt]: Number of images: 800
[09/26 05:37:00 visual_prompt]: Number of classes: 47 / 47
[09/26 05:37:00 visual_prompt]: Loading validation data...
[09/26 05:37:00 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 05:37:01 visual_prompt]: Number of images: 200
[09/26 05:37:01 visual_prompt]: Number of classes: 47 / 47
[09/26 05:37:01 visual_prompt]: Constructing models...
[09/26 05:37:03 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 05:37:03 visual_prompt]: tuned percent:0.576
[09/26 05:37:03 visual_prompt]: Device used for model: 0
[09/26 05:37:03 visual_prompt]: Setting up Evaluator...
[09/26 05:37:03 visual_prompt]: Setting up Trainer...
[09/26 05:37:03 visual_prompt]: 	Setting up the optimizer...
[09/26 05:37:03 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 05:37:10 visual_prompt]: Epoch 1 / 100: avg data time: 6.24e-02, avg batch time: 0.5113, average train loss: 3.9308
[09/26 05:37:12 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1691, average loss: 3.9045
[09/26 05:37:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 05:37:12 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 05:37:12 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 05:37:18 visual_prompt]: Epoch 2 / 100: avg data time: 5.60e-02, avg batch time: 0.5034, average train loss: 3.8820
[09/26 05:37:20 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 3.8799
[09/26 05:37:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 11.50	
[09/26 05:37:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 05:37:27 visual_prompt]: Epoch 3 / 100: avg data time: 5.07e-02, avg batch time: 0.4986, average train loss: 3.8433
[09/26 05:37:28 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1692, average loss: 3.8915
[09/26 05:37:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 11.00	
[09/26 05:37:28 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 05:37:35 visual_prompt]: Epoch 4 / 100: avg data time: 5.52e-02, avg batch time: 0.5043, average train loss: 3.7873
[09/26 05:37:37 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1694, average loss: 3.8071
[09/26 05:37:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 19.50	
[09/26 05:37:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 05:37:44 visual_prompt]: Epoch 5 / 100: avg data time: 5.94e-02, avg batch time: 0.5067, average train loss: 3.6207
[09/26 05:37:45 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1698, average loss: 3.6180
[09/26 05:37:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.00	top5: 26.50	
[09/26 05:37:45 visual_prompt]: Best epoch 5: best metric: 0.070
[09/26 05:37:45 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 05:37:52 visual_prompt]: Epoch 6 / 100: avg data time: 5.11e-02, avg batch time: 0.5004, average train loss: 3.3095
[09/26 05:37:53 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 3.6433
[09/26 05:37:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 6.50	top5: 32.00	
[09/26 05:37:53 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 05:38:00 visual_prompt]: Epoch 7 / 100: avg data time: 5.87e-02, avg batch time: 0.5066, average train loss: 3.0031
[09/26 05:38:02 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1697, average loss: 3.4916
[09/26 05:38:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.50	top5: 36.50	
[09/26 05:38:02 visual_prompt]: Best epoch 7: best metric: 0.125
[09/26 05:38:02 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 05:38:09 visual_prompt]: Epoch 8 / 100: avg data time: 6.60e-02, avg batch time: 0.5136, average train loss: 2.5917
[09/26 05:38:10 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1694, average loss: 2.8647
[09/26 05:38:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 54.50	
[09/26 05:38:10 visual_prompt]: Best epoch 8: best metric: 0.245
[09/26 05:38:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 05:38:17 visual_prompt]: Epoch 9 / 100: avg data time: 5.50e-02, avg batch time: 0.5041, average train loss: 2.1297
[09/26 05:38:19 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1697, average loss: 2.7419
[09/26 05:38:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.50	top5: 60.50	
[09/26 05:38:19 visual_prompt]: Best epoch 9: best metric: 0.265
[09/26 05:38:19 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 05:38:26 visual_prompt]: Epoch 10 / 100: avg data time: 5.94e-02, avg batch time: 0.5080, average train loss: 1.6101
[09/26 05:38:27 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1698, average loss: 2.4998
[09/26 05:38:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 35.00	top5: 67.00	
[09/26 05:38:27 visual_prompt]: Best epoch 10: best metric: 0.350
[09/26 05:38:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 05:38:34 visual_prompt]: Epoch 11 / 100: avg data time: 5.41e-02, avg batch time: 0.5029, average train loss: 1.2447
[09/26 05:38:36 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 2.2898
[09/26 05:38:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 35.50	top5: 70.50	
[09/26 05:38:36 visual_prompt]: Best epoch 11: best metric: 0.355
[09/26 05:38:36 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 05:38:42 visual_prompt]: Epoch 12 / 100: avg data time: 4.39e-02, avg batch time: 0.4925, average train loss: 0.8988
[09/26 05:38:44 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 2.1699
[09/26 05:38:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 75.50	
[09/26 05:38:44 visual_prompt]: Best epoch 12: best metric: 0.425
[09/26 05:38:44 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 05:38:51 visual_prompt]: Epoch 13 / 100: avg data time: 6.25e-02, avg batch time: 0.5112, average train loss: 0.6065
[09/26 05:38:52 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1696, average loss: 2.2983
[09/26 05:38:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.50	top5: 73.00	
[09/26 05:38:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 05:38:59 visual_prompt]: Epoch 14 / 100: avg data time: 5.10e-02, avg batch time: 0.5003, average train loss: 0.3760
[09/26 05:39:01 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1693, average loss: 2.3582
[09/26 05:39:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 72.50	
[09/26 05:39:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 05:39:08 visual_prompt]: Epoch 15 / 100: avg data time: 5.65e-02, avg batch time: 0.5047, average train loss: 0.2814
[09/26 05:39:09 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1698, average loss: 2.1819
[09/26 05:39:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 76.50	
[09/26 05:39:09 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 05:39:16 visual_prompt]: Epoch 16 / 100: avg data time: 4.57e-02, avg batch time: 0.4946, average train loss: 0.1814
[09/26 05:39:18 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1694, average loss: 2.0730
[09/26 05:39:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 76.00	
[09/26 05:39:18 visual_prompt]: Best epoch 16: best metric: 0.475
[09/26 05:39:18 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 05:39:24 visual_prompt]: Epoch 17 / 100: avg data time: 4.88e-02, avg batch time: 0.4983, average train loss: 0.1444
[09/26 05:39:26 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1694, average loss: 2.1572
[09/26 05:39:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.00	top5: 76.50	
[09/26 05:39:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 05:39:33 visual_prompt]: Epoch 18 / 100: avg data time: 4.61e-02, avg batch time: 0.4963, average train loss: 0.0995
[09/26 05:39:34 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1701, average loss: 2.1783
[09/26 05:39:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.50	
[09/26 05:39:34 visual_prompt]: Best epoch 18: best metric: 0.480
[09/26 05:39:34 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 05:39:41 visual_prompt]: Epoch 19 / 100: avg data time: 5.72e-02, avg batch time: 0.5060, average train loss: 0.0783
[09/26 05:39:43 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 2.2712
[09/26 05:39:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.00	top5: 73.50	
[09/26 05:39:43 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 05:39:49 visual_prompt]: Epoch 20 / 100: avg data time: 5.69e-02, avg batch time: 0.5058, average train loss: 0.0601
[09/26 05:39:51 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1698, average loss: 2.2556
[09/26 05:39:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 74.00	
[09/26 05:39:51 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 05:39:58 visual_prompt]: Epoch 21 / 100: avg data time: 5.27e-02, avg batch time: 0.5024, average train loss: 0.0444
[09/26 05:39:59 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1692, average loss: 2.2865
[09/26 05:39:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 76.00	
[09/26 05:39:59 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 05:40:06 visual_prompt]: Epoch 22 / 100: avg data time: 5.23e-02, avg batch time: 0.5018, average train loss: 0.0361
[09/26 05:40:08 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 2.2142
[09/26 05:40:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.00	
[09/26 05:40:08 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 05:40:15 visual_prompt]: Epoch 23 / 100: avg data time: 5.20e-02, avg batch time: 0.5011, average train loss: 0.0306
[09/26 05:40:16 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1696, average loss: 2.2526
[09/26 05:40:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.00	
[09/26 05:40:16 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 05:40:23 visual_prompt]: Epoch 24 / 100: avg data time: 5.66e-02, avg batch time: 0.5056, average train loss: 0.0281
[09/26 05:40:24 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1696, average loss: 2.2683
[09/26 05:40:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 05:40:24 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 05:40:31 visual_prompt]: Epoch 25 / 100: avg data time: 5.36e-02, avg batch time: 0.5021, average train loss: 0.0266
[09/26 05:40:33 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1699, average loss: 2.1925
[09/26 05:40:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.50	
[09/26 05:40:33 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 05:40:40 visual_prompt]: Epoch 26 / 100: avg data time: 4.32e-02, avg batch time: 0.4953, average train loss: 0.0236
[09/26 05:40:41 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1695, average loss: 2.2194
[09/26 05:40:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 77.00	
[09/26 05:40:41 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 05:40:48 visual_prompt]: Epoch 27 / 100: avg data time: 5.34e-02, avg batch time: 0.5022, average train loss: 0.0227
[09/26 05:40:50 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1698, average loss: 2.2209
[09/26 05:40:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 75.50	
[09/26 05:40:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 05:40:56 visual_prompt]: Epoch 28 / 100: avg data time: 5.17e-02, avg batch time: 0.5025, average train loss: 0.0222
[09/26 05:40:58 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1696, average loss: 2.2263
[09/26 05:40:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 76.50	
[09/26 05:40:58 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 05:41:05 visual_prompt]: Epoch 29 / 100: avg data time: 4.44e-02, avg batch time: 0.4946, average train loss: 0.0209
[09/26 05:41:06 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1698, average loss: 2.2415
[09/26 05:41:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 77.00	
[09/26 05:41:06 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 05:41:13 visual_prompt]: Epoch 30 / 100: avg data time: 5.43e-02, avg batch time: 0.5034, average train loss: 0.0204
[09/26 05:41:15 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 2.2589
[09/26 05:41:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 76.00	
[09/26 05:41:15 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 05:41:21 visual_prompt]: Epoch 31 / 100: avg data time: 5.39e-02, avg batch time: 0.5022, average train loss: 0.0202
[09/26 05:41:23 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1695, average loss: 2.2456
[09/26 05:41:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 76.50	
[09/26 05:41:23 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 05:41:30 visual_prompt]: Epoch 32 / 100: avg data time: 5.35e-02, avg batch time: 0.5028, average train loss: 0.0194
[09/26 05:41:31 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1699, average loss: 2.2283
[09/26 05:41:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 76.50	
[09/26 05:41:31 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 05:41:38 visual_prompt]: Epoch 33 / 100: avg data time: 6.01e-02, avg batch time: 0.5106, average train loss: 0.0181
[09/26 05:41:40 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1697, average loss: 2.2043
[09/26 05:41:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.00	
[09/26 05:41:40 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 05:41:47 visual_prompt]: Epoch 34 / 100: avg data time: 5.22e-02, avg batch time: 0.5007, average train loss: 0.0177
[09/26 05:41:48 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1697, average loss: 2.2034
[09/26 05:41:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.00	
[09/26 05:41:48 visual_prompt]: Best epoch 34: best metric: 0.490
[09/26 05:41:48 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 05:41:55 visual_prompt]: Epoch 35 / 100: avg data time: 6.04e-02, avg batch time: 0.5084, average train loss: 0.0173
[09/26 05:41:57 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1696, average loss: 2.2194
[09/26 05:41:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 77.00	
[09/26 05:41:57 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 05:42:03 visual_prompt]: Epoch 36 / 100: avg data time: 5.84e-02, avg batch time: 0.5068, average train loss: 0.0180
[09/26 05:42:05 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1695, average loss: 2.2170
[09/26 05:42:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 76.00	
[09/26 05:42:05 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 05:42:12 visual_prompt]: Epoch 37 / 100: avg data time: 5.64e-02, avg batch time: 0.5055, average train loss: 0.0178
[09/26 05:42:13 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 2.2138
[09/26 05:42:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.00	
[09/26 05:42:13 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 05:42:20 visual_prompt]: Epoch 38 / 100: avg data time: 5.67e-02, avg batch time: 0.5063, average train loss: 0.0168
[09/26 05:42:22 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1696, average loss: 2.2403
[09/26 05:42:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 76.00	
[09/26 05:42:22 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 05:42:29 visual_prompt]: Epoch 39 / 100: avg data time: 5.73e-02, avg batch time: 0.5069, average train loss: 0.0168
[09/26 05:42:30 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1698, average loss: 2.2252
[09/26 05:42:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 75.50	
[09/26 05:42:30 visual_prompt]: Best epoch 39: best metric: 0.505
[09/26 05:42:30 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 05:42:37 visual_prompt]: Epoch 40 / 100: avg data time: 6.17e-02, avg batch time: 0.5109, average train loss: 0.0174
[09/26 05:42:39 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1694, average loss: 2.2123
[09/26 05:42:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 77.00	
[09/26 05:42:39 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 05:42:46 visual_prompt]: Epoch 41 / 100: avg data time: 6.21e-02, avg batch time: 0.5106, average train loss: 0.0163
[09/26 05:42:47 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 2.2253
[09/26 05:42:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.50	
[09/26 05:42:47 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 05:42:54 visual_prompt]: Epoch 42 / 100: avg data time: 5.70e-02, avg batch time: 0.5050, average train loss: 0.0164
[09/26 05:42:56 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1697, average loss: 2.1995
[09/26 05:42:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.50	
[09/26 05:42:56 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 05:43:03 visual_prompt]: Epoch 43 / 100: avg data time: 5.60e-02, avg batch time: 0.5042, average train loss: 0.0161
[09/26 05:43:04 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 2.1974
[09/26 05:43:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 05:43:04 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 05:43:11 visual_prompt]: Epoch 44 / 100: avg data time: 5.39e-02, avg batch time: 0.5026, average train loss: 0.0159
[09/26 05:43:13 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1696, average loss: 2.1979
[09/26 05:43:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 76.50	
[09/26 05:43:13 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 05:43:19 visual_prompt]: Epoch 45 / 100: avg data time: 4.61e-02, avg batch time: 0.5017, average train loss: 0.0158
[09/26 05:43:21 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1697, average loss: 2.1935
[09/26 05:43:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 77.00	
[09/26 05:43:21 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 05:43:28 visual_prompt]: Epoch 46 / 100: avg data time: 5.56e-02, avg batch time: 0.5050, average train loss: 0.0152
[09/26 05:43:29 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1695, average loss: 2.2098
[09/26 05:43:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 76.50	
[09/26 05:43:29 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 05:43:36 visual_prompt]: Epoch 47 / 100: avg data time: 6.61e-02, avg batch time: 0.5141, average train loss: 0.0151
[09/26 05:43:38 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1697, average loss: 2.2264
[09/26 05:43:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 77.50	
[09/26 05:43:38 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 05:43:45 visual_prompt]: Epoch 48 / 100: avg data time: 5.37e-02, avg batch time: 0.5025, average train loss: 0.0156
[09/26 05:43:46 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1695, average loss: 2.1969
[09/26 05:43:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 77.00	
[09/26 05:43:46 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 05:43:53 visual_prompt]: Epoch 49 / 100: avg data time: 6.13e-02, avg batch time: 0.5095, average train loss: 0.0154
[09/26 05:43:55 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 2.1872
[09/26 05:43:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 76.50	
[09/26 05:43:55 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 05:44:02 visual_prompt]: Epoch 50 / 100: avg data time: 5.78e-02, avg batch time: 0.5070, average train loss: 0.0151
[09/26 05:44:03 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1696, average loss: 2.1967
[09/26 05:44:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.50	
[09/26 05:44:03 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 05:44:10 visual_prompt]: Epoch 51 / 100: avg data time: 5.12e-02, avg batch time: 0.4995, average train loss: 0.0147
[09/26 05:44:12 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1698, average loss: 2.2079
[09/26 05:44:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.00	
[09/26 05:44:12 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 05:44:18 visual_prompt]: Epoch 52 / 100: avg data time: 5.05e-02, avg batch time: 0.4999, average train loss: 0.0148
[09/26 05:44:20 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.1695, average loss: 2.2064
[09/26 05:44:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 77.00	
[09/26 05:44:20 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 05:44:27 visual_prompt]: Epoch 53 / 100: avg data time: 6.14e-02, avg batch time: 0.5097, average train loss: 0.0148
[09/26 05:44:28 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1699, average loss: 2.1977
[09/26 05:44:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.50	
[09/26 05:44:28 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 05:44:35 visual_prompt]: Epoch 54 / 100: avg data time: 6.43e-02, avg batch time: 0.5132, average train loss: 0.0149
[09/26 05:44:37 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1697, average loss: 2.2233
[09/26 05:44:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 75.50	
[09/26 05:44:37 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 05:44:44 visual_prompt]: Epoch 55 / 100: avg data time: 5.95e-02, avg batch time: 0.5073, average train loss: 0.0155
[09/26 05:44:46 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 2.2025
[09/26 05:44:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.50	
[09/26 05:44:46 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 05:44:52 visual_prompt]: Epoch 56 / 100: avg data time: 5.28e-02, avg batch time: 0.5012, average train loss: 0.0147
[09/26 05:44:54 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1698, average loss: 2.1994
[09/26 05:44:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 77.00	
[09/26 05:44:54 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 05:45:01 visual_prompt]: Epoch 57 / 100: avg data time: 5.92e-02, avg batch time: 0.5074, average train loss: 0.0146
[09/26 05:45:03 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1696, average loss: 2.2176
[09/26 05:45:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 77.00	
[09/26 05:45:03 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 05:45:09 visual_prompt]: Epoch 58 / 100: avg data time: 5.64e-02, avg batch time: 0.5045, average train loss: 0.0142
[09/26 05:45:11 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1698, average loss: 2.2184
[09/26 05:45:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:45:11 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 05:45:18 visual_prompt]: Epoch 59 / 100: avg data time: 6.30e-02, avg batch time: 0.5127, average train loss: 0.0146
[09/26 05:45:19 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1695, average loss: 2.2079
[09/26 05:45:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.50	
[09/26 05:45:19 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 05:45:26 visual_prompt]: Epoch 60 / 100: avg data time: 5.56e-02, avg batch time: 0.5045, average train loss: 0.0142
[09/26 05:45:28 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1695, average loss: 2.2099
[09/26 05:45:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 76.00	
[09/26 05:45:28 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 05:45:35 visual_prompt]: Epoch 61 / 100: avg data time: 5.41e-02, avg batch time: 0.5024, average train loss: 0.0142
[09/26 05:45:36 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1692, average loss: 2.1921
[09/26 05:45:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 76.00	
[09/26 05:45:36 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 05:45:43 visual_prompt]: Epoch 62 / 100: avg data time: 5.40e-02, avg batch time: 0.5028, average train loss: 0.0137
[09/26 05:45:45 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1695, average loss: 2.2014
[09/26 05:45:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.50	
[09/26 05:45:45 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 05:45:52 visual_prompt]: Epoch 63 / 100: avg data time: 6.13e-02, avg batch time: 0.5103, average train loss: 0.0139
[09/26 05:45:53 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1692, average loss: 2.2097
[09/26 05:45:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.50	
[09/26 05:45:53 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 05:46:00 visual_prompt]: Epoch 64 / 100: avg data time: 5.11e-02, avg batch time: 0.5009, average train loss: 0.0141
[09/26 05:46:02 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1691, average loss: 2.2009
[09/26 05:46:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 78.00	
[09/26 05:46:02 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 05:46:08 visual_prompt]: Epoch 65 / 100: avg data time: 4.94e-02, avg batch time: 0.4994, average train loss: 0.0134
[09/26 05:46:10 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1698, average loss: 2.2079
[09/26 05:46:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.00	
[09/26 05:46:10 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 05:46:17 visual_prompt]: Epoch 66 / 100: avg data time: 5.62e-02, avg batch time: 0.5044, average train loss: 0.0140
[09/26 05:46:18 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 2.2042
[09/26 05:46:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 77.00	
[09/26 05:46:18 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 05:46:25 visual_prompt]: Epoch 67 / 100: avg data time: 6.13e-02, avg batch time: 0.5097, average train loss: 0.0143
[09/26 05:46:27 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1697, average loss: 2.1990
[09/26 05:46:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 05:46:27 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 05:46:34 visual_prompt]: Epoch 68 / 100: avg data time: 6.36e-02, avg batch time: 0.5127, average train loss: 0.0139
[09/26 05:46:35 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1693, average loss: 2.2084
[09/26 05:46:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.50	
[09/26 05:46:35 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 05:46:42 visual_prompt]: Epoch 69 / 100: avg data time: 6.43e-02, avg batch time: 0.5130, average train loss: 0.0138
[09/26 05:46:44 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1698, average loss: 2.2158
[09/26 05:46:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 77.00	
[09/26 05:46:44 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 05:46:51 visual_prompt]: Epoch 70 / 100: avg data time: 6.10e-02, avg batch time: 0.5103, average train loss: 0.0132
[09/26 05:46:52 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1697, average loss: 2.2050
[09/26 05:46:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 76.50	
[09/26 05:46:52 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 05:46:59 visual_prompt]: Epoch 71 / 100: avg data time: 4.73e-02, avg batch time: 0.4986, average train loss: 0.0140
[09/26 05:47:01 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 2.1937
[09/26 05:47:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 76.50	
[09/26 05:47:01 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 05:47:07 visual_prompt]: Epoch 72 / 100: avg data time: 5.15e-02, avg batch time: 0.5001, average train loss: 0.0136
[09/26 05:47:09 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1696, average loss: 2.2023
[09/26 05:47:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 75.50	
[09/26 05:47:09 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 05:47:16 visual_prompt]: Epoch 73 / 100: avg data time: 6.05e-02, avg batch time: 0.5087, average train loss: 0.0135
[09/26 05:47:17 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1694, average loss: 2.2049
[09/26 05:47:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 76.00	
[09/26 05:47:17 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 05:47:24 visual_prompt]: Epoch 74 / 100: avg data time: 5.27e-02, avg batch time: 0.5020, average train loss: 0.0134
[09/26 05:47:26 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1698, average loss: 2.1903
[09/26 05:47:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 76.00	
[09/26 05:47:26 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 05:47:33 visual_prompt]: Epoch 75 / 100: avg data time: 5.77e-02, avg batch time: 0.5069, average train loss: 0.0133
[09/26 05:47:34 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1701, average loss: 2.1852
[09/26 05:47:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 76.00	
[09/26 05:47:34 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 05:47:41 visual_prompt]: Epoch 76 / 100: avg data time: 6.33e-02, avg batch time: 0.5114, average train loss: 0.0133
[09/26 05:47:43 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 2.1856
[09/26 05:47:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.00	
[09/26 05:47:43 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 05:47:49 visual_prompt]: Epoch 77 / 100: avg data time: 5.68e-02, avg batch time: 0.5057, average train loss: 0.0132
[09/26 05:47:51 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1696, average loss: 2.1898
[09/26 05:47:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.00	
[09/26 05:47:51 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 05:47:58 visual_prompt]: Epoch 78 / 100: avg data time: 6.30e-02, avg batch time: 0.5114, average train loss: 0.0133
[09/26 05:48:00 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1696, average loss: 2.1900
[09/26 05:48:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 76.50	
[09/26 05:48:00 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 05:48:06 visual_prompt]: Epoch 79 / 100: avg data time: 5.50e-02, avg batch time: 0.5037, average train loss: 0.0129
[09/26 05:48:08 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1695, average loss: 2.1866
[09/26 05:48:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 76.00	
[09/26 05:48:08 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 05:48:15 visual_prompt]: Epoch 80 / 100: avg data time: 6.34e-02, avg batch time: 0.5112, average train loss: 0.0135
[09/26 05:48:17 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1698, average loss: 2.1850
[09/26 05:48:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 76.00	
[09/26 05:48:17 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 05:48:23 visual_prompt]: Epoch 81 / 100: avg data time: 4.73e-02, avg batch time: 0.4959, average train loss: 0.0130
[09/26 05:48:25 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1697, average loss: 2.1898
[09/26 05:48:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 76.00	
[09/26 05:48:25 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 05:48:32 visual_prompt]: Epoch 82 / 100: avg data time: 5.43e-02, avg batch time: 0.5024, average train loss: 0.0133
[09/26 05:48:33 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1698, average loss: 2.1942
[09/26 05:48:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:48:33 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 05:48:40 visual_prompt]: Epoch 83 / 100: avg data time: 4.88e-02, avg batch time: 0.4981, average train loss: 0.0129
[09/26 05:48:42 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.1703, average loss: 2.1936
[09/26 05:48:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:48:42 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 05:48:49 visual_prompt]: Epoch 84 / 100: avg data time: 5.74e-02, avg batch time: 0.5067, average train loss: 0.0135
[09/26 05:48:50 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1699, average loss: 2.1946
[09/26 05:48:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 76.00	
[09/26 05:48:50 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 05:48:57 visual_prompt]: Epoch 85 / 100: avg data time: 5.18e-02, avg batch time: 0.5005, average train loss: 0.0135
[09/26 05:48:58 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1699, average loss: 2.1934
[09/26 05:48:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 76.00	
[09/26 05:48:58 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 05:49:05 visual_prompt]: Epoch 86 / 100: avg data time: 4.48e-02, avg batch time: 0.4962, average train loss: 0.0130
[09/26 05:49:07 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1698, average loss: 2.1906
[09/26 05:49:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 76.00	
[09/26 05:49:07 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 05:49:13 visual_prompt]: Epoch 87 / 100: avg data time: 4.63e-02, avg batch time: 0.4958, average train loss: 0.0135
[09/26 05:49:15 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1697, average loss: 2.1905
[09/26 05:49:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:49:15 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 05:49:22 visual_prompt]: Epoch 88 / 100: avg data time: 5.14e-02, avg batch time: 0.5006, average train loss: 0.0133
[09/26 05:49:23 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1695, average loss: 2.1897
[09/26 05:49:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:49:23 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 05:49:30 visual_prompt]: Epoch 89 / 100: avg data time: 5.45e-02, avg batch time: 0.5027, average train loss: 0.0131
[09/26 05:49:32 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1697, average loss: 2.1910
[09/26 05:49:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:49:32 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 05:49:39 visual_prompt]: Epoch 90 / 100: avg data time: 4.47e-02, avg batch time: 0.4960, average train loss: 0.0130
[09/26 05:49:40 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1694, average loss: 2.1904
[09/26 05:49:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:49:40 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 05:49:47 visual_prompt]: Epoch 91 / 100: avg data time: 4.86e-02, avg batch time: 0.4979, average train loss: 0.0133
[09/26 05:49:49 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1698, average loss: 2.1908
[09/26 05:49:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:49:49 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 05:49:55 visual_prompt]: Epoch 92 / 100: avg data time: 5.35e-02, avg batch time: 0.5025, average train loss: 0.0129
[09/26 05:49:57 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 2.1903
[09/26 05:49:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:49:57 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 05:50:04 visual_prompt]: Epoch 93 / 100: avg data time: 5.07e-02, avg batch time: 0.4990, average train loss: 0.0127
[09/26 05:50:05 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1697, average loss: 2.1902
[09/26 05:50:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:50:05 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 05:50:12 visual_prompt]: Epoch 94 / 100: avg data time: 4.28e-02, avg batch time: 0.4926, average train loss: 0.0133
[09/26 05:50:14 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1697, average loss: 2.1900
[09/26 05:50:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:50:14 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 05:50:20 visual_prompt]: Epoch 95 / 100: avg data time: 5.57e-02, avg batch time: 0.5065, average train loss: 0.0131
[09/26 05:50:22 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1697, average loss: 2.1901
[09/26 05:50:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:50:22 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 05:50:29 visual_prompt]: Epoch 96 / 100: avg data time: 5.54e-02, avg batch time: 0.5052, average train loss: 0.0128
[09/26 05:50:30 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1696, average loss: 2.1900
[09/26 05:50:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:50:30 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 05:50:37 visual_prompt]: Epoch 97 / 100: avg data time: 6.05e-02, avg batch time: 0.5099, average train loss: 0.0130
[09/26 05:50:39 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1698, average loss: 2.1900
[09/26 05:50:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:50:39 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 05:50:46 visual_prompt]: Epoch 98 / 100: avg data time: 4.62e-02, avg batch time: 0.4944, average train loss: 0.0130
[09/26 05:50:47 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1697, average loss: 2.1900
[09/26 05:50:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:50:47 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 05:50:54 visual_prompt]: Epoch 99 / 100: avg data time: 5.41e-02, avg batch time: 0.5037, average train loss: 0.0136
[09/26 05:50:55 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1699, average loss: 2.1900
[09/26 05:50:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:50:55 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 05:51:02 visual_prompt]: Epoch 100 / 100: avg data time: 4.63e-02, avg batch time: 0.4950, average train loss: 0.0129
[09/26 05:51:04 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1698, average loss: 2.1900
[09/26 05:51:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 05:51:04 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 05:51:04 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 05:51:04 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 05:51:04 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 05:51:04 visual_prompt]: Training with config:
[09/26 05:51:04 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 05:51:04 visual_prompt]: Loading training data...
[09/26 05:51:04 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 05:51:06 visual_prompt]: Number of images: 800
[09/26 05:51:06 visual_prompt]: Number of classes: 47 / 47
[09/26 05:51:06 visual_prompt]: Loading validation data...
[09/26 05:51:06 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 05:51:07 visual_prompt]: Number of images: 200
[09/26 05:51:07 visual_prompt]: Number of classes: 47 / 47
[09/26 05:51:07 visual_prompt]: Constructing models...
[09/26 05:51:09 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 05:51:09 visual_prompt]: tuned percent:0.576
[09/26 05:51:09 visual_prompt]: Device used for model: 0
[09/26 05:51:09 visual_prompt]: Setting up Evaluator...
[09/26 05:51:09 visual_prompt]: Setting up Trainer...
[09/26 05:51:09 visual_prompt]: 	Setting up the optimizer...
[09/26 05:51:09 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 05:51:16 visual_prompt]: Epoch 1 / 100: avg data time: 5.91e-02, avg batch time: 0.5081, average train loss: 3.9293
[09/26 05:51:18 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1687, average loss: 3.9045
[09/26 05:51:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 05:51:18 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 05:51:18 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 05:51:25 visual_prompt]: Epoch 2 / 100: avg data time: 4.64e-02, avg batch time: 0.4962, average train loss: 3.8791
[09/26 05:51:26 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1687, average loss: 3.8857
[09/26 05:51:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 8.50	
[09/26 05:51:26 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 05:51:33 visual_prompt]: Epoch 3 / 100: avg data time: 5.62e-02, avg batch time: 0.5029, average train loss: 3.8366
[09/26 05:51:34 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1691, average loss: 3.8724
[09/26 05:51:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 10.50	
[09/26 05:51:34 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 05:51:41 visual_prompt]: Epoch 4 / 100: avg data time: 4.92e-02, avg batch time: 0.4974, average train loss: 3.7806
[09/26 05:51:43 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1691, average loss: 3.8272
[09/26 05:51:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 14.50	
[09/26 05:51:43 visual_prompt]: Best epoch 4: best metric: 0.045
[09/26 05:51:43 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 05:51:49 visual_prompt]: Epoch 5 / 100: avg data time: 4.83e-02, avg batch time: 0.4973, average train loss: 3.6584
[09/26 05:51:51 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1689, average loss: 3.5975
[09/26 05:51:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.00	top5: 28.50	
[09/26 05:51:51 visual_prompt]: Best epoch 5: best metric: 0.090
[09/26 05:51:51 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 05:51:58 visual_prompt]: Epoch 6 / 100: avg data time: 4.39e-02, avg batch time: 0.4924, average train loss: 3.2862
[09/26 05:51:59 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1692, average loss: 3.4189
[09/26 05:51:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 14.00	top5: 36.50	
[09/26 05:51:59 visual_prompt]: Best epoch 6: best metric: 0.140
[09/26 05:51:59 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 05:52:06 visual_prompt]: Epoch 7 / 100: avg data time: 5.35e-02, avg batch time: 0.5011, average train loss: 2.9023
[09/26 05:52:08 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 3.0765
[09/26 05:52:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 17.50	top5: 47.50	
[09/26 05:52:08 visual_prompt]: Best epoch 7: best metric: 0.175
[09/26 05:52:08 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 05:52:14 visual_prompt]: Epoch 8 / 100: avg data time: 4.98e-02, avg batch time: 0.4980, average train loss: 2.4344
[09/26 05:52:16 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1695, average loss: 3.0469
[09/26 05:52:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 17.50	top5: 53.50	
[09/26 05:52:16 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 05:52:23 visual_prompt]: Epoch 9 / 100: avg data time: 5.40e-02, avg batch time: 0.5014, average train loss: 1.9271
[09/26 05:52:24 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 2.7457
[09/26 05:52:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 29.00	top5: 65.50	
[09/26 05:52:24 visual_prompt]: Best epoch 9: best metric: 0.290
[09/26 05:52:24 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 05:52:31 visual_prompt]: Epoch 10 / 100: avg data time: 5.65e-02, avg batch time: 0.5049, average train loss: 1.5555
[09/26 05:52:33 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1691, average loss: 2.5163
[09/26 05:52:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 37.00	top5: 66.00	
[09/26 05:52:33 visual_prompt]: Best epoch 10: best metric: 0.370
[09/26 05:52:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 05:52:40 visual_prompt]: Epoch 11 / 100: avg data time: 5.23e-02, avg batch time: 0.5000, average train loss: 1.1117
[09/26 05:52:41 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 2.3254
[09/26 05:52:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 39.00	top5: 73.50	
[09/26 05:52:41 visual_prompt]: Best epoch 11: best metric: 0.390
[09/26 05:52:41 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 05:52:48 visual_prompt]: Epoch 12 / 100: avg data time: 4.75e-02, avg batch time: 0.4976, average train loss: 0.7837
[09/26 05:52:50 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 2.1454
[09/26 05:52:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 75.00	
[09/26 05:52:50 visual_prompt]: Best epoch 12: best metric: 0.415
[09/26 05:52:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 05:52:56 visual_prompt]: Epoch 13 / 100: avg data time: 4.45e-02, avg batch time: 0.4948, average train loss: 0.5520
[09/26 05:52:58 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1696, average loss: 2.0778
[09/26 05:52:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 75.00	
[09/26 05:52:58 visual_prompt]: Best epoch 13: best metric: 0.460
[09/26 05:52:58 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 05:53:05 visual_prompt]: Epoch 14 / 100: avg data time: 5.02e-02, avg batch time: 0.4996, average train loss: 0.3458
[09/26 05:53:06 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1695, average loss: 2.1467
[09/26 05:53:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 76.50	
[09/26 05:53:06 visual_prompt]: Best epoch 14: best metric: 0.470
[09/26 05:53:06 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 05:53:13 visual_prompt]: Epoch 15 / 100: avg data time: 4.50e-02, avg batch time: 0.4951, average train loss: 0.2071
[09/26 05:53:14 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1693, average loss: 2.2261
[09/26 05:53:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 76.00	
[09/26 05:53:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 05:53:21 visual_prompt]: Epoch 16 / 100: avg data time: 5.68e-02, avg batch time: 0.5053, average train loss: 0.1534
[09/26 05:53:23 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1689, average loss: 2.1881
[09/26 05:53:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.50	
[09/26 05:53:23 visual_prompt]: Best epoch 16: best metric: 0.485
[09/26 05:53:23 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 05:53:30 visual_prompt]: Epoch 17 / 100: avg data time: 5.52e-02, avg batch time: 0.5039, average train loss: 0.1085
[09/26 05:53:31 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1694, average loss: 2.1559
[09/26 05:53:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 75.00	
[09/26 05:53:31 visual_prompt]: Best epoch 17: best metric: 0.495
[09/26 05:53:31 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 05:53:38 visual_prompt]: Epoch 18 / 100: avg data time: 5.57e-02, avg batch time: 0.5048, average train loss: 0.0746
[09/26 05:53:40 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1694, average loss: 2.1839
[09/26 05:53:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 76.50	
[09/26 05:53:40 visual_prompt]: Best epoch 18: best metric: 0.500
[09/26 05:53:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 05:53:46 visual_prompt]: Epoch 19 / 100: avg data time: 5.17e-02, avg batch time: 0.5016, average train loss: 0.0552
[09/26 05:53:48 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1692, average loss: 2.1383
[09/26 05:53:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 79.50	
[09/26 05:53:48 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 05:53:55 visual_prompt]: Epoch 20 / 100: avg data time: 5.44e-02, avg batch time: 0.5027, average train loss: 0.0395
[09/26 05:53:56 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1694, average loss: 2.1487
[09/26 05:53:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 80.50	
[09/26 05:53:56 visual_prompt]: Best epoch 20: best metric: 0.510
[09/26 05:53:56 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 05:54:03 visual_prompt]: Epoch 21 / 100: avg data time: 5.24e-02, avg batch time: 0.5023, average train loss: 0.0306
[09/26 05:54:05 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1695, average loss: 2.1638
[09/26 05:54:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.50	
[09/26 05:54:05 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 05:54:11 visual_prompt]: Epoch 22 / 100: avg data time: 4.56e-02, avg batch time: 0.4941, average train loss: 0.0245
[09/26 05:54:13 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 2.1729
[09/26 05:54:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 78.00	
[09/26 05:54:13 visual_prompt]: Best epoch 22: best metric: 0.515
[09/26 05:54:13 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 05:54:20 visual_prompt]: Epoch 23 / 100: avg data time: 6.16e-02, avg batch time: 0.5127, average train loss: 0.0212
[09/26 05:54:21 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1694, average loss: 2.1956
[09/26 05:54:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 76.50	
[09/26 05:54:21 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 05:54:28 visual_prompt]: Epoch 24 / 100: avg data time: 6.31e-02, avg batch time: 0.5109, average train loss: 0.0192
[09/26 05:54:30 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1693, average loss: 2.2198
[09/26 05:54:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 76.00	
[09/26 05:54:30 visual_prompt]: Best epoch 24: best metric: 0.520
[09/26 05:54:30 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 05:54:37 visual_prompt]: Epoch 25 / 100: avg data time: 4.23e-02, avg batch time: 0.4927, average train loss: 0.0186
[09/26 05:54:38 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1691, average loss: 2.2111
[09/26 05:54:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 75.50	
[09/26 05:54:38 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 05:54:45 visual_prompt]: Epoch 26 / 100: avg data time: 4.86e-02, avg batch time: 0.4974, average train loss: 0.0163
[09/26 05:54:46 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1692, average loss: 2.2406
[09/26 05:54:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.00	
[09/26 05:54:46 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 05:54:53 visual_prompt]: Epoch 27 / 100: avg data time: 6.32e-02, avg batch time: 0.5112, average train loss: 0.0146
[09/26 05:54:55 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1693, average loss: 2.2495
[09/26 05:54:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 76.00	
[09/26 05:54:55 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 05:55:02 visual_prompt]: Epoch 28 / 100: avg data time: 6.22e-02, avg batch time: 0.5112, average train loss: 0.0144
[09/26 05:55:03 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 2.2331
[09/26 05:55:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 76.50	
[09/26 05:55:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 05:55:10 visual_prompt]: Epoch 29 / 100: avg data time: 4.48e-02, avg batch time: 0.4968, average train loss: 0.0117
[09/26 05:55:12 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1689, average loss: 2.2307
[09/26 05:55:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 76.00	
[09/26 05:55:12 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 05:55:18 visual_prompt]: Epoch 30 / 100: avg data time: 5.31e-02, avg batch time: 0.5024, average train loss: 0.0120
[09/26 05:55:20 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1694, average loss: 2.2459
[09/26 05:55:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 76.00	
[09/26 05:55:20 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 05:55:27 visual_prompt]: Epoch 31 / 100: avg data time: 4.35e-02, avg batch time: 0.4949, average train loss: 0.0115
[09/26 05:55:28 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 2.2591
[09/26 05:55:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 76.50	
[09/26 05:55:28 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 05:55:35 visual_prompt]: Epoch 32 / 100: avg data time: 5.21e-02, avg batch time: 0.5018, average train loss: 0.0101
[09/26 05:55:36 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1690, average loss: 2.2649
[09/26 05:55:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.00	
[09/26 05:55:36 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 05:55:43 visual_prompt]: Epoch 33 / 100: avg data time: 4.69e-02, avg batch time: 0.4959, average train loss: 0.0101
[09/26 05:55:45 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 2.2634
[09/26 05:55:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 77.00	
[09/26 05:55:45 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 05:55:51 visual_prompt]: Epoch 34 / 100: avg data time: 4.50e-02, avg batch time: 0.4952, average train loss: 0.0093
[09/26 05:55:53 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 2.2711
[09/26 05:55:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.50	
[09/26 05:55:53 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 05:56:00 visual_prompt]: Epoch 35 / 100: avg data time: 4.33e-02, avg batch time: 0.4945, average train loss: 0.0091
[09/26 05:56:01 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1690, average loss: 2.2774
[09/26 05:56:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 76.50	
[09/26 05:56:01 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 05:56:08 visual_prompt]: Epoch 36 / 100: avg data time: 5.79e-02, avg batch time: 0.5076, average train loss: 0.0091
[09/26 05:56:10 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1694, average loss: 2.2798
[09/26 05:56:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.00	
[09/26 05:56:10 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 05:56:16 visual_prompt]: Epoch 37 / 100: avg data time: 5.71e-02, avg batch time: 0.5069, average train loss: 0.0091
[09/26 05:56:18 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 2.2874
[09/26 05:56:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 76.50	
[09/26 05:56:18 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 05:56:25 visual_prompt]: Epoch 38 / 100: avg data time: 6.04e-02, avg batch time: 0.5087, average train loss: 0.0084
[09/26 05:56:26 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 2.2997
[09/26 05:56:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.50	
[09/26 05:56:26 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 05:56:33 visual_prompt]: Epoch 39 / 100: avg data time: 6.03e-02, avg batch time: 0.5090, average train loss: 0.0083
[09/26 05:56:35 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 2.3186
[09/26 05:56:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.00	
[09/26 05:56:35 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 05:56:42 visual_prompt]: Epoch 40 / 100: avg data time: 4.72e-02, avg batch time: 0.4973, average train loss: 0.0079
[09/26 05:56:43 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1694, average loss: 2.3311
[09/26 05:56:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 77.50	
[09/26 05:56:43 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 05:56:50 visual_prompt]: Epoch 41 / 100: avg data time: 5.00e-02, avg batch time: 0.4988, average train loss: 0.0081
[09/26 05:56:52 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1695, average loss: 2.3307
[09/26 05:56:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 76.50	
[09/26 05:56:52 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 05:56:58 visual_prompt]: Epoch 42 / 100: avg data time: 4.79e-02, avg batch time: 0.4969, average train loss: 0.0075
[09/26 05:57:00 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1691, average loss: 2.3372
[09/26 05:57:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.00	
[09/26 05:57:00 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 05:57:07 visual_prompt]: Epoch 43 / 100: avg data time: 5.04e-02, avg batch time: 0.4999, average train loss: 0.0079
[09/26 05:57:08 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1693, average loss: 2.3345
[09/26 05:57:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.50	
[09/26 05:57:08 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 05:57:15 visual_prompt]: Epoch 44 / 100: avg data time: 5.58e-02, avg batch time: 0.5035, average train loss: 0.0071
[09/26 05:57:17 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 2.3414
[09/26 05:57:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 77.00	
[09/26 05:57:17 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 05:57:23 visual_prompt]: Epoch 45 / 100: avg data time: 5.53e-02, avg batch time: 0.5063, average train loss: 0.0072
[09/26 05:57:25 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.1693, average loss: 2.3501
[09/26 05:57:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 77.00	
[09/26 05:57:25 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 05:57:32 visual_prompt]: Epoch 46 / 100: avg data time: 5.94e-02, avg batch time: 0.5073, average train loss: 0.0074
[09/26 05:57:34 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1691, average loss: 2.3504
[09/26 05:57:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.50	
[09/26 05:57:34 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 05:57:40 visual_prompt]: Epoch 47 / 100: avg data time: 5.57e-02, avg batch time: 0.5049, average train loss: 0.0066
[09/26 05:57:42 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 2.3471
[09/26 05:57:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.00	
[09/26 05:57:42 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 05:57:49 visual_prompt]: Epoch 48 / 100: avg data time: 6.25e-02, avg batch time: 0.5112, average train loss: 0.0065
[09/26 05:57:50 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 2.3579
[09/26 05:57:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.00	
[09/26 05:57:50 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 05:57:57 visual_prompt]: Epoch 49 / 100: avg data time: 5.31e-02, avg batch time: 0.5032, average train loss: 0.0063
[09/26 05:57:59 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1696, average loss: 2.3598
[09/26 05:57:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 76.50	
[09/26 05:57:59 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 05:58:06 visual_prompt]: Epoch 50 / 100: avg data time: 5.50e-02, avg batch time: 0.5030, average train loss: 0.0064
[09/26 05:58:07 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 2.3640
[09/26 05:58:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.00	
[09/26 05:58:07 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 05:58:14 visual_prompt]: Epoch 51 / 100: avg data time: 4.98e-02, avg batch time: 0.4999, average train loss: 0.0061
[09/26 05:58:15 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1696, average loss: 2.3696
[09/26 05:58:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.00	
[09/26 05:58:15 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 05:58:22 visual_prompt]: Epoch 52 / 100: avg data time: 5.97e-02, avg batch time: 0.5082, average train loss: 0.0061
[09/26 05:58:24 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 2.3690
[09/26 05:58:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.00	
[09/26 05:58:24 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 05:58:31 visual_prompt]: Epoch 53 / 100: avg data time: 5.54e-02, avg batch time: 0.5046, average train loss: 0.0064
[09/26 05:58:32 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1695, average loss: 2.3690
[09/26 05:58:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.50	
[09/26 05:58:32 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 05:58:39 visual_prompt]: Epoch 54 / 100: avg data time: 6.19e-02, avg batch time: 0.5106, average train loss: 0.0058
[09/26 05:58:41 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1697, average loss: 2.3740
[09/26 05:58:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 05:58:41 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 05:58:47 visual_prompt]: Epoch 55 / 100: avg data time: 4.62e-02, avg batch time: 0.4991, average train loss: 0.0064
[09/26 05:58:49 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1693, average loss: 2.3759
[09/26 05:58:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.00	
[09/26 05:58:49 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 05:58:56 visual_prompt]: Epoch 56 / 100: avg data time: 4.48e-02, avg batch time: 0.4956, average train loss: 0.0057
[09/26 05:58:57 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1693, average loss: 2.3781
[09/26 05:58:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 05:58:57 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 05:59:04 visual_prompt]: Epoch 57 / 100: avg data time: 5.26e-02, avg batch time: 0.5031, average train loss: 0.0057
[09/26 05:59:06 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1695, average loss: 2.3794
[09/26 05:59:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.50	
[09/26 05:59:06 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 05:59:13 visual_prompt]: Epoch 58 / 100: avg data time: 6.34e-02, avg batch time: 0.5124, average train loss: 0.0055
[09/26 05:59:14 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 2.3819
[09/26 05:59:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.50	
[09/26 05:59:14 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 05:59:21 visual_prompt]: Epoch 59 / 100: avg data time: 5.57e-02, avg batch time: 0.5055, average train loss: 0.0056
[09/26 05:59:22 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1694, average loss: 2.3857
[09/26 05:59:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 05:59:22 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 05:59:29 visual_prompt]: Epoch 60 / 100: avg data time: 5.99e-02, avg batch time: 0.5082, average train loss: 0.0056
[09/26 05:59:31 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 2.3878
[09/26 05:59:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.50	
[09/26 05:59:31 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 05:59:38 visual_prompt]: Epoch 61 / 100: avg data time: 5.63e-02, avg batch time: 0.5054, average train loss: 0.0055
[09/26 05:59:39 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1689, average loss: 2.3991
[09/26 05:59:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.50	
[09/26 05:59:39 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 05:59:46 visual_prompt]: Epoch 62 / 100: avg data time: 4.95e-02, avg batch time: 0.5001, average train loss: 0.0056
[09/26 05:59:48 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1693, average loss: 2.4023
[09/26 05:59:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.50	
[09/26 05:59:48 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 05:59:55 visual_prompt]: Epoch 63 / 100: avg data time: 6.15e-02, avg batch time: 0.5097, average train loss: 0.0056
[09/26 05:59:56 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1689, average loss: 2.4085
[09/26 05:59:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.50	
[09/26 05:59:56 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 06:00:03 visual_prompt]: Epoch 64 / 100: avg data time: 6.10e-02, avg batch time: 0.5100, average train loss: 0.0051
[09/26 06:00:05 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1694, average loss: 2.4066
[09/26 06:00:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.00	
[09/26 06:00:05 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 06:00:11 visual_prompt]: Epoch 65 / 100: avg data time: 4.66e-02, avg batch time: 0.4965, average train loss: 0.0053
[09/26 06:00:13 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1690, average loss: 2.4032
[09/26 06:00:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.00	
[09/26 06:00:13 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 06:00:20 visual_prompt]: Epoch 66 / 100: avg data time: 5.10e-02, avg batch time: 0.5001, average train loss: 0.0050
[09/26 06:00:21 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1695, average loss: 2.3990
[09/26 06:00:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.00	
[09/26 06:00:21 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 06:00:28 visual_prompt]: Epoch 67 / 100: avg data time: 5.98e-02, avg batch time: 0.5079, average train loss: 0.0050
[09/26 06:00:30 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.1693, average loss: 2.3986
[09/26 06:00:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.00	
[09/26 06:00:30 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 06:00:36 visual_prompt]: Epoch 68 / 100: avg data time: 4.60e-02, avg batch time: 0.4966, average train loss: 0.0053
[09/26 06:00:38 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1694, average loss: 2.4001
[09/26 06:00:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.00	
[09/26 06:00:38 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 06:00:45 visual_prompt]: Epoch 69 / 100: avg data time: 4.93e-02, avg batch time: 0.4988, average train loss: 0.0052
[09/26 06:00:46 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 2.4007
[09/26 06:00:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.50	
[09/26 06:00:46 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 06:00:53 visual_prompt]: Epoch 70 / 100: avg data time: 5.54e-02, avg batch time: 0.5039, average train loss: 0.0053
[09/26 06:00:55 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1692, average loss: 2.3977
[09/26 06:00:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.50	
[09/26 06:00:55 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 06:01:01 visual_prompt]: Epoch 71 / 100: avg data time: 5.52e-02, avg batch time: 0.5043, average train loss: 0.0052
[09/26 06:01:03 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1692, average loss: 2.3965
[09/26 06:01:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.50	
[09/26 06:01:03 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 06:01:10 visual_prompt]: Epoch 72 / 100: avg data time: 4.53e-02, avg batch time: 0.4996, average train loss: 0.0052
[09/26 06:01:11 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1694, average loss: 2.3947
[09/26 06:01:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.50	
[09/26 06:01:11 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 06:01:18 visual_prompt]: Epoch 73 / 100: avg data time: 4.50e-02, avg batch time: 0.4958, average train loss: 0.0049
[09/26 06:01:20 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 2.3946
[09/26 06:01:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.50	
[09/26 06:01:20 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 06:01:27 visual_prompt]: Epoch 74 / 100: avg data time: 6.45e-02, avg batch time: 0.5131, average train loss: 0.0048
[09/26 06:01:28 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1694, average loss: 2.3955
[09/26 06:01:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.50	
[09/26 06:01:28 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 06:01:35 visual_prompt]: Epoch 75 / 100: avg data time: 5.20e-02, avg batch time: 0.5020, average train loss: 0.0050
[09/26 06:01:36 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1689, average loss: 2.3938
[09/26 06:01:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.50	
[09/26 06:01:36 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 06:01:43 visual_prompt]: Epoch 76 / 100: avg data time: 6.00e-02, avg batch time: 0.5090, average train loss: 0.0053
[09/26 06:01:45 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1693, average loss: 2.3946
[09/26 06:01:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.50	
[09/26 06:01:45 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 06:01:52 visual_prompt]: Epoch 77 / 100: avg data time: 4.62e-02, avg batch time: 0.4948, average train loss: 0.0050
[09/26 06:01:53 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1696, average loss: 2.3951
[09/26 06:01:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.50	
[09/26 06:01:53 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 06:02:00 visual_prompt]: Epoch 78 / 100: avg data time: 5.06e-02, avg batch time: 0.5005, average train loss: 0.0050
[09/26 06:02:02 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1690, average loss: 2.3967
[09/26 06:02:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 77.50	
[09/26 06:02:02 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 06:02:09 visual_prompt]: Epoch 79 / 100: avg data time: 5.76e-02, avg batch time: 0.5056, average train loss: 0.0049
[09/26 06:02:10 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1695, average loss: 2.3991
[09/26 06:02:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:02:10 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 06:02:17 visual_prompt]: Epoch 80 / 100: avg data time: 6.25e-02, avg batch time: 0.5103, average train loss: 0.0048
[09/26 06:02:19 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1697, average loss: 2.4007
[09/26 06:02:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:02:19 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 06:02:25 visual_prompt]: Epoch 81 / 100: avg data time: 5.04e-02, avg batch time: 0.4998, average train loss: 0.0048
[09/26 06:02:27 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 2.4016
[09/26 06:02:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:02:27 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 06:02:34 visual_prompt]: Epoch 82 / 100: avg data time: 4.45e-02, avg batch time: 0.4939, average train loss: 0.0048
[09/26 06:02:35 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1698, average loss: 2.4025
[09/26 06:02:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:02:35 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 06:02:42 visual_prompt]: Epoch 83 / 100: avg data time: 4.86e-02, avg batch time: 0.4980, average train loss: 0.0045
[09/26 06:02:44 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1694, average loss: 2.4026
[09/26 06:02:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:02:44 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 06:02:50 visual_prompt]: Epoch 84 / 100: avg data time: 6.33e-02, avg batch time: 0.5113, average train loss: 0.0052
[09/26 06:02:52 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1694, average loss: 2.4041
[09/26 06:02:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:02:52 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 06:02:59 visual_prompt]: Epoch 85 / 100: avg data time: 5.43e-02, avg batch time: 0.5021, average train loss: 0.0046
[09/26 06:03:01 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1692, average loss: 2.4043
[09/26 06:03:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:03:01 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 06:03:08 visual_prompt]: Epoch 86 / 100: avg data time: 6.66e-02, avg batch time: 0.5156, average train loss: 0.0050
[09/26 06:03:09 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 2.4052
[09/26 06:03:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:03:09 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 06:03:16 visual_prompt]: Epoch 87 / 100: avg data time: 5.27e-02, avg batch time: 0.5011, average train loss: 0.0050
[09/26 06:03:17 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1695, average loss: 2.4056
[09/26 06:03:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:03:17 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 06:03:24 visual_prompt]: Epoch 88 / 100: avg data time: 5.95e-02, avg batch time: 0.5079, average train loss: 0.0048
[09/26 06:03:26 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1690, average loss: 2.4056
[09/26 06:03:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:03:26 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 06:03:33 visual_prompt]: Epoch 89 / 100: avg data time: 5.71e-02, avg batch time: 0.5050, average train loss: 0.0047
[09/26 06:03:34 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1692, average loss: 2.4054
[09/26 06:03:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:03:34 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 06:03:41 visual_prompt]: Epoch 90 / 100: avg data time: 5.49e-02, avg batch time: 0.5025, average train loss: 0.0047
[09/26 06:03:43 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 2.4057
[09/26 06:03:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:03:43 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 06:03:50 visual_prompt]: Epoch 91 / 100: avg data time: 5.66e-02, avg batch time: 0.5061, average train loss: 0.0048
[09/26 06:03:51 visual_prompt]: Inference (val):avg data time: 4.57e-05, avg batch time: 0.1692, average loss: 2.4056
[09/26 06:03:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:03:51 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 06:03:58 visual_prompt]: Epoch 92 / 100: avg data time: 5.88e-02, avg batch time: 0.5070, average train loss: 0.0048
[09/26 06:04:00 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1693, average loss: 2.4058
[09/26 06:04:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:04:00 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 06:04:07 visual_prompt]: Epoch 93 / 100: avg data time: 6.04e-02, avg batch time: 0.5099, average train loss: 0.0049
[09/26 06:04:08 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1697, average loss: 2.4056
[09/26 06:04:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:04:08 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 06:04:15 visual_prompt]: Epoch 94 / 100: avg data time: 4.68e-02, avg batch time: 0.4960, average train loss: 0.0048
[09/26 06:04:16 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1693, average loss: 2.4056
[09/26 06:04:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:04:16 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 06:04:23 visual_prompt]: Epoch 95 / 100: avg data time: 5.91e-02, avg batch time: 0.5084, average train loss: 0.0048
[09/26 06:04:25 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1693, average loss: 2.4058
[09/26 06:04:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:04:25 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 06:04:32 visual_prompt]: Epoch 96 / 100: avg data time: 5.93e-02, avg batch time: 0.5077, average train loss: 0.0048
[09/26 06:04:33 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 2.4058
[09/26 06:04:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:04:33 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 06:04:40 visual_prompt]: Epoch 97 / 100: avg data time: 4.86e-02, avg batch time: 0.4984, average train loss: 0.0048
[09/26 06:04:42 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1693, average loss: 2.4058
[09/26 06:04:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:04:42 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 06:04:48 visual_prompt]: Epoch 98 / 100: avg data time: 4.39e-02, avg batch time: 0.4946, average train loss: 0.0048
[09/26 06:04:50 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1693, average loss: 2.4058
[09/26 06:04:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:04:50 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 06:04:57 visual_prompt]: Epoch 99 / 100: avg data time: 5.26e-02, avg batch time: 0.5011, average train loss: 0.0046
[09/26 06:04:58 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1695, average loss: 2.4058
[09/26 06:04:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:04:58 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 06:05:05 visual_prompt]: Epoch 100 / 100: avg data time: 5.38e-02, avg batch time: 0.5032, average train loss: 0.0046
[09/26 06:05:07 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 2.4058
[09/26 06:05:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.50	
[09/26 06:05:07 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 06:05:07 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 06:05:07 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 06:05:07 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 06:05:07 visual_prompt]: Training with config:
[09/26 06:05:07 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 06:05:07 visual_prompt]: Loading training data...
[09/26 06:05:07 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 06:05:09 visual_prompt]: Number of images: 800
[09/26 06:05:09 visual_prompt]: Number of classes: 47 / 47
[09/26 06:05:09 visual_prompt]: Loading validation data...
[09/26 06:05:09 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 06:05:09 visual_prompt]: Number of images: 200
[09/26 06:05:09 visual_prompt]: Number of classes: 47 / 47
[09/26 06:05:09 visual_prompt]: Constructing models...
[09/26 06:05:12 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 06:05:12 visual_prompt]: tuned percent:0.576
[09/26 06:05:12 visual_prompt]: Device used for model: 0
[09/26 06:05:12 visual_prompt]: Setting up Evaluator...
[09/26 06:05:12 visual_prompt]: Setting up Trainer...
[09/26 06:05:12 visual_prompt]: 	Setting up the optimizer...
[09/26 06:05:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 06:05:19 visual_prompt]: Epoch 1 / 100: avg data time: 5.54e-02, avg batch time: 0.5058, average train loss: 3.9286
[09/26 06:05:20 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 3.9045
[09/26 06:05:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 06:05:20 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 06:05:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 06:05:27 visual_prompt]: Epoch 2 / 100: avg data time: 5.00e-02, avg batch time: 0.4980, average train loss: 3.8856
[09/26 06:05:29 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1690, average loss: 3.8857
[09/26 06:05:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 1.50	top5: 11.00	
[09/26 06:05:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 06:05:35 visual_prompt]: Epoch 3 / 100: avg data time: 4.71e-02, avg batch time: 0.4943, average train loss: 3.8515
[09/26 06:05:37 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1690, average loss: 3.8927
[09/26 06:05:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 10.00	
[09/26 06:05:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 06:05:44 visual_prompt]: Epoch 4 / 100: avg data time: 4.71e-02, avg batch time: 0.4958, average train loss: 3.7820
[09/26 06:05:45 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1691, average loss: 3.8177
[09/26 06:05:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 17.00	
[09/26 06:05:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 06:05:52 visual_prompt]: Epoch 5 / 100: avg data time: 6.34e-02, avg batch time: 0.5107, average train loss: 3.5956
[09/26 06:05:54 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1690, average loss: 3.6009
[09/26 06:05:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 27.00	
[09/26 06:05:54 visual_prompt]: Best epoch 5: best metric: 0.095
[09/26 06:05:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 06:06:01 visual_prompt]: Epoch 6 / 100: avg data time: 5.74e-02, avg batch time: 0.5049, average train loss: 3.1999
[09/26 06:06:02 visual_prompt]: Inference (val):avg data time: 4.41e-05, avg batch time: 0.1691, average loss: 3.3414
[09/26 06:06:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.00	top5: 40.00	
[09/26 06:06:02 visual_prompt]: Best epoch 6: best metric: 0.130
[09/26 06:06:02 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 06:06:09 visual_prompt]: Epoch 7 / 100: avg data time: 6.32e-02, avg batch time: 0.5105, average train loss: 2.8466
[09/26 06:06:11 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1690, average loss: 3.3458
[09/26 06:06:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 15.50	top5: 42.00	
[09/26 06:06:11 visual_prompt]: Best epoch 7: best metric: 0.155
[09/26 06:06:11 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 06:06:17 visual_prompt]: Epoch 8 / 100: avg data time: 5.56e-02, avg batch time: 0.5031, average train loss: 2.4843
[09/26 06:06:19 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1690, average loss: 3.1439
[09/26 06:06:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.00	top5: 53.50	
[09/26 06:06:19 visual_prompt]: Best epoch 8: best metric: 0.220
[09/26 06:06:19 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 06:06:26 visual_prompt]: Epoch 9 / 100: avg data time: 6.10e-02, avg batch time: 0.5092, average train loss: 2.0151
[09/26 06:06:28 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1691, average loss: 2.7568
[09/26 06:06:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.00	top5: 60.00	
[09/26 06:06:28 visual_prompt]: Best epoch 9: best metric: 0.260
[09/26 06:06:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 06:06:34 visual_prompt]: Epoch 10 / 100: avg data time: 5.49e-02, avg batch time: 0.5038, average train loss: 1.4510
[09/26 06:06:36 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1692, average loss: 2.4395
[09/26 06:06:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 36.50	top5: 70.50	
[09/26 06:06:36 visual_prompt]: Best epoch 10: best metric: 0.365
[09/26 06:06:36 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 06:06:43 visual_prompt]: Epoch 11 / 100: avg data time: 4.90e-02, avg batch time: 0.4980, average train loss: 1.1414
[09/26 06:06:44 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1692, average loss: 2.2751
[09/26 06:06:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.50	top5: 72.00	
[09/26 06:06:44 visual_prompt]: Best epoch 11: best metric: 0.385
[09/26 06:06:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 06:06:51 visual_prompt]: Epoch 12 / 100: avg data time: 5.00e-02, avg batch time: 0.4987, average train loss: 0.7838
[09/26 06:06:53 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1694, average loss: 2.1675
[09/26 06:06:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 74.50	
[09/26 06:06:53 visual_prompt]: Best epoch 12: best metric: 0.455
[09/26 06:06:53 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 06:06:59 visual_prompt]: Epoch 13 / 100: avg data time: 4.19e-02, avg batch time: 0.4923, average train loss: 0.4972
[09/26 06:07:01 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1692, average loss: 2.1653
[09/26 06:07:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 76.00	
[09/26 06:07:01 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 06:07:08 visual_prompt]: Epoch 14 / 100: avg data time: 4.96e-02, avg batch time: 0.4983, average train loss: 0.3513
[09/26 06:07:09 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1691, average loss: 2.1547
[09/26 06:07:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 74.00	
[09/26 06:07:09 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 06:07:16 visual_prompt]: Epoch 15 / 100: avg data time: 5.15e-02, avg batch time: 0.4991, average train loss: 0.2326
[09/26 06:07:17 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1691, average loss: 2.2534
[09/26 06:07:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 75.50	
[09/26 06:07:17 visual_prompt]: Best epoch 15: best metric: 0.465
[09/26 06:07:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 06:07:24 visual_prompt]: Epoch 16 / 100: avg data time: 5.35e-02, avg batch time: 0.5020, average train loss: 0.1653
[09/26 06:07:26 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1687, average loss: 2.1697
[09/26 06:07:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 78.50	
[09/26 06:07:26 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 06:07:33 visual_prompt]: Epoch 17 / 100: avg data time: 5.98e-02, avg batch time: 0.5078, average train loss: 0.1056
[09/26 06:07:34 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1693, average loss: 2.2108
[09/26 06:07:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 78.50	
[09/26 06:07:34 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 06:07:41 visual_prompt]: Epoch 18 / 100: avg data time: 5.55e-02, avg batch time: 0.5048, average train loss: 0.0620
[09/26 06:07:43 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 2.2054
[09/26 06:07:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 78.00	
[09/26 06:07:43 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 06:07:49 visual_prompt]: Epoch 19 / 100: avg data time: 4.85e-02, avg batch time: 0.4974, average train loss: 0.0508
[09/26 06:07:51 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1691, average loss: 2.2374
[09/26 06:07:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 77.50	
[09/26 06:07:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 06:07:58 visual_prompt]: Epoch 20 / 100: avg data time: 5.38e-02, avg batch time: 0.5025, average train loss: 0.0395
[09/26 06:07:59 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1690, average loss: 2.2921
[09/26 06:07:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 79.50	
[09/26 06:07:59 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 06:08:06 visual_prompt]: Epoch 21 / 100: avg data time: 6.23e-02, avg batch time: 0.5101, average train loss: 0.0284
[09/26 06:08:08 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1691, average loss: 2.2506
[09/26 06:08:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 79.50	
[09/26 06:08:08 visual_prompt]: Best epoch 21: best metric: 0.480
[09/26 06:08:08 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 06:08:15 visual_prompt]: Epoch 22 / 100: avg data time: 5.87e-02, avg batch time: 0.5082, average train loss: 0.0251
[09/26 06:08:16 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1692, average loss: 2.2892
[09/26 06:08:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 79.00	
[09/26 06:08:16 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 06:08:23 visual_prompt]: Epoch 23 / 100: avg data time: 5.66e-02, avg batch time: 0.5056, average train loss: 0.0208
[09/26 06:08:24 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1698, average loss: 2.2431
[09/26 06:08:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 78.00	
[09/26 06:08:24 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 06:08:31 visual_prompt]: Epoch 24 / 100: avg data time: 4.67e-02, avg batch time: 0.4948, average train loss: 0.0172
[09/26 06:08:33 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1691, average loss: 2.2618
[09/26 06:08:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 78.00	
[09/26 06:08:33 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 06:08:40 visual_prompt]: Epoch 25 / 100: avg data time: 6.28e-02, avg batch time: 0.5107, average train loss: 0.0151
[09/26 06:08:41 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 2.3005
[09/26 06:08:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 77.00	
[09/26 06:08:41 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 06:08:48 visual_prompt]: Epoch 26 / 100: avg data time: 5.45e-02, avg batch time: 0.5035, average train loss: 0.0141
[09/26 06:08:50 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 2.3009
[09/26 06:08:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 77.50	
[09/26 06:08:50 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 06:08:56 visual_prompt]: Epoch 27 / 100: avg data time: 4.75e-02, avg batch time: 0.4961, average train loss: 0.0140
[09/26 06:08:58 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1693, average loss: 2.2849
[09/26 06:08:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 77.50	
[09/26 06:08:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 06:09:05 visual_prompt]: Epoch 28 / 100: avg data time: 5.62e-02, avg batch time: 0.5045, average train loss: 0.0128
[09/26 06:09:06 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1695, average loss: 2.2766
[09/26 06:09:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.50	
[09/26 06:09:06 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 06:09:13 visual_prompt]: Epoch 29 / 100: avg data time: 4.84e-02, avg batch time: 0.4987, average train loss: 0.0113
[09/26 06:09:14 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1696, average loss: 2.3012
[09/26 06:09:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 77.50	
[09/26 06:09:14 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 06:09:21 visual_prompt]: Epoch 30 / 100: avg data time: 4.94e-02, avg batch time: 0.4976, average train loss: 0.0109
[09/26 06:09:23 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1688, average loss: 2.3084
[09/26 06:09:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 77.50	
[09/26 06:09:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 06:09:30 visual_prompt]: Epoch 31 / 100: avg data time: 5.19e-02, avg batch time: 0.5006, average train loss: 0.0101
[09/26 06:09:31 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1696, average loss: 2.3077
[09/26 06:09:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 77.50	
[09/26 06:09:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 06:09:38 visual_prompt]: Epoch 32 / 100: avg data time: 6.07e-02, avg batch time: 0.5087, average train loss: 0.0100
[09/26 06:09:40 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1691, average loss: 2.3251
[09/26 06:09:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.00	
[09/26 06:09:40 visual_prompt]: Best epoch 32: best metric: 0.490
[09/26 06:09:40 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 06:09:46 visual_prompt]: Epoch 33 / 100: avg data time: 5.20e-02, avg batch time: 0.5009, average train loss: 0.0087
[09/26 06:09:48 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1691, average loss: 2.3223
[09/26 06:09:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 76.00	
[09/26 06:09:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 06:09:55 visual_prompt]: Epoch 34 / 100: avg data time: 6.33e-02, avg batch time: 0.5112, average train loss: 0.0092
[09/26 06:09:56 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1695, average loss: 2.3240
[09/26 06:09:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.00	
[09/26 06:09:56 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 06:10:03 visual_prompt]: Epoch 35 / 100: avg data time: 5.59e-02, avg batch time: 0.5045, average train loss: 0.0086
[09/26 06:10:05 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1692, average loss: 2.3372
[09/26 06:10:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 77.50	
[09/26 06:10:05 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 06:10:12 visual_prompt]: Epoch 36 / 100: avg data time: 4.47e-02, avg batch time: 0.4960, average train loss: 0.0083
[09/26 06:10:13 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1692, average loss: 2.3447
[09/26 06:10:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.50	
[09/26 06:10:13 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 06:10:20 visual_prompt]: Epoch 37 / 100: avg data time: 5.53e-02, avg batch time: 0.5032, average train loss: 0.0081
[09/26 06:10:21 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 2.3457
[09/26 06:10:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.00	
[09/26 06:10:22 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 06:10:28 visual_prompt]: Epoch 38 / 100: avg data time: 5.39e-02, avg batch time: 0.5025, average train loss: 0.0078
[09/26 06:10:30 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1692, average loss: 2.3504
[09/26 06:10:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 06:10:30 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 06:10:37 visual_prompt]: Epoch 39 / 100: avg data time: 5.10e-02, avg batch time: 0.4996, average train loss: 0.0070
[09/26 06:10:38 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 2.3502
[09/26 06:10:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 78.50	
[09/26 06:10:38 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 06:10:45 visual_prompt]: Epoch 40 / 100: avg data time: 6.23e-02, avg batch time: 0.5115, average train loss: 0.0069
[09/26 06:10:47 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1691, average loss: 2.3511
[09/26 06:10:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 78.50	
[09/26 06:10:47 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 06:10:54 visual_prompt]: Epoch 41 / 100: avg data time: 5.76e-02, avg batch time: 0.5061, average train loss: 0.0069
[09/26 06:10:55 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1694, average loss: 2.3564
[09/26 06:10:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 79.00	
[09/26 06:10:55 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 06:11:02 visual_prompt]: Epoch 42 / 100: avg data time: 5.86e-02, avg batch time: 0.5065, average train loss: 0.0067
[09/26 06:11:04 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1697, average loss: 2.3660
[09/26 06:11:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.50	
[09/26 06:11:04 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 06:11:10 visual_prompt]: Epoch 43 / 100: avg data time: 4.47e-02, avg batch time: 0.4941, average train loss: 0.0069
[09/26 06:11:12 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 2.3820
[09/26 06:11:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.50	
[09/26 06:11:12 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 06:11:18 visual_prompt]: Epoch 44 / 100: avg data time: 4.59e-02, avg batch time: 0.4944, average train loss: 0.0064
[09/26 06:11:20 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1697, average loss: 2.3875
[09/26 06:11:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:11:20 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 06:11:27 visual_prompt]: Epoch 45 / 100: avg data time: 4.75e-02, avg batch time: 0.4970, average train loss: 0.0060
[09/26 06:11:28 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 2.3892
[09/26 06:11:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.50	
[09/26 06:11:28 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 06:11:35 visual_prompt]: Epoch 46 / 100: avg data time: 4.72e-02, avg batch time: 0.4977, average train loss: 0.0061
[09/26 06:11:37 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1696, average loss: 2.3903
[09/26 06:11:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 06:11:37 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 06:11:44 visual_prompt]: Epoch 47 / 100: avg data time: 5.26e-02, avg batch time: 0.5014, average train loss: 0.0057
[09/26 06:11:45 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1695, average loss: 2.3898
[09/26 06:11:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 06:11:45 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 06:11:52 visual_prompt]: Epoch 48 / 100: avg data time: 5.20e-02, avg batch time: 0.5007, average train loss: 0.0060
[09/26 06:11:53 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1696, average loss: 2.3953
[09/26 06:11:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 77.50	
[09/26 06:11:53 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 06:12:00 visual_prompt]: Epoch 49 / 100: avg data time: 6.47e-02, avg batch time: 0.5130, average train loss: 0.0059
[09/26 06:12:02 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1694, average loss: 2.4030
[09/26 06:12:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.50	
[09/26 06:12:02 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 06:12:09 visual_prompt]: Epoch 50 / 100: avg data time: 5.85e-02, avg batch time: 0.5076, average train loss: 0.0054
[09/26 06:12:10 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1696, average loss: 2.3992
[09/26 06:12:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 79.50	
[09/26 06:12:10 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 06:12:17 visual_prompt]: Epoch 51 / 100: avg data time: 5.48e-02, avg batch time: 0.5028, average train loss: 0.0056
[09/26 06:12:19 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1696, average loss: 2.3943
[09/26 06:12:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 79.50	
[09/26 06:12:19 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 06:12:26 visual_prompt]: Epoch 52 / 100: avg data time: 6.12e-02, avg batch time: 0.5098, average train loss: 0.0055
[09/26 06:12:27 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1695, average loss: 2.3935
[09/26 06:12:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 79.50	
[09/26 06:12:27 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 06:12:34 visual_prompt]: Epoch 53 / 100: avg data time: 6.19e-02, avg batch time: 0.5109, average train loss: 0.0051
[09/26 06:12:36 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1691, average loss: 2.3974
[09/26 06:12:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 79.00	
[09/26 06:12:36 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 06:12:43 visual_prompt]: Epoch 54 / 100: avg data time: 4.54e-02, avg batch time: 0.4955, average train loss: 0.0049
[09/26 06:12:44 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1694, average loss: 2.4018
[09/26 06:12:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 78.50	
[09/26 06:12:44 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 06:12:51 visual_prompt]: Epoch 55 / 100: avg data time: 4.52e-02, avg batch time: 0.4948, average train loss: 0.0050
[09/26 06:12:52 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1691, average loss: 2.4074
[09/26 06:12:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 77.50	
[09/26 06:12:52 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 06:12:59 visual_prompt]: Epoch 56 / 100: avg data time: 5.05e-02, avg batch time: 0.5014, average train loss: 0.0050
[09/26 06:13:01 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1694, average loss: 2.4062
[09/26 06:13:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 78.00	
[09/26 06:13:01 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 06:13:07 visual_prompt]: Epoch 57 / 100: avg data time: 5.79e-02, avg batch time: 0.5087, average train loss: 0.0049
[09/26 06:13:09 visual_prompt]: Inference (val):avg data time: 4.91e-05, avg batch time: 0.1701, average loss: 2.4065
[09/26 06:13:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 06:13:09 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 06:13:16 visual_prompt]: Epoch 58 / 100: avg data time: 4.37e-02, avg batch time: 0.4925, average train loss: 0.0049
[09/26 06:13:17 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1696, average loss: 2.3898
[09/26 06:13:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 78.50	
[09/26 06:13:17 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 06:13:24 visual_prompt]: Epoch 59 / 100: avg data time: 4.52e-02, avg batch time: 0.4961, average train loss: 0.0048
[09/26 06:13:26 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1693, average loss: 2.3912
[09/26 06:13:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 78.00	
[09/26 06:13:26 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 06:13:32 visual_prompt]: Epoch 60 / 100: avg data time: 4.41e-02, avg batch time: 0.4935, average train loss: 0.0047
[09/26 06:13:34 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 2.3936
[09/26 06:13:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.50	
[09/26 06:13:34 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 06:13:41 visual_prompt]: Epoch 61 / 100: avg data time: 5.27e-02, avg batch time: 0.5011, average train loss: 0.0050
[09/26 06:13:42 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 2.3980
[09/26 06:13:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 77.00	
[09/26 06:13:42 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 06:13:49 visual_prompt]: Epoch 62 / 100: avg data time: 5.11e-02, avg batch time: 0.5014, average train loss: 0.0045
[09/26 06:13:50 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1695, average loss: 2.4032
[09/26 06:13:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 77.00	
[09/26 06:13:50 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 06:13:57 visual_prompt]: Epoch 63 / 100: avg data time: 5.87e-02, avg batch time: 0.5080, average train loss: 0.0051
[09/26 06:13:59 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1693, average loss: 2.4109
[09/26 06:13:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.00	
[09/26 06:13:59 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 06:14:06 visual_prompt]: Epoch 64 / 100: avg data time: 4.70e-02, avg batch time: 0.4977, average train loss: 0.0046
[09/26 06:14:07 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1690, average loss: 2.4131
[09/26 06:14:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.00	
[09/26 06:14:07 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 06:14:14 visual_prompt]: Epoch 65 / 100: avg data time: 5.60e-02, avg batch time: 0.5043, average train loss: 0.0045
[09/26 06:14:15 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1695, average loss: 2.4120
[09/26 06:14:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.00	
[09/26 06:14:15 visual_prompt]: Best epoch 65: best metric: 0.495
[09/26 06:14:15 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 06:14:22 visual_prompt]: Epoch 66 / 100: avg data time: 4.43e-02, avg batch time: 0.4943, average train loss: 0.0044
[09/26 06:14:24 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1694, average loss: 2.4121
[09/26 06:14:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 06:14:24 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 06:14:31 visual_prompt]: Epoch 67 / 100: avg data time: 4.67e-02, avg batch time: 0.4978, average train loss: 0.0045
[09/26 06:14:32 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1696, average loss: 2.4106
[09/26 06:14:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 77.50	
[09/26 06:14:32 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 06:14:39 visual_prompt]: Epoch 68 / 100: avg data time: 4.37e-02, avg batch time: 0.4926, average train loss: 0.0042
[09/26 06:14:40 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1692, average loss: 2.4077
[09/26 06:14:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 06:14:40 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 06:14:47 visual_prompt]: Epoch 69 / 100: avg data time: 5.14e-02, avg batch time: 0.5015, average train loss: 0.0043
[09/26 06:14:49 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 2.4062
[09/26 06:14:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:14:49 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 06:14:55 visual_prompt]: Epoch 70 / 100: avg data time: 5.36e-02, avg batch time: 0.5022, average train loss: 0.0042
[09/26 06:14:57 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1693, average loss: 2.4040
[09/26 06:14:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.50	
[09/26 06:14:57 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 06:15:04 visual_prompt]: Epoch 71 / 100: avg data time: 4.48e-02, avg batch time: 0.4935, average train loss: 0.0043
[09/26 06:15:05 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 2.4065
[09/26 06:15:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 06:15:05 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 06:15:12 visual_prompt]: Epoch 72 / 100: avg data time: 4.99e-02, avg batch time: 0.4995, average train loss: 0.0043
[09/26 06:15:14 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1694, average loss: 2.4072
[09/26 06:15:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 06:15:14 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 06:15:21 visual_prompt]: Epoch 73 / 100: avg data time: 5.53e-02, avg batch time: 0.5049, average train loss: 0.0042
[09/26 06:15:22 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1690, average loss: 2.4101
[09/26 06:15:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 77.50	
[09/26 06:15:22 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 06:15:29 visual_prompt]: Epoch 74 / 100: avg data time: 5.95e-02, avg batch time: 0.5085, average train loss: 0.0044
[09/26 06:15:30 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1694, average loss: 2.4122
[09/26 06:15:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 77.50	
[09/26 06:15:30 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 06:15:37 visual_prompt]: Epoch 75 / 100: avg data time: 5.58e-02, avg batch time: 0.5038, average train loss: 0.0040
[09/26 06:15:39 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 2.4107
[09/26 06:15:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 06:15:39 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 06:15:46 visual_prompt]: Epoch 76 / 100: avg data time: 5.88e-02, avg batch time: 0.5081, average train loss: 0.0042
[09/26 06:15:47 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1692, average loss: 2.4080
[09/26 06:15:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.50	
[09/26 06:15:47 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 06:15:54 visual_prompt]: Epoch 77 / 100: avg data time: 5.93e-02, avg batch time: 0.5082, average train loss: 0.0042
[09/26 06:15:56 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1697, average loss: 2.4096
[09/26 06:15:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:15:56 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 06:16:03 visual_prompt]: Epoch 78 / 100: avg data time: 5.45e-02, avg batch time: 0.5031, average train loss: 0.0042
[09/26 06:16:04 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1692, average loss: 2.4109
[09/26 06:16:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.50	
[09/26 06:16:04 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 06:16:11 visual_prompt]: Epoch 79 / 100: avg data time: 4.52e-02, avg batch time: 0.4937, average train loss: 0.0039
[09/26 06:16:12 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1691, average loss: 2.4119
[09/26 06:16:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.50	
[09/26 06:16:12 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 06:16:19 visual_prompt]: Epoch 80 / 100: avg data time: 5.42e-02, avg batch time: 0.5034, average train loss: 0.0042
[09/26 06:16:21 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1697, average loss: 2.4133
[09/26 06:16:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.50	
[09/26 06:16:21 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 06:16:28 visual_prompt]: Epoch 81 / 100: avg data time: 5.50e-02, avg batch time: 0.5038, average train loss: 0.0042
[09/26 06:16:29 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1690, average loss: 2.4131
[09/26 06:16:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.50	
[09/26 06:16:29 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 06:16:36 visual_prompt]: Epoch 82 / 100: avg data time: 6.61e-02, avg batch time: 0.5138, average train loss: 0.0043
[09/26 06:16:38 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1696, average loss: 2.4135
[09/26 06:16:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:16:38 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 06:16:45 visual_prompt]: Epoch 83 / 100: avg data time: 6.19e-02, avg batch time: 0.5109, average train loss: 0.0043
[09/26 06:16:46 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1692, average loss: 2.4145
[09/26 06:16:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:16:46 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 06:16:53 visual_prompt]: Epoch 84 / 100: avg data time: 5.34e-02, avg batch time: 0.5010, average train loss: 0.0041
[09/26 06:16:54 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 2.4154
[09/26 06:16:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:16:54 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 06:17:01 visual_prompt]: Epoch 85 / 100: avg data time: 6.08e-02, avg batch time: 0.5107, average train loss: 0.0039
[09/26 06:17:03 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1696, average loss: 2.4156
[09/26 06:17:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:17:03 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 06:17:10 visual_prompt]: Epoch 86 / 100: avg data time: 5.92e-02, avg batch time: 0.5088, average train loss: 0.0038
[09/26 06:17:11 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1692, average loss: 2.4160
[09/26 06:17:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:17:11 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 06:17:18 visual_prompt]: Epoch 87 / 100: avg data time: 6.03e-02, avg batch time: 0.5079, average train loss: 0.0044
[09/26 06:17:20 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1691, average loss: 2.4162
[09/26 06:17:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:17:20 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 06:17:27 visual_prompt]: Epoch 88 / 100: avg data time: 5.79e-02, avg batch time: 0.5056, average train loss: 0.0039
[09/26 06:17:28 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1695, average loss: 2.4163
[09/26 06:17:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:17:28 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 06:17:35 visual_prompt]: Epoch 89 / 100: avg data time: 5.81e-02, avg batch time: 0.5058, average train loss: 0.0042
[09/26 06:17:37 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1696, average loss: 2.4167
[09/26 06:17:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:17:37 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 06:17:44 visual_prompt]: Epoch 90 / 100: avg data time: 6.35e-02, avg batch time: 0.5109, average train loss: 0.0040
[09/26 06:17:45 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1692, average loss: 2.4167
[09/26 06:17:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:17:45 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 06:17:52 visual_prompt]: Epoch 91 / 100: avg data time: 6.26e-02, avg batch time: 0.5103, average train loss: 0.0042
[09/26 06:17:54 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1691, average loss: 2.4168
[09/26 06:17:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:17:54 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 06:18:00 visual_prompt]: Epoch 92 / 100: avg data time: 4.89e-02, avg batch time: 0.4986, average train loss: 0.0040
[09/26 06:18:02 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.1694, average loss: 2.4168
[09/26 06:18:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:18:02 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 06:18:09 visual_prompt]: Epoch 93 / 100: avg data time: 5.89e-02, avg batch time: 0.5086, average train loss: 0.0041
[09/26 06:18:10 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1691, average loss: 2.4167
[09/26 06:18:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:18:10 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 06:18:17 visual_prompt]: Epoch 94 / 100: avg data time: 5.43e-02, avg batch time: 0.5023, average train loss: 0.0041
[09/26 06:18:19 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1693, average loss: 2.4168
[09/26 06:18:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:18:19 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 06:18:26 visual_prompt]: Epoch 95 / 100: avg data time: 5.93e-02, avg batch time: 0.5080, average train loss: 0.0038
[09/26 06:18:27 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1694, average loss: 2.4168
[09/26 06:18:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:18:27 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 06:18:34 visual_prompt]: Epoch 96 / 100: avg data time: 6.01e-02, avg batch time: 0.5082, average train loss: 0.0041
[09/26 06:18:36 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 2.4167
[09/26 06:18:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:18:36 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 06:18:43 visual_prompt]: Epoch 97 / 100: avg data time: 5.48e-02, avg batch time: 0.5032, average train loss: 0.0039
[09/26 06:18:44 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1695, average loss: 2.4167
[09/26 06:18:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:18:44 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 06:18:51 visual_prompt]: Epoch 98 / 100: avg data time: 4.74e-02, avg batch time: 0.4960, average train loss: 0.0041
[09/26 06:18:53 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1696, average loss: 2.4167
[09/26 06:18:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:18:53 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 06:18:59 visual_prompt]: Epoch 99 / 100: avg data time: 4.93e-02, avg batch time: 0.4997, average train loss: 0.0041
[09/26 06:19:01 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1693, average loss: 2.4167
[09/26 06:19:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:19:01 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 06:19:08 visual_prompt]: Epoch 100 / 100: avg data time: 5.38e-02, avg batch time: 0.5016, average train loss: 0.0041
[09/26 06:19:09 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1694, average loss: 2.4167
[09/26 06:19:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 78.00	
[09/26 06:19:09 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 06:19:09 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 06:19:09 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 06:19:09 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 06:19:09 visual_prompt]: Training with config:
[09/26 06:19:09 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 06:19:09 visual_prompt]: Loading training data...
[09/26 06:19:09 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 06:19:11 visual_prompt]: Number of images: 800
[09/26 06:19:11 visual_prompt]: Number of classes: 47 / 47
[09/26 06:19:11 visual_prompt]: Loading validation data...
[09/26 06:19:11 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 06:19:12 visual_prompt]: Number of images: 200
[09/26 06:19:12 visual_prompt]: Number of classes: 47 / 47
[09/26 06:19:12 visual_prompt]: Constructing models...
[09/26 06:19:14 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 06:19:14 visual_prompt]: tuned percent:0.576
[09/26 06:19:15 visual_prompt]: Device used for model: 0
[09/26 06:19:15 visual_prompt]: Setting up Evaluator...
[09/26 06:19:15 visual_prompt]: Setting up Trainer...
[09/26 06:19:15 visual_prompt]: 	Setting up the optimizer...
[09/26 06:19:15 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 06:19:21 visual_prompt]: Epoch 1 / 100: avg data time: 5.65e-02, avg batch time: 0.5045, average train loss: 3.9361
[09/26 06:19:23 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1687, average loss: 3.9045
[09/26 06:19:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 06:19:23 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 06:19:23 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 06:19:30 visual_prompt]: Epoch 2 / 100: avg data time: 5.56e-02, avg batch time: 0.5028, average train loss: 3.8885
[09/26 06:19:31 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1687, average loss: 3.8632
[09/26 06:19:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 19.00	
[09/26 06:19:31 visual_prompt]: Best epoch 2: best metric: 0.050
[09/26 06:19:31 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 06:19:38 visual_prompt]: Epoch 3 / 100: avg data time: 5.58e-02, avg batch time: 0.5028, average train loss: 3.8226
[09/26 06:19:40 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1688, average loss: 3.8518
[09/26 06:19:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 12.00	
[09/26 06:19:40 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 06:19:47 visual_prompt]: Epoch 4 / 100: avg data time: 6.14e-02, avg batch time: 0.5080, average train loss: 3.7401
[09/26 06:19:48 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1691, average loss: 3.7441
[09/26 06:19:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.00	top5: 21.50	
[09/26 06:19:48 visual_prompt]: Best epoch 4: best metric: 0.070
[09/26 06:19:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 06:19:55 visual_prompt]: Epoch 5 / 100: avg data time: 4.14e-02, avg batch time: 0.4913, average train loss: 3.5725
[09/26 06:19:56 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1690, average loss: 3.6435
[09/26 06:19:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 25.50	
[09/26 06:19:56 visual_prompt]: Best epoch 5: best metric: 0.110
[09/26 06:19:56 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 06:20:03 visual_prompt]: Epoch 6 / 100: avg data time: 4.95e-02, avg batch time: 0.4989, average train loss: 3.3315
[09/26 06:20:05 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1691, average loss: 3.3957
[09/26 06:20:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.50	top5: 36.00	
[09/26 06:20:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 06:20:12 visual_prompt]: Epoch 7 / 100: avg data time: 4.60e-02, avg batch time: 0.4952, average train loss: 3.0241
[09/26 06:20:13 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1695, average loss: 3.0685
[09/26 06:20:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 22.50	top5: 49.00	
[09/26 06:20:13 visual_prompt]: Best epoch 7: best metric: 0.225
[09/26 06:20:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 06:20:20 visual_prompt]: Epoch 8 / 100: avg data time: 5.13e-02, avg batch time: 0.4997, average train loss: 2.5508
[09/26 06:20:21 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1690, average loss: 2.8991
[09/26 06:20:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.50	top5: 51.50	
[09/26 06:20:21 visual_prompt]: Best epoch 8: best metric: 0.245
[09/26 06:20:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 06:20:28 visual_prompt]: Epoch 9 / 100: avg data time: 4.82e-02, avg batch time: 0.4975, average train loss: 2.1714
[09/26 06:20:30 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1693, average loss: 2.6343
[09/26 06:20:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 28.50	top5: 62.50	
[09/26 06:20:30 visual_prompt]: Best epoch 9: best metric: 0.285
[09/26 06:20:30 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 06:20:37 visual_prompt]: Epoch 10 / 100: avg data time: 5.07e-02, avg batch time: 0.4998, average train loss: 1.7746
[09/26 06:20:38 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1691, average loss: 2.5019
[09/26 06:20:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 34.00	top5: 68.00	
[09/26 06:20:38 visual_prompt]: Best epoch 10: best metric: 0.340
[09/26 06:20:38 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 06:20:45 visual_prompt]: Epoch 11 / 100: avg data time: 5.98e-02, avg batch time: 0.5074, average train loss: 1.5382
[09/26 06:20:46 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1693, average loss: 2.4476
[09/26 06:20:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 35.50	top5: 64.50	
[09/26 06:20:46 visual_prompt]: Best epoch 11: best metric: 0.355
[09/26 06:20:46 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 06:20:53 visual_prompt]: Epoch 12 / 100: avg data time: 6.00e-02, avg batch time: 0.5081, average train loss: 1.2038
[09/26 06:20:55 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1692, average loss: 2.1501
[09/26 06:20:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 74.50	
[09/26 06:20:55 visual_prompt]: Best epoch 12: best metric: 0.415
[09/26 06:20:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 06:21:02 visual_prompt]: Epoch 13 / 100: avg data time: 4.44e-02, avg batch time: 0.4957, average train loss: 0.9103
[09/26 06:21:03 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1694, average loss: 2.1375
[09/26 06:21:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 75.50	
[09/26 06:21:03 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 06:21:10 visual_prompt]: Epoch 14 / 100: avg data time: 5.36e-02, avg batch time: 0.5014, average train loss: 0.6907
[09/26 06:21:12 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1692, average loss: 1.9066
[09/26 06:21:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 79.00	
[09/26 06:21:12 visual_prompt]: Best epoch 14: best metric: 0.490
[09/26 06:21:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 06:21:18 visual_prompt]: Epoch 15 / 100: avg data time: 5.62e-02, avg batch time: 0.5040, average train loss: 0.5812
[09/26 06:21:20 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1690, average loss: 1.9712
[09/26 06:21:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 73.50	
[09/26 06:21:20 visual_prompt]: Best epoch 15: best metric: 0.510
[09/26 06:21:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 06:21:27 visual_prompt]: Epoch 16 / 100: avg data time: 6.48e-02, avg batch time: 0.5125, average train loss: 0.4797
[09/26 06:21:29 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1693, average loss: 1.9635
[09/26 06:21:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 77.50	
[09/26 06:21:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 06:21:36 visual_prompt]: Epoch 17 / 100: avg data time: 6.00e-02, avg batch time: 0.5090, average train loss: 0.4429
[09/26 06:21:37 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1693, average loss: 1.9171
[09/26 06:21:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 79.00	
[09/26 06:21:37 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 06:21:44 visual_prompt]: Epoch 18 / 100: avg data time: 5.67e-02, avg batch time: 0.5054, average train loss: 0.3955
[09/26 06:21:46 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1693, average loss: 1.9742
[09/26 06:21:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.50	
[09/26 06:21:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 06:21:52 visual_prompt]: Epoch 19 / 100: avg data time: 4.60e-02, avg batch time: 0.4934, average train loss: 0.3332
[09/26 06:21:54 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.1695, average loss: 1.8682
[09/26 06:21:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 78.50	
[09/26 06:21:54 visual_prompt]: Best epoch 19: best metric: 0.520
[09/26 06:21:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 06:22:01 visual_prompt]: Epoch 20 / 100: avg data time: 6.22e-02, avg batch time: 0.5107, average train loss: 0.2841
[09/26 06:22:02 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1693, average loss: 1.9358
[09/26 06:22:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 77.50	
[09/26 06:22:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 06:22:09 visual_prompt]: Epoch 21 / 100: avg data time: 5.41e-02, avg batch time: 0.5037, average train loss: 0.2719
[09/26 06:22:11 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1694, average loss: 1.9015
[09/26 06:22:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 82.50	
[09/26 06:22:11 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 06:22:18 visual_prompt]: Epoch 22 / 100: avg data time: 6.28e-02, avg batch time: 0.5109, average train loss: 0.2646
[09/26 06:22:19 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1692, average loss: 1.9251
[09/26 06:22:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.00	
[09/26 06:22:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 06:22:26 visual_prompt]: Epoch 23 / 100: avg data time: 5.90e-02, avg batch time: 0.5074, average train loss: 0.2734
[09/26 06:22:28 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1692, average loss: 1.9994
[09/26 06:22:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 77.50	
[09/26 06:22:28 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 06:22:35 visual_prompt]: Epoch 24 / 100: avg data time: 6.06e-02, avg batch time: 0.5100, average train loss: 0.2496
[09/26 06:22:36 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1696, average loss: 1.9481
[09/26 06:22:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 77.00	
[09/26 06:22:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 06:22:43 visual_prompt]: Epoch 25 / 100: avg data time: 4.88e-02, avg batch time: 0.4976, average train loss: 0.2425
[09/26 06:22:45 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1692, average loss: 1.9513
[09/26 06:22:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 79.50	
[09/26 06:22:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 06:22:52 visual_prompt]: Epoch 26 / 100: avg data time: 5.99e-02, avg batch time: 0.5077, average train loss: 0.2319
[09/26 06:22:53 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 1.9825
[09/26 06:22:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 80.00	
[09/26 06:22:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 06:23:00 visual_prompt]: Epoch 27 / 100: avg data time: 4.66e-02, avg batch time: 0.4955, average train loss: 0.2461
[09/26 06:23:01 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1695, average loss: 2.1023
[09/26 06:23:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 77.00	
[09/26 06:23:01 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 06:23:08 visual_prompt]: Epoch 28 / 100: avg data time: 5.13e-02, avg batch time: 0.5003, average train loss: 0.2611
[09/26 06:23:10 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 2.1040
[09/26 06:23:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 78.50	
[09/26 06:23:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 06:23:17 visual_prompt]: Epoch 29 / 100: avg data time: 5.26e-02, avg batch time: 0.5013, average train loss: 0.2795
[09/26 06:23:18 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1692, average loss: 2.0667
[09/26 06:23:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.50	
[09/26 06:23:18 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 06:23:25 visual_prompt]: Epoch 30 / 100: avg data time: 5.12e-02, avg batch time: 0.5008, average train loss: 0.2869
[09/26 06:23:27 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 2.0558
[09/26 06:23:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 81.00	
[09/26 06:23:27 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 06:23:33 visual_prompt]: Epoch 31 / 100: avg data time: 4.75e-02, avg batch time: 0.4966, average train loss: 0.3080
[09/26 06:23:35 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1695, average loss: 2.1757
[09/26 06:23:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 77.50	
[09/26 06:23:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 06:23:42 visual_prompt]: Epoch 32 / 100: avg data time: 4.74e-02, avg batch time: 0.4955, average train loss: 0.2987
[09/26 06:23:43 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1690, average loss: 2.0650
[09/26 06:23:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 79.00	
[09/26 06:23:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 06:23:50 visual_prompt]: Epoch 33 / 100: avg data time: 5.84e-02, avg batch time: 0.5073, average train loss: 0.2664
[09/26 06:23:52 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1693, average loss: 2.0843
[09/26 06:23:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.00	
[09/26 06:23:52 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 06:23:58 visual_prompt]: Epoch 34 / 100: avg data time: 5.75e-02, avg batch time: 0.5062, average train loss: 0.3021
[09/26 06:24:00 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1692, average loss: 2.1047
[09/26 06:24:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 79.50	
[09/26 06:24:00 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 06:24:07 visual_prompt]: Epoch 35 / 100: avg data time: 5.35e-02, avg batch time: 0.5014, average train loss: 0.3015
[09/26 06:24:08 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1695, average loss: 2.0549
[09/26 06:24:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 78.00	
[09/26 06:24:08 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 06:24:15 visual_prompt]: Epoch 36 / 100: avg data time: 4.77e-02, avg batch time: 0.4998, average train loss: 0.2700
[09/26 06:24:17 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1695, average loss: 1.9608
[09/26 06:24:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 79.00	
[09/26 06:24:17 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 06:24:24 visual_prompt]: Epoch 37 / 100: avg data time: 5.80e-02, avg batch time: 0.5061, average train loss: 0.2426
[09/26 06:24:25 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1691, average loss: 2.0703
[09/26 06:24:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 80.00	
[09/26 06:24:25 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 06:24:32 visual_prompt]: Epoch 38 / 100: avg data time: 5.15e-02, avg batch time: 0.4997, average train loss: 0.2476
[09/26 06:24:34 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1694, average loss: 1.9452
[09/26 06:24:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 80.50	
[09/26 06:24:34 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 06:24:40 visual_prompt]: Epoch 39 / 100: avg data time: 4.45e-02, avg batch time: 0.4950, average train loss: 0.2158
[09/26 06:24:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 1.9579
[09/26 06:24:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 78.00	
[09/26 06:24:42 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 06:24:49 visual_prompt]: Epoch 40 / 100: avg data time: 6.25e-02, avg batch time: 0.5105, average train loss: 0.1930
[09/26 06:24:50 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1695, average loss: 1.9447
[09/26 06:24:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 77.50	
[09/26 06:24:50 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 06:24:57 visual_prompt]: Epoch 41 / 100: avg data time: 5.67e-02, avg batch time: 0.5053, average train loss: 0.1794
[09/26 06:24:59 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 1.9949
[09/26 06:24:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 78.50	
[09/26 06:24:59 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 06:25:05 visual_prompt]: Epoch 42 / 100: avg data time: 5.04e-02, avg batch time: 0.4996, average train loss: 0.1454
[09/26 06:25:07 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1695, average loss: 1.9044
[09/26 06:25:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 76.50	
[09/26 06:25:07 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 06:25:14 visual_prompt]: Epoch 43 / 100: avg data time: 5.29e-02, avg batch time: 0.5021, average train loss: 0.1404
[09/26 06:25:15 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1693, average loss: 2.0011
[09/26 06:25:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 06:25:15 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 06:25:22 visual_prompt]: Epoch 44 / 100: avg data time: 5.00e-02, avg batch time: 0.4991, average train loss: 0.1273
[09/26 06:25:24 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1693, average loss: 1.9786
[09/26 06:25:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 77.50	
[09/26 06:25:24 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 06:25:30 visual_prompt]: Epoch 45 / 100: avg data time: 4.46e-02, avg batch time: 0.4939, average train loss: 0.1190
[09/26 06:25:32 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 2.0196
[09/26 06:25:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 81.00	
[09/26 06:25:32 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 06:25:39 visual_prompt]: Epoch 46 / 100: avg data time: 4.90e-02, avg batch time: 0.4971, average train loss: 0.1100
[09/26 06:25:40 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1694, average loss: 2.0284
[09/26 06:25:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 80.00	
[09/26 06:25:40 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 06:25:47 visual_prompt]: Epoch 47 / 100: avg data time: 5.81e-02, avg batch time: 0.5067, average train loss: 0.1076
[09/26 06:25:49 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1691, average loss: 1.9771
[09/26 06:25:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 80.00	
[09/26 06:25:49 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 06:25:56 visual_prompt]: Epoch 48 / 100: avg data time: 5.95e-02, avg batch time: 0.5079, average train loss: 0.1035
[09/26 06:25:57 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1696, average loss: 1.9290
[09/26 06:25:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 80.50	
[09/26 06:25:57 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 06:26:04 visual_prompt]: Epoch 49 / 100: avg data time: 5.29e-02, avg batch time: 0.5025, average train loss: 0.1011
[09/26 06:26:05 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1693, average loss: 1.9866
[09/26 06:26:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 79.00	
[09/26 06:26:05 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 06:26:12 visual_prompt]: Epoch 50 / 100: avg data time: 6.28e-02, avg batch time: 0.5104, average train loss: 0.0997
[09/26 06:26:14 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1692, average loss: 1.9027
[09/26 06:26:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 81.50	
[09/26 06:26:14 visual_prompt]: Best epoch 50: best metric: 0.535
[09/26 06:26:14 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 06:26:21 visual_prompt]: Epoch 51 / 100: avg data time: 5.74e-02, avg batch time: 0.5053, average train loss: 0.0991
[09/26 06:26:22 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1692, average loss: 1.9549
[09/26 06:26:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.00	
[09/26 06:26:22 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 06:26:29 visual_prompt]: Epoch 52 / 100: avg data time: 6.17e-02, avg batch time: 0.5103, average train loss: 0.1009
[09/26 06:26:31 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1692, average loss: 1.9297
[09/26 06:26:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 80.00	
[09/26 06:26:31 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 06:26:38 visual_prompt]: Epoch 53 / 100: avg data time: 5.92e-02, avg batch time: 0.5093, average train loss: 0.1013
[09/26 06:26:39 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1687, average loss: 1.9433
[09/26 06:26:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 79.00	
[09/26 06:26:39 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 06:26:46 visual_prompt]: Epoch 54 / 100: avg data time: 5.86e-02, avg batch time: 0.5074, average train loss: 0.1022
[09/26 06:26:48 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1690, average loss: 1.9042
[09/26 06:26:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 55.00	top5: 80.00	
[09/26 06:26:48 visual_prompt]: Best epoch 54: best metric: 0.550
[09/26 06:26:48 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 06:26:55 visual_prompt]: Epoch 55 / 100: avg data time: 5.21e-02, avg batch time: 0.5006, average train loss: 0.1027
[09/26 06:26:56 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 1.9608
[09/26 06:26:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 81.00	
[09/26 06:26:56 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 06:27:03 visual_prompt]: Epoch 56 / 100: avg data time: 4.53e-02, avg batch time: 0.4935, average train loss: 0.1030
[09/26 06:27:04 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1695, average loss: 1.9581
[09/26 06:27:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 79.00	
[09/26 06:27:04 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 06:27:11 visual_prompt]: Epoch 57 / 100: avg data time: 5.92e-02, avg batch time: 0.5076, average train loss: 0.1057
[09/26 06:27:13 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 1.9609
[09/26 06:27:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 81.00	
[09/26 06:27:13 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 06:27:20 visual_prompt]: Epoch 58 / 100: avg data time: 4.98e-02, avg batch time: 0.4992, average train loss: 0.1060
[09/26 06:27:21 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1691, average loss: 1.9931
[09/26 06:27:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 80.50	
[09/26 06:27:21 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 06:27:28 visual_prompt]: Epoch 59 / 100: avg data time: 4.47e-02, avg batch time: 0.4940, average train loss: 0.1057
[09/26 06:27:29 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1694, average loss: 1.9696
[09/26 06:27:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 79.50	
[09/26 06:27:29 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 06:27:36 visual_prompt]: Epoch 60 / 100: avg data time: 6.14e-02, avg batch time: 0.5102, average train loss: 0.1050
[09/26 06:27:38 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1692, average loss: 2.0000
[09/26 06:27:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.50	
[09/26 06:27:38 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 06:27:45 visual_prompt]: Epoch 61 / 100: avg data time: 5.65e-02, avg batch time: 0.5045, average train loss: 0.1044
[09/26 06:27:46 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1694, average loss: 1.9764
[09/26 06:27:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 78.50	
[09/26 06:27:46 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 06:27:53 visual_prompt]: Epoch 62 / 100: avg data time: 5.90e-02, avg batch time: 0.5067, average train loss: 0.1034
[09/26 06:27:55 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1695, average loss: 1.9992
[09/26 06:27:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 81.50	
[09/26 06:27:55 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 06:28:02 visual_prompt]: Epoch 63 / 100: avg data time: 5.36e-02, avg batch time: 0.5022, average train loss: 0.1042
[09/26 06:28:03 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1695, average loss: 2.0435
[09/26 06:28:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 78.50	
[09/26 06:28:03 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 06:28:10 visual_prompt]: Epoch 64 / 100: avg data time: 4.34e-02, avg batch time: 0.4953, average train loss: 0.1040
[09/26 06:28:11 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 1.9591
[09/26 06:28:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 77.50	
[09/26 06:28:11 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 06:28:18 visual_prompt]: Epoch 65 / 100: avg data time: 5.54e-02, avg batch time: 0.5046, average train loss: 0.1035
[09/26 06:28:20 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1698, average loss: 1.9865
[09/26 06:28:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 80.00	
[09/26 06:28:20 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 06:28:27 visual_prompt]: Epoch 66 / 100: avg data time: 5.70e-02, avg batch time: 0.5066, average train loss: 0.1027
[09/26 06:28:28 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 1.9824
[09/26 06:28:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.00	top5: 76.50	
[09/26 06:28:28 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 06:28:35 visual_prompt]: Epoch 67 / 100: avg data time: 5.56e-02, avg batch time: 0.5038, average train loss: 0.1020
[09/26 06:28:37 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1693, average loss: 1.9831
[09/26 06:28:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.50	top5: 79.50	
[09/26 06:28:37 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 06:28:44 visual_prompt]: Epoch 68 / 100: avg data time: 5.82e-02, avg batch time: 0.5061, average train loss: 0.1017
[09/26 06:28:45 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1692, average loss: 1.9905
[09/26 06:28:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 78.50	
[09/26 06:28:45 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 06:28:52 visual_prompt]: Epoch 69 / 100: avg data time: 6.41e-02, avg batch time: 0.5130, average train loss: 0.1018
[09/26 06:28:54 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 1.9859
[09/26 06:28:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 78.00	
[09/26 06:28:54 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 06:29:00 visual_prompt]: Epoch 70 / 100: avg data time: 4.83e-02, avg batch time: 0.4972, average train loss: 0.1016
[09/26 06:29:02 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1689, average loss: 1.9771
[09/26 06:29:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 78.00	
[09/26 06:29:02 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 06:29:09 visual_prompt]: Epoch 71 / 100: avg data time: 5.10e-02, avg batch time: 0.4997, average train loss: 0.1015
[09/26 06:29:10 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 1.9650
[09/26 06:29:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 79.00	
[09/26 06:29:10 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 06:29:17 visual_prompt]: Epoch 72 / 100: avg data time: 5.10e-02, avg batch time: 0.5003, average train loss: 0.1009
[09/26 06:29:19 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 1.9405
[09/26 06:29:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 54.50	top5: 79.00	
[09/26 06:29:19 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 06:29:26 visual_prompt]: Epoch 73 / 100: avg data time: 4.73e-02, avg batch time: 0.4971, average train loss: 0.1007
[09/26 06:29:27 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1690, average loss: 2.0344
[09/26 06:29:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 78.00	
[09/26 06:29:27 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 06:29:34 visual_prompt]: Epoch 74 / 100: avg data time: 5.21e-02, avg batch time: 0.5025, average train loss: 0.1002
[09/26 06:29:35 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 2.0270
[09/26 06:29:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 79.50	
[09/26 06:29:35 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 06:29:42 visual_prompt]: Epoch 75 / 100: avg data time: 5.94e-02, avg batch time: 0.5070, average train loss: 0.1003
[09/26 06:29:44 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1690, average loss: 1.9827
[09/26 06:29:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 78.00	
[09/26 06:29:44 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 06:29:51 visual_prompt]: Epoch 76 / 100: avg data time: 5.42e-02, avg batch time: 0.5039, average train loss: 0.0998
[09/26 06:29:52 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1694, average loss: 1.9925
[09/26 06:29:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.00	
[09/26 06:29:52 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 06:29:59 visual_prompt]: Epoch 77 / 100: avg data time: 6.03e-02, avg batch time: 0.5080, average train loss: 0.0995
[09/26 06:30:00 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 1.9845
[09/26 06:30:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.50	top5: 79.50	
[09/26 06:30:00 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 06:30:07 visual_prompt]: Epoch 78 / 100: avg data time: 5.38e-02, avg batch time: 0.5019, average train loss: 0.0992
[09/26 06:30:09 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1695, average loss: 1.9926
[09/26 06:30:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 78.00	
[09/26 06:30:09 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 06:30:16 visual_prompt]: Epoch 79 / 100: avg data time: 5.97e-02, avg batch time: 0.5075, average train loss: 0.0991
[09/26 06:30:17 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1693, average loss: 1.9994
[09/26 06:30:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 78.50	
[09/26 06:30:17 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 06:30:24 visual_prompt]: Epoch 80 / 100: avg data time: 5.23e-02, avg batch time: 0.5017, average train loss: 0.0985
[09/26 06:30:26 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1695, average loss: 2.0255
[09/26 06:30:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 78.00	
[09/26 06:30:26 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 06:30:32 visual_prompt]: Epoch 81 / 100: avg data time: 5.07e-02, avg batch time: 0.5000, average train loss: 0.0989
[09/26 06:30:34 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1695, average loss: 2.0141
[09/26 06:30:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 80.00	
[09/26 06:30:34 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 06:30:41 visual_prompt]: Epoch 82 / 100: avg data time: 4.73e-02, avg batch time: 0.4959, average train loss: 0.0987
[09/26 06:30:42 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1695, average loss: 1.9924
[09/26 06:30:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.50	top5: 78.50	
[09/26 06:30:42 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 06:30:49 visual_prompt]: Epoch 83 / 100: avg data time: 4.79e-02, avg batch time: 0.4971, average train loss: 0.0988
[09/26 06:30:50 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1695, average loss: 2.0080
[09/26 06:30:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 78.50	
[09/26 06:30:50 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 06:30:57 visual_prompt]: Epoch 84 / 100: avg data time: 5.72e-02, avg batch time: 0.5058, average train loss: 0.0985
[09/26 06:30:59 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 2.0102
[09/26 06:30:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 79.00	
[09/26 06:30:59 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 06:31:06 visual_prompt]: Epoch 85 / 100: avg data time: 5.59e-02, avg batch time: 0.5045, average train loss: 0.0981
[09/26 06:31:07 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1697, average loss: 1.9929
[09/26 06:31:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 78.00	
[09/26 06:31:07 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 06:31:14 visual_prompt]: Epoch 86 / 100: avg data time: 5.70e-02, avg batch time: 0.5062, average train loss: 0.0983
[09/26 06:31:16 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1692, average loss: 2.0299
[09/26 06:31:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.50	
[09/26 06:31:16 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 06:31:22 visual_prompt]: Epoch 87 / 100: avg data time: 4.91e-02, avg batch time: 0.4979, average train loss: 0.0983
[09/26 06:31:24 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1694, average loss: 2.0229
[09/26 06:31:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 51.00	top5: 77.50	
[09/26 06:31:24 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 06:31:31 visual_prompt]: Epoch 88 / 100: avg data time: 5.01e-02, avg batch time: 0.4996, average train loss: 0.0980
[09/26 06:31:32 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1697, average loss: 1.9828
[09/26 06:31:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 79.00	
[09/26 06:31:32 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 06:31:39 visual_prompt]: Epoch 89 / 100: avg data time: 5.90e-02, avg batch time: 0.5079, average train loss: 0.0982
[09/26 06:31:41 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1695, average loss: 1.9953
[09/26 06:31:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 77.00	
[09/26 06:31:41 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 06:31:48 visual_prompt]: Epoch 90 / 100: avg data time: 6.53e-02, avg batch time: 0.5136, average train loss: 0.0979
[09/26 06:31:49 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1691, average loss: 1.9908
[09/26 06:31:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 77.50	
[09/26 06:31:49 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 06:31:56 visual_prompt]: Epoch 91 / 100: avg data time: 5.27e-02, avg batch time: 0.5007, average train loss: 0.0978
[09/26 06:31:57 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1690, average loss: 1.9963
[09/26 06:31:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 78.00	
[09/26 06:31:57 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 06:32:04 visual_prompt]: Epoch 92 / 100: avg data time: 6.35e-02, avg batch time: 0.5115, average train loss: 0.0978
[09/26 06:32:06 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1694, average loss: 1.9928
[09/26 06:32:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 78.50	
[09/26 06:32:06 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 06:32:13 visual_prompt]: Epoch 93 / 100: avg data time: 4.71e-02, avg batch time: 0.4966, average train loss: 0.0976
[09/26 06:32:14 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 1.9949
[09/26 06:32:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 78.50	
[09/26 06:32:14 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 06:32:21 visual_prompt]: Epoch 94 / 100: avg data time: 6.04e-02, avg batch time: 0.5081, average train loss: 0.0978
[09/26 06:32:23 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1695, average loss: 1.9988
[09/26 06:32:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.00	top5: 78.00	
[09/26 06:32:23 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 06:32:30 visual_prompt]: Epoch 95 / 100: avg data time: 5.21e-02, avg batch time: 0.5003, average train loss: 0.0976
[09/26 06:32:31 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1691, average loss: 1.9974
[09/26 06:32:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 78.50	
[09/26 06:32:31 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 06:32:38 visual_prompt]: Epoch 96 / 100: avg data time: 5.94e-02, avg batch time: 0.5072, average train loss: 0.0976
[09/26 06:32:40 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1693, average loss: 1.9985
[09/26 06:32:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 52.50	top5: 78.50	
[09/26 06:32:40 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 06:32:46 visual_prompt]: Epoch 97 / 100: avg data time: 5.74e-02, avg batch time: 0.5053, average train loss: 0.0972
[09/26 06:32:48 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1690, average loss: 2.0006
[09/26 06:32:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 78.00	
[09/26 06:32:48 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 06:32:55 visual_prompt]: Epoch 98 / 100: avg data time: 6.26e-02, avg batch time: 0.5112, average train loss: 0.0974
[09/26 06:32:57 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 2.0006
[09/26 06:32:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 78.00	
[09/26 06:32:57 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 06:33:03 visual_prompt]: Epoch 99 / 100: avg data time: 4.68e-02, avg batch time: 0.4993, average train loss: 0.0975
[09/26 06:33:05 visual_prompt]: Inference (val):avg data time: 4.81e-05, avg batch time: 0.1692, average loss: 2.0006
[09/26 06:33:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 78.00	
[09/26 06:33:05 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 06:33:12 visual_prompt]: Epoch 100 / 100: avg data time: 4.75e-02, avg batch time: 0.4952, average train loss: 0.0977
[09/26 06:33:13 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 2.0007
[09/26 06:33:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 53.00	top5: 78.00	
[09/26 06:33:13 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 06:33:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 06:33:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 06:33:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 06:33:13 visual_prompt]: Training with config:
[09/26 06:33:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 06:33:13 visual_prompt]: Loading training data...
[09/26 06:33:13 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 06:33:15 visual_prompt]: Number of images: 800
[09/26 06:33:15 visual_prompt]: Number of classes: 47 / 47
[09/26 06:33:15 visual_prompt]: Loading validation data...
[09/26 06:33:15 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 06:33:16 visual_prompt]: Number of images: 200
[09/26 06:33:16 visual_prompt]: Number of classes: 47 / 47
[09/26 06:33:16 visual_prompt]: Constructing models...
[09/26 06:33:18 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 06:33:18 visual_prompt]: tuned percent:0.576
[09/26 06:33:19 visual_prompt]: Device used for model: 0
[09/26 06:33:19 visual_prompt]: Setting up Evaluator...
[09/26 06:33:19 visual_prompt]: Setting up Trainer...
[09/26 06:33:19 visual_prompt]: 	Setting up the optimizer...
[09/26 06:33:19 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 06:33:25 visual_prompt]: Epoch 1 / 100: avg data time: 5.67e-02, avg batch time: 0.5043, average train loss: 3.9302
[09/26 06:33:27 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1687, average loss: 3.9045
[09/26 06:33:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 06:33:27 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 06:33:27 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 06:33:34 visual_prompt]: Epoch 2 / 100: avg data time: 4.65e-02, avg batch time: 0.4959, average train loss: 3.8942
[09/26 06:33:35 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1689, average loss: 3.8702
[09/26 06:33:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 2.50	top5: 14.50	
[09/26 06:33:35 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 06:33:42 visual_prompt]: Epoch 3 / 100: avg data time: 5.40e-02, avg batch time: 0.5010, average train loss: 3.8296
[09/26 06:33:44 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1688, average loss: 3.8513
[09/26 06:33:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 14.00	
[09/26 06:33:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 06:33:51 visual_prompt]: Epoch 4 / 100: avg data time: 6.19e-02, avg batch time: 0.5091, average train loss: 3.7332
[09/26 06:33:52 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1688, average loss: 3.7640
[09/26 06:33:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 20.50	
[09/26 06:33:52 visual_prompt]: Best epoch 4: best metric: 0.045
[09/26 06:33:52 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 06:33:59 visual_prompt]: Epoch 5 / 100: avg data time: 4.60e-02, avg batch time: 0.4951, average train loss: 3.5782
[09/26 06:34:01 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1690, average loss: 3.6448
[09/26 06:34:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 12.00	top5: 27.50	
[09/26 06:34:01 visual_prompt]: Best epoch 5: best metric: 0.120
[09/26 06:34:01 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 06:34:07 visual_prompt]: Epoch 6 / 100: avg data time: 6.10e-02, avg batch time: 0.5082, average train loss: 3.3074
[09/26 06:34:09 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1686, average loss: 3.4417
[09/26 06:34:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 11.00	top5: 32.50	
[09/26 06:34:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 06:34:16 visual_prompt]: Epoch 7 / 100: avg data time: 6.00e-02, avg batch time: 0.5086, average train loss: 2.8493
[09/26 06:34:18 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1688, average loss: 3.1311
[09/26 06:34:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.50	top5: 46.50	
[09/26 06:34:18 visual_prompt]: Best epoch 7: best metric: 0.165
[09/26 06:34:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 06:34:24 visual_prompt]: Epoch 8 / 100: avg data time: 5.56e-02, avg batch time: 0.5030, average train loss: 2.4561
[09/26 06:34:26 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1690, average loss: 2.9078
[09/26 06:34:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.00	top5: 54.50	
[09/26 06:34:26 visual_prompt]: Best epoch 8: best metric: 0.270
[09/26 06:34:26 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 06:34:33 visual_prompt]: Epoch 9 / 100: avg data time: 6.30e-02, avg batch time: 0.5117, average train loss: 2.0788
[09/26 06:34:34 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1690, average loss: 2.6809
[09/26 06:34:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 27.00	top5: 56.00	
[09/26 06:34:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 06:34:41 visual_prompt]: Epoch 10 / 100: avg data time: 6.07e-02, avg batch time: 0.5083, average train loss: 1.7507
[09/26 06:34:43 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 2.6017
[09/26 06:34:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 29.50	top5: 63.50	
[09/26 06:34:43 visual_prompt]: Best epoch 10: best metric: 0.295
[09/26 06:34:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 06:34:50 visual_prompt]: Epoch 11 / 100: avg data time: 4.95e-02, avg batch time: 0.4982, average train loss: 1.4680
[09/26 06:34:51 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 2.5394
[09/26 06:34:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 32.00	top5: 66.50	
[09/26 06:34:51 visual_prompt]: Best epoch 11: best metric: 0.320
[09/26 06:34:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 06:34:58 visual_prompt]: Epoch 12 / 100: avg data time: 6.77e-02, avg batch time: 0.5160, average train loss: 1.0960
[09/26 06:35:00 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1689, average loss: 2.2759
[09/26 06:35:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.00	top5: 71.00	
[09/26 06:35:00 visual_prompt]: Best epoch 12: best metric: 0.410
[09/26 06:35:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 06:35:07 visual_prompt]: Epoch 13 / 100: avg data time: 5.78e-02, avg batch time: 0.5059, average train loss: 0.8585
[09/26 06:35:08 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1694, average loss: 2.3232
[09/26 06:35:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.00	top5: 71.00	
[09/26 06:35:08 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 06:35:15 visual_prompt]: Epoch 14 / 100: avg data time: 5.76e-02, avg batch time: 0.5067, average train loss: 0.6050
[09/26 06:35:17 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1693, average loss: 2.1511
[09/26 06:35:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 75.00	
[09/26 06:35:17 visual_prompt]: Best epoch 14: best metric: 0.430
[09/26 06:35:17 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 06:35:23 visual_prompt]: Epoch 15 / 100: avg data time: 4.80e-02, avg batch time: 0.4974, average train loss: 0.4200
[09/26 06:35:25 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1692, average loss: 2.2240
[09/26 06:35:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 75.50	
[09/26 06:35:25 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 06:35:32 visual_prompt]: Epoch 16 / 100: avg data time: 5.47e-02, avg batch time: 0.5035, average train loss: 0.3192
[09/26 06:35:33 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1696, average loss: 2.2254
[09/26 06:35:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 75.50	
[09/26 06:35:33 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 06:35:40 visual_prompt]: Epoch 17 / 100: avg data time: 6.18e-02, avg batch time: 0.5118, average train loss: 0.2160
[09/26 06:35:42 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1696, average loss: 2.2674
[09/26 06:35:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 72.50	
[09/26 06:35:42 visual_prompt]: Best epoch 17: best metric: 0.445
[09/26 06:35:42 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 06:35:49 visual_prompt]: Epoch 18 / 100: avg data time: 5.78e-02, avg batch time: 0.5067, average train loss: 0.1454
[09/26 06:35:50 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 2.2812
[09/26 06:35:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 73.00	
[09/26 06:35:50 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 06:35:57 visual_prompt]: Epoch 19 / 100: avg data time: 6.41e-02, avg batch time: 0.5143, average train loss: 0.1144
[09/26 06:35:59 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1689, average loss: 2.2572
[09/26 06:35:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 77.00	
[09/26 06:35:59 visual_prompt]: Best epoch 19: best metric: 0.450
[09/26 06:35:59 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 06:36:06 visual_prompt]: Epoch 20 / 100: avg data time: 4.69e-02, avg batch time: 0.4992, average train loss: 0.0810
[09/26 06:36:07 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1691, average loss: 2.2270
[09/26 06:36:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 74.50	
[09/26 06:36:07 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 06:36:14 visual_prompt]: Epoch 21 / 100: avg data time: 6.03e-02, avg batch time: 0.5078, average train loss: 0.0684
[09/26 06:36:16 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1689, average loss: 2.2751
[09/26 06:36:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 76.50	
[09/26 06:36:16 visual_prompt]: Best epoch 21: best metric: 0.455
[09/26 06:36:16 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 06:36:23 visual_prompt]: Epoch 22 / 100: avg data time: 6.03e-02, avg batch time: 0.5088, average train loss: 0.0551
[09/26 06:36:25 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1693, average loss: 2.2453
[09/26 06:36:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 75.00	
[09/26 06:36:25 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 06:36:31 visual_prompt]: Epoch 23 / 100: avg data time: 5.60e-02, avg batch time: 0.5036, average train loss: 0.0478
[09/26 06:36:33 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1691, average loss: 2.2791
[09/26 06:36:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.00	top5: 76.00	
[09/26 06:36:33 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 06:36:40 visual_prompt]: Epoch 24 / 100: avg data time: 5.88e-02, avg batch time: 0.5073, average train loss: 0.0460
[09/26 06:36:41 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1694, average loss: 2.3195
[09/26 06:36:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 76.00	
[09/26 06:36:41 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 06:36:48 visual_prompt]: Epoch 25 / 100: avg data time: 6.18e-02, avg batch time: 0.5103, average train loss: 0.0395
[09/26 06:36:50 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1693, average loss: 2.2744
[09/26 06:36:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 76.00	
[09/26 06:36:50 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 06:36:57 visual_prompt]: Epoch 26 / 100: avg data time: 4.57e-02, avg batch time: 0.4957, average train loss: 0.0369
[09/26 06:36:58 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1697, average loss: 2.2987
[09/26 06:36:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 74.50	
[09/26 06:36:58 visual_prompt]: Best epoch 26: best metric: 0.465
[09/26 06:36:58 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 06:37:05 visual_prompt]: Epoch 27 / 100: avg data time: 4.42e-02, avg batch time: 0.4969, average train loss: 0.0321
[09/26 06:37:07 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1695, average loss: 2.3310
[09/26 06:37:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 76.50	
[09/26 06:37:07 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 06:37:14 visual_prompt]: Epoch 28 / 100: avg data time: 6.06e-02, avg batch time: 0.5082, average train loss: 0.0292
[09/26 06:37:15 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1692, average loss: 2.3001
[09/26 06:37:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 77.00	
[09/26 06:37:15 visual_prompt]: Best epoch 28: best metric: 0.480
[09/26 06:37:15 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 06:37:22 visual_prompt]: Epoch 29 / 100: avg data time: 6.03e-02, avg batch time: 0.5088, average train loss: 0.0283
[09/26 06:37:24 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1692, average loss: 2.2981
[09/26 06:37:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 77.00	
[09/26 06:37:24 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 06:37:30 visual_prompt]: Epoch 30 / 100: avg data time: 4.69e-02, avg batch time: 0.4957, average train loss: 0.0246
[09/26 06:37:32 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1691, average loss: 2.3131
[09/26 06:37:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 78.00	
[09/26 06:37:32 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 06:37:39 visual_prompt]: Epoch 31 / 100: avg data time: 5.63e-02, avg batch time: 0.5046, average train loss: 0.0250
[09/26 06:37:40 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 2.2903
[09/26 06:37:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 76.50	
[09/26 06:37:40 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 06:37:47 visual_prompt]: Epoch 32 / 100: avg data time: 5.42e-02, avg batch time: 0.5035, average train loss: 0.0240
[09/26 06:37:49 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1694, average loss: 2.3306
[09/26 06:37:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 76.00	
[09/26 06:37:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 06:37:56 visual_prompt]: Epoch 33 / 100: avg data time: 5.31e-02, avg batch time: 0.5008, average train loss: 0.0213
[09/26 06:37:57 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1690, average loss: 2.3368
[09/26 06:37:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 74.50	
[09/26 06:37:57 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 06:38:04 visual_prompt]: Epoch 34 / 100: avg data time: 5.05e-02, avg batch time: 0.4998, average train loss: 0.0203
[09/26 06:38:06 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1691, average loss: 2.3291
[09/26 06:38:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 75.00	
[09/26 06:38:06 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 06:38:13 visual_prompt]: Epoch 35 / 100: avg data time: 5.51e-02, avg batch time: 0.5031, average train loss: 0.0209
[09/26 06:38:14 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1692, average loss: 2.3209
[09/26 06:38:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.50	
[09/26 06:38:14 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 06:38:21 visual_prompt]: Epoch 36 / 100: avg data time: 4.34e-02, avg batch time: 0.4924, average train loss: 0.0199
[09/26 06:38:22 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1691, average loss: 2.3076
[09/26 06:38:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 76.50	
[09/26 06:38:22 visual_prompt]: Best epoch 36: best metric: 0.495
[09/26 06:38:22 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 06:38:29 visual_prompt]: Epoch 37 / 100: avg data time: 4.82e-02, avg batch time: 0.4965, average train loss: 0.0195
[09/26 06:38:31 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1695, average loss: 2.3052
[09/26 06:38:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 06:38:31 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 06:38:38 visual_prompt]: Epoch 38 / 100: avg data time: 6.21e-02, avg batch time: 0.5105, average train loss: 0.0190
[09/26 06:38:39 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1695, average loss: 2.2983
[09/26 06:38:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 75.00	
[09/26 06:38:39 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 06:38:46 visual_prompt]: Epoch 39 / 100: avg data time: 5.97e-02, avg batch time: 0.5075, average train loss: 0.0184
[09/26 06:38:48 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 2.3041
[09/26 06:38:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 74.50	
[09/26 06:38:48 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 06:38:55 visual_prompt]: Epoch 40 / 100: avg data time: 5.46e-02, avg batch time: 0.5036, average train loss: 0.0183
[09/26 06:38:56 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1689, average loss: 2.3374
[09/26 06:38:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 75.50	
[09/26 06:38:56 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 06:39:03 visual_prompt]: Epoch 41 / 100: avg data time: 4.88e-02, avg batch time: 0.4980, average train loss: 0.0181
[09/26 06:39:04 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 2.3421
[09/26 06:39:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 74.50	
[09/26 06:39:04 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 06:39:11 visual_prompt]: Epoch 42 / 100: avg data time: 5.55e-02, avg batch time: 0.5047, average train loss: 0.0174
[09/26 06:39:13 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1694, average loss: 2.3459
[09/26 06:39:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 76.00	
[09/26 06:39:13 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 06:39:20 visual_prompt]: Epoch 43 / 100: avg data time: 5.70e-02, avg batch time: 0.5059, average train loss: 0.0170
[09/26 06:39:21 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1692, average loss: 2.3345
[09/26 06:39:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 06:39:21 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 06:39:28 visual_prompt]: Epoch 44 / 100: avg data time: 6.05e-02, avg batch time: 0.5087, average train loss: 0.0164
[09/26 06:39:30 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 2.3503
[09/26 06:39:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.00	
[09/26 06:39:30 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 06:39:37 visual_prompt]: Epoch 45 / 100: avg data time: 5.61e-02, avg batch time: 0.5049, average train loss: 0.0165
[09/26 06:39:38 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1696, average loss: 2.3546
[09/26 06:39:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.00	
[09/26 06:39:38 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 06:39:45 visual_prompt]: Epoch 46 / 100: avg data time: 5.77e-02, avg batch time: 0.5069, average train loss: 0.0160
[09/26 06:39:47 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1691, average loss: 2.3563
[09/26 06:39:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 74.50	
[09/26 06:39:47 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 06:39:54 visual_prompt]: Epoch 47 / 100: avg data time: 5.42e-02, avg batch time: 0.5037, average train loss: 0.0158
[09/26 06:39:55 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1696, average loss: 2.3557
[09/26 06:39:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 06:39:55 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 06:40:02 visual_prompt]: Epoch 48 / 100: avg data time: 5.31e-02, avg batch time: 0.5011, average train loss: 0.0156
[09/26 06:40:03 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1694, average loss: 2.3406
[09/26 06:40:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.00	
[09/26 06:40:03 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 06:40:10 visual_prompt]: Epoch 49 / 100: avg data time: 4.60e-02, avg batch time: 0.4956, average train loss: 0.0167
[09/26 06:40:12 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1694, average loss: 2.3289
[09/26 06:40:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 75.50	
[09/26 06:40:12 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 06:40:19 visual_prompt]: Epoch 50 / 100: avg data time: 6.13e-02, avg batch time: 0.5089, average train loss: 0.0159
[09/26 06:40:20 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1694, average loss: 2.3402
[09/26 06:40:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 76.50	
[09/26 06:40:20 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 06:40:27 visual_prompt]: Epoch 51 / 100: avg data time: 5.53e-02, avg batch time: 0.5031, average train loss: 0.0157
[09/26 06:40:29 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1694, average loss: 2.3370
[09/26 06:40:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 76.50	
[09/26 06:40:29 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 06:40:36 visual_prompt]: Epoch 52 / 100: avg data time: 6.27e-02, avg batch time: 0.5123, average train loss: 0.0151
[09/26 06:40:37 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1690, average loss: 2.3335
[09/26 06:40:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 77.00	
[09/26 06:40:37 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 06:40:44 visual_prompt]: Epoch 53 / 100: avg data time: 5.79e-02, avg batch time: 0.5055, average train loss: 0.0145
[09/26 06:40:46 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1693, average loss: 2.3259
[09/26 06:40:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.00	
[09/26 06:40:46 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 06:40:53 visual_prompt]: Epoch 54 / 100: avg data time: 6.18e-02, avg batch time: 0.5104, average train loss: 0.0155
[09/26 06:40:54 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1697, average loss: 2.3327
[09/26 06:40:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.50	
[09/26 06:40:54 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 06:41:01 visual_prompt]: Epoch 55 / 100: avg data time: 6.38e-02, avg batch time: 0.5132, average train loss: 0.0147
[09/26 06:41:03 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1687, average loss: 2.3459
[09/26 06:41:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.50	
[09/26 06:41:03 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 06:41:10 visual_prompt]: Epoch 56 / 100: avg data time: 5.53e-02, avg batch time: 0.5037, average train loss: 0.0156
[09/26 06:41:11 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1696, average loss: 2.3433
[09/26 06:41:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 06:41:11 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 06:41:18 visual_prompt]: Epoch 57 / 100: avg data time: 5.30e-02, avg batch time: 0.5023, average train loss: 0.0147
[09/26 06:41:20 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1691, average loss: 2.3356
[09/26 06:41:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.00	
[09/26 06:41:20 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 06:41:27 visual_prompt]: Epoch 58 / 100: avg data time: 6.13e-02, avg batch time: 0.5090, average train loss: 0.0146
[09/26 06:41:28 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 2.3470
[09/26 06:41:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 76.00	
[09/26 06:41:28 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 06:41:35 visual_prompt]: Epoch 59 / 100: avg data time: 5.42e-02, avg batch time: 0.5042, average train loss: 0.0153
[09/26 06:41:37 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1693, average loss: 2.3530
[09/26 06:41:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.00	
[09/26 06:41:37 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 06:41:44 visual_prompt]: Epoch 60 / 100: avg data time: 5.62e-02, avg batch time: 0.5037, average train loss: 0.0147
[09/26 06:41:45 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1688, average loss: 2.3544
[09/26 06:41:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 75.00	
[09/26 06:41:45 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 06:41:52 visual_prompt]: Epoch 61 / 100: avg data time: 5.84e-02, avg batch time: 0.5081, average train loss: 0.0138
[09/26 06:41:53 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1695, average loss: 2.3491
[09/26 06:41:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.00	
[09/26 06:41:53 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 06:42:00 visual_prompt]: Epoch 62 / 100: avg data time: 5.80e-02, avg batch time: 0.5066, average train loss: 0.0142
[09/26 06:42:02 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1689, average loss: 2.3394
[09/26 06:42:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 75.00	
[09/26 06:42:02 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 06:42:09 visual_prompt]: Epoch 63 / 100: avg data time: 5.33e-02, avg batch time: 0.5021, average train loss: 0.0141
[09/26 06:42:10 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1693, average loss: 2.3221
[09/26 06:42:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 75.00	
[09/26 06:42:10 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 06:42:17 visual_prompt]: Epoch 64 / 100: avg data time: 6.30e-02, avg batch time: 0.5114, average train loss: 0.0143
[09/26 06:42:19 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1693, average loss: 2.3176
[09/26 06:42:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 50.00	top5: 74.50	
[09/26 06:42:19 visual_prompt]: Best epoch 64: best metric: 0.500
[09/26 06:42:19 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 06:42:26 visual_prompt]: Epoch 65 / 100: avg data time: 5.44e-02, avg batch time: 0.5023, average train loss: 0.0136
[09/26 06:42:27 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1692, average loss: 2.3253
[09/26 06:42:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.50	top5: 74.00	
[09/26 06:42:27 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 06:42:34 visual_prompt]: Epoch 66 / 100: avg data time: 5.22e-02, avg batch time: 0.4999, average train loss: 0.0141
[09/26 06:42:36 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1694, average loss: 2.3395
[09/26 06:42:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.00	
[09/26 06:42:36 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 06:42:42 visual_prompt]: Epoch 67 / 100: avg data time: 4.37e-02, avg batch time: 0.4932, average train loss: 0.0136
[09/26 06:42:44 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1694, average loss: 2.3446
[09/26 06:42:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 74.00	
[09/26 06:42:44 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 06:42:51 visual_prompt]: Epoch 68 / 100: avg data time: 5.34e-02, avg batch time: 0.5019, average train loss: 0.0135
[09/26 06:42:52 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1693, average loss: 2.3397
[09/26 06:42:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 74.50	
[09/26 06:42:52 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 06:42:59 visual_prompt]: Epoch 69 / 100: avg data time: 6.48e-02, avg batch time: 0.5138, average train loss: 0.0134
[09/26 06:43:01 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 2.3337
[09/26 06:43:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 74.50	
[09/26 06:43:01 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 06:43:08 visual_prompt]: Epoch 70 / 100: avg data time: 5.78e-02, avg batch time: 0.5078, average train loss: 0.0139
[09/26 06:43:09 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1692, average loss: 2.3264
[09/26 06:43:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 74.50	
[09/26 06:43:09 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 06:43:16 visual_prompt]: Epoch 71 / 100: avg data time: 6.48e-02, avg batch time: 0.5127, average train loss: 0.0132
[09/26 06:43:18 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1691, average loss: 2.3305
[09/26 06:43:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 75.00	
[09/26 06:43:18 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 06:43:25 visual_prompt]: Epoch 72 / 100: avg data time: 6.46e-02, avg batch time: 0.5130, average train loss: 0.0136
[09/26 06:43:26 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1690, average loss: 2.3359
[09/26 06:43:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.50	
[09/26 06:43:26 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 06:43:33 visual_prompt]: Epoch 73 / 100: avg data time: 5.49e-02, avg batch time: 0.5036, average train loss: 0.0137
[09/26 06:43:35 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1688, average loss: 2.3386
[09/26 06:43:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 75.00	
[09/26 06:43:35 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 06:43:42 visual_prompt]: Epoch 74 / 100: avg data time: 5.97e-02, avg batch time: 0.5082, average train loss: 0.0134
[09/26 06:43:43 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1693, average loss: 2.3313
[09/26 06:43:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.00	
[09/26 06:43:43 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 06:43:50 visual_prompt]: Epoch 75 / 100: avg data time: 6.14e-02, avg batch time: 0.5091, average train loss: 0.0133
[09/26 06:43:52 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1690, average loss: 2.3275
[09/26 06:43:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 75.00	
[09/26 06:43:52 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 06:43:58 visual_prompt]: Epoch 76 / 100: avg data time: 5.92e-02, avg batch time: 0.5078, average train loss: 0.0126
[09/26 06:44:00 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1693, average loss: 2.3271
[09/26 06:44:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 74.50	
[09/26 06:44:00 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 06:44:07 visual_prompt]: Epoch 77 / 100: avg data time: 5.98e-02, avg batch time: 0.5073, average train loss: 0.0129
[09/26 06:44:08 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1694, average loss: 2.3287
[09/26 06:44:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 74.50	
[09/26 06:44:08 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 06:44:15 visual_prompt]: Epoch 78 / 100: avg data time: 6.24e-02, avg batch time: 0.5114, average train loss: 0.0135
[09/26 06:44:17 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1690, average loss: 2.3355
[09/26 06:44:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 74.50	
[09/26 06:44:17 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 06:44:24 visual_prompt]: Epoch 79 / 100: avg data time: 4.64e-02, avg batch time: 0.4948, average train loss: 0.0134
[09/26 06:44:25 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1692, average loss: 2.3347
[09/26 06:44:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 74.50	
[09/26 06:44:25 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 06:44:32 visual_prompt]: Epoch 80 / 100: avg data time: 6.01e-02, avg batch time: 0.5082, average train loss: 0.0132
[09/26 06:44:34 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1688, average loss: 2.3308
[09/26 06:44:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.00	
[09/26 06:44:34 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 06:44:41 visual_prompt]: Epoch 81 / 100: avg data time: 5.94e-02, avg batch time: 0.5071, average train loss: 0.0130
[09/26 06:44:42 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1693, average loss: 2.3293
[09/26 06:44:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 75.50	
[09/26 06:44:42 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 06:44:49 visual_prompt]: Epoch 82 / 100: avg data time: 5.67e-02, avg batch time: 0.5041, average train loss: 0.0130
[09/26 06:44:51 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1692, average loss: 2.3293
[09/26 06:44:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:44:51 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 06:44:58 visual_prompt]: Epoch 83 / 100: avg data time: 5.24e-02, avg batch time: 0.4999, average train loss: 0.0134
[09/26 06:44:59 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 2.3297
[09/26 06:44:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 74.50	
[09/26 06:44:59 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 06:45:06 visual_prompt]: Epoch 84 / 100: avg data time: 5.61e-02, avg batch time: 0.5054, average train loss: 0.0129
[09/26 06:45:08 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1691, average loss: 2.3318
[09/26 06:45:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 74.50	
[09/26 06:45:08 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 06:45:15 visual_prompt]: Epoch 85 / 100: avg data time: 6.20e-02, avg batch time: 0.5096, average train loss: 0.0125
[09/26 06:45:16 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1688, average loss: 2.3358
[09/26 06:45:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 74.50	
[09/26 06:45:16 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 06:45:23 visual_prompt]: Epoch 86 / 100: avg data time: 5.80e-02, avg batch time: 0.5073, average train loss: 0.0135
[09/26 06:45:25 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1693, average loss: 2.3348
[09/26 06:45:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 74.50	
[09/26 06:45:25 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 06:45:31 visual_prompt]: Epoch 87 / 100: avg data time: 5.04e-02, avg batch time: 0.4992, average train loss: 0.0132
[09/26 06:45:33 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1690, average loss: 2.3353
[09/26 06:45:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 74.00	
[09/26 06:45:33 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 06:45:40 visual_prompt]: Epoch 88 / 100: avg data time: 6.71e-02, avg batch time: 0.5172, average train loss: 0.0134
[09/26 06:45:42 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1693, average loss: 2.3364
[09/26 06:45:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 74.00	
[09/26 06:45:42 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 06:45:48 visual_prompt]: Epoch 89 / 100: avg data time: 5.60e-02, avg batch time: 0.5041, average train loss: 0.0126
[09/26 06:45:50 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1697, average loss: 2.3366
[09/26 06:45:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:45:50 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 06:45:57 visual_prompt]: Epoch 90 / 100: avg data time: 4.69e-02, avg batch time: 0.4975, average train loss: 0.0135
[09/26 06:45:58 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1693, average loss: 2.3363
[09/26 06:45:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:45:58 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 06:46:05 visual_prompt]: Epoch 91 / 100: avg data time: 5.02e-02, avg batch time: 0.5004, average train loss: 0.0131
[09/26 06:46:07 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 2.3354
[09/26 06:46:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:46:07 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 06:46:14 visual_prompt]: Epoch 92 / 100: avg data time: 5.14e-02, avg batch time: 0.4999, average train loss: 0.0130
[09/26 06:46:15 visual_prompt]: Inference (val):avg data time: 5.79e-05, avg batch time: 0.1691, average loss: 2.3345
[09/26 06:46:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:46:15 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 06:46:22 visual_prompt]: Epoch 93 / 100: avg data time: 5.80e-02, avg batch time: 0.5081, average train loss: 0.0129
[09/26 06:46:24 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1695, average loss: 2.3337
[09/26 06:46:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:46:24 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 06:46:30 visual_prompt]: Epoch 94 / 100: avg data time: 5.21e-02, avg batch time: 0.5025, average train loss: 0.0130
[09/26 06:46:32 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1688, average loss: 2.3333
[09/26 06:46:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:46:32 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 06:46:39 visual_prompt]: Epoch 95 / 100: avg data time: 5.88e-02, avg batch time: 0.5076, average train loss: 0.0130
[09/26 06:46:41 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1691, average loss: 2.3331
[09/26 06:46:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:46:41 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 06:46:47 visual_prompt]: Epoch 96 / 100: avg data time: 5.87e-02, avg batch time: 0.5069, average train loss: 0.0128
[09/26 06:46:49 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1693, average loss: 2.3329
[09/26 06:46:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:46:49 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 06:46:56 visual_prompt]: Epoch 97 / 100: avg data time: 4.31e-02, avg batch time: 0.4916, average train loss: 0.0127
[09/26 06:46:57 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1689, average loss: 2.3328
[09/26 06:46:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:46:57 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 06:47:04 visual_prompt]: Epoch 98 / 100: avg data time: 5.47e-02, avg batch time: 0.5031, average train loss: 0.0130
[09/26 06:47:06 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1692, average loss: 2.3328
[09/26 06:47:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:47:06 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 06:47:12 visual_prompt]: Epoch 99 / 100: avg data time: 5.64e-02, avg batch time: 0.5040, average train loss: 0.0129
[09/26 06:47:14 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1691, average loss: 2.3327
[09/26 06:47:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:47:14 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 06:47:21 visual_prompt]: Epoch 100 / 100: avg data time: 5.94e-02, avg batch time: 0.5081, average train loss: 0.0132
[09/26 06:47:22 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1696, average loss: 2.3327
[09/26 06:47:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 49.00	top5: 74.50	
[09/26 06:47:22 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 06:47:22 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 06:47:22 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 06:47:22 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 06:47:22 visual_prompt]: Training with config:
[09/26 06:47:22 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 06:47:22 visual_prompt]: Loading training data...
[09/26 06:47:22 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 06:47:25 visual_prompt]: Number of images: 800
[09/26 06:47:25 visual_prompt]: Number of classes: 47 / 47
[09/26 06:47:25 visual_prompt]: Loading validation data...
[09/26 06:47:25 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 06:47:25 visual_prompt]: Number of images: 200
[09/26 06:47:25 visual_prompt]: Number of classes: 47 / 47
[09/26 06:47:25 visual_prompt]: Constructing models...
[09/26 06:47:28 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 06:47:28 visual_prompt]: tuned percent:0.576
[09/26 06:47:28 visual_prompt]: Device used for model: 0
[09/26 06:47:28 visual_prompt]: Setting up Evaluator...
[09/26 06:47:28 visual_prompt]: Setting up Trainer...
[09/26 06:47:28 visual_prompt]: 	Setting up the optimizer...
[09/26 06:47:28 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 06:47:35 visual_prompt]: Epoch 1 / 100: avg data time: 5.95e-02, avg batch time: 0.5050, average train loss: 3.9267
[09/26 06:47:36 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1687, average loss: 3.9045
[09/26 06:47:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 06:47:36 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 06:47:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 06:47:43 visual_prompt]: Epoch 2 / 100: avg data time: 6.03e-02, avg batch time: 0.5065, average train loss: 3.8975
[09/26 06:47:45 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1686, average loss: 3.8626
[09/26 06:47:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 18.50	
[09/26 06:47:45 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 06:47:52 visual_prompt]: Epoch 3 / 100: avg data time: 5.68e-02, avg batch time: 0.5042, average train loss: 3.8217
[09/26 06:47:53 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1688, average loss: 3.8698
[09/26 06:47:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 9.50	
[09/26 06:47:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 06:48:00 visual_prompt]: Epoch 4 / 100: avg data time: 6.38e-02, avg batch time: 0.5115, average train loss: 3.7353
[09/26 06:48:02 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1689, average loss: 3.7511
[09/26 06:48:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 5.00	top5: 20.50	
[09/26 06:48:02 visual_prompt]: Best epoch 4: best metric: 0.050
[09/26 06:48:02 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 06:48:09 visual_prompt]: Epoch 5 / 100: avg data time: 5.72e-02, avg batch time: 0.5051, average train loss: 3.5463
[09/26 06:48:10 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1690, average loss: 3.5373
[09/26 06:48:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 8.00	top5: 32.50	
[09/26 06:48:10 visual_prompt]: Best epoch 5: best metric: 0.080
[09/26 06:48:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 06:48:17 visual_prompt]: Epoch 6 / 100: avg data time: 6.04e-02, avg batch time: 0.5088, average train loss: 3.2165
[09/26 06:48:19 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1691, average loss: 3.3099
[09/26 06:48:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 13.00	top5: 39.00	
[09/26 06:48:19 visual_prompt]: Best epoch 6: best metric: 0.130
[09/26 06:48:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 06:48:26 visual_prompt]: Epoch 7 / 100: avg data time: 6.10e-02, avg batch time: 0.5079, average train loss: 2.9103
[09/26 06:48:27 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1688, average loss: 3.2392
[09/26 06:48:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 17.50	top5: 42.00	
[09/26 06:48:27 visual_prompt]: Best epoch 7: best metric: 0.175
[09/26 06:48:27 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 06:48:34 visual_prompt]: Epoch 8 / 100: avg data time: 5.71e-02, avg batch time: 0.5043, average train loss: 2.6060
[09/26 06:48:36 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1689, average loss: 2.9265
[09/26 06:48:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 23.00	top5: 53.00	
[09/26 06:48:36 visual_prompt]: Best epoch 8: best metric: 0.230
[09/26 06:48:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 06:48:43 visual_prompt]: Epoch 9 / 100: avg data time: 6.08e-02, avg batch time: 0.5087, average train loss: 2.1245
[09/26 06:48:44 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 2.8143
[09/26 06:48:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.50	top5: 56.50	
[09/26 06:48:44 visual_prompt]: Best epoch 9: best metric: 0.265
[09/26 06:48:44 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 06:48:51 visual_prompt]: Epoch 10 / 100: avg data time: 5.39e-02, avg batch time: 0.5024, average train loss: 1.6852
[09/26 06:48:53 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1687, average loss: 2.6595
[09/26 06:48:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 28.00	top5: 63.00	
[09/26 06:48:53 visual_prompt]: Best epoch 10: best metric: 0.280
[09/26 06:48:53 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 06:48:59 visual_prompt]: Epoch 11 / 100: avg data time: 5.55e-02, avg batch time: 0.5031, average train loss: 1.3624
[09/26 06:49:01 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1690, average loss: 2.5758
[09/26 06:49:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 29.50	top5: 67.00	
[09/26 06:49:01 visual_prompt]: Best epoch 11: best metric: 0.295
[09/26 06:49:01 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 06:49:08 visual_prompt]: Epoch 12 / 100: avg data time: 4.60e-02, avg batch time: 0.4956, average train loss: 1.1875
[09/26 06:49:09 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1693, average loss: 2.2731
[09/26 06:49:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.50	top5: 71.50	
[09/26 06:49:09 visual_prompt]: Best epoch 12: best metric: 0.385
[09/26 06:49:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 06:49:16 visual_prompt]: Epoch 13 / 100: avg data time: 5.71e-02, avg batch time: 0.5046, average train loss: 0.8454
[09/26 06:49:18 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1685, average loss: 2.3542
[09/26 06:49:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.00	top5: 71.00	
[09/26 06:49:18 visual_prompt]: Best epoch 13: best metric: 0.410
[09/26 06:49:18 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 06:49:24 visual_prompt]: Epoch 14 / 100: avg data time: 5.21e-02, avg batch time: 0.4997, average train loss: 0.6565
[09/26 06:49:26 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1691, average loss: 2.3609
[09/26 06:49:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.50	top5: 73.00	
[09/26 06:49:26 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 06:49:33 visual_prompt]: Epoch 15 / 100: avg data time: 5.97e-02, avg batch time: 0.5082, average train loss: 0.4695
[09/26 06:49:35 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1687, average loss: 2.3136
[09/26 06:49:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.00	top5: 73.50	
[09/26 06:49:35 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 06:49:41 visual_prompt]: Epoch 16 / 100: avg data time: 5.25e-02, avg batch time: 0.5000, average train loss: 0.3302
[09/26 06:49:43 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1691, average loss: 2.2536
[09/26 06:49:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 74.00	
[09/26 06:49:43 visual_prompt]: Best epoch 16: best metric: 0.430
[09/26 06:49:43 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 06:49:50 visual_prompt]: Epoch 17 / 100: avg data time: 5.86e-02, avg batch time: 0.5068, average train loss: 0.2039
[09/26 06:49:51 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1690, average loss: 2.2676
[09/26 06:49:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 77.00	
[09/26 06:49:51 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 06:49:58 visual_prompt]: Epoch 18 / 100: avg data time: 6.06e-02, avg batch time: 0.5083, average train loss: 0.1473
[09/26 06:50:00 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 2.2944
[09/26 06:50:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 77.00	
[09/26 06:50:00 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 06:50:07 visual_prompt]: Epoch 19 / 100: avg data time: 4.28e-02, avg batch time: 0.4908, average train loss: 0.1175
[09/26 06:50:08 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 2.4577
[09/26 06:50:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 36.00	top5: 75.00	
[09/26 06:50:08 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 06:50:15 visual_prompt]: Epoch 20 / 100: avg data time: 4.45e-02, avg batch time: 0.4941, average train loss: 0.0960
[09/26 06:50:16 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1691, average loss: 2.2581
[09/26 06:50:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 77.00	
[09/26 06:50:16 visual_prompt]: Best epoch 20: best metric: 0.460
[09/26 06:50:16 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 06:50:23 visual_prompt]: Epoch 21 / 100: avg data time: 5.18e-02, avg batch time: 0.5005, average train loss: 0.0680
[09/26 06:50:25 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 2.3253
[09/26 06:50:25 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.00	top5: 76.50	
[09/26 06:50:25 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 06:50:32 visual_prompt]: Epoch 22 / 100: avg data time: 5.61e-02, avg batch time: 0.5040, average train loss: 0.0631
[09/26 06:50:34 visual_prompt]: Inference (val):avg data time: 7.25e-05, avg batch time: 0.1710, average loss: 2.3180
[09/26 06:50:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.00	top5: 78.00	
[09/26 06:50:34 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 06:50:40 visual_prompt]: Epoch 23 / 100: avg data time: 5.30e-02, avg batch time: 0.5016, average train loss: 0.0497
[09/26 06:50:42 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1692, average loss: 2.3495
[09/26 06:50:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.00	top5: 78.50	
[09/26 06:50:42 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 06:50:49 visual_prompt]: Epoch 24 / 100: avg data time: 5.76e-02, avg batch time: 0.5050, average train loss: 0.0413
[09/26 06:50:50 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1690, average loss: 2.3516
[09/26 06:50:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 78.00	
[09/26 06:50:50 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 06:50:57 visual_prompt]: Epoch 25 / 100: avg data time: 5.08e-02, avg batch time: 0.5002, average train loss: 0.0359
[09/26 06:50:59 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1688, average loss: 2.3466
[09/26 06:50:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 77.50	
[09/26 06:50:59 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 06:51:05 visual_prompt]: Epoch 26 / 100: avg data time: 4.56e-02, avg batch time: 0.4933, average train loss: 0.0294
[09/26 06:51:07 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1686, average loss: 2.4078
[09/26 06:51:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.50	top5: 78.00	
[09/26 06:51:07 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 06:51:14 visual_prompt]: Epoch 27 / 100: avg data time: 4.45e-02, avg batch time: 0.4943, average train loss: 0.0259
[09/26 06:51:15 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1693, average loss: 2.4154
[09/26 06:51:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 77.50	
[09/26 06:51:15 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 06:51:22 visual_prompt]: Epoch 28 / 100: avg data time: 5.64e-02, avg batch time: 0.5054, average train loss: 0.0255
[09/26 06:51:24 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1693, average loss: 2.4407
[09/26 06:51:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.00	top5: 79.50	
[09/26 06:51:24 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 06:51:31 visual_prompt]: Epoch 29 / 100: avg data time: 6.25e-02, avg batch time: 0.5105, average train loss: 0.0219
[09/26 06:51:32 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1689, average loss: 2.4251
[09/26 06:51:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 78.00	
[09/26 06:51:32 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 06:51:39 visual_prompt]: Epoch 30 / 100: avg data time: 4.65e-02, avg batch time: 0.4956, average train loss: 0.0217
[09/26 06:51:41 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 2.4307
[09/26 06:51:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 78.50	
[09/26 06:51:41 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 06:51:48 visual_prompt]: Epoch 31 / 100: avg data time: 6.14e-02, avg batch time: 0.5089, average train loss: 0.0214
[09/26 06:51:49 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1691, average loss: 2.4197
[09/26 06:51:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 40.00	top5: 79.50	
[09/26 06:51:49 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 06:51:56 visual_prompt]: Epoch 32 / 100: avg data time: 4.57e-02, avg batch time: 0.4946, average train loss: 0.0185
[09/26 06:51:57 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1687, average loss: 2.3959
[09/26 06:51:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 79.00	
[09/26 06:51:57 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 06:52:04 visual_prompt]: Epoch 33 / 100: avg data time: 5.91e-02, avg batch time: 0.5070, average train loss: 0.0176
[09/26 06:52:06 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1691, average loss: 2.4103
[09/26 06:52:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 78.50	
[09/26 06:52:06 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 06:52:13 visual_prompt]: Epoch 34 / 100: avg data time: 5.16e-02, avg batch time: 0.5003, average train loss: 0.0170
[09/26 06:52:14 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1694, average loss: 2.4491
[09/26 06:52:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 76.50	
[09/26 06:52:14 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 06:52:21 visual_prompt]: Epoch 35 / 100: avg data time: 4.38e-02, avg batch time: 0.4938, average train loss: 0.0160
[09/26 06:52:22 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1690, average loss: 2.4613
[09/26 06:52:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 77.00	
[09/26 06:52:22 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 06:52:29 visual_prompt]: Epoch 36 / 100: avg data time: 5.95e-02, avg batch time: 0.5071, average train loss: 0.0153
[09/26 06:52:31 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1693, average loss: 2.4681
[09/26 06:52:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 78.50	
[09/26 06:52:31 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 06:52:38 visual_prompt]: Epoch 37 / 100: avg data time: 4.76e-02, avg batch time: 0.4973, average train loss: 0.0156
[09/26 06:52:39 visual_prompt]: Inference (val):avg data time: 5.13e-05, avg batch time: 0.1690, average loss: 2.4566
[09/26 06:52:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 78.50	
[09/26 06:52:39 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 06:52:46 visual_prompt]: Epoch 38 / 100: avg data time: 6.90e-02, avg batch time: 0.5169, average train loss: 0.0143
[09/26 06:52:48 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 2.4662
[09/26 06:52:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 78.50	
[09/26 06:52:48 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 06:52:55 visual_prompt]: Epoch 39 / 100: avg data time: 6.74e-02, avg batch time: 0.5171, average train loss: 0.0138
[09/26 06:52:56 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1693, average loss: 2.4650
[09/26 06:52:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 78.00	
[09/26 06:52:56 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 06:53:03 visual_prompt]: Epoch 40 / 100: avg data time: 5.83e-02, avg batch time: 0.5063, average train loss: 0.0143
[09/26 06:53:05 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1690, average loss: 2.4718
[09/26 06:53:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 78.50	
[09/26 06:53:05 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 06:53:12 visual_prompt]: Epoch 41 / 100: avg data time: 5.80e-02, avg batch time: 0.5056, average train loss: 0.0132
[09/26 06:53:13 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1692, average loss: 2.4770
[09/26 06:53:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 78.50	
[09/26 06:53:13 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 06:53:20 visual_prompt]: Epoch 42 / 100: avg data time: 5.52e-02, avg batch time: 0.5043, average train loss: 0.0127
[09/26 06:53:22 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1690, average loss: 2.4817
[09/26 06:53:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 79.00	
[09/26 06:53:22 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 06:53:29 visual_prompt]: Epoch 43 / 100: avg data time: 5.17e-02, avg batch time: 0.4998, average train loss: 0.0118
[09/26 06:53:30 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1693, average loss: 2.4976
[09/26 06:53:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 79.00	
[09/26 06:53:30 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 06:53:37 visual_prompt]: Epoch 44 / 100: avg data time: 5.29e-02, avg batch time: 0.5018, average train loss: 0.0121
[09/26 06:53:39 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1689, average loss: 2.4971
[09/26 06:53:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 78.50	
[09/26 06:53:39 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 06:53:46 visual_prompt]: Epoch 45 / 100: avg data time: 5.82e-02, avg batch time: 0.5069, average train loss: 0.0106
[09/26 06:53:47 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1693, average loss: 2.4804
[09/26 06:53:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 78.50	
[09/26 06:53:47 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 06:53:54 visual_prompt]: Epoch 46 / 100: avg data time: 6.21e-02, avg batch time: 0.5108, average train loss: 0.0113
[09/26 06:53:56 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1693, average loss: 2.4736
[09/26 06:53:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 78.00	
[09/26 06:53:56 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 06:54:03 visual_prompt]: Epoch 47 / 100: avg data time: 6.09e-02, avg batch time: 0.5090, average train loss: 0.0105
[09/26 06:54:04 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1693, average loss: 2.4899
[09/26 06:54:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 78.50	
[09/26 06:54:04 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 06:54:11 visual_prompt]: Epoch 48 / 100: avg data time: 5.38e-02, avg batch time: 0.5025, average train loss: 0.0107
[09/26 06:54:13 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1689, average loss: 2.5027
[09/26 06:54:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 78.50	
[09/26 06:54:13 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 06:54:19 visual_prompt]: Epoch 49 / 100: avg data time: 6.41e-02, avg batch time: 0.5120, average train loss: 0.0110
[09/26 06:54:21 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1692, average loss: 2.4996
[09/26 06:54:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.00	top5: 78.50	
[09/26 06:54:21 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 06:54:28 visual_prompt]: Epoch 50 / 100: avg data time: 5.46e-02, avg batch time: 0.5032, average train loss: 0.0102
[09/26 06:54:29 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1692, average loss: 2.4970
[09/26 06:54:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.00	top5: 78.50	
[09/26 06:54:29 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 06:54:36 visual_prompt]: Epoch 51 / 100: avg data time: 5.63e-02, avg batch time: 0.5040, average train loss: 0.0103
[09/26 06:54:38 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1692, average loss: 2.4908
[09/26 06:54:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 78.50	
[09/26 06:54:38 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 06:54:45 visual_prompt]: Epoch 52 / 100: avg data time: 6.21e-02, avg batch time: 0.5115, average train loss: 0.0097
[09/26 06:54:46 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1692, average loss: 2.5106
[09/26 06:54:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 78.50	
[09/26 06:54:46 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 06:54:53 visual_prompt]: Epoch 53 / 100: avg data time: 6.07e-02, avg batch time: 0.5085, average train loss: 0.0102
[09/26 06:54:55 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1699, average loss: 2.5248
[09/26 06:54:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.50	top5: 78.00	
[09/26 06:54:55 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 06:55:02 visual_prompt]: Epoch 54 / 100: avg data time: 6.09e-02, avg batch time: 0.5082, average train loss: 0.0096
[09/26 06:55:03 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1688, average loss: 2.5229
[09/26 06:55:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 77.50	
[09/26 06:55:03 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 06:55:10 visual_prompt]: Epoch 55 / 100: avg data time: 6.14e-02, avg batch time: 0.5105, average train loss: 0.0094
[09/26 06:55:12 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1692, average loss: 2.5248
[09/26 06:55:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 78.00	
[09/26 06:55:12 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 06:55:19 visual_prompt]: Epoch 56 / 100: avg data time: 4.62e-02, avg batch time: 0.4945, average train loss: 0.0088
[09/26 06:55:20 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1693, average loss: 2.5262
[09/26 06:55:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 78.50	
[09/26 06:55:20 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 06:55:27 visual_prompt]: Epoch 57 / 100: avg data time: 5.16e-02, avg batch time: 0.5004, average train loss: 0.0093
[09/26 06:55:28 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.1695, average loss: 2.5311
[09/26 06:55:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 78.50	
[09/26 06:55:28 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 06:55:35 visual_prompt]: Epoch 58 / 100: avg data time: 5.63e-02, avg batch time: 0.5056, average train loss: 0.0086
[09/26 06:55:37 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1689, average loss: 2.5336
[09/26 06:55:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 06:55:37 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 06:55:44 visual_prompt]: Epoch 59 / 100: avg data time: 5.38e-02, avg batch time: 0.5035, average train loss: 0.0083
[09/26 06:55:45 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1692, average loss: 2.5398
[09/26 06:55:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 78.50	
[09/26 06:55:45 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 06:55:52 visual_prompt]: Epoch 60 / 100: avg data time: 4.91e-02, avg batch time: 0.4986, average train loss: 0.0094
[09/26 06:55:54 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1688, average loss: 2.5257
[09/26 06:55:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 78.50	
[09/26 06:55:54 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 06:56:01 visual_prompt]: Epoch 61 / 100: avg data time: 5.61e-02, avg batch time: 0.5051, average train loss: 0.0087
[09/26 06:56:02 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 2.5220
[09/26 06:56:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 78.00	
[09/26 06:56:02 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 06:56:09 visual_prompt]: Epoch 62 / 100: avg data time: 4.85e-02, avg batch time: 0.4976, average train loss: 0.0084
[09/26 06:56:11 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1693, average loss: 2.5261
[09/26 06:56:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 79.00	
[09/26 06:56:11 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 06:56:17 visual_prompt]: Epoch 63 / 100: avg data time: 5.37e-02, avg batch time: 0.5036, average train loss: 0.0086
[09/26 06:56:19 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1689, average loss: 2.5303
[09/26 06:56:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 79.00	
[09/26 06:56:19 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 06:56:26 visual_prompt]: Epoch 64 / 100: avg data time: 5.98e-02, avg batch time: 0.5085, average train loss: 0.0085
[09/26 06:56:28 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1690, average loss: 2.5291
[09/26 06:56:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 79.00	
[09/26 06:56:28 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 06:56:34 visual_prompt]: Epoch 65 / 100: avg data time: 5.73e-02, avg batch time: 0.5057, average train loss: 0.0083
[09/26 06:56:36 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1693, average loss: 2.5324
[09/26 06:56:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 79.50	
[09/26 06:56:36 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 06:56:43 visual_prompt]: Epoch 66 / 100: avg data time: 5.53e-02, avg batch time: 0.5043, average train loss: 0.0086
[09/26 06:56:44 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1691, average loss: 2.5257
[09/26 06:56:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 79.00	
[09/26 06:56:44 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 06:56:51 visual_prompt]: Epoch 67 / 100: avg data time: 5.53e-02, avg batch time: 0.5033, average train loss: 0.0085
[09/26 06:56:53 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1692, average loss: 2.5190
[09/26 06:56:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.50	top5: 79.00	
[09/26 06:56:53 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 06:57:00 visual_prompt]: Epoch 68 / 100: avg data time: 6.26e-02, avg batch time: 0.5105, average train loss: 0.0082
[09/26 06:57:01 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1688, average loss: 2.5203
[09/26 06:57:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.50	
[09/26 06:57:01 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 06:57:08 visual_prompt]: Epoch 69 / 100: avg data time: 5.35e-02, avg batch time: 0.5023, average train loss: 0.0079
[09/26 06:57:10 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1692, average loss: 2.5279
[09/26 06:57:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 79.50	
[09/26 06:57:10 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 06:57:17 visual_prompt]: Epoch 70 / 100: avg data time: 5.83e-02, avg batch time: 0.5070, average train loss: 0.0081
[09/26 06:57:18 visual_prompt]: Inference (val):avg data time: 4.94e-05, avg batch time: 0.1692, average loss: 2.5342
[09/26 06:57:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 79.50	
[09/26 06:57:18 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 06:57:25 visual_prompt]: Epoch 71 / 100: avg data time: 6.09e-02, avg batch time: 0.5085, average train loss: 0.0078
[09/26 06:57:27 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1689, average loss: 2.5346
[09/26 06:57:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 79.50	
[09/26 06:57:27 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 06:57:34 visual_prompt]: Epoch 72 / 100: avg data time: 6.11e-02, avg batch time: 0.5096, average train loss: 0.0081
[09/26 06:57:35 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1692, average loss: 2.5306
[09/26 06:57:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 79.00	
[09/26 06:57:35 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 06:57:42 visual_prompt]: Epoch 73 / 100: avg data time: 5.90e-02, avg batch time: 0.5095, average train loss: 0.0078
[09/26 06:57:44 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1690, average loss: 2.5275
[09/26 06:57:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.00	top5: 79.50	
[09/26 06:57:44 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 06:57:51 visual_prompt]: Epoch 74 / 100: avg data time: 6.17e-02, avg batch time: 0.5093, average train loss: 0.0079
[09/26 06:57:52 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1692, average loss: 2.5247
[09/26 06:57:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 79.00	
[09/26 06:57:52 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 06:57:59 visual_prompt]: Epoch 75 / 100: avg data time: 5.66e-02, avg batch time: 0.5053, average train loss: 0.0080
[09/26 06:58:01 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1690, average loss: 2.5263
[09/26 06:58:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 79.50	
[09/26 06:58:01 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 06:58:07 visual_prompt]: Epoch 76 / 100: avg data time: 5.48e-02, avg batch time: 0.5038, average train loss: 0.0074
[09/26 06:58:09 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1691, average loss: 2.5293
[09/26 06:58:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 79.50	
[09/26 06:58:09 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 06:58:16 visual_prompt]: Epoch 77 / 100: avg data time: 5.71e-02, avg batch time: 0.5049, average train loss: 0.0078
[09/26 06:58:17 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1692, average loss: 2.5311
[09/26 06:58:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.50	
[09/26 06:58:17 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 06:58:24 visual_prompt]: Epoch 78 / 100: avg data time: 4.98e-02, avg batch time: 0.4987, average train loss: 0.0078
[09/26 06:58:26 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1691, average loss: 2.5364
[09/26 06:58:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.50	
[09/26 06:58:26 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 06:58:33 visual_prompt]: Epoch 79 / 100: avg data time: 5.48e-02, avg batch time: 0.5041, average train loss: 0.0075
[09/26 06:58:34 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1688, average loss: 2.5409
[09/26 06:58:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.50	
[09/26 06:58:34 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 06:58:41 visual_prompt]: Epoch 80 / 100: avg data time: 5.39e-02, avg batch time: 0.5019, average train loss: 0.0079
[09/26 06:58:43 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1691, average loss: 2.5417
[09/26 06:58:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.50	
[09/26 06:58:43 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 06:58:49 visual_prompt]: Epoch 81 / 100: avg data time: 5.46e-02, avg batch time: 0.5023, average train loss: 0.0080
[09/26 06:58:51 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 2.5410
[09/26 06:58:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.50	
[09/26 06:58:51 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 06:58:58 visual_prompt]: Epoch 82 / 100: avg data time: 4.39e-02, avg batch time: 0.4945, average train loss: 0.0074
[09/26 06:58:59 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1691, average loss: 2.5422
[09/26 06:58:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.50	
[09/26 06:58:59 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 06:59:06 visual_prompt]: Epoch 83 / 100: avg data time: 5.01e-02, avg batch time: 0.5009, average train loss: 0.0079
[09/26 06:59:08 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1691, average loss: 2.5432
[09/26 06:59:08 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.50	
[09/26 06:59:08 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 06:59:14 visual_prompt]: Epoch 84 / 100: avg data time: 4.37e-02, avg batch time: 0.4934, average train loss: 0.0075
[09/26 06:59:16 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 2.5434
[09/26 06:59:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 79.50	
[09/26 06:59:16 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 06:59:23 visual_prompt]: Epoch 85 / 100: avg data time: 5.27e-02, avg batch time: 0.5009, average train loss: 0.0075
[09/26 06:59:24 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1692, average loss: 2.5420
[09/26 06:59:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 42.50	top5: 79.50	
[09/26 06:59:24 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 06:59:31 visual_prompt]: Epoch 86 / 100: avg data time: 5.09e-02, avg batch time: 0.4997, average train loss: 0.0075
[09/26 06:59:33 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1690, average loss: 2.5422
[09/26 06:59:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.50	
[09/26 06:59:33 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 06:59:39 visual_prompt]: Epoch 87 / 100: avg data time: 4.11e-02, avg batch time: 0.4911, average train loss: 0.0081
[09/26 06:59:41 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1693, average loss: 2.5424
[09/26 06:59:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.50	
[09/26 06:59:41 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 06:59:48 visual_prompt]: Epoch 88 / 100: avg data time: 5.58e-02, avg batch time: 0.5039, average train loss: 0.0082
[09/26 06:59:49 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1689, average loss: 2.5437
[09/26 06:59:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.50	
[09/26 06:59:49 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 06:59:56 visual_prompt]: Epoch 89 / 100: avg data time: 4.92e-02, avg batch time: 0.4989, average train loss: 0.0073
[09/26 06:59:58 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1690, average loss: 2.5446
[09/26 06:59:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 06:59:58 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 07:00:04 visual_prompt]: Epoch 90 / 100: avg data time: 4.49e-02, avg batch time: 0.4947, average train loss: 0.0077
[09/26 07:00:06 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1690, average loss: 2.5449
[09/26 07:00:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 07:00:06 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 07:00:13 visual_prompt]: Epoch 91 / 100: avg data time: 6.01e-02, avg batch time: 0.5077, average train loss: 0.0078
[09/26 07:00:14 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1691, average loss: 2.5448
[09/26 07:00:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 07:00:14 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 07:00:21 visual_prompt]: Epoch 92 / 100: avg data time: 5.53e-02, avg batch time: 0.5031, average train loss: 0.0071
[09/26 07:00:23 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 2.5445
[09/26 07:00:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 07:00:23 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 07:00:30 visual_prompt]: Epoch 93 / 100: avg data time: 5.61e-02, avg batch time: 0.5036, average train loss: 0.0074
[09/26 07:00:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1691, average loss: 2.5448
[09/26 07:00:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 07:00:31 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 07:00:38 visual_prompt]: Epoch 94 / 100: avg data time: 5.92e-02, avg batch time: 0.5073, average train loss: 0.0071
[09/26 07:00:40 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1689, average loss: 2.5450
[09/26 07:00:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 07:00:40 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 07:00:47 visual_prompt]: Epoch 95 / 100: avg data time: 5.17e-02, avg batch time: 0.5000, average train loss: 0.0078
[09/26 07:00:48 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1691, average loss: 2.5452
[09/26 07:00:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 07:00:48 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 07:00:55 visual_prompt]: Epoch 96 / 100: avg data time: 6.17e-02, avg batch time: 0.5092, average train loss: 0.0075
[09/26 07:00:57 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1688, average loss: 2.5454
[09/26 07:00:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 07:00:57 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 07:01:03 visual_prompt]: Epoch 97 / 100: avg data time: 5.56e-02, avg batch time: 0.5044, average train loss: 0.0079
[09/26 07:01:05 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1691, average loss: 2.5454
[09/26 07:01:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 07:01:05 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 07:01:12 visual_prompt]: Epoch 98 / 100: avg data time: 4.51e-02, avg batch time: 0.4940, average train loss: 0.0075
[09/26 07:01:13 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1685, average loss: 2.5454
[09/26 07:01:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 07:01:13 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 07:01:20 visual_prompt]: Epoch 99 / 100: avg data time: 6.64e-02, avg batch time: 0.5154, average train loss: 0.0076
[09/26 07:01:22 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1690, average loss: 2.5454
[09/26 07:01:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 07:01:22 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 07:01:29 visual_prompt]: Epoch 100 / 100: avg data time: 6.01e-02, avg batch time: 0.5078, average train loss: 0.0073
[09/26 07:01:30 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1691, average loss: 2.5454
[09/26 07:01:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 43.00	top5: 79.00	
[09/26 07:01:30 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 07:01:30 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 07:01:30 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dtd', 'DATA.NUMBER_CLASSES', '47', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 07:01:30 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 07:01:30 visual_prompt]: Training with config:
[09/26 07:01:30 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dtd/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dtd', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 47, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 07:01:30 visual_prompt]: Loading training data...
[09/26 07:01:30 visual_prompt]: Constructing vtab-dtd dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split train[:800], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 07:01:33 visual_prompt]: Number of images: 800
[09/26 07:01:33 visual_prompt]: Number of classes: 47 / 47
[09/26 07:01:33 visual_prompt]: Loading validation data...
[09/26 07:01:33 visual_prompt]: Constructing vtab-dtd dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dtd/3.0.1
[INFO: dataset_builder.py:  573]: Reusing dataset dtd (visual_prompt_tuning/data_path/dtd/3.0.1)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dtd for split validation[:200], from visual_prompt_tuning/data_path/dtd/3.0.1
[09/26 07:01:33 visual_prompt]: Number of images: 200
[09/26 07:01:33 visual_prompt]: Number of classes: 47 / 47
[09/26 07:01:33 visual_prompt]: Constructing models...
[09/26 07:01:36 visual_prompt]: Total Parameters: 86295599	 Gradient Parameters: 496943
[09/26 07:01:36 visual_prompt]: tuned percent:0.576
[09/26 07:01:36 visual_prompt]: Device used for model: 0
[09/26 07:01:36 visual_prompt]: Setting up Evaluator...
[09/26 07:01:36 visual_prompt]: Setting up Trainer...
[09/26 07:01:36 visual_prompt]: 	Setting up the optimizer...
[09/26 07:01:36 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 07:01:43 visual_prompt]: Epoch 1 / 100: avg data time: 5.68e-02, avg batch time: 0.5041, average train loss: 3.9380
[09/26 07:01:44 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1686, average loss: 3.9045
[09/26 07:01:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.00	top5: 13.00	
[09/26 07:01:44 visual_prompt]: Best epoch 1: best metric: 0.040
[09/26 07:01:44 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 07:01:51 visual_prompt]: Epoch 2 / 100: avg data time: 5.01e-02, avg batch time: 0.4973, average train loss: 3.8925
[09/26 07:01:52 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1685, average loss: 3.8757
[09/26 07:01:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.00	top5: 13.00	
[09/26 07:01:52 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 07:01:59 visual_prompt]: Epoch 3 / 100: avg data time: 4.70e-02, avg batch time: 0.4948, average train loss: 3.8372
[09/26 07:02:01 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1687, average loss: 3.8527
[09/26 07:02:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 3.50	top5: 15.50	
[09/26 07:02:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 07:02:08 visual_prompt]: Epoch 4 / 100: avg data time: 4.97e-02, avg batch time: 0.4969, average train loss: 3.7501
[09/26 07:02:09 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1688, average loss: 3.7838
[09/26 07:02:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 7.00	top5: 20.50	
[09/26 07:02:09 visual_prompt]: Best epoch 4: best metric: 0.070
[09/26 07:02:09 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 07:02:16 visual_prompt]: Epoch 5 / 100: avg data time: 4.71e-02, avg batch time: 0.4951, average train loss: 3.6141
[09/26 07:02:17 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1688, average loss: 3.6560
[09/26 07:02:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 4.50	top5: 29.00	
[09/26 07:02:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 07:02:24 visual_prompt]: Epoch 6 / 100: avg data time: 5.11e-02, avg batch time: 0.4985, average train loss: 3.3568
[09/26 07:02:26 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1689, average loss: 3.4901
[09/26 07:02:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 9.50	top5: 36.50	
[09/26 07:02:26 visual_prompt]: Best epoch 6: best metric: 0.095
[09/26 07:02:26 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 07:02:32 visual_prompt]: Epoch 7 / 100: avg data time: 5.26e-02, avg batch time: 0.5000, average train loss: 3.0130
[09/26 07:02:34 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1689, average loss: 3.1835
[09/26 07:02:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 16.00	top5: 44.50	
[09/26 07:02:34 visual_prompt]: Best epoch 7: best metric: 0.160
[09/26 07:02:34 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 07:02:41 visual_prompt]: Epoch 8 / 100: avg data time: 5.49e-02, avg batch time: 0.5022, average train loss: 2.6353
[09/26 07:02:42 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1691, average loss: 2.8926
[09/26 07:02:42 visual_prompt]: Classification results with val_vtab-dtd: top1: 24.00	top5: 53.00	
[09/26 07:02:42 visual_prompt]: Best epoch 8: best metric: 0.240
[09/26 07:02:42 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 07:02:49 visual_prompt]: Epoch 9 / 100: avg data time: 4.67e-02, avg batch time: 0.4964, average train loss: 2.2754
[09/26 07:02:51 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1690, average loss: 2.8100
[09/26 07:02:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 26.50	top5: 58.50	
[09/26 07:02:51 visual_prompt]: Best epoch 9: best metric: 0.265
[09/26 07:02:51 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 07:02:58 visual_prompt]: Epoch 10 / 100: avg data time: 5.20e-02, avg batch time: 0.4999, average train loss: 1.8940
[09/26 07:02:59 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1688, average loss: 2.6156
[09/26 07:02:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 29.00	top5: 62.50	
[09/26 07:02:59 visual_prompt]: Best epoch 10: best metric: 0.290
[09/26 07:02:59 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 07:03:06 visual_prompt]: Epoch 11 / 100: avg data time: 4.80e-02, avg batch time: 0.4968, average train loss: 1.5659
[09/26 07:03:07 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1690, average loss: 2.5986
[09/26 07:03:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 30.50	top5: 67.50	
[09/26 07:03:07 visual_prompt]: Best epoch 11: best metric: 0.305
[09/26 07:03:07 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 07:03:14 visual_prompt]: Epoch 12 / 100: avg data time: 4.67e-02, avg batch time: 0.4971, average train loss: 1.2284
[09/26 07:03:16 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1693, average loss: 2.4961
[09/26 07:03:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 32.50	top5: 69.00	
[09/26 07:03:16 visual_prompt]: Best epoch 12: best metric: 0.325
[09/26 07:03:16 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 07:03:23 visual_prompt]: Epoch 13 / 100: avg data time: 4.42e-02, avg batch time: 0.4921, average train loss: 0.8920
[09/26 07:03:24 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1689, average loss: 2.4342
[09/26 07:03:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 38.00	top5: 72.00	
[09/26 07:03:24 visual_prompt]: Best epoch 13: best metric: 0.380
[09/26 07:03:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 07:03:31 visual_prompt]: Epoch 14 / 100: avg data time: 5.97e-02, avg batch time: 0.5071, average train loss: 0.6871
[09/26 07:03:33 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 2.1848
[09/26 07:03:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 41.00	top5: 75.50	
[09/26 07:03:33 visual_prompt]: Best epoch 14: best metric: 0.410
[09/26 07:03:33 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 07:03:40 visual_prompt]: Epoch 15 / 100: avg data time: 6.07e-02, avg batch time: 0.5083, average train loss: 0.4794
[09/26 07:03:41 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1693, average loss: 2.3318
[09/26 07:03:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 39.00	top5: 74.00	
[09/26 07:03:41 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 07:03:48 visual_prompt]: Epoch 16 / 100: avg data time: 6.04e-02, avg batch time: 0.5095, average train loss: 0.3289
[09/26 07:03:50 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1698, average loss: 2.2222
[09/26 07:03:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 75.00	
[09/26 07:03:50 visual_prompt]: Best epoch 16: best metric: 0.460
[09/26 07:03:50 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 07:03:57 visual_prompt]: Epoch 17 / 100: avg data time: 5.93e-02, avg batch time: 0.5066, average train loss: 0.2166
[09/26 07:03:58 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.1694, average loss: 2.2120
[09/26 07:03:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 73.00	
[09/26 07:03:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 07:04:05 visual_prompt]: Epoch 18 / 100: avg data time: 4.79e-02, avg batch time: 0.4958, average train loss: 0.1520
[09/26 07:04:06 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1692, average loss: 2.2743
[09/26 07:04:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 44.50	top5: 75.50	
[09/26 07:04:06 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 07:04:13 visual_prompt]: Epoch 19 / 100: avg data time: 6.08e-02, avg batch time: 0.5083, average train loss: 0.1102
[09/26 07:04:15 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1688, average loss: 2.2528
[09/26 07:04:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 73.50	
[09/26 07:04:15 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 07:04:22 visual_prompt]: Epoch 20 / 100: avg data time: 5.65e-02, avg batch time: 0.5036, average train loss: 0.0866
[09/26 07:04:23 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1690, average loss: 2.2815
[09/26 07:04:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 78.00	
[09/26 07:04:23 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 07:04:30 visual_prompt]: Epoch 21 / 100: avg data time: 5.54e-02, avg batch time: 0.5041, average train loss: 0.0663
[09/26 07:04:32 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1693, average loss: 2.3179
[09/26 07:04:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.50	top5: 76.00	
[09/26 07:04:32 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 07:04:39 visual_prompt]: Epoch 22 / 100: avg data time: 6.41e-02, avg batch time: 0.5127, average train loss: 0.0573
[09/26 07:04:40 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1693, average loss: 2.2946
[09/26 07:04:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 07:04:40 visual_prompt]: Best epoch 22: best metric: 0.470
[09/26 07:04:40 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 07:04:47 visual_prompt]: Epoch 23 / 100: avg data time: 5.57e-02, avg batch time: 0.5030, average train loss: 0.0476
[09/26 07:04:49 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1689, average loss: 2.2780
[09/26 07:04:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 76.50	
[09/26 07:04:49 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 07:04:56 visual_prompt]: Epoch 24 / 100: avg data time: 4.52e-02, avg batch time: 0.4952, average train loss: 0.0382
[09/26 07:04:57 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1689, average loss: 2.2958
[09/26 07:04:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 45.00	top5: 75.50	
[09/26 07:04:57 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 07:05:04 visual_prompt]: Epoch 25 / 100: avg data time: 5.59e-02, avg batch time: 0.5038, average train loss: 0.0302
[09/26 07:05:06 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1694, average loss: 2.3268
[09/26 07:05:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 76.50	
[09/26 07:05:06 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 07:05:13 visual_prompt]: Epoch 26 / 100: avg data time: 6.39e-02, avg batch time: 0.5121, average train loss: 0.0282
[09/26 07:05:14 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1691, average loss: 2.3461
[09/26 07:05:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 07:05:14 visual_prompt]: Best epoch 26: best metric: 0.475
[09/26 07:05:14 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 07:05:21 visual_prompt]: Epoch 27 / 100: avg data time: 5.72e-02, avg batch time: 0.5062, average train loss: 0.0241
[09/26 07:05:23 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1694, average loss: 2.3271
[09/26 07:05:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 75.00	
[09/26 07:05:23 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 07:05:29 visual_prompt]: Epoch 28 / 100: avg data time: 5.68e-02, avg batch time: 0.5042, average train loss: 0.0233
[09/26 07:05:31 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1692, average loss: 2.3398
[09/26 07:05:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 07:05:31 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 07:05:38 visual_prompt]: Epoch 29 / 100: avg data time: 5.88e-02, avg batch time: 0.5070, average train loss: 0.0223
[09/26 07:05:39 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1691, average loss: 2.3554
[09/26 07:05:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:05:39 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 07:05:46 visual_prompt]: Epoch 30 / 100: avg data time: 5.70e-02, avg batch time: 0.5055, average train loss: 0.0207
[09/26 07:05:48 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1694, average loss: 2.3682
[09/26 07:05:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 74.50	
[09/26 07:05:48 visual_prompt]: Best epoch 30: best metric: 0.485
[09/26 07:05:48 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 07:05:55 visual_prompt]: Epoch 31 / 100: avg data time: 6.36e-02, avg batch time: 0.5111, average train loss: 0.0179
[09/26 07:05:57 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1685, average loss: 2.3953
[09/26 07:05:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 74.50	
[09/26 07:05:57 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 07:06:03 visual_prompt]: Epoch 32 / 100: avg data time: 5.96e-02, avg batch time: 0.5092, average train loss: 0.0173
[09/26 07:06:05 visual_prompt]: Inference (val):avg data time: 5.02e-05, avg batch time: 0.1690, average loss: 2.3889
[09/26 07:06:05 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.00	
[09/26 07:06:05 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 07:06:12 visual_prompt]: Epoch 33 / 100: avg data time: 5.86e-02, avg batch time: 0.5069, average train loss: 0.0168
[09/26 07:06:14 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1691, average loss: 2.3842
[09/26 07:06:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 74.50	
[09/26 07:06:14 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 07:06:20 visual_prompt]: Epoch 34 / 100: avg data time: 4.57e-02, avg batch time: 0.4953, average train loss: 0.0159
[09/26 07:06:22 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1691, average loss: 2.3657
[09/26 07:06:22 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 74.00	
[09/26 07:06:22 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 07:06:29 visual_prompt]: Epoch 35 / 100: avg data time: 5.96e-02, avg batch time: 0.5081, average train loss: 0.0157
[09/26 07:06:30 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1690, average loss: 2.3787
[09/26 07:06:30 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 75.50	
[09/26 07:06:30 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 07:06:37 visual_prompt]: Epoch 36 / 100: avg data time: 5.51e-02, avg batch time: 0.5030, average train loss: 0.0154
[09/26 07:06:39 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1691, average loss: 2.4022
[09/26 07:06:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.50	top5: 75.50	
[09/26 07:06:39 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 07:06:46 visual_prompt]: Epoch 37 / 100: avg data time: 5.17e-02, avg batch time: 0.5002, average train loss: 0.0145
[09/26 07:06:47 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1686, average loss: 2.3839
[09/26 07:06:47 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.00	
[09/26 07:06:47 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 07:06:54 visual_prompt]: Epoch 38 / 100: avg data time: 4.80e-02, avg batch time: 0.4969, average train loss: 0.0144
[09/26 07:06:56 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1688, average loss: 2.3852
[09/26 07:06:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 46.00	top5: 75.50	
[09/26 07:06:56 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 07:07:02 visual_prompt]: Epoch 39 / 100: avg data time: 4.79e-02, avg batch time: 0.4973, average train loss: 0.0141
[09/26 07:07:04 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1692, average loss: 2.3949
[09/26 07:07:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.00	
[09/26 07:07:04 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 07:07:11 visual_prompt]: Epoch 40 / 100: avg data time: 5.29e-02, avg batch time: 0.5022, average train loss: 0.0130
[09/26 07:07:12 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1693, average loss: 2.3867
[09/26 07:07:12 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 74.50	
[09/26 07:07:12 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 07:07:19 visual_prompt]: Epoch 41 / 100: avg data time: 4.73e-02, avg batch time: 0.4963, average train loss: 0.0122
[09/26 07:07:20 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1693, average loss: 2.3678
[09/26 07:07:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 75.00	
[09/26 07:07:20 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 07:07:27 visual_prompt]: Epoch 42 / 100: avg data time: 4.51e-02, avg batch time: 0.4949, average train loss: 0.0122
[09/26 07:07:29 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1691, average loss: 2.3855
[09/26 07:07:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:07:29 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 07:07:36 visual_prompt]: Epoch 43 / 100: avg data time: 4.54e-02, avg batch time: 0.4943, average train loss: 0.0111
[09/26 07:07:37 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1688, average loss: 2.4033
[09/26 07:07:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 76.00	
[09/26 07:07:37 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 07:07:44 visual_prompt]: Epoch 44 / 100: avg data time: 4.63e-02, avg batch time: 0.4963, average train loss: 0.0112
[09/26 07:07:45 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1696, average loss: 2.4221
[09/26 07:07:45 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.50	top5: 75.50	
[09/26 07:07:45 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 07:07:52 visual_prompt]: Epoch 45 / 100: avg data time: 5.61e-02, avg batch time: 0.5048, average train loss: 0.0110
[09/26 07:07:54 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1694, average loss: 2.4241
[09/26 07:07:54 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 74.50	
[09/26 07:07:54 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 07:08:01 visual_prompt]: Epoch 46 / 100: avg data time: 4.67e-02, avg batch time: 0.4962, average train loss: 0.0107
[09/26 07:08:02 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1691, average loss: 2.4320
[09/26 07:08:02 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 74.50	
[09/26 07:08:02 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 07:08:09 visual_prompt]: Epoch 47 / 100: avg data time: 4.37e-02, avg batch time: 0.4938, average train loss: 0.0103
[09/26 07:08:10 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1693, average loss: 2.4283
[09/26 07:08:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.50	
[09/26 07:08:10 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 07:08:17 visual_prompt]: Epoch 48 / 100: avg data time: 6.05e-02, avg batch time: 0.5093, average train loss: 0.0098
[09/26 07:08:19 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1692, average loss: 2.4177
[09/26 07:08:19 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.00	
[09/26 07:08:19 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 07:08:26 visual_prompt]: Epoch 49 / 100: avg data time: 5.04e-02, avg batch time: 0.4994, average train loss: 0.0103
[09/26 07:08:27 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1689, average loss: 2.4277
[09/26 07:08:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 76.50	
[09/26 07:08:27 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 07:08:34 visual_prompt]: Epoch 50 / 100: avg data time: 4.50e-02, avg batch time: 0.4933, average train loss: 0.0097
[09/26 07:08:36 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1694, average loss: 2.4424
[09/26 07:08:36 visual_prompt]: Classification results with val_vtab-dtd: top1: 48.00	top5: 75.50	
[09/26 07:08:36 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 07:08:43 visual_prompt]: Epoch 51 / 100: avg data time: 6.11e-02, avg batch time: 0.5088, average train loss: 0.0094
[09/26 07:08:44 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1688, average loss: 2.4368
[09/26 07:08:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:08:44 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 07:08:51 visual_prompt]: Epoch 52 / 100: avg data time: 5.70e-02, avg batch time: 0.5063, average train loss: 0.0096
[09/26 07:08:53 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1692, average loss: 2.4381
[09/26 07:08:53 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 74.50	
[09/26 07:08:53 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 07:09:00 visual_prompt]: Epoch 53 / 100: avg data time: 6.55e-02, avg batch time: 0.5133, average train loss: 0.0088
[09/26 07:09:01 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1688, average loss: 2.4394
[09/26 07:09:01 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.00	
[09/26 07:09:01 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 07:09:08 visual_prompt]: Epoch 54 / 100: avg data time: 5.98e-02, avg batch time: 0.5081, average train loss: 0.0088
[09/26 07:09:10 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1688, average loss: 2.4449
[09/26 07:09:10 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 74.50	
[09/26 07:09:10 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 07:09:17 visual_prompt]: Epoch 55 / 100: avg data time: 5.73e-02, avg batch time: 0.5061, average train loss: 0.0089
[09/26 07:09:18 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1689, average loss: 2.4486
[09/26 07:09:18 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 07:09:18 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 07:09:25 visual_prompt]: Epoch 56 / 100: avg data time: 6.46e-02, avg batch time: 0.5128, average train loss: 0.0086
[09/26 07:09:27 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1694, average loss: 2.4573
[09/26 07:09:27 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.00	
[09/26 07:09:27 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 07:09:34 visual_prompt]: Epoch 57 / 100: avg data time: 5.50e-02, avg batch time: 0.5030, average train loss: 0.0089
[09/26 07:09:35 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1693, average loss: 2.4597
[09/26 07:09:35 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 07:09:35 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 07:09:42 visual_prompt]: Epoch 58 / 100: avg data time: 5.80e-02, avg batch time: 0.5066, average train loss: 0.0081
[09/26 07:09:44 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1689, average loss: 2.4513
[09/26 07:09:44 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 76.00	
[09/26 07:09:44 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 07:09:50 visual_prompt]: Epoch 59 / 100: avg data time: 4.28e-02, avg batch time: 0.4912, average train loss: 0.0083
[09/26 07:09:52 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1694, average loss: 2.4498
[09/26 07:09:52 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 76.00	
[09/26 07:09:52 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 07:09:59 visual_prompt]: Epoch 60 / 100: avg data time: 5.05e-02, avg batch time: 0.4986, average train loss: 0.0077
[09/26 07:10:00 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1688, average loss: 2.4519
[09/26 07:10:00 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 07:10:00 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 07:10:07 visual_prompt]: Epoch 61 / 100: avg data time: 5.86e-02, avg batch time: 0.5081, average train loss: 0.0079
[09/26 07:10:09 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1690, average loss: 2.4530
[09/26 07:10:09 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 07:10:09 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 07:10:16 visual_prompt]: Epoch 62 / 100: avg data time: 5.82e-02, avg batch time: 0.5061, average train loss: 0.0080
[09/26 07:10:17 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1687, average loss: 2.4594
[09/26 07:10:17 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 07:10:17 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 07:10:24 visual_prompt]: Epoch 63 / 100: avg data time: 4.94e-02, avg batch time: 0.4987, average train loss: 0.0081
[09/26 07:10:26 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1688, average loss: 2.4618
[09/26 07:10:26 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 76.00	
[09/26 07:10:26 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 07:10:33 visual_prompt]: Epoch 64 / 100: avg data time: 4.54e-02, avg batch time: 0.4938, average train loss: 0.0079
[09/26 07:10:34 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1690, average loss: 2.4629
[09/26 07:10:34 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 07:10:34 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 07:10:41 visual_prompt]: Epoch 65 / 100: avg data time: 5.55e-02, avg batch time: 0.5043, average train loss: 0.0077
[09/26 07:10:43 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1691, average loss: 2.4622
[09/26 07:10:43 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 76.00	
[09/26 07:10:43 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 07:10:49 visual_prompt]: Epoch 66 / 100: avg data time: 4.66e-02, avg batch time: 0.4952, average train loss: 0.0077
[09/26 07:10:51 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1691, average loss: 2.4606
[09/26 07:10:51 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 07:10:51 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 07:10:58 visual_prompt]: Epoch 67 / 100: avg data time: 4.67e-02, avg batch time: 0.4963, average train loss: 0.0075
[09/26 07:10:59 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1689, average loss: 2.4591
[09/26 07:10:59 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 07:10:59 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 07:11:06 visual_prompt]: Epoch 68 / 100: avg data time: 4.42e-02, avg batch time: 0.4930, average train loss: 0.0073
[09/26 07:11:07 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1692, average loss: 2.4595
[09/26 07:11:07 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 76.00	
[09/26 07:11:07 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 07:11:14 visual_prompt]: Epoch 69 / 100: avg data time: 5.75e-02, avg batch time: 0.5077, average train loss: 0.0074
[09/26 07:11:16 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1691, average loss: 2.4643
[09/26 07:11:16 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 76.00	
[09/26 07:11:16 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 07:11:23 visual_prompt]: Epoch 70 / 100: avg data time: 6.27e-02, avg batch time: 0.5107, average train loss: 0.0077
[09/26 07:11:24 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1689, average loss: 2.4606
[09/26 07:11:24 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.00	top5: 75.50	
[09/26 07:11:24 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 07:11:31 visual_prompt]: Epoch 71 / 100: avg data time: 5.84e-02, avg batch time: 0.5075, average train loss: 0.0079
[09/26 07:11:33 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1689, average loss: 2.4605
[09/26 07:11:33 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:11:33 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 07:11:40 visual_prompt]: Epoch 72 / 100: avg data time: 4.71e-02, avg batch time: 0.4960, average train loss: 0.0072
[09/26 07:11:41 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1691, average loss: 2.4689
[09/26 07:11:41 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:11:41 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 07:11:48 visual_prompt]: Epoch 73 / 100: avg data time: 5.60e-02, avg batch time: 0.5041, average train loss: 0.0070
[09/26 07:11:50 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1694, average loss: 2.4717
[09/26 07:11:50 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 74.50	
[09/26 07:11:50 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 07:11:56 visual_prompt]: Epoch 74 / 100: avg data time: 5.09e-02, avg batch time: 0.4994, average train loss: 0.0070
[09/26 07:11:58 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1689, average loss: 2.4746
[09/26 07:11:58 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 74.50	
[09/26 07:11:58 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 07:12:05 visual_prompt]: Epoch 75 / 100: avg data time: 5.89e-02, avg batch time: 0.5067, average train loss: 0.0071
[09/26 07:12:06 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1694, average loss: 2.4776
[09/26 07:12:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 74.50	
[09/26 07:12:06 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 07:12:13 visual_prompt]: Epoch 76 / 100: avg data time: 5.93e-02, avg batch time: 0.5076, average train loss: 0.0079
[09/26 07:12:15 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1688, average loss: 2.4798
[09/26 07:12:15 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:12:15 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 07:12:22 visual_prompt]: Epoch 77 / 100: avg data time: 5.96e-02, avg batch time: 0.5077, average train loss: 0.0068
[09/26 07:12:23 visual_prompt]: Inference (val):avg data time: 4.67e-05, avg batch time: 0.1693, average loss: 2.4769
[09/26 07:12:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 07:12:23 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 07:12:30 visual_prompt]: Epoch 78 / 100: avg data time: 5.97e-02, avg batch time: 0.5087, average train loss: 0.0069
[09/26 07:12:32 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1692, average loss: 2.4772
[09/26 07:12:32 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 07:12:32 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 07:12:39 visual_prompt]: Epoch 79 / 100: avg data time: 6.24e-02, avg batch time: 0.5105, average train loss: 0.0071
[09/26 07:12:40 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1691, average loss: 2.4780
[09/26 07:12:40 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 07:12:40 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 07:12:47 visual_prompt]: Epoch 80 / 100: avg data time: 6.05e-02, avg batch time: 0.5093, average train loss: 0.0069
[09/26 07:12:49 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1691, average loss: 2.4793
[09/26 07:12:49 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:12:49 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 07:12:56 visual_prompt]: Epoch 81 / 100: avg data time: 5.59e-02, avg batch time: 0.5035, average train loss: 0.0071
[09/26 07:12:57 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.1692, average loss: 2.4783
[09/26 07:12:57 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:12:57 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 07:13:04 visual_prompt]: Epoch 82 / 100: avg data time: 5.45e-02, avg batch time: 0.5024, average train loss: 0.0067
[09/26 07:13:06 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1692, average loss: 2.4781
[09/26 07:13:06 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:13:06 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 07:13:13 visual_prompt]: Epoch 83 / 100: avg data time: 4.23e-02, avg batch time: 0.4947, average train loss: 0.0072
[09/26 07:13:14 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1690, average loss: 2.4768
[09/26 07:13:14 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:13:14 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 07:13:21 visual_prompt]: Epoch 84 / 100: avg data time: 5.87e-02, avg batch time: 0.5068, average train loss: 0.0071
[09/26 07:13:23 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1690, average loss: 2.4761
[09/26 07:13:23 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:13:23 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 07:13:29 visual_prompt]: Epoch 85 / 100: avg data time: 4.58e-02, avg batch time: 0.4966, average train loss: 0.0070
[09/26 07:13:31 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1690, average loss: 2.4753
[09/26 07:13:31 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:13:31 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 07:13:38 visual_prompt]: Epoch 86 / 100: avg data time: 5.45e-02, avg batch time: 0.5023, average train loss: 0.0068
[09/26 07:13:39 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1691, average loss: 2.4759
[09/26 07:13:39 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:13:39 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 07:13:46 visual_prompt]: Epoch 87 / 100: avg data time: 5.41e-02, avg batch time: 0.5025, average train loss: 0.0066
[09/26 07:13:48 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1689, average loss: 2.4768
[09/26 07:13:48 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:13:48 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 07:13:54 visual_prompt]: Epoch 88 / 100: avg data time: 5.34e-02, avg batch time: 0.5037, average train loss: 0.0067
[09/26 07:13:56 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1691, average loss: 2.4766
[09/26 07:13:56 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:13:56 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 07:14:03 visual_prompt]: Epoch 89 / 100: avg data time: 4.65e-02, avg batch time: 0.4952, average train loss: 0.0069
[09/26 07:14:04 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1689, average loss: 2.4766
[09/26 07:14:04 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:14:04 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 07:14:11 visual_prompt]: Epoch 90 / 100: avg data time: 4.66e-02, avg batch time: 0.4966, average train loss: 0.0068
[09/26 07:14:13 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1692, average loss: 2.4768
[09/26 07:14:13 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:14:13 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 07:14:19 visual_prompt]: Epoch 91 / 100: avg data time: 4.42e-02, avg batch time: 0.4928, average train loss: 0.0071
[09/26 07:14:21 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1687, average loss: 2.4772
[09/26 07:14:21 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:14:21 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 07:14:28 visual_prompt]: Epoch 92 / 100: avg data time: 5.39e-02, avg batch time: 0.5039, average train loss: 0.0069
[09/26 07:14:29 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1688, average loss: 2.4775
[09/26 07:14:29 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:14:29 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 07:14:36 visual_prompt]: Epoch 93 / 100: avg data time: 6.51e-02, avg batch time: 0.5127, average train loss: 0.0066
[09/26 07:14:38 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1690, average loss: 2.4774
[09/26 07:14:38 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.00	
[09/26 07:14:38 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 07:14:45 visual_prompt]: Epoch 94 / 100: avg data time: 4.77e-02, avg batch time: 0.4969, average train loss: 0.0066
[09/26 07:14:46 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1691, average loss: 2.4772
[09/26 07:14:46 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 07:14:46 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 07:14:53 visual_prompt]: Epoch 95 / 100: avg data time: 5.87e-02, avg batch time: 0.5065, average train loss: 0.0069
[09/26 07:14:55 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1689, average loss: 2.4772
[09/26 07:14:55 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 07:14:55 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 07:15:01 visual_prompt]: Epoch 96 / 100: avg data time: 5.39e-02, avg batch time: 0.5031, average train loss: 0.0067
[09/26 07:15:03 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1691, average loss: 2.4772
[09/26 07:15:03 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 07:15:03 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 07:15:10 visual_prompt]: Epoch 97 / 100: avg data time: 5.65e-02, avg batch time: 0.5056, average train loss: 0.0071
[09/26 07:15:11 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1687, average loss: 2.4772
[09/26 07:15:11 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 07:15:11 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 07:15:18 visual_prompt]: Epoch 98 / 100: avg data time: 4.69e-02, avg batch time: 0.4977, average train loss: 0.0066
[09/26 07:15:20 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1694, average loss: 2.4772
[09/26 07:15:20 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 07:15:20 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 07:15:27 visual_prompt]: Epoch 99 / 100: avg data time: 5.87e-02, avg batch time: 0.5078, average train loss: 0.0067
[09/26 07:15:28 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1692, average loss: 2.4772
[09/26 07:15:28 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
[09/26 07:15:28 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 07:15:35 visual_prompt]: Epoch 100 / 100: avg data time: 5.95e-02, avg batch time: 0.5079, average train loss: 0.0066
[09/26 07:15:37 visual_prompt]: Inference (val):avg data time: 5.28e-05, avg batch time: 0.1696, average loss: 2.4772
[09/26 07:15:37 visual_prompt]: Classification results with val_vtab-dtd: top1: 47.50	top5: 75.50	
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 289, in <module>
    main(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 284, in main
    train(cfg, args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 113, in train
    seed(cfg)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 136, in seed
    torch.manual_seed(SEED)
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/random.py", line 36, in manual_seed
    seed = int(seed)
           ^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'tuple'

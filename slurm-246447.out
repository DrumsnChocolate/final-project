/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/07 14:54:19 visual_prompt]: Rank of current process: 0. World size: 1
[11/07 14:54:24 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/07 14:54:24 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/07 14:54:24 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/07 14:54:24 visual_prompt]: Training with config:
[11/07 14:54:24 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.005_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/07 14:54:24 visual_prompt]: Loading training data...
[11/07 14:54:24 visual_prompt]: Constructing mammo-cbis dataset train...
[11/07 14:54:24 visual_prompt]: Loading validation data...
[11/07 14:54:24 visual_prompt]: Constructing mammo-cbis dataset val...
[11/07 14:54:24 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/07 14:54:31 visual_prompt]: Enable all parameters update during training
[11/07 14:54:31 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/07 14:54:31 visual_prompt]: tuned percent:100.000
[11/07 14:54:31 visual_prompt]: Device used for model: 0
[11/07 14:54:31 visual_prompt]: Setting up Evaluator...
[11/07 14:54:31 visual_prompt]: Setting up Trainer...
[11/07 14:54:31 visual_prompt]: 	Setting up the optimizer...
[11/07 14:54:31 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/07 15:01:34 visual_prompt]: Epoch 1 / 100: avg data time: 1.13e+01, avg batch time: 12.0818, average train loss: 6.9791
[11/07 15:02:22 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1806, average loss: 6.3857
[11/07 15:02:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/07 15:02:22 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/07 15:09:20 visual_prompt]: Epoch 2 / 100: avg data time: 1.14e+01, avg batch time: 11.9530, average train loss: 3.9256
[11/07 15:10:07 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1801, average loss: 1.0828
[11/07 15:10:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/07 15:10:07 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/07 15:17:42 visual_prompt]: Epoch 3 / 100: avg data time: 1.21e+01, avg batch time: 12.9801, average train loss: 0.9834
[11/07 15:18:29 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1923, average loss: 0.7383
[11/07 15:18:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.75	
[11/07 15:18:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/07 15:25:45 visual_prompt]: Epoch 4 / 100: avg data time: 1.15e+01, avg batch time: 12.4636, average train loss: 0.8579
[11/07 15:26:32 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.2010, average loss: 0.6852
[11/07 15:26:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.68	
[11/07 15:26:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/07 15:33:29 visual_prompt]: Epoch 5 / 100: avg data time: 1.11e+01, avg batch time: 11.8897, average train loss: 0.7967
[11/07 15:34:16 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1952, average loss: 0.9260
[11/07 15:34:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.99	
[11/07 15:34:16 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/07 15:41:15 visual_prompt]: Epoch 6 / 100: avg data time: 1.14e+01, avg batch time: 11.9813, average train loss: 0.7849
[11/07 15:42:02 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1933, average loss: 0.8692
[11/07 15:42:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.99	
[11/07 15:42:02 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/07 15:48:56 visual_prompt]: Epoch 7 / 100: avg data time: 1.12e+01, avg batch time: 11.8201, average train loss: 0.8302
[11/07 15:49:43 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1975, average loss: 0.7134
[11/07 15:49:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 56.09	
[11/07 15:49:43 visual_prompt]: Best epoch 7: best metric: -0.713
[11/07 15:49:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/07 15:56:42 visual_prompt]: Epoch 8 / 100: avg data time: 1.14e+01, avg batch time: 11.9735, average train loss: 0.7933
[11/07 15:57:30 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.2002, average loss: 0.9476
[11/07 15:57:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[11/07 15:57:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/07 16:04:28 visual_prompt]: Epoch 9 / 100: avg data time: 1.13e+01, avg batch time: 11.9426, average train loss: 0.8801
[11/07 16:05:17 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1982, average loss: 0.7957
[11/07 16:05:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.37	
[11/07 16:05:17 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/07 16:12:08 visual_prompt]: Epoch 10 / 100: avg data time: 1.11e+01, avg batch time: 11.7393, average train loss: 0.7332
[11/07 16:12:54 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1931, average loss: 0.9599
[11/07 16:12:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.88	
[11/07 16:12:54 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/07 16:19:46 visual_prompt]: Epoch 11 / 100: avg data time: 1.11e+01, avg batch time: 11.7512, average train loss: 0.7308
[11/07 16:20:32 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1976, average loss: 0.6863
[11/07 16:20:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.49	
[11/07 16:20:32 visual_prompt]: Best epoch 11: best metric: -0.686
[11/07 16:20:32 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/07 16:27:23 visual_prompt]: Epoch 12 / 100: avg data time: 1.11e+01, avg batch time: 11.7365, average train loss: 0.7546
[11/07 16:28:10 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1916, average loss: 0.8950
[11/07 16:28:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.32	
[11/07 16:28:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/07 16:35:01 visual_prompt]: Epoch 13 / 100: avg data time: 1.11e+01, avg batch time: 11.7353, average train loss: 0.7412
[11/07 16:35:47 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1942, average loss: 0.7153
[11/07 16:35:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 54.43	
[11/07 16:35:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/07 16:42:44 visual_prompt]: Epoch 14 / 100: avg data time: 1.13e+01, avg batch time: 11.9182, average train loss: 0.7038
[11/07 16:43:31 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1914, average loss: 0.6802
[11/07 16:43:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.29	
[11/07 16:43:31 visual_prompt]: Best epoch 14: best metric: -0.680
[11/07 16:43:31 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/07 16:50:23 visual_prompt]: Epoch 15 / 100: avg data time: 1.11e+01, avg batch time: 11.7641, average train loss: 0.7309
[11/07 16:51:09 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.2004, average loss: 0.6958
[11/07 16:51:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.02	
[11/07 16:51:09 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/07 16:58:00 visual_prompt]: Epoch 16 / 100: avg data time: 1.11e+01, avg batch time: 11.7445, average train loss: 0.7284
[11/07 16:58:47 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1936, average loss: 0.7002
[11/07 16:58:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.10	
[11/07 16:58:47 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/07 17:05:38 visual_prompt]: Epoch 17 / 100: avg data time: 1.11e+01, avg batch time: 11.7427, average train loss: 0.7242
[11/07 17:06:24 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.1951, average loss: 0.9191
[11/07 17:06:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.68	
[11/07 17:06:24 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/07 17:13:16 visual_prompt]: Epoch 18 / 100: avg data time: 1.11e+01, avg batch time: 11.7739, average train loss: 0.7764
[11/07 17:14:03 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1990, average loss: 0.6813
[11/07 17:14:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.21	
[11/07 17:14:03 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/07 17:20:59 visual_prompt]: Epoch 19 / 100: avg data time: 1.13e+01, avg batch time: 11.8747, average train loss: 0.6973
[11/07 17:21:46 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1867, average loss: 0.7781
[11/07 17:21:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.34	
[11/07 17:21:46 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/07 17:28:43 visual_prompt]: Epoch 20 / 100: avg data time: 1.13e+01, avg batch time: 11.9181, average train loss: 0.7387
[11/07 17:29:31 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.2010, average loss: 0.7487
[11/07 17:29:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 57.24	
[11/07 17:29:31 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/07 17:36:28 visual_prompt]: Epoch 21 / 100: avg data time: 1.13e+01, avg batch time: 11.9086, average train loss: 0.6933
[11/07 17:37:15 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1947, average loss: 0.6942
[11/07 17:37:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.48	
[11/07 17:37:15 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/07 17:44:11 visual_prompt]: Epoch 22 / 100: avg data time: 1.13e+01, avg batch time: 11.9014, average train loss: 0.7029
[11/07 17:44:58 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1901, average loss: 0.8667
[11/07 17:44:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.37	
[11/07 17:44:58 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/07 17:51:54 visual_prompt]: Epoch 23 / 100: avg data time: 1.13e+01, avg batch time: 11.8720, average train loss: 0.7319
[11/07 17:52:41 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1962, average loss: 0.7806
[11/07 17:52:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.98	
[11/07 17:52:41 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/07 17:59:41 visual_prompt]: Epoch 24 / 100: avg data time: 1.14e+01, avg batch time: 12.0060, average train loss: 0.7007
[11/07 18:00:28 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1998, average loss: 0.7004
[11/07 18:00:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.17	
[11/07 18:00:28 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/07 18:07:25 visual_prompt]: Epoch 25 / 100: avg data time: 1.13e+01, avg batch time: 11.8891, average train loss: 0.6932
[11/07 18:08:13 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1814, average loss: 0.8300
[11/07 18:08:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.44	
[11/07 18:08:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/07 18:15:13 visual_prompt]: Epoch 26 / 100: avg data time: 1.14e+01, avg batch time: 11.9959, average train loss: 0.7216
[11/07 18:16:00 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1900, average loss: 0.6856
[11/07 18:16:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.77	
[11/07 18:16:00 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/07 18:22:58 visual_prompt]: Epoch 27 / 100: avg data time: 1.13e+01, avg batch time: 11.9460, average train loss: 0.7146
[11/07 18:23:45 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1991, average loss: 0.6925
[11/07 18:23:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.64	
[11/07 18:23:45 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/07 18:30:47 visual_prompt]: Epoch 28 / 100: avg data time: 1.14e+01, avg batch time: 12.0582, average train loss: 0.7048
[11/07 18:31:35 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1951, average loss: 0.7163
[11/07 18:31:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 58.61	
[11/07 18:31:35 visual_prompt]: Stopping early.
[11/07 18:31:35 visual_prompt]: Rank of current process: 0. World size: 1
[11/07 18:31:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/07 18:31:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/07 18:31:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/07 18:31:35 visual_prompt]: Training with config:
[11/07 18:31:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.005_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/07 18:31:35 visual_prompt]: Loading training data...
[11/07 18:31:35 visual_prompt]: Constructing mammo-cbis dataset train...
[11/07 18:31:35 visual_prompt]: Loading validation data...
[11/07 18:31:35 visual_prompt]: Constructing mammo-cbis dataset val...
[11/07 18:31:35 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/07 18:31:41 visual_prompt]: Enable all parameters update during training
[11/07 18:31:41 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/07 18:31:41 visual_prompt]: tuned percent:100.000
[11/07 18:31:41 visual_prompt]: Device used for model: 0
[11/07 18:31:41 visual_prompt]: Setting up Evaluator...
[11/07 18:31:41 visual_prompt]: Setting up Trainer...
[11/07 18:31:41 visual_prompt]: 	Setting up the optimizer...
[11/07 18:31:41 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/07 18:38:36 visual_prompt]: Epoch 1 / 100: avg data time: 1.10e+01, avg batch time: 11.8565, average train loss: 6.9791
[11/07 18:39:23 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1942, average loss: 6.3857
[11/07 18:39:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/07 18:39:23 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/07 18:46:15 visual_prompt]: Epoch 2 / 100: avg data time: 1.11e+01, avg batch time: 11.7712, average train loss: 3.9256
[11/07 18:47:01 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1979, average loss: 1.0828
[11/07 18:47:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/07 18:47:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/07 18:53:53 visual_prompt]: Epoch 3 / 100: avg data time: 1.11e+01, avg batch time: 11.7698, average train loss: 0.9834
[11/07 18:54:40 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1952, average loss: 0.7383
[11/07 18:54:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.75	
[11/07 18:54:40 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/07 19:01:32 visual_prompt]: Epoch 4 / 100: avg data time: 1.10e+01, avg batch time: 11.7558, average train loss: 0.8579
[11/07 19:02:19 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1982, average loss: 0.6852
[11/07 19:02:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.68	
[11/07 19:02:19 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/07 19:09:11 visual_prompt]: Epoch 5 / 100: avg data time: 1.10e+01, avg batch time: 11.7632, average train loss: 0.7967
[11/07 19:09:57 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1970, average loss: 0.9260
[11/07 19:09:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.99	
[11/07 19:09:57 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/07 19:16:50 visual_prompt]: Epoch 6 / 100: avg data time: 1.10e+01, avg batch time: 11.7764, average train loss: 0.7849
[11/07 19:17:36 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1973, average loss: 0.8692
[11/07 19:17:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.99	
[11/07 19:17:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/07 19:24:28 visual_prompt]: Epoch 7 / 100: avg data time: 1.10e+01, avg batch time: 11.7695, average train loss: 0.8302
[11/07 19:25:15 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1957, average loss: 0.7134
[11/07 19:25:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 56.09	
[11/07 19:25:15 visual_prompt]: Best epoch 7: best metric: -0.713
[11/07 19:25:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/07 19:32:08 visual_prompt]: Epoch 8 / 100: avg data time: 1.11e+01, avg batch time: 11.7804, average train loss: 0.7933
[11/07 19:32:55 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1955, average loss: 0.9476
[11/07 19:32:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[11/07 19:32:55 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/07 19:39:46 visual_prompt]: Epoch 9 / 100: avg data time: 1.11e+01, avg batch time: 11.7493, average train loss: 0.8801
[11/07 19:40:33 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.2023, average loss: 0.7957
[11/07 19:40:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.37	
[11/07 19:40:33 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/07 19:47:25 visual_prompt]: Epoch 10 / 100: avg data time: 1.10e+01, avg batch time: 11.7641, average train loss: 0.7332
[11/07 19:48:11 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1981, average loss: 0.9599
[11/07 19:48:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.88	
[11/07 19:48:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/07 19:55:02 visual_prompt]: Epoch 11 / 100: avg data time: 1.11e+01, avg batch time: 11.7451, average train loss: 0.7308
[11/07 19:55:49 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1918, average loss: 0.6863
[11/07 19:55:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.49	
[11/07 19:55:49 visual_prompt]: Best epoch 11: best metric: -0.686
[11/07 19:55:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/07 20:02:41 visual_prompt]: Epoch 12 / 100: avg data time: 1.11e+01, avg batch time: 11.7516, average train loss: 0.7546
[11/07 20:03:27 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1995, average loss: 0.8950
[11/07 20:03:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.32	
[11/07 20:03:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/07 20:10:22 visual_prompt]: Epoch 13 / 100: avg data time: 1.10e+01, avg batch time: 11.8354, average train loss: 0.7412
[11/07 20:11:09 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1933, average loss: 0.7153
[11/07 20:11:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 54.43	
[11/07 20:11:09 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/07 20:18:01 visual_prompt]: Epoch 14 / 100: avg data time: 1.11e+01, avg batch time: 11.7564, average train loss: 0.7038
[11/07 20:18:47 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1966, average loss: 0.6802
[11/07 20:18:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.29	
[11/07 20:18:47 visual_prompt]: Best epoch 14: best metric: -0.680
[11/07 20:18:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/07 20:25:40 visual_prompt]: Epoch 15 / 100: avg data time: 1.10e+01, avg batch time: 11.7868, average train loss: 0.7309
[11/07 20:26:27 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.2015, average loss: 0.6958
[11/07 20:26:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.02	
[11/07 20:26:27 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/07 20:33:17 visual_prompt]: Epoch 16 / 100: avg data time: 1.11e+01, avg batch time: 11.7257, average train loss: 0.7284
[11/07 20:34:04 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1931, average loss: 0.7002
[11/07 20:34:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.10	
[11/07 20:34:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/07 20:40:57 visual_prompt]: Epoch 17 / 100: avg data time: 1.12e+01, avg batch time: 11.7928, average train loss: 0.7242
[11/07 20:41:44 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1874, average loss: 0.9191
[11/07 20:41:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.68	
[11/07 20:41:44 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/07 20:48:38 visual_prompt]: Epoch 18 / 100: avg data time: 1.12e+01, avg batch time: 11.8213, average train loss: 0.7764
[11/07 20:49:25 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1929, average loss: 0.6813
[11/07 20:49:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.21	
[11/07 20:49:25 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/07 20:56:19 visual_prompt]: Epoch 19 / 100: avg data time: 1.12e+01, avg batch time: 11.8279, average train loss: 0.6973
[11/07 20:57:06 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1938, average loss: 0.7781
[11/07 20:57:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.34	
[11/07 20:57:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/07 21:03:59 visual_prompt]: Epoch 20 / 100: avg data time: 1.12e+01, avg batch time: 11.7840, average train loss: 0.7387
[11/07 21:04:45 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1944, average loss: 0.7487
[11/07 21:04:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 57.24	
[11/07 21:04:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/07 21:11:29 visual_prompt]: Epoch 21 / 100: avg data time: 1.09e+01, avg batch time: 11.5520, average train loss: 0.6933
[11/07 21:12:15 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1930, average loss: 0.6942
[11/07 21:12:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.48	
[11/07 21:12:15 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/07 21:19:00 visual_prompt]: Epoch 22 / 100: avg data time: 1.09e+01, avg batch time: 11.5506, average train loss: 0.7029
[11/07 21:19:46 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1955, average loss: 0.8667
[11/07 21:19:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.37	
[11/07 21:19:46 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/07 21:26:30 visual_prompt]: Epoch 23 / 100: avg data time: 1.10e+01, avg batch time: 11.5577, average train loss: 0.7319
[11/07 21:27:16 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1948, average loss: 0.7806
[11/07 21:27:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.98	
[11/07 21:27:16 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/07 21:34:01 visual_prompt]: Epoch 24 / 100: avg data time: 1.10e+01, avg batch time: 11.5572, average train loss: 0.7007
[11/07 21:34:47 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1956, average loss: 0.7004
[11/07 21:34:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.17	
[11/07 21:34:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/07 21:41:31 visual_prompt]: Epoch 25 / 100: avg data time: 1.09e+01, avg batch time: 11.5509, average train loss: 0.6932
[11/07 21:42:17 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.2032, average loss: 0.8300
[11/07 21:42:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.44	
[11/07 21:42:17 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/07 21:49:02 visual_prompt]: Epoch 26 / 100: avg data time: 1.10e+01, avg batch time: 11.5628, average train loss: 0.7216
[11/07 21:49:48 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.2015, average loss: 0.6856
[11/07 21:49:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.77	
[11/07 21:49:48 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/07 21:56:32 visual_prompt]: Epoch 27 / 100: avg data time: 1.09e+01, avg batch time: 11.5327, average train loss: 0.7146
[11/07 21:57:18 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1976, average loss: 0.6925
[11/07 21:57:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.64	
[11/07 21:57:18 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/07 22:04:02 visual_prompt]: Epoch 28 / 100: avg data time: 1.09e+01, avg batch time: 11.5405, average train loss: 0.7048
[11/07 22:04:48 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1971, average loss: 0.7163
[11/07 22:04:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 58.61	
[11/07 22:04:48 visual_prompt]: Stopping early.
[11/07 22:04:48 visual_prompt]: Rank of current process: 0. World size: 1
[11/07 22:04:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/07 22:04:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/07 22:04:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/07 22:04:48 visual_prompt]: Training with config:
[11/07 22:04:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.005_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/07 22:04:48 visual_prompt]: Loading training data...
[11/07 22:04:48 visual_prompt]: Constructing mammo-cbis dataset train...
[11/07 22:04:48 visual_prompt]: Loading validation data...
[11/07 22:04:48 visual_prompt]: Constructing mammo-cbis dataset val...
[11/07 22:04:48 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/07 22:04:55 visual_prompt]: Enable all parameters update during training
[11/07 22:04:55 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/07 22:04:55 visual_prompt]: tuned percent:100.000
[11/07 22:04:55 visual_prompt]: Device used for model: 0
[11/07 22:04:55 visual_prompt]: Setting up Evaluator...
[11/07 22:04:55 visual_prompt]: Setting up Trainer...
[11/07 22:04:55 visual_prompt]: 	Setting up the optimizer...
[11/07 22:04:55 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/07 22:11:39 visual_prompt]: Epoch 1 / 100: avg data time: 1.09e+01, avg batch time: 11.5503, average train loss: 6.9791
[11/07 22:12:25 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.2004, average loss: 6.3857
[11/07 22:12:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/07 22:12:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/07 22:19:09 visual_prompt]: Epoch 2 / 100: avg data time: 1.09e+01, avg batch time: 11.5321, average train loss: 3.9256
[11/07 22:19:55 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.2005, average loss: 1.0828
[11/07 22:19:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/07 22:19:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/07 22:26:39 visual_prompt]: Epoch 3 / 100: avg data time: 1.09e+01, avg batch time: 11.5422, average train loss: 0.9834
[11/07 22:27:25 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1979, average loss: 0.7383
[11/07 22:27:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.75	
[11/07 22:27:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/07 22:34:09 visual_prompt]: Epoch 4 / 100: avg data time: 1.09e+01, avg batch time: 11.5210, average train loss: 0.8579
[11/07 22:34:55 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1997, average loss: 0.6852
[11/07 22:34:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.68	
[11/07 22:34:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/07 22:41:38 visual_prompt]: Epoch 5 / 100: avg data time: 1.09e+01, avg batch time: 11.5155, average train loss: 0.7967
[11/07 22:42:24 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1987, average loss: 0.9260
[11/07 22:42:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.99	
[11/07 22:42:24 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/07 22:49:07 visual_prompt]: Epoch 6 / 100: avg data time: 1.09e+01, avg batch time: 11.5162, average train loss: 0.7849
[11/07 22:49:54 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.2009, average loss: 0.8692
[11/07 22:49:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.99	
[11/07 22:49:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/07 22:56:37 visual_prompt]: Epoch 7 / 100: avg data time: 1.09e+01, avg batch time: 11.5311, average train loss: 0.8302
[11/07 22:57:23 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1999, average loss: 0.7134
[11/07 22:57:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 56.09	
[11/07 22:57:23 visual_prompt]: Best epoch 7: best metric: -0.713
[11/07 22:57:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/07 23:04:07 visual_prompt]: Epoch 8 / 100: avg data time: 1.09e+01, avg batch time: 11.5354, average train loss: 0.7933
[11/07 23:04:53 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1863, average loss: 0.9476
[11/07 23:04:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.62	
[11/07 23:04:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/07 23:11:37 visual_prompt]: Epoch 9 / 100: avg data time: 1.09e+01, avg batch time: 11.5204, average train loss: 0.8801
[11/07 23:12:23 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1996, average loss: 0.7957
[11/07 23:12:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.37	
[11/07 23:12:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/07 23:19:06 visual_prompt]: Epoch 10 / 100: avg data time: 1.09e+01, avg batch time: 11.5189, average train loss: 0.7332
[11/07 23:19:52 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1935, average loss: 0.9599
[11/07 23:19:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.88	
[11/07 23:19:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/07 23:26:31 visual_prompt]: Epoch 11 / 100: avg data time: 1.08e+01, avg batch time: 11.3761, average train loss: 0.7308
[11/07 23:27:16 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.2013, average loss: 0.6863
[11/07 23:27:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.49	
[11/07 23:27:16 visual_prompt]: Best epoch 11: best metric: -0.686
[11/07 23:27:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/07 23:33:52 visual_prompt]: Epoch 12 / 100: avg data time: 1.07e+01, avg batch time: 11.3208, average train loss: 0.7546
[11/07 23:34:37 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1962, average loss: 0.8950
[11/07 23:34:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.32	
[11/07 23:34:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/07 23:41:19 visual_prompt]: Epoch 13 / 100: avg data time: 1.09e+01, avg batch time: 11.4783, average train loss: 0.7412
[11/07 23:42:05 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1979, average loss: 0.7153
[11/07 23:42:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 54.43	
[11/07 23:42:05 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/07 23:48:49 visual_prompt]: Epoch 14 / 100: avg data time: 1.09e+01, avg batch time: 11.5271, average train loss: 0.7038
[11/07 23:49:35 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1916, average loss: 0.6802
[11/07 23:49:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.29	
[11/07 23:49:35 visual_prompt]: Best epoch 14: best metric: -0.680
[11/07 23:49:35 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/07 23:56:19 visual_prompt]: Epoch 15 / 100: avg data time: 1.09e+01, avg batch time: 11.5463, average train loss: 0.7309
[11/07 23:57:05 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1996, average loss: 0.6958
[11/07 23:57:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.02	
[11/07 23:57:05 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/08 00:03:49 visual_prompt]: Epoch 16 / 100: avg data time: 1.09e+01, avg batch time: 11.5144, average train loss: 0.7284
[11/08 00:04:35 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1967, average loss: 0.7002
[11/08 00:04:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.10	
[11/08 00:04:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/08 00:11:13 visual_prompt]: Epoch 17 / 100: avg data time: 1.08e+01, avg batch time: 11.3841, average train loss: 0.7242
[11/08 00:11:59 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1919, average loss: 0.9191
[11/08 00:11:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.68	
[11/08 00:11:59 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/08 00:18:37 visual_prompt]: Epoch 18 / 100: avg data time: 1.08e+01, avg batch time: 11.3711, average train loss: 0.7764
[11/08 00:19:22 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1977, average loss: 0.6813
[11/08 00:19:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.21	
[11/08 00:19:22 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/08 00:26:02 visual_prompt]: Epoch 19 / 100: avg data time: 1.08e+01, avg batch time: 11.4004, average train loss: 0.6973
[11/08 00:26:47 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1937, average loss: 0.7781
[11/08 00:26:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.34	
[11/08 00:26:47 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/08 00:33:25 visual_prompt]: Epoch 20 / 100: avg data time: 1.08e+01, avg batch time: 11.3769, average train loss: 0.7387
[11/08 00:34:11 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1975, average loss: 0.7487
[11/08 00:34:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.37	rocauc: 57.24	
[11/08 00:34:11 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/08 00:40:49 visual_prompt]: Epoch 21 / 100: avg data time: 1.08e+01, avg batch time: 11.3814, average train loss: 0.6933
[11/08 00:41:35 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.2051, average loss: 0.6942
[11/08 00:41:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.48	
[11/08 00:41:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/08 00:48:13 visual_prompt]: Epoch 22 / 100: avg data time: 1.08e+01, avg batch time: 11.3869, average train loss: 0.7029
[11/08 00:48:59 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1942, average loss: 0.8667
[11/08 00:48:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.37	
[11/08 00:48:59 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/08 00:55:37 visual_prompt]: Epoch 23 / 100: avg data time: 1.08e+01, avg batch time: 11.3676, average train loss: 0.7319
[11/08 00:56:23 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1942, average loss: 0.7806
[11/08 00:56:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.98	
[11/08 00:56:23 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/08 01:03:01 visual_prompt]: Epoch 24 / 100: avg data time: 1.08e+01, avg batch time: 11.3868, average train loss: 0.7007
[11/08 01:03:47 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1933, average loss: 0.7004
[11/08 01:03:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.17	
[11/08 01:03:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/08 01:10:25 visual_prompt]: Epoch 25 / 100: avg data time: 1.08e+01, avg batch time: 11.3734, average train loss: 0.6932
[11/08 01:11:11 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1855, average loss: 0.8300
[11/08 01:11:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.44	
[11/08 01:11:11 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/08 01:17:50 visual_prompt]: Epoch 26 / 100: avg data time: 1.08e+01, avg batch time: 11.3957, average train loss: 0.7216
[11/08 01:18:36 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1850, average loss: 0.6856
[11/08 01:18:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.77	
[11/08 01:18:36 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/08 01:25:12 visual_prompt]: Epoch 27 / 100: avg data time: 1.07e+01, avg batch time: 11.3199, average train loss: 0.7146
[11/08 01:25:57 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1921, average loss: 0.6925
[11/08 01:25:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.64	
[11/08 01:25:57 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/08 01:32:34 visual_prompt]: Epoch 28 / 100: avg data time: 1.08e+01, avg batch time: 11.3499, average train loss: 0.7048
[11/08 01:33:19 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1893, average loss: 0.7163
[11/08 01:33:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 58.61	
[11/08 01:33:19 visual_prompt]: Stopping early.
[11/08 01:33:20 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 01:33:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 01:33:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/08 01:33:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/08 01:33:20 visual_prompt]: Training with config:
[11/08 01:33:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.005_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/08 01:33:20 visual_prompt]: Loading training data...
[11/08 01:33:20 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 01:33:20 visual_prompt]: Loading validation data...
[11/08 01:33:20 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 01:33:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 01:33:27 visual_prompt]: Enable all parameters update during training
[11/08 01:33:27 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/08 01:33:27 visual_prompt]: tuned percent:100.000
[11/08 01:33:27 visual_prompt]: Device used for model: 0
[11/08 01:33:27 visual_prompt]: Setting up Evaluator...
[11/08 01:33:27 visual_prompt]: Setting up Trainer...
[11/08 01:33:27 visual_prompt]: 	Setting up the optimizer...
[11/08 01:33:27 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 01:40:05 visual_prompt]: Epoch 1 / 100: avg data time: 1.08e+01, avg batch time: 11.3484, average train loss: 6.9791
[11/08 01:40:50 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1950, average loss: 6.3857
[11/08 01:40:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/08 01:40:50 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/08 01:47:27 visual_prompt]: Epoch 2 / 100: avg data time: 1.08e+01, avg batch time: 11.3358, average train loss: 7.1642
[11/08 01:48:12 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1917, average loss: 1.1776
[11/08 01:48:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 55.47	
[11/08 01:48:12 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/08 01:54:49 visual_prompt]: Epoch 3 / 100: avg data time: 1.08e+01, avg batch time: 11.3331, average train loss: 3.3523
[11/08 01:55:35 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1878, average loss: 1.1512
[11/08 01:55:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.33	
[11/08 01:55:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/08 02:02:11 visual_prompt]: Epoch 4 / 100: avg data time: 1.07e+01, avg batch time: 11.3236, average train loss: 1.1312
[11/08 02:02:56 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1958, average loss: 1.3375
[11/08 02:02:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.19	
[11/08 02:02:56 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/08 02:09:33 visual_prompt]: Epoch 5 / 100: avg data time: 1.08e+01, avg batch time: 11.3238, average train loss: 2.5304
[11/08 02:10:18 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1866, average loss: 0.7134
[11/08 02:10:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 54.01	
[11/08 02:10:18 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/08 02:16:54 visual_prompt]: Epoch 6 / 100: avg data time: 1.07e+01, avg batch time: 11.2978, average train loss: 2.5519
[11/08 02:17:38 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1823, average loss: 3.9798
[11/08 02:17:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.87	
[11/08 02:17:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/08 02:24:01 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9375, average train loss: 1.8713
[11/08 02:24:44 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1791, average loss: 1.1052
[11/08 02:24:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.46	
[11/08 02:24:44 visual_prompt]: Best epoch 7: best metric: -1.105
[11/08 02:24:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/08 02:31:07 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9318, average train loss: 1.1114
[11/08 02:31:51 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1795, average loss: 0.7703
[11/08 02:31:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.93	rocauc: 54.86	
[11/08 02:31:51 visual_prompt]: Best epoch 8: best metric: -0.770
[11/08 02:31:51 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/08 02:38:13 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.9071, average train loss: 0.8901
[11/08 02:38:56 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1796, average loss: 2.3425
[11/08 02:38:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.45	
[11/08 02:38:56 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/08 02:45:18 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.9113, average train loss: 1.5516
[11/08 02:46:02 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1689, average loss: 0.9545
[11/08 02:46:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/08 02:46:02 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/08 02:52:24 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.9061, average train loss: 3.1054
[11/08 02:53:08 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1760, average loss: 1.3142
[11/08 02:53:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.80	
[11/08 02:53:08 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/08 02:59:30 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.8985, average train loss: 1.0938
[11/08 03:00:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1699, average loss: 1.0759
[11/08 03:00:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.48	
[11/08 03:00:13 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/08 03:06:35 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.9102, average train loss: 0.8543
[11/08 03:07:19 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1789, average loss: 0.9467
[11/08 03:07:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[11/08 03:07:19 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/08 03:13:40 visual_prompt]: Epoch 14 / 100: avg data time: 1.03e+01, avg batch time: 10.8749, average train loss: 0.9961
[11/08 03:14:23 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1771, average loss: 0.9333
[11/08 03:14:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.33	
[11/08 03:14:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/08 03:20:46 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9148, average train loss: 0.9825
[11/08 03:21:29 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1713, average loss: 0.8608
[11/08 03:21:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.63	
[11/08 03:21:29 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/08 03:27:51 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.8941, average train loss: 0.7708
[11/08 03:28:34 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1741, average loss: 0.6881
[11/08 03:28:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 54.88	
[11/08 03:28:34 visual_prompt]: Best epoch 16: best metric: -0.688
[11/08 03:28:34 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/08 03:34:56 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.9026, average train loss: 0.8546
[11/08 03:35:40 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1733, average loss: 1.0498
[11/08 03:35:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.29	
[11/08 03:35:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/08 03:42:02 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.9099, average train loss: 1.2939
[11/08 03:42:45 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1726, average loss: 0.7597
[11/08 03:42:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 54.71	
[11/08 03:42:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/08 03:49:08 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9141, average train loss: 0.7812
[11/08 03:49:51 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1818, average loss: 1.2632
[11/08 03:49:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.13	
[11/08 03:49:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/08 03:56:13 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.9162, average train loss: 1.1966
[11/08 03:56:57 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1789, average loss: 1.1624
[11/08 03:56:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.46	
[11/08 03:56:57 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/08 04:03:19 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.9059, average train loss: 0.9163
[11/08 04:04:02 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1798, average loss: 0.6865
[11/08 04:04:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 54.50	
[11/08 04:04:02 visual_prompt]: Best epoch 21: best metric: -0.687
[11/08 04:04:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/08 04:10:25 visual_prompt]: Epoch 22 / 100: avg data time: 1.04e+01, avg batch time: 10.9249, average train loss: 0.7402
[11/08 04:11:09 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1764, average loss: 0.9014
[11/08 04:11:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.13	
[11/08 04:11:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/08 04:17:30 visual_prompt]: Epoch 23 / 100: avg data time: 1.04e+01, avg batch time: 10.8937, average train loss: 0.7794
[11/08 04:18:14 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1780, average loss: 0.8975
[11/08 04:18:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.01	
[11/08 04:18:14 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/08 04:24:36 visual_prompt]: Epoch 24 / 100: avg data time: 1.04e+01, avg batch time: 10.9210, average train loss: 0.7435
[11/08 04:25:20 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1834, average loss: 0.7279
[11/08 04:25:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.21	
[11/08 04:25:20 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/08 04:31:41 visual_prompt]: Epoch 25 / 100: avg data time: 1.03e+01, avg batch time: 10.8717, average train loss: 0.8491
[11/08 04:32:24 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1726, average loss: 0.7528
[11/08 04:32:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.75	
[11/08 04:32:24 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/08 04:38:47 visual_prompt]: Epoch 26 / 100: avg data time: 1.04e+01, avg batch time: 10.9315, average train loss: 0.7604
[11/08 04:39:30 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1737, average loss: 0.7216
[11/08 04:39:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.07	
[11/08 04:39:30 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/08 04:45:52 visual_prompt]: Epoch 27 / 100: avg data time: 1.04e+01, avg batch time: 10.9021, average train loss: 0.8739
[11/08 04:46:36 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1802, average loss: 0.9465
[11/08 04:46:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.28	
[11/08 04:46:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/08 04:52:58 visual_prompt]: Epoch 28 / 100: avg data time: 1.04e+01, avg batch time: 10.9087, average train loss: 0.8653
[11/08 04:53:42 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1736, average loss: 0.7812
[11/08 04:53:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.08	
[11/08 04:53:42 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/08 05:00:04 visual_prompt]: Epoch 29 / 100: avg data time: 1.04e+01, avg batch time: 10.9130, average train loss: 0.7649
[11/08 05:00:47 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1693, average loss: 1.1866
[11/08 05:00:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/08 05:00:47 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/08 05:07:10 visual_prompt]: Epoch 30 / 100: avg data time: 1.04e+01, avg batch time: 10.9180, average train loss: 0.7701
[11/08 05:07:53 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1708, average loss: 1.0235
[11/08 05:07:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.47	
[11/08 05:07:53 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/08 05:14:14 visual_prompt]: Epoch 31 / 100: avg data time: 1.03e+01, avg batch time: 10.8875, average train loss: 0.8795
[11/08 05:14:58 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1705, average loss: 0.9722
[11/08 05:14:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.95	
[11/08 05:14:58 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/08 05:21:21 visual_prompt]: Epoch 32 / 100: avg data time: 1.04e+01, avg batch time: 10.9271, average train loss: 0.7626
[11/08 05:22:04 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1713, average loss: 0.7604
[11/08 05:22:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.10	
[11/08 05:22:04 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/08 05:28:26 visual_prompt]: Epoch 33 / 100: avg data time: 1.04e+01, avg batch time: 10.8941, average train loss: 0.8340
[11/08 05:29:09 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1772, average loss: 0.8713
[11/08 05:29:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.20	
[11/08 05:29:09 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/08 05:35:31 visual_prompt]: Epoch 34 / 100: avg data time: 1.04e+01, avg batch time: 10.9064, average train loss: 0.8578
[11/08 05:36:15 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1715, average loss: 0.7660
[11/08 05:36:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.65	
[11/08 05:36:15 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0039358216567073594
[11/08 05:42:38 visual_prompt]: Epoch 35 / 100: avg data time: 1.04e+01, avg batch time: 10.9294, average train loss: 0.9168
[11/08 05:43:21 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.1804, average loss: 0.9473
[11/08 05:43:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.49	
[11/08 05:43:21 visual_prompt]: Stopping early.
[11/08 05:43:21 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 05:43:21 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 05:43:21 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/08 05:43:21 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/08 05:43:21 visual_prompt]: Training with config:
[11/08 05:43:21 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/08 05:43:21 visual_prompt]: Loading training data...
[11/08 05:43:21 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 05:43:21 visual_prompt]: Loading validation data...
[11/08 05:43:21 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 05:43:21 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 05:43:23 visual_prompt]: Enable all parameters update during training
[11/08 05:43:23 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/08 05:43:23 visual_prompt]: tuned percent:100.000
[11/08 05:43:23 visual_prompt]: Device used for model: 0
[11/08 05:43:23 visual_prompt]: Setting up Evaluator...
[11/08 05:43:23 visual_prompt]: Setting up Trainer...
[11/08 05:43:23 visual_prompt]: 	Setting up the optimizer...
[11/08 05:43:23 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 05:49:46 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9279, average train loss: 6.9791
[11/08 05:50:29 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1752, average loss: 6.3857
[11/08 05:50:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/08 05:50:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/08 05:56:51 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.9062, average train loss: 2.9795
[11/08 05:57:35 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.1729, average loss: 1.0191
[11/08 05:57:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.43	
[11/08 05:57:35 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/08 06:03:57 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9167, average train loss: 0.9363
[11/08 06:04:41 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1771, average loss: 0.7546
[11/08 06:04:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.68	
[11/08 06:04:41 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/08 06:11:02 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.8919, average train loss: 0.8156
[11/08 06:11:46 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1807, average loss: 0.6800
[11/08 06:11:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.91	
[11/08 06:11:46 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/08 06:18:08 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.9094, average train loss: 0.7900
[11/08 06:18:52 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1760, average loss: 0.6689
[11/08 06:18:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.97	
[11/08 06:18:52 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/08 06:25:15 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9293, average train loss: 0.8251
[11/08 06:25:58 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1729, average loss: 0.7127
[11/08 06:25:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.79	
[11/08 06:25:58 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/08 06:32:21 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9268, average train loss: 0.7236
[11/08 06:33:05 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1765, average loss: 0.6896
[11/08 06:33:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.59	
[11/08 06:33:05 visual_prompt]: Best epoch 7: best metric: -0.690
[11/08 06:33:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/08 06:39:27 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9141, average train loss: 0.7060
[11/08 06:40:10 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1721, average loss: 0.6624
[11/08 06:40:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.49	
[11/08 06:40:10 visual_prompt]: Best epoch 8: best metric: -0.662
[11/08 06:40:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/08 06:46:33 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.9176, average train loss: 0.7656
[11/08 06:47:16 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1806, average loss: 0.6902
[11/08 06:47:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 64.84	
[11/08 06:47:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/08 06:53:39 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.9138, average train loss: 0.6893
[11/08 06:54:22 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1701, average loss: 0.7023
[11/08 06:54:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 67.18	
[11/08 06:54:22 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/08 07:00:44 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.9141, average train loss: 0.6646
[11/08 07:01:28 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1806, average loss: 0.6843
[11/08 07:01:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 67.84	
[11/08 07:01:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/08 07:07:51 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.9159, average train loss: 0.7185
[11/08 07:08:34 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1723, average loss: 0.7410
[11/08 07:08:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.83	
[11/08 07:08:34 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/08 07:14:57 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.9166, average train loss: 0.6984
[11/08 07:15:40 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1731, average loss: 0.6548
[11/08 07:15:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.62	
[11/08 07:15:40 visual_prompt]: Best epoch 13: best metric: -0.655
[11/08 07:15:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/08 07:22:02 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.9110, average train loss: 0.6217
[11/08 07:22:46 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1756, average loss: 0.6592
[11/08 07:22:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.11	
[11/08 07:22:46 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/08 07:29:09 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9377, average train loss: 0.6511
[11/08 07:29:52 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1773, average loss: 0.7042
[11/08 07:29:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 65.20	
[11/08 07:29:52 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/08 07:36:14 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.8997, average train loss: 0.5915
[11/08 07:36:58 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1770, average loss: 0.6971
[11/08 07:36:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 67.61	
[11/08 07:36:58 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/08 07:43:20 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.9147, average train loss: 0.6150
[11/08 07:44:04 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1708, average loss: 0.7181
[11/08 07:44:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 65.96	
[11/08 07:44:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/08 07:50:26 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.9046, average train loss: 0.5716
[11/08 07:51:09 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1791, average loss: 0.6619
[11/08 07:51:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.74	
[11/08 07:51:09 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/08 07:57:32 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9418, average train loss: 0.5241
[11/08 07:58:16 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1818, average loss: 0.6877
[11/08 07:58:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.08	
[11/08 07:58:16 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/08 08:04:38 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.9153, average train loss: 0.5085
[11/08 08:05:22 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1759, average loss: 0.7454
[11/08 08:05:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.68	
[11/08 08:05:22 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/08 08:11:45 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.9352, average train loss: 0.5087
[11/08 08:12:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1804, average loss: 0.9573
[11/08 08:12:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.39	
[11/08 08:12:29 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/08 08:18:51 visual_prompt]: Epoch 22 / 100: avg data time: 1.04e+01, avg batch time: 10.9109, average train loss: 0.5063
[11/08 08:19:34 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1756, average loss: 0.9121
[11/08 08:19:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 62.89	
[11/08 08:19:34 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/08 08:25:57 visual_prompt]: Epoch 23 / 100: avg data time: 1.04e+01, avg batch time: 10.9139, average train loss: 0.5157
[11/08 08:26:40 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1690, average loss: 1.0141
[11/08 08:26:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 65.51	
[11/08 08:26:40 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/08 08:33:03 visual_prompt]: Epoch 24 / 100: avg data time: 1.04e+01, avg batch time: 10.9394, average train loss: 0.6337
[11/08 08:33:47 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1766, average loss: 0.6935
[11/08 08:33:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 65.54	
[11/08 08:33:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/08 08:40:09 visual_prompt]: Epoch 25 / 100: avg data time: 1.04e+01, avg batch time: 10.9208, average train loss: 0.5709
[11/08 08:40:53 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1725, average loss: 0.7454
[11/08 08:40:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 64.57	
[11/08 08:40:53 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/08 08:47:16 visual_prompt]: Epoch 26 / 100: avg data time: 1.04e+01, avg batch time: 10.9196, average train loss: 0.4873
[11/08 08:47:59 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1734, average loss: 0.7384
[11/08 08:47:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 65.34	
[11/08 08:47:59 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/08 08:54:22 visual_prompt]: Epoch 27 / 100: avg data time: 1.04e+01, avg batch time: 10.9187, average train loss: 0.4226
[11/08 08:55:05 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1744, average loss: 1.0856
[11/08 08:55:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 59.35	
[11/08 08:55:05 visual_prompt]: Stopping early.
[11/08 08:55:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 08:55:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 08:55:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/08 08:55:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/08 08:55:05 visual_prompt]: Training with config:
[11/08 08:55:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/08 08:55:05 visual_prompt]: Loading training data...
[11/08 08:55:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 08:55:05 visual_prompt]: Loading validation data...
[11/08 08:55:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 08:55:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 08:55:07 visual_prompt]: Enable all parameters update during training
[11/08 08:55:07 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/08 08:55:07 visual_prompt]: tuned percent:100.000
[11/08 08:55:07 visual_prompt]: Device used for model: 0
[11/08 08:55:07 visual_prompt]: Setting up Evaluator...
[11/08 08:55:07 visual_prompt]: Setting up Trainer...
[11/08 08:55:07 visual_prompt]: 	Setting up the optimizer...
[11/08 08:55:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 09:01:30 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9411, average train loss: 6.9791
[11/08 09:02:14 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1743, average loss: 6.3857
[11/08 09:02:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/08 09:02:14 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/08 09:08:36 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.9159, average train loss: 2.9795
[11/08 09:09:20 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1804, average loss: 1.0191
[11/08 09:09:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.43	
[11/08 09:09:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/08 09:15:42 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9152, average train loss: 0.9363
[11/08 09:16:25 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1833, average loss: 0.7546
[11/08 09:16:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.68	
[11/08 09:16:25 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/08 09:22:48 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.9318, average train loss: 0.8156
[11/08 09:23:32 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1769, average loss: 0.6800
[11/08 09:23:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.91	
[11/08 09:23:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/08 09:29:54 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.9166, average train loss: 0.7900
[11/08 09:30:38 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1718, average loss: 0.6689
[11/08 09:30:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.97	
[11/08 09:30:38 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/08 09:37:01 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9407, average train loss: 0.8251
[11/08 09:37:45 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1785, average loss: 0.7127
[11/08 09:37:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.79	
[11/08 09:37:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/08 09:44:07 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9185, average train loss: 0.7236
[11/08 09:44:50 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1778, average loss: 0.6896
[11/08 09:44:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.59	
[11/08 09:44:50 visual_prompt]: Best epoch 7: best metric: -0.690
[11/08 09:44:50 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/08 09:51:14 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9381, average train loss: 0.7060
[11/08 09:51:57 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1789, average loss: 0.6624
[11/08 09:51:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.49	
[11/08 09:51:57 visual_prompt]: Best epoch 8: best metric: -0.662
[11/08 09:51:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/08 09:58:26 visual_prompt]: Epoch 9 / 100: avg data time: 1.06e+01, avg batch time: 11.1080, average train loss: 0.7656
[11/08 09:59:14 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1714, average loss: 0.6902
[11/08 09:59:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 64.84	
[11/08 09:59:14 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/08 10:05:57 visual_prompt]: Epoch 10 / 100: avg data time: 1.06e+01, avg batch time: 11.5053, average train loss: 0.6893
[11/08 10:06:44 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3541, average loss: 0.7023
[11/08 10:06:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 67.18	
[11/08 10:06:44 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/08 10:13:26 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 11.4944, average train loss: 0.6646
[11/08 10:14:12 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.3625, average loss: 0.6843
[11/08 10:14:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 67.84	
[11/08 10:14:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/08 10:20:53 visual_prompt]: Epoch 12 / 100: avg data time: 1.03e+01, avg batch time: 11.4508, average train loss: 0.7185
[11/08 10:21:41 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.3478, average loss: 0.7410
[11/08 10:21:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.83	
[11/08 10:21:41 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/08 10:28:30 visual_prompt]: Epoch 13 / 100: avg data time: 1.06e+01, avg batch time: 11.6713, average train loss: 0.6984
[11/08 10:29:17 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.3545, average loss: 0.6548
[11/08 10:29:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.62	
[11/08 10:29:17 visual_prompt]: Best epoch 13: best metric: -0.655
[11/08 10:29:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/08 10:36:05 visual_prompt]: Epoch 14 / 100: avg data time: 1.05e+01, avg batch time: 11.6434, average train loss: 0.6217
[11/08 10:36:51 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.3638, average loss: 0.6592
[11/08 10:36:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.11	
[11/08 10:36:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/08 10:43:40 visual_prompt]: Epoch 15 / 100: avg data time: 1.05e+01, avg batch time: 11.6661, average train loss: 0.6511
[11/08 10:44:26 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.3646, average loss: 0.7042
[11/08 10:44:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 65.20	
[11/08 10:44:26 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/08 10:51:12 visual_prompt]: Epoch 16 / 100: avg data time: 1.05e+01, avg batch time: 11.5896, average train loss: 0.5915
[11/08 10:51:58 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.3660, average loss: 0.6971
[11/08 10:51:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 67.61	
[11/08 10:51:58 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/08 10:58:41 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 11.4932, average train loss: 0.6150
[11/08 10:59:27 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.3691, average loss: 0.7181
[11/08 10:59:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 65.96	
[11/08 10:59:27 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/08 11:06:10 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 11.4986, average train loss: 0.5716
[11/08 11:06:56 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.3678, average loss: 0.6619
[11/08 11:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.74	
[11/08 11:06:56 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/08 11:13:38 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 11.4761, average train loss: 0.5241
[11/08 11:14:24 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.3625, average loss: 0.6877
[11/08 11:14:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.08	
[11/08 11:14:24 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/08 11:21:07 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 11.5022, average train loss: 0.5085
[11/08 11:21:53 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.3641, average loss: 0.7454
[11/08 11:21:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.68	
[11/08 11:21:53 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/08 11:28:35 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 11.4772, average train loss: 0.5087
[11/08 11:29:21 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.3482, average loss: 0.9573
[11/08 11:29:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.39	
[11/08 11:29:21 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/08 11:36:04 visual_prompt]: Epoch 22 / 100: avg data time: 1.04e+01, avg batch time: 11.5099, average train loss: 0.5063
[11/08 11:36:50 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3625, average loss: 0.9121
[11/08 11:36:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 62.89	
[11/08 11:36:50 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/08 11:43:32 visual_prompt]: Epoch 23 / 100: avg data time: 1.04e+01, avg batch time: 11.4656, average train loss: 0.5157
[11/08 11:44:18 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3677, average loss: 1.0141
[11/08 11:44:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 65.51	
[11/08 11:44:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/08 11:51:07 visual_prompt]: Epoch 24 / 100: avg data time: 1.06e+01, avg batch time: 11.7004, average train loss: 0.6337
[11/08 11:51:57 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3637, average loss: 0.6935
[11/08 11:51:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 65.54	
[11/08 11:51:57 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/08 11:59:05 visual_prompt]: Epoch 25 / 100: avg data time: 1.11e+01, avg batch time: 12.2431, average train loss: 0.5709
[11/08 11:59:52 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3622, average loss: 0.7454
[11/08 11:59:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 64.57	
[11/08 11:59:52 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/08 12:06:38 visual_prompt]: Epoch 26 / 100: avg data time: 1.05e+01, avg batch time: 11.5853, average train loss: 0.4873
[11/08 12:07:24 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.3613, average loss: 0.7384
[11/08 12:07:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 65.34	
[11/08 12:07:24 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/08 12:14:06 visual_prompt]: Epoch 27 / 100: avg data time: 1.04e+01, avg batch time: 11.4847, average train loss: 0.4226
[11/08 12:14:53 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3516, average loss: 1.0856
[11/08 12:14:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 59.35	
[11/08 12:14:53 visual_prompt]: Stopping early.
[11/08 12:14:53 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 12:14:53 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 12:14:53 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/08 12:14:53 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/08 12:14:53 visual_prompt]: Training with config:
[11/08 12:14:53 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/08 12:14:53 visual_prompt]: Loading training data...
[11/08 12:14:53 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 12:14:53 visual_prompt]: Loading validation data...
[11/08 12:14:53 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 12:14:53 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 12:14:59 visual_prompt]: Enable all parameters update during training
[11/08 12:14:59 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/08 12:14:59 visual_prompt]: tuned percent:100.000
[11/08 12:15:00 visual_prompt]: Device used for model: 0
[11/08 12:15:00 visual_prompt]: Setting up Evaluator...
[11/08 12:15:00 visual_prompt]: Setting up Trainer...
[11/08 12:15:00 visual_prompt]: 	Setting up the optimizer...
[11/08 12:15:00 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 12:21:41 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 11.4719, average train loss: 6.9791
[11/08 12:22:27 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.3668, average loss: 6.3857
[11/08 12:22:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/08 12:22:27 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/08 12:29:09 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 11.4769, average train loss: 2.9795
[11/08 12:29:55 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.3654, average loss: 1.0191
[11/08 12:29:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.43	
[11/08 12:29:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/08 12:36:38 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 11.5082, average train loss: 0.9363
[11/08 12:37:24 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.3636, average loss: 0.7546
[11/08 12:37:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.68	
[11/08 12:37:24 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/08 12:44:07 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 11.4926, average train loss: 0.8156
[11/08 12:44:53 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.3158, average loss: 0.6800
[11/08 12:44:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 59.91	
[11/08 12:44:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/08 12:51:41 visual_prompt]: Epoch 5 / 100: avg data time: 1.05e+01, avg batch time: 11.6590, average train loss: 0.7900
[11/08 12:52:28 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.3606, average loss: 0.6689
[11/08 12:52:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.97	
[11/08 12:52:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/08 12:59:31 visual_prompt]: Epoch 6 / 100: avg data time: 1.10e+01, avg batch time: 12.0747, average train loss: 0.8251
[11/08 13:00:19 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.3595, average loss: 0.7127
[11/08 13:00:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.79	
[11/08 13:00:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/08 13:07:05 visual_prompt]: Epoch 7 / 100: avg data time: 1.05e+01, avg batch time: 11.5922, average train loss: 0.7236
[11/08 13:07:52 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3550, average loss: 0.6896
[11/08 13:07:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.59	
[11/08 13:07:52 visual_prompt]: Best epoch 7: best metric: -0.690
[11/08 13:07:52 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/08 13:14:47 visual_prompt]: Epoch 8 / 100: avg data time: 1.07e+01, avg batch time: 11.8533, average train loss: 0.7060
[11/08 13:15:33 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.3618, average loss: 0.6624
[11/08 13:15:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.49	
[11/08 13:15:33 visual_prompt]: Best epoch 8: best metric: -0.662
[11/08 13:15:33 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/08 13:22:20 visual_prompt]: Epoch 9 / 100: avg data time: 1.05e+01, avg batch time: 11.6174, average train loss: 0.7656
[11/08 13:23:07 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.3666, average loss: 0.6902
[11/08 13:23:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 64.84	
[11/08 13:23:07 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/08 13:30:16 visual_prompt]: Epoch 10 / 100: avg data time: 1.11e+01, avg batch time: 12.2362, average train loss: 0.6893
[11/08 13:31:05 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.3640, average loss: 0.7023
[11/08 13:31:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 67.18	
[11/08 13:31:05 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/08 13:39:15 visual_prompt]: Epoch 11 / 100: avg data time: 1.29e+01, avg batch time: 13.9767, average train loss: 0.6646
[11/08 13:40:23 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.3622, average loss: 0.6843
[11/08 13:40:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 67.84	
[11/08 13:40:23 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/08 13:47:15 visual_prompt]: Epoch 12 / 100: avg data time: 1.07e+01, avg batch time: 11.7727, average train loss: 0.7185
[11/08 13:48:03 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3452, average loss: 0.7410
[11/08 13:48:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 68.83	
[11/08 13:48:03 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/08 13:54:54 visual_prompt]: Epoch 13 / 100: avg data time: 1.06e+01, avg batch time: 11.7425, average train loss: 0.6984
[11/08 13:55:40 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.3522, average loss: 0.6548
[11/08 13:55:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.62	
[11/08 13:55:40 visual_prompt]: Best epoch 13: best metric: -0.655
[11/08 13:55:40 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/08 14:02:47 visual_prompt]: Epoch 14 / 100: avg data time: 1.11e+01, avg batch time: 12.1884, average train loss: 0.6217
[11/08 14:03:42 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3556, average loss: 0.6592
[11/08 14:03:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.11	
[11/08 14:03:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/08 14:10:45 visual_prompt]: Epoch 15 / 100: avg data time: 1.09e+01, avg batch time: 12.0566, average train loss: 0.6511
[11/08 14:11:32 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.3694, average loss: 0.7042
[11/08 14:11:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 65.20	
[11/08 14:11:32 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/08 14:18:24 visual_prompt]: Epoch 16 / 100: avg data time: 1.06e+01, avg batch time: 11.7528, average train loss: 0.5915
[11/08 14:19:11 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.3657, average loss: 0.6971
[11/08 14:19:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 67.61	
[11/08 14:19:11 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/08 14:25:59 visual_prompt]: Epoch 17 / 100: avg data time: 1.05e+01, avg batch time: 11.6450, average train loss: 0.6150
[11/08 14:26:45 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.3561, average loss: 0.7181
[11/08 14:26:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 65.96	
[11/08 14:26:45 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/08 14:33:33 visual_prompt]: Epoch 18 / 100: avg data time: 1.05e+01, avg batch time: 11.6417, average train loss: 0.5716
[11/08 14:34:20 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.3658, average loss: 0.6619
[11/08 14:34:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 69.74	
[11/08 14:34:20 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/08 14:41:15 visual_prompt]: Epoch 19 / 100: avg data time: 1.07e+01, avg batch time: 11.8573, average train loss: 0.5241
[11/08 14:42:02 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.3489, average loss: 0.6877
[11/08 14:42:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.08	
[11/08 14:42:02 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/08 14:49:17 visual_prompt]: Epoch 20 / 100: avg data time: 1.13e+01, avg batch time: 12.4148, average train loss: 0.5085
[11/08 14:50:04 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.3636, average loss: 0.7454
[11/08 14:50:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.68	
[11/08 14:50:04 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/08 14:56:51 visual_prompt]: Epoch 21 / 100: avg data time: 1.05e+01, avg batch time: 11.6210, average train loss: 0.5087
[11/08 14:57:37 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.3558, average loss: 0.9573
[11/08 14:57:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.39	
[11/08 14:57:37 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/08 15:04:23 visual_prompt]: Epoch 22 / 100: avg data time: 1.05e+01, avg batch time: 11.5765, average train loss: 0.5063
[11/08 15:05:10 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.3467, average loss: 0.9121
[11/08 15:05:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 62.89	
[11/08 15:05:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/08 15:12:01 visual_prompt]: Epoch 23 / 100: avg data time: 1.06e+01, avg batch time: 11.7304, average train loss: 0.5157
[11/08 15:12:48 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.3622, average loss: 1.0141
[11/08 15:12:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 65.51	
[11/08 15:12:48 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/08 15:19:36 visual_prompt]: Epoch 24 / 100: avg data time: 1.05e+01, avg batch time: 11.6511, average train loss: 0.6337
[11/08 15:20:22 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.3598, average loss: 0.6935
[11/08 15:20:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 65.54	
[11/08 15:20:22 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/08 15:27:13 visual_prompt]: Epoch 25 / 100: avg data time: 1.06e+01, avg batch time: 11.7281, average train loss: 0.5709
[11/08 15:28:00 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.3653, average loss: 0.7454
[11/08 15:28:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 64.57	
[11/08 15:28:00 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/08 15:34:53 visual_prompt]: Epoch 26 / 100: avg data time: 1.07e+01, avg batch time: 11.7785, average train loss: 0.4873
[11/08 15:35:39 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.3644, average loss: 0.7384
[11/08 15:35:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 65.34	
[11/08 15:35:39 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/08 15:42:30 visual_prompt]: Epoch 27 / 100: avg data time: 1.06e+01, avg batch time: 11.7146, average train loss: 0.4226
[11/08 15:43:17 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.3640, average loss: 1.0856
[11/08 15:43:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 59.35	
[11/08 15:43:17 visual_prompt]: Stopping early.
[11/08 15:43:17 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 15:43:17 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 15:43:17 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/08 15:43:17 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/08 15:43:17 visual_prompt]: Training with config:
[11/08 15:43:17 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.001_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/08 15:43:17 visual_prompt]: Loading training data...
[11/08 15:43:17 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 15:43:17 visual_prompt]: Loading validation data...
[11/08 15:43:17 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 15:43:17 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 15:43:22 visual_prompt]: Enable all parameters update during training
[11/08 15:43:22 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/08 15:43:22 visual_prompt]: tuned percent:100.000
[11/08 15:43:22 visual_prompt]: Device used for model: 0
[11/08 15:43:22 visual_prompt]: Setting up Evaluator...
[11/08 15:43:22 visual_prompt]: Setting up Trainer...
[11/08 15:43:22 visual_prompt]: 	Setting up the optimizer...
[11/08 15:43:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/08 15:50:11 visual_prompt]: Epoch 1 / 100: avg data time: 1.06e+01, avg batch time: 11.6718, average train loss: 6.9791
[11/08 15:50:58 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.3604, average loss: 6.3857
[11/08 15:50:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/08 15:50:58 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0002
[11/08 15:57:49 visual_prompt]: Epoch 2 / 100: avg data time: 1.06e+01, avg batch time: 11.7279, average train loss: 4.7319
[11/08 15:58:36 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.3574, average loss: 1.0649
[11/08 15:58:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 51.03	
[11/08 15:58:36 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0004
[11/08 16:05:27 visual_prompt]: Epoch 3 / 100: avg data time: 1.07e+01, avg batch time: 11.7533, average train loss: 0.9320
[11/08 16:06:15 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.3687, average loss: 0.7129
[11/08 16:06:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.31	
[11/08 16:06:15 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0006
[11/08 16:13:04 visual_prompt]: Epoch 4 / 100: avg data time: 1.06e+01, avg batch time: 11.6985, average train loss: 0.7922
[11/08 16:13:51 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.3605, average loss: 0.7879
[11/08 16:13:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.62	
[11/08 16:13:51 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0008
[11/08 16:20:41 visual_prompt]: Epoch 5 / 100: avg data time: 1.06e+01, avg batch time: 11.7136, average train loss: 0.8139
[11/08 16:21:28 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3326, average loss: 0.6712
[11/08 16:21:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 59.10	
[11/08 16:21:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.001
[11/08 16:28:21 visual_prompt]: Epoch 6 / 100: avg data time: 1.07e+01, avg batch time: 11.7827, average train loss: 0.7711
[11/08 16:29:11 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.3637, average loss: 0.7228
[11/08 16:29:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.26	
[11/08 16:29:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/08 16:36:08 visual_prompt]: Epoch 7 / 100: avg data time: 1.08e+01, avg batch time: 11.9062, average train loss: 0.7319
[11/08 16:36:59 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3659, average loss: 0.8022
[11/08 16:36:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.77	
[11/08 16:36:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/08 16:43:52 visual_prompt]: Epoch 8 / 100: avg data time: 1.07e+01, avg batch time: 11.7848, average train loss: 0.8037
[11/08 16:44:38 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.3585, average loss: 0.7268
[11/08 16:44:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 60.55	
[11/08 16:44:38 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/08 16:51:22 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 11.5420, average train loss: 0.7855
[11/08 16:52:08 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.3666, average loss: 0.7112
[11/08 16:52:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 61.17	
[11/08 16:52:08 visual_prompt]: Best epoch 9: best metric: -0.711
[11/08 16:52:08 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/08 16:58:53 visual_prompt]: Epoch 10 / 100: avg data time: 1.05e+01, avg batch time: 11.5602, average train loss: 0.7169
[11/08 16:59:40 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.3607, average loss: 0.7654
[11/08 16:59:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.63	rocauc: 60.58	
[11/08 16:59:40 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/08 17:06:25 visual_prompt]: Epoch 11 / 100: avg data time: 1.05e+01, avg batch time: 11.5829, average train loss: 0.6711
[11/08 17:07:12 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.3640, average loss: 0.7312
[11/08 17:07:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.53	
[11/08 17:07:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/08 17:14:00 visual_prompt]: Epoch 12 / 100: avg data time: 1.05e+01, avg batch time: 11.6450, average train loss: 0.6936
[11/08 17:14:47 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3652, average loss: 0.7229
[11/08 17:14:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.16	
[11/08 17:14:47 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/08 17:21:42 visual_prompt]: Epoch 13 / 100: avg data time: 1.08e+01, avg batch time: 11.8533, average train loss: 0.7212
[11/08 17:22:30 visual_prompt]: Inference (val):avg data time: 4.88e-05, avg batch time: 0.3633, average loss: 0.7368
[11/08 17:22:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.90	
[11/08 17:22:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/08 17:29:19 visual_prompt]: Epoch 14 / 100: avg data time: 1.06e+01, avg batch time: 11.6720, average train loss: 0.6471
[11/08 17:30:05 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.3674, average loss: 0.6434
[11/08 17:30:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 62.60	
[11/08 17:30:05 visual_prompt]: Best epoch 14: best metric: -0.643
[11/08 17:30:05 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/08 17:36:52 visual_prompt]: Epoch 15 / 100: avg data time: 1.05e+01, avg batch time: 11.6206, average train loss: 0.7018
[11/08 17:37:39 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.3632, average loss: 0.6955
[11/08 17:37:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.30	
[11/08 17:37:39 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/08 17:44:27 visual_prompt]: Epoch 16 / 100: avg data time: 1.05e+01, avg batch time: 11.6509, average train loss: 0.6473
[11/08 17:45:14 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.3666, average loss: 0.9287
[11/08 17:45:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 62.04	
[11/08 17:45:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/08 17:51:53 visual_prompt]: Epoch 17 / 100: avg data time: 1.03e+01, avg batch time: 11.3964, average train loss: 0.6998
[11/08 17:52:39 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.3689, average loss: 0.7083
[11/08 17:52:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 62.37	
[11/08 17:52:39 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/08 17:59:12 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 11.2315, average train loss: 0.6598
[11/08 17:59:57 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.3690, average loss: 0.6687
[11/08 17:59:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 63.06	
[11/08 17:59:57 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/08 18:06:42 visual_prompt]: Epoch 19 / 100: avg data time: 1.05e+01, avg batch time: 11.5604, average train loss: 0.6186
[11/08 18:07:29 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.3596, average loss: 0.7331
[11/08 18:07:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 62.74	
[11/08 18:07:29 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/08 18:14:18 visual_prompt]: Epoch 20 / 100: avg data time: 1.06e+01, avg batch time: 11.6766, average train loss: 0.6756
[11/08 18:15:05 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.3585, average loss: 0.7132
[11/08 18:15:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.61	
[11/08 18:15:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/08 18:21:55 visual_prompt]: Epoch 21 / 100: avg data time: 1.06e+01, avg batch time: 11.7036, average train loss: 0.6154
[11/08 18:22:41 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.3690, average loss: 0.7064
[11/08 18:22:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 61.66	
[11/08 18:22:41 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.000931628240478787
[11/08 18:29:32 visual_prompt]: Epoch 22 / 100: avg data time: 1.06e+01, avg batch time: 11.7186, average train loss: 0.6016
[11/08 18:30:18 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.3671, average loss: 0.7497
[11/08 18:30:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 60.56	
[11/08 18:30:18 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.0009230476262104677
[11/08 18:37:10 visual_prompt]: Epoch 23 / 100: avg data time: 1.06e+01, avg batch time: 11.7450, average train loss: 0.6263
[11/08 18:37:56 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.2768, average loss: 1.0093
[11/08 18:37:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.65	
[11/08 18:37:56 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00091400441557401
[11/08 18:44:41 visual_prompt]: Epoch 24 / 100: avg data time: 1.05e+01, avg batch time: 11.5682, average train loss: 0.5848
[11/08 18:45:27 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.3604, average loss: 0.7263
[11/08 18:45:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 62.03	
[11/08 18:45:27 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0009045084971874737
[11/08 18:52:00 visual_prompt]: Epoch 25 / 100: avg data time: 1.01e+01, avg batch time: 11.2125, average train loss: 0.5785
[11/08 18:52:45 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.3632, average loss: 0.7352
[11/08 18:52:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 60.23	
[11/08 18:52:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.0008945702546981969
[11/08 18:59:13 visual_prompt]: Epoch 26 / 100: avg data time: 9.99e+00, avg batch time: 11.0912, average train loss: 0.5963
[11/08 18:59:58 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.3584, average loss: 0.6996
[11/08 18:59:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 60.71	
[11/08 18:59:58 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0008842005554284296
[11/08 19:06:25 visual_prompt]: Epoch 27 / 100: avg data time: 9.97e+00, avg batch time: 11.0665, average train loss: 0.6058
[11/08 19:07:10 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.3587, average loss: 0.7311
[11/08 19:07:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 60.73	
[11/08 19:07:10 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.000873410738492077
[11/08 19:13:38 visual_prompt]: Epoch 28 / 100: avg data time: 9.98e+00, avg batch time: 11.0815, average train loss: 0.5594
[11/08 19:14:22 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.3652, average loss: 0.7719
[11/08 19:14:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 59.71	
[11/08 19:14:22 visual_prompt]: Stopping early.
[11/08 19:14:22 visual_prompt]: Rank of current process: 0. World size: 1
[11/08 19:14:22 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/08 19:14:22 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.IMGSIZE', '200', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/08 19:14:22 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/08 19:14:22 visual_prompt]: Training with config:
[11/08 19:14:22 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/08 19:14:22 visual_prompt]: Loading training data...
[11/08 19:14:22 visual_prompt]: Constructing mammo-cbis dataset train...
[11/08 19:14:22 visual_prompt]: Loading validation data...
[11/08 19:14:22 visual_prompt]: Constructing mammo-cbis dataset val...
[11/08 19:14:22 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/08 19:14:27 visual_prompt]: Enable all parameters update during training
[11/08 19:14:27 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/08 19:14:27 visual_prompt]: tuned percent:100.000
[11/08 19:14:28 visual_prompt]: Device used for model: 0
[11/08 19:14:28 visual_prompt]: Setting up Evaluator...
[11/08 19:14:28 visual_prompt]: Setting up Trainer...
[11/08 19:14:28 visual_prompt]: 	Setting up the optimizer...
[11/08 19:14:28 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 99, in <module>
    main(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 83, in main
    explore_lrwd_range(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 79, in explore_lrwd_range
    train(cfg, args, test=False)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 44, in train
    trainer.train_classifier(train_loader, val_loader, test_loader)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/engine/trainer.py", line 187, in train_classifier
    train_loss, _ = self.forward_one_batch(X, targets, True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/engine/trainer.py", line 88, in forward_one_batch
    outputs = self.model(inputs)  # (batchsize, num_cls)
              ^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/models/vit_models.py", line 175, in forward
    x = self.enc(x)  # batch_size x self.feat_dim
        ^^^^^^^^^^^
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/models/vit_backbones/vit.py", line 318, in forward
    x, attn_weights = self.transformer(x)
                      ^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/models/vit_backbones/vit.py", line 294, in forward
    encoded, attn_weights = self.encoder(embedding_output)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/models/vit_backbones/vit.py", line 259, in forward
    hidden_states, weights = layer_block(hidden_states)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/models/vit_backbones/vit.py", line 199, in forward
    x, weights = self.attn(x)
                 ^^^^^^^^^^^^
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/models/vit_backbones/vit.py", line 101, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
                       ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 22.16 GiB total capacity; 10.91 GiB already allocated; 56.12 MiB free; 11.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/28 03:23:03 visual_prompt]: Rank of current process: 0. World size: 1
[09/28 03:23:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/28 03:23:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/28 03:23:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/28 03:23:05 visual_prompt]: Training with config:
[09/28 03:23:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/test/seed4796/lr1.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 4796, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/28 03:23:05 visual_prompt]: Loading training data...
2023-09-28 03:23:07.307469: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-28 03:23:09.862577: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-28 03:23:27.541368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[09/28 03:24:00 visual_prompt]: Constructing vtab-dmlab dataset trainval...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800]+validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/28 03:24:02 visual_prompt]: Number of images: 1000
[09/28 03:24:02 visual_prompt]: Number of classes: 6 / 6
[09/28 03:24:02 visual_prompt]: Loading validation data...
[09/28 03:24:02 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/28 03:24:03 visual_prompt]: Number of images: 200
[09/28 03:24:03 visual_prompt]: Number of classes: 6 / 6
[09/28 03:24:03 visual_prompt]: Loading test data...
[09/28 03:24:03 visual_prompt]: Constructing vtab-dmlab dataset test...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split test, from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/28 03:24:42 visual_prompt]: Number of images: 22735
[09/28 03:24:42 visual_prompt]: Number of classes: 6 / 6
[09/28 03:24:42 visual_prompt]: Constructing models...
[09/28 03:24:46 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/28 03:24:46 visual_prompt]: tuned percent:0.540
[09/28 03:25:08 visual_prompt]: Device used for model: 0
[09/28 03:25:08 visual_prompt]: Setting up Evaluator...
[09/28 03:25:08 visual_prompt]: Setting up Trainer...
[09/28 03:25:08 visual_prompt]: 	Setting up the optimizer...
[09/28 03:25:08 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/28 03:25:26 visual_prompt]: Epoch 1 / 100: avg data time: 2.31e-01, avg batch time: 1.0479, average train loss: 2.2430
[09/28 03:25:29 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1669, average loss: 2.2765
[09/28 03:25:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.00	top5: 84.00	
[09/28 03:25:59 visual_prompt]: 	Test 100/356. loss: 2.110, 0.2147 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 03:26:21 visual_prompt]: 	Test 200/356. loss: 2.373, 0.2145 s / batch. (data: 2.93e-05)max mem: 7.80404 GB 
[09/28 03:26:42 visual_prompt]: 	Test 300/356. loss: 2.232, 0.2164 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 03:26:56 visual_prompt]: Inference (test):avg data time: 9.62e-05, avg batch time: 0.2158, average loss: 2.2685
[09/28 03:26:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.32	top5: 77.99	
[09/28 03:26:56 visual_prompt]: Best epoch 1: best metric: 0.150
[09/28 03:26:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/28 03:27:06 visual_prompt]: Epoch 2 / 100: avg data time: 1.05e-01, avg batch time: 0.5606, average train loss: 2.5910
[09/28 03:27:10 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1680, average loss: 1.8326
[09/28 03:27:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 84.00	
[09/28 03:27:33 visual_prompt]: 	Test 100/356. loss: 1.829, 0.2178 s / batch. (data: 3.12e-05)max mem: 7.80404 GB 
[09/28 03:27:55 visual_prompt]: 	Test 200/356. loss: 1.708, 0.2197 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 03:28:17 visual_prompt]: 	Test 300/356. loss: 1.793, 0.2200 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 03:28:31 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.2185, average loss: 1.7898
[09/28 03:28:31 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.93	top5: 88.46	
[09/28 03:28:31 visual_prompt]: Best epoch 2: best metric: 0.230
[09/28 03:28:31 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/28 03:28:42 visual_prompt]: Epoch 3 / 100: avg data time: 1.16e-01, avg batch time: 0.5731, average train loss: 1.8533
[09/28 03:28:45 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1717, average loss: 1.8144
[09/28 03:28:45 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 84.00	
[09/28 03:29:09 visual_prompt]: 	Test 100/356. loss: 1.795, 0.2204 s / batch. (data: 2.88e-05)max mem: 7.80404 GB 
[09/28 03:29:31 visual_prompt]: 	Test 200/356. loss: 1.753, 0.2199 s / batch. (data: 2.81e-05)max mem: 7.80404 GB 
[09/28 03:29:53 visual_prompt]: 	Test 300/356. loss: 1.776, 0.2201 s / batch. (data: 3.05e-05)max mem: 7.80404 GB 
[09/28 03:30:07 visual_prompt]: Inference (test):avg data time: 6.95e-05, avg batch time: 0.2193, average loss: 1.7869
[09/28 03:30:07 visual_prompt]: Classification results with test_vtab-dmlab: top1: 21.18	top5: 88.42	
[09/28 03:30:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/28 03:30:17 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e-01, avg batch time: 0.5616, average train loss: 1.8116
[09/28 03:30:21 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1706, average loss: 1.7779
[09/28 03:30:21 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 83.50	
[09/28 03:30:45 visual_prompt]: 	Test 100/356. loss: 1.805, 0.2196 s / batch. (data: 3.31e-05)max mem: 7.80404 GB 
[09/28 03:31:07 visual_prompt]: 	Test 200/356. loss: 1.728, 0.2202 s / batch. (data: 3.10e-05)max mem: 7.80404 GB 
[09/28 03:31:29 visual_prompt]: 	Test 300/356. loss: 1.765, 0.2198 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 03:31:42 visual_prompt]: Inference (test):avg data time: 3.57e-05, avg batch time: 0.2195, average loss: 1.7682
[09/28 03:31:42 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.59	top5: 85.60	
[09/28 03:31:42 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/28 03:31:53 visual_prompt]: Epoch 5 / 100: avg data time: 1.12e-01, avg batch time: 0.5718, average train loss: 1.8901
[09/28 03:31:56 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1699, average loss: 1.8399
[09/28 03:31:56 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 82.50	
[09/28 03:32:20 visual_prompt]: 	Test 100/356. loss: 1.910, 0.2200 s / batch. (data: 2.81e-05)max mem: 7.80404 GB 
[09/28 03:32:42 visual_prompt]: 	Test 200/356. loss: 1.916, 0.2203 s / batch. (data: 3.15e-05)max mem: 7.80404 GB 
[09/28 03:33:04 visual_prompt]: 	Test 300/356. loss: 1.865, 0.2198 s / batch. (data: 2.67e-05)max mem: 7.80404 GB 
[09/28 03:33:18 visual_prompt]: Inference (test):avg data time: 4.08e-05, avg batch time: 0.2197, average loss: 1.8775
[09/28 03:33:18 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 81.95	
[09/28 03:33:18 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/28 03:33:29 visual_prompt]: Epoch 6 / 100: avg data time: 1.13e-01, avg batch time: 0.5728, average train loss: 1.9508
[09/28 03:33:32 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1705, average loss: 1.8923
[09/28 03:33:32 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.50	
[09/28 03:33:56 visual_prompt]: 	Test 100/356. loss: 1.918, 0.2200 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 03:34:18 visual_prompt]: 	Test 200/356. loss: 2.077, 0.2203 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 03:34:40 visual_prompt]: 	Test 300/356. loss: 1.999, 0.2201 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 03:34:54 visual_prompt]: Inference (test):avg data time: 3.44e-05, avg batch time: 0.2196, average loss: 1.9530
[09/28 03:34:54 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 77.81	
[09/28 03:34:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/28 03:35:04 visual_prompt]: Epoch 7 / 100: avg data time: 1.16e-01, avg batch time: 0.5745, average train loss: 1.8811
[09/28 03:35:08 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1703, average loss: 1.8602
[09/28 03:35:08 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.00	top5: 84.50	
[09/28 03:35:32 visual_prompt]: 	Test 100/356. loss: 1.930, 0.2254 s / batch. (data: 2.98e-05)max mem: 7.80404 GB 
[09/28 03:35:54 visual_prompt]: 	Test 200/356. loss: 2.145, 0.2209 s / batch. (data: 3.15e-05)max mem: 7.80404 GB 
[09/28 03:36:16 visual_prompt]: 	Test 300/356. loss: 1.998, 0.2211 s / batch. (data: 3.19e-05)max mem: 7.80404 GB 
[09/28 03:36:29 visual_prompt]: Inference (test):avg data time: 7.97e-05, avg batch time: 0.2198, average loss: 1.9638
[09/28 03:36:30 visual_prompt]: Classification results with test_vtab-dmlab: top1: 20.22	top5: 77.81	
[09/28 03:36:30 visual_prompt]: Best epoch 7: best metric: 0.260
[09/28 03:36:30 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/28 03:36:40 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e-01, avg batch time: 0.5629, average train loss: 1.9048
[09/28 03:36:43 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1703, average loss: 1.8118
[09/28 03:36:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 85.50	
[09/28 03:37:07 visual_prompt]: 	Test 100/356. loss: 1.911, 0.2198 s / batch. (data: 2.55e-05)max mem: 7.80404 GB 
[09/28 03:37:29 visual_prompt]: 	Test 200/356. loss: 1.898, 0.2218 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 03:37:51 visual_prompt]: 	Test 300/356. loss: 1.929, 0.2206 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 03:38:05 visual_prompt]: Inference (test):avg data time: 3.71e-05, avg batch time: 0.2198, average loss: 1.8676
[09/28 03:38:05 visual_prompt]: Classification results with test_vtab-dmlab: top1: 25.19	top5: 84.67	
[09/28 03:38:05 visual_prompt]: Best epoch 8: best metric: 0.275
[09/28 03:38:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/28 03:38:16 visual_prompt]: Epoch 9 / 100: avg data time: 1.10e-01, avg batch time: 0.5699, average train loss: 1.8473
[09/28 03:38:19 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1703, average loss: 1.8889
[09/28 03:38:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 86.50	
[09/28 03:38:43 visual_prompt]: 	Test 100/356. loss: 1.811, 0.2210 s / batch. (data: 8.01e-05)max mem: 7.80404 GB 
[09/28 03:39:05 visual_prompt]: 	Test 200/356. loss: 1.861, 0.2207 s / batch. (data: 2.67e-05)max mem: 7.80404 GB 
[09/28 03:39:27 visual_prompt]: 	Test 300/356. loss: 1.759, 0.2205 s / batch. (data: 2.72e-05)max mem: 7.80404 GB 
[09/28 03:39:41 visual_prompt]: Inference (test):avg data time: 3.46e-05, avg batch time: 0.2197, average loss: 1.8370
[09/28 03:39:41 visual_prompt]: Classification results with test_vtab-dmlab: top1: 25.17	top5: 86.27	
[09/28 03:39:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/28 03:39:51 visual_prompt]: Epoch 10 / 100: avg data time: 1.07e-01, avg batch time: 0.5665, average train loss: 1.8550
[09/28 03:39:55 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1749, average loss: 1.9779
[09/28 03:39:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 17.50	top5: 84.00	
[09/28 03:40:19 visual_prompt]: 	Test 100/356. loss: 1.988, 0.2192 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 03:40:41 visual_prompt]: 	Test 200/356. loss: 1.811, 0.2196 s / batch. (data: 2.81e-05)max mem: 7.80404 GB 
[09/28 03:41:03 visual_prompt]: 	Test 300/356. loss: 1.902, 0.2205 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 03:41:17 visual_prompt]: Inference (test):avg data time: 3.37e-05, avg batch time: 0.2195, average loss: 1.9297
[09/28 03:41:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.73	top5: 85.40	
[09/28 03:41:17 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/28 03:41:27 visual_prompt]: Epoch 11 / 100: avg data time: 1.16e-01, avg batch time: 0.5752, average train loss: 1.9349
[09/28 03:41:31 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1705, average loss: 3.5797
[09/28 03:41:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 87.00	
[09/28 03:41:55 visual_prompt]: 	Test 100/356. loss: 4.009, 0.2196 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 03:42:17 visual_prompt]: 	Test 200/356. loss: 4.452, 0.2196 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 03:42:39 visual_prompt]: 	Test 300/356. loss: 3.863, 0.2199 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 03:42:52 visual_prompt]: Inference (test):avg data time: 3.38e-05, avg batch time: 0.2197, average loss: 3.9252
[09/28 03:42:52 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 79.67	
[09/28 03:42:52 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/28 03:43:03 visual_prompt]: Epoch 12 / 100: avg data time: 1.22e-01, avg batch time: 0.5804, average train loss: 2.2883
[09/28 03:43:07 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1703, average loss: 2.0627
[09/28 03:43:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 84.00	
[09/28 03:43:30 visual_prompt]: 	Test 100/356. loss: 2.002, 0.2198 s / batch. (data: 2.91e-05)max mem: 7.80404 GB 
[09/28 03:43:52 visual_prompt]: 	Test 200/356. loss: 1.953, 0.2197 s / batch. (data: 2.98e-05)max mem: 7.80404 GB 
[09/28 03:44:14 visual_prompt]: 	Test 300/356. loss: 1.897, 0.2218 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 03:44:28 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.2195, average loss: 2.0016
[09/28 03:44:28 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 85.40	
[09/28 03:44:28 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/28 03:44:39 visual_prompt]: Epoch 13 / 100: avg data time: 1.09e-01, avg batch time: 0.5698, average train loss: 1.8658
[09/28 03:44:42 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1703, average loss: 1.8300
[09/28 03:44:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 85.50	
[09/28 03:45:06 visual_prompt]: 	Test 100/356. loss: 1.842, 0.2205 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 03:45:28 visual_prompt]: 	Test 200/356. loss: 1.745, 0.2216 s / batch. (data: 2.96e-05)max mem: 7.80404 GB 
[09/28 03:45:50 visual_prompt]: 	Test 300/356. loss: 1.824, 0.2222 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 03:46:04 visual_prompt]: Inference (test):avg data time: 3.39e-05, avg batch time: 0.2197, average loss: 1.8105
[09/28 03:46:04 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 84.67	
[09/28 03:46:04 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/28 03:46:15 visual_prompt]: Epoch 14 / 100: avg data time: 1.17e-01, avg batch time: 0.5753, average train loss: 1.8859
[09/28 03:46:18 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1703, average loss: 1.8213
[09/28 03:46:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 85.50	
[09/28 03:46:42 visual_prompt]: 	Test 100/356. loss: 1.935, 0.2195 s / batch. (data: 3.55e-05)max mem: 7.80404 GB 
[09/28 03:47:04 visual_prompt]: 	Test 200/356. loss: 1.910, 0.2207 s / batch. (data: 3.74e-05)max mem: 7.80404 GB 
[09/28 03:47:26 visual_prompt]: 	Test 300/356. loss: 1.894, 0.2210 s / batch. (data: 2.34e-05)max mem: 7.80404 GB 
[09/28 03:47:40 visual_prompt]: Inference (test):avg data time: 3.20e-05, avg batch time: 0.2197, average loss: 1.8808
[09/28 03:47:40 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.67	top5: 84.67	
[09/28 03:47:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/28 03:47:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.20e-01, avg batch time: 0.5794, average train loss: 1.9124
[09/28 03:47:55 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.1705, average loss: 1.8871
[09/28 03:47:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/28 03:48:19 visual_prompt]: 	Test 100/356. loss: 1.872, 0.2201 s / batch. (data: 2.91e-05)max mem: 7.80404 GB 
[09/28 03:48:41 visual_prompt]: 	Test 200/356. loss: 1.708, 0.2198 s / batch. (data: 2.62e-05)max mem: 7.80404 GB 
[09/28 03:49:03 visual_prompt]: 	Test 300/356. loss: 1.836, 0.2207 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 03:49:17 visual_prompt]: Inference (test):avg data time: 8.64e-05, avg batch time: 0.2198, average loss: 1.8230
[09/28 03:49:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.21	top5: 88.42	
[09/28 03:49:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/28 03:49:27 visual_prompt]: Epoch 16 / 100: avg data time: 1.08e-01, avg batch time: 0.5692, average train loss: 1.8738
[09/28 03:49:31 visual_prompt]: Inference (val):avg data time: 1.81e-05, avg batch time: 0.1704, average loss: 1.7640
[09/28 03:49:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 92.50	
[09/28 03:49:55 visual_prompt]: 	Test 100/356. loss: 1.774, 0.2198 s / batch. (data: 2.96e-05)max mem: 7.80404 GB 
[09/28 03:50:17 visual_prompt]: 	Test 200/356. loss: 1.752, 0.2210 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 03:50:39 visual_prompt]: 	Test 300/356. loss: 1.718, 0.2207 s / batch. (data: 2.62e-05)max mem: 7.80404 GB 
[09/28 03:50:53 visual_prompt]: Inference (test):avg data time: 4.11e-05, avg batch time: 0.2197, average loss: 1.7482
[09/28 03:50:53 visual_prompt]: Classification results with test_vtab-dmlab: top1: 19.47	top5: 92.91	
[09/28 03:50:53 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/28 03:51:03 visual_prompt]: Epoch 17 / 100: avg data time: 1.16e-01, avg batch time: 0.5757, average train loss: 1.7888
[09/28 03:51:07 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1699, average loss: 1.9599
[09/28 03:51:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 14.50	top5: 86.50	
[09/28 03:51:31 visual_prompt]: 	Test 100/356. loss: 1.921, 0.2190 s / batch. (data: 2.53e-05)max mem: 7.80404 GB 
[09/28 03:51:53 visual_prompt]: 	Test 200/356. loss: 2.125, 0.2199 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 03:52:15 visual_prompt]: 	Test 300/356. loss: 1.954, 0.2211 s / batch. (data: 2.57e-05)max mem: 7.80404 GB 
[09/28 03:52:29 visual_prompt]: Inference (test):avg data time: 5.32e-05, avg batch time: 0.2198, average loss: 1.9916
[09/28 03:52:29 visual_prompt]: Classification results with test_vtab-dmlab: top1: 15.33	top5: 79.74	
[09/28 03:52:29 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/28 03:52:39 visual_prompt]: Epoch 18 / 100: avg data time: 1.14e-01, avg batch time: 0.5721, average train loss: 1.8371
[09/28 03:52:43 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1706, average loss: 1.7532
[09/28 03:52:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.50	top5: 91.50	
[09/28 03:53:07 visual_prompt]: 	Test 100/356. loss: 1.715, 0.2189 s / batch. (data: 3.12e-05)max mem: 7.80404 GB 
[09/28 03:53:29 visual_prompt]: 	Test 200/356. loss: 1.704, 0.2205 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 03:53:51 visual_prompt]: 	Test 300/356. loss: 1.671, 0.2206 s / batch. (data: 2.96e-05)max mem: 7.80404 GB 
[09/28 03:54:05 visual_prompt]: Inference (test):avg data time: 4.16e-05, avg batch time: 0.2197, average loss: 1.7187
[09/28 03:54:05 visual_prompt]: Classification results with test_vtab-dmlab: top1: 28.83	top5: 89.42	
[09/28 03:54:05 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/28 03:54:16 visual_prompt]: Epoch 19 / 100: avg data time: 1.21e-01, avg batch time: 0.5799, average train loss: 1.7839
[09/28 03:54:19 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1706, average loss: 1.9184
[09/28 03:54:19 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.00	top5: 91.50	
[09/28 03:54:43 visual_prompt]: 	Test 100/356. loss: 1.755, 0.2197 s / batch. (data: 3.03e-05)max mem: 7.80404 GB 
[09/28 03:55:05 visual_prompt]: 	Test 200/356. loss: 1.935, 0.2192 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 03:55:27 visual_prompt]: 	Test 300/356. loss: 1.778, 0.2207 s / batch. (data: 2.74e-05)max mem: 7.80404 GB 
[09/28 03:55:41 visual_prompt]: Inference (test):avg data time: 3.25e-05, avg batch time: 0.2198, average loss: 1.8459
[09/28 03:55:41 visual_prompt]: Classification results with test_vtab-dmlab: top1: 25.30	top5: 93.00	
[09/28 03:55:41 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/28 03:55:51 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e-01, avg batch time: 0.5637, average train loss: 1.6966
[09/28 03:55:55 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1704, average loss: 1.4821
[09/28 03:55:55 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 95.50	
[09/28 03:56:19 visual_prompt]: 	Test 100/356. loss: 1.433, 0.2173 s / batch. (data: 3.12e-05)max mem: 7.80404 GB 
[09/28 03:56:41 visual_prompt]: 	Test 200/356. loss: 1.664, 0.2190 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 03:57:03 visual_prompt]: 	Test 300/356. loss: 1.463, 0.2198 s / batch. (data: 2.98e-05)max mem: 7.80404 GB 
[09/28 03:57:17 visual_prompt]: Inference (test):avg data time: 5.55e-05, avg batch time: 0.2200, average loss: 1.5043
[09/28 03:57:17 visual_prompt]: Classification results with test_vtab-dmlab: top1: 29.36	top5: 95.29	
[09/28 03:57:17 visual_prompt]: Best epoch 20: best metric: 0.320
[09/28 03:57:17 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/28 03:57:27 visual_prompt]: Epoch 21 / 100: avg data time: 1.17e-01, avg batch time: 0.5777, average train loss: 1.8968
[09/28 03:57:31 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1704, average loss: 1.7921
[09/28 03:57:31 visual_prompt]: Classification results with val_vtab-dmlab: top1: 24.00	top5: 84.00	
[09/28 03:57:55 visual_prompt]: 	Test 100/356. loss: 1.769, 0.2192 s / batch. (data: 2.53e-05)max mem: 7.80404 GB 
[09/28 03:58:17 visual_prompt]: 	Test 200/356. loss: 1.755, 0.2196 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 03:58:39 visual_prompt]: 	Test 300/356. loss: 1.760, 0.2205 s / batch. (data: 3.03e-05)max mem: 7.80404 GB 
[09/28 03:58:53 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.2198, average loss: 1.7691
[09/28 03:58:53 visual_prompt]: Classification results with test_vtab-dmlab: top1: 27.02	top5: 88.42	
[09/28 03:58:53 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/28 03:59:03 visual_prompt]: Epoch 22 / 100: avg data time: 1.17e-01, avg batch time: 0.5762, average train loss: 1.8815
[09/28 03:59:07 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1701, average loss: 1.8046
[09/28 03:59:07 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 84.00	
[09/28 03:59:31 visual_prompt]: 	Test 100/356. loss: 1.793, 0.2206 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 03:59:53 visual_prompt]: 	Test 200/356. loss: 1.682, 0.2197 s / batch. (data: 2.72e-05)max mem: 7.80404 GB 
[09/28 04:00:15 visual_prompt]: 	Test 300/356. loss: 1.735, 0.2201 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 04:00:28 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.2195, average loss: 1.7560
[09/28 04:00:28 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 85.41	
[09/28 04:00:28 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/28 04:00:39 visual_prompt]: Epoch 23 / 100: avg data time: 1.19e-01, avg batch time: 0.5788, average train loss: 1.8549
[09/28 04:00:43 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1701, average loss: 1.8231
[09/28 04:00:43 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 84.00	
[09/28 04:01:06 visual_prompt]: 	Test 100/356. loss: 1.799, 0.2184 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 04:01:28 visual_prompt]: 	Test 200/356. loss: 1.604, 0.2202 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 04:01:51 visual_prompt]: 	Test 300/356. loss: 1.645, 0.2206 s / batch. (data: 3.08e-05)max mem: 7.80404 GB 
[09/28 04:02:04 visual_prompt]: Inference (test):avg data time: 3.29e-05, avg batch time: 0.2197, average loss: 1.7274
[09/28 04:02:04 visual_prompt]: Classification results with test_vtab-dmlab: top1: 29.38	top5: 85.40	
[09/28 04:02:04 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/28 04:02:15 visual_prompt]: Epoch 24 / 100: avg data time: 1.11e-01, avg batch time: 0.5705, average train loss: 1.6052
[09/28 04:02:18 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1705, average loss: 1.5094
[09/28 04:02:18 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 97.50	
[09/28 04:02:42 visual_prompt]: 	Test 100/356. loss: 1.606, 0.2200 s / batch. (data: 3.79e-05)max mem: 7.80404 GB 
[09/28 04:03:04 visual_prompt]: 	Test 200/356. loss: 1.583, 0.2200 s / batch. (data: 3.05e-05)max mem: 7.80404 GB 
[09/28 04:03:26 visual_prompt]: 	Test 300/356. loss: 1.503, 0.2206 s / batch. (data: 2.88e-05)max mem: 7.80404 GB 
[09/28 04:03:40 visual_prompt]: Inference (test):avg data time: 3.34e-05, avg batch time: 0.2197, average loss: 1.5436
[09/28 04:03:40 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.78	top5: 96.36	
[09/28 04:03:40 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/28 04:03:50 visual_prompt]: Epoch 25 / 100: avg data time: 1.04e-01, avg batch time: 0.5646, average train loss: 1.7917
[09/28 04:03:54 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1703, average loss: 1.5052
[09/28 04:03:54 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 99.00	
[09/28 04:04:18 visual_prompt]: 	Test 100/356. loss: 1.599, 0.2190 s / batch. (data: 2.74e-05)max mem: 7.80404 GB 
[09/28 04:04:40 visual_prompt]: 	Test 200/356. loss: 1.512, 0.2201 s / batch. (data: 9.11e-05)max mem: 7.80404 GB 
[09/28 04:05:02 visual_prompt]: 	Test 300/356. loss: 1.498, 0.2202 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 04:05:16 visual_prompt]: Inference (test):avg data time: 3.15e-05, avg batch time: 0.2196, average loss: 1.5302
[09/28 04:05:16 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.57	top5: 97.40	
[09/28 04:05:16 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/28 04:05:26 visual_prompt]: Epoch 26 / 100: avg data time: 1.21e-01, avg batch time: 0.5801, average train loss: 1.5309
[09/28 04:05:30 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1701, average loss: 1.5646
[09/28 04:05:30 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 94.00	
[09/28 04:05:54 visual_prompt]: 	Test 100/356. loss: 1.656, 0.2200 s / batch. (data: 2.88e-05)max mem: 7.80404 GB 
[09/28 04:06:16 visual_prompt]: 	Test 200/356. loss: 1.850, 0.2218 s / batch. (data: 3.03e-05)max mem: 7.80404 GB 
[09/28 04:06:38 visual_prompt]: 	Test 300/356. loss: 1.586, 0.2207 s / batch. (data: 7.58e-05)max mem: 7.80404 GB 
[09/28 04:06:52 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.2198, average loss: 1.6460
[09/28 04:06:52 visual_prompt]: Classification results with test_vtab-dmlab: top1: 27.78	top5: 93.46	
[09/28 04:06:52 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/28 04:07:02 visual_prompt]: Epoch 27 / 100: avg data time: 1.06e-01, avg batch time: 0.5659, average train loss: 1.4453
[09/28 04:07:06 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1705, average loss: 1.5867
[09/28 04:07:06 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.50	top5: 95.50	
[09/28 04:07:30 visual_prompt]: 	Test 100/356. loss: 1.427, 0.2189 s / batch. (data: 9.13e-05)max mem: 7.80404 GB 
[09/28 04:07:52 visual_prompt]: 	Test 200/356. loss: 1.581, 0.2200 s / batch. (data: 2.55e-05)max mem: 7.80404 GB 
[09/28 04:08:14 visual_prompt]: 	Test 300/356. loss: 1.488, 0.2214 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 04:08:28 visual_prompt]: Inference (test):avg data time: 3.81e-05, avg batch time: 0.2197, average loss: 1.5386
[09/28 04:08:28 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.00	top5: 97.08	
[09/28 04:08:28 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/28 04:08:38 visual_prompt]: Epoch 28 / 100: avg data time: 1.18e-01, avg batch time: 0.5775, average train loss: 1.5250
[09/28 04:08:42 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1698, average loss: 1.5114
[09/28 04:08:42 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 98.00	
[09/28 04:09:05 visual_prompt]: 	Test 100/356. loss: 1.476, 0.2196 s / batch. (data: 2.74e-05)max mem: 7.80404 GB 
[09/28 04:09:27 visual_prompt]: 	Test 200/356. loss: 1.391, 0.2193 s / batch. (data: 2.29e-05)max mem: 7.80404 GB 
[09/28 04:09:50 visual_prompt]: 	Test 300/356. loss: 1.397, 0.2214 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 04:10:03 visual_prompt]: Inference (test):avg data time: 3.31e-05, avg batch time: 0.2197, average loss: 1.5070
[09/28 04:10:03 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.00	top5: 98.08	
[09/28 04:10:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/28 04:10:14 visual_prompt]: Epoch 29 / 100: avg data time: 1.19e-01, avg batch time: 0.5781, average train loss: 1.5180
[09/28 04:10:17 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1698, average loss: 1.6774
[09/28 04:10:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 98.00	
[09/28 04:10:41 visual_prompt]: 	Test 100/356. loss: 1.587, 0.2200 s / batch. (data: 3.17e-05)max mem: 7.80404 GB 
[09/28 04:11:03 visual_prompt]: 	Test 200/356. loss: 1.585, 0.2206 s / batch. (data: 3.05e-05)max mem: 7.80404 GB 
[09/28 04:11:25 visual_prompt]: 	Test 300/356. loss: 1.445, 0.2209 s / batch. (data: 7.75e-05)max mem: 7.80404 GB 
[09/28 04:11:39 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.2195, average loss: 1.6124
[09/28 04:11:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.62	top5: 97.11	
[09/28 04:11:39 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/28 04:11:49 visual_prompt]: Epoch 30 / 100: avg data time: 1.06e-01, avg batch time: 0.5647, average train loss: 1.4975
[09/28 04:11:53 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1698, average loss: 1.5385
[09/28 04:11:53 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 99.50	
[09/28 04:12:17 visual_prompt]: 	Test 100/356. loss: 1.573, 0.2195 s / batch. (data: 3.17e-05)max mem: 7.80404 GB 
[09/28 04:12:39 visual_prompt]: 	Test 200/356. loss: 1.562, 0.2213 s / batch. (data: 3.03e-05)max mem: 7.80404 GB 
[09/28 04:13:01 visual_prompt]: 	Test 300/356. loss: 1.500, 0.2202 s / batch. (data: 7.20e-05)max mem: 7.80404 GB 
[09/28 04:13:15 visual_prompt]: Inference (test):avg data time: 3.33e-05, avg batch time: 0.2197, average loss: 1.5815
[09/28 04:13:15 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.97	top5: 97.70	
[09/28 04:13:15 visual_prompt]: Best epoch 30: best metric: 0.335
[09/28 04:13:15 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/28 04:13:25 visual_prompt]: Epoch 31 / 100: avg data time: 1.20e-01, avg batch time: 0.5790, average train loss: 1.5148
[09/28 04:13:29 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1698, average loss: 1.5231
[09/28 04:13:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 96.00	
[09/28 04:13:53 visual_prompt]: 	Test 100/356. loss: 1.466, 0.2195 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 04:14:15 visual_prompt]: 	Test 200/356. loss: 1.534, 0.2201 s / batch. (data: 2.98e-05)max mem: 7.80404 GB 
[09/28 04:14:37 visual_prompt]: 	Test 300/356. loss: 1.508, 0.2210 s / batch. (data: 2.55e-05)max mem: 7.80404 GB 
[09/28 04:14:51 visual_prompt]: Inference (test):avg data time: 4.31e-05, avg batch time: 0.2196, average loss: 1.5062
[09/28 04:14:51 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.57	top5: 94.54	
[09/28 04:14:51 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/28 04:15:01 visual_prompt]: Epoch 32 / 100: avg data time: 1.18e-01, avg batch time: 0.5768, average train loss: 1.5374
[09/28 04:15:05 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1698, average loss: 1.5696
[09/28 04:15:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 26.50	top5: 99.00	
[09/28 04:15:29 visual_prompt]: 	Test 100/356. loss: 1.488, 0.2191 s / batch. (data: 2.72e-05)max mem: 7.80404 GB 
[09/28 04:15:51 visual_prompt]: 	Test 200/356. loss: 1.667, 0.2192 s / batch. (data: 2.91e-05)max mem: 7.80404 GB 
[09/28 04:16:13 visual_prompt]: 	Test 300/356. loss: 1.452, 0.2205 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 04:16:27 visual_prompt]: Inference (test):avg data time: 3.16e-05, avg batch time: 0.2196, average loss: 1.5760
[09/28 04:16:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.93	top5: 97.62	
[09/28 04:16:27 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/28 04:16:37 visual_prompt]: Epoch 33 / 100: avg data time: 1.12e-01, avg batch time: 0.5706, average train loss: 1.4393
[09/28 04:16:41 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1697, average loss: 1.4612
[09/28 04:16:41 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 97.50	
[09/28 04:17:05 visual_prompt]: 	Test 100/356. loss: 1.426, 0.2192 s / batch. (data: 2.96e-05)max mem: 7.80404 GB 
[09/28 04:17:27 visual_prompt]: 	Test 200/356. loss: 1.567, 0.2209 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 04:17:49 visual_prompt]: 	Test 300/356. loss: 1.362, 0.2227 s / batch. (data: 3.10e-05)max mem: 7.80404 GB 
[09/28 04:18:03 visual_prompt]: Inference (test):avg data time: 7.65e-05, avg batch time: 0.2198, average loss: 1.4459
[09/28 04:18:03 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.13	top5: 97.66	
[09/28 04:18:03 visual_prompt]: Best epoch 33: best metric: 0.350
[09/28 04:18:03 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/28 04:18:13 visual_prompt]: Epoch 34 / 100: avg data time: 1.15e-01, avg batch time: 0.5740, average train loss: 1.4119
[09/28 04:18:17 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.1698, average loss: 1.5019
[09/28 04:18:17 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 95.00	
[09/28 04:18:40 visual_prompt]: 	Test 100/356. loss: 1.460, 0.2200 s / batch. (data: 3.10e-05)max mem: 7.80404 GB 
[09/28 04:19:02 visual_prompt]: 	Test 200/356. loss: 1.455, 0.2201 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 04:19:25 visual_prompt]: 	Test 300/356. loss: 1.477, 0.2216 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 04:19:38 visual_prompt]: Inference (test):avg data time: 3.49e-05, avg batch time: 0.2197, average loss: 1.5057
[09/28 04:19:39 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.86	top5: 96.32	
[09/28 04:19:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/28 04:19:49 visual_prompt]: Epoch 35 / 100: avg data time: 1.13e-01, avg batch time: 0.5717, average train loss: 1.5749
[09/28 04:19:52 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1702, average loss: 1.4017
[09/28 04:19:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.50	top5: 99.50	
[09/28 04:20:16 visual_prompt]: 	Test 100/356. loss: 1.391, 0.2191 s / batch. (data: 2.98e-05)max mem: 7.80404 GB 
[09/28 04:20:38 visual_prompt]: 	Test 200/356. loss: 1.451, 0.2200 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 04:21:00 visual_prompt]: 	Test 300/356. loss: 1.357, 0.2209 s / batch. (data: 2.55e-05)max mem: 7.80404 GB 
[09/28 04:21:14 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.2199, average loss: 1.4306
[09/28 04:21:14 visual_prompt]: Classification results with test_vtab-dmlab: top1: 27.55	top5: 97.97	
[09/28 04:21:14 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/28 04:21:25 visual_prompt]: Epoch 36 / 100: avg data time: 1.16e-01, avg batch time: 0.5752, average train loss: 1.4677
[09/28 04:21:29 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1694, average loss: 1.4770
[09/28 04:21:29 visual_prompt]: Classification results with val_vtab-dmlab: top1: 32.00	top5: 98.00	
[09/28 04:21:52 visual_prompt]: 	Test 100/356. loss: 1.449, 0.2202 s / batch. (data: 2.93e-05)max mem: 7.80404 GB 
[09/28 04:22:15 visual_prompt]: 	Test 200/356. loss: 1.430, 0.2201 s / batch. (data: 2.38e-05)max mem: 7.80404 GB 
[09/28 04:22:37 visual_prompt]: 	Test 300/356. loss: 1.452, 0.2214 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 04:22:50 visual_prompt]: Inference (test):avg data time: 3.43e-05, avg batch time: 0.2197, average loss: 1.4663
[09/28 04:22:51 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.50	top5: 97.22	
[09/28 04:22:51 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/28 04:23:01 visual_prompt]: Epoch 37 / 100: avg data time: 1.16e-01, avg batch time: 0.5757, average train loss: 1.3657
[09/28 04:23:05 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1699, average loss: 1.6117
[09/28 04:23:05 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 96.00	
[09/28 04:23:29 visual_prompt]: 	Test 100/356. loss: 1.536, 0.2200 s / batch. (data: 3.81e-05)max mem: 7.80404 GB 
[09/28 04:23:51 visual_prompt]: 	Test 200/356. loss: 1.886, 0.2195 s / batch. (data: 2.93e-05)max mem: 7.80404 GB 
[09/28 04:24:13 visual_prompt]: 	Test 300/356. loss: 1.410, 0.2206 s / batch. (data: 2.88e-05)max mem: 7.80404 GB 
[09/28 04:24:26 visual_prompt]: Inference (test):avg data time: 3.31e-05, avg batch time: 0.2196, average loss: 1.6134
[09/28 04:24:27 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.39	top5: 95.84	
[09/28 04:24:27 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/28 04:24:37 visual_prompt]: Epoch 38 / 100: avg data time: 1.12e-01, avg batch time: 0.5721, average train loss: 1.4052
[09/28 04:24:40 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1695, average loss: 1.3357
[09/28 04:24:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 99.50	
[09/28 04:25:04 visual_prompt]: 	Test 100/356. loss: 1.279, 0.2195 s / batch. (data: 2.81e-05)max mem: 7.80404 GB 
[09/28 04:25:26 visual_prompt]: 	Test 200/356. loss: 1.394, 0.2200 s / batch. (data: 2.72e-05)max mem: 7.80404 GB 
[09/28 04:25:48 visual_prompt]: 	Test 300/356. loss: 1.299, 0.2202 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 04:26:02 visual_prompt]: Inference (test):avg data time: 3.33e-05, avg batch time: 0.2196, average loss: 1.3624
[09/28 04:26:02 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.34	top5: 98.11	
[09/28 04:26:02 visual_prompt]: Best epoch 38: best metric: 0.375
[09/28 04:26:02 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/28 04:26:13 visual_prompt]: Epoch 39 / 100: avg data time: 1.10e-01, avg batch time: 0.5706, average train loss: 1.3839
[09/28 04:26:16 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1699, average loss: 1.5706
[09/28 04:26:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 95.50	
[09/28 04:26:40 visual_prompt]: 	Test 100/356. loss: 1.448, 0.2196 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 04:27:02 visual_prompt]: 	Test 200/356. loss: 1.486, 0.2198 s / batch. (data: 2.74e-05)max mem: 7.80404 GB 
[09/28 04:27:24 visual_prompt]: 	Test 300/356. loss: 1.547, 0.2216 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 04:27:38 visual_prompt]: Inference (test):avg data time: 3.22e-05, avg batch time: 0.2197, average loss: 1.5339
[09/28 04:27:38 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.69	top5: 95.20	
[09/28 04:27:38 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/28 04:27:49 visual_prompt]: Epoch 40 / 100: avg data time: 1.17e-01, avg batch time: 0.5750, average train loss: 1.4912
[09/28 04:27:52 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1699, average loss: 1.3361
[09/28 04:27:52 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 99.00	
[09/28 04:28:16 visual_prompt]: 	Test 100/356. loss: 1.389, 0.2203 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 04:28:38 visual_prompt]: 	Test 200/356. loss: 1.478, 0.2195 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 04:29:00 visual_prompt]: 	Test 300/356. loss: 1.387, 0.2204 s / batch. (data: 2.81e-05)max mem: 7.80404 GB 
[09/28 04:29:14 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.2197, average loss: 1.4241
[09/28 04:29:14 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.99	top5: 97.42	
[09/28 04:29:14 visual_prompt]: Best epoch 40: best metric: 0.390
[09/28 04:29:14 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/28 04:29:25 visual_prompt]: Epoch 41 / 100: avg data time: 1.16e-01, avg batch time: 0.5753, average train loss: 1.3695
[09/28 04:29:28 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1695, average loss: 1.2710
[09/28 04:29:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/28 04:29:52 visual_prompt]: 	Test 100/356. loss: 1.272, 0.2191 s / batch. (data: 3.08e-05)max mem: 7.80404 GB 
[09/28 04:30:14 visual_prompt]: 	Test 200/356. loss: 1.449, 0.2201 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 04:30:36 visual_prompt]: 	Test 300/356. loss: 1.392, 0.2216 s / batch. (data: 2.81e-05)max mem: 7.80404 GB 
[09/28 04:30:50 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.2197, average loss: 1.4058
[09/28 04:30:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.69	top5: 97.81	
[09/28 04:30:50 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/28 04:31:00 visual_prompt]: Epoch 42 / 100: avg data time: 1.20e-01, avg batch time: 0.5790, average train loss: 1.3185
[09/28 04:31:04 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1699, average loss: 1.6089
[09/28 04:31:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 92.50	
[09/28 04:31:27 visual_prompt]: 	Test 100/356. loss: 1.564, 0.2198 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 04:31:49 visual_prompt]: 	Test 200/356. loss: 1.415, 0.2199 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 04:32:12 visual_prompt]: 	Test 300/356. loss: 1.640, 0.2200 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 04:32:25 visual_prompt]: Inference (test):avg data time: 3.14e-05, avg batch time: 0.2200, average loss: 1.5765
[09/28 04:32:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.55	top5: 93.33	
[09/28 04:32:25 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/28 04:32:36 visual_prompt]: Epoch 43 / 100: avg data time: 1.13e-01, avg batch time: 0.5729, average train loss: 1.4071
[09/28 04:32:40 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1698, average loss: 1.3868
[09/28 04:32:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.50	top5: 98.50	
[09/28 04:33:04 visual_prompt]: 	Test 100/356. loss: 1.409, 0.2196 s / batch. (data: 5.01e-05)max mem: 7.80404 GB 
[09/28 04:33:26 visual_prompt]: 	Test 200/356. loss: 1.335, 0.2199 s / batch. (data: 2.72e-05)max mem: 7.80404 GB 
[09/28 04:33:48 visual_prompt]: 	Test 300/356. loss: 1.318, 0.2219 s / batch. (data: 4.86e-05)max mem: 7.80404 GB 
[09/28 04:34:01 visual_prompt]: Inference (test):avg data time: 3.12e-05, avg batch time: 0.2198, average loss: 1.4101
[09/28 04:34:02 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.08	top5: 97.70	
[09/28 04:34:02 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/28 04:34:12 visual_prompt]: Epoch 44 / 100: avg data time: 1.15e-01, avg batch time: 0.5733, average train loss: 1.3556
[09/28 04:34:16 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1700, average loss: 1.2310
[09/28 04:34:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 99.00	
[09/28 04:34:39 visual_prompt]: 	Test 100/356. loss: 1.246, 0.2199 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 04:35:01 visual_prompt]: 	Test 200/356. loss: 1.258, 0.2202 s / batch. (data: 2.88e-05)max mem: 7.80404 GB 
[09/28 04:35:23 visual_prompt]: 	Test 300/356. loss: 1.206, 0.2200 s / batch. (data: 3.08e-05)max mem: 7.80404 GB 
[09/28 04:35:37 visual_prompt]: Inference (test):avg data time: 3.24e-05, avg batch time: 0.2198, average loss: 1.2767
[09/28 04:35:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.83	top5: 98.61	
[09/28 04:35:37 visual_prompt]: Best epoch 44: best metric: 0.405
[09/28 04:35:37 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/28 04:35:48 visual_prompt]: Epoch 45 / 100: avg data time: 1.17e-01, avg batch time: 0.5757, average train loss: 1.3597
[09/28 04:35:51 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1697, average loss: 1.3688
[09/28 04:35:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 99.00	
[09/28 04:36:15 visual_prompt]: 	Test 100/356. loss: 1.362, 0.2183 s / batch. (data: 3.05e-05)max mem: 7.80404 GB 
[09/28 04:36:37 visual_prompt]: 	Test 200/356. loss: 1.523, 0.2206 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 04:36:59 visual_prompt]: 	Test 300/356. loss: 1.450, 0.2212 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 04:37:13 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.2198, average loss: 1.4418
[09/28 04:37:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.30	top5: 97.88	
[09/28 04:37:13 visual_prompt]: Best epoch 45: best metric: 0.410
[09/28 04:37:13 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/28 04:37:24 visual_prompt]: Epoch 46 / 100: avg data time: 1.19e-01, avg batch time: 0.5783, average train loss: 1.3389
[09/28 04:37:27 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1699, average loss: 1.4275
[09/28 04:37:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 38.00	top5: 99.00	
[09/28 04:37:51 visual_prompt]: 	Test 100/356. loss: 1.491, 0.2191 s / batch. (data: 3.12e-05)max mem: 7.80404 GB 
[09/28 04:38:13 visual_prompt]: 	Test 200/356. loss: 1.863, 0.2200 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 04:38:35 visual_prompt]: 	Test 300/356. loss: 1.554, 0.2204 s / batch. (data: 2.98e-05)max mem: 7.80404 GB 
[09/28 04:38:49 visual_prompt]: Inference (test):avg data time: 3.50e-05, avg batch time: 0.2198, average loss: 1.5664
[09/28 04:38:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 32.01	top5: 96.49	
[09/28 04:38:49 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/28 04:39:00 visual_prompt]: Epoch 47 / 100: avg data time: 1.16e-01, avg batch time: 0.5757, average train loss: 1.3541
[09/28 04:39:03 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1698, average loss: 1.4982
[09/28 04:39:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.00	top5: 93.50	
[09/28 04:39:27 visual_prompt]: 	Test 100/356. loss: 1.342, 0.2198 s / batch. (data: 2.91e-05)max mem: 7.80404 GB 
[09/28 04:39:49 visual_prompt]: 	Test 200/356. loss: 1.344, 0.2202 s / batch. (data: 2.81e-05)max mem: 7.80404 GB 
[09/28 04:40:11 visual_prompt]: 	Test 300/356. loss: 1.539, 0.2196 s / batch. (data: 2.50e-05)max mem: 7.80404 GB 
[09/28 04:40:25 visual_prompt]: Inference (test):avg data time: 5.39e-05, avg batch time: 0.2198, average loss: 1.5398
[09/28 04:40:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.06	top5: 94.59	
[09/28 04:40:25 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/28 04:40:36 visual_prompt]: Epoch 48 / 100: avg data time: 1.10e-01, avg batch time: 0.5700, average train loss: 1.3669
[09/28 04:40:39 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1700, average loss: 1.2863
[09/28 04:40:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 39.50	top5: 99.00	
[09/28 04:41:03 visual_prompt]: 	Test 100/356. loss: 1.273, 0.2200 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 04:41:25 visual_prompt]: 	Test 200/356. loss: 1.580, 0.2200 s / batch. (data: 2.98e-05)max mem: 7.80404 GB 
[09/28 04:41:47 visual_prompt]: 	Test 300/356. loss: 1.254, 0.2216 s / batch. (data: 2.88e-05)max mem: 7.80404 GB 
[09/28 04:42:01 visual_prompt]: Inference (test):avg data time: 3.29e-05, avg batch time: 0.2199, average loss: 1.4106
[09/28 04:42:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.70	top5: 97.18	
[09/28 04:42:01 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/28 04:42:11 visual_prompt]: Epoch 49 / 100: avg data time: 1.19e-01, avg batch time: 0.5784, average train loss: 1.3203
[09/28 04:42:15 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1696, average loss: 1.3562
[09/28 04:42:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 96.50	
[09/28 04:42:39 visual_prompt]: 	Test 100/356. loss: 1.380, 0.2193 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 04:43:01 visual_prompt]: 	Test 200/356. loss: 1.703, 0.2203 s / batch. (data: 2.93e-05)max mem: 7.80404 GB 
[09/28 04:43:23 visual_prompt]: 	Test 300/356. loss: 1.389, 0.2199 s / batch. (data: 2.93e-05)max mem: 7.80404 GB 
[09/28 04:43:37 visual_prompt]: Inference (test):avg data time: 3.36e-05, avg batch time: 0.2197, average loss: 1.4564
[09/28 04:43:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.60	top5: 96.99	
[09/28 04:43:37 visual_prompt]: Best epoch 49: best metric: 0.425
[09/28 04:43:37 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/28 04:43:47 visual_prompt]: Epoch 50 / 100: avg data time: 1.16e-01, avg batch time: 0.5745, average train loss: 1.4180
[09/28 04:43:51 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1700, average loss: 1.5996
[09/28 04:43:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 97.00	
[09/28 04:44:15 visual_prompt]: 	Test 100/356. loss: 1.517, 0.2194 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 04:44:37 visual_prompt]: 	Test 200/356. loss: 1.423, 0.2197 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 04:44:59 visual_prompt]: 	Test 300/356. loss: 1.431, 0.2205 s / batch. (data: 9.01e-05)max mem: 7.80404 GB 
[09/28 04:45:13 visual_prompt]: Inference (test):avg data time: 3.35e-05, avg batch time: 0.2198, average loss: 1.5394
[09/28 04:45:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.68	top5: 98.31	
[09/28 04:45:13 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/28 04:45:23 visual_prompt]: Epoch 51 / 100: avg data time: 1.20e-01, avg batch time: 0.5789, average train loss: 1.3779
[09/28 04:45:27 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1702, average loss: 1.2571
[09/28 04:45:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 100.00	
[09/28 04:45:50 visual_prompt]: 	Test 100/356. loss: 1.424, 0.2194 s / batch. (data: 2.91e-05)max mem: 7.80404 GB 
[09/28 04:46:12 visual_prompt]: 	Test 200/356. loss: 1.319, 0.2197 s / batch. (data: 1.05e-04)max mem: 7.80404 GB 
[09/28 04:46:35 visual_prompt]: 	Test 300/356. loss: 1.329, 0.2207 s / batch. (data: 2.41e-05)max mem: 7.80404 GB 
[09/28 04:46:48 visual_prompt]: Inference (test):avg data time: 4.01e-05, avg batch time: 0.2197, average loss: 1.4329
[09/28 04:46:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 36.65	top5: 97.47	
[09/28 04:46:49 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/28 04:46:59 visual_prompt]: Epoch 52 / 100: avg data time: 1.10e-01, avg batch time: 0.5693, average train loss: 1.3301
[09/28 04:47:02 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1701, average loss: 1.2744
[09/28 04:47:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.00	top5: 98.50	
[09/28 04:47:26 visual_prompt]: 	Test 100/356. loss: 1.316, 0.2205 s / batch. (data: 2.62e-05)max mem: 7.80404 GB 
[09/28 04:47:48 visual_prompt]: 	Test 200/356. loss: 1.418, 0.2204 s / batch. (data: 2.67e-05)max mem: 7.80404 GB 
[09/28 04:48:10 visual_prompt]: 	Test 300/356. loss: 1.251, 0.2204 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 04:48:24 visual_prompt]: Inference (test):avg data time: 3.71e-05, avg batch time: 0.2197, average loss: 1.3281
[09/28 04:48:24 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.96	top5: 98.23	
[09/28 04:48:24 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/28 04:48:34 visual_prompt]: Epoch 53 / 100: avg data time: 1.15e-01, avg batch time: 0.5738, average train loss: 1.2225
[09/28 04:48:38 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1702, average loss: 1.1701
[09/28 04:48:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 99.50	
[09/28 04:49:02 visual_prompt]: 	Test 100/356. loss: 1.307, 0.2201 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 04:49:24 visual_prompt]: 	Test 200/356. loss: 1.406, 0.2199 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 04:49:46 visual_prompt]: 	Test 300/356. loss: 1.263, 0.2201 s / batch. (data: 3.19e-05)max mem: 7.80404 GB 
[09/28 04:50:00 visual_prompt]: Inference (test):avg data time: 3.10e-05, avg batch time: 0.2199, average loss: 1.3565
[09/28 04:50:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.28	top5: 98.39	
[09/28 04:50:00 visual_prompt]: Best epoch 53: best metric: 0.445
[09/28 04:50:00 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/28 04:50:10 visual_prompt]: Epoch 54 / 100: avg data time: 1.19e-01, avg batch time: 0.5792, average train loss: 1.2785
[09/28 04:50:14 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1700, average loss: 1.3428
[09/28 04:50:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 99.50	
[09/28 04:50:38 visual_prompt]: 	Test 100/356. loss: 1.241, 0.2200 s / batch. (data: 3.29e-05)max mem: 7.80404 GB 
[09/28 04:51:00 visual_prompt]: 	Test 200/356. loss: 1.275, 0.2202 s / batch. (data: 3.27e-05)max mem: 7.80404 GB 
[09/28 04:51:22 visual_prompt]: 	Test 300/356. loss: 1.240, 0.2206 s / batch. (data: 2.96e-05)max mem: 7.80404 GB 
[09/28 04:51:36 visual_prompt]: Inference (test):avg data time: 3.19e-05, avg batch time: 0.2197, average loss: 1.3679
[09/28 04:51:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.68	top5: 98.30	
[09/28 04:51:36 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/28 04:51:47 visual_prompt]: Epoch 55 / 100: avg data time: 1.22e-01, avg batch time: 0.5809, average train loss: 1.2192
[09/28 04:51:50 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1705, average loss: 1.2031
[09/28 04:51:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 100.00	
[09/28 04:52:14 visual_prompt]: 	Test 100/356. loss: 1.381, 0.2208 s / batch. (data: 2.96e-05)max mem: 7.80404 GB 
[09/28 04:52:36 visual_prompt]: 	Test 200/356. loss: 1.405, 0.2210 s / batch. (data: 3.05e-05)max mem: 7.80404 GB 
[09/28 04:52:58 visual_prompt]: 	Test 300/356. loss: 1.365, 0.2204 s / batch. (data: 2.96e-05)max mem: 7.80404 GB 
[09/28 04:53:12 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.2196, average loss: 1.4127
[09/28 04:53:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.07	top5: 97.88	
[09/28 04:53:12 visual_prompt]: Best epoch 55: best metric: 0.460
[09/28 04:53:12 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/28 04:53:22 visual_prompt]: Epoch 56 / 100: avg data time: 1.12e-01, avg batch time: 0.5719, average train loss: 1.1903
[09/28 04:53:26 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1701, average loss: 1.1845
[09/28 04:53:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 98.00	
[09/28 04:53:50 visual_prompt]: 	Test 100/356. loss: 1.146, 0.2192 s / batch. (data: 2.50e-05)max mem: 7.80404 GB 
[09/28 04:54:12 visual_prompt]: 	Test 200/356. loss: 1.544, 0.2207 s / batch. (data: 2.91e-05)max mem: 7.80404 GB 
[09/28 04:54:34 visual_prompt]: 	Test 300/356. loss: 1.384, 0.2198 s / batch. (data: 2.74e-05)max mem: 7.80404 GB 
[09/28 04:54:48 visual_prompt]: Inference (test):avg data time: 3.24e-05, avg batch time: 0.2197, average loss: 1.3664
[09/28 04:54:48 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.67	top5: 98.10	
[09/28 04:54:48 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/28 04:54:58 visual_prompt]: Epoch 57 / 100: avg data time: 1.22e-01, avg batch time: 0.5818, average train loss: 1.2691
[09/28 04:55:02 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1697, average loss: 1.2621
[09/28 04:55:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 45.50	top5: 98.50	
[09/28 04:55:26 visual_prompt]: 	Test 100/356. loss: 1.342, 0.2198 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 04:55:48 visual_prompt]: 	Test 200/356. loss: 1.502, 0.2199 s / batch. (data: 2.91e-05)max mem: 7.80404 GB 
[09/28 04:56:10 visual_prompt]: 	Test 300/356. loss: 1.295, 0.2201 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 04:56:24 visual_prompt]: Inference (test):avg data time: 3.19e-05, avg batch time: 0.2198, average loss: 1.3530
[09/28 04:56:24 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.92	top5: 97.68	
[09/28 04:56:24 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/28 04:56:34 visual_prompt]: Epoch 58 / 100: avg data time: 1.16e-01, avg batch time: 0.5755, average train loss: 1.2189
[09/28 04:56:38 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1703, average loss: 1.1318
[09/28 04:56:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 98.50	
[09/28 04:57:02 visual_prompt]: 	Test 100/356. loss: 1.173, 0.2193 s / batch. (data: 2.88e-05)max mem: 7.80404 GB 
[09/28 04:57:24 visual_prompt]: 	Test 200/356. loss: 1.326, 0.2206 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 04:57:46 visual_prompt]: 	Test 300/356. loss: 1.359, 0.2214 s / batch. (data: 2.38e-05)max mem: 7.80404 GB 
[09/28 04:58:00 visual_prompt]: Inference (test):avg data time: 3.20e-05, avg batch time: 0.2198, average loss: 1.3102
[09/28 04:58:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.85	top5: 97.59	
[09/28 04:58:00 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/28 04:58:10 visual_prompt]: Epoch 59 / 100: avg data time: 1.11e-01, avg batch time: 0.5700, average train loss: 1.1808
[09/28 04:58:14 visual_prompt]: Inference (val):avg data time: 1.99e-05, avg batch time: 0.1699, average loss: 1.0856
[09/28 04:58:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 49.00	top5: 99.00	
[09/28 04:58:37 visual_prompt]: 	Test 100/356. loss: 1.140, 0.2195 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 04:58:59 visual_prompt]: 	Test 200/356. loss: 1.360, 0.2202 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 04:59:22 visual_prompt]: 	Test 300/356. loss: 1.280, 0.2205 s / batch. (data: 2.41e-05)max mem: 7.80404 GB 
[09/28 04:59:36 visual_prompt]: Inference (test):avg data time: 6.74e-05, avg batch time: 0.2198, average loss: 1.2699
[09/28 04:59:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.17	top5: 98.39	
[09/28 04:59:36 visual_prompt]: Best epoch 59: best metric: 0.490
[09/28 04:59:36 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/28 04:59:46 visual_prompt]: Epoch 60 / 100: avg data time: 1.15e-01, avg batch time: 0.5748, average train loss: 1.1717
[09/28 04:59:50 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1698, average loss: 1.2679
[09/28 04:59:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.50	top5: 98.50	
[09/28 05:00:14 visual_prompt]: 	Test 100/356. loss: 1.302, 0.2199 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 05:00:36 visual_prompt]: 	Test 200/356. loss: 1.340, 0.2198 s / batch. (data: 2.38e-05)max mem: 7.80404 GB 
[09/28 05:00:58 visual_prompt]: 	Test 300/356. loss: 1.346, 0.2219 s / batch. (data: 2.93e-05)max mem: 7.80404 GB 
[09/28 05:01:12 visual_prompt]: Inference (test):avg data time: 7.68e-05, avg batch time: 0.2199, average loss: 1.4038
[09/28 05:01:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.78	top5: 98.19	
[09/28 05:01:12 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/28 05:01:22 visual_prompt]: Epoch 61 / 100: avg data time: 1.16e-01, avg batch time: 0.5756, average train loss: 1.2244
[09/28 05:01:26 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1696, average loss: 1.0814
[09/28 05:01:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 48.00	top5: 99.00	
[09/28 05:01:50 visual_prompt]: 	Test 100/356. loss: 1.247, 0.2197 s / batch. (data: 2.93e-05)max mem: 7.80404 GB 
[09/28 05:02:12 visual_prompt]: 	Test 200/356. loss: 1.387, 0.2205 s / batch. (data: 2.67e-05)max mem: 7.80404 GB 
[09/28 05:02:34 visual_prompt]: 	Test 300/356. loss: 1.242, 0.2211 s / batch. (data: 2.74e-05)max mem: 7.80404 GB 
[09/28 05:02:48 visual_prompt]: Inference (test):avg data time: 8.91e-05, avg batch time: 0.2198, average loss: 1.3083
[09/28 05:02:48 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.72	top5: 98.40	
[09/28 05:02:48 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/28 05:02:58 visual_prompt]: Epoch 62 / 100: avg data time: 1.18e-01, avg batch time: 0.5773, average train loss: 1.1653
[09/28 05:03:02 visual_prompt]: Inference (val):avg data time: 1.99e-05, avg batch time: 0.1701, average loss: 1.2233
[09/28 05:03:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.00	top5: 98.00	
[09/28 05:03:25 visual_prompt]: 	Test 100/356. loss: 1.207, 0.2192 s / batch. (data: 2.67e-05)max mem: 7.80404 GB 
[09/28 05:03:48 visual_prompt]: 	Test 200/356. loss: 1.262, 0.2201 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 05:04:10 visual_prompt]: 	Test 300/356. loss: 1.288, 0.2205 s / batch. (data: 2.60e-05)max mem: 7.80404 GB 
[09/28 05:04:23 visual_prompt]: Inference (test):avg data time: 3.94e-05, avg batch time: 0.2198, average loss: 1.3421
[09/28 05:04:24 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.65	top5: 97.54	
[09/28 05:04:24 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/28 05:04:34 visual_prompt]: Epoch 63 / 100: avg data time: 1.18e-01, avg batch time: 0.5773, average train loss: 1.1986
[09/28 05:04:38 visual_prompt]: Inference (val):avg data time: 1.66e-05, avg batch time: 0.1695, average loss: 1.1431
[09/28 05:04:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 43.50	top5: 99.00	
[09/28 05:05:01 visual_prompt]: 	Test 100/356. loss: 1.131, 0.2198 s / batch. (data: 2.26e-05)max mem: 7.80404 GB 
[09/28 05:05:24 visual_prompt]: 	Test 200/356. loss: 1.411, 0.2202 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 05:05:46 visual_prompt]: 	Test 300/356. loss: 1.292, 0.2207 s / batch. (data: 2.57e-05)max mem: 7.80404 GB 
[09/28 05:05:59 visual_prompt]: Inference (test):avg data time: 1.07e-04, avg batch time: 0.2198, average loss: 1.3408
[09/28 05:05:59 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.49	top5: 98.42	
[09/28 05:05:59 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/28 05:06:10 visual_prompt]: Epoch 64 / 100: avg data time: 1.11e-01, avg batch time: 0.5715, average train loss: 1.1878
[09/28 05:06:13 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1706, average loss: 1.2446
[09/28 05:06:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 97.00	
[09/28 05:06:37 visual_prompt]: 	Test 100/356. loss: 1.328, 0.2196 s / batch. (data: 3.53e-05)max mem: 7.80404 GB 
[09/28 05:06:59 visual_prompt]: 	Test 200/356. loss: 1.268, 0.2206 s / batch. (data: 2.74e-05)max mem: 7.80404 GB 
[09/28 05:07:21 visual_prompt]: 	Test 300/356. loss: 1.514, 0.2213 s / batch. (data: 2.74e-05)max mem: 7.80404 GB 
[09/28 05:07:35 visual_prompt]: Inference (test):avg data time: 5.25e-05, avg batch time: 0.2197, average loss: 1.3941
[09/28 05:07:35 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.02	top5: 96.60	
[09/28 05:07:35 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/28 05:07:46 visual_prompt]: Epoch 65 / 100: avg data time: 1.20e-01, avg batch time: 0.5786, average train loss: 1.1198
[09/28 05:07:50 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1705, average loss: 1.0999
[09/28 05:07:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.00	top5: 99.00	
[09/28 05:08:14 visual_prompt]: 	Test 100/356. loss: 1.226, 0.2192 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 05:08:36 visual_prompt]: 	Test 200/356. loss: 1.538, 0.2206 s / batch. (data: 2.98e-05)max mem: 7.80404 GB 
[09/28 05:08:58 visual_prompt]: 	Test 300/356. loss: 1.375, 0.2211 s / batch. (data: 2.72e-05)max mem: 7.80404 GB 
[09/28 05:09:12 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.2198, average loss: 1.3650
[09/28 05:09:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.56	top5: 98.35	
[09/28 05:09:12 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/28 05:09:22 visual_prompt]: Epoch 66 / 100: avg data time: 1.17e-01, avg batch time: 0.5775, average train loss: 1.1106
[09/28 05:09:26 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1704, average loss: 1.2180
[09/28 05:09:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 40.50	top5: 98.50	
[09/28 05:09:49 visual_prompt]: 	Test 100/356. loss: 1.420, 0.2195 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 05:10:12 visual_prompt]: 	Test 200/356. loss: 1.327, 0.2213 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 05:10:34 visual_prompt]: 	Test 300/356. loss: 1.413, 0.2206 s / batch. (data: 2.50e-05)max mem: 7.80404 GB 
[09/28 05:10:48 visual_prompt]: Inference (test):avg data time: 3.25e-05, avg batch time: 0.2198, average loss: 1.4490
[09/28 05:10:48 visual_prompt]: Classification results with test_vtab-dmlab: top1: 37.44	top5: 97.37	
[09/28 05:10:48 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/28 05:10:58 visual_prompt]: Epoch 67 / 100: avg data time: 1.30e-01, avg batch time: 0.5884, average train loss: 1.1201
[09/28 05:11:02 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1704, average loss: 0.9805
[09/28 05:11:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 57.50	top5: 99.50	
[09/28 05:11:26 visual_prompt]: 	Test 100/356. loss: 1.277, 0.2191 s / batch. (data: 2.74e-05)max mem: 7.80404 GB 
[09/28 05:11:48 visual_prompt]: 	Test 200/356. loss: 1.710, 0.2199 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 05:12:10 visual_prompt]: 	Test 300/356. loss: 1.339, 0.2207 s / batch. (data: 3.08e-05)max mem: 7.80404 GB 
[09/28 05:12:24 visual_prompt]: Inference (test):avg data time: 3.05e-05, avg batch time: 0.2197, average loss: 1.3822
[09/28 05:12:24 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.79	top5: 97.78	
[09/28 05:12:24 visual_prompt]: Best epoch 67: best metric: 0.575
[09/28 05:12:24 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/28 05:12:35 visual_prompt]: Epoch 68 / 100: avg data time: 1.18e-01, avg batch time: 0.5773, average train loss: 1.0230
[09/28 05:12:38 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1696, average loss: 1.3214
[09/28 05:12:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 42.50	top5: 99.50	
[09/28 05:13:02 visual_prompt]: 	Test 100/356. loss: 1.578, 0.2196 s / batch. (data: 3.03e-05)max mem: 7.80404 GB 
[09/28 05:13:24 visual_prompt]: 	Test 200/356. loss: 2.267, 0.2199 s / batch. (data: 2.53e-05)max mem: 7.80404 GB 
[09/28 05:13:46 visual_prompt]: 	Test 300/356. loss: 1.692, 0.2203 s / batch. (data: 2.67e-05)max mem: 7.80404 GB 
[09/28 05:14:00 visual_prompt]: Inference (test):avg data time: 6.36e-05, avg batch time: 0.2197, average loss: 1.7472
[09/28 05:14:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 33.58	top5: 93.34	
[09/28 05:14:00 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/28 05:14:10 visual_prompt]: Epoch 69 / 100: avg data time: 1.05e-01, avg batch time: 0.5668, average train loss: 1.1272
[09/28 05:14:14 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1700, average loss: 0.9768
[09/28 05:14:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 53.00	top5: 100.00	
[09/28 05:14:39 visual_prompt]: 	Test 100/356. loss: 1.103, 0.2199 s / batch. (data: 2.67e-05)max mem: 7.80404 GB 
[09/28 05:15:01 visual_prompt]: 	Test 200/356. loss: 1.309, 0.2200 s / batch. (data: 2.91e-05)max mem: 7.80404 GB 
[09/28 05:15:23 visual_prompt]: 	Test 300/356. loss: 1.174, 0.2210 s / batch. (data: 6.51e-05)max mem: 7.80404 GB 
[09/28 05:15:37 visual_prompt]: Inference (test):avg data time: 4.58e-05, avg batch time: 0.2214, average loss: 1.2584
[09/28 05:15:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.11	top5: 98.46	
[09/28 05:15:37 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/28 05:15:47 visual_prompt]: Epoch 70 / 100: avg data time: 1.18e-01, avg batch time: 0.5778, average train loss: 1.0216
[09/28 05:15:51 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1698, average loss: 1.0974
[09/28 05:15:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 99.00	
[09/28 05:16:15 visual_prompt]: 	Test 100/356. loss: 1.355, 0.2193 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 05:16:37 visual_prompt]: 	Test 200/356. loss: 1.679, 0.2201 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 05:16:59 visual_prompt]: 	Test 300/356. loss: 1.613, 0.2207 s / batch. (data: 2.67e-05)max mem: 7.80404 GB 
[09/28 05:17:13 visual_prompt]: Inference (test):avg data time: 7.04e-05, avg batch time: 0.2200, average loss: 1.4788
[09/28 05:17:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.30	top5: 97.51	
[09/28 05:17:13 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/28 05:17:23 visual_prompt]: Epoch 71 / 100: avg data time: 1.16e-01, avg batch time: 0.5759, average train loss: 1.2334
[09/28 05:17:27 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1700, average loss: 1.1202
[09/28 05:17:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 44.50	top5: 99.00	
[09/28 05:17:51 visual_prompt]: 	Test 100/356. loss: 1.185, 0.2193 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 05:18:13 visual_prompt]: 	Test 200/356. loss: 1.280, 0.2222 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 05:18:35 visual_prompt]: 	Test 300/356. loss: 1.173, 0.2209 s / batch. (data: 6.41e-05)max mem: 7.80404 GB 
[09/28 05:18:49 visual_prompt]: Inference (test):avg data time: 3.19e-05, avg batch time: 0.2198, average loss: 1.3039
[09/28 05:18:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 43.93	top5: 97.55	
[09/28 05:18:49 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/28 05:18:59 visual_prompt]: Epoch 72 / 100: avg data time: 1.18e-01, avg batch time: 0.5795, average train loss: 1.0667
[09/28 05:19:03 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1701, average loss: 1.1523
[09/28 05:19:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 46.50	top5: 99.50	
[09/28 05:19:27 visual_prompt]: 	Test 100/356. loss: 1.196, 0.2196 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 05:19:49 visual_prompt]: 	Test 200/356. loss: 1.525, 0.2204 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 05:20:11 visual_prompt]: 	Test 300/356. loss: 1.324, 0.2206 s / batch. (data: 2.38e-05)max mem: 7.80404 GB 
[09/28 05:20:25 visual_prompt]: Inference (test):avg data time: 3.49e-05, avg batch time: 0.2198, average loss: 1.4212
[09/28 05:20:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.18	top5: 97.78	
[09/28 05:20:25 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/28 05:20:35 visual_prompt]: Epoch 73 / 100: avg data time: 1.18e-01, avg batch time: 0.5779, average train loss: 1.0595
[09/28 05:20:39 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1702, average loss: 0.9889
[09/28 05:20:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 57.00	top5: 99.50	
[09/28 05:21:03 visual_prompt]: 	Test 100/356. loss: 1.278, 0.2201 s / batch. (data: 2.55e-05)max mem: 7.80404 GB 
[09/28 05:21:25 visual_prompt]: 	Test 200/356. loss: 1.414, 0.2209 s / batch. (data: 2.72e-05)max mem: 7.80404 GB 
[09/28 05:21:47 visual_prompt]: 	Test 300/356. loss: 1.322, 0.2209 s / batch. (data: 3.17e-05)max mem: 7.80404 GB 
[09/28 05:22:01 visual_prompt]: Inference (test):avg data time: 3.52e-05, avg batch time: 0.2198, average loss: 1.3665
[09/28 05:22:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.68	top5: 98.17	
[09/28 05:22:01 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/28 05:22:11 visual_prompt]: Epoch 74 / 100: avg data time: 1.15e-01, avg batch time: 0.5741, average train loss: 0.9805
[09/28 05:22:15 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1726, average loss: 1.0919
[09/28 05:22:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 50.00	top5: 99.50	
[09/28 05:22:39 visual_prompt]: 	Test 100/356. loss: 1.424, 0.2193 s / batch. (data: 2.31e-05)max mem: 7.80404 GB 
[09/28 05:23:01 visual_prompt]: 	Test 200/356. loss: 1.593, 0.2203 s / batch. (data: 2.96e-05)max mem: 7.80404 GB 
[09/28 05:23:23 visual_prompt]: 	Test 300/356. loss: 1.451, 0.2209 s / batch. (data: 3.67e-05)max mem: 7.80404 GB 
[09/28 05:23:37 visual_prompt]: Inference (test):avg data time: 3.58e-05, avg batch time: 0.2197, average loss: 1.4525
[09/28 05:23:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.47	top5: 97.98	
[09/28 05:23:37 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/28 05:23:47 visual_prompt]: Epoch 75 / 100: avg data time: 1.18e-01, avg batch time: 0.5766, average train loss: 0.9910
[09/28 05:23:51 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1702, average loss: 1.0525
[09/28 05:23:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 53.00	top5: 100.00	
[09/28 05:24:15 visual_prompt]: 	Test 100/356. loss: 1.459, 0.2197 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 05:24:37 visual_prompt]: 	Test 200/356. loss: 1.820, 0.2207 s / batch. (data: 3.79e-05)max mem: 7.80404 GB 
[09/28 05:24:59 visual_prompt]: 	Test 300/356. loss: 1.475, 0.2204 s / batch. (data: 2.81e-05)max mem: 7.80404 GB 
[09/28 05:25:13 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.2198, average loss: 1.5367
[09/28 05:25:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.45	top5: 96.88	
[09/28 05:25:13 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/28 05:25:23 visual_prompt]: Epoch 76 / 100: avg data time: 1.18e-01, avg batch time: 0.5776, average train loss: 0.9712
[09/28 05:25:27 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1700, average loss: 0.8479
[09/28 05:25:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 59.50	top5: 99.50	
[09/28 05:25:51 visual_prompt]: 	Test 100/356. loss: 1.279, 0.2203 s / batch. (data: 3.03e-05)max mem: 7.80404 GB 
[09/28 05:26:13 visual_prompt]: 	Test 200/356. loss: 1.394, 0.2198 s / batch. (data: 3.31e-05)max mem: 7.80404 GB 
[09/28 05:26:35 visual_prompt]: 	Test 300/356. loss: 1.222, 0.2197 s / batch. (data: 7.53e-05)max mem: 7.80404 GB 
[09/28 05:26:49 visual_prompt]: Inference (test):avg data time: 3.29e-05, avg batch time: 0.2197, average loss: 1.3925
[09/28 05:26:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.29	top5: 98.10	
[09/28 05:26:49 visual_prompt]: Best epoch 76: best metric: 0.595
[09/28 05:26:49 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/28 05:26:59 visual_prompt]: Epoch 77 / 100: avg data time: 1.14e-01, avg batch time: 0.5734, average train loss: 0.9312
[09/28 05:27:03 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1697, average loss: 0.9260
[09/28 05:27:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 51.50	top5: 99.00	
[09/28 05:27:27 visual_prompt]: 	Test 100/356. loss: 1.228, 0.2197 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 05:27:49 visual_prompt]: 	Test 200/356. loss: 1.507, 0.2203 s / batch. (data: 2.91e-05)max mem: 7.80404 GB 
[09/28 05:28:11 visual_prompt]: 	Test 300/356. loss: 1.245, 0.2204 s / batch. (data: 2.55e-05)max mem: 7.80404 GB 
[09/28 05:28:25 visual_prompt]: Inference (test):avg data time: 3.53e-05, avg batch time: 0.2198, average loss: 1.4102
[09/28 05:28:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.46	top5: 97.74	
[09/28 05:28:25 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/28 05:28:35 visual_prompt]: Epoch 78 / 100: avg data time: 1.15e-01, avg batch time: 0.5750, average train loss: 0.9018
[09/28 05:28:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1703, average loss: 0.8969
[09/28 05:28:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 57.50	top5: 100.00	
[09/28 05:29:02 visual_prompt]: 	Test 100/356. loss: 1.379, 0.2199 s / batch. (data: 2.60e-05)max mem: 7.80404 GB 
[09/28 05:29:25 visual_prompt]: 	Test 200/356. loss: 1.586, 0.2202 s / batch. (data: 3.05e-05)max mem: 7.80404 GB 
[09/28 05:29:47 visual_prompt]: 	Test 300/356. loss: 1.220, 0.2223 s / batch. (data: 2.96e-05)max mem: 7.80404 GB 
[09/28 05:30:00 visual_prompt]: Inference (test):avg data time: 3.09e-05, avg batch time: 0.2198, average loss: 1.5281
[09/28 05:30:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 38.39	top5: 97.62	
[09/28 05:30:00 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/28 05:30:11 visual_prompt]: Epoch 79 / 100: avg data time: 1.14e-01, avg batch time: 0.5743, average train loss: 0.8499
[09/28 05:30:15 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1698, average loss: 0.7632
[09/28 05:30:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 66.00	top5: 100.00	
[09/28 05:30:38 visual_prompt]: 	Test 100/356. loss: 1.483, 0.2200 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 05:31:00 visual_prompt]: 	Test 200/356. loss: 1.655, 0.2203 s / batch. (data: 2.91e-05)max mem: 7.80404 GB 
[09/28 05:31:23 visual_prompt]: 	Test 300/356. loss: 1.432, 0.2200 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 05:31:37 visual_prompt]: Inference (test):avg data time: 3.17e-05, avg batch time: 0.2198, average loss: 1.5977
[09/28 05:31:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 39.91	top5: 97.93	
[09/28 05:31:37 visual_prompt]: Best epoch 79: best metric: 0.660
[09/28 05:31:37 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/28 05:31:47 visual_prompt]: Epoch 80 / 100: avg data time: 1.21e-01, avg batch time: 0.5800, average train loss: 0.8732
[09/28 05:31:51 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1702, average loss: 0.8033
[09/28 05:31:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 63.00	top5: 100.00	
[09/28 05:32:14 visual_prompt]: 	Test 100/356. loss: 1.252, 0.2198 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 05:32:36 visual_prompt]: 	Test 200/356. loss: 1.521, 0.2198 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 05:32:59 visual_prompt]: 	Test 300/356. loss: 1.296, 0.2205 s / batch. (data: 2.48e-05)max mem: 7.80404 GB 
[09/28 05:33:12 visual_prompt]: Inference (test):avg data time: 3.27e-05, avg batch time: 0.2199, average loss: 1.3967
[09/28 05:33:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.70	top5: 97.94	
[09/28 05:33:13 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/28 05:33:23 visual_prompt]: Epoch 81 / 100: avg data time: 1.23e-01, avg batch time: 0.5829, average train loss: 0.7771
[09/28 05:33:27 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1700, average loss: 0.7835
[09/28 05:33:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 63.50	top5: 100.00	
[09/28 05:33:51 visual_prompt]: 	Test 100/356. loss: 1.391, 0.2208 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 05:34:13 visual_prompt]: 	Test 200/356. loss: 1.728, 0.2207 s / batch. (data: 2.96e-05)max mem: 7.80404 GB 
[09/28 05:34:35 visual_prompt]: 	Test 300/356. loss: 1.355, 0.2207 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 05:34:49 visual_prompt]: Inference (test):avg data time: 3.21e-05, avg batch time: 0.2198, average loss: 1.5417
[09/28 05:34:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.61	top5: 97.63	
[09/28 05:34:49 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/28 05:34:59 visual_prompt]: Epoch 82 / 100: avg data time: 1.17e-01, avg batch time: 0.5773, average train loss: 0.7133
[09/28 05:35:03 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1704, average loss: 0.8893
[09/28 05:35:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 61.50	top5: 99.50	
[09/28 05:35:27 visual_prompt]: 	Test 100/356. loss: 1.708, 0.2199 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 05:35:49 visual_prompt]: 	Test 200/356. loss: 1.604, 0.2209 s / batch. (data: 2.41e-05)max mem: 7.80404 GB 
[09/28 05:36:11 visual_prompt]: 	Test 300/356. loss: 1.558, 0.2205 s / batch. (data: 3.81e-05)max mem: 7.80404 GB 
[09/28 05:36:25 visual_prompt]: Inference (test):avg data time: 3.47e-05, avg batch time: 0.2198, average loss: 1.7703
[09/28 05:36:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.02	top5: 97.89	
[09/28 05:36:25 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/28 05:36:35 visual_prompt]: Epoch 83 / 100: avg data time: 1.17e-01, avg batch time: 0.5764, average train loss: 0.7226
[09/28 05:36:39 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1707, average loss: 0.8973
[09/28 05:36:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 57.50	top5: 100.00	
[09/28 05:37:02 visual_prompt]: 	Test 100/356. loss: 1.670, 0.2206 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 05:37:24 visual_prompt]: 	Test 200/356. loss: 1.994, 0.2205 s / batch. (data: 2.62e-05)max mem: 7.80404 GB 
[09/28 05:37:47 visual_prompt]: 	Test 300/356. loss: 1.500, 0.2217 s / batch. (data: 2.67e-05)max mem: 7.80404 GB 
[09/28 05:38:00 visual_prompt]: Inference (test):avg data time: 3.44e-05, avg batch time: 0.2198, average loss: 1.8487
[09/28 05:38:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.97	top5: 98.01	
[09/28 05:38:00 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/28 05:38:11 visual_prompt]: Epoch 84 / 100: avg data time: 1.15e-01, avg batch time: 0.5751, average train loss: 0.6808
[09/28 05:38:15 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1705, average loss: 0.9432
[09/28 05:38:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 59.00	top5: 100.00	
[09/28 05:38:38 visual_prompt]: 	Test 100/356. loss: 1.898, 0.2203 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 05:39:00 visual_prompt]: 	Test 200/356. loss: 1.860, 0.2197 s / batch. (data: 2.41e-05)max mem: 7.80404 GB 
[09/28 05:39:23 visual_prompt]: 	Test 300/356. loss: 1.949, 0.2203 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 05:39:36 visual_prompt]: Inference (test):avg data time: 6.75e-05, avg batch time: 0.2199, average loss: 1.9401
[09/28 05:39:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.89	top5: 97.94	
[09/28 05:39:36 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/28 05:39:47 visual_prompt]: Epoch 85 / 100: avg data time: 1.08e-01, avg batch time: 0.5666, average train loss: 0.6798
[09/28 05:39:50 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1702, average loss: 0.9670
[09/28 05:39:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 58.50	top5: 100.00	
[09/28 05:40:14 visual_prompt]: 	Test 100/356. loss: 1.964, 0.2193 s / batch. (data: 6.58e-05)max mem: 7.80404 GB 
[09/28 05:40:36 visual_prompt]: 	Test 200/356. loss: 2.222, 0.2210 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 05:40:58 visual_prompt]: 	Test 300/356. loss: 2.035, 0.2205 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 05:41:12 visual_prompt]: Inference (test):avg data time: 3.23e-05, avg batch time: 0.2199, average loss: 2.0466
[09/28 05:41:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.82	top5: 98.13	
[09/28 05:41:12 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/28 05:41:23 visual_prompt]: Epoch 86 / 100: avg data time: 1.25e-01, avg batch time: 0.5837, average train loss: 0.6509
[09/28 05:41:27 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1705, average loss: 0.6843
[09/28 05:41:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 70.00	top5: 100.00	
[09/28 05:41:51 visual_prompt]: 	Test 100/356. loss: 1.586, 0.2199 s / batch. (data: 2.74e-05)max mem: 7.80404 GB 
[09/28 05:42:13 visual_prompt]: 	Test 200/356. loss: 1.703, 0.2205 s / batch. (data: 2.88e-05)max mem: 7.80404 GB 
[09/28 05:42:35 visual_prompt]: 	Test 300/356. loss: 1.848, 0.2216 s / batch. (data: 2.50e-05)max mem: 7.80404 GB 
[09/28 05:42:48 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.2198, average loss: 1.7261
[09/28 05:42:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.93	top5: 97.98	
[09/28 05:42:49 visual_prompt]: Best epoch 86: best metric: 0.700
[09/28 05:42:49 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/28 05:42:59 visual_prompt]: Epoch 87 / 100: avg data time: 1.16e-01, avg batch time: 0.5749, average train loss: 0.5539
[09/28 05:43:02 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1700, average loss: 0.6130
[09/28 05:43:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 72.50	top5: 100.00	
[09/28 05:43:26 visual_prompt]: 	Test 100/356. loss: 1.790, 0.2200 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 05:43:48 visual_prompt]: 	Test 200/356. loss: 2.188, 0.2203 s / batch. (data: 3.05e-05)max mem: 7.80404 GB 
[09/28 05:44:10 visual_prompt]: 	Test 300/356. loss: 2.017, 0.2203 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 05:44:24 visual_prompt]: Inference (test):avg data time: 6.65e-05, avg batch time: 0.2200, average loss: 2.0665
[09/28 05:44:24 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.93	top5: 97.18	
[09/28 05:44:24 visual_prompt]: Best epoch 87: best metric: 0.725
[09/28 05:44:24 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/28 05:44:35 visual_prompt]: Epoch 88 / 100: avg data time: 1.19e-01, avg batch time: 0.5777, average train loss: 0.5136
[09/28 05:44:39 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1700, average loss: 0.5996
[09/28 05:44:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 73.50	top5: 100.00	
[09/28 05:45:03 visual_prompt]: 	Test 100/356. loss: 2.026, 0.2201 s / batch. (data: 2.96e-05)max mem: 7.80404 GB 
[09/28 05:45:25 visual_prompt]: 	Test 200/356. loss: 2.450, 0.2204 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 05:45:47 visual_prompt]: 	Test 300/356. loss: 2.004, 0.2200 s / batch. (data: 2.48e-05)max mem: 7.80404 GB 
[09/28 05:46:01 visual_prompt]: Inference (test):avg data time: 3.19e-05, avg batch time: 0.2199, average loss: 2.1643
[09/28 05:46:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.42	top5: 96.91	
[09/28 05:46:01 visual_prompt]: Best epoch 88: best metric: 0.735
[09/28 05:46:01 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/28 05:46:11 visual_prompt]: Epoch 89 / 100: avg data time: 1.20e-01, avg batch time: 0.5784, average train loss: 0.4847
[09/28 05:46:15 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1703, average loss: 0.6656
[09/28 05:46:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 70.00	top5: 100.00	
[09/28 05:46:38 visual_prompt]: 	Test 100/356. loss: 2.405, 0.2195 s / batch. (data: 2.98e-05)max mem: 7.80404 GB 
[09/28 05:47:00 visual_prompt]: 	Test 200/356. loss: 2.624, 0.2204 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 05:47:22 visual_prompt]: 	Test 300/356. loss: 2.447, 0.2205 s / batch. (data: 7.92e-05)max mem: 7.80404 GB 
[09/28 05:47:36 visual_prompt]: Inference (test):avg data time: 3.18e-05, avg batch time: 0.2199, average loss: 2.3932
[09/28 05:47:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.50	top5: 97.23	
[09/28 05:47:36 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/28 05:47:47 visual_prompt]: Epoch 90 / 100: avg data time: 1.15e-01, avg batch time: 0.5753, average train loss: 0.4734
[09/28 05:47:50 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1700, average loss: 0.5071
[09/28 05:47:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 73.50	top5: 100.00	
[09/28 05:48:14 visual_prompt]: 	Test 100/356. loss: 2.094, 0.2205 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 05:48:36 visual_prompt]: 	Test 200/356. loss: 2.584, 0.2211 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 05:48:58 visual_prompt]: 	Test 300/356. loss: 2.286, 0.2220 s / batch. (data: 2.77e-05)max mem: 7.80404 GB 
[09/28 05:49:12 visual_prompt]: Inference (test):avg data time: 3.13e-05, avg batch time: 0.2197, average loss: 2.2976
[09/28 05:49:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 40.95	top5: 96.61	
[09/28 05:49:12 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/28 05:49:23 visual_prompt]: Epoch 91 / 100: avg data time: 1.25e-01, avg batch time: 0.5844, average train loss: 0.4293
[09/28 05:49:27 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1704, average loss: 0.4685
[09/28 05:49:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 77.00	top5: 100.00	
[09/28 05:49:50 visual_prompt]: 	Test 100/356. loss: 2.332, 0.2199 s / batch. (data: 2.69e-05)max mem: 7.80404 GB 
[09/28 05:50:12 visual_prompt]: 	Test 200/356. loss: 2.527, 0.2209 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 05:50:35 visual_prompt]: 	Test 300/356. loss: 2.363, 0.2198 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 05:50:48 visual_prompt]: Inference (test):avg data time: 3.17e-05, avg batch time: 0.2198, average loss: 2.3584
[09/28 05:50:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.69	top5: 97.51	
[09/28 05:50:49 visual_prompt]: Best epoch 91: best metric: 0.770
[09/28 05:50:49 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/28 05:50:59 visual_prompt]: Epoch 92 / 100: avg data time: 1.17e-01, avg batch time: 0.5764, average train loss: 0.3685
[09/28 05:51:03 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1697, average loss: 0.4689
[09/28 05:51:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 79.50	top5: 100.00	
[09/28 05:51:27 visual_prompt]: 	Test 100/356. loss: 2.615, 0.2266 s / batch. (data: 2.53e-05)max mem: 7.80404 GB 
[09/28 05:51:49 visual_prompt]: 	Test 200/356. loss: 2.826, 0.2203 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 05:52:11 visual_prompt]: 	Test 300/356. loss: 2.568, 0.2214 s / batch. (data: 2.65e-05)max mem: 7.80404 GB 
[09/28 05:52:25 visual_prompt]: Inference (test):avg data time: 6.85e-05, avg batch time: 0.2198, average loss: 2.5538
[09/28 05:52:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.32	top5: 97.35	
[09/28 05:52:25 visual_prompt]: Best epoch 92: best metric: 0.795
[09/28 05:52:25 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/28 05:52:35 visual_prompt]: Epoch 93 / 100: avg data time: 1.20e-01, avg batch time: 0.5805, average train loss: 0.3484
[09/28 05:52:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1700, average loss: 0.3789
[09/28 05:52:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 84.00	top5: 100.00	
[09/28 05:53:03 visual_prompt]: 	Test 100/356. loss: 2.563, 0.2200 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 05:53:25 visual_prompt]: 	Test 200/356. loss: 2.821, 0.2194 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 05:53:47 visual_prompt]: 	Test 300/356. loss: 2.636, 0.2201 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 05:54:01 visual_prompt]: Inference (test):avg data time: 3.26e-05, avg batch time: 0.2197, average loss: 2.5592
[09/28 05:54:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.85	top5: 97.68	
[09/28 05:54:01 visual_prompt]: Best epoch 93: best metric: 0.840
[09/28 05:54:01 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/28 05:54:11 visual_prompt]: Epoch 94 / 100: avg data time: 1.15e-01, avg batch time: 0.5744, average train loss: 0.3196
[09/28 05:54:15 visual_prompt]: Inference (val):avg data time: 1.84e-05, avg batch time: 0.1701, average loss: 0.3778
[09/28 05:54:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 83.50	top5: 100.00	
[09/28 05:54:39 visual_prompt]: 	Test 100/356. loss: 2.610, 0.2200 s / batch. (data: 2.88e-05)max mem: 7.80404 GB 
[09/28 05:55:01 visual_prompt]: 	Test 200/356. loss: 2.950, 0.2212 s / batch. (data: 2.72e-05)max mem: 7.80404 GB 
[09/28 05:55:23 visual_prompt]: 	Test 300/356. loss: 2.731, 0.2200 s / batch. (data: 3.22e-05)max mem: 7.80404 GB 
[09/28 05:55:37 visual_prompt]: Inference (test):avg data time: 3.16e-05, avg batch time: 0.2198, average loss: 2.6461
[09/28 05:55:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.82	top5: 97.45	
[09/28 05:55:37 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/28 05:55:47 visual_prompt]: Epoch 95 / 100: avg data time: 1.22e-01, avg batch time: 0.5826, average train loss: 0.2726
[09/28 05:55:51 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1697, average loss: 0.3415
[09/28 05:55:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 84.50	top5: 100.00	
[09/28 05:56:15 visual_prompt]: 	Test 100/356. loss: 2.489, 0.2192 s / batch. (data: 7.99e-05)max mem: 7.80404 GB 
[09/28 05:56:37 visual_prompt]: 	Test 200/356. loss: 2.958, 0.2204 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 05:56:59 visual_prompt]: 	Test 300/356. loss: 2.654, 0.2205 s / batch. (data: 2.74e-05)max mem: 7.80404 GB 
[09/28 05:57:13 visual_prompt]: Inference (test):avg data time: 4.21e-05, avg batch time: 0.2197, average loss: 2.6492
[09/28 05:57:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.53	top5: 97.45	
[09/28 05:57:13 visual_prompt]: Best epoch 95: best metric: 0.845
[09/28 05:57:13 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/28 05:57:23 visual_prompt]: Epoch 96 / 100: avg data time: 1.22e-01, avg batch time: 0.5816, average train loss: 0.2671
[09/28 05:57:27 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1701, average loss: 0.3726
[09/28 05:57:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 83.00	top5: 100.00	
[09/28 05:57:51 visual_prompt]: 	Test 100/356. loss: 2.546, 0.2198 s / batch. (data: 2.67e-05)max mem: 7.80404 GB 
[09/28 05:58:13 visual_prompt]: 	Test 200/356. loss: 2.898, 0.2199 s / batch. (data: 2.48e-05)max mem: 7.80404 GB 
[09/28 05:58:35 visual_prompt]: 	Test 300/356. loss: 2.696, 0.2205 s / batch. (data: 2.84e-05)max mem: 7.80404 GB 
[09/28 05:58:49 visual_prompt]: Inference (test):avg data time: 3.15e-05, avg batch time: 0.2198, average loss: 2.6488
[09/28 05:58:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.90	top5: 97.69	
[09/28 05:58:49 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/28 05:58:59 visual_prompt]: Epoch 97 / 100: avg data time: 1.07e-01, avg batch time: 0.5657, average train loss: 0.2541
[09/28 05:59:03 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1699, average loss: 0.3491
[09/28 05:59:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 84.50	top5: 100.00	
[09/28 05:59:26 visual_prompt]: 	Test 100/356. loss: 2.520, 0.2197 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 05:59:48 visual_prompt]: 	Test 200/356. loss: 2.929, 0.2196 s / batch. (data: 2.86e-05)max mem: 7.80404 GB 
[09/28 06:00:11 visual_prompt]: 	Test 300/356. loss: 2.663, 0.2201 s / batch. (data: 2.79e-05)max mem: 7.80404 GB 
[09/28 06:00:24 visual_prompt]: Inference (test):avg data time: 3.25e-05, avg batch time: 0.2197, average loss: 2.6603
[09/28 06:00:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.41	top5: 97.47	
[09/28 06:00:25 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/28 06:00:35 visual_prompt]: Epoch 98 / 100: avg data time: 1.06e-01, avg batch time: 0.5664, average train loss: 0.2367
[09/28 06:00:39 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1699, average loss: 0.3531
[09/28 06:00:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 83.50	top5: 100.00	
[09/28 06:01:02 visual_prompt]: 	Test 100/356. loss: 2.574, 0.2198 s / batch. (data: 9.08e-05)max mem: 7.80404 GB 
[09/28 06:01:24 visual_prompt]: 	Test 200/356. loss: 2.973, 0.2210 s / batch. (data: 2.81e-05)max mem: 7.80404 GB 
[09/28 06:01:47 visual_prompt]: 	Test 300/356. loss: 2.706, 0.2200 s / batch. (data: 2.72e-05)max mem: 7.80404 GB 
[09/28 06:02:00 visual_prompt]: Inference (test):avg data time: 3.88e-05, avg batch time: 0.2197, average loss: 2.7032
[09/28 06:02:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.51	top5: 97.43	
[09/28 06:02:01 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/28 06:02:11 visual_prompt]: Epoch 99 / 100: avg data time: 1.16e-01, avg batch time: 0.5751, average train loss: 0.2212
[09/28 06:02:15 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1699, average loss: 0.3247
[09/28 06:02:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 85.50	top5: 100.00	
[09/28 06:02:39 visual_prompt]: 	Test 100/356. loss: 2.636, 0.2198 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 06:03:01 visual_prompt]: 	Test 200/356. loss: 3.029, 0.2207 s / batch. (data: 3.08e-05)max mem: 7.80404 GB 
[09/28 06:03:23 visual_prompt]: 	Test 300/356. loss: 2.764, 0.2207 s / batch. (data: 3.05e-05)max mem: 7.80404 GB 
[09/28 06:03:37 visual_prompt]: Inference (test):avg data time: 3.32e-05, avg batch time: 0.2198, average loss: 2.7340
[09/28 06:03:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.34	top5: 97.44	
[09/28 06:03:37 visual_prompt]: Best epoch 99: best metric: 0.855
[09/28 06:03:37 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/28 06:03:47 visual_prompt]: Epoch 100 / 100: avg data time: 1.24e-01, avg batch time: 0.5848, average train loss: 0.2312
[09/28 06:03:51 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1701, average loss: 0.3258
[09/28 06:03:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 85.50	top5: 100.00	
[09/28 06:04:15 visual_prompt]: 	Test 100/356. loss: 2.637, 0.2209 s / batch. (data: 3.12e-05)max mem: 7.80404 GB 
[09/28 06:04:37 visual_prompt]: 	Test 200/356. loss: 3.029, 0.2211 s / batch. (data: 2.53e-05)max mem: 7.80404 GB 
[09/28 06:04:59 visual_prompt]: 	Test 300/356. loss: 2.769, 0.2213 s / batch. (data: 3.00e-05)max mem: 7.80404 GB 
[09/28 06:05:13 visual_prompt]: Inference (test):avg data time: 3.23e-05, avg batch time: 0.2198, average loss: 2.7368
[09/28 06:05:13 visual_prompt]: Classification results with test_vtab-dmlab: top1: 42.30	top5: 97.42	
[09/28 06:05:13 visual_prompt]: Rank of current process: 0. World size: 1
[09/28 06:05:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/28 06:05:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-dmlab', 'DATA.NUMBER_CLASSES', '6', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/28 06:05:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/28 06:05:13 visual_prompt]: Training with config:
[09/28 06:05:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-dmlab/sup_vitb16_imagenet21k/prompt50/crop224/test/seed9881/lr1.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 9881, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-dmlab', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 6, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/28 06:05:13 visual_prompt]: Loading training data...
[09/28 06:05:13 visual_prompt]: Constructing vtab-dmlab dataset trainval...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split train[:800]+validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/28 06:05:15 visual_prompt]: Number of images: 1000
[09/28 06:05:15 visual_prompt]: Number of classes: 6 / 6
[09/28 06:05:15 visual_prompt]: Loading validation data...
[09/28 06:05:15 visual_prompt]: Constructing vtab-dmlab dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split validation[:200], from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/28 06:05:16 visual_prompt]: Number of images: 200
[09/28 06:05:16 visual_prompt]: Number of classes: 6 / 6
[09/28 06:05:16 visual_prompt]: Loading test data...
[09/28 06:05:16 visual_prompt]: Constructing vtab-dmlab dataset test...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/dmlab/2.0.1
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset dmlab for split test, from visual_prompt_tuning/data_path/dmlab/2.0.1
[09/28 06:05:53 visual_prompt]: Number of images: 22735
[09/28 06:05:53 visual_prompt]: Number of classes: 6 / 6
[09/28 06:05:53 visual_prompt]: Constructing models...
[09/28 06:05:56 visual_prompt]: Total Parameters: 86264070	 Gradient Parameters: 465414
[09/28 06:05:56 visual_prompt]: tuned percent:0.540
[09/28 06:05:56 visual_prompt]: Device used for model: 0
[09/28 06:05:56 visual_prompt]: Setting up Evaluator...
[09/28 06:05:56 visual_prompt]: Setting up Trainer...
[09/28 06:05:56 visual_prompt]: 	Setting up the optimizer...
[09/28 06:05:56 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/28 06:06:07 visual_prompt]: Epoch 1 / 100: avg data time: 1.26e-01, avg batch time: 0.5759, average train loss: 1.9871
[09/28 06:06:11 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.1673, average loss: 1.9740
[09/28 06:06:11 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 86.50	
[09/28 06:06:34 visual_prompt]: 	Test 100/356. loss: 2.065, 0.2161 s / batch. (data: 2.72e-05)max mem: 7.81207 GB 
[09/28 06:06:56 visual_prompt]: 	Test 200/356. loss: 1.911, 0.2185 s / batch. (data: 2.69e-05)max mem: 7.81207 GB 
[09/28 06:07:18 visual_prompt]: 	Test 300/356. loss: 2.088, 0.2200 s / batch. (data: 2.86e-05)max mem: 7.81207 GB 
[09/28 06:07:32 visual_prompt]: Inference (test):avg data time: 3.49e-05, avg batch time: 0.2170, average loss: 2.0096
[09/28 06:07:32 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.57	top5: 85.34	
[09/28 06:07:32 visual_prompt]: Best epoch 1: best metric: 0.210
[09/28 06:07:32 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/28 06:07:42 visual_prompt]: Epoch 2 / 100: avg data time: 1.16e-01, avg batch time: 0.5743, average train loss: 2.2476
[09/28 06:07:46 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1691, average loss: 1.8771
[09/28 06:07:46 visual_prompt]: Classification results with val_vtab-dmlab: top1: 18.50	top5: 84.00	
[09/28 06:08:10 visual_prompt]: 	Test 100/356. loss: 1.858, 0.2199 s / batch. (data: 2.88e-05)max mem: 7.81207 GB 
[09/28 06:08:32 visual_prompt]: 	Test 200/356. loss: 1.735, 0.2185 s / batch. (data: 3.29e-05)max mem: 7.81207 GB 
[09/28 06:08:54 visual_prompt]: 	Test 300/356. loss: 1.800, 0.2202 s / batch. (data: 2.72e-05)max mem: 7.81207 GB 
[09/28 06:09:07 visual_prompt]: Inference (test):avg data time: 3.32e-05, avg batch time: 0.2190, average loss: 1.8242
[09/28 06:09:07 visual_prompt]: Classification results with test_vtab-dmlab: top1: 21.92	top5: 85.40	
[09/28 06:09:07 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/28 06:09:18 visual_prompt]: Epoch 3 / 100: avg data time: 1.19e-01, avg batch time: 0.5788, average train loss: 1.8304
[09/28 06:09:22 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1695, average loss: 1.8052
[09/28 06:09:22 visual_prompt]: Classification results with val_vtab-dmlab: top1: 21.00	top5: 84.00	
[09/28 06:09:46 visual_prompt]: 	Test 100/356. loss: 1.844, 0.2198 s / batch. (data: 2.79e-05)max mem: 7.81207 GB 
[09/28 06:10:08 visual_prompt]: 	Test 200/356. loss: 1.755, 0.2197 s / batch. (data: 2.79e-05)max mem: 7.81207 GB 
[09/28 06:10:30 visual_prompt]: 	Test 300/356. loss: 1.790, 0.2198 s / batch. (data: 2.77e-05)max mem: 7.81207 GB 
[09/28 06:10:43 visual_prompt]: Inference (test):avg data time: 3.31e-05, avg batch time: 0.2195, average loss: 1.7955
[09/28 06:10:44 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.68	top5: 85.40	
[09/28 06:10:44 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/28 06:10:54 visual_prompt]: Epoch 4 / 100: avg data time: 1.10e-01, avg batch time: 0.5711, average train loss: 1.8328
[09/28 06:10:58 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1696, average loss: 1.8779
[09/28 06:10:58 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/28 06:11:22 visual_prompt]: 	Test 100/356. loss: 1.848, 0.2197 s / batch. (data: 3.62e-05)max mem: 7.81207 GB 
[09/28 06:11:44 visual_prompt]: 	Test 200/356. loss: 1.892, 0.2197 s / batch. (data: 2.98e-05)max mem: 7.81207 GB 
[09/28 06:12:06 visual_prompt]: 	Test 300/356. loss: 1.885, 0.2198 s / batch. (data: 3.41e-05)max mem: 7.81207 GB 
[09/28 06:12:20 visual_prompt]: Inference (test):avg data time: 3.35e-05, avg batch time: 0.2195, average loss: 1.8633
[09/28 06:12:20 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 88.42	
[09/28 06:12:20 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/28 06:12:30 visual_prompt]: Epoch 5 / 100: avg data time: 1.10e-01, avg batch time: 0.5708, average train loss: 1.7762
[09/28 06:12:34 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1698, average loss: 2.1160
[09/28 06:12:34 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 80.50	
[09/28 06:12:58 visual_prompt]: 	Test 100/356. loss: 1.963, 0.2182 s / batch. (data: 2.79e-05)max mem: 7.81207 GB 
[09/28 06:13:20 visual_prompt]: 	Test 200/356. loss: 1.792, 0.2195 s / batch. (data: 2.65e-05)max mem: 7.81207 GB 
[09/28 06:13:42 visual_prompt]: 	Test 300/356. loss: 1.991, 0.2196 s / batch. (data: 2.86e-05)max mem: 7.81207 GB 
[09/28 06:13:56 visual_prompt]: Inference (test):avg data time: 4.01e-05, avg batch time: 0.2196, average loss: 1.9775
[09/28 06:13:56 visual_prompt]: Classification results with test_vtab-dmlab: top1: 21.81	top5: 82.08	
[09/28 06:13:56 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/28 06:14:06 visual_prompt]: Epoch 6 / 100: avg data time: 1.18e-01, avg batch time: 0.5781, average train loss: 1.8031
[09/28 06:14:10 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1694, average loss: 1.7713
[09/28 06:14:10 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.00	top5: 91.00	
[09/28 06:14:34 visual_prompt]: 	Test 100/356. loss: 1.823, 0.2202 s / batch. (data: 3.05e-05)max mem: 7.81207 GB 
[09/28 06:14:56 visual_prompt]: 	Test 200/356. loss: 1.659, 0.2202 s / batch. (data: 2.74e-05)max mem: 7.81207 GB 
[09/28 06:15:20 visual_prompt]: 	Test 300/356. loss: 1.728, 0.2202 s / batch. (data: 2.62e-05)max mem: 7.81207 GB 
[09/28 06:15:34 visual_prompt]: Inference (test):avg data time: 7.71e-05, avg batch time: 0.2194, average loss: 1.7382
[09/28 06:15:34 visual_prompt]: Classification results with test_vtab-dmlab: top1: 27.83	top5: 90.16	
[09/28 06:15:34 visual_prompt]: Best epoch 6: best metric: 0.220
[09/28 06:15:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/28 06:15:45 visual_prompt]: Epoch 7 / 100: avg data time: 1.20e-01, avg batch time: 0.5787, average train loss: 1.7169
[09/28 06:15:49 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1698, average loss: 1.8331
[09/28 06:15:49 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.00	top5: 84.00	
[09/28 06:16:12 visual_prompt]: 	Test 100/356. loss: 1.739, 0.2189 s / batch. (data: 6.96e-05)max mem: 7.81207 GB 
[09/28 06:16:34 visual_prompt]: 	Test 200/356. loss: 1.827, 0.2195 s / batch. (data: 2.88e-05)max mem: 7.81207 GB 
[09/28 06:16:56 visual_prompt]: 	Test 300/356. loss: 1.736, 0.2210 s / batch. (data: 4.17e-05)max mem: 7.81207 GB 
[09/28 06:17:10 visual_prompt]: Inference (test):avg data time: 3.28e-05, avg batch time: 0.2194, average loss: 1.7782
[09/28 06:17:10 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.60	top5: 88.42	
[09/28 06:17:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/28 06:17:21 visual_prompt]: Epoch 8 / 100: avg data time: 1.20e-01, avg batch time: 0.5792, average train loss: 1.8035
[09/28 06:17:25 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1692, average loss: 1.9002
[09/28 06:17:25 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 89.00	
[09/28 06:17:49 visual_prompt]: 	Test 100/356. loss: 2.036, 0.2283 s / batch. (data: 3.05e-05)max mem: 7.81207 GB 
[09/28 06:18:11 visual_prompt]: 	Test 200/356. loss: 2.187, 0.2192 s / batch. (data: 2.93e-05)max mem: 7.81207 GB 
[09/28 06:18:33 visual_prompt]: 	Test 300/356. loss: 1.982, 0.2195 s / batch. (data: 2.93e-05)max mem: 7.81207 GB 
[09/28 06:18:46 visual_prompt]: Inference (test):avg data time: 3.35e-05, avg batch time: 0.2195, average loss: 1.9986
[09/28 06:18:47 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 84.32	
[09/28 06:18:47 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/28 06:18:57 visual_prompt]: Epoch 9 / 100: avg data time: 1.22e-01, avg batch time: 0.5816, average train loss: 1.6836
[09/28 06:19:01 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1696, average loss: 2.0351
[09/28 06:19:01 visual_prompt]: Classification results with val_vtab-dmlab: top1: 15.50	top5: 89.50	
[09/28 06:19:25 visual_prompt]: 	Test 100/356. loss: 1.963, 0.2194 s / batch. (data: 2.96e-05)max mem: 7.81207 GB 
[09/28 06:19:47 visual_prompt]: 	Test 200/356. loss: 1.640, 0.2200 s / batch. (data: 7.37e-05)max mem: 7.81207 GB 
[09/28 06:20:09 visual_prompt]: 	Test 300/356. loss: 1.916, 0.2215 s / batch. (data: 2.79e-05)max mem: 7.81207 GB 
[09/28 06:20:23 visual_prompt]: Inference (test):avg data time: 3.39e-05, avg batch time: 0.2196, average loss: 1.9190
[09/28 06:20:23 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.19	top5: 87.08	
[09/28 06:20:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/28 06:20:33 visual_prompt]: Epoch 10 / 100: avg data time: 1.11e-01, avg batch time: 0.5719, average train loss: 1.6034
[09/28 06:20:37 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1694, average loss: 1.5016
[09/28 06:20:37 visual_prompt]: Classification results with val_vtab-dmlab: top1: 36.00	top5: 98.00	
[09/28 06:21:01 visual_prompt]: 	Test 100/356. loss: 1.374, 0.2187 s / batch. (data: 2.96e-05)max mem: 7.81207 GB 
[09/28 06:21:23 visual_prompt]: 	Test 200/356. loss: 1.513, 0.2200 s / batch. (data: 2.79e-05)max mem: 7.81207 GB 
[09/28 06:21:45 visual_prompt]: 	Test 300/356. loss: 1.460, 0.2206 s / batch. (data: 2.91e-05)max mem: 7.81207 GB 
[09/28 06:21:59 visual_prompt]: Inference (test):avg data time: 3.31e-05, avg batch time: 0.2195, average loss: 1.4440
[09/28 06:21:59 visual_prompt]: Classification results with test_vtab-dmlab: top1: 34.51	top5: 97.82	
[09/28 06:21:59 visual_prompt]: Best epoch 10: best metric: 0.360
[09/28 06:21:59 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/28 06:22:10 visual_prompt]: Epoch 11 / 100: avg data time: 1.20e-01, avg batch time: 0.5792, average train loss: 1.5981
[09/28 06:22:13 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1695, average loss: 1.6513
[09/28 06:22:13 visual_prompt]: Classification results with val_vtab-dmlab: top1: 25.50	top5: 96.50	
[09/28 06:22:37 visual_prompt]: 	Test 100/356. loss: 1.382, 0.2197 s / batch. (data: 2.88e-05)max mem: 7.81207 GB 
[09/28 06:22:59 visual_prompt]: 	Test 200/356. loss: 1.600, 0.2215 s / batch. (data: 3.08e-05)max mem: 7.81207 GB 
[09/28 06:23:21 visual_prompt]: 	Test 300/356. loss: 1.549, 0.2203 s / batch. (data: 2.69e-05)max mem: 7.81207 GB 
[09/28 06:23:35 visual_prompt]: Inference (test):avg data time: 4.23e-05, avg batch time: 0.2196, average loss: 1.5809
[09/28 06:23:35 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.88	top5: 97.06	
[09/28 06:23:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/28 06:23:46 visual_prompt]: Epoch 12 / 100: avg data time: 1.18e-01, avg batch time: 0.5781, average train loss: 1.6578
[09/28 06:23:50 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1695, average loss: 1.4409
[09/28 06:23:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 41.50	top5: 96.50	
[09/28 06:24:14 visual_prompt]: 	Test 100/356. loss: 1.339, 0.2198 s / batch. (data: 3.05e-05)max mem: 7.81207 GB 
[09/28 06:24:36 visual_prompt]: 	Test 200/356. loss: 1.409, 0.2198 s / batch. (data: 3.60e-05)max mem: 7.81207 GB 
[09/28 06:24:58 visual_prompt]: 	Test 300/356. loss: 1.319, 0.2210 s / batch. (data: 3.05e-05)max mem: 7.81207 GB 
[09/28 06:25:11 visual_prompt]: Inference (test):avg data time: 3.25e-05, avg batch time: 0.2195, average loss: 1.4071
[09/28 06:25:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.80	top5: 97.34	
[09/28 06:25:12 visual_prompt]: Best epoch 12: best metric: 0.415
[09/28 06:25:12 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/28 06:25:22 visual_prompt]: Epoch 13 / 100: avg data time: 1.23e-01, avg batch time: 0.5820, average train loss: 1.4084
[09/28 06:25:26 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1699, average loss: 1.6087
[09/28 06:25:26 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.00	top5: 93.00	
[09/28 06:25:50 visual_prompt]: 	Test 100/356. loss: 1.586, 0.2197 s / batch. (data: 2.74e-05)max mem: 7.81207 GB 
[09/28 06:26:12 visual_prompt]: 	Test 200/356. loss: 1.442, 0.2201 s / batch. (data: 3.05e-05)max mem: 7.81207 GB 
[09/28 06:26:34 visual_prompt]: 	Test 300/356. loss: 1.559, 0.2202 s / batch. (data: 4.22e-05)max mem: 7.81207 GB 
[09/28 06:26:48 visual_prompt]: Inference (test):avg data time: 3.25e-05, avg batch time: 0.2198, average loss: 1.5955
[09/28 06:26:48 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.10	top5: 93.17	
[09/28 06:26:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/28 06:26:58 visual_prompt]: Epoch 14 / 100: avg data time: 1.11e-01, avg batch time: 0.5729, average train loss: 1.5507
[09/28 06:27:02 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1697, average loss: 1.6961
[09/28 06:27:02 visual_prompt]: Classification results with val_vtab-dmlab: top1: 23.00	top5: 88.00	
[09/28 06:27:26 visual_prompt]: 	Test 100/356. loss: 1.739, 0.2199 s / batch. (data: 3.46e-05)max mem: 7.81207 GB 
[09/28 06:27:48 visual_prompt]: 	Test 200/356. loss: 1.676, 0.2196 s / batch. (data: 3.05e-05)max mem: 7.81207 GB 
[09/28 06:28:10 visual_prompt]: 	Test 300/356. loss: 1.811, 0.2210 s / batch. (data: 3.03e-05)max mem: 7.81207 GB 
[09/28 06:28:24 visual_prompt]: Inference (test):avg data time: 3.44e-05, avg batch time: 0.2196, average loss: 1.7126
[09/28 06:28:24 visual_prompt]: Classification results with test_vtab-dmlab: top1: 24.99	top5: 88.87	
[09/28 06:28:24 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/28 06:28:35 visual_prompt]: Epoch 15 / 100: avg data time: 1.24e-01, avg batch time: 0.5832, average train loss: 1.6448
[09/28 06:28:38 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1695, average loss: 1.5700
[09/28 06:28:38 visual_prompt]: Classification results with val_vtab-dmlab: top1: 37.00	top5: 98.00	
[09/28 06:29:02 visual_prompt]: 	Test 100/356. loss: 1.361, 0.2193 s / batch. (data: 6.20e-05)max mem: 7.81207 GB 
[09/28 06:29:24 visual_prompt]: 	Test 200/356. loss: 1.479, 0.2219 s / batch. (data: 3.05e-05)max mem: 7.81207 GB 
[09/28 06:29:46 visual_prompt]: 	Test 300/356. loss: 1.420, 0.2208 s / batch. (data: 3.00e-05)max mem: 7.81207 GB 
[09/28 06:30:00 visual_prompt]: Inference (test):avg data time: 3.34e-05, avg batch time: 0.2197, average loss: 1.4912
[09/28 06:30:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 41.46	top5: 98.17	
[09/28 06:30:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/28 06:30:11 visual_prompt]: Epoch 16 / 100: avg data time: 1.18e-01, avg batch time: 0.5772, average train loss: 1.5808
[09/28 06:30:14 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1698, average loss: 1.5847
[09/28 06:30:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 30.00	top5: 96.50	
[09/28 06:30:38 visual_prompt]: 	Test 100/356. loss: 1.441, 0.2196 s / batch. (data: 2.81e-05)max mem: 7.81207 GB 
[09/28 06:31:00 visual_prompt]: 	Test 200/356. loss: 1.721, 0.2210 s / batch. (data: 2.81e-05)max mem: 7.81207 GB 
[09/28 06:31:22 visual_prompt]: 	Test 300/356. loss: 1.541, 0.2208 s / batch. (data: 2.86e-05)max mem: 7.81207 GB 
[09/28 06:31:36 visual_prompt]: Inference (test):avg data time: 3.78e-05, avg batch time: 0.2196, average loss: 1.5917
[09/28 06:31:36 visual_prompt]: Classification results with test_vtab-dmlab: top1: 31.25	top5: 96.96	
[09/28 06:31:36 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/28 06:31:47 visual_prompt]: Epoch 17 / 100: avg data time: 1.12e-01, avg batch time: 0.5736, average train loss: 1.7114
[09/28 06:31:50 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1713, average loss: 2.6217
[09/28 06:31:50 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 84.00	
[09/28 06:32:14 visual_prompt]: 	Test 100/356. loss: 2.790, 0.2190 s / batch. (data: 2.86e-05)max mem: 7.81207 GB 
[09/28 06:32:36 visual_prompt]: 	Test 200/356. loss: 2.322, 0.2215 s / batch. (data: 3.05e-05)max mem: 7.81207 GB 
[09/28 06:32:58 visual_prompt]: 	Test 300/356. loss: 2.424, 0.2210 s / batch. (data: 2.77e-05)max mem: 7.81207 GB 
[09/28 06:33:12 visual_prompt]: Inference (test):avg data time: 3.24e-05, avg batch time: 0.2197, average loss: 2.5616
[09/28 06:33:12 visual_prompt]: Classification results with test_vtab-dmlab: top1: 18.56	top5: 85.40	
[09/28 06:33:12 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/28 06:33:23 visual_prompt]: Epoch 18 / 100: avg data time: 1.22e-01, avg batch time: 0.5814, average train loss: 1.8992
[09/28 06:33:27 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1697, average loss: 1.8863
[09/28 06:33:27 visual_prompt]: Classification results with val_vtab-dmlab: top1: 22.50	top5: 97.00	
[09/28 06:33:51 visual_prompt]: 	Test 100/356. loss: 1.993, 0.2195 s / batch. (data: 4.17e-05)max mem: 7.81207 GB 
[09/28 06:34:13 visual_prompt]: 	Test 200/356. loss: 1.892, 0.2199 s / batch. (data: 3.74e-05)max mem: 7.81207 GB 
[09/28 06:34:35 visual_prompt]: 	Test 300/356. loss: 1.841, 0.2203 s / batch. (data: 2.96e-05)max mem: 7.81207 GB 
[09/28 06:34:49 visual_prompt]: Inference (test):avg data time: 5.44e-05, avg batch time: 0.2197, average loss: 1.9037
[09/28 06:34:49 visual_prompt]: Classification results with test_vtab-dmlab: top1: 22.08	top5: 95.92	
[09/28 06:34:49 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/28 06:34:59 visual_prompt]: Epoch 19 / 100: avg data time: 1.22e-01, avg batch time: 0.5806, average train loss: 1.6307
[09/28 06:35:03 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1693, average loss: 1.5980
[09/28 06:35:03 visual_prompt]: Classification results with val_vtab-dmlab: top1: 29.00	top5: 93.00	
[09/28 06:35:27 visual_prompt]: 	Test 100/356. loss: 1.532, 0.2204 s / batch. (data: 3.03e-05)max mem: 7.81207 GB 
[09/28 06:35:49 visual_prompt]: 	Test 200/356. loss: 1.825, 0.2203 s / batch. (data: 2.96e-05)max mem: 7.81207 GB 
[09/28 06:36:11 visual_prompt]: 	Test 300/356. loss: 1.487, 0.2207 s / batch. (data: 3.00e-05)max mem: 7.81207 GB 
[09/28 06:36:25 visual_prompt]: Inference (test):avg data time: 3.39e-05, avg batch time: 0.2196, average loss: 1.6168
[09/28 06:36:25 visual_prompt]: Classification results with test_vtab-dmlab: top1: 32.41	top5: 93.43	
[09/28 06:36:25 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/28 06:36:35 visual_prompt]: Epoch 20 / 100: avg data time: 1.12e-01, avg batch time: 0.5719, average train loss: 1.4774
[09/28 06:36:39 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1696, average loss: 1.5949
[09/28 06:36:39 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.00	top5: 99.00	
[09/28 06:37:03 visual_prompt]: 	Test 100/356. loss: 1.374, 0.2195 s / batch. (data: 2.98e-05)max mem: 7.81207 GB 
[09/28 06:37:25 visual_prompt]: 	Test 200/356. loss: 1.714, 0.2206 s / batch. (data: 8.54e-05)max mem: 7.81207 GB 
[09/28 06:37:47 visual_prompt]: 	Test 300/356. loss: 1.455, 0.2209 s / batch. (data: 2.84e-05)max mem: 7.81207 GB 
[09/28 06:38:01 visual_prompt]: Inference (test):avg data time: 3.37e-05, avg batch time: 0.2196, average loss: 1.5620
[09/28 06:38:01 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.11	top5: 97.99	
[09/28 06:38:01 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/28 06:38:11 visual_prompt]: Epoch 21 / 100: avg data time: 1.28e-01, avg batch time: 0.5865, average train loss: 1.5683
[09/28 06:38:15 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1696, average loss: 1.7476
[09/28 06:38:15 visual_prompt]: Classification results with val_vtab-dmlab: top1: 27.50	top5: 96.50	
[09/28 06:38:39 visual_prompt]: 	Test 100/356. loss: 1.837, 0.2204 s / batch. (data: 8.89e-05)max mem: 7.81207 GB 
[09/28 06:39:01 visual_prompt]: 	Test 200/356. loss: 1.787, 0.2194 s / batch. (data: 2.88e-05)max mem: 7.81207 GB 
[09/28 06:39:23 visual_prompt]: 	Test 300/356. loss: 1.591, 0.2205 s / batch. (data: 3.19e-05)max mem: 7.81207 GB 
[09/28 06:39:37 visual_prompt]: Inference (test):avg data time: 3.57e-05, avg batch time: 0.2196, average loss: 1.7735
[09/28 06:39:37 visual_prompt]: Classification results with test_vtab-dmlab: top1: 28.90	top5: 95.53	
[09/28 06:39:37 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/28 06:39:48 visual_prompt]: Epoch 22 / 100: avg data time: 1.23e-01, avg batch time: 0.5811, average train loss: 1.6235
[09/28 06:39:51 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1696, average loss: 1.6443
[09/28 06:39:51 visual_prompt]: Classification results with val_vtab-dmlab: top1: 31.00	top5: 99.50	
[09/28 06:40:15 visual_prompt]: 	Test 100/356. loss: 1.472, 0.2196 s / batch. (data: 2.91e-05)max mem: 7.81207 GB 
[09/28 06:40:37 visual_prompt]: 	Test 200/356. loss: 1.606, 0.2199 s / batch. (data: 2.79e-05)max mem: 7.81207 GB 
[09/28 06:41:00 visual_prompt]: 	Test 300/356. loss: 1.574, 0.2207 s / batch. (data: 2.48e-05)max mem: 7.81207 GB 
[09/28 06:41:13 visual_prompt]: Inference (test):avg data time: 7.69e-05, avg batch time: 0.2196, average loss: 1.6047
[09/28 06:41:14 visual_prompt]: Classification results with test_vtab-dmlab: top1: 35.41	top5: 97.73	
[09/28 06:41:14 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/28 06:41:24 visual_prompt]: Epoch 23 / 100: avg data time: 1.24e-01, avg batch time: 0.5835, average train loss: 1.4083
[09/28 06:41:28 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1699, average loss: 1.8693
[09/28 06:41:28 visual_prompt]: Classification results with val_vtab-dmlab: top1: 28.00	top5: 91.00	
[09/28 06:41:52 visual_prompt]: 	Test 100/356. loss: 1.962, 0.2191 s / batch. (data: 2.74e-05)max mem: 7.81207 GB 
[09/28 06:42:14 visual_prompt]: 	Test 200/356. loss: 2.129, 0.2198 s / batch. (data: 2.84e-05)max mem: 7.81207 GB 
[09/28 06:42:36 visual_prompt]: 	Test 300/356. loss: 1.816, 0.2212 s / batch. (data: 7.34e-05)max mem: 7.81207 GB 
[09/28 06:42:50 visual_prompt]: Inference (test):avg data time: 6.71e-05, avg batch time: 0.2195, average loss: 2.0076
[09/28 06:42:50 visual_prompt]: Classification results with test_vtab-dmlab: top1: 23.71	top5: 90.60	
[09/28 06:42:50 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/28 06:43:00 visual_prompt]: Epoch 24 / 100: avg data time: 1.28e-01, avg batch time: 0.5867, average train loss: 1.5354
[09/28 06:43:04 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1704, average loss: 1.4802
[09/28 06:43:04 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.50	top5: 99.50	
[09/28 06:43:28 visual_prompt]: 	Test 100/356. loss: 1.597, 0.2185 s / batch. (data: 3.12e-05)max mem: 7.81207 GB 
[09/28 06:43:50 visual_prompt]: 	Test 200/356. loss: 1.674, 0.2193 s / batch. (data: 2.77e-05)max mem: 7.81207 GB 
[09/28 06:44:12 visual_prompt]: 	Test 300/356. loss: 1.452, 0.2210 s / batch. (data: 2.77e-05)max mem: 7.81207 GB 
[09/28 06:44:26 visual_prompt]: Inference (test):avg data time: 4.45e-05, avg batch time: 0.2196, average loss: 1.5663
[09/28 06:44:26 visual_prompt]: Classification results with test_vtab-dmlab: top1: 30.80	top5: 97.97	
[09/28 06:44:26 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/28 06:44:36 visual_prompt]: Epoch 25 / 100: avg data time: 1.10e-01, avg batch time: 0.5703, average train loss: 1.4332
[09/28 06:44:40 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1698, average loss: 1.6656
[09/28 06:44:40 visual_prompt]: Classification results with val_vtab-dmlab: top1: 33.50	top5: 97.50	
[09/28 06:45:04 visual_prompt]: 	Test 100/356. loss: 1.633, 0.2184 s / batch. (data: 3.05e-05)max mem: 7.81207 GB 
[09/28 06:45:26 visual_prompt]: 	Test 200/356. loss: 1.775, 0.2205 s / batch. (data: 3.17e-05)max mem: 7.81207 GB 
[09/28 06:45:48 visual_prompt]: 	Test 300/356. loss: 1.543, 0.2203 s / batch. (data: 2.81e-05)max mem: 7.81207 GB 
[09/28 06:46:02 visual_prompt]: Inference (test):avg data time: 3.35e-05, avg batch time: 0.2196, average loss: 1.6987
[09/28 06:46:02 visual_prompt]: Classification results with test_vtab-dmlab: top1: 32.68	top5: 95.25	
[09/28 06:46:02 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/28 06:46:13 visual_prompt]: Epoch 26 / 100: avg data time: 1.17e-01, avg batch time: 0.5763, average train loss: 1.8414
[09/28 06:46:16 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1699, average loss: 1.7932
[09/28 06:46:16 visual_prompt]: Classification results with val_vtab-dmlab: top1: 20.50	top5: 85.50	
[09/28 06:46:40 visual_prompt]: 	Test 100/356. loss: 1.821, 0.2193 s / batch. (data: 9.01e-05)max mem: 7.81207 GB 
[09/28 06:47:20 visual_prompt]: 	Test 200/356. loss: 1.846, 0.2157 s / batch. (data: 3.46e-05)max mem: 7.81207 GB 
[09/28 06:47:46 visual_prompt]: 	Test 300/356. loss: 1.868, 0.2174 s / batch. (data: 2.91e-05)max mem: 7.81207 GB 
[09/28 06:47:59 visual_prompt]: Inference (test):avg data time: 3.12e-04, avg batch time: 0.2183, average loss: 1.8147
[09/28 06:48:00 visual_prompt]: Classification results with test_vtab-dmlab: top1: 17.38	top5: 85.48	
[09/28 06:48:00 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/28 06:48:10 visual_prompt]: Epoch 27 / 100: avg data time: 1.22e-01, avg batch time: 0.5777, average train loss: 1.7907
[09/28 06:48:14 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.1687, average loss: 1.5649
[09/28 06:48:14 visual_prompt]: Classification results with val_vtab-dmlab: top1: 35.00	top5: 95.50	
visual_prompt_tuning/experiments/vit_vtab.sh: line 41: 244674 Killed                  python visual_prompt_tuning/tune_vtab.py --config-file visual_prompt_tuning/configs/prompt/cub.yaml --train-type "prompt" MODEL.TYPE "vit" DATA.BATCH_SIZE "64" MODEL.PROMPT.NUM_TOKENS "50" MODEL.PROMPT.DEEP "True" MODEL.PROMPT.DROPOUT "0.1" DATA.FEATURE "sup_vitb16_imagenet21k" DATA.NAME "vtab-${dataset}" DATA.NUMBER_CLASSES "${num_classes}" DATA.CROPSIZE "224" MODEL.MODEL_ROOT "${model_root}" DATA.DATAPATH "${data_path}" OUTPUT_DIR "${output_dir}"

/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/22 07:06:40 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 07:06:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 07:06:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/22 07:06:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/22 07:06:42 visual_prompt]: Training with config:
[11/22 07:06:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/22 07:06:42 visual_prompt]: Loading training data...
[11/22 07:06:42 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 07:06:42 visual_prompt]: Loading validation data...
[11/22 07:06:42 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 07:06:42 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 07:06:45 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/22 07:06:45 visual_prompt]: tuned percent:0.525
[11/22 07:06:46 visual_prompt]: Device used for model: 0
[11/22 07:06:46 visual_prompt]: Setting up Evaluator...
[11/22 07:06:46 visual_prompt]: Setting up Trainer...
[11/22 07:06:46 visual_prompt]: 	Setting up the optimizer...
[11/22 07:06:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 07:08:34 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8211 s / batch. (data: 1.05e-02). ETA=12:35:26, max mem: 20.9 GB 
[11/22 07:10:15 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8198 s / batch. (data: 2.98e-04). ETA=12:32:52, max mem: 20.9 GB 
[11/22 07:12:01 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.8280 s / batch. (data: 9.86e-01). ETA=1 day, 3:55:40, max mem: 20.9 GB 
[11/22 07:13:40 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8078 s / batch. (data: 3.24e-04). ETA=12:19:08, max mem: 20.9 GB 
[11/22 07:15:24 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8321 s / batch. (data: 3.88e-04). ETA=12:39:56, max mem: 20.9 GB 
[11/22 07:16:17 visual_prompt]: Epoch 1 / 100: avg data time: 2.07e-01, avg batch time: 1.0340, average train loss: 1.5403
[11/22 07:17:15 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3061, average loss: 1.5201
[11/22 07:17:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/22 07:17:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/22 07:18:59 visual_prompt]: 	Training 100/553. train loss: 11.0660,	1.3324 s / batch. (data: 5.28e-01). ETA=20:13:33, max mem: 20.9 GB 
[11/22 07:20:40 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.6047 s / batch. (data: 7.93e-01). ETA=1 day, 0:18:49, max mem: 20.9 GB 
[11/22 07:22:23 visual_prompt]: 	Training 300/553. train loss: 16.4932,	1.1150 s / batch. (data: 3.05e-01). ETA=16:51:47, max mem: 20.9 GB 
[11/22 07:24:03 visual_prompt]: 	Training 400/553. train loss: 6.3848,	0.8317 s / batch. (data: 7.95e-03). ETA=12:33:22, max mem: 20.9 GB 
[11/22 07:25:45 visual_prompt]: 	Training 500/553. train loss: 1.8677,	0.8109 s / batch. (data: 3.03e-04). ETA=12:13:09, max mem: 20.9 GB 
[11/22 07:26:36 visual_prompt]: Epoch 2 / 100: avg data time: 1.92e-01, avg batch time: 1.0150, average train loss: 14.2649
[11/22 07:27:34 visual_prompt]: Inference (val):avg data time: 2.57e-04, avg batch time: 0.3064, average loss: 67.7520
[11/22 07:27:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.35	
[11/22 07:27:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/22 07:29:17 visual_prompt]: 	Training 100/553. train loss: 36.6844,	0.8192 s / batch. (data: 5.46e-03). ETA=12:18:32, max mem: 20.9 GB 
[11/22 07:31:00 visual_prompt]: 	Training 200/553. train loss: 39.5236,	1.2974 s / batch. (data: 4.97e-01). ETA=19:27:34, max mem: 20.9 GB 
[11/22 07:32:40 visual_prompt]: 	Training 300/553. train loss: 87.4285,	0.8277 s / batch. (data: 7.89e-03). ETA=12:23:28, max mem: 20.9 GB 
[11/22 07:34:23 visual_prompt]: 	Training 400/553. train loss: 14.2160,	0.8214 s / batch. (data: 6.28e-04). ETA=12:16:24, max mem: 20.9 GB 
[11/22 07:36:05 visual_prompt]: 	Training 500/553. train loss: 37.0757,	1.4476 s / batch. (data: 6.22e-01). ETA=21:35:27, max mem: 20.9 GB 
[11/22 07:36:57 visual_prompt]: Epoch 3 / 100: avg data time: 1.96e-01, avg batch time: 1.0175, average train loss: 39.4319
[11/22 07:37:54 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3058, average loss: 28.8792
[11/22 07:37:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.40	
[11/22 07:37:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/22 07:39:40 visual_prompt]: 	Training 100/553. train loss: 151.6634,	0.8074 s / batch. (data: 3.15e-04). ETA=12:00:26, max mem: 20.9 GB 
[11/22 07:41:21 visual_prompt]: 	Training 200/553. train loss: 0.4510,	0.9396 s / batch. (data: 1.11e-01). ETA=13:56:53, max mem: 20.9 GB 
[11/22 07:43:02 visual_prompt]: 	Training 300/553. train loss: 7.5754,	1.3916 s / batch. (data: 5.61e-01). ETA=20:37:09, max mem: 20.9 GB 
[11/22 07:44:40 visual_prompt]: 	Training 400/553. train loss: 67.7845,	1.3809 s / batch. (data: 5.75e-01). ETA=20:25:20, max mem: 20.9 GB 
[11/22 07:46:22 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.5183 s / batch. (data: 2.72e+00). ETA=2 days, 3:56:06, max mem: 20.9 GB 
[11/22 07:47:16 visual_prompt]: Epoch 4 / 100: avg data time: 1.96e-01, avg batch time: 1.0156, average train loss: 53.2946
[11/22 07:48:13 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3086, average loss: 34.9306
[11/22 07:48:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.06	
[11/22 07:48:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/22 07:49:57 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8172 s / batch. (data: 7.97e-03). ETA=12:01:42, max mem: 20.9 GB 
[11/22 07:51:37 visual_prompt]: 	Training 200/553. train loss: 35.7910,	1.2476 s / batch. (data: 4.25e-01). ETA=18:19:44, max mem: 20.9 GB 
[11/22 07:53:20 visual_prompt]: 	Training 300/553. train loss: 13.4560,	0.8405 s / batch. (data: 7.91e-04). ETA=12:19:27, max mem: 20.9 GB 
[11/22 07:55:00 visual_prompt]: 	Training 400/553. train loss: 74.0575,	0.8459 s / batch. (data: 3.35e-02). ETA=12:22:47, max mem: 20.9 GB 
[11/22 07:56:42 visual_prompt]: 	Training 500/553. train loss: 51.1401,	0.8080 s / batch. (data: 7.93e-03). ETA=11:48:09, max mem: 20.9 GB 
[11/22 07:57:35 visual_prompt]: Epoch 5 / 100: avg data time: 1.98e-01, avg batch time: 1.0157, average train loss: 70.3916
[11/22 07:58:33 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3089, average loss: 43.8182
[11/22 07:58:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.23	
[11/22 07:58:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/22 08:00:19 visual_prompt]: 	Training 100/553. train loss: 363.2985,	0.8197 s / batch. (data: 8.24e-04). ETA=11:56:19, max mem: 20.9 GB 
[11/22 08:01:59 visual_prompt]: 	Training 200/553. train loss: 621.9333,	0.8298 s / batch. (data: 1.20e-02). ETA=12:03:49, max mem: 20.9 GB 
[11/22 08:03:38 visual_prompt]: 	Training 300/553. train loss: 82.2972,	0.8281 s / batch. (data: 3.52e-04). ETA=12:00:55, max mem: 20.9 GB 
[11/22 08:05:23 visual_prompt]: 	Training 400/553. train loss: 33.9150,	0.8382 s / batch. (data: 5.45e-03). ETA=12:08:19, max mem: 20.9 GB 
[11/22 08:07:02 visual_prompt]: 	Training 500/553. train loss: 96.2137,	0.8267 s / batch. (data: 1.20e-02). ETA=11:56:58, max mem: 20.9 GB 
[11/22 08:07:55 visual_prompt]: Epoch 6 / 100: avg data time: 1.98e-01, avg batch time: 1.0157, average train loss: 103.8233
[11/22 08:08:52 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3060, average loss: 91.2159
[11/22 08:08:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.52	
[11/22 08:08:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/22 08:10:35 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8177 s / batch. (data: 8.99e-03). ETA=11:47:01, max mem: 20.9 GB 
[11/22 08:12:15 visual_prompt]: 	Training 200/553. train loss: 64.2342,	0.8063 s / batch. (data: 3.13e-04). ETA=11:35:50, max mem: 20.9 GB 
[11/22 08:14:00 visual_prompt]: 	Training 300/553. train loss: 5.0139,	2.0817 s / batch. (data: 1.27e+00). ETA=1 day, 5:53:07, max mem: 20.9 GB 
[11/22 08:15:41 visual_prompt]: 	Training 400/553. train loss: 52.8119,	1.9040 s / batch. (data: 1.10e+00). ETA=1 day, 3:16:52, max mem: 20.9 GB 
[11/22 08:17:20 visual_prompt]: 	Training 500/553. train loss: 68.9438,	0.8048 s / batch. (data: 3.12e-04). ETA=11:30:33, max mem: 20.9 GB 
[11/22 08:18:12 visual_prompt]: Epoch 7 / 100: avg data time: 1.95e-01, avg batch time: 1.0123, average train loss: 96.4033
[11/22 08:19:10 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3072, average loss: 174.5468
[11/22 08:19:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.70	
[11/22 08:19:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/22 08:20:53 visual_prompt]: 	Training 100/553. train loss: 53.3281,	0.8104 s / batch. (data: 3.42e-04). ETA=11:33:18, max mem: 20.9 GB 
[11/22 08:22:36 visual_prompt]: 	Training 200/553. train loss: 583.0183,	0.8132 s / batch. (data: 5.42e-03). ETA=11:34:20, max mem: 20.9 GB 
[11/22 08:24:18 visual_prompt]: 	Training 300/553. train loss: 62.8790,	0.8302 s / batch. (data: 1.47e-02). ETA=11:47:26, max mem: 20.9 GB 
[11/22 08:25:58 visual_prompt]: 	Training 400/553. train loss: 97.0186,	1.0120 s / batch. (data: 1.74e-01). ETA=14:20:40, max mem: 20.9 GB 
[11/22 08:27:40 visual_prompt]: 	Training 500/553. train loss: 524.1109,	1.4920 s / batch. (data: 6.66e-01). ETA=21:06:25, max mem: 20.9 GB 
[11/22 08:28:33 visual_prompt]: Epoch 8 / 100: avg data time: 2.00e-01, avg batch time: 1.0177, average train loss: 112.1865
[11/22 08:29:30 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3051, average loss: 16.1529
[11/22 08:29:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.73	
[11/22 08:29:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/22 08:31:15 visual_prompt]: 	Training 100/553. train loss: 0.3753,	0.8390 s / batch. (data: 2.86e-04). ETA=11:50:00, max mem: 20.9 GB 
[11/22 08:32:55 visual_prompt]: 	Training 200/553. train loss: 171.0539,	0.8109 s / batch. (data: 2.84e-04). ETA=11:24:53, max mem: 20.9 GB 
[11/22 08:34:37 visual_prompt]: 	Training 300/553. train loss: 55.7412,	1.8638 s / batch. (data: 1.06e+00). ETA=1 day, 2:11:05, max mem: 20.9 GB 
[11/22 08:36:19 visual_prompt]: 	Training 400/553. train loss: 35.6600,	0.8205 s / batch. (data: 3.58e-04). ETA=11:30:15, max mem: 20.9 GB 
[11/22 08:38:00 visual_prompt]: 	Training 500/553. train loss: 203.6946,	0.9899 s / batch. (data: 1.77e-01). ETA=13:51:08, max mem: 20.9 GB 
[11/22 08:38:52 visual_prompt]: Epoch 9 / 100: avg data time: 1.97e-01, avg batch time: 1.0151, average train loss: 156.6385
[11/22 08:39:50 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3072, average loss: 34.7765
[11/22 08:39:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.50	
[11/22 08:39:50 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/22 08:41:37 visual_prompt]: 	Training 100/553. train loss: 35.8805,	0.8155 s / batch. (data: 3.00e-04). ETA=11:22:37, max mem: 20.9 GB 
[11/22 08:43:16 visual_prompt]: 	Training 200/553. train loss: 283.7655,	0.8181 s / batch. (data: 1.14e-02). ETA=11:23:25, max mem: 20.9 GB 
[11/22 08:44:58 visual_prompt]: 	Training 300/553. train loss: 90.8150,	2.0540 s / batch. (data: 1.24e+00). ETA=1 day, 4:32:28, max mem: 20.9 GB 
[11/22 08:46:37 visual_prompt]: 	Training 400/553. train loss: 170.0476,	0.8115 s / batch. (data: 5.44e-03). ETA=11:15:11, max mem: 20.9 GB 
[11/22 08:48:19 visual_prompt]: 	Training 500/553. train loss: 16.0459,	1.0708 s / batch. (data: 2.47e-01). ETA=14:49:11, max mem: 20.9 GB 
[11/22 08:49:12 visual_prompt]: Epoch 10 / 100: avg data time: 1.99e-01, avg batch time: 1.0172, average train loss: 156.2801
[11/22 08:50:10 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3083, average loss: 7.4814
[11/22 08:50:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 57.60	
[11/22 08:50:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/22 08:51:56 visual_prompt]: 	Training 100/553. train loss: 436.6214,	0.7999 s / batch. (data: 4.27e-04). ETA=11:02:13, max mem: 20.9 GB 
[11/22 08:53:40 visual_prompt]: 	Training 200/553. train loss: 283.7959,	0.8535 s / batch. (data: 1.05e-02). ETA=11:45:09, max mem: 20.9 GB 
[11/22 08:55:21 visual_prompt]: 	Training 300/553. train loss: 7.6936,	2.1890 s / batch. (data: 1.38e+00). ETA=1 day, 6:04:48, max mem: 20.9 GB 
[11/22 08:57:01 visual_prompt]: 	Training 400/553. train loss: 61.5182,	0.8215 s / batch. (data: 3.16e-04). ETA=11:15:58, max mem: 20.9 GB 
[11/22 08:58:40 visual_prompt]: 	Training 500/553. train loss: 224.7789,	0.8160 s / batch. (data: 3.11e-04). ETA=11:10:04, max mem: 20.9 GB 
[11/22 08:59:33 visual_prompt]: Epoch 11 / 100: avg data time: 1.98e-01, avg batch time: 1.0171, average train loss: 171.8130
[11/22 09:00:31 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3074, average loss: 178.9753
[11/22 09:00:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.97	
[11/22 09:00:31 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/22 09:02:20 visual_prompt]: 	Training 100/553. train loss: 185.6118,	0.9185 s / batch. (data: 1.15e-01). ETA=12:31:53, max mem: 20.9 GB 
[11/22 09:04:02 visual_prompt]: 	Training 200/553. train loss: 41.3509,	0.7972 s / batch. (data: 2.89e-04). ETA=10:51:16, max mem: 20.9 GB 
[11/22 09:05:41 visual_prompt]: 	Training 300/553. train loss: 61.1303,	0.8280 s / batch. (data: 3.19e-04). ETA=11:15:03, max mem: 20.9 GB 
[11/22 09:07:23 visual_prompt]: 	Training 400/553. train loss: 41.7388,	0.8136 s / batch. (data: 2.88e-04). ETA=11:01:59, max mem: 20.9 GB 
[11/22 09:09:04 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8290 s / batch. (data: 1.05e-02). ETA=11:13:08, max mem: 20.9 GB 
[11/22 09:09:56 visual_prompt]: Epoch 12 / 100: avg data time: 2.04e-01, avg batch time: 1.0211, average train loss: 194.8883
[11/22 09:10:54 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3051, average loss: 327.9139
[11/22 09:10:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.36	
[11/22 09:10:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/22 09:12:40 visual_prompt]: 	Training 100/553. train loss: 219.3569,	0.8316 s / batch. (data: 3.05e-04). ETA=11:13:05, max mem: 20.9 GB 
[11/22 09:14:17 visual_prompt]: 	Training 200/553. train loss: 397.0056,	0.8135 s / batch. (data: 5.44e-03). ETA=10:57:05, max mem: 20.9 GB 
[11/22 09:15:59 visual_prompt]: 	Training 300/553. train loss: 206.8777,	1.7480 s / batch. (data: 9.25e-01). ETA=23:29:00, max mem: 20.9 GB 
[11/22 09:17:39 visual_prompt]: 	Training 400/553. train loss: 107.5091,	0.8033 s / batch. (data: 3.29e-04). ETA=10:46:11, max mem: 20.9 GB 
[11/22 09:19:21 visual_prompt]: 	Training 500/553. train loss: 72.6317,	0.8280 s / batch. (data: 7.95e-03). ETA=11:04:41, max mem: 20.9 GB 
[11/22 09:20:14 visual_prompt]: Epoch 13 / 100: avg data time: 1.95e-01, avg batch time: 1.0128, average train loss: 154.9497
[11/22 09:21:11 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3066, average loss: 150.0563
[11/22 09:21:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.86	
[11/22 09:21:11 visual_prompt]: Best epoch 13: best metric: -150.056
[11/22 09:21:11 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/22 09:22:57 visual_prompt]: 	Training 100/553. train loss: 67.0006,	0.8200 s / batch. (data: 3.01e-04). ETA=10:56:07, max mem: 20.9 GB 
[11/22 09:24:37 visual_prompt]: 	Training 200/553. train loss: 235.3682,	1.2591 s / batch. (data: 4.36e-01). ETA=16:45:25, max mem: 20.9 GB 
[11/22 09:26:18 visual_prompt]: 	Training 300/553. train loss: 105.3834,	0.8640 s / batch. (data: 3.08e-04). ETA=11:28:28, max mem: 20.9 GB 
[11/22 09:27:58 visual_prompt]: 	Training 400/553. train loss: 222.6599,	0.8062 s / batch. (data: 3.04e-04). ETA=10:41:02, max mem: 20.9 GB 
[11/22 09:29:41 visual_prompt]: 	Training 500/553. train loss: 442.3081,	0.8200 s / batch. (data: 2.84e-04). ETA=10:50:41, max mem: 20.9 GB 
[11/22 09:30:32 visual_prompt]: Epoch 14 / 100: avg data time: 1.95e-01, avg batch time: 1.0146, average train loss: 190.7561
[11/22 09:31:30 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3086, average loss: 204.7610
[11/22 09:31:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.46	
[11/22 09:31:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/22 09:33:15 visual_prompt]: 	Training 100/553. train loss: 86.5963,	0.8072 s / batch. (data: 8.66e-03). ETA=10:38:28, max mem: 20.9 GB 
[11/22 09:34:55 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8280 s / batch. (data: 7.98e-03). ETA=10:53:33, max mem: 20.9 GB 
[11/22 09:36:38 visual_prompt]: 	Training 300/553. train loss: 107.6311,	0.8560 s / batch. (data: 2.95e-04). ETA=11:14:13, max mem: 20.9 GB 
[11/22 09:38:16 visual_prompt]: 	Training 400/553. train loss: 29.8066,	1.3574 s / batch. (data: 5.56e-01). ETA=17:46:54, max mem: 20.9 GB 
[11/22 09:39:58 visual_prompt]: 	Training 500/553. train loss: 232.2905,	0.8265 s / batch. (data: 5.43e-03). ETA=10:48:14, max mem: 20.9 GB 
[11/22 09:40:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.97e-01, avg batch time: 1.0137, average train loss: 186.5540
[11/22 09:41:48 visual_prompt]: Inference (val):avg data time: 8.64e-05, avg batch time: 0.3069, average loss: 120.4946
[11/22 09:41:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.99	
[11/22 09:41:48 visual_prompt]: Best epoch 15: best metric: -120.495
[11/22 09:41:48 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/22 09:43:32 visual_prompt]: 	Training 100/553. train loss: 43.8713,	0.8391 s / batch. (data: 1.05e-02). ETA=10:55:58, max mem: 20.9 GB 
[11/22 09:45:12 visual_prompt]: 	Training 200/553. train loss: 317.8667,	0.8024 s / batch. (data: 2.93e-04). ETA=10:25:55, max mem: 20.9 GB 
[11/22 09:46:54 visual_prompt]: 	Training 300/553. train loss: 10.5811,	0.8294 s / batch. (data: 1.06e-02). ETA=10:45:38, max mem: 20.9 GB 
[11/22 09:48:35 visual_prompt]: 	Training 400/553. train loss: 195.0448,	0.8151 s / batch. (data: 1.56e-02). ETA=10:33:06, max mem: 20.9 GB 
[11/22 09:50:15 visual_prompt]: 	Training 500/553. train loss: 188.4037,	1.5390 s / batch. (data: 7.12e-01). ETA=19:52:49, max mem: 20.9 GB 
[11/22 09:51:08 visual_prompt]: Epoch 16 / 100: avg data time: 1.96e-01, avg batch time: 1.0124, average train loss: 190.2302
[11/22 09:52:06 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3076, average loss: 2.1358
[11/22 09:52:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 52.24	
[11/22 09:52:06 visual_prompt]: Best epoch 16: best metric: -2.136
[11/22 09:52:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/22 09:53:50 visual_prompt]: 	Training 100/553. train loss: 243.7455,	0.8250 s / batch. (data: 5.84e-03). ETA=10:37:19, max mem: 20.9 GB 
[11/22 09:55:32 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8240 s / batch. (data: 2.84e-04). ETA=10:35:12, max mem: 20.9 GB 
[11/22 09:57:12 visual_prompt]: 	Training 300/553. train loss: 380.4587,	0.8214 s / batch. (data: 3.02e-04). ETA=10:31:48, max mem: 20.9 GB 
[11/22 09:58:53 visual_prompt]: 	Training 400/553. train loss: 251.5679,	1.0160 s / batch. (data: 2.04e-01). ETA=12:59:48, max mem: 20.9 GB 
[11/22 10:00:32 visual_prompt]: 	Training 500/553. train loss: 26.9485,	1.2040 s / batch. (data: 3.66e-01). ETA=15:22:07, max mem: 20.9 GB 
[11/22 10:01:26 visual_prompt]: Epoch 17 / 100: avg data time: 1.95e-01, avg batch time: 1.0124, average train loss: 160.8307
[11/22 10:02:23 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3063, average loss: 67.9313
[11/22 10:02:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.46	
[11/22 10:02:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/22 10:04:09 visual_prompt]: 	Training 100/553. train loss: 337.4742,	0.8070 s / batch. (data: 5.41e-03). ETA=10:16:01, max mem: 20.9 GB 
[11/22 10:05:52 visual_prompt]: 	Training 200/553. train loss: 31.8491,	0.8088 s / batch. (data: 3.04e-04). ETA=10:16:02, max mem: 20.9 GB 
[11/22 10:07:34 visual_prompt]: 	Training 300/553. train loss: 227.8670,	0.8229 s / batch. (data: 7.95e-03). ETA=10:25:25, max mem: 20.9 GB 
[11/22 10:09:15 visual_prompt]: 	Training 400/553. train loss: 127.6485,	0.8327 s / batch. (data: 2.99e-04). ETA=10:31:28, max mem: 20.9 GB 
[11/22 10:10:54 visual_prompt]: 	Training 500/553. train loss: 38.8039,	0.8321 s / batch. (data: 2.80e-04). ETA=10:29:36, max mem: 20.9 GB 
[11/22 10:11:46 visual_prompt]: Epoch 18 / 100: avg data time: 1.99e-01, avg batch time: 1.0171, average train loss: 172.3706
[11/22 10:12:43 visual_prompt]: Inference (val):avg data time: 3.32e-04, avg batch time: 0.3060, average loss: 23.0419
[11/22 10:12:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.30	
[11/22 10:12:43 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/22 10:14:28 visual_prompt]: 	Training 100/553. train loss: 30.3725,	1.3474 s / batch. (data: 5.50e-01). ETA=16:56:03, max mem: 20.9 GB 
[11/22 10:16:10 visual_prompt]: 	Training 200/553. train loss: 76.1996,	0.8320 s / batch. (data: 2.96e-04). ETA=10:26:02, max mem: 20.9 GB 
[11/22 10:17:51 visual_prompt]: 	Training 300/553. train loss: 526.4801,	0.8320 s / batch. (data: 3.11e-04). ETA=10:24:36, max mem: 20.9 GB 
[11/22 10:19:34 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8316 s / batch. (data: 9.29e-04). ETA=10:22:57, max mem: 20.9 GB 
[11/22 10:21:12 visual_prompt]: 	Training 500/553. train loss: 9.3277,	0.8160 s / batch. (data: 3.12e-04). ETA=10:09:53, max mem: 20.9 GB 
[11/22 10:22:05 visual_prompt]: Epoch 19 / 100: avg data time: 1.96e-01, avg batch time: 1.0154, average train loss: 195.7430
[11/22 10:23:02 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3066, average loss: 76.6227
[11/22 10:23:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.08	
[11/22 10:23:02 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/22 10:24:46 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8114 s / batch. (data: 5.44e-03). ETA=10:04:23, max mem: 20.9 GB 
[11/22 10:26:28 visual_prompt]: 	Training 200/553. train loss: 80.9129,	0.8228 s / batch. (data: 5.44e-03). ETA=10:11:29, max mem: 20.9 GB 
[11/22 10:28:09 visual_prompt]: 	Training 300/553. train loss: 117.1381,	0.8210 s / batch. (data: 4.65e-04). ETA=10:08:49, max mem: 20.9 GB 
[11/22 10:29:50 visual_prompt]: 	Training 400/553. train loss: 17.3723,	0.8080 s / batch. (data: 3.14e-04). ETA=9:57:49, max mem: 20.9 GB 
[11/22 10:31:29 visual_prompt]: 	Training 500/553. train loss: 100.9234,	0.8109 s / batch. (data: 3.11e-04). ETA=9:58:37, max mem: 20.9 GB 
[11/22 10:32:24 visual_prompt]: Epoch 20 / 100: avg data time: 1.96e-01, avg batch time: 1.0145, average train loss: 160.8579
[11/22 10:33:22 visual_prompt]: Inference (val):avg data time: 6.22e-05, avg batch time: 0.3128, average loss: 58.2357
[11/22 10:33:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.21	
[11/22 10:33:22 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/22 10:35:08 visual_prompt]: 	Training 100/553. train loss: 113.3547,	0.8120 s / batch. (data: 3.45e-04). ETA=9:57:21, max mem: 20.9 GB 
[11/22 10:36:50 visual_prompt]: 	Training 200/553. train loss: 94.7154,	1.1881 s / batch. (data: 2.96e-02). ETA=14:32:05, max mem: 20.9 GB 
[11/22 10:38:31 visual_prompt]: 	Training 300/553. train loss: 420.3132,	1.1972 s / batch. (data: 3.72e-01). ETA=14:36:46, max mem: 20.9 GB 
[11/22 10:40:11 visual_prompt]: 	Training 400/553. train loss: 258.5422,	0.8254 s / batch. (data: 3.81e-04). ETA=10:03:04, max mem: 20.9 GB 
[11/22 10:41:53 visual_prompt]: 	Training 500/553. train loss: 24.6469,	0.8126 s / batch. (data: 3.11e-04). ETA=9:52:24, max mem: 20.9 GB 
[11/22 10:42:44 visual_prompt]: Epoch 21 / 100: avg data time: 1.99e-01, avg batch time: 1.0169, average train loss: 152.4561
[11/22 10:43:42 visual_prompt]: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3114, average loss: 8.9101
[11/22 10:43:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.52	
[11/22 10:43:42 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/22 10:45:27 visual_prompt]: 	Training 100/553. train loss: 76.3283,	0.8111 s / batch. (data: 2.54e-04). ETA=9:49:11, max mem: 20.9 GB 
[11/22 10:47:08 visual_prompt]: 	Training 200/553. train loss: 141.7162,	0.8000 s / batch. (data: 2.98e-04). ETA=9:39:49, max mem: 20.9 GB 
[11/22 10:48:47 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8194 s / batch. (data: 1.06e-02). ETA=9:52:31, max mem: 20.9 GB 
[11/22 10:50:28 visual_prompt]: 	Training 400/553. train loss: 18.4423,	0.8320 s / batch. (data: 2.89e-04). ETA=10:00:14, max mem: 20.9 GB 
[11/22 10:52:09 visual_prompt]: 	Training 500/553. train loss: 34.7550,	0.8240 s / batch. (data: 3.01e-04). ETA=9:53:06, max mem: 20.9 GB 
[11/22 10:53:03 visual_prompt]: Epoch 22 / 100: avg data time: 1.96e-01, avg batch time: 1.0140, average train loss: 170.8602
[11/22 10:54:02 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3087, average loss: 246.9585
[11/22 10:54:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.10	
[11/22 10:54:02 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/22 10:55:47 visual_prompt]: 	Training 100/553. train loss: 186.1240,	0.8160 s / batch. (data: 2.96e-04). ETA=9:45:15, max mem: 20.9 GB 
[11/22 10:57:29 visual_prompt]: 	Training 200/553. train loss: 272.6903,	0.8518 s / batch. (data: 4.50e-02). ETA=10:09:30, max mem: 20.9 GB 
[11/22 10:59:12 visual_prompt]: 	Training 300/553. train loss: 47.7774,	0.8459 s / batch. (data: 1.62e-02). ETA=10:03:54, max mem: 20.9 GB 
[11/22 11:00:51 visual_prompt]: 	Training 400/553. train loss: 123.1810,	0.8240 s / batch. (data: 8.38e-04). ETA=9:46:53, max mem: 20.9 GB 
[11/22 11:02:30 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8400 s / batch. (data: 1.60e-02). ETA=9:56:53, max mem: 20.9 GB 
[11/22 11:03:23 visual_prompt]: Epoch 23 / 100: avg data time: 1.97e-01, avg batch time: 1.0144, average train loss: 160.6898
[11/22 11:04:20 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3071, average loss: 47.8060
[11/22 11:04:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.12	
[11/22 11:04:20 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/22 11:06:17 visual_prompt]: 	Training 100/553. train loss: 14.8031,	0.8303 s / batch. (data: 3.29e-04). ETA=9:47:50, max mem: 20.9 GB 
[11/22 11:08:00 visual_prompt]: 	Training 200/553. train loss: 150.9551,	0.8225 s / batch. (data: 7.94e-03). ETA=9:40:59, max mem: 20.9 GB 
[11/22 11:09:43 visual_prompt]: 	Training 300/553. train loss: 100.0307,	0.9840 s / batch. (data: 1.74e-01). ETA=11:33:23, max mem: 20.9 GB 
[11/22 11:11:24 visual_prompt]: 	Training 400/553. train loss: 40.2626,	0.8360 s / batch. (data: 3.03e-04). ETA=9:47:42, max mem: 20.9 GB 
[11/22 11:13:07 visual_prompt]: 	Training 500/553. train loss: 169.1389,	0.7993 s / batch. (data: 3.39e-04). ETA=9:20:37, max mem: 20.9 GB 
[11/22 11:13:59 visual_prompt]: Epoch 24 / 100: avg data time: 2.28e-01, avg batch time: 1.0470, average train loss: 156.1355
[11/22 11:14:57 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3071, average loss: 125.1586
[11/22 11:14:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.33	
[11/22 11:14:57 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/22 11:16:44 visual_prompt]: 	Training 100/553. train loss: 52.7218,	0.8263 s / batch. (data: 3.24e-04). ETA=9:37:24, max mem: 20.9 GB 
[11/22 11:18:21 visual_prompt]: 	Training 200/553. train loss: 223.0438,	0.8439 s / batch. (data: 2.07e-02). ETA=9:48:19, max mem: 20.9 GB 
[11/22 11:20:01 visual_prompt]: 	Training 300/553. train loss: 104.7876,	0.8219 s / batch. (data: 3.12e-04). ETA=9:31:34, max mem: 20.9 GB 
[11/22 11:21:41 visual_prompt]: 	Training 400/553. train loss: 683.1397,	1.3708 s / batch. (data: 5.66e-01). ETA=15:51:03, max mem: 20.9 GB 
[11/22 11:23:22 visual_prompt]: 	Training 500/553. train loss: 55.3867,	1.5480 s / batch. (data: 7.20e-01). ETA=17:51:27, max mem: 20.9 GB 
[11/22 11:24:15 visual_prompt]: Epoch 25 / 100: avg data time: 1.91e-01, avg batch time: 1.0088, average train loss: 164.1294
[11/22 11:25:12 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3071, average loss: 7.3253
[11/22 11:25:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.97	
[11/22 11:25:12 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/22 11:26:56 visual_prompt]: 	Training 100/553. train loss: 129.7458,	0.8000 s / batch. (data: 3.80e-04). ETA=9:11:38, max mem: 20.9 GB 
[11/22 11:28:37 visual_prompt]: 	Training 200/553. train loss: 1.3798,	1.8307 s / batch. (data: 1.01e+00). ETA=20:59:20, max mem: 20.9 GB 
[11/22 11:30:19 visual_prompt]: 	Training 300/553. train loss: 7.9059,	0.8280 s / batch. (data: 7.39e-04). ETA=9:28:12, max mem: 20.9 GB 
[11/22 11:31:58 visual_prompt]: 	Training 400/553. train loss: 55.3619,	0.8265 s / batch. (data: 2.92e-04). ETA=9:25:47, max mem: 20.9 GB 
[11/22 11:33:36 visual_prompt]: 	Training 500/553. train loss: 29.2409,	0.8519 s / batch. (data: 1.19e-02). ETA=9:41:46, max mem: 20.9 GB 
[11/22 11:34:29 visual_prompt]: Epoch 26 / 100: avg data time: 1.89e-01, avg batch time: 1.0064, average train loss: 145.6865
[11/22 11:35:26 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3071, average loss: 113.1753
[11/22 11:35:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.51	
[11/22 11:35:26 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/22 11:37:11 visual_prompt]: 	Training 100/553. train loss: 7.8318,	0.8327 s / batch. (data: 5.44e-03). ETA=9:26:31, max mem: 20.9 GB 
[11/22 11:38:50 visual_prompt]: 	Training 200/553. train loss: 111.5258,	1.5315 s / batch. (data: 7.05e-01). ETA=17:19:26, max mem: 20.9 GB 
[11/22 11:40:31 visual_prompt]: 	Training 300/553. train loss: 89.1500,	0.8053 s / batch. (data: 8.33e-03). ETA=9:05:11, max mem: 20.9 GB 
[11/22 11:42:13 visual_prompt]: 	Training 400/553. train loss: 120.9062,	0.8167 s / batch. (data: 1.05e-03). ETA=9:11:33, max mem: 20.9 GB 
[11/22 11:43:53 visual_prompt]: 	Training 500/553. train loss: 298.4228,	0.8421 s / batch. (data: 1.05e-02). ETA=9:27:18, max mem: 20.9 GB 
[11/22 11:44:44 visual_prompt]: Epoch 27 / 100: avg data time: 1.91e-01, avg batch time: 1.0083, average train loss: 150.7523
[11/22 11:45:41 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3058, average loss: 162.6716
[11/22 11:45:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.07	
[11/22 11:45:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/22 11:47:24 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8320 s / batch. (data: 3.25e-04). ETA=9:18:23, max mem: 20.9 GB 
[11/22 11:49:05 visual_prompt]: 	Training 200/553. train loss: 48.0571,	0.9086 s / batch. (data: 8.96e-02). ETA=10:08:16, max mem: 20.9 GB 
[11/22 11:50:47 visual_prompt]: 	Training 300/553. train loss: 1.7146,	1.5486 s / batch. (data: 7.42e-01). ETA=17:14:11, max mem: 20.9 GB 
[11/22 11:52:26 visual_prompt]: 	Training 400/553. train loss: 7.2133,	0.8219 s / batch. (data: 2.00e-03). ETA=9:07:28, max mem: 20.9 GB 
[11/22 11:54:04 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8343 s / batch. (data: 1.03e-02). ETA=9:14:22, max mem: 20.9 GB 
[11/22 11:54:57 visual_prompt]: Epoch 28 / 100: avg data time: 1.88e-01, avg batch time: 1.0061, average train loss: 158.9682
[11/22 11:55:55 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3069, average loss: 344.8593
[11/22 11:55:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.29	
[11/22 11:55:55 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/22 11:57:45 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8005 s / batch. (data: 3.85e-04). ETA=8:49:51, max mem: 20.9 GB 
[11/22 11:59:25 visual_prompt]: 	Training 200/553. train loss: 63.6507,	1.8440 s / batch. (data: 1.02e+00). ETA=20:17:32, max mem: 20.9 GB 
[11/22 12:01:04 visual_prompt]: 	Training 300/553. train loss: 61.6685,	0.8332 s / batch. (data: 3.02e-04). ETA=9:08:45, max mem: 20.9 GB 
[11/22 12:02:41 visual_prompt]: 	Training 400/553. train loss: 166.5122,	1.0060 s / batch. (data: 1.72e-01). ETA=11:00:51, max mem: 20.9 GB 
[11/22 12:04:22 visual_prompt]: 	Training 500/553. train loss: 145.6787,	0.8240 s / batch. (data: 3.04e-04). ETA=8:59:56, max mem: 20.9 GB 
[11/22 12:05:15 visual_prompt]: Epoch 29 / 100: avg data time: 1.94e-01, avg batch time: 1.0122, average train loss: 138.1748
[11/22 12:06:12 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3063, average loss: 183.6311
[11/22 12:06:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.50	
[11/22 12:06:12 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/22 12:07:54 visual_prompt]: 	Training 100/553. train loss: 19.7214,	0.8160 s / batch. (data: 3.35e-03). ETA=8:52:36, max mem: 20.9 GB 
[11/22 12:09:36 visual_prompt]: 	Training 200/553. train loss: 509.7855,	0.8185 s / batch. (data: 3.07e-04). ETA=8:52:51, max mem: 20.9 GB 
[11/22 12:11:15 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.3143 s / batch. (data: 5.10e-01). ETA=14:13:27, max mem: 20.9 GB 
[11/22 12:12:57 visual_prompt]: 	Training 400/553. train loss: 28.2085,	1.2800 s / batch. (data: 4.61e-01). ETA=13:49:03, max mem: 20.9 GB 
[11/22 12:14:38 visual_prompt]: 	Training 500/553. train loss: 231.7779,	1.5640 s / batch. (data: 7.33e-01). ETA=16:50:24, max mem: 20.9 GB 
[11/22 12:15:31 visual_prompt]: Epoch 30 / 100: avg data time: 1.94e-01, avg batch time: 1.0113, average train loss: 124.2348
[11/22 12:16:29 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3063, average loss: 68.2561
[11/22 12:16:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.27	
[11/22 12:16:29 visual_prompt]: Stopping early.
[11/22 12:16:29 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 12:16:29 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 12:16:29 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/22 12:16:29 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/22 12:16:29 visual_prompt]: Training with config:
[11/22 12:16:29 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/22 12:16:29 visual_prompt]: Loading training data...
[11/22 12:16:29 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 12:16:29 visual_prompt]: Loading validation data...
[11/22 12:16:29 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 12:16:29 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 12:16:32 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/22 12:16:32 visual_prompt]: tuned percent:0.525
[11/22 12:16:32 visual_prompt]: Device used for model: 0
[11/22 12:16:32 visual_prompt]: Setting up Evaluator...
[11/22 12:16:32 visual_prompt]: Setting up Trainer...
[11/22 12:16:32 visual_prompt]: 	Setting up the optimizer...
[11/22 12:16:32 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 12:18:17 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8294 s / batch. (data: 9.39e-03). ETA=12:43:05, max mem: 20.9 GB 
[11/22 12:19:56 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8302 s / batch. (data: 5.49e-03). ETA=12:42:23, max mem: 20.9 GB 
[11/22 12:21:40 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.4423 s / batch. (data: 1.64e+00). ETA=1 day, 13:18:48, max mem: 20.9 GB 
[11/22 12:23:18 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8068 s / batch. (data: 3.38e-04). ETA=12:18:12, max mem: 20.9 GB 
[11/22 12:25:02 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8200 s / batch. (data: 7.93e-03). ETA=12:28:55, max mem: 20.9 GB 
[11/22 12:25:55 visual_prompt]: Epoch 1 / 100: avg data time: 1.95e-01, avg batch time: 1.0186, average train loss: 1.5403
[11/22 12:26:53 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3072, average loss: 1.5201
[11/22 12:26:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/22 12:26:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/22 12:28:38 visual_prompt]: 	Training 100/553. train loss: 10.6743,	0.8223 s / batch. (data: 2.96e-04). ETA=12:28:58, max mem: 20.9 GB 
[11/22 12:30:18 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8566 s / batch. (data: 2.07e-02). ETA=12:58:46, max mem: 20.9 GB 
[11/22 12:32:01 visual_prompt]: 	Training 300/553. train loss: 0.8119,	1.1607 s / batch. (data: 3.42e-01). ETA=17:33:16, max mem: 20.9 GB 
[11/22 12:33:41 visual_prompt]: 	Training 400/553. train loss: 11.9164,	0.8297 s / batch. (data: 3.04e-04). ETA=12:31:31, max mem: 20.9 GB 
[11/22 12:35:23 visual_prompt]: 	Training 500/553. train loss: 8.2818,	0.8259 s / batch. (data: 3.10e-04). ETA=12:26:40, max mem: 20.9 GB 
[11/22 12:36:15 visual_prompt]: Epoch 2 / 100: avg data time: 1.92e-01, avg batch time: 1.0151, average train loss: 15.2291
[11/22 12:37:12 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3086, average loss: 15.8872
[11/22 12:37:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.27	
[11/22 12:37:12 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/22 12:38:56 visual_prompt]: 	Training 100/553. train loss: 37.7481,	0.8381 s / batch. (data: 3.19e-04). ETA=12:35:37, max mem: 20.9 GB 
[11/22 12:40:38 visual_prompt]: 	Training 200/553. train loss: 27.9482,	1.1033 s / batch. (data: 2.78e-01). ETA=16:32:53, max mem: 20.9 GB 
[11/22 12:42:19 visual_prompt]: 	Training 300/553. train loss: 10.3654,	0.8480 s / batch. (data: 3.06e-04). ETA=12:41:42, max mem: 20.9 GB 
[11/22 12:44:00 visual_prompt]: 	Training 400/553. train loss: 96.2913,	0.8520 s / batch. (data: 5.42e-03). ETA=12:43:51, max mem: 20.9 GB 
[11/22 12:45:42 visual_prompt]: 	Training 500/553. train loss: 67.0479,	1.3640 s / batch. (data: 5.25e-01). ETA=20:20:38, max mem: 20.9 GB 
[11/22 12:46:33 visual_prompt]: Epoch 3 / 100: avg data time: 1.92e-01, avg batch time: 1.0135, average train loss: 40.2478
[11/22 12:47:31 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3088, average loss: 14.2773
[11/22 12:47:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[11/22 12:47:31 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/22 12:49:17 visual_prompt]: 	Training 100/553. train loss: 16.8942,	0.8188 s / batch. (data: 7.94e-03). ETA=12:10:37, max mem: 20.9 GB 
[11/22 12:50:59 visual_prompt]: 	Training 200/553. train loss: 4.3071,	0.8297 s / batch. (data: 3.66e-04). ETA=12:19:01, max mem: 20.9 GB 
[11/22 12:52:40 visual_prompt]: 	Training 300/553. train loss: 86.3309,	1.4037 s / batch. (data: 5.76e-01). ETA=20:47:53, max mem: 20.9 GB 
[11/22 12:54:17 visual_prompt]: 	Training 400/553. train loss: 79.9603,	1.6160 s / batch. (data: 7.86e-01). ETA=23:53:55, max mem: 20.9 GB 
[11/22 12:56:00 visual_prompt]: 	Training 500/553. train loss: 0.0007,	3.5676 s / batch. (data: 2.76e+00). ETA=2 days, 4:39:44, max mem: 20.9 GB 
[11/22 12:56:54 visual_prompt]: Epoch 4 / 100: avg data time: 1.99e-01, avg batch time: 1.0178, average train loss: 63.0865
[11/22 12:57:52 visual_prompt]: Inference (val):avg data time: 3.37e-04, avg batch time: 0.3050, average loss: 14.3394
[11/22 12:57:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.73	
[11/22 12:57:52 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/22 12:59:35 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8120 s / batch. (data: 3.05e-04). ETA=11:57:04, max mem: 20.9 GB 
[11/22 13:01:17 visual_prompt]: 	Training 200/553. train loss: 38.8275,	1.4666 s / batch. (data: 6.67e-01). ETA=21:32:47, max mem: 20.9 GB 
[11/22 13:02:59 visual_prompt]: 	Training 300/553. train loss: 151.9556,	0.8278 s / batch. (data: 2.95e-04). ETA=12:08:18, max mem: 20.9 GB 
[11/22 13:04:39 visual_prompt]: 	Training 400/553. train loss: 228.9831,	0.8320 s / batch. (data: 5.41e-03). ETA=12:10:36, max mem: 20.9 GB 
[11/22 13:06:20 visual_prompt]: 	Training 500/553. train loss: 162.6530,	0.8518 s / batch. (data: 4.02e-04). ETA=12:26:36, max mem: 20.9 GB 
[11/22 13:07:13 visual_prompt]: Epoch 5 / 100: avg data time: 1.95e-01, avg batch time: 1.0149, average train loss: 59.6810
[11/22 13:08:11 visual_prompt]: Inference (val):avg data time: 4.43e-05, avg batch time: 0.3067, average loss: 53.9044
[11/22 13:08:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.30	
[11/22 13:08:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/22 13:09:57 visual_prompt]: 	Training 100/553. train loss: 39.3588,	0.8200 s / batch. (data: 2.49e-04). ETA=11:56:37, max mem: 20.9 GB 
[11/22 13:11:38 visual_prompt]: 	Training 200/553. train loss: 64.8807,	0.8429 s / batch. (data: 1.89e-02). ETA=12:15:13, max mem: 20.9 GB 
[11/22 13:13:17 visual_prompt]: 	Training 300/553. train loss: 19.3553,	0.8320 s / batch. (data: 1.19e-02). ETA=12:04:18, max mem: 20.9 GB 
[11/22 13:15:03 visual_prompt]: 	Training 400/553. train loss: 41.5402,	0.8280 s / batch. (data: 1.19e-02). ETA=11:59:26, max mem: 20.9 GB 
[11/22 13:16:42 visual_prompt]: 	Training 500/553. train loss: 67.3078,	0.8494 s / batch. (data: 9.34e-03). ETA=12:16:40, max mem: 20.9 GB 
[11/22 13:17:34 visual_prompt]: Epoch 6 / 100: avg data time: 1.98e-01, avg batch time: 1.0182, average train loss: 76.6691
[11/22 13:18:32 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3057, average loss: 44.6269
[11/22 13:18:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.56	
[11/22 13:18:32 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/22 13:20:15 visual_prompt]: 	Training 100/553. train loss: 801.0915,	0.8537 s / batch. (data: 1.33e-03). ETA=12:18:10, max mem: 20.9 GB 
[11/22 13:21:56 visual_prompt]: 	Training 200/553. train loss: 8.7732,	0.8404 s / batch. (data: 5.45e-03). ETA=12:05:19, max mem: 20.9 GB 
[11/22 13:23:41 visual_prompt]: 	Training 300/553. train loss: 77.1621,	2.0592 s / batch. (data: 1.25e+00). ETA=1 day, 5:33:44, max mem: 20.9 GB 
[11/22 13:25:21 visual_prompt]: 	Training 400/553. train loss: 87.9645,	2.1062 s / batch. (data: 1.29e+00). ETA=1 day, 6:10:40, max mem: 20.9 GB 
[11/22 13:27:01 visual_prompt]: 	Training 500/553. train loss: 22.3223,	0.8160 s / batch. (data: 3.15e-04). ETA=11:40:09, max mem: 20.9 GB 
[11/22 13:27:52 visual_prompt]: Epoch 7 / 100: avg data time: 1.94e-01, avg batch time: 1.0126, average train loss: 118.3119
[11/22 13:28:50 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3060, average loss: 200.2819
[11/22 13:28:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.85	
[11/22 13:28:50 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/22 13:30:33 visual_prompt]: 	Training 100/553. train loss: 38.9741,	0.8118 s / batch. (data: 3.28e-04). ETA=11:34:30, max mem: 20.9 GB 
[11/22 13:32:16 visual_prompt]: 	Training 200/553. train loss: 10.0451,	0.8333 s / batch. (data: 5.54e-03). ETA=11:51:29, max mem: 20.9 GB 
[11/22 13:33:57 visual_prompt]: 	Training 300/553. train loss: 94.3741,	0.8280 s / batch. (data: 1.20e-02). ETA=11:45:33, max mem: 20.9 GB 
[11/22 13:35:38 visual_prompt]: 	Training 400/553. train loss: 1.2774,	0.9144 s / batch. (data: 1.07e-01). ETA=12:57:41, max mem: 20.9 GB 
[11/22 13:37:20 visual_prompt]: 	Training 500/553. train loss: 130.4558,	1.3800 s / batch. (data: 5.29e-01). ETA=19:31:20, max mem: 20.9 GB 
[11/22 13:38:13 visual_prompt]: Epoch 8 / 100: avg data time: 2.00e-01, avg batch time: 1.0181, average train loss: 105.7661
[11/22 13:39:11 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3049, average loss: 36.9941
[11/22 13:39:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.93	
[11/22 13:39:11 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/22 13:40:56 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8678 s / batch. (data: 1.18e-02). ETA=12:14:24, max mem: 20.9 GB 
[11/22 13:42:36 visual_prompt]: 	Training 200/553. train loss: 0.3531,	0.8158 s / batch. (data: 3.02e-04). ETA=11:28:59, max mem: 20.9 GB 
[11/22 13:44:17 visual_prompt]: 	Training 300/553. train loss: 52.0310,	1.5774 s / batch. (data: 7.67e-01). ETA=22:09:40, max mem: 20.9 GB 
[11/22 13:46:00 visual_prompt]: 	Training 400/553. train loss: 45.0630,	0.8380 s / batch. (data: 5.54e-03). ETA=11:44:57, max mem: 20.9 GB 
[11/22 13:47:41 visual_prompt]: 	Training 500/553. train loss: 277.6619,	0.9987 s / batch. (data: 1.76e-01). ETA=13:58:32, max mem: 20.9 GB 
[11/22 13:48:33 visual_prompt]: Epoch 9 / 100: avg data time: 1.97e-01, avg batch time: 1.0154, average train loss: 126.2693
[11/22 13:49:31 visual_prompt]: Inference (val):avg data time: 2.06e-04, avg batch time: 0.3076, average loss: 152.5892
[11/22 13:49:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.18	
[11/22 13:49:31 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/22 13:51:19 visual_prompt]: 	Training 100/553. train loss: 187.3826,	0.8020 s / batch. (data: 2.94e-04). ETA=11:11:16, max mem: 20.9 GB 
[11/22 13:52:58 visual_prompt]: 	Training 200/553. train loss: 37.7419,	0.8560 s / batch. (data: 3.26e-04). ETA=11:55:05, max mem: 20.9 GB 
[11/22 13:54:38 visual_prompt]: 	Training 300/553. train loss: 90.5083,	2.8120 s / batch. (data: 1.98e+00). ETA=1 day, 15:04:23, max mem: 20.9 GB 
[11/22 13:56:17 visual_prompt]: 	Training 400/553. train loss: 265.5869,	0.8312 s / batch. (data: 5.52e-03). ETA=11:31:36, max mem: 20.9 GB 
[11/22 13:58:00 visual_prompt]: 	Training 500/553. train loss: 52.3765,	1.1851 s / batch. (data: 3.62e-01). ETA=16:24:05, max mem: 20.9 GB 
[11/22 13:58:54 visual_prompt]: Epoch 10 / 100: avg data time: 1.99e-01, avg batch time: 1.0181, average train loss: 146.8459
[11/22 13:59:51 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3067, average loss: 202.4706
[11/22 13:59:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.96	
[11/22 13:59:51 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/22 14:01:38 visual_prompt]: 	Training 100/553. train loss: 170.1281,	0.8079 s / batch. (data: 3.03e-04). ETA=11:08:48, max mem: 20.9 GB 
[11/22 14:03:21 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8139 s / batch. (data: 2.99e-04). ETA=11:12:23, max mem: 20.9 GB 
[11/22 14:05:00 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1040 s / batch. (data: 1.28e+00). ETA=1 day, 4:54:44, max mem: 20.9 GB 
[11/22 14:06:40 visual_prompt]: 	Training 400/553. train loss: 144.2417,	0.8280 s / batch. (data: 5.37e-03). ETA=11:21:17, max mem: 20.9 GB 
[11/22 14:08:19 visual_prompt]: 	Training 500/553. train loss: 227.5852,	0.8195 s / batch. (data: 5.47e-03). ETA=11:12:59, max mem: 20.9 GB 
[11/22 14:09:11 visual_prompt]: Epoch 11 / 100: avg data time: 1.93e-01, avg batch time: 1.0114, average train loss: 167.5750
[11/22 14:10:09 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3071, average loss: 24.0446
[11/22 14:10:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.40	
[11/22 14:10:09 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/22 14:11:55 visual_prompt]: 	Training 100/553. train loss: 120.9846,	1.1723 s / batch. (data: 3.51e-01). ETA=15:59:42, max mem: 20.9 GB 
[11/22 14:13:37 visual_prompt]: 	Training 200/553. train loss: 51.3779,	1.3800 s / batch. (data: 5.57e-01). ETA=18:47:23, max mem: 20.9 GB 
[11/22 14:15:17 visual_prompt]: 	Training 300/553. train loss: 18.5274,	0.8400 s / batch. (data: 3.89e-04). ETA=11:24:50, max mem: 20.9 GB 
[11/22 14:16:57 visual_prompt]: 	Training 400/553. train loss: 151.8916,	0.8103 s / batch. (data: 3.06e-04). ETA=10:59:14, max mem: 20.9 GB 
[11/22 14:18:38 visual_prompt]: 	Training 500/553. train loss: 212.2830,	0.8240 s / batch. (data: 7.95e-03). ETA=11:09:03, max mem: 20.9 GB 
[11/22 14:19:29 visual_prompt]: Epoch 12 / 100: avg data time: 1.95e-01, avg batch time: 1.0136, average train loss: 174.7716
[11/22 14:20:27 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3061, average loss: 324.7250
[11/22 14:20:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.69	
[11/22 14:20:27 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/22 14:22:13 visual_prompt]: 	Training 100/553. train loss: 46.6780,	0.8480 s / batch. (data: 3.20e-04). ETA=11:26:22, max mem: 20.9 GB 
[11/22 14:23:51 visual_prompt]: 	Training 200/553. train loss: 108.3145,	0.8240 s / batch. (data: 5.27e-04). ETA=11:05:32, max mem: 20.9 GB 
[11/22 14:25:33 visual_prompt]: 	Training 300/553. train loss: 56.2402,	1.8480 s / batch. (data: 1.05e+00). ETA=1 day, 0:49:36, max mem: 20.9 GB 
[11/22 14:27:13 visual_prompt]: 	Training 400/553. train loss: 401.1780,	0.8240 s / batch. (data: 1.20e-02). ETA=11:02:49, max mem: 20.9 GB 
[11/22 14:28:55 visual_prompt]: 	Training 500/553. train loss: 278.3896,	0.8054 s / batch. (data: 2.89e-04). ETA=10:46:29, max mem: 20.9 GB 
[11/22 14:29:48 visual_prompt]: Epoch 13 / 100: avg data time: 1.94e-01, avg batch time: 1.0127, average train loss: 174.0920
[11/22 14:30:45 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3069, average loss: 44.2031
[11/22 14:30:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 50.94	
[11/22 14:30:45 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/22 14:32:31 visual_prompt]: 	Training 100/553. train loss: 1101.5603,	0.8266 s / batch. (data: 2.27e-02). ETA=11:01:25, max mem: 20.9 GB 
[11/22 14:34:12 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.4513 s / batch. (data: 6.26e-01). ETA=19:18:52, max mem: 20.9 GB 
[11/22 14:35:53 visual_prompt]: 	Training 300/553. train loss: 54.3812,	0.8398 s / batch. (data: 5.45e-03). ETA=11:09:13, max mem: 20.9 GB 
[11/22 14:37:33 visual_prompt]: 	Training 400/553. train loss: 266.3464,	0.8094 s / batch. (data: 3.13e-04). ETA=10:43:35, max mem: 20.9 GB 
[11/22 14:39:14 visual_prompt]: 	Training 500/553. train loss: 163.6314,	0.8080 s / batch. (data: 2.67e-04). ETA=10:41:10, max mem: 20.9 GB 
[11/22 14:40:06 visual_prompt]: Epoch 14 / 100: avg data time: 1.95e-01, avg batch time: 1.0141, average train loss: 158.8360
[11/22 14:41:04 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3070, average loss: 98.8413
[11/22 14:41:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.20	
[11/22 14:41:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/22 14:42:49 visual_prompt]: 	Training 100/553. train loss: 106.0826,	1.5520 s / batch. (data: 7.35e-01). ETA=20:27:33, max mem: 20.9 GB 
[11/22 14:44:28 visual_prompt]: 	Training 200/553. train loss: 797.1801,	0.8238 s / batch. (data: 5.51e-03). ETA=10:50:13, max mem: 20.9 GB 
[11/22 14:46:10 visual_prompt]: 	Training 300/553. train loss: 390.2479,	0.8320 s / batch. (data: 2.72e-04). ETA=10:55:18, max mem: 20.9 GB 
[11/22 14:47:49 visual_prompt]: 	Training 400/553. train loss: 66.4679,	1.1760 s / batch. (data: 3.35e-01). ETA=15:24:17, max mem: 20.9 GB 
[11/22 14:49:31 visual_prompt]: 	Training 500/553. train loss: 56.4750,	0.8188 s / batch. (data: 5.45e-03). ETA=10:42:13, max mem: 20.9 GB 
[11/22 14:50:23 visual_prompt]: Epoch 15 / 100: avg data time: 1.93e-01, avg batch time: 1.0110, average train loss: 165.2661
[11/22 14:51:21 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3060, average loss: 518.4259
[11/22 14:51:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[11/22 14:51:21 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/22 14:53:05 visual_prompt]: 	Training 100/553. train loss: 103.7124,	0.8040 s / batch. (data: 3.09e-04). ETA=10:28:31, max mem: 20.9 GB 
[11/22 14:54:46 visual_prompt]: 	Training 200/553. train loss: 263.4484,	0.8098 s / batch. (data: 3.07e-04). ETA=10:31:44, max mem: 20.9 GB 
[11/22 14:56:27 visual_prompt]: 	Training 300/553. train loss: 192.0815,	0.8159 s / batch. (data: 2.97e-04). ETA=10:35:06, max mem: 20.9 GB 
[11/22 14:58:08 visual_prompt]: 	Training 400/553. train loss: 154.5278,	0.8178 s / batch. (data: 2.99e-04). ETA=10:35:12, max mem: 20.9 GB 
[11/22 14:59:48 visual_prompt]: 	Training 500/553. train loss: 77.0727,	1.1738 s / batch. (data: 3.75e-01). ETA=15:09:49, max mem: 20.9 GB 
[11/22 15:00:41 visual_prompt]: Epoch 16 / 100: avg data time: 1.95e-01, avg batch time: 1.0126, average train loss: 160.8979
[11/22 15:01:39 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3070, average loss: 42.0164
[11/22 15:01:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 51.17	
[11/22 15:01:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/22 15:03:24 visual_prompt]: 	Training 100/553. train loss: 53.9767,	0.8160 s / batch. (data: 2.95e-04). ETA=10:30:22, max mem: 20.9 GB 
[11/22 15:05:06 visual_prompt]: 	Training 200/553. train loss: 253.7905,	0.8163 s / batch. (data: 2.95e-04). ETA=10:29:16, max mem: 20.9 GB 
[11/22 15:06:46 visual_prompt]: 	Training 300/553. train loss: 176.7908,	0.8311 s / batch. (data: 5.42e-03). ETA=10:39:15, max mem: 20.9 GB 
[11/22 15:08:26 visual_prompt]: 	Training 400/553. train loss: 53.5881,	0.8120 s / batch. (data: 3.38e-04). ETA=10:23:13, max mem: 20.9 GB 
[11/22 15:10:06 visual_prompt]: 	Training 500/553. train loss: 101.5773,	1.7708 s / batch. (data: 9.41e-01). ETA=22:36:11, max mem: 20.9 GB 
[11/22 15:11:00 visual_prompt]: Epoch 17 / 100: avg data time: 1.97e-01, avg batch time: 1.0146, average train loss: 177.7189
[11/22 15:11:58 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3072, average loss: 180.0793
[11/22 15:11:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.05	
[11/22 15:11:58 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/22 15:13:43 visual_prompt]: 	Training 100/553. train loss: 42.3787,	0.8341 s / batch. (data: 3.03e-04). ETA=10:36:38, max mem: 20.9 GB 
[11/22 15:15:27 visual_prompt]: 	Training 200/553. train loss: 41.7151,	0.8240 s / batch. (data: 8.74e-04). ETA=10:27:34, max mem: 20.9 GB 
[11/22 15:17:08 visual_prompt]: 	Training 300/553. train loss: 76.5457,	0.8208 s / batch. (data: 2.06e-02). ETA=10:23:46, max mem: 20.9 GB 
[11/22 15:18:48 visual_prompt]: 	Training 400/553. train loss: 110.9185,	0.8008 s / batch. (data: 3.14e-04). ETA=10:07:13, max mem: 20.9 GB 
[11/22 15:20:28 visual_prompt]: 	Training 500/553. train loss: 114.4731,	0.8120 s / batch. (data: 3.21e-04). ETA=10:14:25, max mem: 20.9 GB 
[11/22 15:21:19 visual_prompt]: Epoch 18 / 100: avg data time: 1.96e-01, avg batch time: 1.0143, average train loss: 154.6931
[11/22 15:22:17 visual_prompt]: Inference (val):avg data time: 5.96e-04, avg batch time: 0.3072, average loss: 218.2068
[11/22 15:22:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.01	
[11/22 15:22:17 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/22 15:24:01 visual_prompt]: 	Training 100/553. train loss: 100.4358,	0.8126 s / batch. (data: 2.72e-04). ETA=10:12:45, max mem: 20.9 GB 
[11/22 15:25:43 visual_prompt]: 	Training 200/553. train loss: 326.8077,	0.8105 s / batch. (data: 2.60e-04). ETA=10:09:52, max mem: 20.9 GB 
[11/22 15:27:24 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8191 s / batch. (data: 3.19e-04). ETA=10:14:55, max mem: 20.9 GB 
[11/22 15:29:06 visual_prompt]: 	Training 400/553. train loss: 211.2019,	0.8191 s / batch. (data: 5.40e-03). ETA=10:13:33, max mem: 20.9 GB 
[11/22 15:30:42 visual_prompt]: 	Training 500/553. train loss: 568.7255,	0.8339 s / batch. (data: 2.06e-02). ETA=10:23:16, max mem: 20.9 GB 
[11/22 15:31:35 visual_prompt]: Epoch 19 / 100: avg data time: 1.92e-01, avg batch time: 1.0095, average train loss: 181.0746
[11/22 15:32:33 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3068, average loss: 410.1010
[11/22 15:32:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 36.76	
[11/22 15:32:33 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/22 15:34:17 visual_prompt]: 	Training 100/553. train loss: 10.1556,	0.8397 s / batch. (data: 2.37e-02). ETA=10:25:30, max mem: 20.9 GB 
[11/22 15:35:58 visual_prompt]: 	Training 200/553. train loss: 81.5022,	0.8222 s / batch. (data: 4.69e-04). ETA=10:11:05, max mem: 20.9 GB 
[11/22 15:37:39 visual_prompt]: 	Training 300/553. train loss: 175.6327,	0.8320 s / batch. (data: 7.56e-04). ETA=10:16:57, max mem: 20.9 GB 
[11/22 15:39:20 visual_prompt]: 	Training 400/553. train loss: 416.4282,	0.8419 s / batch. (data: 2.06e-02). ETA=10:22:53, max mem: 20.9 GB 
[11/22 15:41:00 visual_prompt]: 	Training 500/553. train loss: 294.9010,	0.8200 s / batch. (data: 3.08e-04). ETA=10:05:21, max mem: 20.9 GB 
[11/22 15:41:54 visual_prompt]: Epoch 20 / 100: avg data time: 1.95e-01, avg batch time: 1.0138, average train loss: 159.3108
[11/22 15:42:51 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3069, average loss: 33.0233
[11/22 15:42:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.73	
[11/22 15:42:51 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/22 15:44:39 visual_prompt]: 	Training 100/553. train loss: 51.4833,	0.8080 s / batch. (data: 3.16e-04). ETA=9:54:26, max mem: 20.9 GB 
[11/22 15:46:19 visual_prompt]: 	Training 200/553. train loss: 244.3890,	0.8140 s / batch. (data: 5.41e-03). ETA=9:57:26, max mem: 20.9 GB 
[11/22 15:48:00 visual_prompt]: 	Training 300/553. train loss: 32.4769,	1.1280 s / batch. (data: 2.99e-01). ETA=13:46:03, max mem: 20.9 GB 
[11/22 15:49:40 visual_prompt]: 	Training 400/553. train loss: 14.3152,	0.8480 s / batch. (data: 3.01e-04). ETA=10:19:36, max mem: 20.9 GB 
[11/22 15:51:22 visual_prompt]: 	Training 500/553. train loss: 38.1298,	0.8160 s / batch. (data: 2.84e-04). ETA=9:54:52, max mem: 20.9 GB 
[11/22 15:52:14 visual_prompt]: Epoch 21 / 100: avg data time: 1.98e-01, avg batch time: 1.0164, average train loss: 169.9309
[11/22 15:53:11 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3061, average loss: 123.6120
[11/22 15:53:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.34	
[11/22 15:53:11 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/22 15:54:56 visual_prompt]: 	Training 100/553. train loss: 72.6496,	0.8560 s / batch. (data: 2.94e-04). ETA=10:21:50, max mem: 20.9 GB 
[11/22 15:56:36 visual_prompt]: 	Training 200/553. train loss: 85.5240,	0.8191 s / batch. (data: 5.42e-03). ETA=9:53:39, max mem: 20.9 GB 
[11/22 15:58:15 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8063 s / batch. (data: 1.05e-02). ETA=9:43:03, max mem: 20.9 GB 
[11/22 15:59:57 visual_prompt]: 	Training 400/553. train loss: 131.4046,	0.8275 s / batch. (data: 3.11e-04). ETA=9:56:58, max mem: 20.9 GB 
[11/22 16:01:38 visual_prompt]: 	Training 500/553. train loss: 29.0696,	0.8270 s / batch. (data: 3.00e-04). ETA=9:55:15, max mem: 20.9 GB 
[11/22 16:02:33 visual_prompt]: Epoch 22 / 100: avg data time: 1.96e-01, avg batch time: 1.0146, average train loss: 160.6525
[11/22 16:03:31 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3073, average loss: 23.9632
[11/22 16:03:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.20	
[11/22 16:03:31 visual_prompt]: Best epoch 22: best metric: -23.963
[11/22 16:03:31 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/22 16:05:16 visual_prompt]: 	Training 100/553. train loss: 80.6201,	0.8676 s / batch. (data: 1.15e-02). ETA=10:22:14, max mem: 20.9 GB 
[11/22 16:06:58 visual_prompt]: 	Training 200/553. train loss: 155.8318,	0.8363 s / batch. (data: 5.47e-03). ETA=9:58:25, max mem: 20.9 GB 
[11/22 16:08:41 visual_prompt]: 	Training 300/553. train loss: 79.0297,	0.8221 s / batch. (data: 5.43e-03). ETA=9:46:55, max mem: 20.9 GB 
[11/22 16:10:20 visual_prompt]: 	Training 400/553. train loss: 192.9761,	0.8404 s / batch. (data: 1.06e-02). ETA=9:58:32, max mem: 20.9 GB 
[11/22 16:11:59 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8130 s / batch. (data: 2.96e-04). ETA=9:37:39, max mem: 20.9 GB 
[11/22 16:12:51 visual_prompt]: Epoch 23 / 100: avg data time: 1.95e-01, avg batch time: 1.0130, average train loss: 154.6760
[11/22 16:13:49 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3071, average loss: 340.3469
[11/22 16:13:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.29	
[11/22 16:13:49 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/22 16:15:31 visual_prompt]: 	Training 100/553. train loss: 2.7256,	0.8240 s / batch. (data: 3.00e-04). ETA=9:43:25, max mem: 20.9 GB 
[11/22 16:17:12 visual_prompt]: 	Training 200/553. train loss: 166.2729,	0.8106 s / batch. (data: 3.04e-04). ETA=9:32:32, max mem: 20.9 GB 
[11/22 16:18:53 visual_prompt]: 	Training 300/553. train loss: 232.3090,	0.9594 s / batch. (data: 1.47e-01). ETA=11:16:04, max mem: 20.9 GB 
[11/22 16:20:34 visual_prompt]: 	Training 400/553. train loss: 35.5179,	0.8403 s / batch. (data: 1.05e-02). ETA=9:50:44, max mem: 20.9 GB 
[11/22 16:22:16 visual_prompt]: 	Training 500/553. train loss: 125.8313,	0.8243 s / batch. (data: 1.06e-02). ETA=9:38:06, max mem: 20.9 GB 
[11/22 16:23:10 visual_prompt]: Epoch 24 / 100: avg data time: 1.96e-01, avg batch time: 1.0139, average train loss: 161.9279
[11/22 16:24:08 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.3060, average loss: 42.9661
[11/22 16:24:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.48	
[11/22 16:24:08 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/22 16:25:55 visual_prompt]: 	Training 100/553. train loss: 334.7375,	0.8181 s / batch. (data: 3.05e-04). ETA=9:31:40, max mem: 20.9 GB 
[11/22 16:27:34 visual_prompt]: 	Training 200/553. train loss: 7.2485,	1.4571 s / batch. (data: 6.35e-01). ETA=16:55:48, max mem: 20.9 GB 
[11/22 16:29:14 visual_prompt]: 	Training 300/553. train loss: 403.8047,	1.4830 s / batch. (data: 6.64e-01). ETA=17:11:24, max mem: 20.9 GB 
[11/22 16:30:55 visual_prompt]: 	Training 400/553. train loss: 52.0730,	1.2640 s / batch. (data: 4.56e-01). ETA=14:36:58, max mem: 20.9 GB 
[11/22 16:32:36 visual_prompt]: 	Training 500/553. train loss: 298.8242,	1.5120 s / batch. (data: 6.90e-01). ETA=17:26:31, max mem: 20.9 GB 
[11/22 16:33:28 visual_prompt]: Epoch 25 / 100: avg data time: 1.95e-01, avg batch time: 1.0135, average train loss: 149.2566
[11/22 16:34:26 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3082, average loss: 127.0611
[11/22 16:34:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.03	
[11/22 16:34:26 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/22 16:36:11 visual_prompt]: 	Training 100/553. train loss: 256.6980,	0.8080 s / batch. (data: 2.98e-04). ETA=9:17:10, max mem: 20.9 GB 
[11/22 16:37:53 visual_prompt]: 	Training 200/553. train loss: 16.0175,	1.8005 s / batch. (data: 9.92e-01). ETA=20:38:35, max mem: 20.9 GB 
[11/22 16:39:36 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8462 s / batch. (data: 1.14e-02). ETA=9:40:42, max mem: 20.9 GB 
[11/22 16:41:16 visual_prompt]: 	Training 400/553. train loss: 180.0629,	0.8248 s / batch. (data: 3.98e-04). ETA=9:24:38, max mem: 20.9 GB 
[11/22 16:42:55 visual_prompt]: 	Training 500/553. train loss: 32.3860,	0.8280 s / batch. (data: 2.90e-04). ETA=9:25:28, max mem: 20.9 GB 
[11/22 16:43:48 visual_prompt]: Epoch 26 / 100: avg data time: 1.97e-01, avg batch time: 1.0146, average train loss: 159.3604
[11/22 16:44:45 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3081, average loss: 10.0895
[11/22 16:44:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 55.72	
[11/22 16:44:45 visual_prompt]: Best epoch 26: best metric: -10.089
[11/22 16:44:45 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/22 16:46:31 visual_prompt]: 	Training 100/553. train loss: 340.0575,	0.8200 s / batch. (data: 2.85e-04). ETA=9:17:54, max mem: 20.9 GB 
[11/22 16:48:10 visual_prompt]: 	Training 200/553. train loss: 360.8433,	1.3840 s / batch. (data: 5.65e-01). ETA=15:39:19, max mem: 20.9 GB 
[11/22 16:49:52 visual_prompt]: 	Training 300/553. train loss: 7.3428,	0.8148 s / batch. (data: 3.08e-04). ETA=9:11:39, max mem: 20.9 GB 
[11/22 16:51:34 visual_prompt]: 	Training 400/553. train loss: 24.9038,	0.8319 s / batch. (data: 2.19e-02). ETA=9:21:49, max mem: 20.9 GB 
[11/22 16:53:16 visual_prompt]: 	Training 500/553. train loss: 30.3918,	0.8320 s / batch. (data: 1.02e-03). ETA=9:20:32, max mem: 20.9 GB 
[11/22 16:54:06 visual_prompt]: Epoch 27 / 100: avg data time: 1.95e-01, avg batch time: 1.0137, average train loss: 136.9341
[11/22 16:55:04 visual_prompt]: Inference (val):avg data time: 3.66e-04, avg batch time: 0.3072, average loss: 98.2299
[11/22 16:55:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/22 16:55:04 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/22 16:56:48 visual_prompt]: 	Training 100/553. train loss: 128.2742,	1.5868 s / batch. (data: 7.73e-01). ETA=17:44:57, max mem: 20.9 GB 
[11/22 16:58:30 visual_prompt]: 	Training 200/553. train loss: 186.6500,	0.8201 s / batch. (data: 9.16e-03). ETA=9:09:02, max mem: 20.9 GB 
[11/22 17:00:21 visual_prompt]: 	Training 300/553. train loss: 20.4521,	1.7412 s / batch. (data: 9.38e-01). ETA=19:22:47, max mem: 20.9 GB 
[11/22 17:02:07 visual_prompt]: 	Training 400/553. train loss: 116.6639,	0.8184 s / batch. (data: 7.97e-03). ETA=9:05:10, max mem: 20.9 GB 
[11/22 17:03:48 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 8.05e-04). ETA=9:04:52, max mem: 20.9 GB 
[11/22 17:04:41 visual_prompt]: Epoch 28 / 100: avg data time: 2.26e-01, avg batch time: 1.0438, average train loss: 154.5328
[11/22 17:05:39 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3052, average loss: 47.1506
[11/22 17:05:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.10	
[11/22 17:05:39 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/22 17:07:30 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8120 s / batch. (data: 2.98e-04). ETA=8:57:28, max mem: 20.9 GB 
[11/22 17:09:14 visual_prompt]: 	Training 200/553. train loss: 33.9879,	2.1977 s / batch. (data: 1.39e+00). ETA=1 day, 0:11:05, max mem: 20.9 GB 
[11/22 17:10:53 visual_prompt]: 	Training 300/553. train loss: 181.5701,	0.8160 s / batch. (data: 3.08e-04). ETA=8:57:25, max mem: 20.9 GB 
[11/22 17:12:31 visual_prompt]: 	Training 400/553. train loss: 188.7184,	1.3195 s / batch. (data: 4.95e-01). ETA=14:26:50, max mem: 20.9 GB 
[11/22 17:14:11 visual_prompt]: 	Training 500/553. train loss: 131.5021,	0.8240 s / batch. (data: 3.34e-04). ETA=8:59:56, max mem: 20.9 GB 
[11/22 17:15:03 visual_prompt]: Epoch 29 / 100: avg data time: 2.03e-01, avg batch time: 1.0202, average train loss: 165.1326
[11/22 17:16:01 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3075, average loss: 137.0830
[11/22 17:16:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.47	
[11/22 17:16:01 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/22 17:17:44 visual_prompt]: 	Training 100/553. train loss: 137.3621,	0.8280 s / batch. (data: 7.93e-03). ETA=9:00:26, max mem: 20.9 GB 
[11/22 17:19:26 visual_prompt]: 	Training 200/553. train loss: 293.8915,	0.8143 s / batch. (data: 2.86e-04). ETA=8:50:09, max mem: 20.9 GB 
[11/22 17:21:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.4214 s / batch. (data: 6.15e-01). ETA=15:23:02, max mem: 20.9 GB 
[11/22 17:22:48 visual_prompt]: 	Training 400/553. train loss: 48.6005,	1.2573 s / batch. (data: 4.40e-01). ETA=13:34:23, max mem: 20.9 GB 
[11/22 17:24:28 visual_prompt]: 	Training 500/553. train loss: 15.0019,	1.5844 s / batch. (data: 7.31e-01). ETA=17:03:37, max mem: 20.9 GB 
[11/22 17:25:22 visual_prompt]: Epoch 30 / 100: avg data time: 1.95e-01, avg batch time: 1.0141, average train loss: 152.4408
[11/22 17:26:20 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3068, average loss: 126.2633
[11/22 17:26:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.01	
[11/22 17:26:20 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/22 17:28:07 visual_prompt]: 	Training 100/553. train loss: 115.0993,	0.8173 s / batch. (data: 1.05e-02). ETA=8:45:54, max mem: 20.9 GB 
[11/22 17:29:49 visual_prompt]: 	Training 200/553. train loss: 250.0156,	0.8327 s / batch. (data: 1.56e-02). ETA=8:54:28, max mem: 20.9 GB 
[11/22 17:31:28 visual_prompt]: 	Training 300/553. train loss: 112.1726,	0.8040 s / batch. (data: 2.94e-04). ETA=8:34:42, max mem: 20.9 GB 
[11/22 17:33:07 visual_prompt]: 	Training 400/553. train loss: 204.9033,	1.3434 s / batch. (data: 5.39e-01). ETA=14:17:44, max mem: 20.9 GB 
[11/22 17:34:48 visual_prompt]: 	Training 500/553. train loss: 57.7240,	0.8073 s / batch. (data: 3.06e-04). ETA=8:34:08, max mem: 20.9 GB 
[11/22 17:35:40 visual_prompt]: Epoch 31 / 100: avg data time: 1.93e-01, avg batch time: 1.0116, average train loss: 147.3782
[11/22 17:36:38 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3052, average loss: 66.5390
[11/22 17:36:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[11/22 17:36:38 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/22 17:38:24 visual_prompt]: 	Training 100/553. train loss: 63.9537,	0.8080 s / batch. (data: 3.06e-04). ETA=8:32:30, max mem: 20.9 GB 
[11/22 17:40:04 visual_prompt]: 	Training 200/553. train loss: 80.4546,	0.8135 s / batch. (data: 3.43e-04). ETA=8:34:37, max mem: 20.9 GB 
[11/22 17:41:48 visual_prompt]: 	Training 300/553. train loss: 192.4452,	0.8155 s / batch. (data: 3.33e-04). ETA=8:34:33, max mem: 20.9 GB 
[11/22 17:43:29 visual_prompt]: 	Training 400/553. train loss: 4.1298,	0.8153 s / batch. (data: 3.21e-04). ETA=8:33:01, max mem: 20.9 GB 
[11/22 17:45:07 visual_prompt]: 	Training 500/553. train loss: 370.3149,	0.8177 s / batch. (data: 3.04e-04). ETA=8:33:12, max mem: 20.9 GB 
[11/22 17:45:58 visual_prompt]: Epoch 32 / 100: avg data time: 1.95e-01, avg batch time: 1.0134, average train loss: 144.6342
[11/22 17:46:56 visual_prompt]: Inference (val):avg data time: 2.23e-04, avg batch time: 0.3082, average loss: 325.2564
[11/22 17:46:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.09	
[11/22 17:46:56 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/22 17:48:39 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8079 s / batch. (data: 3.16e-04). ETA=8:24:59, max mem: 20.9 GB 
[11/22 17:50:22 visual_prompt]: 	Training 200/553. train loss: 107.0186,	1.2680 s / batch. (data: 4.42e-01). ETA=13:10:26, max mem: 20.9 GB 
[11/22 17:52:02 visual_prompt]: 	Training 300/553. train loss: 104.5602,	0.8292 s / batch. (data: 1.94e-02). ETA=8:35:33, max mem: 20.9 GB 
[11/22 17:53:44 visual_prompt]: 	Training 400/553. train loss: 18.8342,	0.8021 s / batch. (data: 2.96e-04). ETA=8:17:19, max mem: 20.9 GB 
[11/22 17:55:23 visual_prompt]: 	Training 500/553. train loss: 92.1204,	0.9600 s / batch. (data: 1.20e-01). ETA=9:53:40, max mem: 20.9 GB 
[11/22 17:56:16 visual_prompt]: Epoch 33 / 100: avg data time: 1.95e-01, avg batch time: 1.0131, average train loss: 163.7719
[11/22 17:57:14 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3059, average loss: 33.5294
[11/22 17:57:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.21	
[11/22 17:57:14 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/22 17:59:01 visual_prompt]: 	Training 100/553. train loss: 100.4352,	0.8680 s / batch. (data: 3.65e-02). ETA=8:54:33, max mem: 20.9 GB 
[11/22 18:00:39 visual_prompt]: 	Training 200/553. train loss: 168.7859,	0.8119 s / batch. (data: 2.88e-04). ETA=8:18:40, max mem: 20.9 GB 
[11/22 18:02:20 visual_prompt]: 	Training 300/553. train loss: 56.7570,	0.8200 s / batch. (data: 3.24e-04). ETA=8:22:14, max mem: 20.9 GB 
[11/22 18:04:02 visual_prompt]: 	Training 400/553. train loss: 172.6722,	0.8040 s / batch. (data: 3.09e-04). ETA=8:11:07, max mem: 20.9 GB 
[11/22 18:05:44 visual_prompt]: 	Training 500/553. train loss: 72.8756,	1.5680 s / batch. (data: 7.45e-01). ETA=15:55:12, max mem: 20.9 GB 
[11/22 18:06:36 visual_prompt]: Epoch 34 / 100: avg data time: 1.97e-01, avg batch time: 1.0152, average train loss: 139.0871
[11/22 18:07:34 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3062, average loss: 100.6480
[11/22 18:07:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.71	
[11/22 18:07:34 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/22 18:09:20 visual_prompt]: 	Training 100/553. train loss: 222.7979,	0.8270 s / batch. (data: 1.09e-02). ETA=8:21:40, max mem: 20.9 GB 
[11/22 18:11:02 visual_prompt]: 	Training 200/553. train loss: 64.9297,	0.8200 s / batch. (data: 3.35e-04). ETA=8:16:03, max mem: 20.9 GB 
[11/22 18:12:41 visual_prompt]: 	Training 300/553. train loss: 56.2827,	0.8273 s / batch. (data: 3.07e-04). ETA=8:19:05, max mem: 20.9 GB 
[11/22 18:14:20 visual_prompt]: 	Training 400/553. train loss: 203.8705,	1.0000 s / batch. (data: 1.60e-01). ETA=10:01:37, max mem: 20.9 GB 
[11/22 18:16:00 visual_prompt]: 	Training 500/553. train loss: 13.9455,	1.2239 s / batch. (data: 3.99e-01). ETA=12:14:19, max mem: 20.9 GB 
[11/22 18:16:53 visual_prompt]: Epoch 35 / 100: avg data time: 1.93e-01, avg batch time: 1.0113, average train loss: 121.9376
[11/22 18:17:51 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3074, average loss: 78.2520
[11/22 18:17:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.94	
[11/22 18:17:51 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/22 18:19:35 visual_prompt]: 	Training 100/553. train loss: 188.7446,	0.8047 s / batch. (data: 3.42e-04). ETA=8:00:46, max mem: 20.9 GB 
[11/22 18:21:16 visual_prompt]: 	Training 200/553. train loss: 150.9630,	0.8319 s / batch. (data: 1.18e-02). ETA=8:15:35, max mem: 20.9 GB 
[11/22 18:22:59 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8420 s / batch. (data: 2.59e-02). ETA=8:20:12, max mem: 20.9 GB 
[11/22 18:24:39 visual_prompt]: 	Training 400/553. train loss: 8.6037,	0.8351 s / batch. (data: 7.06e-03). ETA=8:14:45, max mem: 20.9 GB 
[11/22 18:26:21 visual_prompt]: 	Training 500/553. train loss: 187.1887,	1.1275 s / batch. (data: 2.98e-01). ETA=11:06:04, max mem: 20.9 GB 
[11/22 18:27:10 visual_prompt]: Epoch 36 / 100: avg data time: 1.93e-01, avg batch time: 1.0114, average train loss: 147.0550
[11/22 18:28:08 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3074, average loss: 404.8266
[11/22 18:28:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.33	
[11/22 18:28:08 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/22 18:29:53 visual_prompt]: 	Training 100/553. train loss: 3.6021,	0.8280 s / batch. (data: 7.51e-03). ETA=8:07:02, max mem: 20.9 GB 
[11/22 18:31:34 visual_prompt]: 	Training 200/553. train loss: 141.7495,	0.8280 s / batch. (data: 3.14e-04). ETA=8:05:37, max mem: 20.9 GB 
[11/22 18:33:15 visual_prompt]: 	Training 300/553. train loss: 111.5356,	1.6662 s / batch. (data: 8.54e-01). ETA=16:14:32, max mem: 20.9 GB 
[11/22 18:34:58 visual_prompt]: 	Training 400/553. train loss: 268.0103,	2.0459 s / batch. (data: 1.22e+00). ETA=19:53:11, max mem: 20.9 GB 
[11/22 18:36:35 visual_prompt]: 	Training 500/553. train loss: 109.1986,	1.2664 s / batch. (data: 4.47e-01). ETA=12:16:25, max mem: 20.9 GB 
[11/22 18:37:29 visual_prompt]: Epoch 37 / 100: avg data time: 1.96e-01, avg batch time: 1.0147, average train loss: 144.4816
[11/22 18:38:27 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3044, average loss: 276.0451
[11/22 18:38:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.37	
[11/22 18:38:27 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[11/22 18:40:11 visual_prompt]: 	Training 100/553. train loss: 46.1960,	0.9733 s / batch. (data: 1.77e-01). ETA=9:23:29, max mem: 20.9 GB 
[11/22 18:41:52 visual_prompt]: 	Training 200/553. train loss: 26.8297,	0.8280 s / batch. (data: 7.93e-03). ETA=7:58:01, max mem: 20.9 GB 
[11/22 18:43:34 visual_prompt]: 	Training 300/553. train loss: 63.9591,	0.8120 s / batch. (data: 3.24e-04). ETA=7:47:27, max mem: 20.9 GB 
[11/22 18:45:13 visual_prompt]: 	Training 400/553. train loss: 0.0000,	1.1147 s / batch. (data: 2.80e-01). ETA=10:39:48, max mem: 20.9 GB 
[11/22 18:46:57 visual_prompt]: 	Training 500/553. train loss: 132.2172,	0.8142 s / batch. (data: 4.28e-04). ETA=7:45:58, max mem: 20.9 GB 
[11/22 18:47:48 visual_prompt]: Epoch 38 / 100: avg data time: 1.96e-01, avg batch time: 1.0140, average train loss: 141.5284
[11/22 18:48:46 visual_prompt]: Inference (val):avg data time: 1.47e-04, avg batch time: 0.3060, average loss: 468.5666
[11/22 18:48:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.83	
[11/22 18:48:46 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[11/22 18:50:29 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.7943 s / batch. (data: 2.93e-04). ETA=7:32:34, max mem: 20.9 GB 
[11/22 18:52:14 visual_prompt]: 	Training 200/553. train loss: 318.9404,	0.8252 s / batch. (data: 2.99e-04). ETA=7:48:49, max mem: 20.9 GB 
[11/22 18:53:57 visual_prompt]: 	Training 300/553. train loss: 309.1207,	0.8263 s / batch. (data: 7.95e-03). ETA=7:48:03, max mem: 20.9 GB 
[11/22 18:55:35 visual_prompt]: 	Training 400/553. train loss: 135.6061,	0.9549 s / batch. (data: 1.33e-01). ETA=8:59:16, max mem: 20.9 GB 
[11/22 18:57:17 visual_prompt]: 	Training 500/553. train loss: 73.8458,	1.9400 s / batch. (data: 1.14e+00). ETA=18:12:24, max mem: 20.9 GB 
[11/22 18:58:07 visual_prompt]: Epoch 39 / 100: avg data time: 1.95e-01, avg batch time: 1.0143, average train loss: 140.0020
[11/22 18:59:05 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3101, average loss: 137.8621
[11/22 18:59:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.12	
[11/22 18:59:05 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[11/22 19:00:51 visual_prompt]: 	Training 100/553. train loss: 257.4938,	0.8272 s / batch. (data: 3.37e-04). ETA=7:43:42, max mem: 20.9 GB 
[11/22 19:02:31 visual_prompt]: 	Training 200/553. train loss: 12.0638,	0.8399 s / batch. (data: 7.93e-03). ETA=7:49:25, max mem: 20.9 GB 
[11/22 19:04:13 visual_prompt]: 	Training 300/553. train loss: 318.8055,	0.8255 s / batch. (data: 8.11e-04). ETA=7:39:59, max mem: 20.9 GB 
[11/22 19:05:55 visual_prompt]: 	Training 400/553. train loss: 26.2209,	0.8049 s / batch. (data: 2.94e-04). ETA=7:27:08, max mem: 20.9 GB 
[11/22 19:07:35 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8244 s / batch. (data: 8.48e-04). ETA=7:36:36, max mem: 20.9 GB 
[11/22 19:08:29 visual_prompt]: Epoch 40 / 100: avg data time: 2.00e-01, avg batch time: 1.0195, average train loss: 100.7834
[11/22 19:09:27 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3065, average loss: 243.3437
[11/22 19:09:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.03	
[11/22 19:09:27 visual_prompt]: Stopping early.
[11/22 19:09:27 visual_prompt]: Rank of current process: 0. World size: 1
[11/22 19:09:27 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 19:09:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/22 19:09:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/22 19:09:27 visual_prompt]: Training with config:
[11/22 19:09:27 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/22 19:09:27 visual_prompt]: Loading training data...
[11/22 19:09:27 visual_prompt]: Constructing mammo-cbis dataset train...
[11/22 19:09:27 visual_prompt]: Loading validation data...
[11/22 19:09:27 visual_prompt]: Constructing mammo-cbis dataset val...
[11/22 19:09:27 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/22 19:09:30 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/22 19:09:30 visual_prompt]: tuned percent:0.525
[11/22 19:09:30 visual_prompt]: Device used for model: 0
[11/22 19:09:30 visual_prompt]: Setting up Evaluator...
[11/22 19:09:30 visual_prompt]: Setting up Trainer...
[11/22 19:09:30 visual_prompt]: 	Setting up the optimizer...
[11/22 19:09:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/22 19:11:14 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8320 s / batch. (data: 3.04e-04). ETA=12:45:25, max mem: 20.9 GB 
[11/22 19:12:54 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8080 s / batch. (data: 3.00e-04). ETA=12:22:00, max mem: 20.9 GB 
[11/22 19:14:37 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.2440 s / batch. (data: 4.13e-01). ETA=19:00:19, max mem: 20.9 GB 
[11/22 19:16:17 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8384 s / batch. (data: 3.05e-04). ETA=12:47:06, max mem: 20.9 GB 
[11/22 19:18:00 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8200 s / batch. (data: 2.97e-04). ETA=12:28:53, max mem: 20.9 GB 
[11/22 19:18:53 visual_prompt]: Epoch 1 / 100: avg data time: 1.96e-01, avg batch time: 1.0182, average train loss: 1.5403
[11/22 19:19:51 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3067, average loss: 1.5201
[11/22 19:19:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/22 19:19:51 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/22 19:21:36 visual_prompt]: 	Training 100/553. train loss: 25.0325,	0.8280 s / batch. (data: 3.21e-04). ETA=12:34:07, max mem: 20.9 GB 
[11/22 19:23:17 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8098 s / batch. (data: 3.11e-04). ETA=12:16:14, max mem: 20.9 GB 
[11/22 19:25:00 visual_prompt]: 	Training 300/553. train loss: 7.4490,	1.1475 s / batch. (data: 3.43e-01). ETA=17:21:19, max mem: 20.9 GB 
[11/22 19:26:40 visual_prompt]: 	Training 400/553. train loss: 29.1996,	0.8092 s / batch. (data: 2.91e-04). ETA=12:12:58, max mem: 20.9 GB 
[11/22 19:28:23 visual_prompt]: 	Training 500/553. train loss: 80.6536,	0.8200 s / batch. (data: 3.10e-04). ETA=12:21:20, max mem: 20.9 GB 
[11/22 19:29:14 visual_prompt]: Epoch 2 / 100: avg data time: 1.96e-01, avg batch time: 1.0176, average train loss: 26.5332
[11/22 19:30:12 visual_prompt]: Inference (val):avg data time: 9.74e-05, avg batch time: 0.3077, average loss: 67.9892
[11/22 19:30:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.61	
[11/22 19:30:12 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/22 19:31:55 visual_prompt]: 	Training 100/553. train loss: 60.1347,	0.8081 s / batch. (data: 2.62e-04). ETA=12:08:32, max mem: 20.9 GB 
[11/22 19:33:38 visual_prompt]: 	Training 200/553. train loss: 29.4592,	2.4093 s / batch. (data: 1.58e+00). ETA=1 day, 12:08:10, max mem: 20.9 GB 
[11/22 19:35:18 visual_prompt]: 	Training 300/553. train loss: 40.9004,	0.8193 s / batch. (data: 2.69e-04). ETA=12:15:56, max mem: 20.9 GB 
[11/22 19:37:00 visual_prompt]: 	Training 400/553. train loss: 14.7090,	0.8108 s / batch. (data: 2.92e-04). ETA=12:06:57, max mem: 20.9 GB 
[11/22 19:38:42 visual_prompt]: 	Training 500/553. train loss: 23.5847,	1.2956 s / batch. (data: 4.67e-01). ETA=19:19:24, max mem: 20.9 GB 
[11/22 19:39:34 visual_prompt]: Epoch 3 / 100: avg data time: 1.96e-01, avg batch time: 1.0158, average train loss: 43.5661
[11/22 19:40:32 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3059, average loss: 40.6659
[11/22 19:40:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.00	
[11/22 19:40:32 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/22 19:42:20 visual_prompt]: 	Training 100/553. train loss: 39.0314,	0.8360 s / batch. (data: 5.43e-03). ETA=12:25:59, max mem: 20.9 GB 
[11/22 19:44:01 visual_prompt]: 	Training 200/553. train loss: 61.0410,	0.8365 s / batch. (data: 7.93e-03). ETA=12:25:03, max mem: 20.9 GB 
[11/22 19:45:43 visual_prompt]: 	Training 300/553. train loss: 14.5468,	1.6924 s / batch. (data: 8.86e-01). ETA=1 day, 1:04:32, max mem: 20.9 GB 
[11/22 19:47:19 visual_prompt]: 	Training 400/553. train loss: 25.0609,	0.8312 s / batch. (data: 5.45e-03). ETA=12:17:35, max mem: 20.9 GB 
[11/22 19:49:02 visual_prompt]: 	Training 500/553. train loss: 145.8390,	3.1777 s / batch. (data: 2.37e+00). ETA=1 day, 22:54:26, max mem: 20.9 GB 
[11/22 19:49:57 visual_prompt]: Epoch 4 / 100: avg data time: 2.00e-01, avg batch time: 1.0200, average train loss: 41.9266
[11/22 19:50:55 visual_prompt]: Inference (val):avg data time: 2.16e-04, avg batch time: 0.3058, average loss: 32.3341
[11/22 19:50:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.60	
[11/22 19:50:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/22 19:52:39 visual_prompt]: 	Training 100/553. train loss: 386.8663,	0.8280 s / batch. (data: 5.43e-03). ETA=12:11:14, max mem: 20.9 GB 
[11/22 19:54:20 visual_prompt]: 	Training 200/553. train loss: 53.7453,	1.1360 s / batch. (data: 3.12e-01). ETA=16:41:20, max mem: 20.9 GB 
[11/22 19:56:02 visual_prompt]: 	Training 300/553. train loss: 214.9673,	0.8240 s / batch. (data: 3.12e-04). ETA=12:04:58, max mem: 20.9 GB 
[11/22 19:57:42 visual_prompt]: 	Training 400/553. train loss: 27.8766,	0.8057 s / batch. (data: 3.83e-04). ETA=11:47:30, max mem: 20.9 GB 
[11/22 19:59:23 visual_prompt]: 	Training 500/553. train loss: 19.4011,	0.8169 s / batch. (data: 2.99e-04). ETA=11:55:57, max mem: 20.9 GB 
[11/22 20:00:17 visual_prompt]: Epoch 5 / 100: avg data time: 1.98e-01, avg batch time: 1.0159, average train loss: 85.3709
[11/22 20:01:15 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3076, average loss: 19.8969
[11/22 20:01:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.41	
[11/22 20:01:15 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/22 20:03:02 visual_prompt]: 	Training 100/553. train loss: 88.2537,	0.8274 s / batch. (data: 8.38e-04). ETA=12:03:06, max mem: 20.9 GB 
[11/22 20:04:42 visual_prompt]: 	Training 200/553. train loss: 21.5776,	0.8365 s / batch. (data: 3.55e-04). ETA=12:09:38, max mem: 20.9 GB 
[11/22 20:06:22 visual_prompt]: 	Training 300/553. train loss: 217.9171,	0.8292 s / batch. (data: 3.63e-04). ETA=12:01:55, max mem: 20.9 GB 
[11/22 20:08:07 visual_prompt]: 	Training 400/553. train loss: 6.9724,	0.8440 s / batch. (data: 3.26e-04). ETA=12:13:23, max mem: 20.9 GB 
[11/22 20:09:46 visual_prompt]: 	Training 500/553. train loss: 1.2620,	0.8335 s / batch. (data: 2.07e-02). ETA=12:02:52, max mem: 20.9 GB 
[11/22 20:10:38 visual_prompt]: Epoch 6 / 100: avg data time: 2.00e-01, avg batch time: 1.0178, average train loss: 87.7367
[11/22 20:11:36 visual_prompt]: Inference (val):avg data time: 1.63e-04, avg batch time: 0.3076, average loss: 26.4101
[11/22 20:11:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.30	
[11/22 20:11:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/22 20:13:19 visual_prompt]: 	Training 100/553. train loss: 50.2073,	0.8360 s / batch. (data: 3.02e-04). ETA=12:02:54, max mem: 20.9 GB 
[11/22 20:15:01 visual_prompt]: 	Training 200/553. train loss: 18.5600,	0.8095 s / batch. (data: 5.41e-03). ETA=11:38:39, max mem: 20.9 GB 
[11/22 20:16:44 visual_prompt]: 	Training 300/553. train loss: 4.7670,	2.0360 s / batch. (data: 1.18e+00). ETA=1 day, 5:13:45, max mem: 20.9 GB 
[11/22 20:18:26 visual_prompt]: 	Training 400/553. train loss: 29.0680,	1.9117 s / batch. (data: 1.10e+00). ETA=1 day, 3:23:28, max mem: 20.9 GB 
[11/22 20:20:06 visual_prompt]: 	Training 500/553. train loss: 176.5945,	0.8223 s / batch. (data: 3.30e-04). ETA=11:45:33, max mem: 20.9 GB 
[11/22 20:20:57 visual_prompt]: Epoch 7 / 100: avg data time: 1.96e-01, avg batch time: 1.0150, average train loss: 99.9768
[11/22 20:21:55 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3067, average loss: 111.2492
[11/22 20:21:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.62	
[11/22 20:21:55 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/22 20:23:38 visual_prompt]: 	Training 100/553. train loss: 282.0644,	0.8170 s / batch. (data: 4.15e-04). ETA=11:38:54, max mem: 20.9 GB 
[11/22 20:25:21 visual_prompt]: 	Training 200/553. train loss: 32.4692,	0.8661 s / batch. (data: 2.60e-02). ETA=12:19:27, max mem: 20.9 GB 
[11/22 20:27:03 visual_prompt]: 	Training 300/553. train loss: 50.4068,	0.8001 s / batch. (data: 3.11e-04). ETA=11:21:48, max mem: 20.9 GB 
[11/22 20:28:44 visual_prompt]: 	Training 400/553. train loss: 234.5956,	1.1240 s / batch. (data: 3.21e-01). ETA=15:55:54, max mem: 20.9 GB 
[11/22 20:30:26 visual_prompt]: 	Training 500/553. train loss: 329.2100,	1.5640 s / batch. (data: 7.38e-01). ETA=22:07:31, max mem: 20.9 GB 
[11/22 20:31:19 visual_prompt]: Epoch 8 / 100: avg data time: 2.02e-01, avg batch time: 1.0189, average train loss: 113.6568
[11/22 20:32:17 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3065, average loss: 4.6736
[11/22 20:32:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.02	
[11/22 20:32:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/22 20:34:02 visual_prompt]: 	Training 100/553. train loss: 38.1540,	0.8320 s / batch. (data: 2.95e-04). ETA=11:44:05, max mem: 20.9 GB 
[11/22 20:35:42 visual_prompt]: 	Training 200/553. train loss: 2.7190,	0.8400 s / batch. (data: 5.42e-03). ETA=11:49:27, max mem: 20.9 GB 
[11/22 20:37:23 visual_prompt]: 	Training 300/553. train loss: 53.2733,	1.7921 s / batch. (data: 9.83e-01). ETA=1 day, 1:10:37, max mem: 20.9 GB 
[11/22 20:39:05 visual_prompt]: 	Training 400/553. train loss: 240.5974,	0.8029 s / batch. (data: 3.03e-04). ETA=11:15:29, max mem: 20.9 GB 
[11/22 20:40:47 visual_prompt]: 	Training 500/553. train loss: 59.7817,	1.0980 s / batch. (data: 2.85e-01). ETA=15:21:51, max mem: 20.9 GB 
[11/22 20:41:39 visual_prompt]: Epoch 9 / 100: avg data time: 1.98e-01, avg batch time: 1.0163, average train loss: 133.1418
[11/22 20:42:37 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3062, average loss: 184.4700
[11/22 20:42:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.80	
[11/22 20:42:37 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/22 20:44:25 visual_prompt]: 	Training 100/553. train loss: 277.8878,	0.8263 s / batch. (data: 7.85e-04). ETA=11:31:39, max mem: 20.9 GB 
[11/22 20:46:04 visual_prompt]: 	Training 200/553. train loss: 8.4880,	0.8213 s / batch. (data: 3.88e-04). ETA=11:26:04, max mem: 20.9 GB 
[11/22 20:47:44 visual_prompt]: 	Training 300/553. train loss: 49.5425,	1.3902 s / batch. (data: 5.63e-01). ETA=19:19:02, max mem: 20.9 GB 
[11/22 20:49:23 visual_prompt]: 	Training 400/553. train loss: 7.6892,	0.8075 s / batch. (data: 3.24e-04). ETA=11:11:52, max mem: 20.9 GB 
[11/22 20:51:06 visual_prompt]: 	Training 500/553. train loss: 26.2307,	0.8140 s / batch. (data: 1.20e-02). ETA=11:15:55, max mem: 20.9 GB 
[11/22 20:51:58 visual_prompt]: Epoch 10 / 100: avg data time: 1.96e-01, avg batch time: 1.0147, average train loss: 164.3878
[11/22 20:52:56 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3077, average loss: 31.7203
[11/22 20:52:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.98	
[11/22 20:52:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/22 20:54:43 visual_prompt]: 	Training 100/553. train loss: 9.9833,	0.8170 s / batch. (data: 8.97e-03). ETA=11:16:22, max mem: 20.9 GB 
[11/22 20:56:26 visual_prompt]: 	Training 200/553. train loss: 469.3429,	0.8400 s / batch. (data: 1.20e-02). ETA=11:33:59, max mem: 20.9 GB 
[11/22 20:58:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1760 s / batch. (data: 1.36e+00). ETA=1 day, 5:54:08, max mem: 20.9 GB 
[11/22 20:59:47 visual_prompt]: 	Training 400/553. train loss: 100.8531,	0.8159 s / batch. (data: 3.31e-04). ETA=11:11:19, max mem: 20.9 GB 
[11/22 21:01:27 visual_prompt]: 	Training 500/553. train loss: 263.9169,	0.8041 s / batch. (data: 4.26e-03). ETA=11:00:16, max mem: 20.9 GB 
[11/22 21:02:19 visual_prompt]: Epoch 11 / 100: avg data time: 2.00e-01, avg batch time: 1.0178, average train loss: 146.0352
[11/22 21:03:17 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.3077, average loss: 132.6761
[11/22 21:03:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.21	
[11/22 21:03:17 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/22 21:05:04 visual_prompt]: 	Training 100/553. train loss: 60.8919,	0.8158 s / batch. (data: 7.98e-03). ETA=11:07:47, max mem: 20.9 GB 
[11/22 21:06:46 visual_prompt]: 	Training 200/553. train loss: 56.2458,	0.8564 s / batch. (data: 2.87e-02). ETA=11:39:36, max mem: 20.9 GB 
[11/22 21:08:26 visual_prompt]: 	Training 300/553. train loss: 37.1902,	0.8400 s / batch. (data: 2.41e-02). ETA=11:24:50, max mem: 20.9 GB 
[11/22 21:10:07 visual_prompt]: 	Training 400/553. train loss: 9.3180,	0.8453 s / batch. (data: 5.39e-03). ETA=11:27:45, max mem: 20.9 GB 
[11/22 21:11:48 visual_prompt]: 	Training 500/553. train loss: 13.2477,	0.8508 s / batch. (data: 1.48e-02). ETA=11:30:48, max mem: 20.9 GB 
[11/22 21:12:39 visual_prompt]: Epoch 12 / 100: avg data time: 1.98e-01, avg batch time: 1.0161, average train loss: 172.4881
[11/22 21:13:37 visual_prompt]: Inference (val):avg data time: 5.55e-04, avg batch time: 0.3076, average loss: 32.2117
[11/22 21:13:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.13	
[11/22 21:13:37 visual_prompt]: Best epoch 12: best metric: -32.212
[11/22 21:13:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/22 21:15:24 visual_prompt]: 	Training 100/553. train loss: 126.3914,	0.8157 s / batch. (data: 7.98e-03). ETA=11:00:12, max mem: 20.9 GB 
[11/22 21:17:02 visual_prompt]: 	Training 200/553. train loss: 6.3517,	0.8405 s / batch. (data: 2.97e-04). ETA=11:18:53, max mem: 20.9 GB 
[11/22 21:18:44 visual_prompt]: 	Training 300/553. train loss: 238.2179,	1.8112 s / batch. (data: 1.00e+00). ETA=1 day, 0:19:54, max mem: 20.9 GB 
[11/22 21:20:24 visual_prompt]: 	Training 400/553. train loss: 48.1541,	0.8200 s / batch. (data: 7.94e-03). ETA=10:59:34, max mem: 20.9 GB 
[11/22 21:22:06 visual_prompt]: 	Training 500/553. train loss: 359.6395,	0.8280 s / batch. (data: 2.93e-04). ETA=11:04:40, max mem: 20.9 GB 
[11/22 21:22:59 visual_prompt]: Epoch 13 / 100: avg data time: 1.98e-01, avg batch time: 1.0158, average train loss: 158.3108
[11/22 21:23:57 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3066, average loss: 64.0487
[11/22 21:23:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.74	
[11/22 21:23:57 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/22 21:25:43 visual_prompt]: 	Training 100/553. train loss: 354.7159,	0.8280 s / batch. (data: 3.17e-04). ETA=11:02:31, max mem: 20.9 GB 
[11/22 21:27:24 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.2615 s / batch. (data: 4.50e-01). ETA=16:47:19, max mem: 20.9 GB 
[11/22 21:29:05 visual_prompt]: 	Training 300/553. train loss: 281.6381,	0.8120 s / batch. (data: 3.48e-04). ETA=10:47:00, max mem: 20.9 GB 
[11/22 21:30:45 visual_prompt]: 	Training 400/553. train loss: 143.7888,	0.7967 s / batch. (data: 3.06e-04). ETA=10:33:32, max mem: 20.9 GB 
[11/22 21:32:26 visual_prompt]: 	Training 500/553. train loss: 306.5684,	0.8039 s / batch. (data: 3.14e-04). ETA=10:37:53, max mem: 20.9 GB 
[11/22 21:33:18 visual_prompt]: Epoch 14 / 100: avg data time: 1.97e-01, avg batch time: 1.0142, average train loss: 151.1336
[11/22 21:34:16 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3074, average loss: 183.7384
[11/22 21:34:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.66	
[11/22 21:34:16 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/22 21:36:01 visual_prompt]: 	Training 100/553. train loss: 105.0922,	0.8146 s / batch. (data: 3.20e-04). ETA=10:44:20, max mem: 20.9 GB 
[11/22 21:37:40 visual_prompt]: 	Training 200/553. train loss: 1187.5264,	0.8263 s / batch. (data: 1.05e-02). ETA=10:52:12, max mem: 20.9 GB 
[11/22 21:39:23 visual_prompt]: 	Training 300/553. train loss: 32.1880,	0.8200 s / batch. (data: 3.09e-04). ETA=10:45:51, max mem: 20.9 GB 
[11/22 21:41:01 visual_prompt]: 	Training 400/553. train loss: 28.2467,	0.9705 s / batch. (data: 1.73e-01). ETA=12:42:48, max mem: 20.9 GB 
[11/22 21:42:43 visual_prompt]: 	Training 500/553. train loss: 68.7989,	0.8149 s / batch. (data: 2.88e-04). ETA=10:39:08, max mem: 20.9 GB 
[11/22 21:43:37 visual_prompt]: Epoch 15 / 100: avg data time: 1.96e-01, avg batch time: 1.0138, average train loss: 206.1229
[11/22 21:44:34 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3081, average loss: 118.2958
[11/22 21:44:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.74	
[11/22 21:44:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/22 21:46:18 visual_prompt]: 	Training 100/553. train loss: 202.4007,	0.8381 s / batch. (data: 1.21e-02). ETA=10:55:13, max mem: 20.9 GB 
[11/22 21:48:00 visual_prompt]: 	Training 200/553. train loss: 48.7037,	0.8274 s / batch. (data: 2.88e-04). ETA=10:45:28, max mem: 20.9 GB 
[11/22 21:49:41 visual_prompt]: 	Training 300/553. train loss: 37.0103,	0.8320 s / batch. (data: 5.44e-03). ETA=10:47:38, max mem: 20.9 GB 
[11/22 21:51:22 visual_prompt]: 	Training 400/553. train loss: 27.4519,	0.8176 s / batch. (data: 8.06e-04). ETA=10:35:06, max mem: 20.9 GB 
[11/22 21:53:03 visual_prompt]: 	Training 500/553. train loss: 121.7329,	2.0600 s / batch. (data: 1.24e+00). ETA=1 day, 2:36:38, max mem: 20.9 GB 
[11/22 21:53:57 visual_prompt]: Epoch 16 / 100: avg data time: 2.01e-01, avg batch time: 1.0173, average train loss: 164.7299
[11/22 21:54:55 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3061, average loss: 117.7863
[11/22 21:54:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.32	
[11/22 21:54:55 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/22 21:56:40 visual_prompt]: 	Training 100/553. train loss: 58.3650,	0.8387 s / batch. (data: 3.10e-04). ETA=10:47:53, max mem: 20.9 GB 
[11/22 21:58:22 visual_prompt]: 	Training 200/553. train loss: 227.1207,	0.8125 s / batch. (data: 5.44e-03). ETA=10:26:18, max mem: 20.9 GB 
[11/22 22:00:02 visual_prompt]: 	Training 300/553. train loss: 46.0636,	0.8352 s / batch. (data: 1.12e-02). ETA=10:42:26, max mem: 20.9 GB 
[11/22 22:01:42 visual_prompt]: 	Training 400/553. train loss: 114.2639,	1.2680 s / batch. (data: 4.43e-01). ETA=16:13:15, max mem: 20.9 GB 
[11/22 22:03:22 visual_prompt]: 	Training 500/553. train loss: 87.6181,	1.6496 s / batch. (data: 8.27e-01). ETA=21:03:20, max mem: 20.9 GB 
[11/22 22:04:16 visual_prompt]: Epoch 17 / 100: avg data time: 1.97e-01, avg batch time: 1.0141, average train loss: 168.7049
[11/22 22:05:14 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3067, average loss: 22.6526
[11/22 22:05:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.86	
[11/22 22:05:14 visual_prompt]: Best epoch 17: best metric: -22.653
[11/22 22:05:14 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/22 22:07:00 visual_prompt]: 	Training 100/553. train loss: 131.4120,	0.8515 s / batch. (data: 3.23e-04). ETA=10:49:57, max mem: 20.9 GB 
[11/22 22:08:43 visual_prompt]: 	Training 200/553. train loss: 82.1683,	0.8280 s / batch. (data: 3.16e-04). ETA=10:30:40, max mem: 20.9 GB 
[11/22 22:10:24 visual_prompt]: 	Training 300/553. train loss: 208.3451,	0.8094 s / batch. (data: 5.41e-03). ETA=10:15:07, max mem: 20.9 GB 
[11/22 22:12:05 visual_prompt]: 	Training 400/553. train loss: 118.2526,	0.8272 s / batch. (data: 5.42e-03). ETA=10:27:15, max mem: 20.9 GB 
[11/22 22:13:45 visual_prompt]: 	Training 500/553. train loss: 105.4762,	0.8200 s / batch. (data: 2.91e-04). ETA=10:20:27, max mem: 20.9 GB 
[11/22 22:14:36 visual_prompt]: Epoch 18 / 100: avg data time: 1.98e-01, avg batch time: 1.0157, average train loss: 173.0700
[11/22 22:15:34 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3075, average loss: 234.8130
[11/22 22:15:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.62	
[11/22 22:15:34 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/22 22:17:19 visual_prompt]: 	Training 100/553. train loss: 102.9204,	0.8703 s / batch. (data: 5.87e-02). ETA=10:56:17, max mem: 20.9 GB 
[11/22 22:19:00 visual_prompt]: 	Training 200/553. train loss: 134.7831,	0.8160 s / batch. (data: 2.68e-04). ETA=10:14:00, max mem: 20.9 GB 
[11/22 22:20:42 visual_prompt]: 	Training 300/553. train loss: 185.0444,	0.8080 s / batch. (data: 2.89e-04). ETA=10:06:36, max mem: 20.9 GB 
[11/22 22:22:23 visual_prompt]: 	Training 400/553. train loss: 27.8588,	0.8210 s / batch. (data: 3.53e-04). ETA=10:14:58, max mem: 20.9 GB 
[11/22 22:24:00 visual_prompt]: 	Training 500/553. train loss: 32.7093,	0.8392 s / batch. (data: 1.92e-02). ETA=10:27:15, max mem: 20.9 GB 
[11/22 22:24:52 visual_prompt]: Epoch 19 / 100: avg data time: 1.92e-01, avg batch time: 1.0092, average train loss: 148.0561
[11/22 22:25:50 visual_prompt]: Inference (val):avg data time: 8.87e-05, avg batch time: 0.3056, average loss: 392.9309
[11/22 22:25:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.46	
[11/22 22:25:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/22 22:27:34 visual_prompt]: 	Training 100/553. train loss: 4.1941,	0.8492 s / batch. (data: 2.13e-02). ETA=10:32:34, max mem: 20.9 GB 
[11/22 22:29:16 visual_prompt]: 	Training 200/553. train loss: 0.8545,	0.8293 s / batch. (data: 1.05e-02). ETA=10:16:22, max mem: 20.9 GB 
[11/22 22:30:57 visual_prompt]: 	Training 300/553. train loss: 68.5057,	0.8400 s / batch. (data: 7.72e-04). ETA=10:22:52, max mem: 20.9 GB 
[11/22 22:32:38 visual_prompt]: 	Training 400/553. train loss: 805.9830,	0.8309 s / batch. (data: 1.09e-02). ETA=10:14:48, max mem: 20.9 GB 
[11/22 22:34:18 visual_prompt]: 	Training 500/553. train loss: 203.7889,	0.8225 s / batch. (data: 1.05e-02). ETA=10:07:10, max mem: 20.9 GB 
[11/22 22:35:12 visual_prompt]: Epoch 20 / 100: avg data time: 1.97e-01, avg batch time: 1.0151, average train loss: 131.3989
[11/22 22:36:10 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3076, average loss: 165.6595
[11/22 22:36:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.91	
[11/22 22:36:10 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/22 22:37:58 visual_prompt]: 	Training 100/553. train loss: 19.3996,	0.8412 s / batch. (data: 2.11e-02). ETA=10:18:50, max mem: 20.9 GB 
[11/22 22:39:37 visual_prompt]: 	Training 200/553. train loss: 379.2604,	0.8054 s / batch. (data: 3.02e-04). ETA=9:51:10, max mem: 20.9 GB 
[11/22 22:41:19 visual_prompt]: 	Training 300/553. train loss: 641.3419,	1.1684 s / batch. (data: 3.44e-01). ETA=14:15:41, max mem: 20.9 GB 
[11/22 22:42:58 visual_prompt]: 	Training 400/553. train loss: 61.0983,	0.8280 s / batch. (data: 2.95e-04). ETA=10:04:58, max mem: 20.9 GB 
[11/22 22:44:41 visual_prompt]: 	Training 500/553. train loss: 171.5970,	0.8160 s / batch. (data: 2.98e-04). ETA=9:54:52, max mem: 20.9 GB 
[11/22 22:45:33 visual_prompt]: Epoch 21 / 100: avg data time: 2.01e-01, avg batch time: 1.0185, average train loss: 151.6709
[11/22 22:46:31 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3074, average loss: 111.4839
[11/22 22:46:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.89	
[11/22 22:46:31 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/22 22:48:15 visual_prompt]: 	Training 100/553. train loss: 122.2994,	0.8087 s / batch. (data: 3.14e-04). ETA=9:47:28, max mem: 20.9 GB 
[11/22 22:49:56 visual_prompt]: 	Training 200/553. train loss: 80.7615,	0.8161 s / batch. (data: 3.24e-04). ETA=9:51:28, max mem: 20.9 GB 
[11/22 22:51:35 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8006 s / batch. (data: 3.26e-04). ETA=9:38:56, max mem: 20.9 GB 
[11/22 22:53:17 visual_prompt]: 	Training 400/553. train loss: 179.3458,	0.8120 s / batch. (data: 5.41e-03). ETA=9:45:47, max mem: 20.9 GB 
[11/22 22:54:58 visual_prompt]: 	Training 500/553. train loss: 114.1174,	0.7983 s / batch. (data: 3.18e-04). ETA=9:34:34, max mem: 20.9 GB 
[11/22 22:55:52 visual_prompt]: Epoch 22 / 100: avg data time: 1.96e-01, avg batch time: 1.0138, average train loss: 152.8411
[11/22 22:56:50 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3063, average loss: 75.6044
[11/22 22:56:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.51	
[11/22 22:56:50 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/22 22:58:36 visual_prompt]: 	Training 100/553. train loss: 238.0629,	0.8005 s / batch. (data: 3.38e-04). ETA=9:34:08, max mem: 20.9 GB 
[11/22 23:00:18 visual_prompt]: 	Training 200/553. train loss: 108.5072,	0.8080 s / batch. (data: 3.09e-04). ETA=9:38:10, max mem: 20.9 GB 
[11/22 23:02:01 visual_prompt]: 	Training 300/553. train loss: 15.3152,	0.8440 s / batch. (data: 7.78e-04). ETA=10:02:31, max mem: 20.9 GB 
[11/22 23:03:40 visual_prompt]: 	Training 400/553. train loss: 370.6310,	0.8400 s / batch. (data: 7.98e-04). ETA=9:58:16, max mem: 20.9 GB 
[11/22 23:05:19 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8239 s / batch. (data: 2.99e-04). ETA=9:45:28, max mem: 20.9 GB 
[11/22 23:06:11 visual_prompt]: Epoch 23 / 100: avg data time: 1.97e-01, avg batch time: 1.0149, average train loss: 164.8058
[11/22 23:07:09 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3053, average loss: 19.6481
[11/22 23:07:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.39	
[11/22 23:07:09 visual_prompt]: Best epoch 23: best metric: -19.648
[11/22 23:07:09 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/22 23:08:52 visual_prompt]: 	Training 100/553. train loss: 220.7708,	0.8359 s / batch. (data: 1.19e-02). ETA=9:51:51, max mem: 20.9 GB 
[11/22 23:10:32 visual_prompt]: 	Training 200/553. train loss: 57.7030,	0.8493 s / batch. (data: 3.08e-04). ETA=9:59:52, max mem: 20.9 GB 
[11/22 23:12:15 visual_prompt]: 	Training 300/553. train loss: 165.2409,	1.2262 s / batch. (data: 4.16e-01). ETA=14:24:05, max mem: 20.9 GB 
[11/22 23:13:55 visual_prompt]: 	Training 400/553. train loss: 54.8669,	0.8066 s / batch. (data: 3.10e-04). ETA=9:27:01, max mem: 20.9 GB 
[11/22 23:15:38 visual_prompt]: 	Training 500/553. train loss: 233.0795,	0.8268 s / batch. (data: 1.05e-02). ETA=9:39:51, max mem: 20.9 GB 
[11/22 23:16:31 visual_prompt]: Epoch 24 / 100: avg data time: 1.97e-01, avg batch time: 1.0163, average train loss: 146.3013
[11/22 23:17:29 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3070, average loss: 171.0182
[11/22 23:17:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.56	
[11/22 23:17:29 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/22 23:19:18 visual_prompt]: 	Training 100/553. train loss: 15.2874,	0.8480 s / batch. (data: 3.06e-04). ETA=9:52:35, max mem: 20.9 GB 
[11/22 23:20:56 visual_prompt]: 	Training 200/553. train loss: 186.4530,	0.8152 s / batch. (data: 3.59e-04). ETA=9:28:17, max mem: 20.9 GB 
[11/22 23:22:36 visual_prompt]: 	Training 300/553. train loss: 25.8697,	0.8360 s / batch. (data: 3.18e-04). ETA=9:41:23, max mem: 20.9 GB 
[11/22 23:24:17 visual_prompt]: 	Training 400/553. train loss: 33.5844,	1.3971 s / batch. (data: 5.95e-01). ETA=16:09:20, max mem: 20.9 GB 
[11/22 23:26:00 visual_prompt]: 	Training 500/553. train loss: 120.7112,	1.6360 s / batch. (data: 8.11e-01). ETA=18:52:20, max mem: 20.9 GB 
[11/22 23:26:52 visual_prompt]: Epoch 25 / 100: avg data time: 1.99e-01, avg batch time: 1.0170, average train loss: 152.1037
[11/22 23:27:50 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3067, average loss: 209.1437
[11/22 23:27:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.07	
[11/22 23:27:50 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/22 23:29:35 visual_prompt]: 	Training 100/553. train loss: 83.1610,	0.8080 s / batch. (data: 3.18e-04). ETA=9:17:10, max mem: 20.9 GB 
[11/22 23:31:17 visual_prompt]: 	Training 200/553. train loss: 598.5208,	1.7505 s / batch. (data: 9.36e-01). ETA=20:04:11, max mem: 20.9 GB 
[11/22 23:32:59 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8320 s / batch. (data: 8.21e-04). ETA=9:30:56, max mem: 20.9 GB 
[11/22 23:34:38 visual_prompt]: 	Training 400/553. train loss: 457.4002,	0.8381 s / batch. (data: 2.22e-02). ETA=9:33:46, max mem: 20.9 GB 
[11/22 23:36:17 visual_prompt]: 	Training 500/553. train loss: 34.4658,	0.8164 s / batch. (data: 3.09e-04). ETA=9:17:32, max mem: 20.9 GB 
[11/22 23:37:10 visual_prompt]: Epoch 26 / 100: avg data time: 1.95e-01, avg batch time: 1.0124, average train loss: 149.0037
[11/22 23:38:08 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3068, average loss: 44.7650
[11/22 23:38:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.99	
[11/22 23:38:08 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/22 23:39:53 visual_prompt]: 	Training 100/553. train loss: 14.7403,	0.8320 s / batch. (data: 3.22e-04). ETA=9:26:04, max mem: 20.9 GB 
[11/22 23:41:33 visual_prompt]: 	Training 200/553. train loss: 319.8436,	1.2691 s / batch. (data: 4.41e-01). ETA=14:21:18, max mem: 20.9 GB 
[11/22 23:43:14 visual_prompt]: 	Training 300/553. train loss: 199.6077,	0.8290 s / batch. (data: 2.81e-02). ETA=9:21:16, max mem: 20.9 GB 
[11/22 23:44:56 visual_prompt]: 	Training 400/553. train loss: 0.2961,	0.8347 s / batch. (data: 7.76e-04). ETA=9:23:45, max mem: 20.9 GB 
[11/22 23:46:37 visual_prompt]: 	Training 500/553. train loss: 63.2854,	0.8108 s / batch. (data: 2.88e-04). ETA=9:06:15, max mem: 20.9 GB 
[11/22 23:47:28 visual_prompt]: Epoch 27 / 100: avg data time: 1.95e-01, avg batch time: 1.0127, average train loss: 175.4549
[11/22 23:48:25 visual_prompt]: Inference (val):avg data time: 1.75e-04, avg batch time: 0.3055, average loss: 296.2717
[11/22 23:48:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 34.15	
[11/22 23:48:25 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/22 23:50:09 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.7938 s / batch. (data: 3.08e-04). ETA=8:52:43, max mem: 20.9 GB 
[11/22 23:51:51 visual_prompt]: 	Training 200/553. train loss: 147.5573,	0.8200 s / batch. (data: 3.04e-04). ETA=9:08:58, max mem: 20.9 GB 
[11/22 23:53:31 visual_prompt]: 	Training 300/553. train loss: 39.2931,	1.5800 s / batch. (data: 7.62e-01). ETA=17:35:10, max mem: 20.9 GB 
[11/22 23:55:11 visual_prompt]: 	Training 400/553. train loss: 13.6634,	0.8240 s / batch. (data: 3.12e-04). ETA=9:08:53, max mem: 20.9 GB 
[11/22 23:56:50 visual_prompt]: 	Training 500/553. train loss: 440.2202,	0.8480 s / batch. (data: 5.42e-03). ETA=9:23:27, max mem: 20.9 GB 
[11/22 23:57:43 visual_prompt]: Epoch 28 / 100: avg data time: 1.91e-01, avg batch time: 1.0087, average train loss: 158.8234
[11/22 23:58:41 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3061, average loss: 254.6606
[11/22 23:58:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.74	
[11/22 23:58:41 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/23 00:00:33 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8341 s / batch. (data: 1.08e-02). ETA=9:12:06, max mem: 20.9 GB 
[11/23 00:02:12 visual_prompt]: 	Training 200/553. train loss: 346.1683,	1.8219 s / batch. (data: 1.02e+00). ETA=20:02:58, max mem: 20.9 GB 
[11/23 00:03:51 visual_prompt]: 	Training 300/553. train loss: 34.8636,	0.8370 s / batch. (data: 6.81e-04). ETA=9:11:13, max mem: 20.9 GB 
[11/23 00:05:28 visual_prompt]: 	Training 400/553. train loss: 364.2102,	1.4139 s / batch. (data: 6.04e-01). ETA=15:28:50, max mem: 20.9 GB 
[11/23 00:07:08 visual_prompt]: 	Training 500/553. train loss: 46.9617,	0.8484 s / batch. (data: 1.40e-02). ETA=9:15:54, max mem: 20.9 GB 
[11/23 00:08:00 visual_prompt]: Epoch 29 / 100: avg data time: 1.93e-01, avg batch time: 1.0107, average train loss: 139.2024
[11/23 00:08:58 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3074, average loss: 19.5667
[11/23 00:08:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.65	
[11/23 00:08:58 visual_prompt]: Best epoch 29: best metric: -19.567
[11/23 00:08:58 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/23 00:10:42 visual_prompt]: 	Training 100/553. train loss: 154.6503,	0.8338 s / batch. (data: 9.77e-03). ETA=9:04:15, max mem: 20.9 GB 
[11/23 00:12:24 visual_prompt]: 	Training 200/553. train loss: 16.3775,	0.8240 s / batch. (data: 2.90e-04). ETA=8:56:28, max mem: 20.9 GB 
[11/23 00:14:02 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.3040 s / batch. (data: 4.96e-01). ETA=14:06:47, max mem: 20.9 GB 
[11/23 00:15:45 visual_prompt]: 	Training 400/553. train loss: 98.6266,	1.2644 s / batch. (data: 4.33e-01). ETA=13:38:59, max mem: 20.9 GB 
[11/23 00:17:24 visual_prompt]: 	Training 500/553. train loss: 48.5852,	1.6947 s / batch. (data: 8.86e-01). ETA=18:14:52, max mem: 20.9 GB 
[11/23 00:18:18 visual_prompt]: Epoch 30 / 100: avg data time: 1.95e-01, avg batch time: 1.0127, average train loss: 140.6919
[11/23 00:19:16 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3065, average loss: 85.6259
[11/23 00:19:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.40	
[11/23 00:19:16 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[11/23 00:21:02 visual_prompt]: 	Training 100/553. train loss: 122.3913,	0.8280 s / batch. (data: 2.91e-04). ETA=8:52:48, max mem: 20.9 GB 
[11/23 00:22:44 visual_prompt]: 	Training 200/553. train loss: 31.0906,	0.8225 s / batch. (data: 3.13e-04). ETA=8:47:55, max mem: 20.9 GB 
[11/23 00:24:22 visual_prompt]: 	Training 300/553. train loss: 229.8587,	0.8228 s / batch. (data: 1.56e-02). ETA=8:46:44, max mem: 20.9 GB 
[11/23 00:26:02 visual_prompt]: 	Training 400/553. train loss: 157.4476,	1.0999 s / batch. (data: 2.97e-01). ETA=11:42:17, max mem: 20.9 GB 
[11/23 00:27:42 visual_prompt]: 	Training 500/553. train loss: 97.8932,	0.8127 s / batch. (data: 2.80e-04). ETA=8:37:32, max mem: 20.9 GB 
[11/23 00:28:34 visual_prompt]: Epoch 31 / 100: avg data time: 1.92e-01, avg batch time: 1.0090, average train loss: 142.1276
[11/23 00:29:32 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3072, average loss: 81.8708
[11/23 00:29:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.45	
[11/23 00:29:32 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[11/23 00:31:17 visual_prompt]: 	Training 100/553. train loss: 165.4758,	0.8404 s / batch. (data: 1.56e-02). ETA=8:53:03, max mem: 20.9 GB 
[11/23 00:32:57 visual_prompt]: 	Training 200/553. train loss: 15.5328,	0.8588 s / batch. (data: 7.74e-04). ETA=9:03:15, max mem: 20.9 GB 
[11/23 00:34:41 visual_prompt]: 	Training 300/553. train loss: 174.6999,	0.8177 s / batch. (data: 3.16e-04). ETA=8:35:57, max mem: 20.9 GB 
[11/23 00:36:22 visual_prompt]: 	Training 400/553. train loss: 54.7532,	0.8160 s / batch. (data: 2.90e-04). ETA=8:33:29, max mem: 20.9 GB 
[11/23 00:38:00 visual_prompt]: 	Training 500/553. train loss: 37.7764,	0.8181 s / batch. (data: 3.04e-04). ETA=8:33:27, max mem: 20.9 GB 
[11/23 00:38:51 visual_prompt]: Epoch 32 / 100: avg data time: 1.94e-01, avg batch time: 1.0109, average train loss: 139.3340
[11/23 00:39:49 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3066, average loss: 19.7126
[11/23 00:39:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.32	
[11/23 00:39:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[11/23 00:41:32 visual_prompt]: 	Training 100/553. train loss: 4.8962,	0.8200 s / batch. (data: 3.01e-04). ETA=8:32:33, max mem: 20.9 GB 
[11/23 00:43:14 visual_prompt]: 	Training 200/553. train loss: 18.0701,	1.2346 s / batch. (data: 4.28e-01). ETA=12:49:39, max mem: 20.9 GB 
[11/23 00:44:54 visual_prompt]: 	Training 300/553. train loss: 210.4590,	0.8492 s / batch. (data: 2.08e-02). ETA=8:47:57, max mem: 20.9 GB 
[11/23 00:46:35 visual_prompt]: 	Training 400/553. train loss: 330.1152,	0.8160 s / batch. (data: 3.22e-04). ETA=8:25:59, max mem: 20.9 GB 
[11/23 00:48:16 visual_prompt]: 	Training 500/553. train loss: 507.6531,	0.8227 s / batch. (data: 3.60e-04). ETA=8:28:44, max mem: 20.9 GB 
[11/23 00:49:07 visual_prompt]: Epoch 33 / 100: avg data time: 1.90e-01, avg batch time: 1.0088, average train loss: 143.6312
[11/23 00:50:04 visual_prompt]: Inference (val):avg data time: 1.73e-04, avg batch time: 0.3061, average loss: 127.1067
[11/23 00:50:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.03	
[11/23 00:50:04 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[11/23 00:51:51 visual_prompt]: 	Training 100/553. train loss: 19.8462,	0.8399 s / batch. (data: 1.20e-02). ETA=8:37:15, max mem: 20.9 GB 
[11/23 00:53:29 visual_prompt]: 	Training 200/553. train loss: 18.9643,	1.1600 s / batch. (data: 3.22e-01). ETA=11:52:28, max mem: 20.9 GB 
[11/23 00:55:09 visual_prompt]: 	Training 300/553. train loss: 309.5136,	0.8440 s / batch. (data: 2.96e-04). ETA=8:36:58, max mem: 20.9 GB 
[11/23 00:56:50 visual_prompt]: 	Training 400/553. train loss: 8.1546,	0.8303 s / batch. (data: 2.92e-04). ETA=8:27:10, max mem: 20.9 GB 
[11/23 00:58:31 visual_prompt]: 	Training 500/553. train loss: 112.1696,	1.5637 s / batch. (data: 7.22e-01). ETA=15:52:35, max mem: 20.9 GB 
[11/23 00:59:23 visual_prompt]: Epoch 34 / 100: avg data time: 1.91e-01, avg batch time: 1.0093, average train loss: 149.7802
[11/23 01:00:20 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3071, average loss: 111.8793
[11/23 01:00:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.49	
[11/23 01:00:20 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[11/23 01:02:07 visual_prompt]: 	Training 100/553. train loss: 103.1113,	0.8452 s / batch. (data: 9.14e-03). ETA=8:32:43, max mem: 20.9 GB 
[11/23 01:03:49 visual_prompt]: 	Training 200/553. train loss: 27.2771,	0.8240 s / batch. (data: 3.07e-04). ETA=8:18:30, max mem: 20.9 GB 
[11/23 01:05:27 visual_prompt]: 	Training 300/553. train loss: 37.5424,	0.8330 s / batch. (data: 2.23e-02). ETA=8:22:32, max mem: 20.9 GB 
[11/23 01:07:06 visual_prompt]: 	Training 400/553. train loss: 14.9664,	0.8680 s / batch. (data: 4.23e-02). ETA=8:42:12, max mem: 20.9 GB 
[11/23 01:08:46 visual_prompt]: 	Training 500/553. train loss: 55.5980,	1.2600 s / batch. (data: 4.44e-01). ETA=12:35:58, max mem: 20.9 GB 
[11/23 01:09:39 visual_prompt]: Epoch 35 / 100: avg data time: 1.90e-01, avg batch time: 1.0093, average train loss: 149.4510
[11/23 01:10:36 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3068, average loss: 265.0159
[11/23 01:10:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.16	
[11/23 01:10:36 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[11/23 01:12:20 visual_prompt]: 	Training 100/553. train loss: 115.7175,	0.8242 s / batch. (data: 1.64e-02). ETA=8:12:24, max mem: 20.9 GB 
[11/23 01:14:02 visual_prompt]: 	Training 200/553. train loss: 506.0617,	0.8320 s / batch. (data: 7.93e-03). ETA=8:15:39, max mem: 20.9 GB 
[11/23 01:15:45 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8146 s / batch. (data: 3.00e-04). ETA=8:03:57, max mem: 20.9 GB 
[11/23 01:17:25 visual_prompt]: 	Training 400/553. train loss: 23.3570,	0.8280 s / batch. (data: 3.09e-04). ETA=8:10:30, max mem: 20.9 GB 
[11/23 01:19:07 visual_prompt]: 	Training 500/553. train loss: 290.1146,	1.2520 s / batch. (data: 4.29e-01). ETA=12:19:37, max mem: 20.9 GB 
[11/23 01:19:57 visual_prompt]: Epoch 36 / 100: avg data time: 1.95e-01, avg batch time: 1.0128, average train loss: 140.2568
[11/23 01:20:55 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3040, average loss: 111.5739
[11/23 01:20:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.40	
[11/23 01:20:55 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[11/23 01:22:40 visual_prompt]: 	Training 100/553. train loss: 141.5817,	0.8340 s / batch. (data: 1.57e-02). ETA=8:10:33, max mem: 20.9 GB 
[11/23 01:24:20 visual_prompt]: 	Training 200/553. train loss: 168.3323,	0.8049 s / batch. (data: 2.99e-04). ETA=7:52:05, max mem: 20.9 GB 
[11/23 01:26:02 visual_prompt]: 	Training 300/553. train loss: 281.6743,	1.3466 s / batch. (data: 5.25e-01). ETA=13:07:33, max mem: 20.9 GB 
[11/23 01:27:45 visual_prompt]: 	Training 400/553. train loss: 3.9071,	2.1080 s / batch. (data: 1.27e+00). ETA=20:29:23, max mem: 20.9 GB 
[11/23 01:29:23 visual_prompt]: 	Training 500/553. train loss: 34.7462,	1.2959 s / batch. (data: 4.79e-01). ETA=12:33:38, max mem: 20.9 GB 
[11/23 01:30:17 visual_prompt]: Epoch 37 / 100: avg data time: 1.99e-01, avg batch time: 1.0171, average train loss: 140.7057
[11/23 01:31:15 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3047, average loss: 52.1396
[11/23 01:31:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.22	
[11/23 01:31:15 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[11/23 01:32:59 visual_prompt]: 	Training 100/553. train loss: 6.9585,	0.8734 s / batch. (data: 6.55e-02). ETA=8:25:41, max mem: 20.9 GB 
[11/23 01:34:41 visual_prompt]: 	Training 200/553. train loss: 107.9177,	1.4624 s / batch. (data: 6.43e-01). ETA=14:04:15, max mem: 20.9 GB 
[11/23 01:36:22 visual_prompt]: 	Training 300/553. train loss: 33.5977,	0.8240 s / batch. (data: 3.11e-04). ETA=7:54:19, max mem: 20.9 GB 
[11/23 01:38:02 visual_prompt]: 	Training 400/553. train loss: 126.3971,	0.8440 s / batch. (data: 3.54e-04). ETA=8:04:27, max mem: 20.9 GB 
[11/23 01:39:45 visual_prompt]: 	Training 500/553. train loss: 5.5677,	0.8364 s / batch. (data: 2.06e-02). ETA=7:58:41, max mem: 20.9 GB 
[11/23 01:40:36 visual_prompt]: Epoch 38 / 100: avg data time: 1.96e-01, avg batch time: 1.0141, average train loss: 140.6521
[11/23 01:41:34 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3076, average loss: 276.6873
[11/23 01:41:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.70	
[11/23 01:41:34 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[11/23 01:43:18 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8602 s / batch. (data: 2.44e-02). ETA=8:10:06, max mem: 20.9 GB 
[11/23 01:45:03 visual_prompt]: 	Training 200/553. train loss: 413.8148,	0.8148 s / batch. (data: 2.99e-04). ETA=7:42:53, max mem: 20.9 GB 
[11/23 01:46:48 visual_prompt]: 	Training 300/553. train loss: 124.1934,	0.8271 s / batch. (data: 7.97e-03). ETA=7:48:29, max mem: 20.9 GB 
[11/23 01:48:26 visual_prompt]: 	Training 400/553. train loss: 115.5211,	1.5200 s / batch. (data: 7.01e-01). ETA=14:18:26, max mem: 20.9 GB 
[11/23 01:50:07 visual_prompt]: 	Training 500/553. train loss: 40.5285,	1.8807 s / batch. (data: 1.06e+00). ETA=17:39:00, max mem: 20.9 GB 
[11/23 01:50:57 visual_prompt]: Epoch 39 / 100: avg data time: 1.99e-01, avg batch time: 1.0181, average train loss: 108.4750
[11/23 01:51:55 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3059, average loss: 209.0526
[11/23 01:51:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.69	
[11/23 01:51:55 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[11/23 01:53:41 visual_prompt]: 	Training 100/553. train loss: 375.7561,	0.8214 s / batch. (data: 2.83e-04). ETA=7:40:26, max mem: 20.9 GB 
[11/23 01:55:21 visual_prompt]: 	Training 200/553. train loss: 3.0492,	0.8098 s / batch. (data: 2.98e-04). ETA=7:32:35, max mem: 20.9 GB 
[11/23 01:57:04 visual_prompt]: 	Training 300/553. train loss: 100.3886,	0.8154 s / batch. (data: 1.54e-02). ETA=7:34:22, max mem: 20.9 GB 
[11/23 01:58:45 visual_prompt]: 	Training 400/553. train loss: 117.8543,	0.8276 s / batch. (data: 8.35e-04). ETA=7:39:47, max mem: 20.9 GB 
[11/23 02:00:25 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8560 s / batch. (data: 5.38e-03). ETA=7:54:07, max mem: 20.9 GB 
[11/23 02:01:19 visual_prompt]: Epoch 40 / 100: avg data time: 2.00e-01, avg batch time: 1.0184, average train loss: 132.2321
[11/23 02:02:17 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3078, average loss: 34.4287
[11/23 02:02:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.58	
[11/23 02:02:17 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[11/23 02:04:06 visual_prompt]: 	Training 100/553. train loss: 74.8127,	0.8194 s / batch. (data: 3.02e-04). ETA=7:31:46, max mem: 20.9 GB 
[11/23 02:05:49 visual_prompt]: 	Training 200/553. train loss: 32.2467,	0.8230 s / batch. (data: 7.83e-04). ETA=7:32:24, max mem: 20.9 GB 
[11/23 02:07:30 visual_prompt]: 	Training 300/553. train loss: 170.9885,	0.8200 s / batch. (data: 3.00e-04). ETA=7:29:21, max mem: 20.9 GB 
[11/23 02:09:10 visual_prompt]: 	Training 400/553. train loss: 106.4628,	0.8308 s / batch. (data: 2.87e-04). ETA=7:33:53, max mem: 20.9 GB 
[11/23 02:10:48 visual_prompt]: 	Training 500/553. train loss: 99.3491,	0.8224 s / batch. (data: 8.06e-04). ETA=7:27:54, max mem: 20.9 GB 
[11/23 02:11:38 visual_prompt]: Epoch 41 / 100: avg data time: 1.97e-01, avg batch time: 1.0157, average train loss: 141.9813
[11/23 02:12:36 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3074, average loss: 86.1727
[11/23 02:12:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.03	
[11/23 02:12:36 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[11/23 02:14:20 visual_prompt]: 	Training 100/553. train loss: 261.2810,	0.8200 s / batch. (data: 2.89e-04). ETA=7:24:31, max mem: 20.9 GB 
[11/23 02:16:02 visual_prompt]: 	Training 200/553. train loss: 1197.9874,	0.8018 s / batch. (data: 3.02e-04). ETA=7:13:18, max mem: 20.9 GB 
[11/23 02:17:43 visual_prompt]: 	Training 300/553. train loss: 27.7327,	0.8239 s / batch. (data: 6.40e-04). ETA=7:23:55, max mem: 20.9 GB 
[11/23 02:19:24 visual_prompt]: 	Training 400/553. train loss: 58.5433,	0.8160 s / batch. (data: 3.32e-04). ETA=7:18:16, max mem: 20.9 GB 
[11/23 02:21:03 visual_prompt]: 	Training 500/553. train loss: 0.5195,	0.8254 s / batch. (data: 1.08e-02). ETA=7:21:57, max mem: 20.9 GB 
[11/23 02:21:57 visual_prompt]: Epoch 42 / 100: avg data time: 1.97e-01, avg batch time: 1.0142, average train loss: 173.6028
[11/23 02:22:56 visual_prompt]: Inference (val):avg data time: 1.78e-04, avg batch time: 0.3066, average loss: 80.3345
[11/23 02:22:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.94	
[11/23 02:22:56 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[11/23 02:24:42 visual_prompt]: 	Training 100/553. train loss: 210.0464,	0.8512 s / batch. (data: 1.05e-02). ETA=7:33:36, max mem: 20.9 GB 
[11/23 02:26:22 visual_prompt]: 	Training 200/553. train loss: 132.1355,	0.8011 s / batch. (data: 2.75e-04). ETA=7:05:34, max mem: 20.9 GB 
[11/23 02:28:00 visual_prompt]: 	Training 300/553. train loss: 264.3711,	0.8280 s / batch. (data: 5.40e-03). ETA=7:18:29, max mem: 20.9 GB 
[11/23 02:29:40 visual_prompt]: 	Training 400/553. train loss: 103.4724,	0.8240 s / batch. (data: 2.87e-04). ETA=7:14:59, max mem: 20.9 GB 
[11/23 02:31:24 visual_prompt]: 	Training 500/553. train loss: 181.9322,	0.8326 s / batch. (data: 1.05e-02). ETA=7:18:08, max mem: 20.9 GB 
[11/23 02:32:16 visual_prompt]: Epoch 43 / 100: avg data time: 1.96e-01, avg batch time: 1.0128, average train loss: 111.9246
[11/23 02:33:14 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3077, average loss: 79.2636
[11/23 02:33:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.53	
[11/23 02:33:14 visual_prompt]: Stopping early.
[11/23 02:33:14 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 02:33:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 02:33:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/23 02:33:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 02:33:14 visual_prompt]: Training with config:
[11/23 02:33:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr50.0_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/23 02:33:14 visual_prompt]: Loading training data...
[11/23 02:33:14 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 02:33:14 visual_prompt]: Loading validation data...
[11/23 02:33:14 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 02:33:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 02:33:17 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 02:33:17 visual_prompt]: tuned percent:0.525
[11/23 02:33:17 visual_prompt]: Device used for model: 0
[11/23 02:33:17 visual_prompt]: Setting up Evaluator...
[11/23 02:33:17 visual_prompt]: Setting up Trainer...
[11/23 02:33:17 visual_prompt]: 	Setting up the optimizer...
[11/23 02:33:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 02:35:02 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8240 s / batch. (data: 2.95e-04). ETA=12:38:05, max mem: 20.9 GB 
[11/23 02:36:42 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8206 s / batch. (data: 9.61e-03). ETA=12:33:36, max mem: 20.9 GB 
[11/23 02:38:25 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3243 s / batch. (data: 5.16e-01). ETA=20:13:55, max mem: 20.9 GB 
[11/23 02:40:05 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8137 s / batch. (data: 7.94e-03). ETA=12:24:33, max mem: 20.9 GB 
[11/23 02:41:49 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8240 s / batch. (data: 8.38e-04). ETA=12:32:33, max mem: 20.9 GB 
[11/23 02:42:42 visual_prompt]: Epoch 1 / 100: avg data time: 1.97e-01, avg batch time: 1.0210, average train loss: 1.5403
[11/23 02:43:39 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3069, average loss: 1.5201
[11/23 02:43:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 02:43:39 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[11/23 02:45:23 visual_prompt]: 	Training 100/553. train loss: 32.7968,	0.9309 s / batch. (data: 1.12e-01). ETA=14:07:52, max mem: 20.9 GB 
[11/23 02:47:04 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8280 s / batch. (data: 3.38e-04). ETA=12:32:44, max mem: 20.9 GB 
[11/23 02:48:48 visual_prompt]: 	Training 300/553. train loss: 2.9277,	0.9693 s / batch. (data: 1.62e-01). ETA=14:39:33, max mem: 20.9 GB 
[11/23 02:50:27 visual_prompt]: 	Training 400/553. train loss: 36.1745,	0.8177 s / batch. (data: 1.13e-02). ETA=12:20:39, max mem: 20.9 GB 
[11/23 02:52:09 visual_prompt]: 	Training 500/553. train loss: 13.4565,	0.8440 s / batch. (data: 7.96e-03). ETA=12:43:05, max mem: 20.9 GB 
[11/23 02:53:01 visual_prompt]: Epoch 2 / 100: avg data time: 1.94e-01, avg batch time: 1.0157, average train loss: 22.7402
[11/23 02:53:59 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3062, average loss: 21.8746
[11/23 02:53:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.07	
[11/23 02:53:59 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[11/23 02:55:43 visual_prompt]: 	Training 100/553. train loss: 16.2583,	0.8381 s / batch. (data: 1.05e-02). ETA=12:35:36, max mem: 20.9 GB 
[11/23 02:57:25 visual_prompt]: 	Training 200/553. train loss: 23.2656,	0.8120 s / batch. (data: 2.93e-04). ETA=12:10:43, max mem: 20.9 GB 
[11/23 02:59:05 visual_prompt]: 	Training 300/553. train loss: 11.3496,	0.8320 s / batch. (data: 3.34e-04). ETA=12:27:20, max mem: 20.9 GB 
[11/23 03:00:48 visual_prompt]: 	Training 400/553. train loss: 189.8524,	0.8320 s / batch. (data: 5.42e-03). ETA=12:25:56, max mem: 20.9 GB 
[11/23 03:02:30 visual_prompt]: 	Training 500/553. train loss: 22.7126,	1.2791 s / batch. (data: 4.37e-01). ETA=19:04:40, max mem: 20.9 GB 
[11/23 03:03:21 visual_prompt]: Epoch 3 / 100: avg data time: 1.96e-01, avg batch time: 1.0165, average train loss: 34.7822
[11/23 03:04:19 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3060, average loss: 19.8792
[11/23 03:04:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.07	
[11/23 03:04:19 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[11/23 03:06:05 visual_prompt]: 	Training 100/553. train loss: 6.3957,	0.8387 s / batch. (data: 7.09e-04). ETA=12:28:23, max mem: 20.9 GB 
[11/23 03:07:47 visual_prompt]: 	Training 200/553. train loss: 52.6492,	0.8586 s / batch. (data: 5.47e-03). ETA=12:44:45, max mem: 20.9 GB 
[11/23 03:09:28 visual_prompt]: 	Training 300/553. train loss: 10.5361,	1.4819 s / batch. (data: 6.53e-01). ETA=21:57:26, max mem: 20.9 GB 
[11/23 03:11:04 visual_prompt]: 	Training 400/553. train loss: 52.3675,	1.1217 s / batch. (data: 3.25e-01). ETA=16:35:18, max mem: 20.9 GB 
[11/23 03:12:48 visual_prompt]: 	Training 500/553. train loss: 40.5577,	3.4687 s / batch. (data: 2.64e+00). ETA=2 days, 3:12:08, max mem: 20.9 GB 
[11/23 03:13:41 visual_prompt]: Epoch 4 / 100: avg data time: 1.98e-01, avg batch time: 1.0170, average train loss: 63.2746
[11/23 03:14:39 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3053, average loss: 170.5102
[11/23 03:14:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.57	
[11/23 03:14:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[11/23 03:16:23 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8140 s / batch. (data: 3.09e-04). ETA=11:58:53, max mem: 20.9 GB 
[11/23 03:18:04 visual_prompt]: 	Training 200/553. train loss: 3.0704,	1.0611 s / batch. (data: 2.26e-01). ETA=15:35:17, max mem: 20.9 GB 
[11/23 03:19:46 visual_prompt]: 	Training 300/553. train loss: 18.1324,	0.8303 s / batch. (data: 6.27e-03). ETA=12:10:30, max mem: 20.9 GB 
[11/23 03:21:26 visual_prompt]: 	Training 400/553. train loss: 79.8581,	0.8600 s / batch. (data: 2.98e-04). ETA=12:35:11, max mem: 20.9 GB 
[11/23 03:23:08 visual_prompt]: 	Training 500/553. train loss: 23.4526,	0.8120 s / batch. (data: 3.15e-04). ETA=11:51:41, max mem: 20.9 GB 
[11/23 03:24:01 visual_prompt]: Epoch 5 / 100: avg data time: 1.98e-01, avg batch time: 1.0163, average train loss: 79.2135
[11/23 03:24:59 visual_prompt]: Inference (val):avg data time: 1.57e-04, avg batch time: 0.3046, average loss: 143.7809
[11/23 03:24:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.37	
[11/23 03:24:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[11/23 03:26:46 visual_prompt]: 	Training 100/553. train loss: 6.2837,	0.8360 s / batch. (data: 3.17e-04). ETA=12:10:35, max mem: 20.9 GB 
[11/23 03:28:26 visual_prompt]: 	Training 200/553. train loss: 118.3204,	0.8240 s / batch. (data: 3.15e-04). ETA=11:58:43, max mem: 20.9 GB 
[11/23 03:30:05 visual_prompt]: 	Training 300/553. train loss: 54.3363,	0.8343 s / batch. (data: 1.05e-02). ETA=12:06:20, max mem: 20.9 GB 
[11/23 03:31:51 visual_prompt]: 	Training 400/553. train loss: 99.6455,	0.8278 s / batch. (data: 3.15e-04). ETA=11:59:19, max mem: 20.9 GB 
[11/23 03:33:30 visual_prompt]: 	Training 500/553. train loss: 30.0783,	0.8184 s / batch. (data: 3.71e-04). ETA=11:49:44, max mem: 20.9 GB 
[11/23 03:34:22 visual_prompt]: Epoch 6 / 100: avg data time: 1.98e-01, avg batch time: 1.0174, average train loss: 66.1335
[11/23 03:35:20 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3071, average loss: 99.1012
[11/23 03:35:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.41	
[11/23 03:35:20 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[11/23 03:37:03 visual_prompt]: 	Training 100/553. train loss: 0.2706,	0.8082 s / batch. (data: 2.90e-04). ETA=11:38:48, max mem: 20.9 GB 
[11/23 03:38:44 visual_prompt]: 	Training 200/553. train loss: 25.0724,	0.8135 s / batch. (data: 3.93e-04). ETA=11:42:03, max mem: 20.9 GB 
[11/23 03:40:29 visual_prompt]: 	Training 300/553. train loss: 10.4053,	2.0254 s / batch. (data: 1.21e+00). ETA=1 day, 5:04:34, max mem: 20.9 GB 
[11/23 03:42:10 visual_prompt]: 	Training 400/553. train loss: 14.4586,	2.0200 s / batch. (data: 1.20e+00). ETA=1 day, 4:56:35, max mem: 20.9 GB 
[11/23 03:43:49 visual_prompt]: 	Training 500/553. train loss: 24.6965,	0.8242 s / batch. (data: 3.07e-04). ETA=11:47:10, max mem: 20.9 GB 
[11/23 03:44:40 visual_prompt]: Epoch 7 / 100: avg data time: 1.93e-01, avg batch time: 1.0120, average train loss: 73.2909
[11/23 03:45:38 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3052, average loss: 76.7275
[11/23 03:45:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.65	
[11/23 03:45:38 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[11/23 03:47:21 visual_prompt]: 	Training 100/553. train loss: 15.1555,	0.8082 s / batch. (data: 2.89e-04). ETA=11:31:21, max mem: 20.9 GB 
[11/23 03:49:03 visual_prompt]: 	Training 200/553. train loss: 95.7314,	0.8200 s / batch. (data: 3.11e-04). ETA=11:40:09, max mem: 20.9 GB 
[11/23 03:50:44 visual_prompt]: 	Training 300/553. train loss: 156.5140,	0.8371 s / batch. (data: 2.06e-02). ETA=11:53:20, max mem: 20.9 GB 
[11/23 03:52:25 visual_prompt]: 	Training 400/553. train loss: 166.6465,	0.8241 s / batch. (data: 4.78e-03). ETA=11:40:51, max mem: 20.9 GB 
[11/23 03:54:05 visual_prompt]: 	Training 500/553. train loss: 285.2204,	1.5800 s / batch. (data: 7.51e-01). ETA=22:21:07, max mem: 20.9 GB 
[11/23 03:54:59 visual_prompt]: Epoch 8 / 100: avg data time: 1.97e-01, avg batch time: 1.0138, average train loss: 99.5197
[11/23 03:55:57 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3065, average loss: 35.6332
[11/23 03:55:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.89	
[11/23 03:55:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[11/23 03:57:42 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8382 s / batch. (data: 5.44e-03). ETA=11:49:22, max mem: 20.9 GB 
[11/23 03:59:22 visual_prompt]: 	Training 200/553. train loss: 43.8888,	0.8115 s / batch. (data: 3.15e-04). ETA=11:25:21, max mem: 20.9 GB 
[11/23 04:01:04 visual_prompt]: 	Training 300/553. train loss: 137.8746,	1.9202 s / batch. (data: 1.11e+00). ETA=1 day, 2:58:37, max mem: 20.9 GB 
[11/23 04:02:45 visual_prompt]: 	Training 400/553. train loss: 35.5964,	0.8033 s / batch. (data: 7.96e-03). ETA=11:15:49, max mem: 20.9 GB 
[11/23 04:04:27 visual_prompt]: 	Training 500/553. train loss: 182.5312,	0.9520 s / batch. (data: 1.28e-01). ETA=13:19:17, max mem: 20.9 GB 
[11/23 04:05:18 visual_prompt]: Epoch 9 / 100: avg data time: 1.98e-01, avg batch time: 1.0157, average train loss: 78.0807
[11/23 04:06:16 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3056, average loss: 328.8175
[11/23 04:06:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.86	
[11/23 04:06:16 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[11/23 04:08:04 visual_prompt]: 	Training 100/553. train loss: 26.8022,	0.8120 s / batch. (data: 3.25e-04). ETA=11:19:40, max mem: 20.9 GB 
[11/23 04:09:43 visual_prompt]: 	Training 200/553. train loss: 121.5240,	0.8240 s / batch. (data: 3.10e-04). ETA=11:28:22, max mem: 20.9 GB 
[11/23 04:11:24 visual_prompt]: 	Training 300/553. train loss: 160.5820,	2.1674 s / batch. (data: 1.35e+00). ETA=1 day, 6:07:01, max mem: 20.9 GB 
[11/23 04:13:02 visual_prompt]: 	Training 400/553. train loss: 168.6008,	0.8320 s / batch. (data: 7.93e-03). ETA=11:32:15, max mem: 20.9 GB 
[11/23 04:14:44 visual_prompt]: 	Training 500/553. train loss: 579.5065,	0.8111 s / batch. (data: 4.37e-04). ETA=11:13:30, max mem: 20.9 GB 
[11/23 04:15:37 visual_prompt]: Epoch 10 / 100: avg data time: 1.96e-01, avg batch time: 1.0135, average train loss: 138.4034
[11/23 04:16:35 visual_prompt]: Inference (val):avg data time: 3.33e-04, avg batch time: 0.3068, average loss: 7.5994
[11/23 04:16:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.49	
[11/23 04:16:35 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[11/23 04:18:21 visual_prompt]: 	Training 100/553. train loss: 88.7464,	0.8278 s / batch. (data: 7.94e-03). ETA=11:25:15, max mem: 20.9 GB 
[11/23 04:20:04 visual_prompt]: 	Training 200/553. train loss: 85.2204,	0.8085 s / batch. (data: 3.15e-04). ETA=11:07:59, max mem: 20.9 GB 
[11/23 04:21:45 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.4680 s / batch. (data: 1.65e+00). ETA=1 day, 9:54:52, max mem: 20.9 GB 
[11/23 04:23:23 visual_prompt]: 	Training 400/553. train loss: 99.9953,	0.8360 s / batch. (data: 7.95e-03). ETA=11:27:52, max mem: 20.9 GB 
[11/23 04:25:03 visual_prompt]: 	Training 500/553. train loss: 108.4333,	0.8339 s / batch. (data: 5.42e-03). ETA=11:24:47, max mem: 20.9 GB 
[11/23 04:25:55 visual_prompt]: Epoch 11 / 100: avg data time: 1.95e-01, avg batch time: 1.0119, average train loss: 111.4261
[11/23 04:26:52 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3052, average loss: 323.4079
[11/23 04:26:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.19	
[11/23 04:26:52 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/23 04:28:39 visual_prompt]: 	Training 100/553. train loss: 40.6332,	0.9414 s / batch. (data: 1.34e-01). ETA=12:50:38, max mem: 20.9 GB 
[11/23 04:30:21 visual_prompt]: 	Training 200/553. train loss: 21.3151,	0.8240 s / batch. (data: 5.42e-03). ETA=11:13:11, max mem: 20.9 GB 
[11/23 04:31:59 visual_prompt]: 	Training 300/553. train loss: 58.6921,	0.8267 s / batch. (data: 3.19e-04). ETA=11:14:00, max mem: 20.9 GB 
[11/23 04:33:41 visual_prompt]: 	Training 400/553. train loss: 200.1610,	0.8400 s / batch. (data: 7.95e-03). ETA=11:23:26, max mem: 20.9 GB 
[11/23 04:35:21 visual_prompt]: 	Training 500/553. train loss: 466.1448,	0.8297 s / batch. (data: 7.99e-04). ETA=11:13:41, max mem: 20.9 GB 
[11/23 04:36:13 visual_prompt]: Epoch 12 / 100: avg data time: 1.95e-01, avg batch time: 1.0131, average train loss: 115.6275
[11/23 04:37:11 visual_prompt]: Inference (val):avg data time: 1.92e-04, avg batch time: 0.3062, average loss: 121.8249
[11/23 04:37:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.01	
[11/23 04:37:11 visual_prompt]: Best epoch 12: best metric: -121.825
[11/23 04:37:11 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[11/23 04:38:58 visual_prompt]: 	Training 100/553. train loss: 104.7513,	0.8200 s / batch. (data: 7.95e-03). ETA=11:03:42, max mem: 20.9 GB 
[11/23 04:40:34 visual_prompt]: 	Training 200/553. train loss: 6.9765,	0.8280 s / batch. (data: 3.29e-04). ETA=11:08:47, max mem: 20.9 GB 
[11/23 04:42:17 visual_prompt]: 	Training 300/553. train loss: 98.9173,	1.8364 s / batch. (data: 1.04e+00). ETA=1 day, 0:40:17, max mem: 20.9 GB 
[11/23 04:43:56 visual_prompt]: 	Training 400/553. train loss: 208.4926,	0.8067 s / batch. (data: 3.13e-04). ETA=10:48:56, max mem: 20.9 GB 
[11/23 04:45:38 visual_prompt]: 	Training 500/553. train loss: 157.6126,	0.8072 s / batch. (data: 3.05e-04). ETA=10:47:55, max mem: 20.9 GB 
[11/23 04:46:31 visual_prompt]: Epoch 13 / 100: avg data time: 1.94e-01, avg batch time: 1.0123, average train loss: 87.1781
[11/23 04:47:29 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3052, average loss: 192.0014
[11/23 04:47:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.44	
[11/23 04:47:29 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[11/23 04:49:15 visual_prompt]: 	Training 100/553. train loss: 31.8856,	0.8405 s / batch. (data: 2.97e-04). ETA=11:12:31, max mem: 20.9 GB 
[11/23 04:50:56 visual_prompt]: 	Training 200/553. train loss: 20.6185,	1.1719 s / batch. (data: 3.37e-01). ETA=15:35:48, max mem: 20.9 GB 
[11/23 04:52:36 visual_prompt]: 	Training 300/553. train loss: 50.9220,	0.8195 s / batch. (data: 7.95e-03). ETA=10:53:01, max mem: 20.9 GB 
[11/23 04:54:16 visual_prompt]: 	Training 400/553. train loss: 113.5434,	0.8290 s / batch. (data: 1.05e-02). ETA=10:59:11, max mem: 20.9 GB 
[11/23 04:55:58 visual_prompt]: 	Training 500/553. train loss: 16.4016,	0.8057 s / batch. (data: 3.03e-04). ETA=10:39:19, max mem: 20.9 GB 
[11/23 04:56:49 visual_prompt]: Epoch 14 / 100: avg data time: 1.94e-01, avg batch time: 1.0127, average train loss: 95.4231
[11/23 04:57:47 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3065, average loss: 35.9889
[11/23 04:57:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.24	
[11/23 04:57:47 visual_prompt]: Best epoch 14: best metric: -35.989
[11/23 04:57:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[11/23 04:59:32 visual_prompt]: 	Training 100/553. train loss: 77.4675,	1.2160 s / batch. (data: 4.02e-01). ETA=16:01:47, max mem: 20.9 GB 
[11/23 05:01:11 visual_prompt]: 	Training 200/553. train loss: 24.2004,	0.8458 s / batch. (data: 5.46e-03). ETA=11:07:35, max mem: 20.9 GB 
[11/23 05:02:54 visual_prompt]: 	Training 300/553. train loss: 144.2614,	0.8200 s / batch. (data: 7.50e-04). ETA=10:45:52, max mem: 20.9 GB 
[11/23 05:04:32 visual_prompt]: 	Training 400/553. train loss: 97.0569,	1.0840 s / batch. (data: 2.83e-01). ETA=14:11:59, max mem: 20.9 GB 
[11/23 05:06:15 visual_prompt]: 	Training 500/553. train loss: 26.6353,	0.8120 s / batch. (data: 3.44e-04). ETA=10:36:52, max mem: 20.9 GB 
[11/23 05:07:08 visual_prompt]: Epoch 15 / 100: avg data time: 1.96e-01, avg batch time: 1.0143, average train loss: 137.0193
[11/23 05:08:06 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3076, average loss: 318.4160
[11/23 05:08:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.17	
[11/23 05:08:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[11/23 05:09:50 visual_prompt]: 	Training 100/553. train loss: 188.9208,	0.8329 s / batch. (data: 1.05e-02). ETA=10:51:07, max mem: 20.9 GB 
[11/23 05:11:31 visual_prompt]: 	Training 200/553. train loss: 44.1214,	0.8046 s / batch. (data: 3.31e-04). ETA=10:27:38, max mem: 20.9 GB 
[11/23 05:13:12 visual_prompt]: 	Training 300/553. train loss: 83.9578,	0.8440 s / batch. (data: 2.97e-04). ETA=10:56:59, max mem: 20.9 GB 
[11/23 05:14:53 visual_prompt]: 	Training 400/553. train loss: 95.3845,	0.8337 s / batch. (data: 7.41e-04). ETA=10:47:36, max mem: 20.9 GB 
[11/23 05:16:33 visual_prompt]: 	Training 500/553. train loss: 26.6261,	0.8151 s / batch. (data: 3.30e-04). ETA=10:31:45, max mem: 20.9 GB 
[11/23 05:17:27 visual_prompt]: Epoch 16 / 100: avg data time: 1.95e-01, avg batch time: 1.0141, average train loss: 90.8547
[11/23 05:18:25 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3065, average loss: 12.5311
[11/23 05:18:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/23 05:18:25 visual_prompt]: Best epoch 16: best metric: -12.531
[11/23 05:18:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[11/23 05:20:08 visual_prompt]: 	Training 100/553. train loss: 123.2160,	0.8136 s / batch. (data: 3.28e-04). ETA=10:28:33, max mem: 20.9 GB 
[11/23 05:21:51 visual_prompt]: 	Training 200/553. train loss: 49.1753,	0.8280 s / batch. (data: 2.87e-04). ETA=10:38:16, max mem: 20.9 GB 
[11/23 05:23:31 visual_prompt]: 	Training 300/553. train loss: 124.8512,	0.8039 s / batch. (data: 3.34e-04). ETA=10:18:22, max mem: 20.9 GB 
[11/23 05:25:11 visual_prompt]: 	Training 400/553. train loss: 30.3349,	1.1080 s / batch. (data: 2.91e-01). ETA=14:10:25, max mem: 20.9 GB 
[11/23 05:26:51 visual_prompt]: 	Training 500/553. train loss: 288.6173,	1.2891 s / batch. (data: 4.69e-01). ETA=16:27:14, max mem: 20.9 GB 
[11/23 05:27:45 visual_prompt]: Epoch 17 / 100: avg data time: 1.95e-01, avg batch time: 1.0138, average train loss: 94.9273
[11/23 05:28:44 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3067, average loss: 44.3459
[11/23 05:28:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.74	
[11/23 05:28:44 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[11/23 05:30:29 visual_prompt]: 	Training 100/553. train loss: 103.0587,	0.8165 s / batch. (data: 7.95e-03). ETA=10:23:13, max mem: 20.9 GB 
[11/23 05:32:12 visual_prompt]: 	Training 200/553. train loss: 12.4207,	0.8221 s / batch. (data: 3.22e-04). ETA=10:26:11, max mem: 20.9 GB 
[11/23 05:33:53 visual_prompt]: 	Training 300/553. train loss: 114.4238,	0.8239 s / batch. (data: 3.17e-04). ETA=10:26:08, max mem: 20.9 GB 
[11/23 05:35:34 visual_prompt]: 	Training 400/553. train loss: 77.0176,	0.8144 s / batch. (data: 2.88e-04). ETA=10:17:35, max mem: 20.9 GB 
[11/23 05:37:14 visual_prompt]: 	Training 500/553. train loss: 48.2409,	0.9331 s / batch. (data: 1.27e-01). ETA=11:46:01, max mem: 20.9 GB 
[11/23 05:38:06 visual_prompt]: Epoch 18 / 100: avg data time: 1.98e-01, avg batch time: 1.0164, average train loss: 95.3656
[11/23 05:39:04 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3069, average loss: 165.1938
[11/23 05:39:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.67	
[11/23 05:39:04 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[11/23 05:40:48 visual_prompt]: 	Training 100/553. train loss: 37.5920,	0.8120 s / batch. (data: 3.25e-04). ETA=10:12:18, max mem: 20.9 GB 
[11/23 05:42:30 visual_prompt]: 	Training 200/553. train loss: 78.9635,	0.8200 s / batch. (data: 3.09e-04). ETA=10:17:00, max mem: 20.9 GB 
[11/23 05:44:12 visual_prompt]: 	Training 300/553. train loss: 444.9015,	0.8158 s / batch. (data: 3.07e-04). ETA=10:12:28, max mem: 20.9 GB 
[11/23 05:45:54 visual_prompt]: 	Training 400/553. train loss: 80.1770,	0.8242 s / batch. (data: 8.10e-04). ETA=10:17:25, max mem: 20.9 GB 
[11/23 05:47:29 visual_prompt]: 	Training 500/553. train loss: 267.2652,	0.8193 s / batch. (data: 1.56e-02). ETA=10:12:20, max mem: 20.9 GB 
[11/23 05:48:23 visual_prompt]: Epoch 19 / 100: avg data time: 1.93e-01, avg batch time: 1.0108, average train loss: 111.7685
[11/23 05:49:21 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3041, average loss: 261.0260
[11/23 05:49:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.96	
[11/23 05:49:21 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[11/23 05:51:03 visual_prompt]: 	Training 100/553. train loss: 41.4846,	0.8140 s / batch. (data: 3.22e-04). ETA=10:06:22, max mem: 20.9 GB 
[11/23 05:52:47 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8293 s / batch. (data: 1.06e-02). ETA=10:16:19, max mem: 20.9 GB 
[11/23 05:54:28 visual_prompt]: 	Training 300/553. train loss: 32.8508,	0.8560 s / batch. (data: 8.12e-04). ETA=10:34:46, max mem: 20.9 GB 
[11/23 05:56:08 visual_prompt]: 	Training 400/553. train loss: 91.8494,	0.8280 s / batch. (data: 3.16e-04). ETA=10:12:37, max mem: 20.9 GB 
[11/23 05:57:49 visual_prompt]: 	Training 500/553. train loss: 37.3914,	0.8100 s / batch. (data: 3.01e-04). ETA=9:57:58, max mem: 20.9 GB 
[11/23 05:58:43 visual_prompt]: Epoch 20 / 100: avg data time: 1.98e-01, avg batch time: 1.0173, average train loss: 83.9298
[11/23 05:59:41 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3058, average loss: 56.4203
[11/23 05:59:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.07	
[11/23 05:59:41 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[11/23 06:01:29 visual_prompt]: 	Training 100/553. train loss: 44.5488,	0.8196 s / batch. (data: 3.17e-04). ETA=10:02:59, max mem: 20.9 GB 
[11/23 06:03:09 visual_prompt]: 	Training 200/553. train loss: 0.0001,	0.8200 s / batch. (data: 3.03e-04). ETA=10:01:52, max mem: 20.9 GB 
[11/23 06:04:49 visual_prompt]: 	Training 300/553. train loss: 287.9181,	1.3000 s / batch. (data: 4.73e-01). ETA=15:52:02, max mem: 20.9 GB 
[11/23 06:06:29 visual_prompt]: 	Training 400/553. train loss: 398.9373,	0.8200 s / batch. (data: 3.02e-04). ETA=9:59:08, max mem: 20.9 GB 
[11/23 06:08:11 visual_prompt]: 	Training 500/553. train loss: 99.6390,	0.8335 s / batch. (data: 2.14e-02). ETA=10:07:37, max mem: 20.9 GB 
[11/23 06:09:02 visual_prompt]: Epoch 21 / 100: avg data time: 1.97e-01, avg batch time: 1.0146, average train loss: 98.6770
[11/23 06:10:00 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3066, average loss: 27.0083
[11/23 06:10:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.06	
[11/23 06:10:00 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[11/23 06:11:45 visual_prompt]: 	Training 100/553. train loss: 215.8323,	0.8440 s / batch. (data: 7.95e-03). ETA=10:13:07, max mem: 20.9 GB 
[11/23 06:13:26 visual_prompt]: 	Training 200/553. train loss: 27.9844,	0.8449 s / batch. (data: 8.83e-03). ETA=10:12:20, max mem: 20.9 GB 
[11/23 06:15:04 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8018 s / batch. (data: 3.04e-04). ETA=9:39:45, max mem: 20.9 GB 
[11/23 06:16:46 visual_prompt]: 	Training 400/553. train loss: 2.6714,	0.8120 s / batch. (data: 2.92e-04). ETA=9:45:48, max mem: 20.9 GB 
[11/23 06:18:27 visual_prompt]: 	Training 500/553. train loss: 100.9906,	0.7974 s / batch. (data: 3.25e-04). ETA=9:33:58, max mem: 20.9 GB 
[11/23 06:19:21 visual_prompt]: Epoch 22 / 100: avg data time: 1.97e-01, avg batch time: 1.0130, average train loss: 81.1880
[11/23 06:20:19 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3052, average loss: 61.1178
[11/23 06:20:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.34	
[11/23 06:20:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[11/23 06:22:05 visual_prompt]: 	Training 100/553. train loss: 140.7388,	0.8200 s / batch. (data: 5.44e-03). ETA=9:48:07, max mem: 20.9 GB 
[11/23 06:23:46 visual_prompt]: 	Training 200/553. train loss: 43.6858,	0.8440 s / batch. (data: 1.20e-02). ETA=10:03:55, max mem: 20.9 GB 
[11/23 06:25:29 visual_prompt]: 	Training 300/553. train loss: 9.4103,	0.8400 s / batch. (data: 3.27e-04). ETA=9:59:39, max mem: 20.9 GB 
[11/23 06:27:08 visual_prompt]: 	Training 400/553. train loss: 2.1598,	0.8575 s / batch. (data: 3.25e-04). ETA=10:10:45, max mem: 20.9 GB 
[11/23 06:28:47 visual_prompt]: 	Training 500/553. train loss: 317.9267,	0.8200 s / batch. (data: 2.91e-04). ETA=9:42:40, max mem: 20.9 GB 
[11/23 06:29:39 visual_prompt]: Epoch 23 / 100: avg data time: 1.94e-01, avg batch time: 1.0132, average train loss: 82.0441
[11/23 06:30:37 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3064, average loss: 39.6244
[11/23 06:30:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.19	
[11/23 06:30:37 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[11/23 06:32:20 visual_prompt]: 	Training 100/553. train loss: 20.0959,	1.0920 s / batch. (data: 2.67e-01). ETA=12:53:09, max mem: 20.9 GB 
[11/23 06:34:01 visual_prompt]: 	Training 200/553. train loss: 130.3906,	0.8400 s / batch. (data: 3.03e-04). ETA=9:53:19, max mem: 20.9 GB 
[11/23 06:35:42 visual_prompt]: 	Training 300/553. train loss: 8.9930,	0.9920 s / batch. (data: 1.83e-01). ETA=11:39:01, max mem: 20.9 GB 
[11/23 06:37:24 visual_prompt]: 	Training 400/553. train loss: 39.7635,	0.8086 s / batch. (data: 3.92e-04). ETA=9:28:28, max mem: 20.9 GB 
[11/23 06:39:06 visual_prompt]: 	Training 500/553. train loss: 108.8881,	0.8040 s / batch. (data: 3.08e-04). ETA=9:23:54, max mem: 20.9 GB 
[11/23 06:39:59 visual_prompt]: Epoch 24 / 100: avg data time: 1.98e-01, avg batch time: 1.0166, average train loss: 81.1871
[11/23 06:40:57 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3058, average loss: 28.5256
[11/23 06:40:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 64.54	
[11/23 06:40:57 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[11/23 06:42:45 visual_prompt]: 	Training 100/553. train loss: 91.4839,	0.8200 s / batch. (data: 3.04e-04). ETA=9:33:00, max mem: 20.9 GB 
[11/23 06:44:23 visual_prompt]: 	Training 200/553. train loss: 77.1709,	1.0120 s / batch. (data: 1.93e-01). ETA=11:45:30, max mem: 20.9 GB 
[11/23 06:46:04 visual_prompt]: 	Training 300/553. train loss: 43.7938,	0.8359 s / batch. (data: 4.71e-04). ETA=9:41:20, max mem: 20.9 GB 
[11/23 06:47:46 visual_prompt]: 	Training 400/553. train loss: 102.3129,	1.3868 s / batch. (data: 5.60e-01). ETA=16:02:09, max mem: 20.9 GB 
[11/23 06:49:27 visual_prompt]: 	Training 500/553. train loss: 86.9998,	1.6286 s / batch. (data: 7.95e-01). ETA=18:47:11, max mem: 20.9 GB 
[11/23 06:50:19 visual_prompt]: Epoch 25 / 100: avg data time: 1.98e-01, avg batch time: 1.0161, average train loss: 77.0688
[11/23 06:51:17 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3062, average loss: 23.5282
[11/23 06:51:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 67.86	
[11/23 06:51:17 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[11/23 06:53:02 visual_prompt]: 	Training 100/553. train loss: 27.9279,	0.8241 s / batch. (data: 3.18e-04). ETA=9:28:19, max mem: 20.9 GB 
[11/23 06:54:45 visual_prompt]: 	Training 200/553. train loss: 114.5106,	1.7098 s / batch. (data: 9.06e-01). ETA=19:36:10, max mem: 20.9 GB 
[11/23 06:56:27 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8201 s / batch. (data: 3.40e-04). ETA=9:22:47, max mem: 20.9 GB 
[11/23 06:58:07 visual_prompt]: 	Training 400/553. train loss: 5.7127,	0.8108 s / batch. (data: 3.02e-04). ETA=9:15:02, max mem: 20.9 GB 
[11/23 06:59:46 visual_prompt]: 	Training 500/553. train loss: 82.9827,	0.8229 s / batch. (data: 4.11e-03). ETA=9:22:00, max mem: 20.9 GB 
[11/23 07:00:39 visual_prompt]: Epoch 26 / 100: avg data time: 1.98e-01, avg batch time: 1.0147, average train loss: 83.6908
[11/23 07:01:37 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3059, average loss: 96.7490
[11/23 07:01:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.30	
[11/23 07:01:37 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[11/23 07:03:23 visual_prompt]: 	Training 100/553. train loss: 25.1682,	0.8004 s / batch. (data: 3.21e-04). ETA=9:04:33, max mem: 20.9 GB 
[11/23 07:05:03 visual_prompt]: 	Training 200/553. train loss: 37.1963,	1.2832 s / batch. (data: 4.67e-01). ETA=14:30:55, max mem: 20.9 GB 
[11/23 07:06:44 visual_prompt]: 	Training 300/553. train loss: 144.5471,	0.8280 s / batch. (data: 7.95e-03). ETA=9:20:34, max mem: 20.9 GB 
[11/23 07:08:26 visual_prompt]: 	Training 400/553. train loss: 49.3821,	0.8445 s / batch. (data: 7.90e-04). ETA=9:30:21, max mem: 20.9 GB 
[11/23 07:10:08 visual_prompt]: 	Training 500/553. train loss: 55.3535,	0.8423 s / batch. (data: 1.10e-02). ETA=9:27:29, max mem: 20.9 GB 
[11/23 07:10:58 visual_prompt]: Epoch 27 / 100: avg data time: 1.98e-01, avg batch time: 1.0152, average train loss: 103.2925
[11/23 07:11:56 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.3055, average loss: 34.6598
[11/23 07:11:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 69.30	
[11/23 07:11:56 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[11/23 07:13:40 visual_prompt]: 	Training 100/553. train loss: 13.8401,	0.8372 s / batch. (data: 9.12e-03). ETA=9:21:51, max mem: 20.9 GB 
[11/23 07:15:22 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0187 s / batch. (data: 2.08e-01). ETA=11:21:59, max mem: 20.9 GB 
[11/23 07:17:04 visual_prompt]: 	Training 300/553. train loss: 211.2960,	1.5640 s / batch. (data: 7.39e-01). ETA=17:24:28, max mem: 20.9 GB 
[11/23 07:18:44 visual_prompt]: 	Training 400/553. train loss: 51.1757,	0.8223 s / batch. (data: 3.04e-04). ETA=9:07:46, max mem: 20.9 GB 
[11/23 07:20:24 visual_prompt]: 	Training 500/553. train loss: 0.0495,	0.8220 s / batch. (data: 7.94e-03). ETA=9:06:12, max mem: 20.9 GB 
[11/23 07:21:17 visual_prompt]: Epoch 28 / 100: avg data time: 1.96e-01, avg batch time: 1.0142, average train loss: 84.6978
[11/23 07:22:15 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3064, average loss: 36.3148
[11/23 07:22:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 68.10	
[11/23 07:22:15 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[11/23 07:24:06 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8163 s / batch. (data: 5.42e-03). ETA=9:00:20, max mem: 20.9 GB 
[11/23 07:25:47 visual_prompt]: 	Training 200/553. train loss: 3.2430,	1.9242 s / batch. (data: 1.12e+00). ETA=21:10:28, max mem: 20.9 GB 
[11/23 07:27:26 visual_prompt]: 	Training 300/553. train loss: 189.5812,	0.8261 s / batch. (data: 5.62e-03). ETA=9:04:03, max mem: 20.9 GB 
[11/23 07:29:03 visual_prompt]: 	Training 400/553. train loss: 149.5882,	1.0970 s / batch. (data: 2.89e-01). ETA=12:00:40, max mem: 20.9 GB 
[11/23 07:30:45 visual_prompt]: 	Training 500/553. train loss: 33.2032,	0.8136 s / batch. (data: 3.01e-04). ETA=8:53:05, max mem: 20.9 GB 
[11/23 07:31:37 visual_prompt]: Epoch 29 / 100: avg data time: 1.97e-01, avg batch time: 1.0160, average train loss: 90.5373
[11/23 07:32:35 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3065, average loss: 19.8268
[11/23 07:32:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 67.25	
[11/23 07:32:35 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[11/23 07:34:19 visual_prompt]: 	Training 100/553. train loss: 47.6176,	0.8269 s / batch. (data: 3.33e-04). ETA=8:59:42, max mem: 20.9 GB 
[11/23 07:36:01 visual_prompt]: 	Training 200/553. train loss: 65.9663,	0.8406 s / batch. (data: 1.05e-02). ETA=9:07:16, max mem: 20.9 GB 
[11/23 07:37:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8608 s / batch. (data: 5.90e-02). ETA=9:18:58, max mem: 20.9 GB 
[11/23 07:39:23 visual_prompt]: 	Training 400/553. train loss: 375.7125,	1.0480 s / batch. (data: 2.35e-01). ETA=11:18:48, max mem: 20.9 GB 
[11/23 07:41:04 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.6729 s / batch. (data: 8.76e-01). ETA=18:00:48, max mem: 20.9 GB 
[11/23 07:41:58 visual_prompt]: Epoch 30 / 100: avg data time: 1.98e-01, avg batch time: 1.0168, average train loss: 86.8718
[11/23 07:42:56 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.3063, average loss: 26.1372
[11/23 07:42:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 68.86	
[11/23 07:42:56 visual_prompt]: Stopping early.
[11/23 07:42:56 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 07:42:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 07:42:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/23 07:42:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 07:42:56 visual_prompt]: Training with config:
[11/23 07:42:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/23 07:42:56 visual_prompt]: Loading training data...
[11/23 07:42:56 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 07:42:56 visual_prompt]: Loading validation data...
[11/23 07:42:56 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 07:42:56 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 07:42:58 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 07:42:58 visual_prompt]: tuned percent:0.525
[11/23 07:42:58 visual_prompt]: Device used for model: 0
[11/23 07:42:58 visual_prompt]: Setting up Evaluator...
[11/23 07:42:58 visual_prompt]: Setting up Trainer...
[11/23 07:42:58 visual_prompt]: 	Setting up the optimizer...
[11/23 07:42:58 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 07:44:43 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8331 s / batch. (data: 3.09e-04). ETA=12:46:27, max mem: 20.9 GB 
[11/23 07:46:23 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8338 s / batch. (data: 1.05e-02). ETA=12:45:43, max mem: 20.9 GB 
[11/23 07:48:07 visual_prompt]: 	Training 300/553. train loss: 1.3905,	2.0400 s / batch. (data: 1.19e+00). ETA=1 day, 7:09:59, max mem: 20.9 GB 
[11/23 07:49:46 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8225 s / batch. (data: 3.14e-04). ETA=12:32:36, max mem: 20.9 GB 
[11/23 07:51:30 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8267 s / batch. (data: 7.92e-04). ETA=12:35:01, max mem: 20.9 GB 
[11/23 07:52:23 visual_prompt]: Epoch 1 / 100: avg data time: 1.96e-01, avg batch time: 1.0205, average train loss: 1.5403
[11/23 07:53:21 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3081, average loss: 1.5201
[11/23 07:53:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 07:53:21 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/23 07:55:06 visual_prompt]: 	Training 100/553. train loss: 12.5197,	0.8326 s / batch. (data: 9.62e-03). ETA=12:38:17, max mem: 20.9 GB 
[11/23 07:56:46 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8097 s / batch. (data: 3.25e-04). ETA=12:16:07, max mem: 20.9 GB 
[11/23 07:58:29 visual_prompt]: 	Training 300/553. train loss: 11.5898,	1.0633 s / batch. (data: 2.47e-01). ETA=16:04:55, max mem: 20.9 GB 
[11/23 08:00:10 visual_prompt]: 	Training 400/553. train loss: 19.1761,	0.8298 s / batch. (data: 1.20e-02). ETA=12:31:37, max mem: 20.9 GB 
[11/23 08:01:52 visual_prompt]: 	Training 500/553. train loss: 3.7540,	0.8366 s / batch. (data: 6.49e-03). ETA=12:36:22, max mem: 20.9 GB 
[11/23 08:02:44 visual_prompt]: Epoch 2 / 100: avg data time: 1.93e-01, avg batch time: 1.0172, average train loss: 8.2145
[11/23 08:03:42 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.3050, average loss: 5.2982
[11/23 08:03:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.54	
[11/23 08:03:42 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/23 08:05:26 visual_prompt]: 	Training 100/553. train loss: 43.9622,	0.8302 s / batch. (data: 5.45e-03). ETA=12:28:28, max mem: 20.9 GB 
[11/23 08:07:09 visual_prompt]: 	Training 200/553. train loss: 15.4211,	0.8258 s / batch. (data: 1.05e-02). ETA=12:23:05, max mem: 20.9 GB 
[11/23 08:08:48 visual_prompt]: 	Training 300/553. train loss: 16.5867,	0.8342 s / batch. (data: 5.43e-03). ETA=12:29:21, max mem: 20.9 GB 
[11/23 08:10:31 visual_prompt]: 	Training 400/553. train loss: 14.2569,	0.8382 s / batch. (data: 5.43e-03). ETA=12:31:28, max mem: 20.9 GB 
[11/23 08:12:13 visual_prompt]: 	Training 500/553. train loss: 12.0256,	1.3314 s / batch. (data: 5.02e-01). ETA=19:51:28, max mem: 20.9 GB 
[11/23 08:13:04 visual_prompt]: Epoch 3 / 100: avg data time: 1.93e-01, avg batch time: 1.0164, average train loss: 19.3897
[11/23 08:14:02 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3064, average loss: 39.4502
[11/23 08:14:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.94	
[11/23 08:14:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/23 08:15:48 visual_prompt]: 	Training 100/553. train loss: 53.0640,	0.8120 s / batch. (data: 3.28e-04). ETA=12:04:33, max mem: 20.9 GB 
[11/23 08:17:30 visual_prompt]: 	Training 200/553. train loss: 50.8620,	0.8440 s / batch. (data: 3.31e-04). ETA=12:31:42, max mem: 20.9 GB 
[11/23 08:19:12 visual_prompt]: 	Training 300/553. train loss: 58.1711,	1.5840 s / batch. (data: 7.62e-01). ETA=23:28:13, max mem: 20.9 GB 
[11/23 08:20:48 visual_prompt]: 	Training 400/553. train loss: 24.9885,	1.2230 s / batch. (data: 3.85e-01). ETA=18:05:15, max mem: 20.9 GB 
[11/23 08:22:31 visual_prompt]: 	Training 500/553. train loss: 0.0108,	3.2216 s / batch. (data: 2.42e+00). ETA=1 day, 23:33:18, max mem: 20.9 GB 
[11/23 08:23:26 visual_prompt]: Epoch 4 / 100: avg data time: 1.97e-01, avg batch time: 1.0187, average train loss: 24.8907
[11/23 08:24:24 visual_prompt]: Inference (val):avg data time: 2.05e-04, avg batch time: 0.3084, average loss: 20.4354
[11/23 08:24:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.85	
[11/23 08:24:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/23 08:26:07 visual_prompt]: 	Training 100/553. train loss: 32.3181,	0.8160 s / batch. (data: 3.07e-04). ETA=12:00:38, max mem: 20.9 GB 
[11/23 08:27:49 visual_prompt]: 	Training 200/553. train loss: 23.3870,	1.2031 s / batch. (data: 4.05e-01). ETA=17:40:31, max mem: 20.9 GB 
[11/23 08:29:31 visual_prompt]: 	Training 300/553. train loss: 34.9429,	0.7979 s / batch. (data: 3.26e-04). ETA=11:41:59, max mem: 20.9 GB 
[11/23 08:31:11 visual_prompt]: 	Training 400/553. train loss: 5.3576,	0.8170 s / batch. (data: 3.00e-04). ETA=11:57:24, max mem: 20.9 GB 
[11/23 08:32:52 visual_prompt]: 	Training 500/553. train loss: 38.4655,	0.8182 s / batch. (data: 3.06e-04). ETA=11:57:05, max mem: 20.9 GB 
[11/23 08:33:45 visual_prompt]: Epoch 5 / 100: avg data time: 1.95e-01, avg batch time: 1.0154, average train loss: 35.4545
[11/23 08:34:44 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3070, average loss: 52.2161
[11/23 08:34:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.09	
[11/23 08:34:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/23 08:36:30 visual_prompt]: 	Training 100/553. train loss: 22.3957,	0.8108 s / batch. (data: 2.99e-04). ETA=11:48:32, max mem: 20.9 GB 
[11/23 08:38:11 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8181 s / batch. (data: 3.21e-04). ETA=11:53:33, max mem: 20.9 GB 
[11/23 08:39:51 visual_prompt]: 	Training 300/553. train loss: 100.9606,	0.8188 s / batch. (data: 3.21e-04). ETA=11:52:51, max mem: 20.9 GB 
[11/23 08:41:36 visual_prompt]: 	Training 400/553. train loss: 36.3876,	0.8432 s / batch. (data: 2.32e-02). ETA=12:12:42, max mem: 20.9 GB 
[11/23 08:43:15 visual_prompt]: 	Training 500/553. train loss: 7.5757,	0.8224 s / batch. (data: 3.36e-04). ETA=11:53:14, max mem: 20.9 GB 
[11/23 08:44:07 visual_prompt]: Epoch 6 / 100: avg data time: 1.98e-01, avg batch time: 1.0193, average train loss: 46.4969
[11/23 08:45:05 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3063, average loss: 2.9002
[11/23 08:45:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 42.29	
[11/23 08:45:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/23 08:46:49 visual_prompt]: 	Training 100/553. train loss: 79.7656,	0.8080 s / batch. (data: 2.91e-04). ETA=11:38:41, max mem: 20.9 GB 
[11/23 08:48:30 visual_prompt]: 	Training 200/553. train loss: 23.5082,	0.8177 s / batch. (data: 3.00e-04). ETA=11:45:40, max mem: 20.9 GB 
[11/23 08:50:15 visual_prompt]: 	Training 300/553. train loss: 89.3683,	1.7681 s / batch. (data: 9.53e-01). ETA=1 day, 1:23:00, max mem: 20.9 GB 
[11/23 08:51:56 visual_prompt]: 	Training 400/553. train loss: 9.0612,	2.0906 s / batch. (data: 1.28e+00). ETA=1 day, 5:57:19, max mem: 20.9 GB 
[11/23 08:53:35 visual_prompt]: 	Training 500/553. train loss: 2.2699,	0.8240 s / batch. (data: 3.33e-04). ETA=11:47:00, max mem: 20.9 GB 
[11/23 08:54:27 visual_prompt]: Epoch 7 / 100: avg data time: 1.95e-01, avg batch time: 1.0151, average train loss: 53.5550
[11/23 08:55:25 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3062, average loss: 85.8453
[11/23 08:55:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.41	
[11/23 08:55:25 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/23 08:57:08 visual_prompt]: 	Training 100/553. train loss: 54.0753,	0.8480 s / batch. (data: 3.07e-04). ETA=12:05:28, max mem: 20.9 GB 
[11/23 08:58:51 visual_prompt]: 	Training 200/553. train loss: 8.1587,	0.8480 s / batch. (data: 1.19e-02). ETA=12:04:00, max mem: 20.9 GB 
[11/23 09:00:32 visual_prompt]: 	Training 300/553. train loss: 10.8678,	0.8531 s / batch. (data: 3.03e-04). ETA=12:07:00, max mem: 20.9 GB 
[11/23 09:02:13 visual_prompt]: 	Training 400/553. train loss: 60.3140,	0.8120 s / batch. (data: 1.18e-02). ETA=11:30:36, max mem: 20.9 GB 
[11/23 09:03:55 visual_prompt]: 	Training 500/553. train loss: 200.6376,	1.5917 s / batch. (data: 7.85e-01). ETA=22:31:04, max mem: 20.9 GB 
[11/23 09:04:48 visual_prompt]: Epoch 8 / 100: avg data time: 1.99e-01, avg batch time: 1.0183, average train loss: 58.2043
[11/23 09:05:46 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3059, average loss: 165.8387
[11/23 09:05:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.33	
[11/23 09:05:46 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/23 09:07:31 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8101 s / batch. (data: 3.23e-04). ETA=11:25:35, max mem: 20.9 GB 
[11/23 09:09:11 visual_prompt]: 	Training 200/553. train loss: 10.1231,	0.8193 s / batch. (data: 2.93e-04). ETA=11:31:58, max mem: 20.9 GB 
[11/23 09:10:53 visual_prompt]: 	Training 300/553. train loss: 65.8969,	1.7720 s / batch. (data: 9.58e-01). ETA=1 day, 0:53:40, max mem: 20.9 GB 
[11/23 09:12:34 visual_prompt]: 	Training 400/553. train loss: 66.5996,	0.8305 s / batch. (data: 1.19e-02). ETA=11:38:38, max mem: 20.9 GB 
[11/23 09:14:16 visual_prompt]: 	Training 500/553. train loss: 21.8308,	0.9879 s / batch. (data: 1.91e-01). ETA=13:49:24, max mem: 20.9 GB 
[11/23 09:15:08 visual_prompt]: Epoch 9 / 100: avg data time: 1.97e-01, avg batch time: 1.0155, average train loss: 71.2444
[11/23 09:16:06 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3071, average loss: 110.4002
[11/23 09:16:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.95	
[11/23 09:16:06 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/23 09:17:53 visual_prompt]: 	Training 100/553. train loss: 133.3066,	0.8150 s / batch. (data: 7.95e-03). ETA=11:22:11, max mem: 20.9 GB 
[11/23 09:19:33 visual_prompt]: 	Training 200/553. train loss: 84.7284,	0.8460 s / batch. (data: 5.42e-03). ETA=11:46:43, max mem: 20.9 GB 
[11/23 09:21:13 visual_prompt]: 	Training 300/553. train loss: 576.9364,	0.8200 s / batch. (data: 3.21e-04). ETA=11:23:38, max mem: 20.9 GB 
[11/23 09:22:52 visual_prompt]: 	Training 400/553. train loss: 51.9027,	0.8280 s / batch. (data: 2.95e-04). ETA=11:28:56, max mem: 20.9 GB 
[11/23 09:24:34 visual_prompt]: 	Training 500/553. train loss: 23.9710,	0.8229 s / batch. (data: 3.16e-04). ETA=11:23:18, max mem: 20.9 GB 
[11/23 09:25:27 visual_prompt]: Epoch 10 / 100: avg data time: 1.96e-01, avg batch time: 1.0150, average train loss: 87.3272
[11/23 09:26:25 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3084, average loss: 112.9292
[11/23 09:26:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.07	
[11/23 09:26:25 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/23 09:28:13 visual_prompt]: 	Training 100/553. train loss: 114.9638,	0.8485 s / batch. (data: 2.45e-02). ETA=11:42:27, max mem: 20.9 GB 
[11/23 09:29:56 visual_prompt]: 	Training 200/553. train loss: 88.0898,	0.8270 s / batch. (data: 2.98e-04). ETA=11:23:12, max mem: 20.9 GB 
[11/23 09:31:37 visual_prompt]: 	Training 300/553. train loss: 14.5853,	2.1391 s / batch. (data: 1.32e+00). ETA=1 day, 5:23:38, max mem: 20.9 GB 
[11/23 09:33:16 visual_prompt]: 	Training 400/553. train loss: 141.8358,	0.8096 s / batch. (data: 3.00e-04). ETA=11:06:12, max mem: 20.9 GB 
[11/23 09:34:55 visual_prompt]: 	Training 500/553. train loss: 142.3403,	0.8264 s / batch. (data: 1.14e-02). ETA=11:18:36, max mem: 20.9 GB 
[11/23 09:35:47 visual_prompt]: Epoch 11 / 100: avg data time: 1.97e-01, avg batch time: 1.0162, average train loss: 88.3990
[11/23 09:36:45 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3067, average loss: 112.8124
[11/23 09:36:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.28	
[11/23 09:36:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/23 09:38:32 visual_prompt]: 	Training 100/553. train loss: 19.0796,	0.8530 s / batch. (data: 4.47e-02). ETA=11:38:15, max mem: 20.9 GB 
[11/23 09:40:15 visual_prompt]: 	Training 200/553. train loss: 83.0412,	1.7608 s / batch. (data: 9.46e-01). ETA=23:58:29, max mem: 20.9 GB 
[11/23 09:41:54 visual_prompt]: 	Training 300/553. train loss: 119.7655,	0.8379 s / batch. (data: 2.99e-04). ETA=11:23:09, max mem: 20.9 GB 
[11/23 09:43:35 visual_prompt]: 	Training 400/553. train loss: 22.7943,	0.8453 s / batch. (data: 1.05e-02). ETA=11:27:42, max mem: 20.9 GB 
[11/23 09:45:17 visual_prompt]: 	Training 500/553. train loss: 38.2310,	0.8039 s / batch. (data: 4.75e-04). ETA=10:52:45, max mem: 20.9 GB 
[11/23 09:46:08 visual_prompt]: Epoch 12 / 100: avg data time: 1.99e-01, avg batch time: 1.0181, average train loss: 85.2541
[11/23 09:47:06 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3064, average loss: 152.8936
[11/23 09:47:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.11	
[11/23 09:47:06 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/23 09:48:53 visual_prompt]: 	Training 100/553. train loss: 23.9103,	0.8182 s / batch. (data: 5.57e-03). ETA=11:02:17, max mem: 20.9 GB 
[11/23 09:50:31 visual_prompt]: 	Training 200/553. train loss: 49.2319,	0.8295 s / batch. (data: 2.79e-04). ETA=11:09:59, max mem: 20.9 GB 
[11/23 09:52:13 visual_prompt]: 	Training 300/553. train loss: 28.9799,	1.9256 s / batch. (data: 1.11e+00). ETA=1 day, 1:52:07, max mem: 20.9 GB 
[11/23 09:53:53 visual_prompt]: 	Training 400/553. train loss: 96.5915,	0.8240 s / batch. (data: 3.08e-04). ETA=11:02:48, max mem: 20.9 GB 
[11/23 09:55:35 visual_prompt]: 	Training 500/553. train loss: 92.0966,	0.8347 s / batch. (data: 7.95e-03). ETA=11:10:01, max mem: 20.9 GB 
[11/23 09:56:27 visual_prompt]: Epoch 13 / 100: avg data time: 1.95e-01, avg batch time: 1.0142, average train loss: 92.7903
[11/23 09:57:26 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3062, average loss: 21.4387
[11/23 09:57:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.53	
[11/23 09:57:26 visual_prompt]: Best epoch 13: best metric: -21.439
[11/23 09:57:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/23 09:59:11 visual_prompt]: 	Training 100/553. train loss: 13.0888,	0.8234 s / batch. (data: 1.05e-02). ETA=10:58:53, max mem: 20.9 GB 
[11/23 10:00:52 visual_prompt]: 	Training 200/553. train loss: 48.8425,	1.3589 s / batch. (data: 5.47e-01). ETA=18:05:05, max mem: 20.9 GB 
[11/23 10:02:34 visual_prompt]: 	Training 300/553. train loss: 38.0073,	0.8253 s / batch. (data: 3.19e-04). ETA=10:57:38, max mem: 20.9 GB 
[11/23 10:04:15 visual_prompt]: 	Training 400/553. train loss: 27.0583,	0.8117 s / batch. (data: 2.83e-04). ETA=10:45:26, max mem: 20.9 GB 
[11/23 10:05:55 visual_prompt]: 	Training 500/553. train loss: 136.6361,	0.8307 s / batch. (data: 5.47e-03). ETA=10:59:10, max mem: 20.9 GB 
[11/23 10:06:48 visual_prompt]: Epoch 14 / 100: avg data time: 1.98e-01, avg batch time: 1.0165, average train loss: 85.7605
[11/23 10:07:46 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3057, average loss: 75.6512
[11/23 10:07:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.40	
[11/23 10:07:46 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/23 10:09:31 visual_prompt]: 	Training 100/553. train loss: 33.4414,	1.3405 s / batch. (data: 5.35e-01). ETA=17:40:17, max mem: 20.9 GB 
[11/23 10:11:10 visual_prompt]: 	Training 200/553. train loss: 190.2530,	0.8320 s / batch. (data: 7.95e-03). ETA=10:56:42, max mem: 20.9 GB 
[11/23 10:12:54 visual_prompt]: 	Training 300/553. train loss: 39.7126,	0.8480 s / batch. (data: 1.11e-02). ETA=11:07:53, max mem: 20.9 GB 
[11/23 10:14:33 visual_prompt]: 	Training 400/553. train loss: 36.3495,	1.2960 s / batch. (data: 4.94e-01). ETA=16:58:36, max mem: 20.9 GB 
[11/23 10:16:15 visual_prompt]: 	Training 500/553. train loss: 174.3483,	0.8050 s / batch. (data: 3.13e-04). ETA=10:31:20, max mem: 20.9 GB 
[11/23 10:17:08 visual_prompt]: Epoch 15 / 100: avg data time: 1.96e-01, avg batch time: 1.0160, average train loss: 81.3469
[11/23 10:18:06 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3058, average loss: 42.3722
[11/23 10:18:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.24	
[11/23 10:18:06 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/23 10:19:50 visual_prompt]: 	Training 100/553. train loss: 59.0136,	0.8400 s / batch. (data: 3.11e-04). ETA=10:56:38, max mem: 20.9 GB 
[11/23 10:21:32 visual_prompt]: 	Training 200/553. train loss: 124.4367,	0.8241 s / batch. (data: 3.04e-04). ETA=10:42:51, max mem: 20.9 GB 
[11/23 10:23:13 visual_prompt]: 	Training 300/553. train loss: 10.7524,	0.8200 s / batch. (data: 2.46e-04). ETA=10:38:16, max mem: 20.9 GB 
[11/23 10:24:53 visual_prompt]: 	Training 400/553. train loss: 123.1823,	0.8440 s / batch. (data: 8.87e-04). ETA=10:55:33, max mem: 20.9 GB 
[11/23 10:26:34 visual_prompt]: 	Training 500/553. train loss: 230.5610,	1.4560 s / batch. (data: 6.53e-01). ETA=18:48:32, max mem: 20.9 GB 
[11/23 10:27:27 visual_prompt]: Epoch 16 / 100: avg data time: 1.96e-01, avg batch time: 1.0145, average train loss: 92.4808
[11/23 10:28:25 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3083, average loss: 6.6274
[11/23 10:28:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.14	
[11/23 10:28:25 visual_prompt]: Best epoch 16: best metric: -6.627
[11/23 10:28:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/23 10:30:09 visual_prompt]: 	Training 100/553. train loss: 2.6529,	0.8461 s / batch. (data: 1.56e-02). ETA=10:53:40, max mem: 20.9 GB 
[11/23 10:31:51 visual_prompt]: 	Training 200/553. train loss: 152.5670,	0.8044 s / batch. (data: 2.44e-04). ETA=10:20:04, max mem: 20.9 GB 
[11/23 10:33:32 visual_prompt]: 	Training 300/553. train loss: 253.1240,	0.8194 s / batch. (data: 3.21e-04). ETA=10:30:17, max mem: 20.9 GB 
[11/23 10:35:13 visual_prompt]: 	Training 400/553. train loss: 36.0315,	1.2295 s / batch. (data: 4.07e-01). ETA=15:43:39, max mem: 20.9 GB 
[11/23 10:36:54 visual_prompt]: 	Training 500/553. train loss: 14.2437,	1.7133 s / batch. (data: 8.87e-01). ETA=21:52:09, max mem: 20.9 GB 
[11/23 10:37:48 visual_prompt]: Epoch 17 / 100: avg data time: 1.99e-01, avg batch time: 1.0188, average train loss: 97.3880
[11/23 10:38:46 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3085, average loss: 34.5270
[11/23 10:38:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 37.29	
[11/23 10:38:46 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/23 10:40:32 visual_prompt]: 	Training 100/553. train loss: 21.5394,	0.8201 s / batch. (data: 2.81e-04). ETA=10:25:58, max mem: 20.9 GB 
[11/23 10:42:15 visual_prompt]: 	Training 200/553. train loss: 102.2191,	0.8429 s / batch. (data: 7.93e-04). ETA=10:41:59, max mem: 20.9 GB 
[11/23 10:43:57 visual_prompt]: 	Training 300/553. train loss: 108.1322,	0.8061 s / batch. (data: 5.42e-03). ETA=10:12:38, max mem: 20.9 GB 
[11/23 10:45:37 visual_prompt]: 	Training 400/553. train loss: 52.3275,	0.8235 s / batch. (data: 2.84e-04). ETA=10:24:28, max mem: 20.9 GB 
[11/23 10:47:18 visual_prompt]: 	Training 500/553. train loss: 47.7289,	1.7747 s / batch. (data: 9.57e-01). ETA=22:22:48, max mem: 20.9 GB 
[11/23 10:48:10 visual_prompt]: Epoch 18 / 100: avg data time: 2.01e-01, avg batch time: 1.0182, average train loss: 86.3528
[11/23 10:49:07 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3066, average loss: 2.3279
[11/23 10:49:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 40.65	rocauc: 37.02	
[11/23 10:49:07 visual_prompt]: Best epoch 18: best metric: -2.328
[11/23 10:49:07 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/23 10:50:53 visual_prompt]: 	Training 100/553. train loss: 69.7756,	1.1382 s / batch. (data: 3.21e-01). ETA=14:18:17, max mem: 20.9 GB 
[11/23 10:52:34 visual_prompt]: 	Training 200/553. train loss: 22.1144,	0.8116 s / batch. (data: 3.16e-04). ETA=10:10:40, max mem: 20.9 GB 
[11/23 10:54:15 visual_prompt]: 	Training 300/553. train loss: 408.8005,	0.8231 s / batch. (data: 3.12e-04). ETA=10:17:56, max mem: 20.9 GB 
[11/23 10:55:58 visual_prompt]: 	Training 400/553. train loss: 72.7405,	0.8170 s / batch. (data: 7.67e-04). ETA=10:12:00, max mem: 20.9 GB 
[11/23 10:57:34 visual_prompt]: 	Training 500/553. train loss: 109.6554,	0.8193 s / batch. (data: 2.78e-04). ETA=10:12:23, max mem: 20.9 GB 
[11/23 10:58:27 visual_prompt]: Epoch 19 / 100: avg data time: 1.93e-01, avg batch time: 1.0126, average train loss: 89.7050
[11/23 10:59:25 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3069, average loss: 100.0859
[11/23 10:59:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.88	
[11/23 10:59:25 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/23 11:01:09 visual_prompt]: 	Training 100/553. train loss: 79.4471,	0.8319 s / batch. (data: 2.87e-04). ETA=10:19:38, max mem: 20.9 GB 
[11/23 11:02:51 visual_prompt]: 	Training 200/553. train loss: 157.0300,	0.8144 s / batch. (data: 2.99e-04). ETA=10:05:16, max mem: 20.9 GB 
[11/23 11:04:32 visual_prompt]: 	Training 300/553. train loss: 8.1106,	0.8240 s / batch. (data: 8.02e-04). ETA=10:11:01, max mem: 20.9 GB 
[11/23 11:06:13 visual_prompt]: 	Training 400/553. train loss: 2.5973,	0.8507 s / batch. (data: 5.43e-03). ETA=10:29:24, max mem: 20.9 GB 
[11/23 11:07:53 visual_prompt]: 	Training 500/553. train loss: 169.9998,	0.8049 s / batch. (data: 3.02e-04). ETA=9:54:11, max mem: 20.9 GB 
[11/23 11:08:48 visual_prompt]: Epoch 20 / 100: avg data time: 1.97e-01, avg batch time: 1.0165, average train loss: 91.2562
[11/23 11:09:45 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.3069, average loss: 90.9191
[11/23 11:09:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.28	
[11/23 11:09:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/23 11:11:33 visual_prompt]: 	Training 100/553. train loss: 4.2993,	0.8320 s / batch. (data: 2.99e-04). ETA=10:12:04, max mem: 20.9 GB 
[11/23 11:13:13 visual_prompt]: 	Training 200/553. train loss: 36.2410,	0.8280 s / batch. (data: 3.16e-04). ETA=10:07:44, max mem: 20.9 GB 
[11/23 11:14:53 visual_prompt]: 	Training 300/553. train loss: 135.9170,	1.1178 s / batch. (data: 3.14e-01). ETA=13:38:34, max mem: 20.9 GB 
[11/23 11:16:34 visual_prompt]: 	Training 400/553. train loss: 9.0030,	0.8285 s / batch. (data: 7.94e-03). ETA=10:05:23, max mem: 20.9 GB 
[11/23 11:18:16 visual_prompt]: 	Training 500/553. train loss: 113.5523,	0.8181 s / batch. (data: 5.43e-03). ETA=9:56:23, max mem: 20.9 GB 
[11/23 11:19:08 visual_prompt]: Epoch 21 / 100: avg data time: 1.97e-01, avg batch time: 1.0163, average train loss: 95.3262
[11/23 11:20:06 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3072, average loss: 4.7616
[11/23 11:20:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 52.42	
[11/23 11:20:06 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/23 11:21:50 visual_prompt]: 	Training 100/553. train loss: 50.1048,	0.8160 s / batch. (data: 3.05e-04). ETA=9:52:46, max mem: 20.9 GB 
[11/23 11:23:31 visual_prompt]: 	Training 200/553. train loss: 62.6233,	0.8085 s / batch. (data: 5.42e-03). ETA=9:45:58, max mem: 20.9 GB 
[11/23 11:25:12 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8348 s / batch. (data: 2.87e-04). ETA=10:03:41, max mem: 20.9 GB 
[11/23 11:26:56 visual_prompt]: 	Training 400/553. train loss: 54.8575,	0.8213 s / batch. (data: 3.05e-04). ETA=9:52:32, max mem: 20.9 GB 
[11/23 11:28:37 visual_prompt]: 	Training 500/553. train loss: 48.8533,	0.8029 s / batch. (data: 2.99e-04). ETA=9:37:56, max mem: 20.9 GB 
[11/23 11:29:31 visual_prompt]: Epoch 22 / 100: avg data time: 2.03e-01, avg batch time: 1.0220, average train loss: 75.0675
[11/23 11:30:29 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3080, average loss: 47.6330
[11/23 11:30:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/23 11:30:29 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/23 11:32:15 visual_prompt]: 	Training 100/553. train loss: 15.8041,	0.8270 s / batch. (data: 7.94e-03). ETA=9:53:08, max mem: 20.9 GB 
[11/23 11:33:57 visual_prompt]: 	Training 200/553. train loss: 60.4298,	0.9414 s / batch. (data: 1.27e-01). ETA=11:13:38, max mem: 20.9 GB 
[11/23 11:35:40 visual_prompt]: 	Training 300/553. train loss: 9.5525,	0.8434 s / batch. (data: 1.10e-02). ETA=10:02:08, max mem: 20.9 GB 
[11/23 11:37:18 visual_prompt]: 	Training 400/553. train loss: 76.4443,	0.8305 s / batch. (data: 7.95e-04). ETA=9:51:32, max mem: 20.9 GB 
[11/23 11:38:58 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8065 s / batch. (data: 3.12e-04). ETA=9:33:05, max mem: 20.9 GB 
[11/23 11:39:50 visual_prompt]: Epoch 23 / 100: avg data time: 1.96e-01, avg batch time: 1.0154, average train loss: 81.9495
[11/23 11:40:49 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3051, average loss: 119.5339
[11/23 11:40:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.64	
[11/23 11:40:49 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/23 11:42:31 visual_prompt]: 	Training 100/553. train loss: 16.3050,	0.8234 s / batch. (data: 1.32e-02). ETA=9:43:00, max mem: 20.9 GB 
[11/23 11:44:11 visual_prompt]: 	Training 200/553. train loss: 61.7671,	0.8137 s / batch. (data: 3.01e-04). ETA=9:34:45, max mem: 20.9 GB 
[11/23 11:45:53 visual_prompt]: 	Training 300/553. train loss: 61.0474,	1.0760 s / batch. (data: 2.55e-01). ETA=12:38:13, max mem: 20.9 GB 
[11/23 11:47:35 visual_prompt]: 	Training 400/553. train loss: 97.8063,	0.8047 s / batch. (data: 2.62e-04). ETA=9:25:41, max mem: 20.9 GB 
[11/23 11:49:18 visual_prompt]: 	Training 500/553. train loss: 311.2027,	0.8229 s / batch. (data: 3.04e-04). ETA=9:37:10, max mem: 20.9 GB 
[11/23 11:50:11 visual_prompt]: Epoch 24 / 100: avg data time: 1.97e-01, avg batch time: 1.0159, average train loss: 82.1351
[11/23 11:51:09 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3079, average loss: 112.9347
[11/23 11:51:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.77	
[11/23 11:51:09 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/23 11:52:57 visual_prompt]: 	Training 100/553. train loss: 107.3279,	0.8145 s / batch. (data: 5.40e-03). ETA=9:29:11, max mem: 20.9 GB 
[11/23 11:54:34 visual_prompt]: 	Training 200/553. train loss: 86.5670,	0.8479 s / batch. (data: 8.96e-03). ETA=9:51:05, max mem: 20.9 GB 
[11/23 11:56:16 visual_prompt]: 	Training 300/553. train loss: 123.3767,	0.8328 s / batch. (data: 1.19e-02). ETA=9:39:10, max mem: 20.9 GB 
[11/23 11:57:56 visual_prompt]: 	Training 400/553. train loss: 207.6805,	1.3944 s / batch. (data: 5.76e-01). ETA=16:07:24, max mem: 20.9 GB 
[11/23 11:59:38 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.6150 s / batch. (data: 8.14e-01). ETA=18:37:46, max mem: 20.9 GB 
[11/23 12:00:30 visual_prompt]: Epoch 25 / 100: avg data time: 1.97e-01, avg batch time: 1.0157, average train loss: 88.6766
[11/23 12:01:28 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3088, average loss: 65.0240
[11/23 12:01:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.04	
[11/23 12:01:28 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/23 12:03:14 visual_prompt]: 	Training 100/553. train loss: 30.0034,	0.8160 s / batch. (data: 3.10e-04). ETA=9:22:41, max mem: 20.9 GB 
[11/23 12:04:57 visual_prompt]: 	Training 200/553. train loss: 67.1940,	1.8869 s / batch. (data: 1.07e+00). ETA=21:38:01, max mem: 20.9 GB 
[11/23 12:06:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8268 s / batch. (data: 2.93e-04). ETA=9:27:22, max mem: 20.9 GB 
[11/23 12:08:22 visual_prompt]: 	Training 400/553. train loss: 92.3177,	0.8204 s / batch. (data: 3.52e-04). ETA=9:21:37, max mem: 20.9 GB 
[11/23 12:10:01 visual_prompt]: 	Training 500/553. train loss: 38.7327,	0.8390 s / batch. (data: 2.56e-02). ETA=9:32:57, max mem: 20.9 GB 
[11/23 12:10:54 visual_prompt]: Epoch 26 / 100: avg data time: 2.03e-01, avg batch time: 1.0216, average train loss: 69.7019
[11/23 12:11:52 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3084, average loss: 14.4951
[11/23 12:11:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.32	
[11/23 12:11:52 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/23 12:13:38 visual_prompt]: 	Training 100/553. train loss: 219.0955,	0.8189 s / batch. (data: 3.27e-04). ETA=9:17:11, max mem: 20.9 GB 
[11/23 12:15:19 visual_prompt]: 	Training 200/553. train loss: 145.9541,	1.4538 s / batch. (data: 6.28e-01). ETA=16:26:40, max mem: 20.9 GB 
[11/23 12:17:00 visual_prompt]: 	Training 300/553. train loss: 24.4080,	0.8127 s / batch. (data: 5.46e-03). ETA=9:10:12, max mem: 20.9 GB 
[11/23 12:18:41 visual_prompt]: 	Training 400/553. train loss: 44.4365,	0.8135 s / batch. (data: 5.43e-03). ETA=9:09:24, max mem: 20.9 GB 
[11/23 12:20:24 visual_prompt]: 	Training 500/553. train loss: 161.3302,	0.8444 s / batch. (data: 1.56e-02). ETA=9:28:52, max mem: 20.9 GB 
[11/23 12:21:15 visual_prompt]: Epoch 27 / 100: avg data time: 1.99e-01, avg batch time: 1.0188, average train loss: 74.8556
[11/23 12:22:14 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3075, average loss: 34.8650
[11/23 12:22:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.19	
[11/23 12:22:14 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/23 12:23:59 visual_prompt]: 	Training 100/553. train loss: 86.5541,	0.8119 s / batch. (data: 3.17e-04). ETA=9:04:55, max mem: 20.9 GB 
[11/23 12:25:42 visual_prompt]: 	Training 200/553. train loss: 22.8274,	0.8399 s / batch. (data: 1.05e-02). ETA=9:22:17, max mem: 20.9 GB 
[11/23 12:27:24 visual_prompt]: 	Training 300/553. train loss: 59.2656,	1.6608 s / batch. (data: 8.33e-01). ETA=18:29:08, max mem: 20.9 GB 
[11/23 12:29:05 visual_prompt]: 	Training 400/553. train loss: 150.4455,	0.8160 s / batch. (data: 3.37e-04). ETA=9:03:33, max mem: 20.9 GB 
[11/23 12:30:44 visual_prompt]: 	Training 500/553. train loss: 147.9897,	0.8240 s / batch. (data: 7.94e-03). ETA=9:07:31, max mem: 20.9 GB 
[11/23 12:31:37 visual_prompt]: Epoch 28 / 100: avg data time: 1.97e-01, avg batch time: 1.0164, average train loss: 69.9625
[11/23 12:32:34 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3056, average loss: 70.9363
[11/23 12:32:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.18	
[11/23 12:32:34 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/23 12:34:25 visual_prompt]: 	Training 100/553. train loss: 76.6778,	0.8317 s / batch. (data: 2.78e-04). ETA=9:10:31, max mem: 20.9 GB 
[11/23 12:36:04 visual_prompt]: 	Training 200/553. train loss: 29.0995,	1.8440 s / batch. (data: 1.02e+00). ETA=20:17:32, max mem: 20.9 GB 
[11/23 12:37:42 visual_prompt]: 	Training 300/553. train loss: 55.0209,	0.8440 s / batch. (data: 7.18e-04). ETA=9:15:52, max mem: 20.9 GB 
[11/23 12:39:19 visual_prompt]: 	Training 400/553. train loss: 17.1176,	0.8247 s / batch. (data: 3.17e-04). ETA=9:01:48, max mem: 20.9 GB 
[11/23 12:41:00 visual_prompt]: 	Training 500/553. train loss: 68.7080,	0.7984 s / batch. (data: 2.63e-04). ETA=8:43:07, max mem: 20.9 GB 
[11/23 12:41:52 visual_prompt]: Epoch 29 / 100: avg data time: 1.89e-01, avg batch time: 1.0091, average train loss: 77.5595
[11/23 12:42:50 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3048, average loss: 139.9693
[11/23 12:42:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.88	
[11/23 12:42:50 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/23 12:44:33 visual_prompt]: 	Training 100/553. train loss: 95.7120,	0.8509 s / batch. (data: 1.15e-02). ETA=9:15:23, max mem: 20.9 GB 
[11/23 12:46:14 visual_prompt]: 	Training 200/553. train loss: 103.8336,	0.8250 s / batch. (data: 2.93e-04). ETA=8:57:08, max mem: 20.9 GB 
[11/23 12:47:53 visual_prompt]: 	Training 300/553. train loss: 66.8459,	1.6443 s / batch. (data: 8.30e-01). ETA=17:47:45, max mem: 20.9 GB 
[11/23 12:49:35 visual_prompt]: 	Training 400/553. train loss: 17.2390,	1.1669 s / batch. (data: 3.60e-01). ETA=12:35:49, max mem: 20.9 GB 
[11/23 12:51:14 visual_prompt]: 	Training 500/553. train loss: 25.4649,	1.4896 s / batch. (data: 6.81e-01). ETA=16:02:21, max mem: 20.9 GB 
[11/23 12:52:08 visual_prompt]: Epoch 30 / 100: avg data time: 1.89e-01, avg batch time: 1.0085, average train loss: 77.3183
[11/23 12:53:05 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3064, average loss: 59.4566
[11/23 12:53:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.43	
[11/23 12:53:05 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/23 12:54:51 visual_prompt]: 	Training 100/553. train loss: 11.5690,	0.8359 s / batch. (data: 3.20e-04). ETA=8:57:55, max mem: 20.9 GB 
[11/23 12:56:33 visual_prompt]: 	Training 200/553. train loss: 33.0901,	0.8160 s / batch. (data: 3.25e-04). ETA=8:43:43, max mem: 20.9 GB 
[11/23 12:58:10 visual_prompt]: 	Training 300/553. train loss: 22.6195,	0.8214 s / batch. (data: 3.09e-04). ETA=8:45:48, max mem: 20.9 GB 
[11/23 12:59:49 visual_prompt]: 	Training 400/553. train loss: 42.0631,	0.8100 s / batch. (data: 3.00e-04). ETA=8:37:10, max mem: 20.9 GB 
[11/23 13:01:29 visual_prompt]: 	Training 500/553. train loss: 14.5550,	0.8160 s / batch. (data: 2.95e-04). ETA=8:39:38, max mem: 20.9 GB 
[11/23 13:02:21 visual_prompt]: Epoch 31 / 100: avg data time: 1.85e-01, avg batch time: 1.0047, average train loss: 73.1653
[11/23 13:03:18 visual_prompt]: Inference (val):avg data time: 3.20e-04, avg batch time: 0.3047, average loss: 39.5140
[11/23 13:03:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.60	
[11/23 13:03:18 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/23 13:05:03 visual_prompt]: 	Training 100/553. train loss: 94.3087,	0.8317 s / batch. (data: 5.88e-03). ETA=8:47:33, max mem: 20.9 GB 
[11/23 13:06:43 visual_prompt]: 	Training 200/553. train loss: 99.7104,	0.8321 s / batch. (data: 2.73e-04). ETA=8:46:24, max mem: 20.9 GB 
[11/23 13:08:26 visual_prompt]: 	Training 300/553. train loss: 35.2079,	0.8344 s / batch. (data: 1.19e-02). ETA=8:46:26, max mem: 20.9 GB 
[11/23 13:10:07 visual_prompt]: 	Training 400/553. train loss: 178.5225,	0.8222 s / batch. (data: 3.01e-04). ETA=8:37:23, max mem: 20.9 GB 
[11/23 13:11:44 visual_prompt]: 	Training 500/553. train loss: 31.5324,	0.8280 s / batch. (data: 2.81e-04). ETA=8:39:40, max mem: 20.9 GB 
[11/23 13:12:34 visual_prompt]: Epoch 32 / 100: avg data time: 1.85e-01, avg batch time: 1.0051, average train loss: 80.8004
[11/23 13:13:32 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3076, average loss: 143.6310
[11/23 13:13:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.18	
[11/23 13:13:32 visual_prompt]: Stopping early.
[11/23 13:13:32 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 13:13:32 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 13:13:32 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/23 13:13:32 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 13:13:32 visual_prompt]: Training with config:
[11/23 13:13:32 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/23 13:13:32 visual_prompt]: Loading training data...
[11/23 13:13:32 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 13:13:32 visual_prompt]: Loading validation data...
[11/23 13:13:32 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 13:13:32 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 13:13:35 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 13:13:35 visual_prompt]: tuned percent:0.525
[11/23 13:13:35 visual_prompt]: Device used for model: 0
[11/23 13:13:35 visual_prompt]: Setting up Evaluator...
[11/23 13:13:35 visual_prompt]: Setting up Trainer...
[11/23 13:13:35 visual_prompt]: 	Setting up the optimizer...
[11/23 13:13:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 13:15:18 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8320 s / batch. (data: 1.05e-02). ETA=12:45:25, max mem: 20.9 GB 
[11/23 13:16:57 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8080 s / batch. (data: 2.97e-04). ETA=12:22:01, max mem: 20.9 GB 
[11/23 13:18:39 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.6480 s / batch. (data: 8.24e-01). ETA=1 day, 1:10:39, max mem: 20.9 GB 
[11/23 13:20:17 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8219 s / batch. (data: 5.42e-03). ETA=12:32:01, max mem: 20.9 GB 
[11/23 13:22:00 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8170 s / batch. (data: 7.62e-04). ETA=12:26:12, max mem: 20.9 GB 
[11/23 13:22:53 visual_prompt]: Epoch 1 / 100: avg data time: 1.87e-01, avg batch time: 1.0088, average train loss: 1.5403
[11/23 13:23:50 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3070, average loss: 1.5201
[11/23 13:23:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 13:23:50 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/23 13:25:34 visual_prompt]: 	Training 100/553. train loss: 11.4707,	0.8160 s / batch. (data: 3.23e-04). ETA=12:23:12, max mem: 20.9 GB 
[11/23 13:27:14 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8217 s / batch. (data: 3.16e-04). ETA=12:26:58, max mem: 20.9 GB 
[11/23 13:28:56 visual_prompt]: 	Training 300/553. train loss: 5.9929,	1.0410 s / batch. (data: 2.28e-01). ETA=15:44:38, max mem: 20.9 GB 
[11/23 13:30:35 visual_prompt]: 	Training 400/553. train loss: 7.5676,	0.8280 s / batch. (data: 2.99e-04). ETA=12:29:59, max mem: 20.9 GB 
[11/23 13:32:17 visual_prompt]: 	Training 500/553. train loss: 1.9307,	0.8280 s / batch. (data: 9.96e-04). ETA=12:28:37, max mem: 20.9 GB 
[11/23 13:33:08 visual_prompt]: Epoch 2 / 100: avg data time: 1.86e-01, avg batch time: 1.0084, average train loss: 9.8064
[11/23 13:34:05 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3075, average loss: 21.4495
[11/23 13:34:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.40	
[11/23 13:34:05 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/23 13:35:48 visual_prompt]: 	Training 100/553. train loss: 15.6780,	0.8306 s / batch. (data: 3.16e-04). ETA=12:28:52, max mem: 20.9 GB 
[11/23 13:37:30 visual_prompt]: 	Training 200/553. train loss: 6.2633,	0.8253 s / batch. (data: 2.86e-04). ETA=12:22:43, max mem: 20.9 GB 
[11/23 13:39:09 visual_prompt]: 	Training 300/553. train loss: 8.7254,	0.8440 s / batch. (data: 2.84e-04). ETA=12:38:06, max mem: 20.9 GB 
[11/23 13:40:50 visual_prompt]: 	Training 400/553. train loss: 15.1111,	0.8514 s / batch. (data: 1.05e-02). ETA=12:43:22, max mem: 20.9 GB 
[11/23 13:42:31 visual_prompt]: 	Training 500/553. train loss: 71.0609,	1.2386 s / batch. (data: 4.25e-01). ETA=18:28:26, max mem: 20.9 GB 
[11/23 13:43:22 visual_prompt]: Epoch 3 / 100: avg data time: 1.84e-01, avg batch time: 1.0074, average train loss: 14.2301
[11/23 13:44:20 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3075, average loss: 21.8537
[11/23 13:44:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.46	
[11/23 13:44:20 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/23 13:46:06 visual_prompt]: 	Training 100/553. train loss: 9.9626,	0.8400 s / batch. (data: 2.84e-04). ETA=12:29:34, max mem: 20.9 GB 
[11/23 13:47:47 visual_prompt]: 	Training 200/553. train loss: 59.3786,	0.8160 s / batch. (data: 7.96e-03). ETA=12:06:47, max mem: 20.9 GB 
[11/23 13:49:28 visual_prompt]: 	Training 300/553. train loss: 4.1254,	1.5068 s / batch. (data: 6.98e-01). ETA=22:19:32, max mem: 20.9 GB 
[11/23 13:51:04 visual_prompt]: 	Training 400/553. train loss: 6.7726,	1.4480 s / batch. (data: 6.33e-01). ETA=21:24:52, max mem: 20.9 GB 
[11/23 13:52:46 visual_prompt]: 	Training 500/553. train loss: 49.7587,	3.1600 s / batch. (data: 2.33e+00). ETA=1 day, 22:38:43, max mem: 20.9 GB 
[11/23 13:53:40 visual_prompt]: Epoch 4 / 100: avg data time: 1.90e-01, avg batch time: 1.0133, average train loss: 26.8838
[11/23 13:54:37 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3079, average loss: 25.4921
[11/23 13:54:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/23 13:54:37 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/23 13:56:20 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8080 s / batch. (data: 2.87e-04). ETA=11:53:35, max mem: 20.9 GB 
[11/23 13:58:01 visual_prompt]: 	Training 200/553. train loss: 14.0414,	1.2567 s / batch. (data: 4.13e-01). ETA=18:27:46, max mem: 20.9 GB 
[11/23 13:59:42 visual_prompt]: 	Training 300/553. train loss: 54.3054,	0.8200 s / batch. (data: 2.97e-04). ETA=12:01:25, max mem: 20.9 GB 
[11/23 14:01:21 visual_prompt]: 	Training 400/553. train loss: 31.3902,	0.8400 s / batch. (data: 3.00e-04). ETA=12:17:38, max mem: 20.9 GB 
[11/23 14:03:02 visual_prompt]: 	Training 500/553. train loss: 138.4451,	0.8053 s / batch. (data: 7.95e-03). ETA=11:45:51, max mem: 20.9 GB 
[11/23 14:03:55 visual_prompt]: Epoch 5 / 100: avg data time: 1.87e-01, avg batch time: 1.0084, average train loss: 29.4678
[11/23 14:04:53 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3063, average loss: 44.2434
[11/23 14:04:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.93	
[11/23 14:04:53 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/23 14:06:39 visual_prompt]: 	Training 100/553. train loss: 152.0571,	0.8240 s / batch. (data: 8.22e-04). ETA=12:00:04, max mem: 20.9 GB 
[11/23 14:08:18 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8320 s / batch. (data: 5.44e-03). ETA=12:05:41, max mem: 20.9 GB 
[11/23 14:09:57 visual_prompt]: 	Training 300/553. train loss: 257.1109,	0.8311 s / batch. (data: 5.47e-03). ETA=12:03:35, max mem: 20.9 GB 
[11/23 14:11:41 visual_prompt]: 	Training 400/553. train loss: 63.9711,	0.7996 s / batch. (data: 4.63e-04). ETA=11:34:48, max mem: 20.9 GB 
[11/23 14:13:19 visual_prompt]: 	Training 500/553. train loss: 22.8794,	0.8240 s / batch. (data: 3.26e-04). ETA=11:54:36, max mem: 20.9 GB 
[11/23 14:14:11 visual_prompt]: Epoch 6 / 100: avg data time: 1.90e-01, avg batch time: 1.0102, average train loss: 43.9060
[11/23 14:15:09 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3068, average loss: 43.3593
[11/23 14:15:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.47	
[11/23 14:15:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/23 14:16:52 visual_prompt]: 	Training 100/553. train loss: 202.3694,	0.8520 s / batch. (data: 1.60e-02). ETA=12:16:43, max mem: 20.9 GB 
[11/23 14:18:31 visual_prompt]: 	Training 200/553. train loss: 34.1242,	0.8320 s / batch. (data: 1.20e-02). ETA=11:58:03, max mem: 20.9 GB 
[11/23 14:20:16 visual_prompt]: 	Training 300/553. train loss: 39.2158,	2.0171 s / batch. (data: 1.19e+00). ETA=1 day, 4:57:25, max mem: 20.9 GB 
[11/23 14:21:56 visual_prompt]: 	Training 400/553. train loss: 74.3282,	1.9291 s / batch. (data: 1.12e+00). ETA=1 day, 3:38:26, max mem: 20.9 GB 
[11/23 14:23:34 visual_prompt]: 	Training 500/553. train loss: 79.8908,	0.8369 s / batch. (data: 2.41e-03). ETA=11:58:04, max mem: 20.9 GB 
[11/23 14:24:25 visual_prompt]: Epoch 7 / 100: avg data time: 1.86e-01, avg batch time: 1.0061, average train loss: 55.4997
[11/23 14:25:23 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3064, average loss: 23.6999
[11/23 14:25:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.52	
[11/23 14:25:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/23 14:27:05 visual_prompt]: 	Training 100/553. train loss: 2.0181,	0.8261 s / batch. (data: 3.23e-04). ETA=11:46:40, max mem: 20.9 GB 
[11/23 14:28:47 visual_prompt]: 	Training 200/553. train loss: 61.1637,	0.8248 s / batch. (data: 2.82e-03). ETA=11:44:13, max mem: 20.9 GB 
[11/23 14:30:27 visual_prompt]: 	Training 300/553. train loss: 145.5445,	0.8115 s / batch. (data: 3.00e-04). ETA=11:31:33, max mem: 20.9 GB 
[11/23 14:32:07 visual_prompt]: 	Training 400/553. train loss: 41.8939,	0.8092 s / batch. (data: 3.02e-04). ETA=11:28:13, max mem: 20.9 GB 
[11/23 14:33:48 visual_prompt]: 	Training 500/553. train loss: 21.5642,	1.5600 s / batch. (data: 7.34e-01). ETA=22:04:08, max mem: 20.9 GB 
[11/23 14:34:40 visual_prompt]: Epoch 8 / 100: avg data time: 1.88e-01, avg batch time: 1.0084, average train loss: 56.5643
[11/23 14:35:38 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3079, average loss: 57.6910
[11/23 14:35:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.04	
[11/23 14:35:38 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/23 14:37:22 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.21e-04). ETA=11:47:28, max mem: 20.9 GB 
[11/23 14:39:01 visual_prompt]: 	Training 200/553. train loss: 785.9756,	0.8200 s / batch. (data: 2.98e-04). ETA=11:32:35, max mem: 20.9 GB 
[11/23 14:40:42 visual_prompt]: 	Training 300/553. train loss: 56.0445,	1.8611 s / batch. (data: 1.03e+00). ETA=1 day, 2:08:46, max mem: 20.9 GB 
[11/23 14:42:23 visual_prompt]: 	Training 400/553. train loss: 135.3336,	0.8062 s / batch. (data: 2.89e-04). ETA=11:18:15, max mem: 20.9 GB 
[11/23 14:44:04 visual_prompt]: 	Training 500/553. train loss: 26.5560,	0.9437 s / batch. (data: 1.38e-01). ETA=13:12:20, max mem: 20.9 GB 
[11/23 14:44:54 visual_prompt]: Epoch 9 / 100: avg data time: 1.87e-01, avg batch time: 1.0066, average train loss: 68.2584
[11/23 14:45:52 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3065, average loss: 19.6656
[11/23 14:45:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.07	
[11/23 14:45:52 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/23 14:47:39 visual_prompt]: 	Training 100/553. train loss: 115.1190,	0.8060 s / batch. (data: 2.99e-04). ETA=11:14:38, max mem: 20.9 GB 
[11/23 14:49:18 visual_prompt]: 	Training 200/553. train loss: 64.3213,	0.8426 s / batch. (data: 3.00e-04). ETA=11:43:52, max mem: 20.9 GB 
[11/23 14:50:58 visual_prompt]: 	Training 300/553. train loss: 6.0597,	1.2173 s / batch. (data: 3.87e-01). ETA=16:54:53, max mem: 20.9 GB 
[11/23 14:52:35 visual_prompt]: 	Training 400/553. train loss: 90.0477,	0.8366 s / batch. (data: 1.05e-02). ETA=11:36:05, max mem: 20.9 GB 
[11/23 14:54:17 visual_prompt]: 	Training 500/553. train loss: 8.8133,	0.8560 s / batch. (data: 3.52e-04). ETA=11:50:48, max mem: 20.9 GB 
[11/23 14:55:10 visual_prompt]: Epoch 10 / 100: avg data time: 1.90e-01, avg batch time: 1.0090, average train loss: 76.3979
[11/23 14:56:07 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3067, average loss: 44.8438
[11/23 14:56:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[11/23 14:56:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/23 14:57:53 visual_prompt]: 	Training 100/553. train loss: 196.5261,	0.8160 s / batch. (data: 2.86e-04). ETA=11:15:30, max mem: 20.9 GB 
[11/23 14:59:35 visual_prompt]: 	Training 200/553. train loss: 278.2780,	0.8520 s / batch. (data: 7.97e-03). ETA=11:43:52, max mem: 20.9 GB 
[11/23 15:01:14 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1160 s / batch. (data: 1.29e+00). ETA=1 day, 5:04:38, max mem: 20.9 GB 
[11/23 15:02:53 visual_prompt]: 	Training 400/553. train loss: 36.4821,	0.8199 s / batch. (data: 7.52e-04). ETA=11:14:37, max mem: 20.9 GB 
[11/23 15:04:32 visual_prompt]: 	Training 500/553. train loss: 162.8521,	0.8145 s / batch. (data: 3.46e-04). ETA=11:08:49, max mem: 20.9 GB 
[11/23 15:05:23 visual_prompt]: Epoch 11 / 100: avg data time: 1.86e-01, avg batch time: 1.0052, average train loss: 83.0005
[11/23 15:06:20 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3074, average loss: 18.7106
[11/23 15:06:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.66	
[11/23 15:06:21 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/23 15:08:06 visual_prompt]: 	Training 100/553. train loss: 126.5586,	0.8560 s / batch. (data: 3.50e-02). ETA=11:40:43, max mem: 20.9 GB 
[11/23 15:09:47 visual_prompt]: 	Training 200/553. train loss: 152.5565,	0.8033 s / batch. (data: 3.18e-04). ETA=10:56:16, max mem: 20.9 GB 
[11/23 15:11:26 visual_prompt]: 	Training 300/553. train loss: 21.7700,	0.8385 s / batch. (data: 6.50e-03). ETA=11:23:37, max mem: 20.9 GB 
[11/23 15:13:06 visual_prompt]: 	Training 400/553. train loss: 93.6778,	0.8025 s / batch. (data: 3.10e-04). ETA=10:52:55, max mem: 20.9 GB 
[11/23 15:14:47 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.7996 s / batch. (data: 4.99e-04). ETA=10:49:16, max mem: 20.9 GB 
[11/23 15:15:38 visual_prompt]: Epoch 12 / 100: avg data time: 1.89e-01, avg batch time: 1.0077, average train loss: 86.6268
[11/23 15:16:35 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3065, average loss: 228.3232
[11/23 15:16:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.58	
[11/23 15:16:35 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/23 15:18:21 visual_prompt]: 	Training 100/553. train loss: 23.4021,	0.8080 s / batch. (data: 3.05e-04). ETA=10:53:58, max mem: 20.9 GB 
[11/23 15:19:57 visual_prompt]: 	Training 200/553. train loss: 55.1706,	0.8280 s / batch. (data: 3.00e-04). ETA=11:08:48, max mem: 20.9 GB 
[11/23 15:21:38 visual_prompt]: 	Training 300/553. train loss: 16.4816,	1.7708 s / batch. (data: 9.48e-01). ETA=23:47:22, max mem: 20.9 GB 
[11/23 15:23:17 visual_prompt]: 	Training 400/553. train loss: 7.3962,	0.8377 s / batch. (data: 1.33e-02). ETA=11:13:51, max mem: 20.9 GB 
[11/23 15:24:59 visual_prompt]: 	Training 500/553. train loss: 32.3706,	0.8160 s / batch. (data: 4.48e-03). ETA=10:55:02, max mem: 20.9 GB 
[11/23 15:25:50 visual_prompt]: Epoch 13 / 100: avg data time: 1.85e-01, avg batch time: 1.0040, average train loss: 89.5001
[11/23 15:26:48 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3056, average loss: 41.0821
[11/23 15:26:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.43	
[11/23 15:26:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/23 15:28:33 visual_prompt]: 	Training 100/553. train loss: 79.9684,	0.8313 s / batch. (data: 7.26e-03). ETA=11:05:11, max mem: 20.9 GB 
[11/23 15:30:13 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.2360 s / batch. (data: 4.09e-01). ETA=16:26:58, max mem: 20.9 GB 
[11/23 15:31:53 visual_prompt]: 	Training 300/553. train loss: 81.4320,	0.8040 s / batch. (data: 3.04e-04). ETA=10:40:41, max mem: 20.9 GB 
[11/23 15:33:33 visual_prompt]: 	Training 400/553. train loss: 23.2426,	0.8320 s / batch. (data: 1.19e-02). ETA=11:01:35, max mem: 20.9 GB 
[11/23 15:35:13 visual_prompt]: 	Training 500/553. train loss: 128.8445,	0.8049 s / batch. (data: 2.75e-04). ETA=10:38:42, max mem: 20.9 GB 
[11/23 15:36:04 visual_prompt]: Epoch 14 / 100: avg data time: 1.88e-01, avg batch time: 1.0057, average train loss: 94.1733
[11/23 15:37:01 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3062, average loss: 72.9262
[11/23 15:37:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.32	
[11/23 15:37:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/23 15:38:45 visual_prompt]: 	Training 100/553. train loss: 107.7245,	1.0118 s / batch. (data: 2.00e-01). ETA=13:20:16, max mem: 20.9 GB 
[11/23 15:40:24 visual_prompt]: 	Training 200/553. train loss: 338.1583,	0.8301 s / batch. (data: 1.59e-02). ETA=10:55:11, max mem: 20.9 GB 
[11/23 15:42:06 visual_prompt]: 	Training 300/553. train loss: 10.7235,	0.8069 s / batch. (data: 2.94e-04). ETA=10:35:30, max mem: 20.9 GB 
[11/23 15:43:43 visual_prompt]: 	Training 400/553. train loss: 204.6113,	0.9286 s / batch. (data: 1.16e-01). ETA=12:09:48, max mem: 20.9 GB 
[11/23 15:45:24 visual_prompt]: 	Training 500/553. train loss: 120.6582,	0.8240 s / batch. (data: 5.40e-03). ETA=10:46:16, max mem: 20.9 GB 
[11/23 15:46:18 visual_prompt]: Epoch 15 / 100: avg data time: 1.86e-01, avg batch time: 1.0057, average train loss: 93.8931
[11/23 15:47:15 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3060, average loss: 5.1922
[11/23 15:47:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.96	
[11/23 15:47:15 visual_prompt]: Best epoch 15: best metric: -5.192
[11/23 15:47:15 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/23 15:48:58 visual_prompt]: 	Training 100/553. train loss: 40.4202,	0.8072 s / batch. (data: 2.59e-04). ETA=10:31:02, max mem: 20.9 GB 
[11/23 15:50:38 visual_prompt]: 	Training 200/553. train loss: 140.7945,	0.8050 s / batch. (data: 2.92e-04). ETA=10:27:59, max mem: 20.9 GB 
[11/23 15:52:19 visual_prompt]: 	Training 300/553. train loss: 13.6437,	0.8221 s / batch. (data: 2.99e-04). ETA=10:39:56, max mem: 20.9 GB 
[11/23 15:53:59 visual_prompt]: 	Training 400/553. train loss: 64.7440,	0.8286 s / batch. (data: 2.02e-02). ETA=10:43:36, max mem: 20.9 GB 
[11/23 15:55:39 visual_prompt]: 	Training 500/553. train loss: 203.5595,	0.8062 s / batch. (data: 3.24e-04). ETA=10:24:53, max mem: 20.9 GB 
[11/23 15:56:32 visual_prompt]: Epoch 16 / 100: avg data time: 1.88e-01, avg batch time: 1.0071, average train loss: 78.6311
[11/23 15:57:29 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3076, average loss: 4.1551
[11/23 15:57:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.29	
[11/23 15:57:29 visual_prompt]: Best epoch 16: best metric: -4.155
[11/23 15:57:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/23 15:59:12 visual_prompt]: 	Training 100/553. train loss: 3.5588,	0.8346 s / batch. (data: 2.27e-02). ETA=10:44:46, max mem: 20.9 GB 
[11/23 16:00:54 visual_prompt]: 	Training 200/553. train loss: 182.7799,	0.8080 s / batch. (data: 3.62e-04). ETA=10:22:52, max mem: 20.9 GB 
[11/23 16:02:33 visual_prompt]: 	Training 300/553. train loss: 111.7145,	0.8260 s / batch. (data: 4.32e-04). ETA=10:35:21, max mem: 20.9 GB 
[11/23 16:04:13 visual_prompt]: 	Training 400/553. train loss: 320.7078,	1.1556 s / batch. (data: 3.41e-01). ETA=14:46:58, max mem: 20.9 GB 
[11/23 16:05:53 visual_prompt]: 	Training 500/553. train loss: 124.6979,	1.6720 s / batch. (data: 8.13e-01). ETA=21:20:31, max mem: 20.9 GB 
[11/23 16:06:46 visual_prompt]: Epoch 17 / 100: avg data time: 1.87e-01, avg batch time: 1.0060, average train loss: 80.3591
[11/23 16:07:43 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3056, average loss: 219.9071
[11/23 16:07:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.28	
[11/23 16:07:43 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/23 16:09:28 visual_prompt]: 	Training 100/553. train loss: 65.2004,	0.8241 s / batch. (data: 3.02e-04). ETA=10:29:01, max mem: 20.9 GB 
[11/23 16:11:10 visual_prompt]: 	Training 200/553. train loss: 30.3554,	0.8368 s / batch. (data: 3.07e-03). ETA=10:37:23, max mem: 20.9 GB 
[11/23 16:12:50 visual_prompt]: 	Training 300/553. train loss: 118.4125,	0.8440 s / batch. (data: 1.59e-02). ETA=10:41:25, max mem: 20.9 GB 
[11/23 16:14:30 visual_prompt]: 	Training 400/553. train loss: 23.7052,	0.8280 s / batch. (data: 3.02e-04). ETA=10:27:53, max mem: 20.9 GB 
[11/23 16:16:09 visual_prompt]: 	Training 500/553. train loss: 84.4155,	0.8260 s / batch. (data: 2.94e-04). ETA=10:24:58, max mem: 20.9 GB 
[11/23 16:17:01 visual_prompt]: Epoch 18 / 100: avg data time: 1.88e-01, avg batch time: 1.0077, average train loss: 81.1541
[11/23 16:17:58 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3060, average loss: 55.5910
[11/23 16:17:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.31	
[11/23 16:17:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/23 16:19:42 visual_prompt]: 	Training 100/553. train loss: 33.5354,	1.1343 s / batch. (data: 3.37e-01). ETA=14:15:23, max mem: 20.9 GB 
[11/23 16:21:22 visual_prompt]: 	Training 200/553. train loss: 48.6493,	0.8440 s / batch. (data: 7.96e-03). ETA=10:35:03, max mem: 20.9 GB 
[11/23 16:23:03 visual_prompt]: 	Training 300/553. train loss: 153.9044,	0.8340 s / batch. (data: 2.21e-02). ETA=10:26:07, max mem: 20.9 GB 
[11/23 16:24:44 visual_prompt]: 	Training 400/553. train loss: 50.0509,	0.8387 s / batch. (data: 7.95e-04). ETA=10:28:15, max mem: 20.9 GB 
[11/23 16:26:21 visual_prompt]: 	Training 500/553. train loss: 2.3170,	0.8176 s / batch. (data: 2.90e-04). ETA=10:11:06, max mem: 20.9 GB 
[11/23 16:27:13 visual_prompt]: Epoch 19 / 100: avg data time: 1.83e-01, avg batch time: 1.0039, average train loss: 72.0063
[11/23 16:28:10 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3069, average loss: 17.9899
[11/23 16:28:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.09	
[11/23 16:28:10 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/23 16:29:53 visual_prompt]: 	Training 100/553. train loss: 5.2882,	0.8200 s / batch. (data: 5.43e-03). ETA=10:10:46, max mem: 20.9 GB 
[11/23 16:31:35 visual_prompt]: 	Training 200/553. train loss: 18.6954,	0.8290 s / batch. (data: 3.14e-04). ETA=10:16:06, max mem: 20.9 GB 
[11/23 16:33:15 visual_prompt]: 	Training 300/553. train loss: 22.6528,	0.8172 s / batch. (data: 5.41e-03). ETA=10:06:00, max mem: 20.9 GB 
[11/23 16:34:54 visual_prompt]: 	Training 400/553. train loss: 22.2894,	0.8099 s / batch. (data: 3.12e-04). ETA=9:59:14, max mem: 20.9 GB 
[11/23 16:36:34 visual_prompt]: 	Training 500/553. train loss: 41.9790,	0.8160 s / batch. (data: 2.68e-04). ETA=10:02:25, max mem: 20.9 GB 
[11/23 16:37:28 visual_prompt]: Epoch 20 / 100: avg data time: 1.89e-01, avg batch time: 1.0076, average train loss: 79.3548
[11/23 16:38:25 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3056, average loss: 51.9980
[11/23 16:38:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.62	
[11/23 16:38:25 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/23 16:40:11 visual_prompt]: 	Training 100/553. train loss: 77.1954,	1.1092 s / batch. (data: 2.91e-01). ETA=13:36:00, max mem: 20.9 GB 
[11/23 16:41:50 visual_prompt]: 	Training 200/553. train loss: 90.5409,	0.8158 s / batch. (data: 3.01e-04). ETA=9:58:48, max mem: 20.9 GB 
[11/23 16:43:30 visual_prompt]: 	Training 300/553. train loss: 120.1923,	1.1307 s / batch. (data: 3.15e-01). ETA=13:48:04, max mem: 20.9 GB 
[11/23 16:45:09 visual_prompt]: 	Training 400/553. train loss: 23.0740,	0.8335 s / batch. (data: 3.25e-04). ETA=10:08:59, max mem: 20.9 GB 
[11/23 16:46:50 visual_prompt]: 	Training 500/553. train loss: 62.0444,	0.8051 s / batch. (data: 3.01e-04). ETA=9:46:56, max mem: 20.9 GB 
[11/23 16:47:41 visual_prompt]: Epoch 21 / 100: avg data time: 1.87e-01, avg batch time: 1.0061, average train loss: 80.6375
[11/23 16:48:39 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3059, average loss: 5.7173
[11/23 16:48:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.95	
[11/23 16:48:39 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/23 16:50:22 visual_prompt]: 	Training 100/553. train loss: 45.5380,	0.8168 s / batch. (data: 1.19e-02). ETA=9:53:21, max mem: 20.9 GB 
[11/23 16:52:02 visual_prompt]: 	Training 200/553. train loss: 14.3998,	0.8440 s / batch. (data: 2.75e-04). ETA=10:11:41, max mem: 20.9 GB 
[11/23 16:53:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8240 s / batch. (data: 3.21e-04). ETA=9:55:51, max mem: 20.9 GB 
[11/23 16:55:21 visual_prompt]: 	Training 400/553. train loss: 41.4337,	0.8136 s / batch. (data: 6.70e-03). ETA=9:46:59, max mem: 20.9 GB 
[11/23 16:57:01 visual_prompt]: 	Training 500/553. train loss: 41.4893,	0.8452 s / batch. (data: 9.14e-03). ETA=10:08:22, max mem: 20.9 GB 
[11/23 16:57:56 visual_prompt]: Epoch 22 / 100: avg data time: 1.88e-01, avg batch time: 1.0066, average train loss: 82.7818
[11/23 16:58:53 visual_prompt]: Inference (val):avg data time: 2.44e-04, avg batch time: 0.3077, average loss: 30.7563
[11/23 16:58:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[11/23 16:58:53 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/23 17:00:41 visual_prompt]: 	Training 100/553. train loss: 123.4828,	0.8440 s / batch. (data: 1.19e-02). ETA=10:05:20, max mem: 20.9 GB 
[11/23 17:02:23 visual_prompt]: 	Training 200/553. train loss: 15.2011,	0.8400 s / batch. (data: 2.61e-04). ETA=10:01:05, max mem: 20.9 GB 
[11/23 17:04:06 visual_prompt]: 	Training 300/553. train loss: 234.7354,	0.8280 s / batch. (data: 4.21e-04). ETA=9:51:04, max mem: 20.9 GB 
[11/23 17:05:45 visual_prompt]: 	Training 400/553. train loss: 102.2313,	0.8361 s / batch. (data: 5.43e-03). ETA=9:55:28, max mem: 20.9 GB 
[11/23 17:07:23 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8176 s / batch. (data: 2.99e-04). ETA=9:40:59, max mem: 20.9 GB 
[11/23 17:08:16 visual_prompt]: Epoch 23 / 100: avg data time: 1.98e-01, avg batch time: 1.0173, average train loss: 75.8306
[11/23 17:09:13 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3059, average loss: 146.7328
[11/23 17:09:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.00	
[11/23 17:09:13 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/23 17:10:55 visual_prompt]: 	Training 100/553. train loss: 80.0707,	0.8124 s / batch. (data: 4.00e-04). ETA=9:35:11, max mem: 20.9 GB 
[11/23 17:12:34 visual_prompt]: 	Training 200/553. train loss: 151.4542,	0.7980 s / batch. (data: 2.80e-04). ETA=9:23:41, max mem: 20.9 GB 
[11/23 17:14:17 visual_prompt]: 	Training 300/553. train loss: 10.0722,	1.0280 s / batch. (data: 2.21e-01). ETA=12:04:24, max mem: 20.9 GB 
[11/23 17:15:58 visual_prompt]: 	Training 400/553. train loss: 18.1664,	0.8275 s / batch. (data: 3.50e-04). ETA=9:41:44, max mem: 20.9 GB 
[11/23 17:17:40 visual_prompt]: 	Training 500/553. train loss: 34.1217,	0.8303 s / batch. (data: 3.25e-04). ETA=9:42:17, max mem: 20.9 GB 
[11/23 17:18:33 visual_prompt]: Epoch 24 / 100: avg data time: 1.93e-01, avg batch time: 1.0117, average train loss: 79.6634
[11/23 17:19:31 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3066, average loss: 23.4213
[11/23 17:19:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.63	
[11/23 17:19:31 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/23 17:21:19 visual_prompt]: 	Training 100/553. train loss: 141.2382,	0.8182 s / batch. (data: 5.45e-03). ETA=9:31:43, max mem: 20.9 GB 
[11/23 17:22:56 visual_prompt]: 	Training 200/553. train loss: 37.6976,	1.2120 s / batch. (data: 3.85e-01). ETA=14:04:55, max mem: 20.9 GB 
[11/23 17:24:36 visual_prompt]: 	Training 300/553. train loss: 216.4294,	0.9320 s / batch. (data: 1.10e-01). ETA=10:48:09, max mem: 20.9 GB 
[11/23 17:26:17 visual_prompt]: 	Training 400/553. train loss: 105.7682,	1.3561 s / batch. (data: 5.24e-01). ETA=15:40:50, max mem: 20.9 GB 
[11/23 17:27:59 visual_prompt]: 	Training 500/553. train loss: 40.7476,	1.5525 s / batch. (data: 7.35e-01). ETA=17:54:31, max mem: 20.9 GB 
[11/23 17:28:51 visual_prompt]: Epoch 25 / 100: avg data time: 1.96e-01, avg batch time: 1.0137, average train loss: 73.2479
[11/23 17:29:49 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3075, average loss: 121.4401
[11/23 17:29:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[11/23 17:29:49 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/23 17:31:34 visual_prompt]: 	Training 100/553. train loss: 232.7358,	0.8240 s / batch. (data: 2.95e-04). ETA=9:28:13, max mem: 20.9 GB 
[11/23 17:33:16 visual_prompt]: 	Training 200/553. train loss: 22.3119,	1.9473 s / batch. (data: 1.11e+00). ETA=22:19:33, max mem: 20.9 GB 
[11/23 17:34:58 visual_prompt]: 	Training 300/553. train loss: 224.7032,	0.8323 s / batch. (data: 7.44e-04). ETA=9:31:08, max mem: 20.9 GB 
[11/23 17:36:38 visual_prompt]: 	Training 400/553. train loss: 11.7838,	0.8216 s / batch. (data: 2.76e-04). ETA=9:22:26, max mem: 20.9 GB 
[11/23 17:38:18 visual_prompt]: 	Training 500/553. train loss: 139.0402,	0.8120 s / batch. (data: 2.97e-04). ETA=9:14:30, max mem: 20.9 GB 
[11/23 17:39:10 visual_prompt]: Epoch 26 / 100: avg data time: 1.95e-01, avg batch time: 1.0141, average train loss: 90.0629
[11/23 17:40:08 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3062, average loss: 45.1047
[11/23 17:40:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.26	
[11/23 17:40:08 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/23 17:41:53 visual_prompt]: 	Training 100/553. train loss: 22.0259,	0.8320 s / batch. (data: 3.06e-04). ETA=9:26:04, max mem: 20.9 GB 
[11/23 17:43:36 visual_prompt]: 	Training 200/553. train loss: 166.5758,	1.4085 s / batch. (data: 6.06e-01). ETA=15:55:55, max mem: 20.9 GB 
[11/23 17:45:17 visual_prompt]: 	Training 300/553. train loss: 47.4541,	0.8021 s / batch. (data: 3.24e-04). ETA=9:03:02, max mem: 20.9 GB 
[11/23 17:46:59 visual_prompt]: 	Training 400/553. train loss: 40.6689,	0.8456 s / batch. (data: 7.50e-04). ETA=9:31:03, max mem: 20.9 GB 
[11/23 17:48:40 visual_prompt]: 	Training 500/553. train loss: 119.7567,	0.8400 s / batch. (data: 3.12e-04). ETA=9:25:53, max mem: 20.9 GB 
[11/23 17:49:31 visual_prompt]: Epoch 27 / 100: avg data time: 1.99e-01, avg batch time: 1.0178, average train loss: 77.0611
[11/23 17:50:28 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3054, average loss: 40.9811
[11/23 17:50:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.89	
[11/23 17:50:28 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/23 17:52:12 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8416 s / batch. (data: 2.84e-02). ETA=9:24:49, max mem: 20.9 GB 
[11/23 17:53:53 visual_prompt]: 	Training 200/553. train loss: 54.0814,	0.8240 s / batch. (data: 3.01e-04). ETA=9:11:40, max mem: 20.9 GB 
[11/23 17:55:36 visual_prompt]: 	Training 300/553. train loss: 81.7058,	1.5342 s / batch. (data: 7.26e-01). ETA=17:04:34, max mem: 20.9 GB 
[11/23 17:57:15 visual_prompt]: 	Training 400/553. train loss: 65.5140,	0.8455 s / batch. (data: 9.44e-03). ETA=9:23:12, max mem: 20.9 GB 
[11/23 17:58:54 visual_prompt]: 	Training 500/553. train loss: 182.4296,	0.8179 s / batch. (data: 3.94e-04). ETA=9:03:29, max mem: 20.9 GB 
[11/23 17:59:49 visual_prompt]: Epoch 28 / 100: avg data time: 1.94e-01, avg batch time: 1.0128, average train loss: 82.3342
[11/23 18:00:46 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3072, average loss: 18.1313
[11/23 18:00:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.82	
[11/23 18:00:46 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/23 18:02:37 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8335 s / batch. (data: 2.15e-02). ETA=9:11:44, max mem: 20.9 GB 
[11/23 18:04:17 visual_prompt]: 	Training 200/553. train loss: 130.6215,	1.7870 s / batch. (data: 9.77e-01). ETA=19:39:53, max mem: 20.9 GB 
[11/23 18:05:55 visual_prompt]: 	Training 300/553. train loss: 62.4456,	0.8269 s / batch. (data: 1.10e-02). ETA=9:04:37, max mem: 20.9 GB 
[11/23 18:07:32 visual_prompt]: 	Training 400/553. train loss: 62.6526,	1.2942 s / batch. (data: 4.85e-01). ETA=14:10:11, max mem: 20.9 GB 
[11/23 18:09:13 visual_prompt]: 	Training 500/553. train loss: 33.2094,	0.8094 s / batch. (data: 3.18e-04). ETA=8:50:21, max mem: 20.9 GB 
[11/23 18:10:05 visual_prompt]: Epoch 29 / 100: avg data time: 1.91e-01, avg batch time: 1.0100, average train loss: 68.3521
[11/23 18:11:02 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3060, average loss: 34.2258
[11/23 18:11:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.92	
[11/23 18:11:02 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/23 18:12:45 visual_prompt]: 	Training 100/553. train loss: 201.8579,	0.8018 s / batch. (data: 2.90e-04). ETA=8:43:19, max mem: 20.9 GB 
[11/23 18:14:27 visual_prompt]: 	Training 200/553. train loss: 57.9761,	0.8460 s / batch. (data: 2.19e-02). ETA=9:10:47, max mem: 20.9 GB 
[11/23 18:16:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.3760 s / batch. (data: 5.48e-01). ETA=14:53:32, max mem: 20.9 GB 
[11/23 18:17:48 visual_prompt]: 	Training 400/553. train loss: 193.9628,	1.1080 s / batch. (data: 2.85e-01). ETA=11:57:40, max mem: 20.9 GB 
[11/23 18:19:28 visual_prompt]: 	Training 500/553. train loss: 112.9856,	1.5504 s / batch. (data: 7.42e-01). ETA=16:41:38, max mem: 20.9 GB 
[11/23 18:20:22 visual_prompt]: Epoch 30 / 100: avg data time: 1.92e-01, avg batch time: 1.0122, average train loss: 75.3280
[11/23 18:21:20 visual_prompt]: Inference (val):avg data time: 5.08e-05, avg batch time: 0.3080, average loss: 84.2754
[11/23 18:21:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.72	
[11/23 18:21:20 visual_prompt]: Stopping early.
[11/23 18:21:20 visual_prompt]: Rank of current process: 0. World size: 1
[11/23 18:21:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 18:21:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/23 18:21:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 18:21:20 visual_prompt]: Training with config:
[11/23 18:21:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/23 18:21:20 visual_prompt]: Loading training data...
[11/23 18:21:20 visual_prompt]: Constructing mammo-cbis dataset train...
[11/23 18:21:20 visual_prompt]: Loading validation data...
[11/23 18:21:20 visual_prompt]: Constructing mammo-cbis dataset val...
[11/23 18:21:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/23 18:21:26 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 18:21:26 visual_prompt]: tuned percent:0.525
[11/23 18:21:26 visual_prompt]: Device used for model: 0
[11/23 18:21:26 visual_prompt]: Setting up Evaluator...
[11/23 18:21:26 visual_prompt]: Setting up Trainer...
[11/23 18:21:26 visual_prompt]: 	Setting up the optimizer...
[11/23 18:21:26 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/23 18:23:10 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8440 s / batch. (data: 7.95e-03). ETA=12:56:28, max mem: 20.9 GB 
[11/23 18:24:50 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8325 s / batch. (data: 3.71e-04). ETA=12:44:28, max mem: 20.9 GB 
[11/23 18:26:34 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.7862 s / batch. (data: 9.61e-01). ETA=1 day, 3:17:19, max mem: 20.9 GB 
[11/23 18:28:13 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8106 s / batch. (data: 2.93e-04). ETA=12:21:43, max mem: 20.9 GB 
[11/23 18:29:55 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8189 s / batch. (data: 2.98e-04). ETA=12:27:57, max mem: 20.9 GB 
[11/23 18:30:49 visual_prompt]: Epoch 1 / 100: avg data time: 1.92e-01, avg batch time: 1.0168, average train loss: 1.5403
[11/23 18:31:46 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3064, average loss: 1.5201
[11/23 18:31:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 18:31:46 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/23 18:33:30 visual_prompt]: 	Training 100/553. train loss: 9.3182,	0.8360 s / batch. (data: 3.17e-04). ETA=12:41:22, max mem: 20.9 GB 
[11/23 18:35:11 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.5273 s / batch. (data: 7.09e-01). ETA=23:08:28, max mem: 20.9 GB 
[11/23 18:36:53 visual_prompt]: 	Training 300/553. train loss: 5.1931,	1.0830 s / batch. (data: 2.56e-01). ETA=16:22:47, max mem: 20.9 GB 
[11/23 18:38:33 visual_prompt]: 	Training 400/553. train loss: 0.6944,	0.8640 s / batch. (data: 5.41e-03). ETA=13:02:36, max mem: 20.9 GB 
[11/23 18:40:15 visual_prompt]: 	Training 500/553. train loss: 0.5637,	0.8215 s / batch. (data: 3.12e-04). ETA=12:22:42, max mem: 20.9 GB 
[11/23 18:41:07 visual_prompt]: Epoch 2 / 100: avg data time: 1.90e-01, avg batch time: 1.0130, average train loss: 9.8902
[11/23 18:42:04 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3099, average loss: 11.6990
[11/23 18:42:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.64	
[11/23 18:42:04 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/23 18:43:48 visual_prompt]: 	Training 100/553. train loss: 20.6916,	0.8439 s / batch. (data: 4.43e-04). ETA=12:40:51, max mem: 20.9 GB 
[11/23 18:45:29 visual_prompt]: 	Training 200/553. train loss: 7.1333,	0.8155 s / batch. (data: 5.43e-03). ETA=12:13:54, max mem: 20.9 GB 
[11/23 18:47:09 visual_prompt]: 	Training 300/553. train loss: 9.6720,	0.8370 s / batch. (data: 5.42e-03). ETA=12:31:51, max mem: 20.9 GB 
[11/23 18:48:50 visual_prompt]: 	Training 400/553. train loss: 15.5484,	0.8415 s / batch. (data: 1.05e-02). ETA=12:34:25, max mem: 20.9 GB 
[11/23 18:50:31 visual_prompt]: 	Training 500/553. train loss: 4.5820,	1.4400 s / batch. (data: 6.07e-01). ETA=21:28:38, max mem: 20.9 GB 
[11/23 18:51:24 visual_prompt]: Epoch 3 / 100: avg data time: 1.88e-01, avg batch time: 1.0111, average train loss: 12.6822
[11/23 18:52:21 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3067, average loss: 9.8754
[11/23 18:52:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.28	
[11/23 18:52:21 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/23 18:54:08 visual_prompt]: 	Training 100/553. train loss: 14.6318,	0.8228 s / batch. (data: 2.99e-04). ETA=12:14:12, max mem: 20.9 GB 
[11/23 18:55:49 visual_prompt]: 	Training 200/553. train loss: 14.6047,	0.8438 s / batch. (data: 5.46e-03). ETA=12:31:33, max mem: 20.9 GB 
[11/23 18:57:30 visual_prompt]: 	Training 300/553. train loss: 12.7162,	1.6525 s / batch. (data: 8.38e-01). ETA=1 day, 0:29:03, max mem: 20.9 GB 
[11/23 18:59:06 visual_prompt]: 	Training 400/553. train loss: 4.5224,	0.8653 s / batch. (data: 3.80e-02). ETA=12:47:49, max mem: 20.9 GB 
[11/23 19:00:49 visual_prompt]: 	Training 500/553. train loss: 0.0000,	2.9855 s / batch. (data: 2.18e+00). ETA=1 day, 20:04:10, max mem: 20.9 GB 
[11/23 19:01:44 visual_prompt]: Epoch 4 / 100: avg data time: 1.94e-01, avg batch time: 1.0163, average train loss: 17.3637
[11/23 19:02:41 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3044, average loss: 93.1347
[11/23 19:02:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.46	
[11/23 19:02:41 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/23 19:04:24 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8395 s / batch. (data: 1.01e-03). ETA=12:21:25, max mem: 20.9 GB 
[11/23 19:06:06 visual_prompt]: 	Training 200/553. train loss: 6.3354,	1.1680 s / batch. (data: 3.59e-01). ETA=17:09:34, max mem: 20.9 GB 
[11/23 19:07:48 visual_prompt]: 	Training 300/553. train loss: 36.2018,	0.8099 s / batch. (data: 4.34e-04). ETA=11:52:32, max mem: 20.9 GB 
[11/23 19:09:28 visual_prompt]: 	Training 400/553. train loss: 41.4358,	0.8335 s / batch. (data: 5.94e-03). ETA=12:11:55, max mem: 20.9 GB 
[11/23 19:11:09 visual_prompt]: 	Training 500/553. train loss: 32.2609,	0.8240 s / batch. (data: 5.45e-03). ETA=12:02:13, max mem: 20.9 GB 
[11/23 19:12:02 visual_prompt]: Epoch 5 / 100: avg data time: 1.91e-01, avg batch time: 1.0133, average train loss: 27.6257
[11/23 19:12:59 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3050, average loss: 89.6345
[11/23 19:12:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.05	
[11/23 19:12:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/23 19:14:45 visual_prompt]: 	Training 100/553. train loss: 9.4687,	0.8400 s / batch. (data: 7.72e-04). ETA=12:14:05, max mem: 20.9 GB 
[11/23 19:16:26 visual_prompt]: 	Training 200/553. train loss: 8.8481,	0.8440 s / batch. (data: 2.98e-04). ETA=12:16:10, max mem: 20.9 GB 
[11/23 19:18:05 visual_prompt]: 	Training 300/553. train loss: 5.5799,	0.8665 s / batch. (data: 1.59e-02). ETA=12:34:20, max mem: 20.9 GB 
[11/23 19:19:49 visual_prompt]: 	Training 400/553. train loss: 35.0367,	0.8200 s / batch. (data: 3.73e-04). ETA=11:52:30, max mem: 20.9 GB 
[11/23 19:21:28 visual_prompt]: 	Training 500/553. train loss: 27.2777,	0.8262 s / batch. (data: 3.07e-04). ETA=11:56:29, max mem: 20.9 GB 
[11/23 19:22:19 visual_prompt]: Epoch 6 / 100: avg data time: 1.91e-01, avg batch time: 1.0126, average train loss: 34.5054
[11/23 19:23:17 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3073, average loss: 11.0577
[11/23 19:23:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.58	
[11/23 19:23:17 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/23 19:25:01 visual_prompt]: 	Training 100/553. train loss: 0.0337,	0.8386 s / batch. (data: 2.25e-02). ETA=12:05:06, max mem: 20.9 GB 
[11/23 19:26:41 visual_prompt]: 	Training 200/553. train loss: 22.7581,	0.8066 s / batch. (data: 2.96e-04). ETA=11:36:09, max mem: 20.9 GB 
[11/23 19:28:25 visual_prompt]: 	Training 300/553. train loss: 3.9200,	1.6417 s / batch. (data: 8.23e-01). ETA=23:34:07, max mem: 20.9 GB 
[11/23 19:30:06 visual_prompt]: 	Training 400/553. train loss: 17.9251,	1.9555 s / batch. (data: 1.14e+00). ETA=1 day, 4:01:06, max mem: 20.9 GB 
[11/23 19:31:45 visual_prompt]: 	Training 500/553. train loss: 135.0038,	0.8211 s / batch. (data: 1.05e-02). ETA=11:44:33, max mem: 20.9 GB 
[11/23 19:32:36 visual_prompt]: Epoch 7 / 100: avg data time: 1.90e-01, avg batch time: 1.0098, average train loss: 43.3462
[11/23 19:33:33 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3064, average loss: 30.7992
[11/23 19:33:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.91	
[11/23 19:33:33 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/23 19:35:16 visual_prompt]: 	Training 100/553. train loss: 92.5026,	0.8240 s / batch. (data: 3.06e-04). ETA=11:44:55, max mem: 20.9 GB 
[11/23 19:36:59 visual_prompt]: 	Training 200/553. train loss: 133.4921,	0.8180 s / batch. (data: 3.16e-04). ETA=11:38:24, max mem: 20.9 GB 
[11/23 19:38:39 visual_prompt]: 	Training 300/553. train loss: 234.0344,	0.8449 s / batch. (data: 1.56e-02). ETA=12:00:01, max mem: 20.9 GB 
[11/23 19:40:21 visual_prompt]: 	Training 400/553. train loss: 94.3837,	0.8240 s / batch. (data: 1.20e-02). ETA=11:40:49, max mem: 20.9 GB 
[11/23 19:42:01 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4411 s / batch. (data: 6.47e-01). ETA=20:23:15, max mem: 20.9 GB 
[11/23 19:42:54 visual_prompt]: Epoch 8 / 100: avg data time: 1.94e-01, avg batch time: 1.0140, average train loss: 73.7924
[11/23 19:43:52 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3063, average loss: 203.1180
[11/23 19:43:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.60	
[11/23 19:43:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/23 19:45:36 visual_prompt]: 	Training 100/553. train loss: 89.9425,	0.8160 s / batch. (data: 2.87e-04). ETA=11:30:33, max mem: 20.9 GB 
[11/23 19:47:16 visual_prompt]: 	Training 200/553. train loss: 24.2411,	0.8343 s / batch. (data: 5.43e-03). ETA=11:44:41, max mem: 20.9 GB 
[11/23 19:48:57 visual_prompt]: 	Training 300/553. train loss: 93.5788,	1.6131 s / batch. (data: 8.06e-01). ETA=22:39:43, max mem: 20.9 GB 
[11/23 19:50:39 visual_prompt]: 	Training 400/553. train loss: 12.0334,	0.8227 s / batch. (data: 3.01e-04). ETA=11:32:08, max mem: 20.9 GB 
[11/23 19:52:19 visual_prompt]: 	Training 500/553. train loss: 3.0766,	1.0145 s / batch. (data: 1.89e-01). ETA=14:11:48, max mem: 20.9 GB 
[11/23 19:53:11 visual_prompt]: Epoch 9 / 100: avg data time: 1.90e-01, avg batch time: 1.0111, average train loss: 48.2814
[11/23 19:54:09 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3071, average loss: 42.4104
[11/23 19:54:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[11/23 19:54:09 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/23 19:55:56 visual_prompt]: 	Training 100/553. train loss: 184.7925,	0.8031 s / batch. (data: 4.10e-04). ETA=11:12:15, max mem: 20.9 GB 
[11/23 19:57:35 visual_prompt]: 	Training 200/553. train loss: 3.7531,	0.8286 s / batch. (data: 1.05e-02). ETA=11:32:12, max mem: 20.9 GB 
[11/23 19:59:15 visual_prompt]: 	Training 300/553. train loss: 19.7834,	1.4344 s / batch. (data: 6.26e-01). ETA=19:55:54, max mem: 20.9 GB 
[11/23 20:00:55 visual_prompt]: 	Training 400/553. train loss: 71.5181,	0.8163 s / batch. (data: 1.06e-02). ETA=11:19:11, max mem: 20.9 GB 
[11/23 20:02:36 visual_prompt]: 	Training 500/553. train loss: 93.8140,	0.9770 s / batch. (data: 1.53e-01). ETA=13:31:16, max mem: 20.9 GB 
[11/23 20:03:28 visual_prompt]: Epoch 10 / 100: avg data time: 1.93e-01, avg batch time: 1.0120, average train loss: 82.9872
[11/23 20:04:26 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3072, average loss: 48.1733
[11/23 20:04:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.14	
[11/23 20:04:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/23 20:06:12 visual_prompt]: 	Training 100/553. train loss: 151.3405,	0.8043 s / batch. (data: 3.05e-04). ETA=11:05:51, max mem: 20.9 GB 
[11/23 20:07:55 visual_prompt]: 	Training 200/553. train loss: 99.0040,	0.8234 s / batch. (data: 8.86e-03). ETA=11:20:14, max mem: 20.9 GB 
[11/23 20:09:34 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.9715 s / batch. (data: 1.18e+00). ETA=1 day, 3:05:28, max mem: 20.9 GB 
[11/23 20:11:13 visual_prompt]: 	Training 400/553. train loss: 181.2127,	0.8038 s / batch. (data: 3.09e-04). ETA=11:01:23, max mem: 20.9 GB 
[11/23 20:12:52 visual_prompt]: 	Training 500/553. train loss: 98.6965,	0.8042 s / batch. (data: 2.71e-04). ETA=11:00:23, max mem: 20.9 GB 
[11/23 20:13:44 visual_prompt]: Epoch 11 / 100: avg data time: 1.89e-01, avg batch time: 1.0088, average train loss: 72.5064
[11/23 20:14:41 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3062, average loss: 107.5733
[11/23 20:14:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.46	
[11/23 20:14:41 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/23 20:16:27 visual_prompt]: 	Training 100/553. train loss: 54.0880,	0.8017 s / batch. (data: 2.98e-04). ETA=10:56:16, max mem: 20.9 GB 
[11/23 20:18:09 visual_prompt]: 	Training 200/553. train loss: 48.0289,	0.8160 s / batch. (data: 3.17e-04). ETA=11:06:37, max mem: 20.9 GB 
[11/23 20:19:48 visual_prompt]: 	Training 300/553. train loss: 37.7816,	0.8560 s / batch. (data: 3.14e-04). ETA=11:37:51, max mem: 20.9 GB 
[11/23 20:21:29 visual_prompt]: 	Training 400/553. train loss: 92.9019,	0.8200 s / batch. (data: 7.94e-03). ETA=11:07:09, max mem: 20.9 GB 
[11/23 20:23:10 visual_prompt]: 	Training 500/553. train loss: 41.5009,	0.8223 s / batch. (data: 1.04e-02). ETA=11:07:40, max mem: 20.9 GB 
[11/23 20:24:01 visual_prompt]: Epoch 12 / 100: avg data time: 1.92e-01, avg batch time: 1.0124, average train loss: 69.8166
[11/23 20:24:59 visual_prompt]: Inference (val):avg data time: 1.81e-04, avg batch time: 0.3068, average loss: 98.0493
[11/23 20:24:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.43	
[11/23 20:24:59 visual_prompt]: Best epoch 12: best metric: -98.049
[11/23 20:24:59 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/23 20:26:45 visual_prompt]: 	Training 100/553. train loss: 73.2958,	0.8200 s / batch. (data: 7.96e-03). ETA=11:03:43, max mem: 20.9 GB 
[11/23 20:28:23 visual_prompt]: 	Training 200/553. train loss: 69.2323,	0.8280 s / batch. (data: 2.99e-04). ETA=11:08:47, max mem: 20.9 GB 
[11/23 20:30:04 visual_prompt]: 	Training 300/553. train loss: 30.0433,	1.8075 s / batch. (data: 1.00e+00). ETA=1 day, 0:16:58, max mem: 20.9 GB 
[11/23 20:31:43 visual_prompt]: 	Training 400/553. train loss: 241.2708,	0.8425 s / batch. (data: 1.06e-02). ETA=11:17:43, max mem: 20.9 GB 
[11/23 20:33:25 visual_prompt]: 	Training 500/553. train loss: 34.8105,	0.8080 s / batch. (data: 3.02e-04). ETA=10:48:36, max mem: 20.9 GB 
[11/23 20:34:18 visual_prompt]: Epoch 13 / 100: avg data time: 1.91e-01, avg batch time: 1.0098, average train loss: 97.3977
[11/23 20:35:15 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3077, average loss: 54.5825
[11/23 20:35:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.08	
[11/23 20:35:15 visual_prompt]: Best epoch 13: best metric: -54.582
[11/23 20:35:15 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/23 20:37:00 visual_prompt]: 	Training 100/553. train loss: 58.1163,	0.8200 s / batch. (data: 3.43e-04). ETA=10:56:09, max mem: 20.9 GB 
[11/23 20:38:41 visual_prompt]: 	Training 200/553. train loss: 0.0389,	1.1715 s / batch. (data: 3.64e-01). ETA=15:35:27, max mem: 20.9 GB 
[11/23 20:40:21 visual_prompt]: 	Training 300/553. train loss: 34.0453,	0.8360 s / batch. (data: 5.44e-03). ETA=11:06:08, max mem: 20.9 GB 
[11/23 20:42:01 visual_prompt]: 	Training 400/553. train loss: 20.2472,	0.8269 s / batch. (data: 2.80e-04). ETA=10:57:32, max mem: 20.9 GB 
[11/23 20:43:42 visual_prompt]: 	Training 500/553. train loss: 164.4967,	0.8120 s / batch. (data: 2.88e-04). ETA=10:44:20, max mem: 20.9 GB 
[11/23 20:44:33 visual_prompt]: Epoch 14 / 100: avg data time: 1.88e-01, avg batch time: 1.0084, average train loss: 61.9254
[11/23 20:45:30 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3061, average loss: 27.4170
[11/23 20:45:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.04	
[11/23 20:45:30 visual_prompt]: Best epoch 14: best metric: -27.417
[11/23 20:45:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/23 20:47:15 visual_prompt]: 	Training 100/553. train loss: 110.0935,	1.5103 s / batch. (data: 7.02e-01). ETA=19:54:33, max mem: 20.9 GB 
[11/23 20:48:54 visual_prompt]: 	Training 200/553. train loss: 372.1039,	0.8200 s / batch. (data: 2.83e-04). ETA=10:47:13, max mem: 20.9 GB 
[11/23 20:50:36 visual_prompt]: 	Training 300/553. train loss: 5.5879,	0.8326 s / batch. (data: 5.43e-03). ETA=10:55:47, max mem: 20.9 GB 
[11/23 20:52:14 visual_prompt]: 	Training 400/553. train loss: 0.3316,	0.8215 s / batch. (data: 3.22e-04). ETA=10:45:41, max mem: 20.9 GB 
[11/23 20:53:55 visual_prompt]: 	Training 500/553. train loss: 112.7657,	0.8405 s / batch. (data: 1.05e-02). ETA=10:59:13, max mem: 20.9 GB 
[11/23 20:54:49 visual_prompt]: Epoch 15 / 100: avg data time: 1.90e-01, avg batch time: 1.0094, average train loss: 87.3216
[11/23 20:55:46 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3079, average loss: 210.5192
[11/23 20:55:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.00	
[11/23 20:55:46 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/23 20:57:30 visual_prompt]: 	Training 100/553. train loss: 16.6388,	0.8275 s / batch. (data: 1.14e-02). ETA=10:46:52, max mem: 20.9 GB 
[11/23 20:59:10 visual_prompt]: 	Training 200/553. train loss: 101.8530,	0.8272 s / batch. (data: 2.98e-04). ETA=10:45:17, max mem: 20.9 GB 
[11/23 21:00:51 visual_prompt]: 	Training 300/553. train loss: 149.5668,	0.8194 s / batch. (data: 5.05e-03). ETA=10:37:49, max mem: 20.9 GB 
[11/23 21:02:32 visual_prompt]: 	Training 400/553. train loss: 25.5012,	0.8201 s / batch. (data: 7.94e-04). ETA=10:37:00, max mem: 20.9 GB 
[11/23 21:04:12 visual_prompt]: 	Training 500/553. train loss: 156.1897,	1.6160 s / batch. (data: 7.99e-01). ETA=20:52:31, max mem: 20.9 GB 
[11/23 21:05:04 visual_prompt]: Epoch 16 / 100: avg data time: 1.90e-01, avg batch time: 1.0089, average train loss: 82.4612
[11/23 21:06:02 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3054, average loss: 25.8207
[11/23 21:06:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.73	
[11/23 21:06:02 visual_prompt]: Best epoch 16: best metric: -25.821
[11/23 21:06:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/23 21:07:46 visual_prompt]: 	Training 100/553. train loss: 79.3474,	0.8224 s / batch. (data: 3.21e-04). ETA=10:35:18, max mem: 20.9 GB 
[11/23 21:09:28 visual_prompt]: 	Training 200/553. train loss: 313.7220,	0.8127 s / batch. (data: 2.49e-04). ETA=10:26:29, max mem: 20.9 GB 
[11/23 21:11:08 visual_prompt]: 	Training 300/553. train loss: 178.4385,	0.8103 s / batch. (data: 5.43e-03). ETA=10:23:15, max mem: 20.9 GB 
[11/23 21:12:48 visual_prompt]: 	Training 400/553. train loss: 55.0876,	1.1057 s / batch. (data: 2.86e-01). ETA=14:08:41, max mem: 20.9 GB 
[11/23 21:14:28 visual_prompt]: 	Training 500/553. train loss: 61.1381,	1.0211 s / batch. (data: 2.14e-01). ETA=13:02:00, max mem: 20.9 GB 
[11/23 21:15:22 visual_prompt]: Epoch 17 / 100: avg data time: 1.92e-01, avg batch time: 1.0116, average train loss: 96.4472
[11/23 21:16:19 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3091, average loss: 52.4195
[11/23 21:16:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.80	
[11/23 21:16:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/23 21:18:04 visual_prompt]: 	Training 100/553. train loss: 179.4604,	0.8320 s / batch. (data: 5.44e-03). ETA=10:35:04, max mem: 20.9 GB 
[11/23 21:19:48 visual_prompt]: 	Training 200/553. train loss: 6.3386,	0.8339 s / batch. (data: 7.82e-04). ETA=10:35:09, max mem: 20.9 GB 
[11/23 21:21:29 visual_prompt]: 	Training 300/553. train loss: 5.8222,	0.8075 s / batch. (data: 3.11e-04). ETA=10:13:42, max mem: 20.9 GB 
[11/23 21:23:09 visual_prompt]: 	Training 400/553. train loss: 27.2760,	0.8320 s / batch. (data: 3.19e-04). ETA=10:30:54, max mem: 20.9 GB 
[11/23 21:24:49 visual_prompt]: 	Training 500/553. train loss: 103.9776,	0.8160 s / batch. (data: 3.02e-04). ETA=10:17:24, max mem: 20.9 GB 
[11/23 21:25:40 visual_prompt]: Epoch 18 / 100: avg data time: 1.94e-01, avg batch time: 1.0139, average train loss: 108.9397
[11/23 21:26:38 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3057, average loss: 174.6840
[11/23 21:26:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.30	
[11/23 21:26:38 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/23 21:28:24 visual_prompt]: 	Training 100/553. train loss: 62.0611,	1.3941 s / batch. (data: 5.85e-01). ETA=17:31:15, max mem: 20.9 GB 
[11/23 21:30:05 visual_prompt]: 	Training 200/553. train loss: 12.6486,	0.8558 s / batch. (data: 1.56e-02). ETA=10:43:55, max mem: 20.9 GB 
[11/23 21:31:46 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.2680 s / batch. (data: 4.58e-01). ETA=15:51:58, max mem: 20.9 GB 
[11/23 21:33:29 visual_prompt]: 	Training 400/553. train loss: 51.9433,	0.7976 s / batch. (data: 3.27e-04). ETA=9:57:26, max mem: 20.9 GB 
[11/23 21:35:05 visual_prompt]: 	Training 500/553. train loss: 25.7980,	0.8217 s / batch. (data: 1.06e-02). ETA=10:14:09, max mem: 20.9 GB 
[11/23 21:35:59 visual_prompt]: Epoch 19 / 100: avg data time: 1.94e-01, avg batch time: 1.0136, average train loss: 68.3429
[11/23 21:36:57 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3063, average loss: 5.8235
[11/23 21:36:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.74	
[11/23 21:36:57 visual_prompt]: Best epoch 19: best metric: -5.824
[11/23 21:36:57 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/23 21:38:39 visual_prompt]: 	Training 100/553. train loss: 9.4218,	0.8648 s / batch. (data: 2.07e-02). ETA=10:44:12, max mem: 20.9 GB 
[11/23 21:40:22 visual_prompt]: 	Training 200/553. train loss: 10.9273,	0.8168 s / batch. (data: 2.98e-04). ETA=10:07:04, max mem: 20.9 GB 
[11/23 21:42:03 visual_prompt]: 	Training 300/553. train loss: 76.0858,	0.8160 s / batch. (data: 3.01e-04). ETA=10:05:05, max mem: 20.9 GB 
[11/23 21:43:43 visual_prompt]: 	Training 400/553. train loss: 187.2953,	0.8040 s / batch. (data: 3.63e-04). ETA=9:54:52, max mem: 20.9 GB 
[11/23 21:45:23 visual_prompt]: 	Training 500/553. train loss: 45.7235,	0.8160 s / batch. (data: 2.93e-04). ETA=10:02:22, max mem: 20.9 GB 
[11/23 21:46:17 visual_prompt]: Epoch 20 / 100: avg data time: 1.93e-01, avg batch time: 1.0125, average train loss: 91.9706
[11/23 21:47:14 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3065, average loss: 98.4013
[11/23 21:47:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.25	
[11/23 21:47:14 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/23 21:49:01 visual_prompt]: 	Training 100/553. train loss: 251.7324,	0.8360 s / batch. (data: 1.20e-02). ETA=10:14:59, max mem: 20.9 GB 
[11/23 21:50:41 visual_prompt]: 	Training 200/553. train loss: 142.9680,	0.8065 s / batch. (data: 4.53e-04). ETA=9:51:57, max mem: 20.9 GB 
[11/23 21:52:21 visual_prompt]: 	Training 300/553. train loss: 511.1374,	1.2149 s / batch. (data: 3.94e-01). ETA=14:49:43, max mem: 20.9 GB 
[11/23 21:54:01 visual_prompt]: 	Training 400/553. train loss: 197.3434,	0.8051 s / batch. (data: 3.13e-04). ETA=9:48:17, max mem: 20.9 GB 
[11/23 21:55:43 visual_prompt]: 	Training 500/553. train loss: 40.3862,	0.8320 s / batch. (data: 3.07e-04). ETA=10:06:31, max mem: 20.9 GB 
[11/23 21:56:34 visual_prompt]: Epoch 21 / 100: avg data time: 1.93e-01, avg batch time: 1.0126, average train loss: 107.1739
[11/23 21:57:32 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3072, average loss: 11.7291
[11/23 21:57:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.31	
[11/23 21:57:32 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/23 21:59:16 visual_prompt]: 	Training 100/553. train loss: 129.5088,	0.8094 s / batch. (data: 3.00e-04). ETA=9:47:57, max mem: 20.9 GB 
[11/23 22:00:57 visual_prompt]: 	Training 200/553. train loss: 8.2347,	0.8359 s / batch. (data: 4.44e-04). ETA=10:05:52, max mem: 20.9 GB 
[11/23 22:02:35 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8242 s / batch. (data: 3.09e-04). ETA=9:56:01, max mem: 20.9 GB 
[11/23 22:04:17 visual_prompt]: 	Training 400/553. train loss: 119.1904,	0.8542 s / batch. (data: 1.02e-02). ETA=10:16:17, max mem: 20.9 GB 
[11/23 22:05:57 visual_prompt]: 	Training 500/553. train loss: 41.2609,	0.8065 s / batch. (data: 7.94e-03). ETA=9:40:28, max mem: 20.9 GB 
[11/23 22:06:51 visual_prompt]: Epoch 22 / 100: avg data time: 1.91e-01, avg batch time: 1.0107, average train loss: 77.9296
[11/23 22:07:49 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3072, average loss: 18.1174
[11/23 22:07:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.81	
[11/23 22:07:49 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/23 22:09:35 visual_prompt]: 	Training 100/553. train loss: 18.7215,	0.8341 s / batch. (data: 3.05e-04). ETA=9:58:14, max mem: 20.9 GB 
[11/23 22:11:17 visual_prompt]: 	Training 200/553. train loss: 2.1599,	0.8882 s / batch. (data: 7.77e-02). ETA=10:35:32, max mem: 20.9 GB 
[11/23 22:12:59 visual_prompt]: 	Training 300/553. train loss: 92.5511,	0.8256 s / batch. (data: 1.61e-02). ETA=9:49:22, max mem: 20.9 GB 
[11/23 22:14:38 visual_prompt]: 	Training 400/553. train loss: 17.3159,	0.8600 s / batch. (data: 8.16e-04). ETA=10:12:30, max mem: 20.9 GB 
[11/23 22:16:17 visual_prompt]: 	Training 500/553. train loss: 85.7274,	0.8280 s / batch. (data: 3.03e-04). ETA=9:48:22, max mem: 20.9 GB 
[11/23 22:17:10 visual_prompt]: Epoch 23 / 100: avg data time: 1.93e-01, avg batch time: 1.0134, average train loss: 68.7983
[11/23 22:18:08 visual_prompt]: Inference (val):avg data time: 3.71e-04, avg batch time: 0.3076, average loss: 93.9192
[11/23 22:18:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.86	
[11/23 22:18:08 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/23 22:19:50 visual_prompt]: 	Training 100/553. train loss: 121.3363,	0.8123 s / batch. (data: 5.43e-03). ETA=9:35:07, max mem: 20.9 GB 
[11/23 22:21:29 visual_prompt]: 	Training 200/553. train loss: 30.4765,	0.8360 s / batch. (data: 3.04e-04). ETA=9:50:29, max mem: 20.9 GB 
[11/23 22:23:11 visual_prompt]: 	Training 300/553. train loss: 30.0802,	1.1585 s / batch. (data: 3.39e-01). ETA=13:36:23, max mem: 20.9 GB 
[11/23 22:24:52 visual_prompt]: 	Training 400/553. train loss: 24.1946,	0.8280 s / batch. (data: 2.91e-04). ETA=9:42:04, max mem: 20.9 GB 
[11/23 22:26:34 visual_prompt]: 	Training 500/553. train loss: 155.4979,	0.8000 s / batch. (data: 3.18e-04). ETA=9:21:02, max mem: 20.9 GB 
[11/23 22:27:28 visual_prompt]: Epoch 24 / 100: avg data time: 1.93e-01, avg batch time: 1.0121, average train loss: 77.4350
[11/23 22:28:25 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3078, average loss: 59.1853
[11/23 22:28:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.16	
[11/23 22:28:25 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/23 22:30:13 visual_prompt]: 	Training 100/553. train loss: 197.0322,	0.8195 s / batch. (data: 3.19e-04). ETA=9:32:39, max mem: 20.9 GB 
[11/23 22:31:50 visual_prompt]: 	Training 200/553. train loss: 35.0548,	1.2183 s / batch. (data: 4.00e-01). ETA=14:09:17, max mem: 20.9 GB 
[11/23 22:33:31 visual_prompt]: 	Training 300/553. train loss: 140.3271,	0.8201 s / batch. (data: 3.46e-04). ETA=9:30:21, max mem: 20.9 GB 
[11/23 22:35:12 visual_prompt]: 	Training 400/553. train loss: 81.6939,	1.3584 s / batch. (data: 5.33e-01). ETA=15:42:27, max mem: 20.9 GB 
[11/23 22:36:52 visual_prompt]: 	Training 500/553. train loss: 61.1020,	1.3720 s / batch. (data: 5.58e-01). ETA=15:49:37, max mem: 20.9 GB 
[11/23 22:37:45 visual_prompt]: Epoch 25 / 100: avg data time: 1.93e-01, avg batch time: 1.0127, average train loss: 73.0670
[11/23 22:38:43 visual_prompt]: Inference (val):avg data time: 1.81e-04, avg batch time: 0.3069, average loss: 142.0155
[11/23 22:38:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/23 22:38:43 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/23 22:40:28 visual_prompt]: 	Training 100/553. train loss: 29.4657,	0.8214 s / batch. (data: 1.19e-02). ETA=9:26:26, max mem: 20.9 GB 
[11/23 22:42:11 visual_prompt]: 	Training 200/553. train loss: 385.2049,	1.8040 s / batch. (data: 9.87e-01). ETA=20:41:00, max mem: 20.9 GB 
[11/23 22:43:52 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8133 s / batch. (data: 5.42e-03). ETA=9:18:07, max mem: 20.9 GB 
[11/23 22:45:32 visual_prompt]: 	Training 400/553. train loss: 54.7317,	0.8181 s / batch. (data: 7.95e-03). ETA=9:20:01, max mem: 20.9 GB 
[11/23 22:47:10 visual_prompt]: 	Training 500/553. train loss: 57.9050,	0.8133 s / batch. (data: 3.03e-04). ETA=9:15:23, max mem: 20.9 GB 
[11/23 22:48:02 visual_prompt]: Epoch 26 / 100: avg data time: 1.91e-01, avg batch time: 1.0105, average train loss: 73.4555
[11/23 22:49:00 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3058, average loss: 80.7753
[11/23 22:49:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.97	
[11/23 22:49:00 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/23 22:50:45 visual_prompt]: 	Training 100/553. train loss: 146.1667,	0.8520 s / batch. (data: 3.04e-04). ETA=9:39:41, max mem: 20.9 GB 
[11/23 22:52:25 visual_prompt]: 	Training 200/553. train loss: 171.7426,	1.4640 s / batch. (data: 6.52e-01). ETA=16:33:35, max mem: 20.9 GB 
[11/23 22:54:06 visual_prompt]: 	Training 300/553. train loss: 71.7340,	0.8080 s / batch. (data: 5.45e-03). ETA=9:07:03, max mem: 20.9 GB 
[11/23 22:55:48 visual_prompt]: 	Training 400/553. train loss: 35.2200,	0.8418 s / batch. (data: 7.64e-04). ETA=9:28:32, max mem: 20.9 GB 
[11/23 22:57:30 visual_prompt]: 	Training 500/553. train loss: 3.3718,	0.8258 s / batch. (data: 3.14e-04). ETA=9:16:18, max mem: 20.9 GB 
[11/23 22:58:21 visual_prompt]: Epoch 27 / 100: avg data time: 1.94e-01, avg batch time: 1.0144, average train loss: 64.3172
[11/23 22:59:19 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3071, average loss: 175.3402
[11/23 22:59:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.95	
[11/23 22:59:19 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/23 23:01:02 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8829 s / batch. (data: 8.04e-02). ETA=9:52:31, max mem: 20.9 GB 
[11/23 23:02:45 visual_prompt]: 	Training 200/553. train loss: 155.6715,	0.8120 s / batch. (data: 3.13e-04). ETA=9:03:37, max mem: 20.9 GB 
[11/23 23:04:26 visual_prompt]: 	Training 300/553. train loss: 14.1574,	1.4868 s / batch. (data: 6.65e-01). ETA=16:32:54, max mem: 20.9 GB 
[11/23 23:06:06 visual_prompt]: 	Training 400/553. train loss: 171.3097,	0.8397 s / batch. (data: 7.92e-03). ETA=9:19:22, max mem: 20.9 GB 
[11/23 23:07:45 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8370 s / batch. (data: 9.00e-03). ETA=9:16:12, max mem: 20.9 GB 
[11/23 23:08:40 visual_prompt]: Epoch 28 / 100: avg data time: 1.93e-01, avg batch time: 1.0133, average train loss: 73.8999
[11/23 23:09:38 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3057, average loss: 56.4914
[11/23 23:09:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.33	
[11/23 23:09:38 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/23 23:11:29 visual_prompt]: 	Training 100/553. train loss: 34.7881,	0.8383 s / batch. (data: 1.03e-02). ETA=9:14:55, max mem: 20.9 GB 
[11/23 23:13:09 visual_prompt]: 	Training 200/553. train loss: 0.0895,	1.7629 s / batch. (data: 9.53e-01). ETA=19:23:59, max mem: 20.9 GB 
[11/23 23:14:48 visual_prompt]: 	Training 300/553. train loss: 121.7846,	0.8189 s / batch. (data: 1.05e-02). ETA=8:59:18, max mem: 20.9 GB 
[11/23 23:16:25 visual_prompt]: 	Training 400/553. train loss: 119.9498,	1.3651 s / batch. (data: 5.64e-01). ETA=14:56:47, max mem: 20.9 GB 
[11/23 23:18:07 visual_prompt]: 	Training 500/553. train loss: 61.3509,	0.8072 s / batch. (data: 5.45e-03). ETA=8:48:54, max mem: 20.9 GB 
[11/23 23:19:00 visual_prompt]: Epoch 29 / 100: avg data time: 1.96e-01, avg batch time: 1.0160, average train loss: 84.4679
[11/23 23:19:57 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3053, average loss: 26.3829
[11/23 23:19:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.01	
[11/23 23:19:57 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/23 23:21:41 visual_prompt]: 	Training 100/553. train loss: 59.7197,	0.8419 s / batch. (data: 2.94e-04). ETA=9:09:31, max mem: 20.9 GB 
[11/23 23:23:23 visual_prompt]: 	Training 200/553. train loss: 133.5192,	0.8200 s / batch. (data: 7.92e-03). ETA=8:53:51, max mem: 20.9 GB 
[11/23 23:25:02 visual_prompt]: 	Training 300/553. train loss: 78.8985,	1.0894 s / batch. (data: 2.66e-01). ETA=11:47:25, max mem: 20.9 GB 
[11/23 23:26:45 visual_prompt]: 	Training 400/553. train loss: 3.7242,	1.3098 s / batch. (data: 4.66e-01). ETA=14:08:22, max mem: 20.9 GB 
[11/23 23:28:25 visual_prompt]: 	Training 500/553. train loss: 44.4254,	1.5400 s / batch. (data: 7.06e-01). ETA=16:34:55, max mem: 20.9 GB 
[11/23 23:29:19 visual_prompt]: Epoch 30 / 100: avg data time: 1.96e-01, avg batch time: 1.0156, average train loss: 76.9443
[11/23 23:30:17 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3071, average loss: 15.7663
[11/23 23:30:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.12	
[11/23 23:30:17 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[11/23 23:32:03 visual_prompt]: 	Training 100/553. train loss: 12.9275,	0.8253 s / batch. (data: 2.77e-04). ETA=8:51:05, max mem: 20.9 GB 
[11/23 23:33:46 visual_prompt]: 	Training 200/553. train loss: 239.9660,	0.8111 s / batch. (data: 7.93e-03). ETA=8:40:34, max mem: 20.9 GB 
[11/23 23:35:25 visual_prompt]: 	Training 300/553. train loss: 74.7414,	0.8480 s / batch. (data: 7.99e-03). ETA=9:02:52, max mem: 20.9 GB 
[11/23 23:37:04 visual_prompt]: 	Training 400/553. train loss: 25.2471,	1.4462 s / batch. (data: 6.24e-01). ETA=15:23:22, max mem: 20.9 GB 
[11/23 23:38:45 visual_prompt]: 	Training 500/553. train loss: 30.2537,	0.8339 s / batch. (data: 2.70e-02). ETA=8:51:04, max mem: 20.9 GB 
[11/23 23:39:37 visual_prompt]: Epoch 31 / 100: avg data time: 1.92e-01, avg batch time: 1.0121, average train loss: 63.6474
[11/23 23:40:35 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3086, average loss: 18.8359
[11/23 23:40:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.38	
[11/23 23:40:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[11/23 23:42:22 visual_prompt]: 	Training 100/553. train loss: 15.2904,	0.8306 s / batch. (data: 8.12e-04). ETA=8:46:50, max mem: 20.9 GB 
[11/23 23:44:03 visual_prompt]: 	Training 200/553. train loss: 58.0532,	0.8319 s / batch. (data: 3.18e-04). ETA=8:46:17, max mem: 20.9 GB 
[11/23 23:45:46 visual_prompt]: 	Training 300/553. train loss: 40.2767,	0.9796 s / batch. (data: 1.76e-01). ETA=10:18:05, max mem: 20.9 GB 
[11/23 23:47:29 visual_prompt]: 	Training 400/553. train loss: 32.6062,	0.8320 s / batch. (data: 7.95e-03). ETA=8:43:34, max mem: 20.9 GB 
[11/23 23:49:07 visual_prompt]: 	Training 500/553. train loss: 2.6893,	0.8200 s / batch. (data: 5.44e-03). ETA=8:34:37, max mem: 20.9 GB 
[11/23 23:49:58 visual_prompt]: Epoch 32 / 100: avg data time: 1.98e-01, avg batch time: 1.0185, average train loss: 68.8120
[11/23 23:50:56 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3077, average loss: 18.6799
[11/23 23:50:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.72	
[11/23 23:50:56 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[11/23 23:52:40 visual_prompt]: 	Training 100/553. train loss: 0.1960,	1.3800 s / batch. (data: 5.51e-01). ETA=14:22:34, max mem: 20.9 GB 
[11/23 23:54:22 visual_prompt]: 	Training 200/553. train loss: 67.9366,	1.4440 s / batch. (data: 6.09e-01). ETA=15:00:10, max mem: 20.9 GB 
[11/23 23:56:02 visual_prompt]: 	Training 300/553. train loss: 51.7071,	0.8200 s / batch. (data: 3.04e-04). ETA=8:29:49, max mem: 20.9 GB 
[11/23 23:57:44 visual_prompt]: 	Training 400/553. train loss: 24.1078,	0.8120 s / batch. (data: 2.80e-04). ETA=8:23:30, max mem: 20.9 GB 
[11/23 23:59:23 visual_prompt]: 	Training 500/553. train loss: 8.8907,	0.8239 s / batch. (data: 1.56e-02). ETA=8:29:30, max mem: 20.9 GB 
[11/24 00:00:16 visual_prompt]: Epoch 33 / 100: avg data time: 1.93e-01, avg batch time: 1.0131, average train loss: 60.6009
[11/24 00:01:14 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3068, average loss: 30.7730
[11/24 00:01:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.16	
[11/24 00:01:14 visual_prompt]: Stopping early.
[11/24 00:01:14 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 00:01:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 00:01:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/24 00:01:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 00:01:14 visual_prompt]: Training with config:
[11/24 00:01:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/24 00:01:14 visual_prompt]: Loading training data...
[11/24 00:01:14 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 00:01:14 visual_prompt]: Loading validation data...
[11/24 00:01:14 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 00:01:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 00:01:17 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 00:01:17 visual_prompt]: tuned percent:0.525
[11/24 00:01:17 visual_prompt]: Device used for model: 0
[11/24 00:01:17 visual_prompt]: Setting up Evaluator...
[11/24 00:01:17 visual_prompt]: Setting up Trainer...
[11/24 00:01:17 visual_prompt]: 	Setting up the optimizer...
[11/24 00:01:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 00:03:01 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8200 s / batch. (data: 2.99e-04). ETA=12:34:24, max mem: 20.9 GB 
[11/24 00:04:40 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8242 s / batch. (data: 2.81e-04). ETA=12:36:54, max mem: 20.9 GB 
[11/24 00:06:24 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.5082 s / batch. (data: 7.01e-01). ETA=23:02:29, max mem: 20.9 GB 
[11/24 00:08:02 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8151 s / batch. (data: 7.97e-03). ETA=12:25:46, max mem: 20.9 GB 
[11/24 00:09:45 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8080 s / batch. (data: 3.11e-04). ETA=12:17:57, max mem: 20.9 GB 
[11/24 00:10:38 visual_prompt]: Epoch 1 / 100: avg data time: 1.92e-01, avg batch time: 1.0148, average train loss: 1.5403
[11/24 00:11:36 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3078, average loss: 1.5201
[11/24 00:11:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 00:11:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[11/24 00:13:20 visual_prompt]: 	Training 100/553. train loss: 7.2424,	0.8083 s / batch. (data: 3.16e-04). ETA=12:16:10, max mem: 20.9 GB 
[11/24 00:15:00 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8175 s / batch. (data: 5.42e-03). ETA=12:23:12, max mem: 20.9 GB 
[11/24 00:16:43 visual_prompt]: 	Training 300/553. train loss: 4.3327,	1.0760 s / batch. (data: 2.54e-01). ETA=16:16:23, max mem: 20.9 GB 
[11/24 00:18:22 visual_prompt]: 	Training 400/553. train loss: 1.6245,	0.8076 s / batch. (data: 3.06e-04). ETA=12:11:29, max mem: 20.9 GB 
[11/24 00:20:05 visual_prompt]: 	Training 500/553. train loss: 1.6753,	0.8359 s / batch. (data: 2.56e-02). ETA=12:35:47, max mem: 20.9 GB 
[11/24 00:20:55 visual_prompt]: Epoch 2 / 100: avg data time: 1.89e-01, avg batch time: 1.0124, average train loss: 12.4687
[11/24 00:21:53 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3077, average loss: 17.9150
[11/24 00:21:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.10	
[11/24 00:21:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[11/24 00:23:36 visual_prompt]: 	Training 100/553. train loss: 28.1278,	0.8437 s / batch. (data: 1.36e-02). ETA=12:40:39, max mem: 20.9 GB 
[11/24 00:25:18 visual_prompt]: 	Training 200/553. train loss: 10.4513,	2.3451 s / batch. (data: 1.52e+00). ETA=1 day, 11:10:22, max mem: 20.9 GB 
[11/24 00:26:57 visual_prompt]: 	Training 300/553. train loss: 2.1308,	0.8498 s / batch. (data: 5.73e-03). ETA=12:43:17, max mem: 20.9 GB 
[11/24 00:28:39 visual_prompt]: 	Training 400/553. train loss: 64.6666,	0.8290 s / batch. (data: 1.56e-02). ETA=12:23:17, max mem: 20.9 GB 
[11/24 00:30:20 visual_prompt]: 	Training 500/553. train loss: 6.3802,	1.1360 s / batch. (data: 3.10e-01). ETA=16:56:35, max mem: 20.9 GB 
[11/24 00:31:12 visual_prompt]: Epoch 3 / 100: avg data time: 1.88e-01, avg batch time: 1.0105, average train loss: 13.6450
[11/24 00:32:10 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3063, average loss: 12.2957
[11/24 00:32:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.88	
[11/24 00:32:10 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[11/24 00:33:56 visual_prompt]: 	Training 100/553. train loss: 14.1667,	0.8397 s / batch. (data: 3.24e-04). ETA=12:29:21, max mem: 20.9 GB 
[11/24 00:35:37 visual_prompt]: 	Training 200/553. train loss: 13.1459,	0.8320 s / batch. (data: 3.22e-04). ETA=12:21:03, max mem: 20.9 GB 
[11/24 00:37:17 visual_prompt]: 	Training 300/553. train loss: 5.1815,	1.0320 s / batch. (data: 2.26e-01). ETA=15:17:27, max mem: 20.9 GB 
[11/24 00:38:54 visual_prompt]: 	Training 400/553. train loss: 0.4137,	1.4891 s / batch. (data: 6.62e-01). ETA=22:01:20, max mem: 20.9 GB 
[11/24 00:40:37 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.4141 s / batch. (data: 2.60e+00). ETA=2 days, 2:23:50, max mem: 20.9 GB 
[11/24 00:41:30 visual_prompt]: Epoch 4 / 100: avg data time: 1.91e-01, avg batch time: 1.0138, average train loss: 18.2450
[11/24 00:42:28 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3058, average loss: 5.3036
[11/24 00:42:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/24 00:42:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[11/24 00:44:12 visual_prompt]: 	Training 100/553. train loss: 83.4474,	0.8319 s / batch. (data: 3.11e-04). ETA=12:14:42, max mem: 20.9 GB 
[11/24 00:45:52 visual_prompt]: 	Training 200/553. train loss: 0.9112,	1.3655 s / batch. (data: 5.58e-01). ETA=20:03:36, max mem: 20.9 GB 
[11/24 00:47:34 visual_prompt]: 	Training 300/553. train loss: 73.5406,	0.8225 s / batch. (data: 3.06e-04). ETA=12:03:36, max mem: 20.9 GB 
[11/24 00:49:13 visual_prompt]: 	Training 400/553. train loss: 2.6459,	0.8320 s / batch. (data: 7.94e-03). ETA=12:10:36, max mem: 20.9 GB 
[11/24 00:50:55 visual_prompt]: 	Training 500/553. train loss: 22.0819,	0.8081 s / batch. (data: 2.83e-04). ETA=11:48:15, max mem: 20.9 GB 
[11/24 00:51:48 visual_prompt]: Epoch 5 / 100: avg data time: 1.90e-01, avg batch time: 1.0133, average train loss: 25.7894
[11/24 00:52:47 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3054, average loss: 3.6946
[11/24 00:52:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.25	
[11/24 00:52:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[11/24 00:54:33 visual_prompt]: 	Training 100/553. train loss: 24.2818,	0.7987 s / batch. (data: 2.94e-04). ETA=11:38:01, max mem: 20.9 GB 
[11/24 00:56:13 visual_prompt]: 	Training 200/553. train loss: 99.6738,	0.8255 s / batch. (data: 4.38e-04). ETA=12:00:04, max mem: 20.9 GB 
[11/24 00:57:52 visual_prompt]: 	Training 300/553. train loss: 8.5172,	0.8387 s / batch. (data: 3.25e-04). ETA=12:10:08, max mem: 20.9 GB 
[11/24 00:59:36 visual_prompt]: 	Training 400/553. train loss: 18.0969,	0.8400 s / batch. (data: 7.94e-03). ETA=12:09:54, max mem: 20.9 GB 
[11/24 01:01:15 visual_prompt]: 	Training 500/553. train loss: 11.3561,	0.8400 s / batch. (data: 3.42e-04). ETA=12:08:28, max mem: 20.9 GB 
[11/24 01:02:07 visual_prompt]: Epoch 6 / 100: avg data time: 1.91e-01, avg batch time: 1.0137, average train loss: 23.5509
[11/24 01:03:05 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3076, average loss: 6.1940
[11/24 01:03:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.42	
[11/24 01:03:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[11/24 01:04:49 visual_prompt]: 	Training 100/553. train loss: 71.1854,	0.8276 s / batch. (data: 7.85e-04). ETA=11:55:40, max mem: 20.9 GB 
[11/24 01:06:29 visual_prompt]: 	Training 200/553. train loss: 5.1444,	0.8240 s / batch. (data: 7.92e-03). ETA=11:51:06, max mem: 20.9 GB 
[11/24 01:08:13 visual_prompt]: 	Training 300/553. train loss: 18.4046,	2.1916 s / batch. (data: 1.37e+00). ETA=1 day, 7:27:43, max mem: 20.9 GB 
[11/24 01:09:54 visual_prompt]: 	Training 400/553. train loss: 13.8397,	1.8804 s / batch. (data: 1.06e+00). ETA=1 day, 2:56:34, max mem: 20.9 GB 
[11/24 01:11:34 visual_prompt]: 	Training 500/553. train loss: 68.1425,	0.8347 s / batch. (data: 1.47e-02). ETA=11:56:13, max mem: 20.9 GB 
[11/24 01:12:24 visual_prompt]: Epoch 7 / 100: avg data time: 1.89e-01, avg batch time: 1.0107, average train loss: 24.9745
[11/24 01:13:22 visual_prompt]: Inference (val):avg data time: 3.86e-04, avg batch time: 0.3078, average loss: 6.3741
[11/24 01:13:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.37	
[11/24 01:13:22 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[11/24 01:15:04 visual_prompt]: 	Training 100/553. train loss: 69.9379,	0.8360 s / batch. (data: 1.60e-02). ETA=11:55:10, max mem: 20.9 GB 
[11/24 01:16:47 visual_prompt]: 	Training 200/553. train loss: 120.0211,	0.8428 s / batch. (data: 1.07e-02). ETA=11:59:37, max mem: 20.9 GB 
[11/24 01:18:28 visual_prompt]: 	Training 300/553. train loss: 14.7993,	0.8239 s / batch. (data: 3.21e-04). ETA=11:42:06, max mem: 20.9 GB 
[11/24 01:20:09 visual_prompt]: 	Training 400/553. train loss: 60.3261,	0.8462 s / batch. (data: 3.16e-04). ETA=11:59:41, max mem: 20.9 GB 
[11/24 01:21:49 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4303 s / batch. (data: 5.96e-01). ETA=20:14:05, max mem: 20.9 GB 
[11/24 01:22:42 visual_prompt]: Epoch 8 / 100: avg data time: 1.92e-01, avg batch time: 1.0134, average train loss: 35.4590
[11/24 01:23:40 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3043, average loss: 29.6084
[11/24 01:23:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.89	
[11/24 01:23:40 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[11/24 01:25:25 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8442 s / batch. (data: 1.54e-02). ETA=11:54:22, max mem: 20.9 GB 
[11/24 01:27:04 visual_prompt]: 	Training 200/553. train loss: 17.5484,	0.8188 s / batch. (data: 2.73e-04). ETA=11:31:31, max mem: 20.9 GB 
[11/24 01:28:46 visual_prompt]: 	Training 300/553. train loss: 11.5687,	1.8200 s / batch. (data: 9.82e-01). ETA=1 day, 1:34:07, max mem: 20.9 GB 
[11/24 01:30:28 visual_prompt]: 	Training 400/553. train loss: 20.6134,	0.8067 s / batch. (data: 3.37e-04). ETA=11:18:37, max mem: 20.9 GB 
[11/24 01:32:10 visual_prompt]: 	Training 500/553. train loss: 41.5786,	0.8863 s / batch. (data: 7.18e-02). ETA=12:24:05, max mem: 20.9 GB 
[11/24 01:33:01 visual_prompt]: Epoch 9 / 100: avg data time: 1.94e-01, avg batch time: 1.0152, average train loss: 28.5791
[11/24 01:33:59 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3042, average loss: 23.8281
[11/24 01:33:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.98	
[11/24 01:33:59 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[11/24 01:35:46 visual_prompt]: 	Training 100/553. train loss: 82.2706,	0.8279 s / batch. (data: 1.05e-02). ETA=11:32:59, max mem: 20.9 GB 
[11/24 01:37:25 visual_prompt]: 	Training 200/553. train loss: 1.1150,	0.8320 s / batch. (data: 3.32e-04). ETA=11:35:01, max mem: 20.9 GB 
[11/24 01:39:06 visual_prompt]: 	Training 300/553. train loss: 30.2138,	1.0655 s / batch. (data: 2.60e-01). ETA=14:48:20, max mem: 20.9 GB 
[11/24 01:40:46 visual_prompt]: 	Training 400/553. train loss: 40.2294,	0.8363 s / batch. (data: 3.19e-04). ETA=11:35:49, max mem: 20.9 GB 
[11/24 01:42:28 visual_prompt]: 	Training 500/553. train loss: 7.8690,	0.8336 s / batch. (data: 3.06e-04). ETA=11:32:10, max mem: 20.9 GB 
[11/24 01:43:21 visual_prompt]: Epoch 10 / 100: avg data time: 1.94e-01, avg batch time: 1.0156, average train loss: 33.4559
[11/24 01:44:19 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3062, average loss: 1.7496
[11/24 01:44:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.90	
[11/24 01:44:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[11/24 01:46:06 visual_prompt]: 	Training 100/553. train loss: 52.0340,	0.8123 s / batch. (data: 2.85e-04). ETA=11:12:25, max mem: 20.9 GB 
[11/24 01:47:48 visual_prompt]: 	Training 200/553. train loss: 52.9471,	0.8240 s / batch. (data: 3.25e-04). ETA=11:20:43, max mem: 20.9 GB 
[11/24 01:49:28 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1507 s / batch. (data: 1.35e+00). ETA=1 day, 5:33:16, max mem: 20.9 GB 
[11/24 01:51:08 visual_prompt]: 	Training 400/553. train loss: 18.9793,	0.8163 s / batch. (data: 6.18e-04). ETA=11:11:42, max mem: 20.9 GB 
[11/24 01:52:48 visual_prompt]: 	Training 500/553. train loss: 18.9895,	0.8400 s / batch. (data: 3.05e-04). ETA=11:29:45, max mem: 20.9 GB 
[11/24 01:53:40 visual_prompt]: Epoch 11 / 100: avg data time: 1.93e-01, avg batch time: 1.0147, average train loss: 33.8645
[11/24 01:54:38 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3071, average loss: 56.5795
[11/24 01:54:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.93	
[11/24 01:54:38 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/24 01:56:25 visual_prompt]: 	Training 100/553. train loss: 4.4777,	0.8460 s / batch. (data: 2.76e-02). ETA=11:32:32, max mem: 20.9 GB 
[11/24 01:58:07 visual_prompt]: 	Training 200/553. train loss: 12.2439,	0.8240 s / batch. (data: 5.48e-03). ETA=11:13:11, max mem: 20.9 GB 
[11/24 01:59:46 visual_prompt]: 	Training 300/553. train loss: 6.2091,	0.8081 s / batch. (data: 3.60e-04). ETA=10:58:47, max mem: 20.9 GB 
[11/24 02:01:28 visual_prompt]: 	Training 400/553. train loss: 36.6586,	0.8301 s / batch. (data: 1.07e-02). ETA=11:15:25, max mem: 20.9 GB 
[11/24 02:03:09 visual_prompt]: 	Training 500/553. train loss: 194.9420,	0.8141 s / batch. (data: 1.05e-02). ETA=11:01:00, max mem: 20.9 GB 
[11/24 02:04:00 visual_prompt]: Epoch 12 / 100: avg data time: 1.96e-01, avg batch time: 1.0160, average train loss: 38.2417
[11/24 02:04:58 visual_prompt]: Inference (val):avg data time: 3.97e-04, avg batch time: 0.3060, average loss: 78.2413
[11/24 02:04:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.06	
[11/24 02:04:58 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/24 02:06:45 visual_prompt]: 	Training 100/553. train loss: 25.1437,	0.8160 s / batch. (data: 3.31e-04). ETA=11:00:26, max mem: 20.9 GB 
[11/24 02:08:23 visual_prompt]: 	Training 200/553. train loss: 12.9302,	0.8400 s / batch. (data: 5.54e-03). ETA=11:18:27, max mem: 20.9 GB 
[11/24 02:10:05 visual_prompt]: 	Training 300/553. train loss: 32.4434,	1.8382 s / batch. (data: 1.03e+00). ETA=1 day, 0:41:44, max mem: 20.9 GB 
[11/24 02:11:45 visual_prompt]: 	Training 400/553. train loss: 112.1674,	0.8205 s / batch. (data: 7.93e-03). ETA=11:00:03, max mem: 20.9 GB 
[11/24 02:13:28 visual_prompt]: 	Training 500/553. train loss: 73.3396,	0.8221 s / batch. (data: 7.98e-03). ETA=10:59:56, max mem: 20.9 GB 
[11/24 02:14:21 visual_prompt]: Epoch 13 / 100: avg data time: 1.96e-01, avg batch time: 1.0165, average train loss: 42.1476
[11/24 02:15:18 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3058, average loss: 14.5400
[11/24 02:15:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.97	
[11/24 02:15:18 visual_prompt]: Best epoch 13: best metric: -14.540
[11/24 02:15:18 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/24 02:17:04 visual_prompt]: 	Training 100/553. train loss: 32.7932,	0.8266 s / batch. (data: 2.98e-04). ETA=11:01:24, max mem: 20.9 GB 
[11/24 02:18:47 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.0174 s / batch. (data: 2.09e-01). ETA=13:32:26, max mem: 20.9 GB 
[11/24 02:20:27 visual_prompt]: 	Training 300/553. train loss: 16.0893,	0.9069 s / batch. (data: 7.65e-02). ETA=12:02:38, max mem: 20.9 GB 
[11/24 02:22:08 visual_prompt]: 	Training 400/553. train loss: 9.4391,	0.8252 s / batch. (data: 3.34e-04). ETA=10:56:10, max mem: 20.9 GB 
[11/24 02:23:48 visual_prompt]: 	Training 500/553. train loss: 11.7443,	0.8400 s / batch. (data: 3.10e-04). ETA=11:06:34, max mem: 20.9 GB 
[11/24 02:24:40 visual_prompt]: Epoch 14 / 100: avg data time: 1.93e-01, avg batch time: 1.0155, average train loss: 26.6456
[11/24 02:25:38 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3052, average loss: 7.4371
[11/24 02:25:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.78	
[11/24 02:25:38 visual_prompt]: Best epoch 14: best metric: -7.437
[11/24 02:25:38 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/24 02:27:23 visual_prompt]: 	Training 100/553. train loss: 49.4517,	0.8115 s / batch. (data: 3.93e-04). ETA=10:41:51, max mem: 20.9 GB 
[11/24 02:29:03 visual_prompt]: 	Training 200/553. train loss: 73.2785,	0.8370 s / batch. (data: 5.47e-03). ETA=11:00:36, max mem: 20.9 GB 
[11/24 02:30:46 visual_prompt]: 	Training 300/553. train loss: 45.8299,	0.8265 s / batch. (data: 1.05e-02). ETA=10:50:59, max mem: 20.9 GB 
[11/24 02:32:25 visual_prompt]: 	Training 400/553. train loss: 3.9994,	0.8320 s / batch. (data: 3.35e-04). ETA=10:53:55, max mem: 20.9 GB 
[11/24 02:34:08 visual_prompt]: 	Training 500/553. train loss: 12.0135,	0.8059 s / batch. (data: 3.05e-04). ETA=10:32:04, max mem: 20.9 GB 
[11/24 02:35:02 visual_prompt]: Epoch 15 / 100: avg data time: 1.99e-01, avg batch time: 1.0195, average train loss: 42.6542
[11/24 02:36:00 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3061, average loss: 119.1599
[11/24 02:36:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.93	
[11/24 02:36:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/24 02:37:44 visual_prompt]: 	Training 100/553. train loss: 28.5121,	0.8269 s / batch. (data: 5.44e-03). ETA=10:46:26, max mem: 20.9 GB 
[11/24 02:39:25 visual_prompt]: 	Training 200/553. train loss: 4.4193,	0.8200 s / batch. (data: 2.80e-04). ETA=10:39:40, max mem: 20.9 GB 
[11/24 02:41:08 visual_prompt]: 	Training 300/553. train loss: 18.1549,	0.8320 s / batch. (data: 2.93e-04). ETA=10:47:38, max mem: 20.9 GB 
[11/24 02:42:49 visual_prompt]: 	Training 400/553. train loss: 8.0040,	0.8060 s / batch. (data: 3.03e-04). ETA=10:26:05, max mem: 20.9 GB 
[11/24 02:44:29 visual_prompt]: 	Training 500/553. train loss: 2.9585,	1.4466 s / batch. (data: 6.38e-01). ETA=18:41:16, max mem: 20.9 GB 
[11/24 02:45:23 visual_prompt]: Epoch 16 / 100: avg data time: 1.97e-01, avg batch time: 1.0177, average train loss: 32.4045
[11/24 02:46:21 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3060, average loss: 11.1458
[11/24 02:46:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.12	
[11/24 02:46:21 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/24 02:48:04 visual_prompt]: 	Training 100/553. train loss: 0.0228,	0.8301 s / batch. (data: 1.05e-02). ETA=10:41:17, max mem: 20.9 GB 
[11/24 02:49:47 visual_prompt]: 	Training 200/553. train loss: 133.8049,	0.8038 s / batch. (data: 3.03e-04). ETA=10:19:35, max mem: 20.9 GB 
[11/24 02:51:27 visual_prompt]: 	Training 300/553. train loss: 80.5348,	0.8219 s / batch. (data: 2.64e-04). ETA=10:32:11, max mem: 20.9 GB 
[11/24 02:53:08 visual_prompt]: 	Training 400/553. train loss: 0.2826,	1.1880 s / batch. (data: 3.55e-01). ETA=15:11:50, max mem: 20.9 GB 
[11/24 02:54:49 visual_prompt]: 	Training 500/553. train loss: 58.0921,	1.7435 s / batch. (data: 9.32e-01). ETA=22:15:18, max mem: 20.9 GB 
[11/24 02:55:42 visual_prompt]: Epoch 17 / 100: avg data time: 1.94e-01, avg batch time: 1.0150, average train loss: 34.7385
[11/24 02:56:40 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3069, average loss: 13.7672
[11/24 02:56:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 65.09	
[11/24 02:56:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/24 02:58:25 visual_prompt]: 	Training 100/553. train loss: 1.1119,	0.8443 s / batch. (data: 2.56e-02). ETA=10:44:26, max mem: 20.9 GB 
[11/24 03:00:09 visual_prompt]: 	Training 200/553. train loss: 7.7370,	0.8081 s / batch. (data: 3.00e-04). ETA=10:15:31, max mem: 20.9 GB 
[11/24 03:01:51 visual_prompt]: 	Training 300/553. train loss: 15.1899,	0.8538 s / batch. (data: 7.95e-03). ETA=10:48:54, max mem: 20.9 GB 
[11/24 03:03:33 visual_prompt]: 	Training 400/553. train loss: 19.8836,	0.8120 s / batch. (data: 3.17e-04). ETA=10:15:43, max mem: 20.9 GB 
[11/24 03:05:14 visual_prompt]: 	Training 500/553. train loss: 7.3998,	0.8408 s / batch. (data: 5.74e-03). ETA=10:36:09, max mem: 20.9 GB 
[11/24 03:06:06 visual_prompt]: Epoch 18 / 100: avg data time: 2.01e-01, avg batch time: 1.0224, average train loss: 38.3788
[11/24 03:07:04 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3079, average loss: 55.3526
[11/24 03:07:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.85	
[11/24 03:07:04 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/24 03:08:49 visual_prompt]: 	Training 100/553. train loss: 26.4222,	0.8187 s / batch. (data: 1.07e-02). ETA=10:17:23, max mem: 20.9 GB 
[11/24 03:10:30 visual_prompt]: 	Training 200/553. train loss: 3.7860,	0.8213 s / batch. (data: 3.40e-04). ETA=10:17:58, max mem: 20.9 GB 
[11/24 03:12:11 visual_prompt]: 	Training 300/553. train loss: 22.6057,	0.8320 s / batch. (data: 3.25e-04). ETA=10:24:38, max mem: 20.9 GB 
[11/24 03:13:54 visual_prompt]: 	Training 400/553. train loss: 12.2948,	0.8411 s / batch. (data: 8.08e-04). ETA=10:30:04, max mem: 20.9 GB 
[11/24 03:15:30 visual_prompt]: 	Training 500/553. train loss: 7.6475,	0.8448 s / batch. (data: 8.89e-03). ETA=10:31:25, max mem: 20.9 GB 
[11/24 03:16:23 visual_prompt]: Epoch 19 / 100: avg data time: 1.89e-01, avg batch time: 1.0116, average train loss: 27.2900
[11/24 03:17:21 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3077, average loss: 93.1748
[11/24 03:17:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.80	
[11/24 03:17:21 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/24 03:19:04 visual_prompt]: 	Training 100/553. train loss: 39.9212,	1.0074 s / batch. (data: 1.69e-01). ETA=12:30:25, max mem: 20.9 GB 
[11/24 03:20:47 visual_prompt]: 	Training 200/553. train loss: 2.5359,	0.8182 s / batch. (data: 7.93e-03). ETA=10:08:04, max mem: 20.9 GB 
[11/24 03:22:28 visual_prompt]: 	Training 300/553. train loss: 85.2379,	0.8306 s / batch. (data: 1.07e-02). ETA=10:15:55, max mem: 20.9 GB 
[11/24 03:24:09 visual_prompt]: 	Training 400/553. train loss: 90.9195,	0.8235 s / batch. (data: 1.05e-02). ETA=10:09:15, max mem: 20.9 GB 
[11/24 03:25:49 visual_prompt]: 	Training 500/553. train loss: 29.1014,	0.8076 s / batch. (data: 2.87e-04). ETA=9:56:11, max mem: 20.9 GB 
[11/24 03:26:43 visual_prompt]: Epoch 20 / 100: avg data time: 1.97e-01, avg batch time: 1.0169, average train loss: 36.6560
[11/24 03:27:42 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3059, average loss: 14.7902
[11/24 03:27:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.40	
[11/24 03:27:42 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/24 03:29:29 visual_prompt]: 	Training 100/553. train loss: 19.7591,	1.0985 s / batch. (data: 2.68e-01). ETA=13:28:09, max mem: 20.9 GB 
[11/24 03:31:09 visual_prompt]: 	Training 200/553. train loss: 0.0096,	0.8224 s / batch. (data: 3.00e-04). ETA=10:03:40, max mem: 20.9 GB 
[11/24 03:32:50 visual_prompt]: 	Training 300/553. train loss: 136.8840,	1.0470 s / batch. (data: 2.26e-01). ETA=12:46:43, max mem: 20.9 GB 
[11/24 03:34:31 visual_prompt]: 	Training 400/553. train loss: 25.3831,	0.8560 s / batch. (data: 1.19e-02). ETA=10:25:26, max mem: 20.9 GB 
[11/24 03:36:13 visual_prompt]: 	Training 500/553. train loss: 31.8285,	0.8298 s / batch. (data: 5.42e-03). ETA=10:04:53, max mem: 20.9 GB 
[11/24 03:37:05 visual_prompt]: Epoch 21 / 100: avg data time: 1.96e-01, avg batch time: 1.0178, average train loss: 28.5649
[11/24 03:38:02 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3066, average loss: 14.4644
[11/24 03:38:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 67.94	
[11/24 03:38:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/24 03:39:46 visual_prompt]: 	Training 100/553. train loss: 26.2604,	0.8082 s / batch. (data: 3.15e-04). ETA=9:47:08, max mem: 20.9 GB 
[11/24 03:41:28 visual_prompt]: 	Training 200/553. train loss: 38.4190,	0.8228 s / batch. (data: 2.94e-04). ETA=9:56:20, max mem: 20.9 GB 
[11/24 03:43:07 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8069 s / batch. (data: 5.45e-03). ETA=9:43:30, max mem: 20.9 GB 
[11/24 03:44:49 visual_prompt]: 	Training 400/553. train loss: 8.0782,	0.8179 s / batch. (data: 3.05e-04). ETA=9:50:03, max mem: 20.9 GB 
[11/24 03:46:30 visual_prompt]: 	Training 500/553. train loss: 24.0354,	0.8399 s / batch. (data: 4.34e-04). ETA=10:04:34, max mem: 20.9 GB 
[11/24 03:47:24 visual_prompt]: Epoch 22 / 100: avg data time: 1.94e-01, avg batch time: 1.0152, average train loss: 31.6379
[11/24 03:48:22 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3078, average loss: 22.8661
[11/24 03:48:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.05	
[11/24 03:48:22 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/24 03:50:09 visual_prompt]: 	Training 100/553. train loss: 69.6765,	0.8403 s / batch. (data: 1.23e-02). ETA=10:02:40, max mem: 20.9 GB 
[11/24 03:51:51 visual_prompt]: 	Training 200/553. train loss: 26.6767,	0.8280 s / batch. (data: 7.94e-03). ETA=9:52:28, max mem: 20.9 GB 
[11/24 03:53:33 visual_prompt]: 	Training 300/553. train loss: 4.3607,	0.8359 s / batch. (data: 1.12e-03). ETA=9:56:44, max mem: 20.9 GB 
[11/24 03:55:12 visual_prompt]: 	Training 400/553. train loss: 16.4394,	0.8240 s / batch. (data: 3.44e-04). ETA=9:46:53, max mem: 20.9 GB 
[11/24 03:56:51 visual_prompt]: 	Training 500/553. train loss: 0.2407,	0.8459 s / batch. (data: 5.43e-03). ETA=10:01:02, max mem: 20.9 GB 
[11/24 03:57:44 visual_prompt]: Epoch 23 / 100: avg data time: 1.93e-01, avg batch time: 1.0158, average train loss: 26.8034
[11/24 03:58:42 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3045, average loss: 13.1462
[11/24 03:58:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.32	
[11/24 03:58:42 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/24 04:00:24 visual_prompt]: 	Training 100/553. train loss: 6.5169,	0.8417 s / batch. (data: 3.08e-04). ETA=9:55:54, max mem: 20.9 GB 
[11/24 04:02:04 visual_prompt]: 	Training 200/553. train loss: 11.6133,	0.8320 s / batch. (data: 2.82e-04). ETA=9:47:41, max mem: 20.9 GB 
[11/24 04:03:46 visual_prompt]: 	Training 300/553. train loss: 7.3411,	0.8756 s / batch. (data: 6.37e-02). ETA=10:17:01, max mem: 20.9 GB 
[11/24 04:05:28 visual_prompt]: 	Training 400/553. train loss: 23.0182,	0.8440 s / batch. (data: 3.04e-04). ETA=9:53:20, max mem: 20.9 GB 
[11/24 04:07:10 visual_prompt]: 	Training 500/553. train loss: 8.7479,	0.8165 s / batch. (data: 5.45e-03). ETA=9:32:40, max mem: 20.9 GB 
[11/24 04:08:03 visual_prompt]: Epoch 24 / 100: avg data time: 1.94e-01, avg batch time: 1.0147, average train loss: 27.4048
[11/24 04:09:01 visual_prompt]: Inference (val):avg data time: 2.25e-04, avg batch time: 0.3083, average loss: 12.1995
[11/24 04:09:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 65.38	
[11/24 04:09:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/24 04:10:49 visual_prompt]: 	Training 100/553. train loss: 59.5628,	0.8214 s / batch. (data: 7.73e-03). ETA=9:34:01, max mem: 20.9 GB 
[11/24 04:12:27 visual_prompt]: 	Training 200/553. train loss: 14.7123,	1.1480 s / batch. (data: 2.93e-01). ETA=13:20:18, max mem: 20.9 GB 
[11/24 04:14:07 visual_prompt]: 	Training 300/553. train loss: 24.4865,	0.8244 s / batch. (data: 3.91e-03). ETA=9:33:21, max mem: 20.9 GB 
[11/24 04:15:47 visual_prompt]: 	Training 400/553. train loss: 0.6782,	1.3480 s / batch. (data: 5.30e-01). ETA=15:35:14, max mem: 20.9 GB 
[11/24 04:17:28 visual_prompt]: 	Training 500/553. train loss: 9.5160,	1.5493 s / batch. (data: 7.33e-01). ETA=17:52:20, max mem: 20.9 GB 
[11/24 04:18:21 visual_prompt]: Epoch 25 / 100: avg data time: 1.90e-01, avg batch time: 1.0129, average train loss: 27.0481
[11/24 04:19:19 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3066, average loss: 73.2002
[11/24 04:19:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.01	
[11/24 04:19:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/24 04:21:04 visual_prompt]: 	Training 100/553. train loss: 6.2010,	0.8214 s / batch. (data: 4.37e-04). ETA=9:26:24, max mem: 20.9 GB 
[11/24 04:22:45 visual_prompt]: 	Training 200/553. train loss: 49.9791,	1.8560 s / batch. (data: 1.03e+00). ETA=21:16:45, max mem: 20.9 GB 
[11/24 04:24:27 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8412 s / batch. (data: 1.11e-02). ETA=9:37:16, max mem: 20.9 GB 
[11/24 04:26:07 visual_prompt]: 	Training 400/553. train loss: 3.7007,	0.8225 s / batch. (data: 5.43e-03). ETA=9:23:03, max mem: 20.9 GB 
[11/24 04:27:45 visual_prompt]: 	Training 500/553. train loss: 2.9932,	0.8406 s / batch. (data: 5.43e-03). ETA=9:34:04, max mem: 20.9 GB 
[11/24 04:28:37 visual_prompt]: Epoch 26 / 100: avg data time: 1.89e-01, avg batch time: 1.0097, average train loss: 27.0992
[11/24 04:29:35 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3046, average loss: 12.7327
[11/24 04:29:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 70.10	
[11/24 04:29:35 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/24 04:31:21 visual_prompt]: 	Training 100/553. train loss: 9.6838,	0.8440 s / batch. (data: 3.17e-04). ETA=9:34:13, max mem: 20.9 GB 
[11/24 04:33:01 visual_prompt]: 	Training 200/553. train loss: 61.5466,	1.4340 s / batch. (data: 6.16e-01). ETA=16:13:13, max mem: 20.9 GB 
[11/24 04:34:42 visual_prompt]: 	Training 300/553. train loss: 41.0477,	0.8077 s / batch. (data: 3.26e-04). ETA=9:06:48, max mem: 20.9 GB 
[11/24 04:36:23 visual_prompt]: 	Training 400/553. train loss: 66.3537,	0.8150 s / batch. (data: 2.98e-04). ETA=9:10:26, max mem: 20.9 GB 
[11/24 04:38:05 visual_prompt]: 	Training 500/553. train loss: 35.7847,	0.8280 s / batch. (data: 3.12e-04). ETA=9:17:48, max mem: 20.9 GB 
[11/24 04:38:55 visual_prompt]: Epoch 27 / 100: avg data time: 1.90e-01, avg batch time: 1.0123, average train loss: 27.1342
[11/24 04:39:53 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3074, average loss: 86.1610
[11/24 04:39:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.93	
[11/24 04:39:53 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/24 04:41:37 visual_prompt]: 	Training 100/553. train loss: 76.4337,	0.8337 s / batch. (data: 3.01e-04). ETA=9:19:33, max mem: 20.9 GB 
[11/24 04:43:18 visual_prompt]: 	Training 200/553. train loss: 29.6996,	0.8147 s / batch. (data: 3.33e-04). ETA=9:05:27, max mem: 20.9 GB 
[11/24 04:45:00 visual_prompt]: 	Training 300/553. train loss: 10.3368,	1.5938 s / batch. (data: 7.68e-01). ETA=17:44:23, max mem: 20.9 GB 
[11/24 04:46:40 visual_prompt]: 	Training 400/553. train loss: 18.3533,	0.8140 s / batch. (data: 3.05e-04). ETA=9:02:15, max mem: 20.9 GB 
[11/24 04:48:20 visual_prompt]: 	Training 500/553. train loss: 108.3515,	0.8520 s / batch. (data: 8.04e-04). ETA=9:26:08, max mem: 20.9 GB 
[11/24 04:49:13 visual_prompt]: Epoch 28 / 100: avg data time: 1.91e-01, avg batch time: 1.0129, average train loss: 27.1525
[11/24 04:50:11 visual_prompt]: Inference (val):avg data time: 4.07e-04, avg batch time: 0.3081, average loss: 49.8572
[11/24 04:50:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.52	
[11/24 04:50:11 visual_prompt]: Stopping early.
[11/24 04:50:11 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 04:50:11 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 04:50:11 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/24 04:50:11 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 04:50:11 visual_prompt]: Training with config:
[11/24 04:50:11 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/24 04:50:11 visual_prompt]: Loading training data...
[11/24 04:50:11 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 04:50:11 visual_prompt]: Loading validation data...
[11/24 04:50:11 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 04:50:11 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 04:50:14 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 04:50:14 visual_prompt]: tuned percent:0.525
[11/24 04:50:14 visual_prompt]: Device used for model: 0
[11/24 04:50:14 visual_prompt]: Setting up Evaluator...
[11/24 04:50:14 visual_prompt]: Setting up Trainer...
[11/24 04:50:14 visual_prompt]: 	Setting up the optimizer...
[11/24 04:50:14 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 04:51:59 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8320 s / batch. (data: 2.84e-04). ETA=12:45:26, max mem: 20.9 GB 
[11/24 04:53:38 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8426 s / batch. (data: 2.26e-02). ETA=12:53:47, max mem: 20.9 GB 
[11/24 04:55:22 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.2825 s / batch. (data: 4.62e-01). ETA=19:35:37, max mem: 20.9 GB 
[11/24 04:57:02 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8220 s / batch. (data: 2.83e-04). ETA=12:32:10, max mem: 20.9 GB 
[11/24 04:58:45 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8363 s / batch. (data: 6.07e-03). ETA=12:43:51, max mem: 20.9 GB 
[11/24 04:59:38 visual_prompt]: Epoch 1 / 100: avg data time: 1.95e-01, avg batch time: 1.0197, average train loss: 1.5403
[11/24 05:00:36 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3057, average loss: 1.5201
[11/24 05:00:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 05:00:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/24 05:02:20 visual_prompt]: 	Training 100/553. train loss: 2.9190,	0.9022 s / batch. (data: 8.50e-02). ETA=13:41:44, max mem: 20.9 GB 
[11/24 05:04:01 visual_prompt]: 	Training 200/553. train loss: 0.0037,	0.8120 s / batch. (data: 3.36e-04). ETA=12:18:13, max mem: 20.9 GB 
[11/24 05:05:44 visual_prompt]: 	Training 300/553. train loss: 7.8096,	1.0489 s / batch. (data: 2.27e-01). ETA=15:51:51, max mem: 20.9 GB 
[11/24 05:07:23 visual_prompt]: 	Training 400/553. train loss: 0.6231,	0.8218 s / batch. (data: 3.22e-04). ETA=12:24:23, max mem: 20.9 GB 
[11/24 05:09:06 visual_prompt]: 	Training 500/553. train loss: 3.2929,	0.8289 s / batch. (data: 3.18e-04). ETA=12:29:26, max mem: 20.9 GB 
[11/24 05:09:57 visual_prompt]: Epoch 2 / 100: avg data time: 1.92e-01, avg batch time: 1.0152, average train loss: 3.4209
[11/24 05:10:55 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3064, average loss: 19.1750
[11/24 05:10:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.67	
[11/24 05:10:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/24 05:12:39 visual_prompt]: 	Training 100/553. train loss: 0.7196,	0.9761 s / batch. (data: 1.57e-01). ETA=14:39:58, max mem: 20.9 GB 
[11/24 05:14:21 visual_prompt]: 	Training 200/553. train loss: 1.8945,	2.1160 s / batch. (data: 1.28e+00). ETA=1 day, 7:44:12, max mem: 20.9 GB 
[11/24 05:16:02 visual_prompt]: 	Training 300/553. train loss: 4.1691,	0.8146 s / batch. (data: 5.41e-03). ETA=12:11:44, max mem: 20.9 GB 
[11/24 05:17:43 visual_prompt]: 	Training 400/553. train loss: 55.2133,	0.8445 s / batch. (data: 3.10e-04). ETA=12:37:07, max mem: 20.9 GB 
[11/24 05:19:26 visual_prompt]: 	Training 500/553. train loss: 7.2534,	1.3080 s / batch. (data: 4.91e-01). ETA=19:30:30, max mem: 20.9 GB 
[11/24 05:20:17 visual_prompt]: Epoch 3 / 100: avg data time: 1.90e-01, avg batch time: 1.0146, average train loss: 6.8817
[11/24 05:21:14 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3075, average loss: 4.1820
[11/24 05:21:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.68	
[11/24 05:21:14 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/24 05:23:01 visual_prompt]: 	Training 100/553. train loss: 20.7751,	0.8374 s / batch. (data: 9.34e-03). ETA=12:27:13, max mem: 20.9 GB 
[11/24 05:24:43 visual_prompt]: 	Training 200/553. train loss: 2.4443,	0.8328 s / batch. (data: 2.52e-02). ETA=12:21:43, max mem: 20.9 GB 
[11/24 05:26:24 visual_prompt]: 	Training 300/553. train loss: 2.6300,	1.1540 s / batch. (data: 3.36e-01). ETA=17:05:54, max mem: 20.9 GB 
[11/24 05:28:02 visual_prompt]: 	Training 400/553. train loss: 5.1956,	1.5385 s / batch. (data: 7.32e-01). ETA=22:45:11, max mem: 20.9 GB 
[11/24 05:29:46 visual_prompt]: 	Training 500/553. train loss: 49.1465,	3.7823 s / batch. (data: 2.98e+00). ETA=2 days, 7:49:53, max mem: 20.9 GB 
[11/24 05:30:40 visual_prompt]: Epoch 4 / 100: avg data time: 1.99e-01, avg batch time: 1.0218, average train loss: 9.3442
[11/24 05:31:38 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3077, average loss: 2.7230
[11/24 05:31:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.14	
[11/24 05:31:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/24 05:33:22 visual_prompt]: 	Training 100/553. train loss: 0.3351,	0.8480 s / batch. (data: 7.92e-03). ETA=12:28:54, max mem: 20.9 GB 
[11/24 05:35:03 visual_prompt]: 	Training 200/553. train loss: 12.2037,	1.2560 s / batch. (data: 4.19e-01). ETA=18:27:07, max mem: 20.9 GB 
[11/24 05:36:46 visual_prompt]: 	Training 300/553. train loss: 47.6977,	0.8344 s / batch. (data: 3.09e-04). ETA=12:14:07, max mem: 20.9 GB 
[11/24 05:38:26 visual_prompt]: 	Training 400/553. train loss: 16.4583,	0.8299 s / batch. (data: 3.13e-04). ETA=12:08:46, max mem: 20.9 GB 
[11/24 05:40:07 visual_prompt]: 	Training 500/553. train loss: 4.7859,	0.8233 s / batch. (data: 1.13e-02). ETA=12:01:34, max mem: 20.9 GB 
[11/24 05:41:01 visual_prompt]: Epoch 5 / 100: avg data time: 1.94e-01, avg batch time: 1.0176, average train loss: 13.9010
[11/24 05:41:58 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3050, average loss: 92.5483
[11/24 05:41:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[11/24 05:41:58 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/24 05:43:45 visual_prompt]: 	Training 100/553. train loss: 1.8484,	0.8264 s / batch. (data: 7.66e-04). ETA=12:02:11, max mem: 20.9 GB 
[11/24 05:45:26 visual_prompt]: 	Training 200/553. train loss: 11.7367,	0.8371 s / batch. (data: 7.96e-03). ETA=12:10:10, max mem: 20.9 GB 
[11/24 05:47:05 visual_prompt]: 	Training 300/553. train loss: 3.2738,	0.8332 s / batch. (data: 5.45e-03). ETA=12:05:20, max mem: 20.9 GB 
[11/24 05:48:51 visual_prompt]: 	Training 400/553. train loss: 8.4288,	0.8305 s / batch. (data: 9.29e-03). ETA=12:01:37, max mem: 20.9 GB 
[11/24 05:50:30 visual_prompt]: 	Training 500/553. train loss: 2.4603,	0.8296 s / batch. (data: 5.46e-03). ETA=11:59:26, max mem: 20.9 GB 
[11/24 05:51:22 visual_prompt]: Epoch 6 / 100: avg data time: 1.97e-01, avg batch time: 1.0195, average train loss: 21.8691
[11/24 05:52:20 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3063, average loss: 2.3365
[11/24 05:52:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.85	
[11/24 05:52:20 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/24 05:54:04 visual_prompt]: 	Training 100/553. train loss: 13.2510,	0.8280 s / batch. (data: 3.26e-04). ETA=11:55:58, max mem: 20.9 GB 
[11/24 05:55:45 visual_prompt]: 	Training 200/553. train loss: 23.3658,	0.8164 s / batch. (data: 7.95e-03). ETA=11:44:33, max mem: 20.9 GB 
[11/24 05:57:30 visual_prompt]: 	Training 300/553. train loss: 12.4615,	1.8640 s / batch. (data: 1.06e+00). ETA=1 day, 2:45:34, max mem: 20.9 GB 
[11/24 05:59:11 visual_prompt]: 	Training 400/553. train loss: 9.6258,	1.9600 s / batch. (data: 1.13e+00). ETA=1 day, 4:05:00, max mem: 20.9 GB 
[11/24 06:00:50 visual_prompt]: 	Training 500/553. train loss: 4.9146,	1.0840 s / batch. (data: 2.41e-01). ETA=15:30:06, max mem: 20.9 GB 
[11/24 06:01:42 visual_prompt]: Epoch 7 / 100: avg data time: 1.92e-01, avg batch time: 1.0151, average train loss: 20.0644
[11/24 06:02:40 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3066, average loss: 8.4642
[11/24 06:02:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.60	
[11/24 06:02:40 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/24 06:04:23 visual_prompt]: 	Training 100/553. train loss: 9.8224,	0.8314 s / batch. (data: 5.47e-03). ETA=11:51:15, max mem: 20.9 GB 
[11/24 06:06:06 visual_prompt]: 	Training 200/553. train loss: 39.4100,	0.8111 s / batch. (data: 3.15e-04). ETA=11:32:31, max mem: 20.9 GB 
[11/24 06:07:48 visual_prompt]: 	Training 300/553. train loss: 13.7431,	0.8240 s / batch. (data: 3.06e-04). ETA=11:42:09, max mem: 20.9 GB 
[11/24 06:09:28 visual_prompt]: 	Training 400/553. train loss: 51.2663,	0.8254 s / batch. (data: 1.05e-02). ETA=11:42:00, max mem: 20.9 GB 
[11/24 06:11:09 visual_prompt]: 	Training 500/553. train loss: 139.0039,	1.4280 s / batch. (data: 5.84e-01). ETA=20:12:06, max mem: 20.9 GB 
[11/24 06:12:04 visual_prompt]: Epoch 8 / 100: avg data time: 1.97e-01, avg batch time: 1.0196, average train loss: 25.8392
[11/24 06:13:02 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3071, average loss: 7.0291
[11/24 06:13:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.43	
[11/24 06:13:02 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/24 06:14:47 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8209 s / batch. (data: 3.06e-04). ETA=11:34:41, max mem: 20.9 GB 
[11/24 06:16:28 visual_prompt]: 	Training 200/553. train loss: 13.8168,	0.8232 s / batch. (data: 5.61e-03). ETA=11:35:17, max mem: 20.9 GB 
[11/24 06:18:09 visual_prompt]: 	Training 300/553. train loss: 3.5134,	1.9172 s / batch. (data: 1.09e+00). ETA=1 day, 2:56:03, max mem: 20.9 GB 
[11/24 06:19:52 visual_prompt]: 	Training 400/553. train loss: 10.2903,	0.8320 s / batch. (data: 3.17e-04). ETA=11:39:55, max mem: 20.9 GB 
[11/24 06:21:34 visual_prompt]: 	Training 500/553. train loss: 21.3491,	1.0280 s / batch. (data: 2.09e-01). ETA=14:23:06, max mem: 20.9 GB 
[11/24 06:22:26 visual_prompt]: Epoch 9 / 100: avg data time: 1.96e-01, avg batch time: 1.0191, average train loss: 23.0162
[11/24 06:23:23 visual_prompt]: Inference (val):avg data time: 1.80e-04, avg batch time: 0.3068, average loss: 42.8388
[11/24 06:23:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.46	
[11/24 06:23:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/24 06:25:11 visual_prompt]: 	Training 100/553. train loss: 12.5214,	0.8400 s / batch. (data: 3.10e-04). ETA=11:43:08, max mem: 20.9 GB 
[11/24 06:26:50 visual_prompt]: 	Training 200/553. train loss: 6.6046,	0.8160 s / batch. (data: 3.06e-04). ETA=11:21:41, max mem: 20.9 GB 
[11/24 06:28:32 visual_prompt]: 	Training 300/553. train loss: 6.6750,	0.8599 s / batch. (data: 3.40e-04). ETA=11:56:57, max mem: 20.9 GB 
[11/24 06:30:10 visual_prompt]: 	Training 400/553. train loss: 5.2920,	0.9138 s / batch. (data: 7.29e-02). ETA=12:40:18, max mem: 20.9 GB 
[11/24 06:31:52 visual_prompt]: 	Training 500/553. train loss: 19.9925,	1.1287 s / batch. (data: 3.14e-01). ETA=15:37:15, max mem: 20.9 GB 
[11/24 06:32:46 visual_prompt]: Epoch 10 / 100: avg data time: 1.95e-01, avg batch time: 1.0165, average train loss: 30.4111
[11/24 06:33:44 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3055, average loss: 0.9898
[11/24 06:33:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.75	
[11/24 06:33:44 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/24 06:35:31 visual_prompt]: 	Training 100/553. train loss: 15.8778,	0.8341 s / batch. (data: 3.11e-04). ETA=11:30:28, max mem: 20.9 GB 
[11/24 06:37:13 visual_prompt]: 	Training 200/553. train loss: 24.1415,	0.8662 s / batch. (data: 3.03e-02). ETA=11:55:35, max mem: 20.9 GB 
[11/24 06:38:55 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.4208 s / batch. (data: 1.60e+00). ETA=1 day, 9:15:57, max mem: 20.9 GB 
[11/24 06:40:35 visual_prompt]: 	Training 400/553. train loss: 43.0872,	0.8169 s / batch. (data: 3.22e-04). ETA=11:12:09, max mem: 20.9 GB 
[11/24 06:42:15 visual_prompt]: 	Training 500/553. train loss: 45.0347,	0.8185 s / batch. (data: 1.28e-02). ETA=11:12:05, max mem: 20.9 GB 
[11/24 06:43:07 visual_prompt]: Epoch 11 / 100: avg data time: 1.95e-01, avg batch time: 1.0183, average train loss: 34.2581
[11/24 06:44:05 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3056, average loss: 21.7759
[11/24 06:44:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.63	
[11/24 06:44:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/24 06:45:51 visual_prompt]: 	Training 100/553. train loss: 39.2219,	0.8291 s / batch. (data: 3.07e-04). ETA=11:18:42, max mem: 20.9 GB 
[11/24 06:47:33 visual_prompt]: 	Training 200/553. train loss: 7.6297,	0.8194 s / batch. (data: 3.14e-04). ETA=11:09:23, max mem: 20.9 GB 
[11/24 06:49:13 visual_prompt]: 	Training 300/553. train loss: 12.9869,	0.8521 s / batch. (data: 7.94e-03). ETA=11:34:39, max mem: 20.9 GB 
[11/24 06:50:53 visual_prompt]: 	Training 400/553. train loss: 20.0534,	0.8240 s / batch. (data: 7.97e-03). ETA=11:10:24, max mem: 20.9 GB 
[11/24 06:52:35 visual_prompt]: 	Training 500/553. train loss: 7.4051,	0.8396 s / batch. (data: 5.42e-03). ETA=11:21:41, max mem: 20.9 GB 
[11/24 06:53:26 visual_prompt]: Epoch 12 / 100: avg data time: 1.92e-01, avg batch time: 1.0147, average train loss: 34.2184
[11/24 06:54:24 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3068, average loss: 33.8616
[11/24 06:54:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.16	
[11/24 06:54:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/24 06:56:10 visual_prompt]: 	Training 100/553. train loss: 18.5837,	1.7166 s / batch. (data: 9.01e-01). ETA=23:09:23, max mem: 20.9 GB 
[11/24 06:57:48 visual_prompt]: 	Training 200/553. train loss: 119.0935,	0.8420 s / batch. (data: 5.89e-03). ETA=11:20:04, max mem: 20.9 GB 
[11/24 06:59:30 visual_prompt]: 	Training 300/553. train loss: 21.8950,	1.8360 s / batch. (data: 1.01e+00). ETA=1 day, 0:39:56, max mem: 20.9 GB 
[11/24 07:01:10 visual_prompt]: 	Training 400/553. train loss: 13.5954,	0.8568 s / batch. (data: 2.07e-02). ETA=11:29:11, max mem: 20.9 GB 
[11/24 07:02:52 visual_prompt]: 	Training 500/553. train loss: 20.9357,	0.8265 s / batch. (data: 5.44e-03). ETA=11:03:28, max mem: 20.9 GB 
[11/24 07:03:44 visual_prompt]: Epoch 13 / 100: avg data time: 1.91e-01, avg batch time: 1.0118, average train loss: 36.6054
[11/24 07:04:41 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3054, average loss: 12.1257
[11/24 07:04:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[11/24 07:04:41 visual_prompt]: Best epoch 13: best metric: -12.126
[11/24 07:04:41 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/24 07:06:27 visual_prompt]: 	Training 100/553. train loss: 1.8615,	0.8104 s / batch. (data: 3.03e-04). ETA=10:48:26, max mem: 20.9 GB 
[11/24 07:08:07 visual_prompt]: 	Training 200/553. train loss: 6.4021,	1.3647 s / batch. (data: 5.38e-01). ETA=18:09:43, max mem: 20.9 GB 
[11/24 07:09:48 visual_prompt]: 	Training 300/553. train loss: 68.5546,	0.8364 s / batch. (data: 1.05e-02). ETA=11:06:29, max mem: 20.9 GB 
[11/24 07:11:29 visual_prompt]: 	Training 400/553. train loss: 5.1330,	0.8640 s / batch. (data: 7.93e-03). ETA=11:27:01, max mem: 20.9 GB 
[11/24 07:13:10 visual_prompt]: 	Training 500/553. train loss: 9.8359,	0.8145 s / batch. (data: 7.95e-03). ETA=10:46:18, max mem: 20.9 GB 
[11/24 07:14:01 visual_prompt]: Epoch 14 / 100: avg data time: 1.89e-01, avg batch time: 1.0109, average train loss: 32.7403
[11/24 07:14:58 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.3073, average loss: 76.2857
[11/24 07:14:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.90	
[11/24 07:14:58 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/24 07:16:43 visual_prompt]: 	Training 100/553. train loss: 58.4551,	0.8095 s / batch. (data: 3.20e-04). ETA=10:40:17, max mem: 20.9 GB 
[11/24 07:18:22 visual_prompt]: 	Training 200/553. train loss: 188.4184,	0.8239 s / batch. (data: 4.43e-04). ETA=10:50:19, max mem: 20.9 GB 
[11/24 07:20:05 visual_prompt]: 	Training 300/553. train loss: 24.2248,	0.8113 s / batch. (data: 3.07e-04). ETA=10:39:01, max mem: 20.9 GB 
[11/24 07:21:44 visual_prompt]: 	Training 400/553. train loss: 5.2649,	1.3189 s / batch. (data: 5.00e-01). ETA=17:16:35, max mem: 20.9 GB 
[11/24 07:23:26 visual_prompt]: 	Training 500/553. train loss: 15.9856,	0.8120 s / batch. (data: 3.16e-04). ETA=10:36:50, max mem: 20.9 GB 
[11/24 07:24:19 visual_prompt]: Epoch 15 / 100: avg data time: 1.92e-01, avg batch time: 1.0135, average train loss: 36.2378
[11/24 07:25:17 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3077, average loss: 21.1190
[11/24 07:25:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.29	
[11/24 07:25:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/24 07:27:01 visual_prompt]: 	Training 100/553. train loss: 136.7389,	0.8236 s / batch. (data: 3.34e-04). ETA=10:43:49, max mem: 20.9 GB 
[11/24 07:28:42 visual_prompt]: 	Training 200/553. train loss: 59.3486,	0.8301 s / batch. (data: 1.10e-02). ETA=10:47:31, max mem: 20.9 GB 
[11/24 07:30:22 visual_prompt]: 	Training 300/553. train loss: 30.4218,	0.8185 s / batch. (data: 1.17e-02). ETA=10:37:07, max mem: 20.9 GB 
[11/24 07:32:03 visual_prompt]: 	Training 400/553. train loss: 17.3393,	0.8200 s / batch. (data: 3.05e-04). ETA=10:36:55, max mem: 20.9 GB 
[11/24 07:33:43 visual_prompt]: 	Training 500/553. train loss: 11.4081,	1.5898 s / batch. (data: 7.83e-01). ETA=20:32:12, max mem: 20.9 GB 
[11/24 07:34:36 visual_prompt]: Epoch 16 / 100: avg data time: 1.91e-01, avg batch time: 1.0121, average train loss: 32.5930
[11/24 07:35:34 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3068, average loss: 44.3229
[11/24 07:35:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.94	
[11/24 07:35:34 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/24 07:37:18 visual_prompt]: 	Training 100/553. train loss: 55.8716,	0.8337 s / batch. (data: 2.57e-02). ETA=10:44:04, max mem: 20.9 GB 
[11/24 07:39:00 visual_prompt]: 	Training 200/553. train loss: 54.9315,	0.8092 s / batch. (data: 3.23e-04). ETA=10:23:45, max mem: 20.9 GB 
[11/24 07:40:40 visual_prompt]: 	Training 300/553. train loss: 15.8055,	0.8380 s / batch. (data: 2.88e-04). ETA=10:44:36, max mem: 20.9 GB 
[11/24 07:42:20 visual_prompt]: 	Training 400/553. train loss: 2.8833,	1.3200 s / batch. (data: 5.00e-01). ETA=16:53:08, max mem: 20.9 GB 
[11/24 07:44:00 visual_prompt]: 	Training 500/553. train loss: 5.9224,	1.6239 s / batch. (data: 8.04e-01). ETA=20:43:40, max mem: 20.9 GB 
[11/24 07:44:54 visual_prompt]: Epoch 17 / 100: avg data time: 1.90e-01, avg batch time: 1.0120, average train loss: 31.9066
[11/24 07:45:52 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3062, average loss: 22.5387
[11/24 07:45:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.52	
[11/24 07:45:52 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/24 07:47:37 visual_prompt]: 	Training 100/553. train loss: 33.3468,	0.8160 s / batch. (data: 5.42e-03). ETA=10:22:52, max mem: 20.9 GB 
[11/24 07:49:20 visual_prompt]: 	Training 200/553. train loss: 15.3985,	0.8266 s / batch. (data: 5.41e-03). ETA=10:29:35, max mem: 20.9 GB 
[11/24 07:51:01 visual_prompt]: 	Training 300/553. train loss: 36.3336,	0.8113 s / batch. (data: 3.04e-04). ETA=10:16:36, max mem: 20.9 GB 
[11/24 07:52:41 visual_prompt]: 	Training 400/553. train loss: 26.4739,	0.8320 s / batch. (data: 3.07e-04). ETA=10:30:55, max mem: 20.9 GB 
[11/24 07:54:21 visual_prompt]: 	Training 500/553. train loss: 26.0536,	0.8281 s / batch. (data: 2.81e-04). ETA=10:26:34, max mem: 20.9 GB 
[11/24 07:55:12 visual_prompt]: Epoch 18 / 100: avg data time: 1.92e-01, avg batch time: 1.0141, average train loss: 32.3624
[11/24 07:56:11 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3066, average loss: 28.3415
[11/24 07:56:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.76	
[11/24 07:56:11 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/24 07:57:56 visual_prompt]: 	Training 100/553. train loss: 73.0952,	0.8320 s / batch. (data: 2.99e-04). ETA=10:27:24, max mem: 20.9 GB 
[11/24 07:59:37 visual_prompt]: 	Training 200/553. train loss: 18.0639,	0.8405 s / batch. (data: 2.45e-02). ETA=10:32:26, max mem: 20.9 GB 
[11/24 08:01:17 visual_prompt]: 	Training 300/553. train loss: 0.0051,	1.0732 s / batch. (data: 2.65e-01). ETA=13:25:41, max mem: 20.9 GB 
[11/24 08:03:00 visual_prompt]: 	Training 400/553. train loss: 8.9777,	0.8357 s / batch. (data: 8.34e-04). ETA=10:25:59, max mem: 20.9 GB 
[11/24 08:04:36 visual_prompt]: 	Training 500/553. train loss: 29.4289,	0.8168 s / batch. (data: 3.19e-04). ETA=10:10:28, max mem: 20.9 GB 
[11/24 08:05:29 visual_prompt]: Epoch 19 / 100: avg data time: 1.88e-01, avg batch time: 1.0103, average train loss: 37.7777
[11/24 08:06:27 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3079, average loss: 163.5188
[11/24 08:06:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.06	
[11/24 08:06:27 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/24 08:08:10 visual_prompt]: 	Training 100/553. train loss: 11.4015,	0.8573 s / batch. (data: 2.73e-04). ETA=10:38:33, max mem: 20.9 GB 
[11/24 08:09:53 visual_prompt]: 	Training 200/553. train loss: 34.3164,	0.8447 s / batch. (data: 1.56e-02). ETA=10:27:47, max mem: 20.9 GB 
[11/24 08:11:33 visual_prompt]: 	Training 300/553. train loss: 34.0807,	0.8415 s / batch. (data: 9.19e-04). ETA=10:24:03, max mem: 20.9 GB 
[11/24 08:13:14 visual_prompt]: 	Training 400/553. train loss: 1.3187,	0.8280 s / batch. (data: 3.16e-04). ETA=10:12:38, max mem: 20.9 GB 
[11/24 08:14:54 visual_prompt]: 	Training 500/553. train loss: 16.2321,	0.8201 s / batch. (data: 2.90e-04). ETA=10:05:22, max mem: 20.9 GB 
[11/24 08:15:48 visual_prompt]: Epoch 20 / 100: avg data time: 1.92e-01, avg batch time: 1.0136, average train loss: 33.0191
[11/24 08:16:45 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3049, average loss: 21.6157
[11/24 08:16:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.95	
[11/24 08:16:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/24 08:18:33 visual_prompt]: 	Training 100/553. train loss: 13.6234,	0.8200 s / batch. (data: 3.19e-04). ETA=10:03:14, max mem: 20.9 GB 
[11/24 08:20:12 visual_prompt]: 	Training 200/553. train loss: 70.9600,	0.8196 s / batch. (data: 3.03e-04). ETA=10:01:33, max mem: 20.9 GB 
[11/24 08:21:53 visual_prompt]: 	Training 300/553. train loss: 131.5904,	1.0585 s / batch. (data: 2.37e-01). ETA=12:55:11, max mem: 20.9 GB 
[11/24 08:23:32 visual_prompt]: 	Training 400/553. train loss: 4.7542,	0.8247 s / batch. (data: 3.04e-04). ETA=10:02:35, max mem: 20.9 GB 
[11/24 08:25:14 visual_prompt]: 	Training 500/553. train loss: 5.1656,	0.8116 s / batch. (data: 5.45e-03). ETA=9:51:41, max mem: 20.9 GB 
[11/24 08:26:06 visual_prompt]: Epoch 21 / 100: avg data time: 1.91e-01, avg batch time: 1.0133, average train loss: 32.3908
[11/24 08:27:03 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3048, average loss: 59.2093
[11/24 08:27:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.51	
[11/24 08:27:03 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/24 08:28:47 visual_prompt]: 	Training 100/553. train loss: 74.4270,	0.8280 s / batch. (data: 2.99e-04). ETA=10:01:30, max mem: 20.9 GB 
[11/24 08:30:28 visual_prompt]: 	Training 200/553. train loss: 1.8805,	0.8505 s / batch. (data: 1.06e-02). ETA=10:16:26, max mem: 20.9 GB 
[11/24 08:32:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8240 s / batch. (data: 3.07e-04). ETA=9:55:51, max mem: 20.9 GB 
[11/24 08:33:48 visual_prompt]: 	Training 400/553. train loss: 60.2997,	0.8160 s / batch. (data: 2.77e-04). ETA=9:48:42, max mem: 20.9 GB 
[11/24 08:35:29 visual_prompt]: 	Training 500/553. train loss: 14.4501,	0.8201 s / batch. (data: 3.27e-04). ETA=9:50:15, max mem: 20.9 GB 
[11/24 08:36:22 visual_prompt]: Epoch 22 / 100: avg data time: 1.88e-01, avg batch time: 1.0101, average train loss: 31.3885
[11/24 08:37:20 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3076, average loss: 37.5095
[11/24 08:37:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.10	
[11/24 08:37:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/24 08:39:06 visual_prompt]: 	Training 100/553. train loss: 14.7998,	0.9917 s / batch. (data: 1.66e-01). ETA=11:51:15, max mem: 20.9 GB 
[11/24 08:40:48 visual_prompt]: 	Training 200/553. train loss: 26.9610,	0.8144 s / batch. (data: 1.56e-02). ETA=9:42:45, max mem: 20.9 GB 
[11/24 08:42:31 visual_prompt]: 	Training 300/553. train loss: 38.7181,	0.8677 s / batch. (data: 7.60e-04). ETA=10:19:28, max mem: 20.9 GB 
[11/24 08:44:09 visual_prompt]: 	Training 400/553. train loss: 20.7494,	0.8303 s / batch. (data: 3.04e-04). ETA=9:51:22, max mem: 20.9 GB 
[11/24 08:45:47 visual_prompt]: 	Training 500/553. train loss: 2.8258,	0.8280 s / batch. (data: 3.01e-04). ETA=9:48:21, max mem: 20.9 GB 
[11/24 08:46:40 visual_prompt]: Epoch 23 / 100: avg data time: 1.91e-01, avg batch time: 1.0118, average train loss: 30.7184
[11/24 08:47:37 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3064, average loss: 31.7203
[11/24 08:47:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.50	
[11/24 08:47:37 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/24 08:49:19 visual_prompt]: 	Training 100/553. train loss: 21.1932,	0.8246 s / batch. (data: 1.56e-02). ETA=9:43:49, max mem: 20.9 GB 
[11/24 08:50:59 visual_prompt]: 	Training 200/553. train loss: 4.5434,	0.8320 s / batch. (data: 3.03e-04). ETA=9:47:39, max mem: 20.9 GB 
[11/24 08:52:41 visual_prompt]: 	Training 300/553. train loss: 5.0614,	1.0610 s / batch. (data: 2.41e-01). ETA=12:27:38, max mem: 20.9 GB 
[11/24 08:54:22 visual_prompt]: 	Training 400/553. train loss: 27.5653,	0.8890 s / batch. (data: 2.10e-02). ETA=10:25:00, max mem: 20.9 GB 
[11/24 08:56:04 visual_prompt]: 	Training 500/553. train loss: 37.7972,	0.8440 s / batch. (data: 1.19e-02). ETA=9:51:56, max mem: 20.9 GB 
[11/24 08:56:57 visual_prompt]: Epoch 24 / 100: avg data time: 1.90e-01, avg batch time: 1.0116, average train loss: 29.8137
[11/24 08:57:54 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3068, average loss: 7.4391
[11/24 08:57:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.25	
[11/24 08:57:54 visual_prompt]: Best epoch 24: best metric: -7.439
[11/24 08:57:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/24 08:59:42 visual_prompt]: 	Training 100/553. train loss: 53.5473,	0.8480 s / batch. (data: 3.05e-04). ETA=9:52:33, max mem: 20.9 GB 
[11/24 09:01:20 visual_prompt]: 	Training 200/553. train loss: 11.9055,	0.8221 s / batch. (data: 2.99e-04). ETA=9:33:06, max mem: 20.9 GB 
[11/24 09:03:00 visual_prompt]: 	Training 300/553. train loss: 5.0566,	0.8080 s / batch. (data: 3.19e-04). ETA=9:21:56, max mem: 20.9 GB 
[11/24 09:04:41 visual_prompt]: 	Training 400/553. train loss: 9.4415,	1.4271 s / batch. (data: 6.06e-01). ETA=16:30:06, max mem: 20.9 GB 
[11/24 09:06:23 visual_prompt]: 	Training 500/553. train loss: 11.4700,	1.6961 s / batch. (data: 8.58e-01). ETA=19:33:55, max mem: 20.9 GB 
[11/24 09:07:15 visual_prompt]: Epoch 25 / 100: avg data time: 1.92e-01, avg batch time: 1.0139, average train loss: 32.5727
[11/24 09:08:13 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3080, average loss: 24.6369
[11/24 09:08:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/24 09:08:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/24 09:09:57 visual_prompt]: 	Training 100/553. train loss: 2.8801,	0.8631 s / batch. (data: 3.64e-02). ETA=9:55:09, max mem: 20.9 GB 
[11/24 09:11:39 visual_prompt]: 	Training 200/553. train loss: 21.5737,	1.7275 s / batch. (data: 9.15e-01). ETA=19:48:24, max mem: 20.9 GB 
[11/24 09:13:21 visual_prompt]: 	Training 300/553. train loss: 17.6660,	0.8400 s / batch. (data: 3.26e-04). ETA=9:36:26, max mem: 20.9 GB 
[11/24 09:15:03 visual_prompt]: 	Training 400/553. train loss: 17.7804,	0.8128 s / batch. (data: 2.71e-04). ETA=9:16:25, max mem: 20.9 GB 
[11/24 09:16:42 visual_prompt]: 	Training 500/553. train loss: 55.5703,	0.8515 s / batch. (data: 5.42e-03). ETA=9:41:28, max mem: 20.9 GB 
[11/24 09:17:34 visual_prompt]: Epoch 26 / 100: avg data time: 1.92e-01, avg batch time: 1.0146, average train loss: 29.8481
[11/24 09:18:32 visual_prompt]: Inference (val):avg data time: 2.05e-04, avg batch time: 0.3063, average loss: 8.4448
[11/24 09:18:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.15	
[11/24 09:18:32 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/24 09:20:17 visual_prompt]: 	Training 100/553. train loss: 73.5949,	0.8240 s / batch. (data: 7.93e-03). ETA=9:20:37, max mem: 20.9 GB 
[11/24 09:21:57 visual_prompt]: 	Training 200/553. train loss: 44.7822,	1.4121 s / batch. (data: 5.92e-01). ETA=15:58:23, max mem: 20.9 GB 
[11/24 09:23:38 visual_prompt]: 	Training 300/553. train loss: 13.6333,	0.8320 s / batch. (data: 3.16e-04). ETA=9:23:19, max mem: 20.9 GB 
[11/24 09:25:20 visual_prompt]: 	Training 400/553. train loss: 92.7445,	0.8203 s / batch. (data: 3.11e-04). ETA=9:14:01, max mem: 20.9 GB 
[11/24 09:27:02 visual_prompt]: 	Training 500/553. train loss: 17.2910,	0.8307 s / batch. (data: 5.81e-03). ETA=9:19:39, max mem: 20.9 GB 
[11/24 09:27:52 visual_prompt]: Epoch 27 / 100: avg data time: 1.92e-01, avg batch time: 1.0136, average train loss: 31.4765
[11/24 09:28:50 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3072, average loss: 15.9482
[11/24 09:28:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.11	
[11/24 09:28:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/24 09:30:34 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8109 s / batch. (data: 3.24e-04). ETA=9:04:12, max mem: 20.9 GB 
[11/24 09:32:15 visual_prompt]: 	Training 200/553. train loss: 32.3063,	0.8539 s / batch. (data: 1.19e-02). ETA=9:31:39, max mem: 20.9 GB 
[11/24 09:33:57 visual_prompt]: 	Training 300/553. train loss: 14.7498,	1.4598 s / batch. (data: 6.32e-01). ETA=16:14:52, max mem: 20.9 GB 
[11/24 09:35:38 visual_prompt]: 	Training 400/553. train loss: 76.6420,	0.8360 s / batch. (data: 6.96e-04). ETA=9:16:53, max mem: 20.9 GB 
[11/24 09:37:17 visual_prompt]: 	Training 500/553. train loss: 21.1955,	0.8447 s / batch. (data: 1.56e-02). ETA=9:21:15, max mem: 20.9 GB 
[11/24 09:38:11 visual_prompt]: Epoch 28 / 100: avg data time: 1.91e-01, avg batch time: 1.0133, average train loss: 31.9558
[11/24 09:39:09 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3076, average loss: 46.2851
[11/24 09:39:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.27	
[11/24 09:39:09 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/24 09:41:00 visual_prompt]: 	Training 100/553. train loss: 59.1458,	0.8294 s / batch. (data: 1.05e-02). ETA=9:09:01, max mem: 20.9 GB 
[11/24 09:42:40 visual_prompt]: 	Training 200/553. train loss: 37.0747,	1.8138 s / batch. (data: 1.01e+00). ETA=19:57:33, max mem: 20.9 GB 
[11/24 09:44:18 visual_prompt]: 	Training 300/553. train loss: 23.9400,	0.8150 s / batch. (data: 3.03e-04). ETA=8:56:46, max mem: 20.9 GB 
[11/24 09:45:56 visual_prompt]: 	Training 400/553. train loss: 9.1341,	0.8320 s / batch. (data: 3.13e-04). ETA=9:06:33, max mem: 20.9 GB 
[11/24 09:47:38 visual_prompt]: 	Training 500/553. train loss: 5.2619,	0.8282 s / batch. (data: 5.47e-03). ETA=9:02:41, max mem: 20.9 GB 
[11/24 09:48:30 visual_prompt]: Epoch 29 / 100: avg data time: 1.94e-01, avg batch time: 1.0151, average train loss: 37.2370
[11/24 09:49:28 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3064, average loss: 36.1800
[11/24 09:49:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.64	
[11/24 09:49:28 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/24 09:51:11 visual_prompt]: 	Training 100/553. train loss: 57.3054,	0.8429 s / batch. (data: 1.60e-02). ETA=9:10:11, max mem: 20.9 GB 
[11/24 09:52:54 visual_prompt]: 	Training 200/553. train loss: 20.0120,	0.8360 s / batch. (data: 3.00e-04). ETA=9:04:16, max mem: 20.9 GB 
[11/24 09:54:34 visual_prompt]: 	Training 300/553. train loss: 58.9110,	1.3594 s / batch. (data: 5.18e-01). ETA=14:42:47, max mem: 20.9 GB 
[11/24 09:56:17 visual_prompt]: 	Training 400/553. train loss: 32.9440,	1.1735 s / batch. (data: 3.55e-01). ETA=12:40:03, max mem: 20.9 GB 
[11/24 09:57:57 visual_prompt]: 	Training 500/553. train loss: 113.4729,	1.5120 s / batch. (data: 6.72e-01). ETA=16:16:49, max mem: 20.9 GB 
[11/24 09:58:52 visual_prompt]: Epoch 30 / 100: avg data time: 1.96e-01, avg batch time: 1.0192, average train loss: 28.7434
[11/24 09:59:50 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3077, average loss: 22.9904
[11/24 09:59:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.86	
[11/24 09:59:50 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/24 10:01:36 visual_prompt]: 	Training 100/553. train loss: 23.2558,	0.8315 s / batch. (data: 7.95e-03). ETA=8:55:03, max mem: 20.9 GB 
[11/24 10:03:19 visual_prompt]: 	Training 200/553. train loss: 109.4730,	0.8284 s / batch. (data: 5.43e-03). ETA=8:51:42, max mem: 20.9 GB 
[11/24 10:04:58 visual_prompt]: 	Training 300/553. train loss: 45.3426,	0.8201 s / batch. (data: 2.89e-04). ETA=8:45:00, max mem: 20.9 GB 
[11/24 10:06:37 visual_prompt]: 	Training 400/553. train loss: 52.4664,	0.8910 s / batch. (data: 6.31e-02). ETA=9:28:52, max mem: 20.9 GB 
[11/24 10:08:19 visual_prompt]: 	Training 500/553. train loss: 31.6548,	0.8199 s / batch. (data: 6.28e-04). ETA=8:42:08, max mem: 20.9 GB 
[11/24 10:09:11 visual_prompt]: Epoch 31 / 100: avg data time: 1.91e-01, avg batch time: 1.0141, average train loss: 26.0752
[11/24 10:10:09 visual_prompt]: Inference (val):avg data time: 2.65e-04, avg batch time: 0.3052, average loss: 31.8197
[11/24 10:10:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.21	
[11/24 10:10:09 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/24 10:11:55 visual_prompt]: 	Training 100/553. train loss: 25.7262,	0.8310 s / batch. (data: 1.05e-02). ETA=8:47:06, max mem: 20.9 GB 
[11/24 10:13:35 visual_prompt]: 	Training 200/553. train loss: 15.7171,	0.8596 s / batch. (data: 8.25e-04). ETA=9:03:46, max mem: 20.9 GB 
[11/24 10:15:20 visual_prompt]: 	Training 300/553. train loss: 92.0406,	0.8419 s / batch. (data: 1.55e-02). ETA=8:51:13, max mem: 20.9 GB 
[11/24 10:17:01 visual_prompt]: 	Training 400/553. train loss: 13.8397,	0.8333 s / batch. (data: 4.83e-04). ETA=8:44:23, max mem: 20.9 GB 
[11/24 10:18:40 visual_prompt]: 	Training 500/553. train loss: 23.7470,	0.8200 s / batch. (data: 2.91e-04). ETA=8:34:39, max mem: 20.9 GB 
[11/24 10:19:30 visual_prompt]: Epoch 32 / 100: avg data time: 1.94e-01, avg batch time: 1.0158, average train loss: 30.6038
[11/24 10:20:28 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3058, average loss: 15.6869
[11/24 10:20:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.55	
[11/24 10:20:28 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/24 10:22:12 visual_prompt]: 	Training 100/553. train loss: 30.3722,	1.8580 s / batch. (data: 1.04e+00). ETA=19:21:21, max mem: 20.9 GB 
[11/24 10:23:55 visual_prompt]: 	Training 200/553. train loss: 36.4146,	1.2051 s / batch. (data: 4.07e-01). ETA=12:31:15, max mem: 20.9 GB 
[11/24 10:25:35 visual_prompt]: 	Training 300/553. train loss: 26.4358,	0.8233 s / batch. (data: 1.07e-02). ETA=8:31:53, max mem: 20.9 GB 
[11/24 10:27:16 visual_prompt]: 	Training 400/553. train loss: 7.8397,	0.8264 s / batch. (data: 1.44e-02). ETA=8:32:26, max mem: 20.9 GB 
[11/24 10:28:58 visual_prompt]: 	Training 500/553. train loss: 13.7290,	0.8200 s / batch. (data: 4.94e-04). ETA=8:27:04, max mem: 20.9 GB 
[11/24 10:29:50 visual_prompt]: Epoch 33 / 100: avg data time: 1.93e-01, avg batch time: 1.0154, average train loss: 29.5350
[11/24 10:30:48 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.3065, average loss: 87.5976
[11/24 10:30:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.27	
[11/24 10:30:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/24 10:32:34 visual_prompt]: 	Training 100/553. train loss: 15.7223,	0.8350 s / batch. (data: 3.20e-04). ETA=8:34:14, max mem: 20.9 GB 
[11/24 10:34:13 visual_prompt]: 	Training 200/553. train loss: 19.8765,	0.8169 s / batch. (data: 3.16e-04). ETA=8:21:42, max mem: 20.9 GB 
[11/24 10:35:54 visual_prompt]: 	Training 300/553. train loss: 59.4217,	0.8200 s / batch. (data: 3.67e-04). ETA=8:22:14, max mem: 20.9 GB 
[11/24 10:37:37 visual_prompt]: 	Training 400/553. train loss: 37.4711,	0.8117 s / batch. (data: 5.62e-03). ETA=8:15:49, max mem: 20.9 GB 
[11/24 10:39:19 visual_prompt]: 	Training 500/553. train loss: 44.6877,	1.6720 s / batch. (data: 8.57e-01). ETA=16:58:33, max mem: 20.9 GB 
[11/24 10:40:12 visual_prompt]: Epoch 34 / 100: avg data time: 1.96e-01, avg batch time: 1.0191, average train loss: 30.3251
[11/24 10:41:10 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3075, average loss: 19.5082
[11/24 10:41:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.82	
[11/24 10:41:10 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/24 10:42:57 visual_prompt]: 	Training 100/553. train loss: 28.2258,	0.8243 s / batch. (data: 1.55e-02). ETA=8:20:01, max mem: 20.9 GB 
[11/24 10:44:40 visual_prompt]: 	Training 200/553. train loss: 40.1643,	0.8091 s / batch. (data: 3.08e-04). ETA=8:09:30, max mem: 20.9 GB 
[11/24 10:46:20 visual_prompt]: 	Training 300/553. train loss: 20.8998,	0.8294 s / batch. (data: 2.80e-04). ETA=8:20:21, max mem: 20.9 GB 
[11/24 10:47:59 visual_prompt]: 	Training 400/553. train loss: 16.4035,	0.9194 s / batch. (data: 9.86e-02). ETA=9:13:08, max mem: 20.9 GB 
[11/24 10:49:39 visual_prompt]: 	Training 500/553. train loss: 92.4864,	1.2419 s / batch. (data: 4.15e-01). ETA=12:25:06, max mem: 20.9 GB 
[11/24 10:50:32 visual_prompt]: Epoch 35 / 100: avg data time: 1.95e-01, avg batch time: 1.0176, average train loss: 32.1592
[11/24 10:51:31 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3047, average loss: 29.1534
[11/24 10:51:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.99	
[11/24 10:51:31 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[11/24 10:53:15 visual_prompt]: 	Training 100/553. train loss: 5.5278,	0.8185 s / batch. (data: 3.37e-04). ETA=8:08:59, max mem: 20.9 GB 
[11/24 10:54:57 visual_prompt]: 	Training 200/553. train loss: 61.8225,	0.8101 s / batch. (data: 5.42e-03). ETA=8:02:37, max mem: 20.9 GB 
[11/24 10:56:39 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8280 s / batch. (data: 2.91e-04). ETA=8:11:54, max mem: 20.9 GB 
[11/24 10:58:20 visual_prompt]: 	Training 400/553. train loss: 26.8228,	0.8185 s / batch. (data: 2.95e-04). ETA=8:04:53, max mem: 20.9 GB 
[11/24 11:00:03 visual_prompt]: 	Training 500/553. train loss: 8.0681,	0.8240 s / batch. (data: 3.16e-04). ETA=8:06:47, max mem: 20.9 GB 
[11/24 11:00:53 visual_prompt]: Epoch 36 / 100: avg data time: 1.94e-01, avg batch time: 1.0166, average train loss: 27.7423
[11/24 11:01:51 visual_prompt]: Inference (val):avg data time: 4.85e-04, avg batch time: 0.3071, average loss: 88.0162
[11/24 11:01:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.70	
[11/24 11:01:51 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[11/24 11:03:36 visual_prompt]: 	Training 100/553. train loss: 7.0561,	0.8201 s / batch. (data: 1.20e-02). ETA=8:02:22, max mem: 20.9 GB 
[11/24 11:05:17 visual_prompt]: 	Training 200/553. train loss: 14.3619,	0.8400 s / batch. (data: 7.94e-03). ETA=8:12:41, max mem: 20.9 GB 
[11/24 11:06:58 visual_prompt]: 	Training 300/553. train loss: 26.8646,	1.1445 s / batch. (data: 3.28e-01). ETA=11:09:21, max mem: 20.9 GB 
[11/24 11:08:42 visual_prompt]: 	Training 400/553. train loss: 46.8616,	2.1065 s / batch. (data: 1.29e+00). ETA=20:28:32, max mem: 20.9 GB 
[11/24 11:10:19 visual_prompt]: 	Training 500/553. train loss: 29.4414,	0.9437 s / batch. (data: 1.34e-01). ETA=9:08:46, max mem: 20.9 GB 
[11/24 11:11:13 visual_prompt]: Epoch 37 / 100: avg data time: 1.94e-01, avg batch time: 1.0172, average train loss: 24.0381
[11/24 11:12:11 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3061, average loss: 10.0697
[11/24 11:12:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.56	
[11/24 11:12:11 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[11/24 11:13:54 visual_prompt]: 	Training 100/553. train loss: 12.8534,	0.8305 s / batch. (data: 1.05e-02). ETA=8:00:50, max mem: 20.9 GB 
[11/24 11:15:36 visual_prompt]: 	Training 200/553. train loss: 10.3631,	0.8799 s / batch. (data: 6.57e-02). ETA=8:28:00, max mem: 20.9 GB 
[11/24 11:17:19 visual_prompt]: 	Training 300/553. train loss: 28.5636,	0.8206 s / batch. (data: 5.44e-03). ETA=7:52:24, max mem: 20.9 GB 
[11/24 11:18:58 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8454 s / batch. (data: 7.58e-04). ETA=8:05:16, max mem: 20.9 GB 
[11/24 11:20:41 visual_prompt]: 	Training 500/553. train loss: 4.4356,	0.8360 s / batch. (data: 3.33e-04). ETA=7:58:27, max mem: 20.9 GB 
[11/24 11:21:32 visual_prompt]: Epoch 38 / 100: avg data time: 1.91e-01, avg batch time: 1.0133, average train loss: 25.6685
[11/24 11:22:30 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3078, average loss: 5.1620
[11/24 11:22:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 53.38	
[11/24 11:22:30 visual_prompt]: Best epoch 38: best metric: -5.162
[11/24 11:22:30 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[11/24 11:24:13 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8278 s / batch. (data: 7.72e-03). ETA=7:51:38, max mem: 20.9 GB 
[11/24 11:25:58 visual_prompt]: 	Training 200/553. train loss: 40.6512,	0.8469 s / batch. (data: 1.49e-02). ETA=8:01:06, max mem: 20.9 GB 
[11/24 11:27:42 visual_prompt]: 	Training 300/553. train loss: 23.4809,	0.8240 s / batch. (data: 5.42e-03). ETA=7:46:44, max mem: 20.9 GB 
[11/24 11:29:20 visual_prompt]: 	Training 400/553. train loss: 0.3322,	1.0263 s / batch. (data: 2.12e-01). ETA=9:39:37, max mem: 20.9 GB 
[11/24 11:31:01 visual_prompt]: 	Training 500/553. train loss: 21.3553,	1.6986 s / batch. (data: 8.98e-01). ETA=15:56:30, max mem: 20.9 GB 
[11/24 11:31:51 visual_prompt]: Epoch 39 / 100: avg data time: 1.93e-01, avg batch time: 1.0151, average train loss: 22.3797
[11/24 11:32:49 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3057, average loss: 9.7386
[11/24 11:32:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.07	
[11/24 11:32:49 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[11/24 11:34:35 visual_prompt]: 	Training 100/553. train loss: 39.0284,	0.8280 s / batch. (data: 2.99e-04). ETA=7:44:07, max mem: 20.9 GB 
[11/24 11:36:15 visual_prompt]: 	Training 200/553. train loss: 44.7136,	0.8200 s / batch. (data: 3.18e-04). ETA=7:38:17, max mem: 20.9 GB 
[11/24 11:37:57 visual_prompt]: 	Training 300/553. train loss: 1.4765,	0.8076 s / batch. (data: 3.11e-04). ETA=7:30:02, max mem: 20.9 GB 
[11/24 11:39:38 visual_prompt]: 	Training 400/553. train loss: 12.7666,	0.8307 s / batch. (data: 7.12e-04). ETA=7:41:28, max mem: 20.9 GB 
[11/24 11:41:18 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8360 s / batch. (data: 7.96e-03). ETA=7:43:03, max mem: 20.9 GB 
[11/24 11:42:12 visual_prompt]: Epoch 40 / 100: avg data time: 1.95e-01, avg batch time: 1.0170, average train loss: 26.6628
[11/24 11:43:09 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.3065, average loss: 21.7155
[11/24 11:43:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 39.32	
[11/24 11:43:09 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[11/24 11:44:58 visual_prompt]: 	Training 100/553. train loss: 26.5522,	0.8360 s / batch. (data: 3.05e-04). ETA=7:40:55, max mem: 20.9 GB 
[11/24 11:46:41 visual_prompt]: 	Training 200/553. train loss: 25.0975,	0.8239 s / batch. (data: 5.88e-03). ETA=7:32:52, max mem: 20.9 GB 
[11/24 11:48:21 visual_prompt]: 	Training 300/553. train loss: 8.7110,	0.8320 s / batch. (data: 1.19e-02). ETA=7:35:55, max mem: 20.9 GB 
[11/24 11:50:02 visual_prompt]: 	Training 400/553. train loss: 24.0848,	0.8597 s / batch. (data: 7.58e-04). ETA=7:49:40, max mem: 20.9 GB 
[11/24 11:51:40 visual_prompt]: 	Training 500/553. train loss: 2.9224,	0.8252 s / batch. (data: 4.52e-04). ETA=7:29:26, max mem: 20.9 GB 
[11/24 11:52:30 visual_prompt]: Epoch 41 / 100: avg data time: 1.91e-01, avg batch time: 1.0134, average train loss: 26.3409
[11/24 11:53:28 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3066, average loss: 32.4843
[11/24 11:53:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.24	
[11/24 11:53:28 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[11/24 11:55:11 visual_prompt]: 	Training 100/553. train loss: 15.6477,	0.8183 s / batch. (data: 3.20e-04). ETA=7:23:36, max mem: 20.9 GB 
[11/24 11:56:52 visual_prompt]: 	Training 200/553. train loss: 18.8049,	0.8113 s / batch. (data: 3.35e-04). ETA=7:18:26, max mem: 20.9 GB 
[11/24 11:58:34 visual_prompt]: 	Training 300/553. train loss: 6.5058,	0.8490 s / batch. (data: 1.05e-02). ETA=7:37:25, max mem: 20.9 GB 
[11/24 12:00:15 visual_prompt]: 	Training 400/553. train loss: 17.2064,	0.8101 s / batch. (data: 3.24e-04). ETA=7:15:06, max mem: 20.9 GB 
[11/24 12:01:55 visual_prompt]: 	Training 500/553. train loss: 91.9209,	0.8206 s / batch. (data: 3.07e-04). ETA=7:19:23, max mem: 20.9 GB 
[11/24 12:02:48 visual_prompt]: Epoch 42 / 100: avg data time: 1.91e-01, avg batch time: 1.0137, average train loss: 24.4209
[11/24 12:03:46 visual_prompt]: Inference (val):avg data time: 1.67e-04, avg batch time: 0.3062, average loss: 36.9176
[11/24 12:03:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.04	
[11/24 12:03:46 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[11/24 12:05:33 visual_prompt]: 	Training 100/553. train loss: 7.3888,	0.8276 s / batch. (data: 5.44e-03). ETA=7:21:02, max mem: 20.9 GB 
[11/24 12:07:12 visual_prompt]: 	Training 200/553. train loss: 115.5747,	0.8199 s / batch. (data: 7.89e-03). ETA=7:15:34, max mem: 20.9 GB 
[11/24 12:08:51 visual_prompt]: 	Training 300/553. train loss: 42.1443,	0.8228 s / batch. (data: 1.55e-02). ETA=7:15:44, max mem: 20.9 GB 
[11/24 12:10:31 visual_prompt]: 	Training 400/553. train loss: 59.4686,	0.8213 s / batch. (data: 2.93e-04). ETA=7:13:35, max mem: 20.9 GB 
[11/24 12:12:13 visual_prompt]: 	Training 500/553. train loss: 38.3393,	0.8600 s / batch. (data: 3.54e-04). ETA=7:32:33, max mem: 20.9 GB 
[11/24 12:13:07 visual_prompt]: Epoch 43 / 100: avg data time: 1.92e-01, avg batch time: 1.0144, average train loss: 26.1458
[11/24 12:14:05 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3065, average loss: 18.0466
[11/24 12:14:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.67	
[11/24 12:14:05 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[11/24 12:15:51 visual_prompt]: 	Training 100/553. train loss: 41.6996,	1.0185 s / batch. (data: 2.08e-01). ETA=8:53:22, max mem: 20.9 GB 
[11/24 12:17:34 visual_prompt]: 	Training 200/553. train loss: 24.9968,	0.8160 s / batch. (data: 2.98e-04). ETA=7:05:57, max mem: 20.9 GB 
[11/24 12:19:12 visual_prompt]: 	Training 300/553. train loss: 40.6746,	0.8348 s / batch. (data: 5.43e-03). ETA=7:14:24, max mem: 20.9 GB 
[11/24 12:20:52 visual_prompt]: 	Training 400/553. train loss: 50.0597,	0.8075 s / batch. (data: 3.08e-04). ETA=6:58:48, max mem: 20.9 GB 
[11/24 12:22:34 visual_prompt]: 	Training 500/553. train loss: 11.9130,	0.8080 s / batch. (data: 3.04e-04). ETA=6:57:44, max mem: 20.9 GB 
[11/24 12:23:27 visual_prompt]: Epoch 44 / 100: avg data time: 1.93e-01, avg batch time: 1.0153, average train loss: 26.5415
[11/24 12:24:25 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3060, average loss: 106.2100
[11/24 12:24:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.68	
[11/24 12:24:25 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[11/24 12:26:11 visual_prompt]: 	Training 100/553. train loss: 18.1458,	0.8412 s / batch. (data: 1.05e-02). ETA=7:12:44, max mem: 20.9 GB 
[11/24 12:27:48 visual_prompt]: 	Training 200/553. train loss: 2.2040,	0.8233 s / batch. (data: 1.56e-02). ETA=7:02:11, max mem: 20.9 GB 
[11/24 12:29:31 visual_prompt]: 	Training 300/553. train loss: 35.3088,	0.8356 s / batch. (data: 1.15e-02). ETA=7:07:07, max mem: 20.9 GB 
[11/24 12:31:09 visual_prompt]: 	Training 400/553. train loss: 11.2677,	0.8168 s / batch. (data: 3.43e-04). ETA=6:56:06, max mem: 20.9 GB 
[11/24 12:32:53 visual_prompt]: 	Training 500/553. train loss: 35.0762,	0.8520 s / batch. (data: 3.22e-04). ETA=7:12:38, max mem: 20.9 GB 
[11/24 12:33:46 visual_prompt]: Epoch 45 / 100: avg data time: 1.90e-01, avg batch time: 1.0138, average train loss: 21.2374
[11/24 12:34:44 visual_prompt]: Inference (val):avg data time: 7.89e-05, avg batch time: 0.3161, average loss: 8.9792
[11/24 12:34:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.90	
[11/24 12:34:44 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[11/24 12:36:30 visual_prompt]: 	Training 100/553. train loss: 0.5297,	1.5840 s / batch. (data: 7.61e-01). ETA=13:20:19, max mem: 20.9 GB 
[11/24 12:38:13 visual_prompt]: 	Training 200/553. train loss: 2.9668,	0.8280 s / batch. (data: 3.05e-04). ETA=6:56:58, max mem: 20.9 GB 
[11/24 12:39:53 visual_prompt]: 	Training 300/553. train loss: 76.9602,	0.8247 s / batch. (data: 1.38e-02). ETA=6:53:56, max mem: 20.9 GB 
[11/24 12:41:34 visual_prompt]: 	Training 400/553. train loss: 27.3665,	0.8556 s / batch. (data: 1.56e-02). ETA=7:07:59, max mem: 20.9 GB 
[11/24 12:43:12 visual_prompt]: 	Training 500/553. train loss: 0.6555,	0.8200 s / batch. (data: 2.92e-04). ETA=6:48:50, max mem: 20.9 GB 
[11/24 12:44:06 visual_prompt]: Epoch 46 / 100: avg data time: 1.93e-01, avg batch time: 1.0152, average train loss: 24.1686
[11/24 12:45:04 visual_prompt]: Inference (val):avg data time: 4.67e-05, avg batch time: 0.3059, average loss: 13.5975
[11/24 12:45:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.89	
[11/24 12:45:04 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[11/24 12:46:50 visual_prompt]: 	Training 100/553. train loss: 10.5285,	0.8529 s / batch. (data: 5.42e-03). ETA=7:03:04, max mem: 20.9 GB 
[11/24 12:48:27 visual_prompt]: 	Training 200/553. train loss: 78.0075,	0.8043 s / batch. (data: 3.99e-04). ETA=6:37:37, max mem: 20.9 GB 
[11/24 12:50:10 visual_prompt]: 	Training 300/553. train loss: 5.8748,	0.8187 s / batch. (data: 7.68e-04). ETA=6:43:22, max mem: 20.9 GB 
[11/24 12:51:51 visual_prompt]: 	Training 400/553. train loss: 41.5618,	0.8221 s / batch. (data: 5.97e-03). ETA=6:43:40, max mem: 20.9 GB 
[11/24 12:53:31 visual_prompt]: 	Training 500/553. train loss: 12.8149,	0.8188 s / batch. (data: 3.04e-04). ETA=6:40:42, max mem: 20.9 GB 
[11/24 12:54:25 visual_prompt]: Epoch 47 / 100: avg data time: 1.90e-01, avg batch time: 1.0138, average train loss: 22.0154
[11/24 12:55:23 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3071, average loss: 19.2693
[11/24 12:55:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[11/24 12:55:23 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[11/24 12:57:09 visual_prompt]: 	Training 100/553. train loss: 67.7261,	0.8238 s / batch. (data: 2.30e-02). ETA=6:41:03, max mem: 20.9 GB 
[11/24 12:58:50 visual_prompt]: 	Training 200/553. train loss: 3.7868,	0.8367 s / batch. (data: 2.97e-04). ETA=6:45:55, max mem: 20.9 GB 
[11/24 13:00:33 visual_prompt]: 	Training 300/553. train loss: 0.6299,	1.9800 s / batch. (data: 1.16e+00). ETA=15:57:17, max mem: 20.9 GB 
[11/24 13:02:10 visual_prompt]: 	Training 400/553. train loss: 0.0083,	0.8414 s / batch. (data: 1.33e-02). ETA=6:45:23, max mem: 20.9 GB 
[11/24 13:03:52 visual_prompt]: 	Training 500/553. train loss: 20.4112,	0.8098 s / batch. (data: 2.83e-04). ETA=6:28:48, max mem: 20.9 GB 
[11/24 13:04:45 visual_prompt]: Epoch 48 / 100: avg data time: 1.93e-01, avg batch time: 1.0159, average train loss: 20.5710
[11/24 13:05:43 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3088, average loss: 63.7910
[11/24 13:05:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.25	
[11/24 13:05:43 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[11/24 13:07:27 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8406 s / batch. (data: 7.97e-03). ETA=6:41:28, max mem: 20.9 GB 
[11/24 13:09:07 visual_prompt]: 	Training 200/553. train loss: 20.9715,	0.8357 s / batch. (data: 3.14e-04). ETA=6:37:43, max mem: 20.9 GB 
[11/24 13:10:49 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8222 s / batch. (data: 3.06e-04). ETA=6:29:55, max mem: 20.9 GB 
[11/24 13:12:32 visual_prompt]: 	Training 400/553. train loss: 2.9353,	0.8250 s / batch. (data: 5.43e-03). ETA=6:29:53, max mem: 20.9 GB 
[11/24 13:14:13 visual_prompt]: 	Training 500/553. train loss: 14.3700,	0.8294 s / batch. (data: 6.69e-04). ETA=6:30:34, max mem: 20.9 GB 
[11/24 13:15:06 visual_prompt]: Epoch 49 / 100: avg data time: 1.97e-01, avg batch time: 1.0190, average train loss: 21.5894
[11/24 13:16:04 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3068, average loss: 18.1432
[11/24 13:16:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.55	
[11/24 13:16:04 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[11/24 13:17:51 visual_prompt]: 	Training 100/553. train loss: 13.4888,	0.8231 s / batch. (data: 3.17e-04). ETA=6:25:32, max mem: 20.9 GB 
[11/24 13:19:32 visual_prompt]: 	Training 200/553. train loss: 18.3344,	0.8096 s / batch. (data: 2.98e-04). ETA=6:17:51, max mem: 20.9 GB 
[11/24 13:21:12 visual_prompt]: 	Training 300/553. train loss: 32.4627,	0.8320 s / batch. (data: 2.60e-04). ETA=6:26:56, max mem: 20.9 GB 
[11/24 13:22:52 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8160 s / batch. (data: 3.11e-04). ETA=6:18:07, max mem: 20.9 GB 
[11/24 13:24:34 visual_prompt]: 	Training 500/553. train loss: 30.3753,	0.8267 s / batch. (data: 1.05e-02). ETA=6:21:42, max mem: 20.9 GB 
[11/24 13:25:26 visual_prompt]: Epoch 50 / 100: avg data time: 1.93e-01, avg batch time: 1.0150, average train loss: 20.2678
[11/24 13:26:24 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3075, average loss: 19.8499
[11/24 13:26:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.79	
[11/24 13:26:24 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[11/24 13:28:09 visual_prompt]: 	Training 100/553. train loss: 18.9532,	1.2840 s / batch. (data: 4.67e-01). ETA=9:49:33, max mem: 20.9 GB 
[11/24 13:29:50 visual_prompt]: 	Training 200/553. train loss: 28.8640,	0.8200 s / batch. (data: 3.12e-04). ETA=6:15:09, max mem: 20.9 GB 
[11/24 13:31:33 visual_prompt]: 	Training 300/553. train loss: 2.2775,	0.8120 s / batch. (data: 3.15e-04). ETA=6:10:08, max mem: 20.9 GB 
[11/24 13:33:14 visual_prompt]: 	Training 400/553. train loss: 2.9341,	1.4861 s / batch. (data: 6.78e-01). ETA=11:14:56, max mem: 20.9 GB 
[11/24 13:34:55 visual_prompt]: 	Training 500/553. train loss: 17.1177,	0.8320 s / batch. (data: 3.15e-04). ETA=6:16:28, max mem: 20.9 GB 
[11/24 13:35:45 visual_prompt]: Epoch 51 / 100: avg data time: 1.92e-01, avg batch time: 1.0155, average train loss: 19.0705
[11/24 13:36:43 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3070, average loss: 5.8342
[11/24 13:36:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.24	
[11/24 13:36:43 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[11/24 13:38:32 visual_prompt]: 	Training 100/553. train loss: 12.4204,	0.8244 s / batch. (data: 5.43e-03). ETA=6:10:56, max mem: 20.9 GB 
[11/24 13:40:12 visual_prompt]: 	Training 200/553. train loss: 8.5228,	0.8263 s / batch. (data: 2.92e-04). ETA=6:10:26, max mem: 20.9 GB 
[11/24 13:41:55 visual_prompt]: 	Training 300/553. train loss: 41.9723,	0.8335 s / batch. (data: 2.30e-02). ETA=6:12:16, max mem: 20.9 GB 
[11/24 13:43:38 visual_prompt]: 	Training 400/553. train loss: 88.8701,	0.8264 s / batch. (data: 3.03e-04). ETA=6:07:41, max mem: 20.9 GB 
[11/24 13:45:14 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.32e-04). ETA=6:03:29, max mem: 20.9 GB 
[11/24 13:46:06 visual_prompt]: Epoch 52 / 100: avg data time: 1.95e-01, avg batch time: 1.0180, average train loss: 21.3370
[11/24 13:47:04 visual_prompt]: Inference (val):avg data time: 1.74e-04, avg batch time: 0.3058, average loss: 3.1040
[11/24 13:47:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 61.22	
[11/24 13:47:04 visual_prompt]: Best epoch 52: best metric: -3.104
[11/24 13:47:04 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[11/24 13:48:49 visual_prompt]: 	Training 100/553. train loss: 34.9381,	0.8208 s / batch. (data: 5.46e-03). ETA=6:01:43, max mem: 20.9 GB 
[11/24 13:50:30 visual_prompt]: 	Training 200/553. train loss: 59.1264,	0.8486 s / batch. (data: 5.91e-03). ETA=6:12:34, max mem: 20.9 GB 
[11/24 13:52:10 visual_prompt]: 	Training 300/553. train loss: 30.7643,	0.8017 s / batch. (data: 4.16e-04). ETA=5:50:39, max mem: 20.9 GB 
[11/24 13:53:52 visual_prompt]: 	Training 400/553. train loss: 6.2285,	1.1736 s / batch. (data: 3.66e-01). ETA=8:31:22, max mem: 20.9 GB 
[11/24 13:55:32 visual_prompt]: 	Training 500/553. train loss: 28.4080,	0.8200 s / batch. (data: 2.96e-04). ETA=5:55:56, max mem: 20.9 GB 
[11/24 13:56:26 visual_prompt]: Epoch 53 / 100: avg data time: 1.93e-01, avg batch time: 1.0161, average train loss: 19.9654
[11/24 13:57:24 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3073, average loss: 10.2193
[11/24 13:57:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.50	
[11/24 13:57:24 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[11/24 13:59:11 visual_prompt]: 	Training 100/553. train loss: 0.3219,	0.8461 s / batch. (data: 1.05e-02). ETA=6:05:05, max mem: 20.9 GB 
[11/24 14:00:52 visual_prompt]: 	Training 200/553. train loss: 6.3546,	0.8200 s / batch. (data: 3.08e-04). ETA=5:52:29, max mem: 20.9 GB 
[11/24 14:02:31 visual_prompt]: 	Training 300/553. train loss: 41.7299,	0.8240 s / batch. (data: 2.92e-04). ETA=5:52:49, max mem: 20.9 GB 
[11/24 14:04:12 visual_prompt]: 	Training 400/553. train loss: 34.6027,	0.8302 s / batch. (data: 3.04e-04). ETA=5:54:05, max mem: 20.9 GB 
[11/24 14:05:54 visual_prompt]: 	Training 500/553. train loss: 15.0122,	0.8455 s / batch. (data: 5.44e-03). ETA=5:59:12, max mem: 20.9 GB 
[11/24 14:06:46 visual_prompt]: Epoch 54 / 100: avg data time: 1.95e-01, avg batch time: 1.0171, average train loss: 20.5318
[11/24 14:07:44 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3078, average loss: 55.9242
[11/24 14:07:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.39	
[11/24 14:07:44 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[11/24 14:09:29 visual_prompt]: 	Training 100/553. train loss: 1.7116,	0.8320 s / batch. (data: 3.20e-04). ETA=5:51:21, max mem: 20.9 GB 
[11/24 14:11:08 visual_prompt]: 	Training 200/553. train loss: 6.7196,	0.8192 s / batch. (data: 3.17e-04). ETA=5:44:33, max mem: 20.9 GB 
[11/24 14:12:50 visual_prompt]: 	Training 300/553. train loss: 17.1665,	0.8160 s / batch. (data: 3.01e-04). ETA=5:41:51, max mem: 20.9 GB 
[11/24 14:14:31 visual_prompt]: 	Training 400/553. train loss: 3.7559,	0.8440 s / batch. (data: 3.20e-04). ETA=5:52:12, max mem: 20.9 GB 
[11/24 14:16:13 visual_prompt]: 	Training 500/553. train loss: 7.0962,	0.8236 s / batch. (data: 3.14e-04). ETA=5:42:18, max mem: 20.9 GB 
[11/24 14:17:06 visual_prompt]: Epoch 55 / 100: avg data time: 1.92e-01, avg batch time: 1.0150, average train loss: 18.2025
[11/24 14:18:04 visual_prompt]: Inference (val):avg data time: 2.84e-04, avg batch time: 0.3045, average loss: 2.4929
[11/24 14:18:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.29	
[11/24 14:18:04 visual_prompt]: Best epoch 55: best metric: -2.493
[11/24 14:18:04 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[11/24 14:19:50 visual_prompt]: 	Training 100/553. train loss: 0.9013,	0.8280 s / batch. (data: 7.98e-03). ETA=5:42:02, max mem: 20.9 GB 
[11/24 14:21:29 visual_prompt]: 	Training 200/553. train loss: 2.3837,	0.8334 s / batch. (data: 8.75e-03). ETA=5:42:53, max mem: 20.9 GB 
[11/24 14:23:12 visual_prompt]: 	Training 300/553. train loss: 17.0712,	0.8524 s / batch. (data: 1.09e-02). ETA=5:49:16, max mem: 20.9 GB 
[11/24 14:24:55 visual_prompt]: 	Training 400/553. train loss: 7.0236,	0.8600 s / batch. (data: 7.76e-04). ETA=5:50:56, max mem: 20.9 GB 
[11/24 14:26:35 visual_prompt]: 	Training 500/553. train loss: 19.5637,	2.3570 s / batch. (data: 1.55e+00). ETA=15:57:55, max mem: 20.9 GB 
[11/24 14:27:26 visual_prompt]: Epoch 56 / 100: avg data time: 1.94e-01, avg batch time: 1.0158, average train loss: 15.3884
[11/24 14:28:24 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3079, average loss: 25.3055
[11/24 14:28:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.47	
[11/24 14:28:24 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[11/24 14:30:12 visual_prompt]: 	Training 100/553. train loss: 15.1756,	0.8177 s / batch. (data: 5.45e-03). ETA=5:30:15, max mem: 20.9 GB 
[11/24 14:31:51 visual_prompt]: 	Training 200/553. train loss: 45.6182,	0.8040 s / batch. (data: 3.23e-04). ETA=5:23:21, max mem: 20.9 GB 
[11/24 14:33:31 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8489 s / batch. (data: 5.42e-03). ETA=5:40:01, max mem: 20.9 GB 
[11/24 14:35:12 visual_prompt]: 	Training 400/553. train loss: 7.1038,	0.8223 s / batch. (data: 3.17e-04). ETA=5:27:58, max mem: 20.9 GB 
[11/24 14:36:51 visual_prompt]: 	Training 500/553. train loss: 67.1635,	0.8286 s / batch. (data: 5.96e-03). ETA=5:29:06, max mem: 20.9 GB 
[11/24 14:37:45 visual_prompt]: Epoch 57 / 100: avg data time: 1.91e-01, avg batch time: 1.0140, average train loss: 15.5547
[11/24 14:38:43 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3073, average loss: 5.7535
[11/24 14:38:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.17	
[11/24 14:38:43 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[11/24 14:40:28 visual_prompt]: 	Training 100/553. train loss: 7.4500,	1.1600 s / batch. (data: 3.26e-01). ETA=7:37:47, max mem: 20.9 GB 
[11/24 14:42:10 visual_prompt]: 	Training 200/553. train loss: 6.8628,	0.8124 s / batch. (data: 3.09e-04). ETA=5:19:14, max mem: 20.9 GB 
[11/24 14:43:53 visual_prompt]: 	Training 300/553. train loss: 24.6648,	0.8213 s / batch. (data: 2.83e-04). ETA=5:21:23, max mem: 20.9 GB 
[11/24 14:45:33 visual_prompt]: 	Training 400/553. train loss: 8.5359,	0.8224 s / batch. (data: 2.98e-04). ETA=5:20:26, max mem: 20.9 GB 
[11/24 14:47:13 visual_prompt]: 	Training 500/553. train loss: 185.6109,	0.8291 s / batch. (data: 1.05e-02). ETA=5:21:39, max mem: 20.9 GB 
[11/24 14:48:05 visual_prompt]: Epoch 58 / 100: avg data time: 1.92e-01, avg batch time: 1.0163, average train loss: 17.6176
[11/24 14:49:03 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3060, average loss: 7.0374
[11/24 14:49:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.23	
[11/24 14:49:03 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[11/24 14:50:50 visual_prompt]: 	Training 100/553. train loss: 1.1424,	0.8160 s / batch. (data: 3.55e-04). ETA=5:14:30, max mem: 20.9 GB 
[11/24 14:52:32 visual_prompt]: 	Training 200/553. train loss: 5.8374,	0.8232 s / batch. (data: 3.08e-04). ETA=5:15:54, max mem: 20.9 GB 
[11/24 14:54:12 visual_prompt]: 	Training 300/553. train loss: 27.1948,	0.8325 s / batch. (data: 5.42e-03). ETA=5:18:06, max mem: 20.9 GB 
[11/24 14:55:52 visual_prompt]: 	Training 400/553. train loss: 11.9470,	0.8151 s / batch. (data: 4.21e-04). ETA=5:10:05, max mem: 20.9 GB 
[11/24 14:57:35 visual_prompt]: 	Training 500/553. train loss: 6.1322,	0.8200 s / batch. (data: 7.03e-04). ETA=5:10:35, max mem: 20.9 GB 
[11/24 14:58:26 visual_prompt]: Epoch 59 / 100: avg data time: 1.95e-01, avg batch time: 1.0182, average train loss: 17.8184
[11/24 14:59:24 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3068, average loss: 3.6967
[11/24 14:59:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.48	
[11/24 14:59:24 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[11/24 15:01:09 visual_prompt]: 	Training 100/553. train loss: 2.9260,	0.8334 s / batch. (data: 5.42e-03). ETA=5:13:32, max mem: 20.9 GB 
[11/24 15:02:49 visual_prompt]: 	Training 200/553. train loss: 42.8757,	0.8125 s / batch. (data: 3.28e-04). ETA=5:04:18, max mem: 20.9 GB 
[11/24 15:04:28 visual_prompt]: 	Training 300/553. train loss: 38.8731,	2.0132 s / batch. (data: 1.18e+00). ETA=12:30:41, max mem: 20.9 GB 
[11/24 15:06:10 visual_prompt]: 	Training 400/553. train loss: 0.8077,	1.2103 s / batch. (data: 3.99e-01). ETA=7:29:16, max mem: 20.9 GB 
[11/24 15:07:51 visual_prompt]: 	Training 500/553. train loss: 3.3147,	0.8147 s / batch. (data: 2.67e-04). ETA=5:01:03, max mem: 20.9 GB 
[11/24 15:08:44 visual_prompt]: Epoch 60 / 100: avg data time: 1.90e-01, avg batch time: 1.0129, average train loss: 15.8165
[11/24 15:09:42 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3074, average loss: 15.3824
[11/24 15:09:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.05	
[11/24 15:09:42 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[11/24 15:11:28 visual_prompt]: 	Training 100/553. train loss: 29.5074,	0.8535 s / batch. (data: 1.75e-02). ETA=5:13:15, max mem: 20.9 GB 
[11/24 15:13:10 visual_prompt]: 	Training 200/553. train loss: 20.0876,	2.0040 s / batch. (data: 1.20e+00). ETA=12:12:07, max mem: 20.9 GB 
[11/24 15:14:52 visual_prompt]: 	Training 300/553. train loss: 8.4036,	1.6068 s / batch. (data: 7.88e-01). ETA=9:44:20, max mem: 20.9 GB 
[11/24 15:16:30 visual_prompt]: 	Training 400/553. train loss: 16.5174,	0.8231 s / batch. (data: 1.05e-02). ETA=4:57:58, max mem: 20.9 GB 
[11/24 15:18:11 visual_prompt]: 	Training 500/553. train loss: 34.2611,	2.8000 s / batch. (data: 1.98e+00). ETA=16:48:55, max mem: 20.9 GB 
[11/24 15:19:02 visual_prompt]: Epoch 61 / 100: avg data time: 1.89e-01, avg batch time: 1.0129, average train loss: 15.8167
[11/24 15:20:00 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3069, average loss: 10.9515
[11/24 15:20:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.38	
[11/24 15:20:00 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[11/24 15:21:46 visual_prompt]: 	Training 100/553. train loss: 22.4552,	0.8612 s / batch. (data: 8.33e-04). ETA=5:08:07, max mem: 20.9 GB 
[11/24 15:23:25 visual_prompt]: 	Training 200/553. train loss: 10.1873,	0.8172 s / batch. (data: 2.85e-04). ETA=4:51:02, max mem: 20.9 GB 
[11/24 15:25:06 visual_prompt]: 	Training 300/553. train loss: 27.1875,	0.8377 s / batch. (data: 5.43e-03). ETA=4:56:54, max mem: 20.9 GB 
[11/24 15:26:47 visual_prompt]: 	Training 400/553. train loss: 9.0329,	0.8133 s / batch. (data: 2.99e-04). ETA=4:46:54, max mem: 20.9 GB 
[11/24 15:28:26 visual_prompt]: 	Training 500/553. train loss: 14.9380,	0.8260 s / batch. (data: 1.55e-02). ETA=4:50:01, max mem: 20.9 GB 
[11/24 15:29:20 visual_prompt]: Epoch 62 / 100: avg data time: 1.90e-01, avg batch time: 1.0137, average train loss: 13.4324
[11/24 15:30:18 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3073, average loss: 13.0417
[11/24 15:30:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.03	
[11/24 15:30:18 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[11/24 15:32:07 visual_prompt]: 	Training 100/553. train loss: 2.1797,	0.8240 s / batch. (data: 2.82e-04). ETA=4:47:12, max mem: 20.9 GB 
[11/24 15:33:53 visual_prompt]: 	Training 200/553. train loss: 5.8354,	0.8159 s / batch. (data: 7.77e-03). ETA=4:43:01, max mem: 20.9 GB 
[11/24 15:35:31 visual_prompt]: 	Training 300/553. train loss: 2.0536,	0.8256 s / batch. (data: 3.06e-04). ETA=4:45:00, max mem: 20.9 GB 
[11/24 15:37:08 visual_prompt]: 	Training 400/553. train loss: 11.0377,	0.8240 s / batch. (data: 3.15e-04). ETA=4:43:06, max mem: 20.9 GB 
[11/24 15:38:46 visual_prompt]: 	Training 500/553. train loss: 7.3771,	0.8358 s / batch. (data: 5.44e-03). ETA=4:45:45, max mem: 20.9 GB 
[11/24 15:39:38 visual_prompt]: Epoch 63 / 100: avg data time: 1.88e-01, avg batch time: 1.0114, average train loss: 12.5359
[11/24 15:40:35 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3070, average loss: 14.8224
[11/24 15:40:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.12	
[11/24 15:40:35 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[11/24 15:42:22 visual_prompt]: 	Training 100/553. train loss: 4.9937,	0.8376 s / batch. (data: 3.22e-04). ETA=4:44:14, max mem: 20.9 GB 
[11/24 15:44:05 visual_prompt]: 	Training 200/553. train loss: 66.3867,	0.8280 s / batch. (data: 2.93e-04). ETA=4:39:35, max mem: 20.9 GB 
[11/24 15:45:44 visual_prompt]: 	Training 300/553. train loss: 8.0966,	0.8126 s / batch. (data: 3.82e-04). ETA=4:33:02, max mem: 20.9 GB 
[11/24 15:47:26 visual_prompt]: 	Training 400/553. train loss: 9.2367,	1.0807 s / batch. (data: 2.51e-01). ETA=6:01:19, max mem: 20.9 GB 
[11/24 15:49:07 visual_prompt]: 	Training 500/553. train loss: 31.7402,	0.8383 s / batch. (data: 2.94e-04). ETA=4:38:53, max mem: 20.9 GB 
[11/24 15:50:00 visual_prompt]: Epoch 64 / 100: avg data time: 1.97e-01, avg batch time: 1.0201, average train loss: 12.2981
[11/24 15:50:57 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3064, average loss: 4.9562
[11/24 15:50:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.95	
[11/24 15:50:57 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[11/24 15:52:47 visual_prompt]: 	Training 100/553. train loss: 8.1335,	0.8708 s / batch. (data: 3.65e-02). ETA=4:47:27, max mem: 20.9 GB 
[11/24 15:54:28 visual_prompt]: 	Training 200/553. train loss: 6.1695,	1.6264 s / batch. (data: 8.21e-01). ETA=8:54:12, max mem: 20.9 GB 
[11/24 15:56:07 visual_prompt]: 	Training 300/553. train loss: 17.0906,	1.2640 s / batch. (data: 4.51e-01). ETA=6:53:04, max mem: 20.9 GB 
[11/24 15:57:47 visual_prompt]: 	Training 400/553. train loss: 9.3688,	0.8239 s / batch. (data: 1.09e-03). ETA=4:27:53, max mem: 20.9 GB 
[11/24 15:59:27 visual_prompt]: 	Training 500/553. train loss: 17.0593,	0.8120 s / batch. (data: 3.06e-04). ETA=4:22:39, max mem: 20.9 GB 
[11/24 16:00:18 visual_prompt]: Epoch 65 / 100: avg data time: 1.89e-01, avg batch time: 1.0132, average train loss: 11.4889
[11/24 16:01:16 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3059, average loss: 7.8149
[11/24 16:01:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.17	
[11/24 16:01:16 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[11/24 16:03:00 visual_prompt]: 	Training 100/553. train loss: 6.1508,	0.8371 s / batch. (data: 5.43e-03). ETA=4:28:39, max mem: 20.9 GB 
[11/24 16:04:42 visual_prompt]: 	Training 200/553. train loss: 1.5804,	1.7702 s / batch. (data: 9.52e-01). ETA=9:25:08, max mem: 20.9 GB 
[11/24 16:06:24 visual_prompt]: 	Training 300/553. train loss: 5.7337,	0.8307 s / batch. (data: 2.99e-04). ETA=4:23:49, max mem: 20.9 GB 
[11/24 16:08:04 visual_prompt]: 	Training 400/553. train loss: 7.5557,	0.8175 s / batch. (data: 3.12e-04). ETA=4:18:15, max mem: 20.9 GB 
[11/24 16:09:43 visual_prompt]: 	Training 500/553. train loss: 17.6168,	0.8441 s / batch. (data: 7.95e-03). ETA=4:25:15, max mem: 20.9 GB 
[11/24 16:10:37 visual_prompt]: Epoch 66 / 100: avg data time: 1.93e-01, avg batch time: 1.0151, average train loss: 10.6113
[11/24 16:11:35 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3069, average loss: 6.4178
[11/24 16:11:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.53	
[11/24 16:11:35 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[11/24 16:13:21 visual_prompt]: 	Training 100/553. train loss: 12.9475,	0.8371 s / batch. (data: 1.56e-02). ETA=4:20:54, max mem: 20.9 GB 
[11/24 16:15:02 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8252 s / batch. (data: 3.10e-04). ETA=4:15:50, max mem: 20.9 GB 
[11/24 16:16:40 visual_prompt]: 	Training 300/553. train loss: 14.8793,	0.8344 s / batch. (data: 1.19e-02). ETA=4:17:17, max mem: 20.9 GB 
[11/24 16:18:17 visual_prompt]: 	Training 400/553. train loss: 13.0800,	0.8200 s / batch. (data: 2.56e-04). ETA=4:11:29, max mem: 20.9 GB 
[11/24 16:20:00 visual_prompt]: 	Training 500/553. train loss: 14.7161,	1.5706 s / batch. (data: 7.48e-01). ETA=7:59:04, max mem: 20.9 GB 
[11/24 16:20:54 visual_prompt]: Epoch 67 / 100: avg data time: 1.85e-01, avg batch time: 1.0104, average train loss: 9.5718
[11/24 16:21:51 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3052, average loss: 7.2660
[11/24 16:21:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 37.47	
[11/24 16:21:51 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[11/24 16:23:36 visual_prompt]: 	Training 100/553. train loss: 0.9755,	0.8440 s / batch. (data: 7.72e-03). ETA=4:15:17, max mem: 20.9 GB 
[11/24 16:25:19 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.1920 s / batch. (data: 3.50e-01). ETA=5:58:34, max mem: 20.9 GB 
[11/24 16:26:56 visual_prompt]: 	Training 300/553. train loss: 6.9514,	0.8340 s / batch. (data: 2.84e-04). ETA=4:09:30, max mem: 20.9 GB 
[11/24 16:28:37 visual_prompt]: 	Training 400/553. train loss: 15.7196,	0.8291 s / batch. (data: 3.00e-04). ETA=4:06:39, max mem: 20.9 GB 
[11/24 16:30:17 visual_prompt]: 	Training 500/553. train loss: 13.9565,	0.8261 s / batch. (data: 1.05e-02). ETA=4:04:23, max mem: 20.9 GB 
[11/24 16:31:10 visual_prompt]: Epoch 68 / 100: avg data time: 1.87e-01, avg batch time: 1.0100, average train loss: 9.5590
[11/24 16:32:07 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3046, average loss: 2.5293
[11/24 16:32:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.56	
[11/24 16:32:07 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[11/24 16:33:52 visual_prompt]: 	Training 100/553. train loss: 10.2932,	0.8210 s / batch. (data: 3.16e-04). ETA=4:00:46, max mem: 20.9 GB 
[11/24 16:35:32 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8782 s / batch. (data: 6.23e-02). ETA=4:16:04, max mem: 20.9 GB 
[11/24 16:37:13 visual_prompt]: 	Training 300/553. train loss: 17.7103,	0.8320 s / batch. (data: 7.94e-03). ETA=4:01:13, max mem: 20.9 GB 
[11/24 16:38:54 visual_prompt]: 	Training 400/553. train loss: 21.1518,	0.8160 s / batch. (data: 3.11e-04). ETA=3:55:13, max mem: 20.9 GB 
[11/24 16:40:35 visual_prompt]: 	Training 500/553. train loss: 9.3876,	0.8543 s / batch. (data: 1.03e-02). ETA=4:04:50, max mem: 20.9 GB 
[11/24 16:41:28 visual_prompt]: Epoch 69 / 100: avg data time: 1.91e-01, avg batch time: 1.0133, average train loss: 9.4999
[11/24 16:42:26 visual_prompt]: Inference (val):avg data time: 2.16e-04, avg batch time: 0.3068, average loss: 5.6805
[11/24 16:42:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.36	
[11/24 16:42:26 visual_prompt]: Stopping early.
[11/24 16:42:26 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 16:42:26 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 16:42:26 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/24 16:42:26 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 16:42:26 visual_prompt]: Training with config:
[11/24 16:42:26 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/24 16:42:26 visual_prompt]: Loading training data...
[11/24 16:42:26 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 16:42:26 visual_prompt]: Loading validation data...
[11/24 16:42:26 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 16:42:26 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 16:42:31 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 16:42:31 visual_prompt]: tuned percent:0.525
[11/24 16:42:31 visual_prompt]: Device used for model: 0
[11/24 16:42:31 visual_prompt]: Setting up Evaluator...
[11/24 16:42:31 visual_prompt]: Setting up Trainer...
[11/24 16:42:31 visual_prompt]: 	Setting up the optimizer...
[11/24 16:42:31 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 16:44:16 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8415 s / batch. (data: 1.35e-02). ETA=12:54:10, max mem: 20.9 GB 
[11/24 16:45:56 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8108 s / batch. (data: 2.95e-04). ETA=12:24:37, max mem: 20.9 GB 
[11/24 16:47:38 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.9200 s / batch. (data: 1.07e-01). ETA=14:03:21, max mem: 20.9 GB 
[11/24 16:49:19 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8063 s / batch. (data: 2.92e-04). ETA=12:17:47, max mem: 20.9 GB 
[11/24 16:51:01 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8149 s / batch. (data: 3.33e-04). ETA=12:24:18, max mem: 20.9 GB 
[11/24 16:51:54 visual_prompt]: Epoch 1 / 100: avg data time: 1.94e-01, avg batch time: 1.0178, average train loss: 1.5403
[11/24 16:52:52 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3079, average loss: 1.5201
[11/24 16:52:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 16:52:52 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/24 16:54:37 visual_prompt]: 	Training 100/553. train loss: 3.8541,	1.8280 s / batch. (data: 1.01e+00). ETA=1 day, 3:44:53, max mem: 20.9 GB 
[11/24 16:56:17 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8200 s / batch. (data: 2.95e-04). ETA=12:25:28, max mem: 20.9 GB 
[11/24 16:58:00 visual_prompt]: 	Training 300/553. train loss: 2.6410,	1.1049 s / batch. (data: 2.75e-01). ETA=16:42:39, max mem: 20.9 GB 
[11/24 16:59:39 visual_prompt]: 	Training 400/553. train loss: 1.6751,	0.8400 s / batch. (data: 3.24e-04). ETA=12:40:51, max mem: 20.9 GB 
[11/24 17:01:22 visual_prompt]: 	Training 500/553. train loss: 0.5674,	0.8280 s / batch. (data: 3.05e-04). ETA=12:28:37, max mem: 20.9 GB 
[11/24 17:02:13 visual_prompt]: Epoch 2 / 100: avg data time: 1.89e-01, avg batch time: 1.0138, average train loss: 3.2359
[11/24 17:03:11 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3077, average loss: 10.4632
[11/24 17:03:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.18	
[11/24 17:03:11 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/24 17:04:55 visual_prompt]: 	Training 100/553. train loss: 24.3151,	0.8322 s / batch. (data: 5.43e-03). ETA=12:30:19, max mem: 20.9 GB 
[11/24 17:06:36 visual_prompt]: 	Training 200/553. train loss: 4.6712,	0.8061 s / batch. (data: 3.18e-04). ETA=12:05:22, max mem: 20.9 GB 
[11/24 17:08:16 visual_prompt]: 	Training 300/553. train loss: 2.6836,	0.8351 s / batch. (data: 1.05e-02). ETA=12:30:08, max mem: 20.9 GB 
[11/24 17:09:58 visual_prompt]: 	Training 400/553. train loss: 6.9202,	0.8212 s / batch. (data: 2.97e-04). ETA=12:16:17, max mem: 20.9 GB 
[11/24 17:11:40 visual_prompt]: 	Training 500/553. train loss: 2.7821,	1.3720 s / batch. (data: 5.29e-01). ETA=20:27:48, max mem: 20.9 GB 
[11/24 17:12:31 visual_prompt]: Epoch 3 / 100: avg data time: 1.89e-01, avg batch time: 1.0140, average train loss: 6.4887
[11/24 17:13:29 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3066, average loss: 7.2072
[11/24 17:13:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[11/24 17:13:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/24 17:15:15 visual_prompt]: 	Training 100/553. train loss: 21.5205,	0.8250 s / batch. (data: 3.94e-04). ETA=12:16:12, max mem: 20.9 GB 
[11/24 17:16:57 visual_prompt]: 	Training 200/553. train loss: 17.5341,	0.8283 s / batch. (data: 5.44e-03). ETA=12:17:45, max mem: 20.9 GB 
[11/24 17:18:37 visual_prompt]: 	Training 300/553. train loss: 1.5394,	1.0187 s / batch. (data: 2.07e-01). ETA=15:05:40, max mem: 20.9 GB 
[11/24 17:20:14 visual_prompt]: 	Training 400/553. train loss: 17.8045,	1.1160 s / batch. (data: 2.93e-01). ETA=16:30:18, max mem: 20.9 GB 
[11/24 17:21:57 visual_prompt]: 	Training 500/553. train loss: 15.0734,	3.6239 s / batch. (data: 2.78e+00). ETA=2 days, 5:29:38, max mem: 20.9 GB 
[11/24 17:22:51 visual_prompt]: Epoch 4 / 100: avg data time: 1.92e-01, avg batch time: 1.0159, average train loss: 8.8992
[11/24 17:23:49 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3082, average loss: 7.4956
[11/24 17:23:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.73	
[11/24 17:23:49 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/24 17:25:33 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8219 s / batch. (data: 2.90e-04). ETA=12:05:52, max mem: 20.9 GB 
[11/24 17:27:13 visual_prompt]: 	Training 200/553. train loss: 26.0106,	1.2292 s / batch. (data: 4.12e-01). ETA=18:03:27, max mem: 20.9 GB 
[11/24 17:28:55 visual_prompt]: 	Training 300/553. train loss: 27.0224,	0.8073 s / batch. (data: 3.12e-04). ETA=11:50:14, max mem: 20.9 GB 
[11/24 17:30:35 visual_prompt]: 	Training 400/553. train loss: 5.8865,	0.8266 s / batch. (data: 5.46e-03). ETA=12:05:53, max mem: 20.9 GB 
[11/24 17:32:17 visual_prompt]: 	Training 500/553. train loss: 20.3104,	0.8319 s / batch. (data: 1.19e-02). ETA=12:09:10, max mem: 20.9 GB 
[11/24 17:33:10 visual_prompt]: Epoch 5 / 100: avg data time: 1.90e-01, avg batch time: 1.0145, average train loss: 13.5538
[11/24 17:34:07 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3087, average loss: 19.2477
[11/24 17:34:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.87	
[11/24 17:34:07 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/24 17:35:54 visual_prompt]: 	Training 100/553. train loss: 18.3519,	0.8309 s / batch. (data: 5.46e-03). ETA=12:06:09, max mem: 20.9 GB 
[11/24 17:37:35 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8551 s / batch. (data: 1.56e-02). ETA=12:25:52, max mem: 20.9 GB 
[11/24 17:39:14 visual_prompt]: 	Training 300/553. train loss: 10.2809,	0.8087 s / batch. (data: 2.85e-04). ETA=11:44:01, max mem: 20.9 GB 
[11/24 17:40:59 visual_prompt]: 	Training 400/553. train loss: 44.9343,	0.8160 s / batch. (data: 3.49e-04). ETA=11:49:01, max mem: 20.9 GB 
[11/24 17:42:39 visual_prompt]: 	Training 500/553. train loss: 30.3506,	1.0275 s / batch. (data: 1.91e-01). ETA=14:51:03, max mem: 20.9 GB 
[11/24 17:43:32 visual_prompt]: Epoch 6 / 100: avg data time: 1.97e-01, avg batch time: 1.0206, average train loss: 16.0221
[11/24 17:44:30 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3071, average loss: 6.9090
[11/24 17:44:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.85	
[11/24 17:44:30 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/24 17:46:13 visual_prompt]: 	Training 100/553. train loss: 21.6591,	0.8500 s / batch. (data: 1.05e-02). ETA=12:14:59, max mem: 20.9 GB 
[11/24 17:47:54 visual_prompt]: 	Training 200/553. train loss: 10.9828,	0.8160 s / batch. (data: 5.46e-03). ETA=11:44:11, max mem: 20.9 GB 
[11/24 17:49:38 visual_prompt]: 	Training 300/553. train loss: 16.7040,	2.0126 s / batch. (data: 1.21e+00). ETA=1 day, 4:53:33, max mem: 20.9 GB 
[11/24 17:51:20 visual_prompt]: 	Training 400/553. train loss: 3.1928,	1.8480 s / batch. (data: 1.01e+00). ETA=1 day, 2:28:43, max mem: 20.9 GB 
[11/24 17:52:58 visual_prompt]: 	Training 500/553. train loss: 8.6548,	0.8374 s / batch. (data: 9.77e-03). ETA=11:58:29, max mem: 20.9 GB 
[11/24 17:53:50 visual_prompt]: Epoch 7 / 100: avg data time: 1.89e-01, avg batch time: 1.0121, average train loss: 20.5133
[11/24 17:54:47 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3076, average loss: 20.0773
[11/24 17:54:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.11	
[11/24 17:54:47 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/24 17:56:30 visual_prompt]: 	Training 100/553. train loss: 65.4587,	0.8023 s / batch. (data: 3.09e-04). ETA=11:26:22, max mem: 20.9 GB 
[11/24 17:58:13 visual_prompt]: 	Training 200/553. train loss: 5.9290,	0.8440 s / batch. (data: 3.43e-04). ETA=12:00:36, max mem: 20.9 GB 
[11/24 17:59:55 visual_prompt]: 	Training 300/553. train loss: 20.1993,	0.8336 s / batch. (data: 5.72e-03). ETA=11:50:20, max mem: 20.9 GB 
[11/24 18:01:36 visual_prompt]: 	Training 400/553. train loss: 4.9946,	0.8374 s / batch. (data: 3.02e-04). ETA=11:52:12, max mem: 20.9 GB 
[11/24 18:03:17 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.4328 s / batch. (data: 6.10e-01). ETA=20:16:11, max mem: 20.9 GB 
[11/24 18:04:10 visual_prompt]: Epoch 8 / 100: avg data time: 1.94e-01, avg batch time: 1.0179, average train loss: 21.8628
[11/24 18:05:08 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3066, average loss: 4.8054
[11/24 18:05:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.08	
[11/24 18:05:08 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/24 18:06:54 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8273 s / batch. (data: 3.04e-04). ETA=11:40:05, max mem: 20.9 GB 
[11/24 18:08:34 visual_prompt]: 	Training 200/553. train loss: 19.8405,	0.8283 s / batch. (data: 1.52e-02). ETA=11:39:36, max mem: 20.9 GB 
[11/24 18:10:15 visual_prompt]: 	Training 300/553. train loss: 5.5630,	1.8933 s / batch. (data: 1.07e+00). ETA=1 day, 2:35:56, max mem: 20.9 GB 
[11/24 18:11:58 visual_prompt]: 	Training 400/553. train loss: 23.2621,	0.8338 s / batch. (data: 5.45e-03). ETA=11:41:24, max mem: 20.9 GB 
[11/24 18:13:40 visual_prompt]: 	Training 500/553. train loss: 11.8249,	1.0779 s / batch. (data: 2.55e-01). ETA=15:05:00, max mem: 20.9 GB 
[11/24 18:14:31 visual_prompt]: Epoch 9 / 100: avg data time: 1.93e-01, avg batch time: 1.0170, average train loss: 22.3752
[11/24 18:15:29 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3071, average loss: 28.7980
[11/24 18:15:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.92	
[11/24 18:15:29 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/24 18:17:17 visual_prompt]: 	Training 100/553. train loss: 51.8149,	0.8240 s / batch. (data: 1.35e-02). ETA=11:29:43, max mem: 20.9 GB 
[11/24 18:18:56 visual_prompt]: 	Training 200/553. train loss: 40.6482,	0.8218 s / batch. (data: 2.98e-04). ETA=11:26:32, max mem: 20.9 GB 
[11/24 18:20:36 visual_prompt]: 	Training 300/553. train loss: 216.0547,	0.9881 s / batch. (data: 1.82e-01). ETA=13:43:45, max mem: 20.9 GB 
[11/24 18:22:16 visual_prompt]: 	Training 400/553. train loss: 9.2732,	0.8269 s / batch. (data: 3.11e-04). ETA=11:27:59, max mem: 20.9 GB 
[11/24 18:23:58 visual_prompt]: 	Training 500/553. train loss: 3.4918,	0.8070 s / batch. (data: 3.24e-04). ETA=11:10:08, max mem: 20.9 GB 
[11/24 18:24:51 visual_prompt]: Epoch 10 / 100: avg data time: 1.95e-01, avg batch time: 1.0169, average train loss: 33.7567
[11/24 18:25:49 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3065, average loss: 27.9118
[11/24 18:25:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 49.09	
[11/24 18:25:49 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/24 18:27:36 visual_prompt]: 	Training 100/553. train loss: 25.8767,	0.8320 s / batch. (data: 3.10e-04). ETA=11:28:43, max mem: 20.9 GB 
[11/24 18:29:19 visual_prompt]: 	Training 200/553. train loss: 43.7321,	0.8360 s / batch. (data: 7.97e-03). ETA=11:30:41, max mem: 20.9 GB 
[11/24 18:30:59 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1527 s / batch. (data: 1.33e+00). ETA=1 day, 5:34:52, max mem: 20.9 GB 
[11/24 18:32:39 visual_prompt]: 	Training 400/553. train loss: 3.9212,	0.8315 s / batch. (data: 1.05e-02). ETA=11:24:10, max mem: 20.9 GB 
[11/24 18:34:19 visual_prompt]: 	Training 500/553. train loss: 7.5434,	0.8160 s / batch. (data: 3.09e-04). ETA=11:10:04, max mem: 20.9 GB 
[11/24 18:35:11 visual_prompt]: Epoch 11 / 100: avg data time: 1.93e-01, avg batch time: 1.0150, average train loss: 28.0970
[11/24 18:36:09 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3075, average loss: 22.0926
[11/24 18:36:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.95	
[11/24 18:36:09 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/24 18:37:55 visual_prompt]: 	Training 100/553. train loss: 13.9978,	0.8320 s / batch. (data: 3.19e-04). ETA=11:21:04, max mem: 20.9 GB 
[11/24 18:39:37 visual_prompt]: 	Training 200/553. train loss: 86.0450,	0.8189 s / batch. (data: 3.14e-04). ETA=11:08:58, max mem: 20.9 GB 
[11/24 18:41:16 visual_prompt]: 	Training 300/553. train loss: 195.4582,	0.8200 s / batch. (data: 3.52e-04). ETA=11:08:33, max mem: 20.9 GB 
[11/24 18:42:58 visual_prompt]: 	Training 400/553. train loss: 32.3267,	0.8400 s / batch. (data: 4.37e-04). ETA=11:23:24, max mem: 20.9 GB 
[11/24 18:44:39 visual_prompt]: 	Training 500/553. train loss: 104.1752,	0.8357 s / batch. (data: 1.00e-02). ETA=11:18:33, max mem: 20.9 GB 
[11/24 18:45:31 visual_prompt]: Epoch 12 / 100: avg data time: 1.95e-01, avg batch time: 1.0160, average train loss: 34.8851
[11/24 18:46:29 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.3063, average loss: 8.3360
[11/24 18:46:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.81	
[11/24 18:46:29 visual_prompt]: Best epoch 12: best metric: -8.336
[11/24 18:46:29 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/24 18:48:15 visual_prompt]: 	Training 100/553. train loss: 12.3251,	0.8175 s / batch. (data: 1.19e-02). ETA=11:01:39, max mem: 20.9 GB 
[11/24 18:49:53 visual_prompt]: 	Training 200/553. train loss: 2.0794,	0.8104 s / batch. (data: 5.37e-04). ETA=10:54:36, max mem: 20.9 GB 
[11/24 18:51:35 visual_prompt]: 	Training 300/553. train loss: 61.6420,	1.6317 s / batch. (data: 8.01e-01). ETA=21:55:16, max mem: 20.9 GB 
[11/24 18:53:15 visual_prompt]: 	Training 400/553. train loss: 211.3320,	0.8360 s / batch. (data: 7.94e-03). ETA=11:12:28, max mem: 20.9 GB 
[11/24 18:54:58 visual_prompt]: 	Training 500/553. train loss: 41.9996,	0.8411 s / batch. (data: 1.56e-02). ETA=11:15:08, max mem: 20.9 GB 
[11/24 18:55:50 visual_prompt]: Epoch 13 / 100: avg data time: 1.95e-01, avg batch time: 1.0155, average train loss: 33.2361
[11/24 18:56:48 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3057, average loss: 23.4402
[11/24 18:56:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.31	
[11/24 18:56:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/24 18:58:35 visual_prompt]: 	Training 100/553. train loss: 58.2859,	0.8320 s / batch. (data: 3.04e-04). ETA=11:05:44, max mem: 20.9 GB 
[11/24 19:00:16 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.2561 s / batch. (data: 4.44e-01). ETA=16:43:00, max mem: 20.9 GB 
[11/24 19:01:57 visual_prompt]: 	Training 300/553. train loss: 8.4469,	0.9760 s / batch. (data: 1.49e-01). ETA=12:57:42, max mem: 20.9 GB 
[11/24 19:03:38 visual_prompt]: 	Training 400/553. train loss: 24.6987,	0.8080 s / batch. (data: 3.27e-04). ETA=10:42:32, max mem: 20.9 GB 
[11/24 19:05:19 visual_prompt]: 	Training 500/553. train loss: 10.9003,	0.8400 s / batch. (data: 3.21e-04). ETA=11:06:32, max mem: 20.9 GB 
[11/24 19:06:10 visual_prompt]: Epoch 14 / 100: avg data time: 1.93e-01, avg batch time: 1.0146, average train loss: 27.9338
[11/24 19:07:08 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3059, average loss: 1.4110
[11/24 19:07:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.01	
[11/24 19:07:08 visual_prompt]: Best epoch 14: best metric: -1.411
[11/24 19:07:08 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/24 19:08:52 visual_prompt]: 	Training 100/553. train loss: 33.2693,	0.8243 s / batch. (data: 1.06e-02). ETA=10:52:01, max mem: 20.9 GB 
[11/24 19:10:32 visual_prompt]: 	Training 200/553. train loss: 175.5086,	0.8240 s / batch. (data: 6.46e-04). ETA=10:50:21, max mem: 20.9 GB 
[11/24 19:12:16 visual_prompt]: 	Training 300/553. train loss: 14.0868,	0.8375 s / batch. (data: 7.69e-03). ETA=10:59:38, max mem: 20.9 GB 
[11/24 19:13:55 visual_prompt]: 	Training 400/553. train loss: 43.9856,	0.8300 s / batch. (data: 3.24e-04). ETA=10:52:22, max mem: 20.9 GB 
[11/24 19:15:37 visual_prompt]: 	Training 500/553. train loss: 22.7769,	0.8243 s / batch. (data: 1.19e-02). ETA=10:46:32, max mem: 20.9 GB 
[11/24 19:16:30 visual_prompt]: Epoch 15 / 100: avg data time: 1.95e-01, avg batch time: 1.0169, average train loss: 30.8940
[11/24 19:17:28 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3066, average loss: 51.1618
[11/24 19:17:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 38.25	
[11/24 19:17:28 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/24 19:19:12 visual_prompt]: 	Training 100/553. train loss: 29.5155,	0.7988 s / batch. (data: 4.57e-04). ETA=10:24:27, max mem: 20.9 GB 
[11/24 19:20:55 visual_prompt]: 	Training 200/553. train loss: 37.1712,	0.8142 s / batch. (data: 3.50e-04). ETA=10:35:08, max mem: 20.9 GB 
[11/24 19:22:36 visual_prompt]: 	Training 300/553. train loss: 95.8599,	0.8507 s / batch. (data: 1.06e-02). ETA=11:02:10, max mem: 20.9 GB 
[11/24 19:24:17 visual_prompt]: 	Training 400/553. train loss: 46.6106,	0.8160 s / batch. (data: 3.29e-04). ETA=10:33:48, max mem: 20.9 GB 
[11/24 19:25:58 visual_prompt]: 	Training 500/553. train loss: 5.6730,	1.2200 s / batch. (data: 3.89e-01). ETA=15:45:35, max mem: 20.9 GB 
[11/24 19:26:51 visual_prompt]: Epoch 16 / 100: avg data time: 1.96e-01, avg batch time: 1.0175, average train loss: 30.0371
[11/24 19:27:49 visual_prompt]: Inference (val):avg data time: 2.60e-04, avg batch time: 0.3041, average loss: 16.7110
[11/24 19:27:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.45	
[11/24 19:27:49 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/24 19:29:33 visual_prompt]: 	Training 100/553. train loss: 0.9629,	0.8225 s / batch. (data: 2.97e-04). ETA=10:35:26, max mem: 20.9 GB 
[11/24 19:31:16 visual_prompt]: 	Training 200/553. train loss: 1.0105,	0.8384 s / batch. (data: 1.44e-02). ETA=10:46:18, max mem: 20.9 GB 
[11/24 19:32:56 visual_prompt]: 	Training 300/553. train loss: 31.9760,	0.8280 s / batch. (data: 2.86e-04). ETA=10:36:53, max mem: 20.9 GB 
[11/24 19:34:37 visual_prompt]: 	Training 400/553. train loss: 60.9852,	1.1176 s / batch. (data: 3.19e-01). ETA=14:17:46, max mem: 20.9 GB 
[11/24 19:36:17 visual_prompt]: 	Training 500/553. train loss: 10.4717,	1.3160 s / batch. (data: 4.79e-01). ETA=16:47:53, max mem: 20.9 GB 
[11/24 19:37:11 visual_prompt]: Epoch 17 / 100: avg data time: 1.95e-01, avg batch time: 1.0162, average train loss: 32.3750
[11/24 19:38:10 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.3067, average loss: 26.2978
[11/24 19:38:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.64	
[11/24 19:38:10 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/24 19:39:56 visual_prompt]: 	Training 100/553. train loss: 37.6888,	0.8206 s / batch. (data: 2.73e-04). ETA=10:26:21, max mem: 20.9 GB 
[11/24 19:41:39 visual_prompt]: 	Training 200/553. train loss: 47.4924,	0.8460 s / batch. (data: 5.46e-03). ETA=10:44:22, max mem: 20.9 GB 
[11/24 19:43:20 visual_prompt]: 	Training 300/553. train loss: 42.6714,	0.8200 s / batch. (data: 2.97e-04). ETA=10:23:11, max mem: 20.9 GB 
[11/24 19:45:01 visual_prompt]: 	Training 400/553. train loss: 5.9467,	0.8083 s / batch. (data: 3.12e-04). ETA=10:12:59, max mem: 20.9 GB 
[11/24 19:46:41 visual_prompt]: 	Training 500/553. train loss: 25.6926,	1.9440 s / batch. (data: 1.12e+00). ETA=1 day, 0:30:55, max mem: 20.9 GB 
[11/24 19:47:33 visual_prompt]: Epoch 18 / 100: avg data time: 1.97e-01, avg batch time: 1.0187, average train loss: 33.0730
[11/24 19:48:31 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3073, average loss: 18.0114
[11/24 19:48:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.93	
[11/24 19:48:31 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/24 19:50:16 visual_prompt]: 	Training 100/553. train loss: 6.7463,	1.4708 s / batch. (data: 6.35e-01). ETA=18:29:09, max mem: 20.9 GB 
[11/24 19:51:58 visual_prompt]: 	Training 200/553. train loss: 18.9321,	0.8085 s / batch. (data: 3.21e-04). ETA=10:08:18, max mem: 20.9 GB 
[11/24 19:53:39 visual_prompt]: 	Training 300/553. train loss: 18.4357,	0.8155 s / batch. (data: 5.46e-03). ETA=10:12:13, max mem: 20.9 GB 
[11/24 19:55:22 visual_prompt]: 	Training 400/553. train loss: 20.8769,	0.8171 s / batch. (data: 3.13e-04). ETA=10:12:03, max mem: 20.9 GB 
[11/24 19:56:59 visual_prompt]: 	Training 500/553. train loss: 38.7176,	0.8411 s / batch. (data: 5.43e-03). ETA=10:28:38, max mem: 20.9 GB 
[11/24 19:57:52 visual_prompt]: Epoch 19 / 100: avg data time: 1.93e-01, avg batch time: 1.0145, average train loss: 33.2714
[11/24 19:58:50 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3066, average loss: 45.4579
[11/24 19:58:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.63	
[11/24 19:58:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/24 20:00:34 visual_prompt]: 	Training 100/553. train loss: 12.9582,	0.8074 s / batch. (data: 3.19e-04). ETA=10:01:26, max mem: 20.9 GB 
[11/24 20:02:16 visual_prompt]: 	Training 200/553. train loss: 39.6115,	0.8307 s / batch. (data: 1.05e-02). ETA=10:17:25, max mem: 20.9 GB 
[11/24 20:03:57 visual_prompt]: 	Training 300/553. train loss: 121.8446,	0.8158 s / batch. (data: 5.42e-03). ETA=10:04:57, max mem: 20.9 GB 
[11/24 20:05:37 visual_prompt]: 	Training 400/553. train loss: 47.2035,	0.8240 s / batch. (data: 3.18e-04). ETA=10:09:40, max mem: 20.9 GB 
[11/24 20:07:18 visual_prompt]: 	Training 500/553. train loss: 39.6276,	0.8045 s / batch. (data: 2.86e-04). ETA=9:53:53, max mem: 20.9 GB 
[11/24 20:08:12 visual_prompt]: Epoch 20 / 100: avg data time: 1.94e-01, avg batch time: 1.0153, average train loss: 35.4635
[11/24 20:09:10 visual_prompt]: Inference (val):avg data time: 2.62e-04, avg batch time: 0.3072, average loss: 42.7701
[11/24 20:09:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.73	
[11/24 20:09:10 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/24 20:10:58 visual_prompt]: 	Training 100/553. train loss: 47.9980,	0.8221 s / batch. (data: 3.32e-04). ETA=10:04:47, max mem: 20.9 GB 
[11/24 20:12:38 visual_prompt]: 	Training 200/553. train loss: 94.1435,	0.8120 s / batch. (data: 2.99e-04). ETA=9:56:01, max mem: 20.9 GB 
[11/24 20:14:18 visual_prompt]: 	Training 300/553. train loss: 185.8542,	0.9334 s / batch. (data: 1.10e-01). ETA=11:23:34, max mem: 20.9 GB 
[11/24 20:15:58 visual_prompt]: 	Training 400/553. train loss: 1.6639,	0.8158 s / batch. (data: 2.93e-04). ETA=9:56:03, max mem: 20.9 GB 
[11/24 20:17:41 visual_prompt]: 	Training 500/553. train loss: 20.4397,	0.8384 s / batch. (data: 1.32e-02). ETA=10:11:12, max mem: 20.9 GB 
[11/24 20:18:32 visual_prompt]: Epoch 21 / 100: avg data time: 1.95e-01, avg batch time: 1.0171, average train loss: 36.2581
[11/24 20:19:30 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3063, average loss: 52.4051
[11/24 20:19:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.97	
[11/24 20:19:30 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/24 20:21:14 visual_prompt]: 	Training 100/553. train loss: 27.0023,	0.8240 s / batch. (data: 4.33e-04). ETA=9:58:34, max mem: 20.9 GB 
[11/24 20:22:55 visual_prompt]: 	Training 200/553. train loss: 39.1766,	0.8171 s / batch. (data: 3.01e-04). ETA=9:52:13, max mem: 20.9 GB 
[11/24 20:24:34 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8155 s / batch. (data: 7.96e-03). ETA=9:49:41, max mem: 20.9 GB 
[11/24 20:26:16 visual_prompt]: 	Training 400/553. train loss: 20.2337,	0.8175 s / batch. (data: 5.46e-03). ETA=9:49:46, max mem: 20.9 GB 
[11/24 20:27:57 visual_prompt]: 	Training 500/553. train loss: 19.9813,	0.8114 s / batch. (data: 1.05e-02). ETA=9:44:00, max mem: 20.9 GB 
[11/24 20:28:52 visual_prompt]: Epoch 22 / 100: avg data time: 1.94e-01, avg batch time: 1.0148, average train loss: 32.4458
[11/24 20:29:50 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3067, average loss: 15.4074
[11/24 20:29:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.64	
[11/24 20:29:50 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/24 20:31:36 visual_prompt]: 	Training 100/553. train loss: 26.0183,	0.8301 s / batch. (data: 3.02e-04). ETA=9:55:23, max mem: 20.9 GB 
[11/24 20:33:18 visual_prompt]: 	Training 200/553. train loss: 13.7293,	0.8348 s / batch. (data: 1.69e-02). ETA=9:57:22, max mem: 20.9 GB 
[11/24 20:35:01 visual_prompt]: 	Training 300/553. train loss: 21.7702,	0.8080 s / batch. (data: 3.06e-04). ETA=9:36:50, max mem: 20.9 GB 
[11/24 20:36:40 visual_prompt]: 	Training 400/553. train loss: 6.2527,	0.8215 s / batch. (data: 7.79e-04). ETA=9:45:04, max mem: 20.9 GB 
[11/24 20:38:19 visual_prompt]: 	Training 500/553. train loss: 15.2459,	0.8160 s / batch. (data: 3.15e-04). ETA=9:39:49, max mem: 20.9 GB 
[11/24 20:39:12 visual_prompt]: Epoch 23 / 100: avg data time: 1.93e-01, avg batch time: 1.0160, average train loss: 34.6301
[11/24 20:40:10 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3087, average loss: 54.3078
[11/24 20:40:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.92	
[11/24 20:40:10 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/24 20:41:53 visual_prompt]: 	Training 100/553. train loss: 3.8505,	0.8480 s / batch. (data: 3.20e-04). ETA=10:00:23, max mem: 20.9 GB 
[11/24 20:43:34 visual_prompt]: 	Training 200/553. train loss: 44.8869,	0.8463 s / batch. (data: 9.65e-03). ETA=9:57:48, max mem: 20.9 GB 
[11/24 20:45:15 visual_prompt]: 	Training 300/553. train loss: 32.5267,	1.0840 s / batch. (data: 2.39e-01). ETA=12:43:52, max mem: 20.9 GB 
[11/24 20:46:57 visual_prompt]: 	Training 400/553. train loss: 2.1398,	0.8203 s / batch. (data: 1.05e-02). ETA=9:36:43, max mem: 20.9 GB 
[11/24 20:48:39 visual_prompt]: 	Training 500/553. train loss: 1.0928,	0.8482 s / batch. (data: 5.50e-03). ETA=9:54:54, max mem: 20.9 GB 
[11/24 20:49:32 visual_prompt]: Epoch 24 / 100: avg data time: 1.94e-01, avg batch time: 1.0164, average train loss: 31.5102
[11/24 20:50:30 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3083, average loss: 17.1132
[11/24 20:50:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.29	
[11/24 20:50:30 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/24 20:52:19 visual_prompt]: 	Training 100/553. train loss: 16.7239,	0.8144 s / batch. (data: 2.98e-04). ETA=9:29:05, max mem: 20.9 GB 
[11/24 20:53:57 visual_prompt]: 	Training 200/553. train loss: 9.5156,	0.8160 s / batch. (data: 3.23e-04). ETA=9:28:51, max mem: 20.9 GB 
[11/24 20:55:38 visual_prompt]: 	Training 300/553. train loss: 21.6935,	0.8164 s / batch. (data: 3.01e-04). ETA=9:27:46, max mem: 20.9 GB 
[11/24 20:57:18 visual_prompt]: 	Training 400/553. train loss: 3.4372,	1.2500 s / batch. (data: 4.40e-01). ETA=14:27:13, max mem: 20.9 GB 
[11/24 20:59:00 visual_prompt]: 	Training 500/553. train loss: 18.2334,	1.5840 s / batch. (data: 7.45e-01). ETA=18:16:22, max mem: 20.9 GB 
[11/24 20:59:54 visual_prompt]: Epoch 25 / 100: avg data time: 1.97e-01, avg batch time: 1.0186, average train loss: 34.4249
[11/24 21:00:52 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3069, average loss: 63.0027
[11/24 21:00:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.89	
[11/24 21:00:52 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/24 21:02:37 visual_prompt]: 	Training 100/553. train loss: 72.3546,	0.8204 s / batch. (data: 3.19e-04). ETA=9:25:45, max mem: 20.9 GB 
[11/24 21:04:19 visual_prompt]: 	Training 200/553. train loss: 101.9321,	1.8560 s / batch. (data: 1.05e+00). ETA=21:16:45, max mem: 20.9 GB 
[11/24 21:06:02 visual_prompt]: 	Training 300/553. train loss: 27.4071,	0.8200 s / batch. (data: 3.05e-04). ETA=9:22:43, max mem: 20.9 GB 
[11/24 21:07:41 visual_prompt]: 	Training 400/553. train loss: 30.2635,	0.8200 s / batch. (data: 3.00e-04). ETA=9:21:21, max mem: 20.9 GB 
[11/24 21:09:22 visual_prompt]: 	Training 500/553. train loss: 83.6241,	0.8200 s / batch. (data: 2.91e-04). ETA=9:20:00, max mem: 20.9 GB 
[11/24 21:10:14 visual_prompt]: Epoch 26 / 100: avg data time: 1.93e-01, avg batch time: 1.0173, average train loss: 31.8418
[11/24 21:11:12 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3062, average loss: 21.1550
[11/24 21:11:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.35	
[11/24 21:11:12 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/24 21:12:58 visual_prompt]: 	Training 100/553. train loss: 0.4042,	0.8280 s / batch. (data: 3.01e-04). ETA=9:23:20, max mem: 20.9 GB 
[11/24 21:14:39 visual_prompt]: 	Training 200/553. train loss: 3.8618,	1.5160 s / batch. (data: 6.90e-01). ETA=17:08:54, max mem: 20.9 GB 
[11/24 21:16:20 visual_prompt]: 	Training 300/553. train loss: 61.9464,	0.8066 s / batch. (data: 7.93e-03). ETA=9:06:07, max mem: 20.9 GB 
[11/24 21:18:02 visual_prompt]: 	Training 400/553. train loss: 5.1087,	0.8360 s / batch. (data: 3.17e-04). ETA=9:24:36, max mem: 20.9 GB 
[11/24 21:19:45 visual_prompt]: 	Training 500/553. train loss: 4.4242,	0.8391 s / batch. (data: 5.90e-03). ETA=9:25:19, max mem: 20.9 GB 
[11/24 21:20:35 visual_prompt]: Epoch 27 / 100: avg data time: 1.95e-01, avg batch time: 1.0171, average train loss: 28.6694
[11/24 21:21:33 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3072, average loss: 20.8540
[11/24 21:21:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/24 21:21:33 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/24 21:23:18 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8120 s / batch. (data: 5.40e-03). ETA=9:04:59, max mem: 20.9 GB 
[11/24 21:24:59 visual_prompt]: 	Training 200/553. train loss: 2.3331,	0.8430 s / batch. (data: 1.56e-02). ETA=9:24:20, max mem: 20.9 GB 
[11/24 21:26:40 visual_prompt]: 	Training 300/553. train loss: 10.2766,	1.5950 s / batch. (data: 7.88e-01). ETA=17:45:11, max mem: 20.9 GB 
[11/24 21:28:21 visual_prompt]: 	Training 400/553. train loss: 74.0288,	0.8592 s / batch. (data: 2.81e-04). ETA=9:32:21, max mem: 20.9 GB 
[11/24 21:30:00 visual_prompt]: 	Training 500/553. train loss: 41.2319,	0.8200 s / batch. (data: 3.52e-04). ETA=9:04:53, max mem: 20.9 GB 
[11/24 21:30:54 visual_prompt]: Epoch 28 / 100: avg data time: 1.92e-01, avg batch time: 1.0142, average train loss: 27.4799
[11/24 21:31:52 visual_prompt]: Inference (val):avg data time: 4.58e-05, avg batch time: 0.3068, average loss: 42.9842
[11/24 21:31:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.68	
[11/24 21:31:52 visual_prompt]: Stopping early.
[11/24 21:31:52 visual_prompt]: Rank of current process: 0. World size: 1
[11/24 21:31:52 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/24 21:31:52 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/24 21:31:52 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/24 21:31:52 visual_prompt]: Training with config:
[11/24 21:31:52 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/24 21:31:52 visual_prompt]: Loading training data...
[11/24 21:31:52 visual_prompt]: Constructing mammo-cbis dataset train...
[11/24 21:31:52 visual_prompt]: Loading validation data...
[11/24 21:31:52 visual_prompt]: Constructing mammo-cbis dataset val...
[11/24 21:31:52 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/24 21:31:55 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/24 21:31:55 visual_prompt]: tuned percent:0.525
[11/24 21:31:55 visual_prompt]: Device used for model: 0
[11/24 21:31:55 visual_prompt]: Setting up Evaluator...
[11/24 21:31:55 visual_prompt]: Setting up Trainer...
[11/24 21:31:55 visual_prompt]: 	Setting up the optimizer...
[11/24 21:31:56 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/24 21:33:40 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8440 s / batch. (data: 1.19e-02). ETA=12:56:29, max mem: 20.9 GB 
[11/24 21:35:20 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8480 s / batch. (data: 2.97e-04). ETA=12:58:42, max mem: 20.9 GB 
[11/24 21:37:03 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.0791 s / batch. (data: 2.33e-01). ETA=16:29:08, max mem: 20.9 GB 
[11/24 21:38:43 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8069 s / batch. (data: 2.99e-04). ETA=12:18:17, max mem: 20.9 GB 
[11/24 21:40:26 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8600 s / batch. (data: 7.26e-04). ETA=13:05:27, max mem: 20.9 GB 
[11/24 21:41:20 visual_prompt]: Epoch 1 / 100: avg data time: 1.97e-01, avg batch time: 1.0210, average train loss: 1.5403
[11/24 21:42:18 visual_prompt]: Inference (val):avg data time: 3.40e-04, avg batch time: 0.3053, average loss: 1.5201
[11/24 21:42:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/24 21:42:18 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/24 21:44:02 visual_prompt]: 	Training 100/553. train loss: 10.4643,	0.8253 s / batch. (data: 3.64e-04). ETA=12:31:39, max mem: 20.9 GB 
[11/24 21:45:43 visual_prompt]: 	Training 200/553. train loss: 0.0010,	1.5640 s / batch. (data: 7.19e-01). ETA=23:41:51, max mem: 20.9 GB 
[11/24 21:47:26 visual_prompt]: 	Training 300/553. train loss: 5.4646,	1.0901 s / batch. (data: 2.57e-01). ETA=16:29:14, max mem: 20.9 GB 
[11/24 21:49:05 visual_prompt]: 	Training 400/553. train loss: 5.3334,	0.8307 s / batch. (data: 3.61e-04). ETA=12:32:24, max mem: 20.9 GB 
[11/24 21:50:48 visual_prompt]: 	Training 500/553. train loss: 9.4859,	0.8360 s / batch. (data: 1.19e-02). ETA=12:35:50, max mem: 20.9 GB 
[11/24 21:51:40 visual_prompt]: Epoch 2 / 100: avg data time: 1.91e-01, avg batch time: 1.0154, average train loss: 4.9669
[11/24 21:52:38 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3058, average loss: 17.1788
[11/24 21:52:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.88	
[11/24 21:52:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/24 21:54:21 visual_prompt]: 	Training 100/553. train loss: 5.1646,	0.8250 s / batch. (data: 1.05e-02). ETA=12:23:46, max mem: 20.9 GB 
[11/24 21:56:04 visual_prompt]: 	Training 200/553. train loss: 2.0174,	1.7687 s / batch. (data: 9.63e-01). ETA=1 day, 2:31:36, max mem: 20.9 GB 
[11/24 21:57:44 visual_prompt]: 	Training 300/553. train loss: 3.7236,	0.8285 s / batch. (data: 1.24e-02). ETA=12:24:10, max mem: 20.9 GB 
[11/24 21:59:25 visual_prompt]: 	Training 400/553. train loss: 2.2168,	0.8102 s / batch. (data: 2.97e-04). ETA=12:06:21, max mem: 20.9 GB 
[11/24 22:01:08 visual_prompt]: 	Training 500/553. train loss: 6.8552,	1.4035 s / batch. (data: 5.72e-01). ETA=20:55:59, max mem: 20.9 GB 
[11/24 22:01:59 visual_prompt]: Epoch 3 / 100: avg data time: 1.91e-01, avg batch time: 1.0141, average train loss: 5.8513
[11/24 22:02:57 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3069, average loss: 14.7225
[11/24 22:02:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.89	
[11/24 22:02:57 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/24 22:04:44 visual_prompt]: 	Training 100/553. train loss: 0.7517,	0.8353 s / batch. (data: 3.06e-04). ETA=12:25:22, max mem: 20.9 GB 
[11/24 22:06:25 visual_prompt]: 	Training 200/553. train loss: 6.1720,	0.8210 s / batch. (data: 1.05e-02). ETA=12:11:15, max mem: 20.9 GB 
[11/24 22:08:06 visual_prompt]: 	Training 300/553. train loss: 2.6758,	1.6637 s / batch. (data: 8.41e-01). ETA=1 day, 0:39:01, max mem: 20.9 GB 
[11/24 22:09:43 visual_prompt]: 	Training 400/553. train loss: 13.2228,	1.5198 s / batch. (data: 6.97e-01). ETA=22:28:37, max mem: 20.9 GB 
[11/24 22:11:26 visual_prompt]: 	Training 500/553. train loss: 0.6358,	3.4154 s / batch. (data: 2.60e+00). ETA=2 days, 2:24:57, max mem: 20.9 GB 
[11/24 22:12:20 visual_prompt]: Epoch 4 / 100: avg data time: 1.96e-01, avg batch time: 1.0185, average train loss: 10.6225
[11/24 22:13:18 visual_prompt]: Inference (val):avg data time: 2.53e-04, avg batch time: 0.3057, average loss: 5.6601
[11/24 22:13:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.18	
[11/24 22:13:18 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/24 22:15:02 visual_prompt]: 	Training 100/553. train loss: 28.5359,	0.8122 s / batch. (data: 5.42e-03). ETA=11:57:14, max mem: 20.9 GB 
[11/24 22:16:43 visual_prompt]: 	Training 200/553. train loss: 2.2963,	1.4441 s / batch. (data: 6.13e-01). ETA=21:12:54, max mem: 20.9 GB 
[11/24 22:18:25 visual_prompt]: 	Training 300/553. train loss: 5.5521,	0.8120 s / batch. (data: 2.89e-04). ETA=11:54:22, max mem: 20.9 GB 
[11/24 22:20:05 visual_prompt]: 	Training 400/553. train loss: 3.1498,	0.8296 s / batch. (data: 3.09e-04). ETA=12:08:31, max mem: 20.9 GB 
[11/24 22:21:47 visual_prompt]: 	Training 500/553. train loss: 14.7823,	0.8509 s / batch. (data: 2.28e-02). ETA=12:25:46, max mem: 20.9 GB 
[11/24 22:22:41 visual_prompt]: Epoch 5 / 100: avg data time: 1.94e-01, avg batch time: 1.0173, average train loss: 11.0127
[11/24 22:23:39 visual_prompt]: Inference (val):avg data time: 1.02e-04, avg batch time: 0.3072, average loss: 19.4735
[11/24 22:23:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.91	
[11/24 22:23:39 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/24 22:25:26 visual_prompt]: 	Training 100/553. train loss: 5.0224,	0.8258 s / batch. (data: 3.03e-04). ETA=12:01:43, max mem: 20.9 GB 
[11/24 22:27:07 visual_prompt]: 	Training 200/553. train loss: 19.3753,	0.8397 s / batch. (data: 2.06e-02). ETA=12:12:26, max mem: 20.9 GB 
[11/24 22:28:46 visual_prompt]: 	Training 300/553. train loss: 3.0734,	0.8400 s / batch. (data: 3.32e-04). ETA=12:11:16, max mem: 20.9 GB 
[11/24 22:30:31 visual_prompt]: 	Training 400/553. train loss: 9.9597,	0.8240 s / batch. (data: 3.30e-04). ETA=11:55:58, max mem: 20.9 GB 
[11/24 22:32:11 visual_prompt]: 	Training 500/553. train loss: 16.4667,	0.8068 s / batch. (data: 3.51e-04). ETA=11:39:42, max mem: 20.9 GB 
[11/24 22:33:03 visual_prompt]: Epoch 6 / 100: avg data time: 1.95e-01, avg batch time: 1.0190, average train loss: 10.0695
[11/24 22:34:01 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3063, average loss: 8.6839
[11/24 22:34:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.73	
[11/24 22:34:01 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/24 22:35:44 visual_prompt]: 	Training 100/553. train loss: 8.0542,	0.8415 s / batch. (data: 5.46e-03). ETA=12:07:36, max mem: 20.9 GB 
[11/24 22:37:25 visual_prompt]: 	Training 200/553. train loss: 2.1701,	0.8133 s / batch. (data: 5.45e-03). ETA=11:41:52, max mem: 20.9 GB 
[11/24 22:39:10 visual_prompt]: 	Training 300/553. train loss: 27.3659,	1.9768 s / batch. (data: 1.17e+00). ETA=1 day, 4:22:43, max mem: 20.9 GB 
[11/24 22:40:51 visual_prompt]: 	Training 400/553. train loss: 0.6175,	2.0920 s / batch. (data: 1.25e+00). ETA=1 day, 5:58:29, max mem: 20.9 GB 
[11/24 22:42:31 visual_prompt]: 	Training 500/553. train loss: 5.7167,	0.8707 s / batch. (data: 2.27e-02). ETA=12:27:04, max mem: 20.9 GB 
[11/24 22:43:22 visual_prompt]: Epoch 7 / 100: avg data time: 1.92e-01, avg batch time: 1.0151, average train loss: 12.7131
[11/24 22:44:20 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3066, average loss: 3.9841
[11/24 22:44:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.10	
[11/24 22:44:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/24 22:46:04 visual_prompt]: 	Training 100/553. train loss: 7.3276,	0.8526 s / batch. (data: 1.06e-02). ETA=12:09:23, max mem: 20.9 GB 
[11/24 22:47:46 visual_prompt]: 	Training 200/553. train loss: 51.3404,	0.8362 s / batch. (data: 1.05e-02). ETA=11:53:55, max mem: 20.9 GB 
[11/24 22:49:28 visual_prompt]: 	Training 300/553. train loss: 0.8880,	0.8203 s / batch. (data: 3.11e-04). ETA=11:39:03, max mem: 20.9 GB 
[11/24 22:51:09 visual_prompt]: 	Training 400/553. train loss: 19.9931,	0.8176 s / batch. (data: 5.46e-03). ETA=11:35:23, max mem: 20.9 GB 
[11/24 22:52:51 visual_prompt]: 	Training 500/553. train loss: 0.0000,	1.5560 s / batch. (data: 7.24e-01). ETA=22:00:43, max mem: 20.9 GB 
[11/24 22:53:44 visual_prompt]: Epoch 8 / 100: avg data time: 1.96e-01, avg batch time: 1.0190, average train loss: 25.0387
[11/24 22:54:42 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3070, average loss: 11.9663
[11/24 22:54:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.23	
[11/24 22:54:42 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/24 22:56:27 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.16e-04). ETA=11:33:54, max mem: 20.9 GB 
[11/24 22:58:07 visual_prompt]: 	Training 200/553. train loss: 18.9466,	0.8488 s / batch. (data: 3.28e-02). ETA=11:56:56, max mem: 20.9 GB 
[11/24 22:59:49 visual_prompt]: 	Training 300/553. train loss: 1.4170,	1.8610 s / batch. (data: 1.04e+00). ETA=1 day, 2:08:42, max mem: 20.9 GB 
[11/24 23:01:31 visual_prompt]: 	Training 400/553. train loss: 2.0662,	0.8128 s / batch. (data: 3.66e-04). ETA=11:23:47, max mem: 20.9 GB 
[11/24 23:03:12 visual_prompt]: 	Training 500/553. train loss: 12.9637,	1.1753 s / batch. (data: 3.54e-01). ETA=16:26:45, max mem: 20.9 GB 
[11/24 23:04:04 visual_prompt]: Epoch 9 / 100: avg data time: 1.93e-01, avg batch time: 1.0160, average train loss: 21.0203
[11/24 23:05:02 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3065, average loss: 80.6455
[11/24 23:05:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.99	
[11/24 23:05:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/24 23:06:49 visual_prompt]: 	Training 100/553. train loss: 54.4525,	0.8295 s / batch. (data: 1.56e-02). ETA=11:34:17, max mem: 20.9 GB 
[11/24 23:08:29 visual_prompt]: 	Training 200/553. train loss: 18.4336,	0.8320 s / batch. (data: 3.08e-04). ETA=11:35:01, max mem: 20.9 GB 
[11/24 23:10:09 visual_prompt]: 	Training 300/553. train loss: 29.5771,	0.8079 s / batch. (data: 4.78e-04). ETA=11:13:32, max mem: 20.9 GB 
[11/24 23:11:47 visual_prompt]: 	Training 400/553. train loss: 51.3899,	0.8126 s / batch. (data: 1.13e-02). ETA=11:16:08, max mem: 20.9 GB 
[11/24 23:13:29 visual_prompt]: 	Training 500/553. train loss: 3.2221,	0.8138 s / batch. (data: 5.42e-03). ETA=11:15:44, max mem: 20.9 GB 
[11/24 23:14:22 visual_prompt]: Epoch 10 / 100: avg data time: 1.92e-01, avg batch time: 1.0134, average train loss: 23.4597
[11/24 23:15:20 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3078, average loss: 35.3130
[11/24 23:15:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.92	
[11/24 23:15:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/24 23:17:07 visual_prompt]: 	Training 100/553. train loss: 49.0889,	0.8208 s / batch. (data: 3.09e-04). ETA=11:19:28, max mem: 20.9 GB 
[11/24 23:18:50 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8116 s / batch. (data: 2.89e-04). ETA=11:10:31, max mem: 20.9 GB 
[11/24 23:20:30 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2105 s / batch. (data: 1.40e+00). ETA=1 day, 6:22:32, max mem: 20.9 GB 
[11/24 23:22:10 visual_prompt]: 	Training 400/553. train loss: 47.5715,	0.8101 s / batch. (data: 2.79e-04). ETA=11:06:37, max mem: 20.9 GB 
[11/24 23:23:49 visual_prompt]: 	Training 500/553. train loss: 90.2490,	0.8284 s / batch. (data: 1.49e-02). ETA=11:20:14, max mem: 20.9 GB 
[11/24 23:24:41 visual_prompt]: Epoch 11 / 100: avg data time: 1.93e-01, avg batch time: 1.0138, average train loss: 40.8844
[11/24 23:25:39 visual_prompt]: Inference (val):avg data time: 4.21e-04, avg batch time: 0.3076, average loss: 19.9013
[11/24 23:25:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.81	
[11/24 23:25:39 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/24 23:27:26 visual_prompt]: 	Training 100/553. train loss: 33.3353,	0.8360 s / batch. (data: 1.20e-02). ETA=11:24:21, max mem: 20.9 GB 
[11/24 23:29:08 visual_prompt]: 	Training 200/553. train loss: 8.5810,	0.8300 s / batch. (data: 7.73e-03). ETA=11:18:06, max mem: 20.9 GB 
[11/24 23:30:48 visual_prompt]: 	Training 300/553. train loss: 37.9235,	0.8380 s / batch. (data: 3.14e-04). ETA=11:23:10, max mem: 20.9 GB 
[11/24 23:32:30 visual_prompt]: 	Training 400/553. train loss: 5.5246,	0.8483 s / batch. (data: 8.26e-03). ETA=11:30:11, max mem: 20.9 GB 
[11/24 23:34:11 visual_prompt]: 	Training 500/553. train loss: 361.9593,	0.8280 s / batch. (data: 1.21e-02). ETA=11:12:16, max mem: 20.9 GB 
[11/24 23:35:02 visual_prompt]: Epoch 12 / 100: avg data time: 1.96e-01, avg batch time: 1.0183, average train loss: 33.7319
[11/24 23:36:00 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3056, average loss: 6.8668
[11/24 23:36:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.22	
[11/24 23:36:00 visual_prompt]: Best epoch 12: best metric: -6.867
[11/24 23:36:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/24 23:37:47 visual_prompt]: 	Training 100/553. train loss: 23.7033,	0.8346 s / batch. (data: 5.44e-03). ETA=11:15:31, max mem: 20.9 GB 
[11/24 23:39:25 visual_prompt]: 	Training 200/553. train loss: 24.2083,	0.8404 s / batch. (data: 1.56e-02). ETA=11:18:50, max mem: 20.9 GB 
[11/24 23:41:07 visual_prompt]: 	Training 300/553. train loss: 11.9505,	1.7927 s / batch. (data: 9.64e-01). ETA=1 day, 0:05:00, max mem: 20.9 GB 
[11/24 23:42:46 visual_prompt]: 	Training 400/553. train loss: 68.1326,	0.8447 s / batch. (data: 1.20e-02). ETA=11:19:30, max mem: 20.9 GB 
[11/24 23:44:28 visual_prompt]: 	Training 500/553. train loss: 43.9326,	0.8287 s / batch. (data: 1.19e-02). ETA=11:05:15, max mem: 20.9 GB 
[11/24 23:45:21 visual_prompt]: Epoch 13 / 100: avg data time: 1.91e-01, avg batch time: 1.0129, average train loss: 31.1751
[11/24 23:46:18 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3084, average loss: 9.2663
[11/24 23:46:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.03	
[11/24 23:46:18 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/24 23:48:04 visual_prompt]: 	Training 100/553. train loss: 9.2130,	0.8360 s / batch. (data: 3.76e-04). ETA=11:08:56, max mem: 20.9 GB 
[11/24 23:49:45 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.2640 s / batch. (data: 4.37e-01). ETA=16:49:17, max mem: 20.9 GB 
[11/24 23:51:25 visual_prompt]: 	Training 300/553. train loss: 19.4283,	0.9045 s / batch. (data: 7.36e-02). ETA=12:00:47, max mem: 20.9 GB 
[11/24 23:53:06 visual_prompt]: 	Training 400/553. train loss: 13.2197,	1.2320 s / batch. (data: 3.96e-01). ETA=16:19:39, max mem: 20.9 GB 
[11/24 23:54:48 visual_prompt]: 	Training 500/553. train loss: 6.1606,	0.8256 s / batch. (data: 3.02e-04). ETA=10:55:09, max mem: 20.9 GB 
[11/24 23:55:38 visual_prompt]: Epoch 14 / 100: avg data time: 1.91e-01, avg batch time: 1.0124, average train loss: 30.6143
[11/24 23:56:36 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3060, average loss: 9.9501
[11/24 23:56:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.47	
[11/24 23:56:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/24 23:58:21 visual_prompt]: 	Training 100/553. train loss: 16.6267,	0.8134 s / batch. (data: 5.45e-03). ETA=10:43:22, max mem: 20.9 GB 
[11/25 00:00:01 visual_prompt]: 	Training 200/553. train loss: 84.0703,	0.8599 s / batch. (data: 1.99e-02). ETA=11:18:45, max mem: 20.9 GB 
[11/25 00:01:44 visual_prompt]: 	Training 300/553. train loss: 69.5441,	0.8320 s / batch. (data: 8.21e-04). ETA=10:55:16, max mem: 20.9 GB 
[11/25 00:03:22 visual_prompt]: 	Training 400/553. train loss: 11.5677,	0.8360 s / batch. (data: 3.32e-04). ETA=10:57:03, max mem: 20.9 GB 
[11/25 00:05:04 visual_prompt]: 	Training 500/553. train loss: 1.6814,	0.8328 s / batch. (data: 2.91e-04). ETA=10:53:10, max mem: 20.9 GB 
[11/25 00:05:57 visual_prompt]: Epoch 15 / 100: avg data time: 1.92e-01, avg batch time: 1.0139, average train loss: 28.5416
[11/25 00:06:55 visual_prompt]: Inference (val):avg data time: 2.65e-04, avg batch time: 0.3072, average loss: 5.6738
[11/25 00:06:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.66	
[11/25 00:06:55 visual_prompt]: Best epoch 15: best metric: -5.674
[11/25 00:06:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/25 00:08:39 visual_prompt]: 	Training 100/553. train loss: 2.0761,	0.8199 s / batch. (data: 4.36e-04). ETA=10:40:55, max mem: 20.9 GB 
[11/25 00:10:21 visual_prompt]: 	Training 200/553. train loss: 74.7176,	0.8400 s / batch. (data: 4.62e-04). ETA=10:55:14, max mem: 20.9 GB 
[11/25 00:12:02 visual_prompt]: 	Training 300/553. train loss: 70.4559,	0.8193 s / batch. (data: 5.43e-03). ETA=10:37:45, max mem: 20.9 GB 
[11/25 00:13:43 visual_prompt]: 	Training 400/553. train loss: 32.9066,	0.8457 s / batch. (data: 7.60e-04). ETA=10:56:53, max mem: 20.9 GB 
[11/25 00:15:24 visual_prompt]: 	Training 500/553. train loss: 10.4075,	1.2693 s / batch. (data: 4.59e-01). ETA=16:23:47, max mem: 20.9 GB 
[11/25 00:16:17 visual_prompt]: Epoch 16 / 100: avg data time: 1.95e-01, avg batch time: 1.0164, average train loss: 35.6242
[11/25 00:17:15 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3054, average loss: 27.4504
[11/25 00:17:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.72	
[11/25 00:17:15 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/25 00:19:00 visual_prompt]: 	Training 100/553. train loss: 10.0498,	0.8440 s / batch. (data: 1.20e-02). ETA=10:52:01, max mem: 20.9 GB 
[11/25 00:20:42 visual_prompt]: 	Training 200/553. train loss: 2.1062,	0.8429 s / batch. (data: 1.05e-02). ETA=10:49:44, max mem: 20.9 GB 
[11/25 00:22:22 visual_prompt]: 	Training 300/553. train loss: 47.0998,	0.8176 s / batch. (data: 2.77e-04). ETA=10:28:51, max mem: 20.9 GB 
[11/25 00:24:02 visual_prompt]: 	Training 400/553. train loss: 48.3595,	0.8200 s / batch. (data: 1.13e-02). ETA=10:29:20, max mem: 20.9 GB 
[11/25 00:25:43 visual_prompt]: 	Training 500/553. train loss: 1.4778,	1.1587 s / batch. (data: 3.31e-01). ETA=14:47:24, max mem: 20.9 GB 
[11/25 00:26:37 visual_prompt]: Epoch 17 / 100: avg data time: 1.94e-01, avg batch time: 1.0162, average train loss: 27.8454
[11/25 00:27:35 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3067, average loss: 27.0778
[11/25 00:27:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.98	
[11/25 00:27:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/25 00:29:21 visual_prompt]: 	Training 100/553. train loss: 38.1946,	0.8394 s / batch. (data: 7.97e-03). ETA=10:40:43, max mem: 20.9 GB 
[11/25 00:31:04 visual_prompt]: 	Training 200/553. train loss: 16.9148,	0.8059 s / batch. (data: 3.13e-04). ETA=10:13:49, max mem: 20.9 GB 
[11/25 00:32:45 visual_prompt]: 	Training 300/553. train loss: 4.3457,	0.8237 s / batch. (data: 8.99e-03). ETA=10:26:00, max mem: 20.9 GB 
[11/25 00:34:26 visual_prompt]: 	Training 400/553. train loss: 22.5713,	0.8262 s / batch. (data: 2.93e-04). ETA=10:26:33, max mem: 20.9 GB 
[11/25 00:36:07 visual_prompt]: 	Training 500/553. train loss: 4.5779,	0.8131 s / batch. (data: 3.06e-04). ETA=10:15:14, max mem: 20.9 GB 
[11/25 00:36:58 visual_prompt]: Epoch 18 / 100: avg data time: 1.96e-01, avg batch time: 1.0175, average train loss: 34.1536
[11/25 00:37:56 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3044, average loss: 8.4810
[11/25 00:37:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.20	
[11/25 00:37:56 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/25 00:39:41 visual_prompt]: 	Training 100/553. train loss: 9.2575,	1.1340 s / batch. (data: 3.20e-01). ETA=14:15:07, max mem: 20.9 GB 
[11/25 00:41:23 visual_prompt]: 	Training 200/553. train loss: 21.5479,	0.8137 s / batch. (data: 6.77e-04). ETA=10:12:13, max mem: 20.9 GB 
[11/25 00:43:05 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8640 s / batch. (data: 1.19e-02). ETA=10:48:41, max mem: 20.9 GB 
[11/25 00:44:46 visual_prompt]: 	Training 400/553. train loss: 9.2315,	0.8207 s / batch. (data: 7.91e-04). ETA=10:14:47, max mem: 20.9 GB 
[11/25 00:46:23 visual_prompt]: 	Training 500/553. train loss: 0.2463,	0.8098 s / batch. (data: 3.28e-04). ETA=10:05:17, max mem: 20.9 GB 
[11/25 00:47:17 visual_prompt]: Epoch 19 / 100: avg data time: 1.92e-01, avg batch time: 1.0142, average train loss: 26.0521
[11/25 00:48:15 visual_prompt]: Inference (val):avg data time: 1.27e-04, avg batch time: 0.3060, average loss: 82.6336
[11/25 00:48:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.47	
[11/25 00:48:15 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/25 00:49:59 visual_prompt]: 	Training 100/553. train loss: 8.7580,	0.8294 s / batch. (data: 2.89e-04). ETA=10:17:47, max mem: 20.9 GB 
[11/25 00:51:40 visual_prompt]: 	Training 200/553. train loss: 1.7225,	0.8269 s / batch. (data: 3.24e-04). ETA=10:14:34, max mem: 20.9 GB 
[11/25 00:53:22 visual_prompt]: 	Training 300/553. train loss: 5.0633,	0.8225 s / batch. (data: 3.02e-04). ETA=10:09:54, max mem: 20.9 GB 
[11/25 00:55:02 visual_prompt]: 	Training 400/553. train loss: 18.6988,	0.8480 s / batch. (data: 3.07e-04). ETA=10:27:24, max mem: 20.9 GB 
[11/25 00:56:42 visual_prompt]: 	Training 500/553. train loss: 0.4188,	0.8341 s / batch. (data: 5.43e-03). ETA=10:15:44, max mem: 20.9 GB 
[11/25 00:57:36 visual_prompt]: Epoch 20 / 100: avg data time: 1.93e-01, avg batch time: 1.0151, average train loss: 28.4255
[11/25 00:58:34 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3056, average loss: 20.9842
[11/25 00:58:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.62	
[11/25 00:58:34 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/25 01:00:23 visual_prompt]: 	Training 100/553. train loss: 31.0035,	0.8209 s / batch. (data: 3.03e-04). ETA=10:03:54, max mem: 20.9 GB 
[11/25 01:02:03 visual_prompt]: 	Training 200/553. train loss: 56.9735,	0.8406 s / batch. (data: 1.19e-02). ETA=10:16:58, max mem: 20.9 GB 
[11/25 01:03:43 visual_prompt]: 	Training 300/553. train loss: 62.3923,	0.8280 s / batch. (data: 2.78e-04). ETA=10:06:21, max mem: 20.9 GB 
[11/25 01:05:23 visual_prompt]: 	Training 400/553. train loss: 42.8589,	0.8059 s / batch. (data: 3.17e-04). ETA=9:48:48, max mem: 20.9 GB 
[11/25 01:07:05 visual_prompt]: 	Training 500/553. train loss: 15.1508,	0.8160 s / batch. (data: 2.97e-04). ETA=9:54:52, max mem: 20.9 GB 
[11/25 01:07:57 visual_prompt]: Epoch 21 / 100: avg data time: 1.95e-01, avg batch time: 1.0179, average train loss: 22.8141
[11/25 01:08:55 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3052, average loss: 1.3783
[11/25 01:08:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/25 01:08:55 visual_prompt]: Best epoch 21: best metric: -1.378
[11/25 01:08:55 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/25 01:10:39 visual_prompt]: 	Training 100/553. train loss: 19.0938,	0.8376 s / batch. (data: 5.41e-03). ETA=10:08:29, max mem: 20.9 GB 
[11/25 01:12:20 visual_prompt]: 	Training 200/553. train loss: 4.2181,	0.8149 s / batch. (data: 4.35e-04). ETA=9:50:35, max mem: 20.9 GB 
[11/25 01:13:59 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8083 s / batch. (data: 3.25e-04). ETA=9:44:27, max mem: 20.9 GB 
[11/25 01:15:41 visual_prompt]: 	Training 400/553. train loss: 35.0689,	0.8240 s / batch. (data: 3.08e-04). ETA=9:54:27, max mem: 20.9 GB 
[11/25 01:17:22 visual_prompt]: 	Training 500/553. train loss: 61.7992,	0.8254 s / batch. (data: 3.25e-04). ETA=9:54:05, max mem: 20.9 GB 
[11/25 01:18:16 visual_prompt]: Epoch 22 / 100: avg data time: 1.94e-01, avg batch time: 1.0141, average train loss: 28.6100
[11/25 01:19:14 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3047, average loss: 40.7791
[11/25 01:19:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.47	
[11/25 01:19:14 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/25 01:21:01 visual_prompt]: 	Training 100/553. train loss: 17.7667,	0.8280 s / batch. (data: 2.78e-04). ETA=9:53:53, max mem: 20.9 GB 
[11/25 01:22:43 visual_prompt]: 	Training 200/553. train loss: 75.8020,	0.8803 s / batch. (data: 6.44e-02). ETA=10:29:54, max mem: 20.9 GB 
[11/25 01:24:25 visual_prompt]: 	Training 300/553. train loss: 12.9878,	0.8203 s / batch. (data: 3.02e-04). ETA=9:45:36, max mem: 20.9 GB 
[11/25 01:26:04 visual_prompt]: 	Training 400/553. train loss: 5.6556,	0.8567 s / batch. (data: 8.26e-04). ETA=10:10:11, max mem: 20.9 GB 
[11/25 01:27:44 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8009 s / batch. (data: 3.02e-04). ETA=9:29:07, max mem: 20.9 GB 
[11/25 01:28:37 visual_prompt]: Epoch 23 / 100: avg data time: 1.94e-01, avg batch time: 1.0168, average train loss: 26.6718
[11/25 01:29:35 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3066, average loss: 2.5826
[11/25 01:29:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.97	
[11/25 01:29:35 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/25 01:31:17 visual_prompt]: 	Training 100/553. train loss: 99.1741,	0.8120 s / batch. (data: 3.01e-04). ETA=9:34:55, max mem: 20.9 GB 
[11/25 01:32:58 visual_prompt]: 	Training 200/553. train loss: 30.3081,	0.8079 s / batch. (data: 2.61e-04). ETA=9:30:38, max mem: 20.9 GB 
[11/25 01:34:39 visual_prompt]: 	Training 300/553. train loss: 11.2261,	1.1043 s / batch. (data: 2.92e-01). ETA=12:58:09, max mem: 20.9 GB 
[11/25 01:36:21 visual_prompt]: 	Training 400/553. train loss: 1.6055,	0.8101 s / batch. (data: 3.59e-04). ETA=9:29:31, max mem: 20.9 GB 
[11/25 01:38:03 visual_prompt]: 	Training 500/553. train loss: 91.2325,	0.8459 s / batch. (data: 1.56e-02). ETA=9:53:18, max mem: 20.9 GB 
[11/25 01:38:58 visual_prompt]: Epoch 24 / 100: avg data time: 1.96e-01, avg batch time: 1.0176, average train loss: 29.2039
[11/25 01:39:56 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3078, average loss: 2.2943
[11/25 01:39:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.61	
[11/25 01:39:56 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/25 01:41:44 visual_prompt]: 	Training 100/553. train loss: 11.0729,	0.8360 s / batch. (data: 1.59e-02). ETA=9:44:11, max mem: 20.9 GB 
[11/25 01:43:22 visual_prompt]: 	Training 200/553. train loss: 41.8671,	0.8280 s / batch. (data: 3.11e-04). ETA=9:37:14, max mem: 20.9 GB 
[11/25 01:45:03 visual_prompt]: 	Training 300/553. train loss: 31.9652,	0.8280 s / batch. (data: 3.11e-04). ETA=9:35:50, max mem: 20.9 GB 
[11/25 01:46:43 visual_prompt]: 	Training 400/553. train loss: 5.9064,	1.4034 s / batch. (data: 5.87e-01). ETA=16:13:39, max mem: 20.9 GB 
[11/25 01:48:25 visual_prompt]: 	Training 500/553. train loss: 37.1354,	1.4806 s / batch. (data: 6.63e-01). ETA=17:04:47, max mem: 20.9 GB 
[11/25 01:49:18 visual_prompt]: Epoch 25 / 100: avg data time: 1.93e-01, avg batch time: 1.0158, average train loss: 39.3230
[11/25 01:50:16 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3064, average loss: 95.5960
[11/25 01:50:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.02	
[11/25 01:50:16 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/25 01:52:00 visual_prompt]: 	Training 100/553. train loss: 20.8582,	0.8493 s / batch. (data: 4.14e-02). ETA=9:45:39, max mem: 20.9 GB 
[11/25 01:53:44 visual_prompt]: 	Training 200/553. train loss: 25.1038,	1.8427 s / batch. (data: 1.03e+00). ETA=21:07:36, max mem: 20.9 GB 
[11/25 01:55:27 visual_prompt]: 	Training 300/553. train loss: 26.8676,	0.8144 s / batch. (data: 3.39e-04). ETA=9:18:54, max mem: 20.9 GB 
[11/25 01:57:07 visual_prompt]: 	Training 400/553. train loss: 48.3365,	0.8116 s / batch. (data: 3.60e-04). ETA=9:15:36, max mem: 20.9 GB 
[11/25 01:58:45 visual_prompt]: 	Training 500/553. train loss: 8.3600,	0.8144 s / batch. (data: 4.31e-04). ETA=9:16:08, max mem: 20.9 GB 
[11/25 01:59:38 visual_prompt]: Epoch 26 / 100: avg data time: 1.93e-01, avg batch time: 1.0158, average train loss: 25.6008
[11/25 02:00:36 visual_prompt]: Inference (val):avg data time: 2.19e-04, avg batch time: 0.3065, average loss: 31.2810
[11/25 02:00:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.85	
[11/25 02:00:36 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/25 02:02:23 visual_prompt]: 	Training 100/553. train loss: 1.9237,	0.8268 s / batch. (data: 3.16e-04). ETA=9:22:31, max mem: 20.9 GB 
[11/25 02:04:03 visual_prompt]: 	Training 200/553. train loss: 16.5352,	1.4200 s / batch. (data: 5.97e-01). ETA=16:03:44, max mem: 20.9 GB 
[11/25 02:05:45 visual_prompt]: 	Training 300/553. train loss: 26.6875,	0.8242 s / batch. (data: 3.36e-04). ETA=9:17:58, max mem: 20.9 GB 
[11/25 02:07:27 visual_prompt]: 	Training 400/553. train loss: 22.0815,	0.8298 s / batch. (data: 1.10e-02). ETA=9:20:25, max mem: 20.9 GB 
[11/25 02:09:09 visual_prompt]: 	Training 500/553. train loss: 22.2407,	0.8080 s / batch. (data: 3.09e-04). ETA=9:04:21, max mem: 20.9 GB 
[11/25 02:10:00 visual_prompt]: Epoch 27 / 100: avg data time: 1.97e-01, avg batch time: 1.0190, average train loss: 29.5681
[11/25 02:10:58 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3083, average loss: 9.5758
[11/25 02:10:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.81	
[11/25 02:10:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/25 02:12:42 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8260 s / batch. (data: 3.08e-04). ETA=9:14:23, max mem: 20.9 GB 
[11/25 02:14:24 visual_prompt]: 	Training 200/553. train loss: 6.5409,	0.8200 s / batch. (data: 3.19e-04). ETA=9:08:58, max mem: 20.9 GB 
[11/25 02:16:07 visual_prompt]: 	Training 300/553. train loss: 50.0483,	1.7640 s / batch. (data: 9.48e-01). ETA=19:38:02, max mem: 20.9 GB 
[11/25 02:17:47 visual_prompt]: 	Training 400/553. train loss: 83.2198,	0.8601 s / batch. (data: 7.56e-04). ETA=9:32:56, max mem: 20.9 GB 
[11/25 02:19:27 visual_prompt]: 	Training 500/553. train loss: 151.5318,	0.8312 s / batch. (data: 1.05e-02). ETA=9:12:18, max mem: 20.9 GB 
[11/25 02:20:19 visual_prompt]: Epoch 28 / 100: avg data time: 1.92e-01, avg batch time: 1.0142, average train loss: 33.4609
[11/25 02:21:17 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3064, average loss: 80.3306
[11/25 02:21:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.43	
[11/25 02:21:17 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/25 02:23:08 visual_prompt]: 	Training 100/553. train loss: 1.9684,	0.8410 s / batch. (data: 7.80e-04). ETA=9:16:39, max mem: 20.9 GB 
[11/25 02:24:48 visual_prompt]: 	Training 200/553. train loss: 0.9400,	1.8479 s / batch. (data: 1.04e+00). ETA=20:20:06, max mem: 20.9 GB 
[11/25 02:26:28 visual_prompt]: 	Training 300/553. train loss: 7.0530,	0.8240 s / batch. (data: 2.99e-04). ETA=9:02:39, max mem: 20.9 GB 
[11/25 02:28:06 visual_prompt]: 	Training 400/553. train loss: 34.1399,	1.3721 s / batch. (data: 5.55e-01). ETA=15:01:21, max mem: 20.9 GB 
[11/25 02:29:47 visual_prompt]: 	Training 500/553. train loss: 18.4471,	0.8288 s / batch. (data: 7.93e-03). ETA=9:03:03, max mem: 20.9 GB 
[11/25 02:30:39 visual_prompt]: Epoch 29 / 100: avg data time: 1.94e-01, avg batch time: 1.0162, average train loss: 24.1689
[11/25 02:31:38 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3058, average loss: 9.8496
[11/25 02:31:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.99	
[11/25 02:31:38 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/25 02:33:21 visual_prompt]: 	Training 100/553. train loss: 60.4559,	0.8200 s / batch. (data: 7.94e-03). ETA=8:55:13, max mem: 20.9 GB 
[11/25 02:35:03 visual_prompt]: 	Training 200/553. train loss: 74.9746,	0.8160 s / batch. (data: 3.07e-04). ETA=8:51:15, max mem: 20.9 GB 
[11/25 02:36:43 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.1690 s / batch. (data: 3.29e-01). ETA=12:39:06, max mem: 20.9 GB 
[11/25 02:38:25 visual_prompt]: 	Training 400/553. train loss: 12.4831,	1.0704 s / batch. (data: 2.54e-01). ETA=11:33:17, max mem: 20.9 GB 
[11/25 02:40:05 visual_prompt]: 	Training 500/553. train loss: 11.8023,	1.4102 s / batch. (data: 5.98e-01). ETA=15:11:05, max mem: 20.9 GB 
[11/25 02:40:59 visual_prompt]: Epoch 30 / 100: avg data time: 1.94e-01, avg batch time: 1.0156, average train loss: 32.8756
[11/25 02:41:57 visual_prompt]: Inference (val):avg data time: 1.82e-04, avg batch time: 0.3063, average loss: 43.5503
[11/25 02:41:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.66	
[11/25 02:41:57 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[11/25 02:43:44 visual_prompt]: 	Training 100/553. train loss: 11.9113,	0.8073 s / batch. (data: 3.62e-04). ETA=8:39:28, max mem: 20.9 GB 
[11/25 02:45:26 visual_prompt]: 	Training 200/553. train loss: 36.5928,	0.8271 s / batch. (data: 2.78e-04). ETA=8:50:51, max mem: 20.9 GB 
[11/25 02:47:05 visual_prompt]: 	Training 300/553. train loss: 67.0444,	0.8146 s / batch. (data: 2.83e-04). ETA=8:41:30, max mem: 20.9 GB 
[11/25 02:48:45 visual_prompt]: 	Training 400/553. train loss: 103.4922,	1.8723 s / batch. (data: 1.06e+00). ETA=19:55:26, max mem: 20.9 GB 
[11/25 02:50:26 visual_prompt]: 	Training 500/553. train loss: 14.1946,	0.8400 s / batch. (data: 3.08e-04). ETA=8:54:56, max mem: 20.9 GB 
[11/25 02:51:18 visual_prompt]: Epoch 31 / 100: avg data time: 1.92e-01, avg batch time: 1.0132, average train loss: 28.0165
[11/25 02:52:16 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3068, average loss: 30.4209
[11/25 02:52:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.14	
[11/25 02:52:16 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[11/25 02:54:02 visual_prompt]: 	Training 100/553. train loss: 2.3228,	0.8265 s / batch. (data: 5.93e-03). ETA=8:44:12, max mem: 20.9 GB 
[11/25 02:55:43 visual_prompt]: 	Training 200/553. train loss: 34.4871,	0.8324 s / batch. (data: 5.88e-03). ETA=8:46:35, max mem: 20.9 GB 
[11/25 02:57:27 visual_prompt]: 	Training 300/553. train loss: 5.7643,	0.8064 s / batch. (data: 2.79e-04). ETA=8:28:46, max mem: 20.9 GB 
[11/25 02:59:08 visual_prompt]: 	Training 400/553. train loss: 13.2650,	0.8240 s / batch. (data: 2.95e-04). ETA=8:38:30, max mem: 20.9 GB 
[11/25 03:00:46 visual_prompt]: 	Training 500/553. train loss: 23.5068,	0.8446 s / batch. (data: 2.45e-02). ETA=8:50:04, max mem: 20.9 GB 
[11/25 03:01:37 visual_prompt]: Epoch 32 / 100: avg data time: 1.93e-01, avg batch time: 1.0150, average train loss: 25.0306
[11/25 03:02:35 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3073, average loss: 22.8979
[11/25 03:02:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.84	
[11/25 03:02:35 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[11/25 03:04:19 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.3743 s / batch. (data: 5.54e-01). ETA=14:19:02, max mem: 20.9 GB 
[11/25 03:06:01 visual_prompt]: 	Training 200/553. train loss: 6.4431,	1.4280 s / batch. (data: 5.95e-01). ETA=14:50:12, max mem: 20.9 GB 
[11/25 03:07:41 visual_prompt]: 	Training 300/553. train loss: 6.4296,	0.8457 s / batch. (data: 1.05e-02). ETA=8:45:46, max mem: 20.9 GB 
[11/25 03:09:23 visual_prompt]: 	Training 400/553. train loss: 9.9908,	0.8082 s / batch. (data: 3.04e-04). ETA=8:21:06, max mem: 20.9 GB 
[11/25 03:11:04 visual_prompt]: 	Training 500/553. train loss: 21.1172,	0.8109 s / batch. (data: 5.54e-03). ETA=8:21:28, max mem: 20.9 GB 
[11/25 03:11:55 visual_prompt]: Epoch 33 / 100: avg data time: 1.91e-01, avg batch time: 1.0128, average train loss: 22.6111
[11/25 03:12:53 visual_prompt]: Inference (val):avg data time: 1.77e-04, avg batch time: 0.3065, average loss: 99.1680
[11/25 03:12:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.55	
[11/25 03:12:53 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[11/25 03:14:40 visual_prompt]: 	Training 100/553. train loss: 1.5003,	0.8147 s / batch. (data: 5.46e-03). ETA=8:21:43, max mem: 20.9 GB 
[11/25 03:16:18 visual_prompt]: 	Training 200/553. train loss: 5.6217,	0.8160 s / batch. (data: 7.99e-03). ETA=8:21:11, max mem: 20.9 GB 
[11/25 03:17:58 visual_prompt]: 	Training 300/553. train loss: 25.5805,	0.8146 s / batch. (data: 5.41e-03). ETA=8:18:58, max mem: 20.9 GB 
[11/25 03:19:41 visual_prompt]: 	Training 400/553. train loss: 4.7217,	0.8169 s / batch. (data: 2.84e-04). ETA=8:18:59, max mem: 20.9 GB 
[11/25 03:21:22 visual_prompt]: 	Training 500/553. train loss: 1.9305,	1.5200 s / batch. (data: 6.95e-01). ETA=15:25:57, max mem: 20.9 GB 
[11/25 03:22:14 visual_prompt]: Epoch 34 / 100: avg data time: 1.91e-01, avg batch time: 1.0134, average train loss: 21.0316
[11/25 03:23:12 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3073, average loss: 14.2374
[11/25 03:23:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.20	
[11/25 03:23:12 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[11/25 03:24:59 visual_prompt]: 	Training 100/553. train loss: 14.9943,	0.8111 s / batch. (data: 2.83e-04). ETA=8:12:00, max mem: 20.9 GB 
[11/25 03:26:41 visual_prompt]: 	Training 200/553. train loss: 24.9822,	0.8194 s / batch. (data: 3.19e-04). ETA=8:15:42, max mem: 20.9 GB 
[11/25 03:28:21 visual_prompt]: 	Training 300/553. train loss: 18.4030,	0.8348 s / batch. (data: 5.44e-03). ETA=8:23:37, max mem: 20.9 GB 
[11/25 03:30:00 visual_prompt]: 	Training 400/553. train loss: 50.2889,	0.8686 s / batch. (data: 5.48e-02). ETA=8:42:34, max mem: 20.9 GB 
[11/25 03:31:41 visual_prompt]: 	Training 500/553. train loss: 4.6468,	1.2520 s / batch. (data: 4.37e-01). ETA=12:31:09, max mem: 20.9 GB 
[11/25 03:32:33 visual_prompt]: Epoch 35 / 100: avg data time: 1.92e-01, avg batch time: 1.0154, average train loss: 20.0561
[11/25 03:33:31 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3060, average loss: 4.8598
[11/25 03:33:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.05	
[11/25 03:33:31 visual_prompt]: Stopping early.
[11/25 03:33:31 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 03:33:31 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 03:33:31 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/25 03:33:31 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 03:33:31 visual_prompt]: Training with config:
[11/25 03:33:31 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr10.0_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/25 03:33:31 visual_prompt]: Loading training data...
[11/25 03:33:31 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 03:33:31 visual_prompt]: Loading validation data...
[11/25 03:33:31 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 03:33:32 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 03:33:34 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 03:33:34 visual_prompt]: tuned percent:0.525
[11/25 03:33:34 visual_prompt]: Device used for model: 0
[11/25 03:33:34 visual_prompt]: Setting up Evaluator...
[11/25 03:33:34 visual_prompt]: Setting up Trainer...
[11/25 03:33:34 visual_prompt]: 	Setting up the optimizer...
[11/25 03:33:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 03:35:19 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8277 s / batch. (data: 7.98e-03). ETA=12:41:28, max mem: 20.9 GB 
[11/25 03:36:59 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8440 s / batch. (data: 7.94e-03). ETA=12:55:04, max mem: 20.9 GB 
[11/25 03:38:43 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3520 s / batch. (data: 5.28e-01). ETA=20:39:19, max mem: 20.9 GB 
[11/25 03:40:22 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8280 s / batch. (data: 2.89e-04). ETA=12:37:37, max mem: 20.9 GB 
[11/25 03:42:05 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8360 s / batch. (data: 3.26e-04). ETA=12:43:32, max mem: 20.9 GB 
[11/25 03:42:58 visual_prompt]: Epoch 1 / 100: avg data time: 1.94e-01, avg batch time: 1.0180, average train loss: 1.5403
[11/25 03:43:56 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3071, average loss: 1.5201
[11/25 03:43:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 03:43:56 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[11/25 03:45:40 visual_prompt]: 	Training 100/553. train loss: 2.4547,	0.8200 s / batch. (data: 3.16e-04). ETA=12:26:50, max mem: 20.9 GB 
[11/25 03:47:21 visual_prompt]: 	Training 200/553. train loss: 0.0000,	0.8274 s / batch. (data: 3.29e-04). ETA=12:32:14, max mem: 20.9 GB 
[11/25 03:49:04 visual_prompt]: 	Training 300/553. train loss: 7.6994,	1.0080 s / batch. (data: 1.86e-01). ETA=15:14:42, max mem: 20.9 GB 
[11/25 03:50:44 visual_prompt]: 	Training 400/553. train loss: 0.7945,	0.8291 s / batch. (data: 1.05e-02). ETA=12:30:58, max mem: 20.9 GB 
[11/25 03:52:27 visual_prompt]: 	Training 500/553. train loss: 0.8968,	0.8241 s / batch. (data: 5.44e-03). ETA=12:25:03, max mem: 20.9 GB 
[11/25 03:53:18 visual_prompt]: Epoch 2 / 100: avg data time: 1.92e-01, avg batch time: 1.0170, average train loss: 4.2426
[11/25 03:54:16 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.3077, average loss: 0.7099
[11/25 03:54:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.19	rocauc: 59.89	
[11/25 03:54:16 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[11/25 03:55:59 visual_prompt]: 	Training 100/553. train loss: 6.4990,	0.8372 s / batch. (data: 2.12e-02). ETA=12:34:49, max mem: 20.9 GB 
[11/25 03:57:42 visual_prompt]: 	Training 200/553. train loss: 4.0049,	0.8109 s / batch. (data: 2.94e-04). ETA=12:09:44, max mem: 20.9 GB 
[11/25 03:59:22 visual_prompt]: 	Training 300/553. train loss: 0.8738,	0.8338 s / batch. (data: 7.92e-04). ETA=12:28:59, max mem: 20.9 GB 
[11/25 04:01:03 visual_prompt]: 	Training 400/553. train loss: 8.9774,	0.8191 s / batch. (data: 5.42e-03). ETA=12:14:22, max mem: 20.9 GB 
[11/25 04:02:46 visual_prompt]: 	Training 500/553. train loss: 7.3340,	1.4640 s / batch. (data: 6.40e-01). ETA=21:50:08, max mem: 20.9 GB 
[11/25 04:03:37 visual_prompt]: Epoch 3 / 100: avg data time: 1.91e-01, avg batch time: 1.0147, average train loss: 7.6226
[11/25 04:04:35 visual_prompt]: Inference (val):avg data time: 4.27e-04, avg batch time: 0.3066, average loss: 5.2789
[11/25 04:04:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.01	
[11/25 04:04:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[11/25 04:06:22 visual_prompt]: 	Training 100/553. train loss: 7.9321,	0.8218 s / batch. (data: 1.05e-02). ETA=12:13:20, max mem: 20.9 GB 
[11/25 04:08:03 visual_prompt]: 	Training 200/553. train loss: 12.9808,	0.8240 s / batch. (data: 7.95e-03). ETA=12:13:55, max mem: 20.9 GB 
[11/25 04:09:45 visual_prompt]: 	Training 300/553. train loss: 8.3782,	1.5240 s / batch. (data: 7.01e-01). ETA=22:34:49, max mem: 20.9 GB 
[11/25 04:11:23 visual_prompt]: 	Training 400/553. train loss: 0.5039,	1.7422 s / batch. (data: 9.09e-01). ETA=1 day, 1:45:54, max mem: 20.9 GB 
[11/25 04:13:06 visual_prompt]: 	Training 500/553. train loss: 31.1022,	3.5235 s / batch. (data: 2.72e+00). ETA=2 days, 4:00:40, max mem: 20.9 GB 
[11/25 04:14:00 visual_prompt]: Epoch 4 / 100: avg data time: 1.97e-01, avg batch time: 1.0207, average train loss: 8.4598
[11/25 04:14:58 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3048, average loss: 31.7809
[11/25 04:14:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/25 04:14:58 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[11/25 04:16:41 visual_prompt]: 	Training 100/553. train loss: 0.0982,	0.8120 s / batch. (data: 2.98e-04). ETA=11:57:06, max mem: 20.9 GB 
[11/25 04:18:23 visual_prompt]: 	Training 200/553. train loss: 5.2413,	1.2320 s / batch. (data: 4.01e-01). ETA=18:05:56, max mem: 20.9 GB 
[11/25 04:20:06 visual_prompt]: 	Training 300/553. train loss: 3.9706,	0.8380 s / batch. (data: 5.41e-03). ETA=12:17:18, max mem: 20.9 GB 
[11/25 04:21:46 visual_prompt]: 	Training 400/553. train loss: 0.8584,	0.8265 s / batch. (data: 2.99e-04). ETA=12:05:44, max mem: 20.9 GB 
[11/25 04:23:28 visual_prompt]: 	Training 500/553. train loss: 8.9242,	0.8200 s / batch. (data: 3.08e-04). ETA=11:58:42, max mem: 20.9 GB 
[11/25 04:24:21 visual_prompt]: Epoch 5 / 100: avg data time: 1.96e-01, avg batch time: 1.0182, average train loss: 10.1281
[11/25 04:25:19 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3076, average loss: 22.4373
[11/25 04:25:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/25 04:25:19 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[11/25 04:27:06 visual_prompt]: 	Training 100/553. train loss: 3.7443,	0.8576 s / batch. (data: 1.05e-02). ETA=12:29:26, max mem: 20.9 GB 
[11/25 04:28:46 visual_prompt]: 	Training 200/553. train loss: 5.5833,	0.8360 s / batch. (data: 3.64e-04). ETA=12:09:10, max mem: 20.9 GB 
[11/25 04:30:26 visual_prompt]: 	Training 300/553. train loss: 2.3348,	0.8205 s / batch. (data: 8.49e-03). ETA=11:54:20, max mem: 20.9 GB 
[11/25 04:32:11 visual_prompt]: 	Training 400/553. train loss: 26.8347,	0.8308 s / batch. (data: 4.31e-04). ETA=12:01:55, max mem: 20.9 GB 
[11/25 04:33:50 visual_prompt]: 	Training 500/553. train loss: 1.0403,	0.8200 s / batch. (data: 3.28e-04). ETA=11:51:08, max mem: 20.9 GB 
[11/25 04:34:43 visual_prompt]: Epoch 6 / 100: avg data time: 1.96e-01, avg batch time: 1.0191, average train loss: 11.6939
[11/25 04:35:40 visual_prompt]: Inference (val):avg data time: 1.78e-04, avg batch time: 0.3067, average loss: 5.0968
[11/25 04:35:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.36	
[11/25 04:35:40 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[11/25 04:37:24 visual_prompt]: 	Training 100/553. train loss: 10.6278,	0.8120 s / batch. (data: 2.98e-04). ETA=11:42:07, max mem: 20.9 GB 
[11/25 04:39:04 visual_prompt]: 	Training 200/553. train loss: 8.8718,	0.8140 s / batch. (data: 3.27e-04). ETA=11:42:28, max mem: 20.9 GB 
[11/25 04:40:49 visual_prompt]: 	Training 300/553. train loss: 16.6753,	1.9367 s / batch. (data: 1.11e+00). ETA=1 day, 3:48:14, max mem: 20.9 GB 
[11/25 04:42:30 visual_prompt]: 	Training 400/553. train loss: 9.0358,	1.9320 s / batch. (data: 1.12e+00). ETA=1 day, 3:40:54, max mem: 20.9 GB 
[11/25 04:44:10 visual_prompt]: 	Training 500/553. train loss: 48.6467,	0.8353 s / batch. (data: 1.19e-02). ETA=11:56:42, max mem: 20.9 GB 
[11/25 04:45:02 visual_prompt]: Epoch 7 / 100: avg data time: 1.92e-01, avg batch time: 1.0143, average train loss: 14.1636
[11/25 04:46:00 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3069, average loss: 17.4630
[11/25 04:46:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.64	
[11/25 04:46:00 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[11/25 04:47:43 visual_prompt]: 	Training 100/553. train loss: 50.7500,	0.8440 s / batch. (data: 7.93e-03). ETA=12:02:00, max mem: 20.9 GB 
[11/25 04:49:26 visual_prompt]: 	Training 200/553. train loss: 3.8612,	0.8240 s / batch. (data: 3.11e-04). ETA=11:43:33, max mem: 20.9 GB 
[11/25 04:51:08 visual_prompt]: 	Training 300/553. train loss: 33.2820,	0.8285 s / batch. (data: 2.87e-04). ETA=11:45:58, max mem: 20.9 GB 
[11/25 04:52:50 visual_prompt]: 	Training 400/553. train loss: 1.3699,	1.1463 s / batch. (data: 3.30e-01). ETA=16:14:56, max mem: 20.9 GB 
[11/25 04:54:32 visual_prompt]: 	Training 500/553. train loss: 0.3061,	1.6040 s / batch. (data: 7.73e-01). ETA=22:41:30, max mem: 20.9 GB 
[11/25 04:55:24 visual_prompt]: Epoch 8 / 100: avg data time: 1.97e-01, avg batch time: 1.0208, average train loss: 14.8445
[11/25 04:56:22 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3059, average loss: 2.0214
[11/25 04:56:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 62.54	
[11/25 04:56:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[11/25 04:58:08 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8348 s / batch. (data: 5.45e-03). ETA=11:46:27, max mem: 20.9 GB 
[11/25 04:59:48 visual_prompt]: 	Training 200/553. train loss: 3.6899,	0.8149 s / batch. (data: 3.21e-04). ETA=11:28:17, max mem: 20.9 GB 
[11/25 05:01:29 visual_prompt]: 	Training 300/553. train loss: 13.8040,	1.7940 s / batch. (data: 9.82e-01). ETA=1 day, 1:12:13, max mem: 20.9 GB 
[11/25 05:03:12 visual_prompt]: 	Training 400/553. train loss: 12.5228,	0.8240 s / batch. (data: 7.97e-03). ETA=11:33:13, max mem: 20.9 GB 
[11/25 05:04:53 visual_prompt]: 	Training 500/553. train loss: 8.6952,	0.8182 s / batch. (data: 3.28e-04). ETA=11:26:58, max mem: 20.9 GB 
[11/25 05:05:45 visual_prompt]: Epoch 9 / 100: avg data time: 1.93e-01, avg batch time: 1.0171, average train loss: 14.6695
[11/25 05:06:43 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3074, average loss: 9.3240
[11/25 05:06:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.57	
[11/25 05:06:43 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[11/25 05:08:30 visual_prompt]: 	Training 100/553. train loss: 36.7253,	0.8279 s / batch. (data: 3.10e-04). ETA=11:32:57, max mem: 20.9 GB 
[11/25 05:10:10 visual_prompt]: 	Training 200/553. train loss: 19.9705,	0.8321 s / batch. (data: 7.94e-03). ETA=11:35:05, max mem: 20.9 GB 
[11/25 05:11:50 visual_prompt]: 	Training 300/553. train loss: 46.9929,	1.7560 s / batch. (data: 9.22e-01). ETA=1 day, 0:24:00, max mem: 20.9 GB 
[11/25 05:13:28 visual_prompt]: 	Training 400/553. train loss: 38.0591,	0.8120 s / batch. (data: 3.04e-04). ETA=11:15:37, max mem: 20.9 GB 
[11/25 05:15:11 visual_prompt]: 	Training 500/553. train loss: 6.8261,	0.8195 s / batch. (data: 8.84e-03). ETA=11:20:28, max mem: 20.9 GB 
[11/25 05:16:05 visual_prompt]: Epoch 10 / 100: avg data time: 1.92e-01, avg batch time: 1.0152, average train loss: 22.4447
[11/25 05:17:03 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3059, average loss: 16.9751
[11/25 05:17:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.92	
[11/25 05:17:03 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[11/25 05:18:50 visual_prompt]: 	Training 100/553. train loss: 74.9341,	0.8313 s / batch. (data: 5.41e-03). ETA=11:28:08, max mem: 20.9 GB 
[11/25 05:20:33 visual_prompt]: 	Training 200/553. train loss: 29.2614,	0.8400 s / batch. (data: 7.94e-03). ETA=11:33:58, max mem: 20.9 GB 
[11/25 05:22:12 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2175 s / batch. (data: 1.42e+00). ETA=1 day, 6:28:21, max mem: 20.9 GB 
[11/25 05:23:51 visual_prompt]: 	Training 400/553. train loss: 18.4664,	0.8240 s / batch. (data: 7.99e-03). ETA=11:18:01, max mem: 20.9 GB 
[11/25 05:25:31 visual_prompt]: 	Training 500/553. train loss: 7.8388,	0.8183 s / batch. (data: 3.44e-04). ETA=11:11:59, max mem: 20.9 GB 
[11/25 05:26:22 visual_prompt]: Epoch 11 / 100: avg data time: 1.90e-01, avg batch time: 1.0121, average train loss: 22.0659
[11/25 05:27:20 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3053, average loss: 2.4523
[11/25 05:27:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 69.60	
[11/25 05:27:20 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[11/25 05:29:07 visual_prompt]: 	Training 100/553. train loss: 3.9416,	0.8335 s / batch. (data: 7.93e-03). ETA=11:22:18, max mem: 20.9 GB 
[11/25 05:30:49 visual_prompt]: 	Training 200/553. train loss: 24.5018,	0.8240 s / batch. (data: 2.83e-04). ETA=11:13:10, max mem: 20.9 GB 
[11/25 05:32:29 visual_prompt]: 	Training 300/553. train loss: 32.0297,	0.8424 s / batch. (data: 5.42e-03). ETA=11:26:46, max mem: 20.9 GB 
[11/25 05:34:10 visual_prompt]: 	Training 400/553. train loss: 28.2493,	0.8200 s / batch. (data: 3.16e-04). ETA=11:07:09, max mem: 20.9 GB 
[11/25 05:35:51 visual_prompt]: 	Training 500/553. train loss: 82.2206,	0.8162 s / batch. (data: 7.95e-03). ETA=11:02:43, max mem: 20.9 GB 
[11/25 05:36:43 visual_prompt]: Epoch 12 / 100: avg data time: 1.93e-01, avg batch time: 1.0171, average train loss: 18.6137
[11/25 05:37:40 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3076, average loss: 22.1416
[11/25 05:37:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.73	
[11/25 05:37:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[11/25 05:39:27 visual_prompt]: 	Training 100/553. train loss: 10.2265,	1.2984 s / batch. (data: 4.81e-01). ETA=17:30:56, max mem: 20.9 GB 
[11/25 05:41:05 visual_prompt]: 	Training 200/553. train loss: 12.1518,	0.8280 s / batch. (data: 2.61e-04). ETA=11:08:46, max mem: 20.9 GB 
[11/25 05:42:47 visual_prompt]: 	Training 300/553. train loss: 6.5078,	1.7474 s / batch. (data: 9.42e-01). ETA=23:28:32, max mem: 20.9 GB 
[11/25 05:44:26 visual_prompt]: 	Training 400/553. train loss: 5.5350,	0.8286 s / batch. (data: 5.43e-03). ETA=11:06:30, max mem: 20.9 GB 
[11/25 05:46:08 visual_prompt]: 	Training 500/553. train loss: 30.8013,	0.8371 s / batch. (data: 3.20e-04). ETA=11:11:56, max mem: 20.9 GB 
[11/25 05:47:01 visual_prompt]: Epoch 13 / 100: avg data time: 1.90e-01, avg batch time: 1.0128, average train loss: 19.2707
[11/25 05:47:58 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3072, average loss: 9.5355
[11/25 05:47:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.06	
[11/25 05:47:58 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[11/25 05:49:44 visual_prompt]: 	Training 100/553. train loss: 3.6630,	0.8109 s / batch. (data: 3.18e-04). ETA=10:48:50, max mem: 20.9 GB 
[11/25 05:51:25 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.1656 s / batch. (data: 3.55e-01). ETA=15:30:45, max mem: 20.9 GB 
[11/25 05:53:06 visual_prompt]: 	Training 300/553. train loss: 13.1360,	0.8417 s / batch. (data: 1.05e-02). ETA=11:10:42, max mem: 20.9 GB 
[11/25 05:54:46 visual_prompt]: 	Training 400/553. train loss: 8.2468,	0.8240 s / batch. (data: 3.01e-04). ETA=10:55:13, max mem: 20.9 GB 
[11/25 05:56:27 visual_prompt]: 	Training 500/553. train loss: 6.0314,	0.8413 s / batch. (data: 5.44e-03). ETA=11:07:37, max mem: 20.9 GB 
[11/25 05:57:20 visual_prompt]: Epoch 14 / 100: avg data time: 1.93e-01, avg batch time: 1.0149, average train loss: 20.2889
[11/25 05:58:18 visual_prompt]: Inference (val):avg data time: 1.91e-04, avg batch time: 0.3063, average loss: 5.4824
[11/25 05:58:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.59	
[11/25 05:58:18 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[11/25 06:00:03 visual_prompt]: 	Training 100/553. train loss: 2.4841,	0.8200 s / batch. (data: 3.15e-04). ETA=10:48:34, max mem: 20.9 GB 
[11/25 06:01:42 visual_prompt]: 	Training 200/553. train loss: 0.0061,	0.8379 s / batch. (data: 1.38e-02). ETA=11:01:21, max mem: 20.9 GB 
[11/25 06:03:25 visual_prompt]: 	Training 300/553. train loss: 34.1132,	0.8276 s / batch. (data: 5.43e-03). ETA=10:51:52, max mem: 20.9 GB 
[11/25 06:05:04 visual_prompt]: 	Training 400/553. train loss: 30.7254,	0.8310 s / batch. (data: 3.05e-04). ETA=10:53:09, max mem: 20.9 GB 
[11/25 06:06:46 visual_prompt]: 	Training 500/553. train loss: 8.6655,	0.8467 s / batch. (data: 2.20e-02). ETA=11:04:03, max mem: 20.9 GB 
[11/25 06:07:39 visual_prompt]: Epoch 15 / 100: avg data time: 1.92e-01, avg batch time: 1.0149, average train loss: 21.3577
[11/25 06:08:37 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3047, average loss: 4.9152
[11/25 06:08:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.58	
[11/25 06:08:37 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[11/25 06:10:21 visual_prompt]: 	Training 100/553. train loss: 25.2839,	0.8056 s / batch. (data: 5.42e-03). ETA=10:29:48, max mem: 20.9 GB 
[11/25 06:12:02 visual_prompt]: 	Training 200/553. train loss: 20.3574,	0.8503 s / batch. (data: 1.56e-02). ETA=11:03:16, max mem: 20.9 GB 
[11/25 06:13:42 visual_prompt]: 	Training 300/553. train loss: 11.8521,	0.8338 s / batch. (data: 5.00e-04). ETA=10:49:03, max mem: 20.9 GB 
[11/25 06:15:24 visual_prompt]: 	Training 400/553. train loss: 19.4738,	0.8400 s / batch. (data: 7.19e-04). ETA=10:52:29, max mem: 20.9 GB 
[11/25 06:17:04 visual_prompt]: 	Training 500/553. train loss: 3.9495,	1.0128 s / batch. (data: 1.85e-01). ETA=13:04:59, max mem: 20.9 GB 
[11/25 06:17:58 visual_prompt]: Epoch 16 / 100: avg data time: 1.90e-01, avg batch time: 1.0139, average train loss: 17.7663
[11/25 06:18:56 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3076, average loss: 2.0187
[11/25 06:18:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 64.72	
[11/25 06:18:56 visual_prompt]: Best epoch 16: best metric: -2.019
[11/25 06:18:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[11/25 06:20:40 visual_prompt]: 	Training 100/553. train loss: 8.9375,	0.8334 s / batch. (data: 1.20e-02). ETA=10:43:50, max mem: 20.9 GB 
[11/25 06:22:22 visual_prompt]: 	Training 200/553. train loss: 45.9730,	0.8063 s / batch. (data: 2.57e-04). ETA=10:21:34, max mem: 20.9 GB 
[11/25 06:24:02 visual_prompt]: 	Training 300/553. train loss: 22.9622,	0.8262 s / batch. (data: 2.89e-04). ETA=10:35:30, max mem: 20.9 GB 
[11/25 06:25:43 visual_prompt]: 	Training 400/553. train loss: 14.7286,	1.2537 s / batch. (data: 4.38e-01). ETA=16:02:13, max mem: 20.9 GB 
[11/25 06:27:22 visual_prompt]: 	Training 500/553. train loss: 7.3184,	1.6382 s / batch. (data: 8.28e-01). ETA=20:54:38, max mem: 20.9 GB 
[11/25 06:28:17 visual_prompt]: Epoch 17 / 100: avg data time: 1.91e-01, avg batch time: 1.0144, average train loss: 19.2786
[11/25 06:29:15 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3071, average loss: 27.4367
[11/25 06:29:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.51	
[11/25 06:29:15 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[11/25 06:31:00 visual_prompt]: 	Training 100/553. train loss: 5.2559,	0.8275 s / batch. (data: 1.55e-02). ETA=10:31:37, max mem: 20.9 GB 
[11/25 06:32:43 visual_prompt]: 	Training 200/553. train loss: 3.6647,	0.8129 s / batch. (data: 3.20e-04). ETA=10:19:09, max mem: 20.9 GB 
[11/25 06:34:24 visual_prompt]: 	Training 300/553. train loss: 5.1940,	0.8199 s / batch. (data: 1.21e-02). ETA=10:23:07, max mem: 20.9 GB 
[11/25 06:36:05 visual_prompt]: 	Training 400/553. train loss: 25.8491,	0.8213 s / batch. (data: 3.11e-04). ETA=10:22:49, max mem: 20.9 GB 
[11/25 06:37:44 visual_prompt]: 	Training 500/553. train loss: 14.6559,	0.8279 s / batch. (data: 1.05e-02). ETA=10:26:25, max mem: 20.9 GB 
[11/25 06:38:36 visual_prompt]: Epoch 18 / 100: avg data time: 1.92e-01, avg batch time: 1.0150, average train loss: 21.2044
[11/25 06:39:34 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3068, average loss: 31.1111
[11/25 06:39:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.99	
[11/25 06:39:34 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[11/25 06:41:19 visual_prompt]: 	Training 100/553. train loss: 8.5300,	1.3509 s / batch. (data: 5.31e-01). ETA=16:58:41, max mem: 20.9 GB 
[11/25 06:42:59 visual_prompt]: 	Training 200/553. train loss: 10.2427,	0.8261 s / batch. (data: 5.43e-03). ETA=10:21:35, max mem: 20.9 GB 
[11/25 06:44:41 visual_prompt]: 	Training 300/553. train loss: 0.0001,	0.8335 s / batch. (data: 2.06e-02). ETA=10:25:46, max mem: 20.9 GB 
[11/25 06:46:23 visual_prompt]: 	Training 400/553. train loss: 9.9992,	0.8492 s / batch. (data: 7.96e-04). ETA=10:36:08, max mem: 20.9 GB 
[11/25 06:47:59 visual_prompt]: 	Training 500/553. train loss: 8.1673,	0.8160 s / batch. (data: 3.13e-04). ETA=10:09:55, max mem: 20.9 GB 
[11/25 06:48:52 visual_prompt]: Epoch 19 / 100: avg data time: 1.85e-01, avg batch time: 1.0083, average train loss: 16.9489
[11/25 06:49:49 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3084, average loss: 9.2377
[11/25 06:49:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 66.11	
[11/25 06:49:49 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[11/25 06:51:32 visual_prompt]: 	Training 100/553. train loss: 25.5115,	0.8320 s / batch. (data: 3.03e-04). ETA=10:19:44, max mem: 20.9 GB 
[11/25 06:53:15 visual_prompt]: 	Training 200/553. train loss: 2.2696,	0.8296 s / batch. (data: 5.43e-03). ETA=10:16:36, max mem: 20.9 GB 
[11/25 06:54:55 visual_prompt]: 	Training 300/553. train loss: 13.7544,	0.8436 s / batch. (data: 5.47e-03). ETA=10:25:34, max mem: 20.9 GB 
[11/25 06:56:36 visual_prompt]: 	Training 400/553. train loss: 30.7075,	0.8480 s / batch. (data: 7.94e-03). ETA=10:27:24, max mem: 20.9 GB 
[11/25 06:58:15 visual_prompt]: 	Training 500/553. train loss: 84.3555,	0.8122 s / batch. (data: 3.04e-04). ETA=9:59:34, max mem: 20.9 GB 
[11/25 06:59:09 visual_prompt]: Epoch 20 / 100: avg data time: 1.90e-01, avg batch time: 1.0128, average train loss: 21.6830
[11/25 07:00:07 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3070, average loss: 7.5366
[11/25 07:00:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.91	
[11/25 07:00:07 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[11/25 07:01:54 visual_prompt]: 	Training 100/553. train loss: 3.1234,	0.8374 s / batch. (data: 9.38e-03). ETA=10:16:04, max mem: 20.9 GB 
[11/25 07:03:33 visual_prompt]: 	Training 200/553. train loss: 36.0146,	0.8248 s / batch. (data: 5.43e-03). ETA=10:05:22, max mem: 20.9 GB 
[11/25 07:05:15 visual_prompt]: 	Training 300/553. train loss: 81.9775,	1.0888 s / batch. (data: 2.76e-01). ETA=13:17:19, max mem: 20.9 GB 
[11/25 07:06:54 visual_prompt]: 	Training 400/553. train loss: 1.2556,	0.8707 s / batch. (data: 1.05e-02). ETA=10:36:12, max mem: 20.9 GB 
[11/25 07:08:36 visual_prompt]: 	Training 500/553. train loss: 7.6277,	0.8332 s / batch. (data: 9.15e-03). ETA=10:07:25, max mem: 20.9 GB 
[11/25 07:09:28 visual_prompt]: Epoch 21 / 100: avg data time: 1.91e-01, avg batch time: 1.0133, average train loss: 16.5011
[11/25 07:10:25 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3075, average loss: 7.3796
[11/25 07:10:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 67.41	
[11/25 07:10:25 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[11/25 07:12:09 visual_prompt]: 	Training 100/553. train loss: 47.6375,	0.8449 s / batch. (data: 1.56e-02). ETA=10:13:44, max mem: 20.9 GB 
[11/25 07:13:50 visual_prompt]: 	Training 200/553. train loss: 13.8386,	0.8160 s / batch. (data: 7.97e-03). ETA=9:51:24, max mem: 20.9 GB 
[11/25 07:15:28 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8200 s / batch. (data: 3.27e-04). ETA=9:52:57, max mem: 20.9 GB 
[11/25 07:17:10 visual_prompt]: 	Training 400/553. train loss: 4.3740,	0.8217 s / batch. (data: 2.96e-04). ETA=9:52:48, max mem: 20.9 GB 
[11/25 07:18:51 visual_prompt]: 	Training 500/553. train loss: 23.4144,	0.8200 s / batch. (data: 7.94e-03). ETA=9:50:12, max mem: 20.9 GB 
[11/25 07:19:44 visual_prompt]: Epoch 22 / 100: avg data time: 1.88e-01, avg batch time: 1.0104, average train loss: 15.0954
[11/25 07:20:42 visual_prompt]: Inference (val):avg data time: 5.25e-05, avg batch time: 0.3080, average loss: 4.8684
[11/25 07:20:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 68.48	
[11/25 07:20:42 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[11/25 07:22:28 visual_prompt]: 	Training 100/553. train loss: 9.3038,	0.8403 s / batch. (data: 1.06e-02). ETA=10:02:40, max mem: 20.9 GB 
[11/25 07:24:10 visual_prompt]: 	Training 200/553. train loss: 66.7489,	0.9204 s / batch. (data: 7.42e-02). ETA=10:58:37, max mem: 20.9 GB 
[11/25 07:25:52 visual_prompt]: 	Training 300/553. train loss: 12.7685,	0.8236 s / batch. (data: 7.80e-04). ETA=9:47:57, max mem: 20.9 GB 
[11/25 07:27:30 visual_prompt]: 	Training 400/553. train loss: 9.6662,	0.8527 s / batch. (data: 8.01e-04). ETA=10:07:18, max mem: 20.9 GB 
[11/25 07:29:09 visual_prompt]: 	Training 500/553. train loss: 29.2016,	0.8204 s / batch. (data: 2.85e-04). ETA=9:42:59, max mem: 20.9 GB 
[11/25 07:30:01 visual_prompt]: Epoch 23 / 100: avg data time: 1.89e-01, avg batch time: 1.0108, average train loss: 19.5543
[11/25 07:30:59 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3083, average loss: 8.3050
[11/25 07:30:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 67.63	
[11/25 07:30:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[11/25 07:32:42 visual_prompt]: 	Training 100/553. train loss: 40.1461,	0.8439 s / batch. (data: 1.06e-02). ETA=9:57:31, max mem: 20.9 GB 
[11/25 07:34:22 visual_prompt]: 	Training 200/553. train loss: 23.6031,	0.8513 s / batch. (data: 7.67e-04). ETA=10:01:17, max mem: 20.9 GB 
[11/25 07:36:04 visual_prompt]: 	Training 300/553. train loss: 2.1520,	1.1108 s / batch. (data: 2.72e-01). ETA=13:02:44, max mem: 20.9 GB 
[11/25 07:37:43 visual_prompt]: 	Training 400/553. train loss: 34.9383,	0.8389 s / batch. (data: 2.40e-02). ETA=9:49:45, max mem: 20.9 GB 
[11/25 07:39:27 visual_prompt]: 	Training 500/553. train loss: 33.8811,	0.8442 s / batch. (data: 5.46e-03). ETA=9:52:03, max mem: 20.9 GB 
[11/25 07:40:20 visual_prompt]: Epoch 24 / 100: avg data time: 1.92e-01, avg batch time: 1.0137, average train loss: 22.2876
[11/25 07:41:18 visual_prompt]: Inference (val):avg data time: 5.33e-04, avg batch time: 0.3076, average loss: 48.8690
[11/25 07:41:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.95	
[11/25 07:41:18 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[11/25 07:43:06 visual_prompt]: 	Training 100/553. train loss: 26.6344,	0.8239 s / batch. (data: 3.07e-04). ETA=9:35:45, max mem: 20.9 GB 
[11/25 07:44:43 visual_prompt]: 	Training 200/553. train loss: 9.9352,	0.8205 s / batch. (data: 1.05e-02). ETA=9:31:58, max mem: 20.9 GB 
[11/25 07:46:24 visual_prompt]: 	Training 300/553. train loss: 7.5309,	0.8185 s / batch. (data: 3.29e-04). ETA=9:29:13, max mem: 20.9 GB 
[11/25 07:48:04 visual_prompt]: 	Training 400/553. train loss: 1.2932,	1.4193 s / batch. (data: 5.68e-01). ETA=16:24:44, max mem: 20.9 GB 
[11/25 07:49:45 visual_prompt]: 	Training 500/553. train loss: 9.2158,	1.5160 s / batch. (data: 6.75e-01). ETA=17:29:17, max mem: 20.9 GB 
[11/25 07:50:38 visual_prompt]: Epoch 25 / 100: avg data time: 1.89e-01, avg batch time: 1.0121, average train loss: 16.1054
[11/25 07:51:36 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3062, average loss: 20.5659
[11/25 07:51:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.31	
[11/25 07:51:36 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[11/25 07:53:20 visual_prompt]: 	Training 100/553. train loss: 25.2308,	0.8329 s / batch. (data: 2.09e-02). ETA=9:34:19, max mem: 20.9 GB 
[11/25 07:55:02 visual_prompt]: 	Training 200/553. train loss: 77.3484,	1.7526 s / batch. (data: 9.48e-01). ETA=20:05:38, max mem: 20.9 GB 
[11/25 07:56:44 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8254 s / batch. (data: 8.33e-04). ETA=9:26:24, max mem: 20.9 GB 
[11/25 07:58:23 visual_prompt]: 	Training 400/553. train loss: 18.4817,	0.8195 s / batch. (data: 1.56e-02). ETA=9:21:02, max mem: 20.9 GB 
[11/25 08:00:02 visual_prompt]: 	Training 500/553. train loss: 1.8468,	0.8350 s / batch. (data: 5.46e-03). ETA=9:30:13, max mem: 20.9 GB 
[11/25 08:00:54 visual_prompt]: Epoch 26 / 100: avg data time: 1.88e-01, avg batch time: 1.0102, average train loss: 16.2934
[11/25 08:01:52 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3074, average loss: 30.2772
[11/25 08:01:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.28	
[11/25 08:01:52 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[11/25 08:03:38 visual_prompt]: 	Training 100/553. train loss: 0.0244,	0.8093 s / batch. (data: 3.09e-04). ETA=9:10:38, max mem: 20.9 GB 
[11/25 08:05:19 visual_prompt]: 	Training 200/553. train loss: 27.9886,	1.4249 s / batch. (data: 6.09e-01). ETA=16:07:02, max mem: 20.9 GB 
[11/25 08:06:59 visual_prompt]: 	Training 300/553. train loss: 10.9421,	0.8087 s / batch. (data: 3.39e-04). ETA=9:07:30, max mem: 20.9 GB 
[11/25 08:08:41 visual_prompt]: 	Training 400/553. train loss: 11.9090,	0.8360 s / batch. (data: 4.27e-04). ETA=9:24:34, max mem: 20.9 GB 
[11/25 08:10:23 visual_prompt]: 	Training 500/553. train loss: 27.1996,	0.8360 s / batch. (data: 8.05e-04). ETA=9:23:13, max mem: 20.9 GB 
[11/25 08:11:13 visual_prompt]: Epoch 27 / 100: avg data time: 1.91e-01, avg batch time: 1.0148, average train loss: 16.6082
[11/25 08:12:11 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3073, average loss: 9.7985
[11/25 08:12:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 70.27	
[11/25 08:12:11 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[11/25 08:13:55 visual_prompt]: 	Training 100/553. train loss: 10.9440,	0.8200 s / batch. (data: 2.86e-04). ETA=9:10:20, max mem: 20.9 GB 
[11/25 08:15:36 visual_prompt]: 	Training 200/553. train loss: 19.2374,	0.8279 s / batch. (data: 3.55e-04). ETA=9:14:17, max mem: 20.9 GB 
[11/25 08:17:18 visual_prompt]: 	Training 300/553. train loss: 10.8698,	1.4480 s / batch. (data: 6.17e-01). ETA=16:07:00, max mem: 20.9 GB 
[11/25 08:18:59 visual_prompt]: 	Training 400/553. train loss: 2.6526,	0.8132 s / batch. (data: 2.67e-04). ETA=9:01:43, max mem: 20.9 GB 
[11/25 08:20:37 visual_prompt]: 	Training 500/553. train loss: 4.4598,	0.8320 s / batch. (data: 2.67e-04). ETA=9:12:51, max mem: 20.9 GB 
[11/25 08:21:31 visual_prompt]: Epoch 28 / 100: avg data time: 1.90e-01, avg batch time: 1.0118, average train loss: 18.9422
[11/25 08:22:29 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3053, average loss: 4.2045
[11/25 08:22:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 70.05	
[11/25 08:22:29 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[11/25 08:24:19 visual_prompt]: 	Training 100/553. train loss: 7.5982,	0.8320 s / batch. (data: 2.99e-04). ETA=9:10:43, max mem: 20.9 GB 
[11/25 08:26:00 visual_prompt]: 	Training 200/553. train loss: 63.5579,	1.8320 s / batch. (data: 1.01e+00). ETA=20:09:36, max mem: 20.9 GB 
[11/25 08:27:38 visual_prompt]: 	Training 300/553. train loss: 7.7657,	0.8264 s / batch. (data: 7.67e-04). ETA=9:04:14, max mem: 20.9 GB 
[11/25 08:29:16 visual_prompt]: 	Training 400/553. train loss: 9.4884,	1.1172 s / batch. (data: 3.04e-01). ETA=12:13:57, max mem: 20.9 GB 
[11/25 08:30:57 visual_prompt]: 	Training 500/553. train loss: 23.7843,	0.8447 s / batch. (data: 5.49e-03). ETA=9:13:31, max mem: 20.9 GB 
[11/25 08:31:49 visual_prompt]: Epoch 29 / 100: avg data time: 1.91e-01, avg batch time: 1.0136, average train loss: 19.7971
[11/25 08:32:47 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3064, average loss: 5.3655
[11/25 08:32:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 69.41	
[11/25 08:32:47 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[11/25 08:34:30 visual_prompt]: 	Training 100/553. train loss: 22.5831,	0.8387 s / batch. (data: 1.59e-02). ETA=9:07:25, max mem: 20.9 GB 
[11/25 08:36:21 visual_prompt]: 	Training 200/553. train loss: 40.8704,	0.8214 s / batch. (data: 1.10e-02). ETA=8:54:46, max mem: 20.9 GB 
[11/25 08:38:00 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.1199 s / batch. (data: 3.02e-01). ETA=12:07:15, max mem: 20.9 GB 
[11/25 08:39:42 visual_prompt]: 	Training 400/553. train loss: 15.9673,	1.2678 s / batch. (data: 4.53e-01). ETA=13:41:09, max mem: 20.9 GB 
[11/25 08:41:22 visual_prompt]: 	Training 500/553. train loss: 6.7287,	1.5393 s / batch. (data: 6.99e-01). ETA=16:34:28, max mem: 20.9 GB 
[11/25 08:42:16 visual_prompt]: Epoch 30 / 100: avg data time: 2.05e-01, avg batch time: 1.0281, average train loss: 13.3866
[11/25 08:43:14 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3070, average loss: 33.6132
[11/25 08:43:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 72.45	
[11/25 08:43:14 visual_prompt]: Stopping early.
[11/25 08:43:14 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 08:43:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 08:43:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/25 08:43:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 08:43:14 visual_prompt]: Training with config:
[11/25 08:43:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/25 08:43:14 visual_prompt]: Loading training data...
[11/25 08:43:14 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 08:43:14 visual_prompt]: Loading validation data...
[11/25 08:43:14 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 08:43:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 08:43:16 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 08:43:16 visual_prompt]: tuned percent:0.525
[11/25 08:43:17 visual_prompt]: Device used for model: 0
[11/25 08:43:17 visual_prompt]: Setting up Evaluator...
[11/25 08:43:17 visual_prompt]: Setting up Trainer...
[11/25 08:43:17 visual_prompt]: 	Setting up the optimizer...
[11/25 08:43:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 08:45:01 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8419 s / batch. (data: 9.85e-03). ETA=12:54:32, max mem: 20.9 GB 
[11/25 08:46:41 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8200 s / batch. (data: 2.64e-04). ETA=12:33:03, max mem: 20.9 GB 
[11/25 08:48:24 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3680 s / batch. (data: 5.46e-01). ETA=20:53:58, max mem: 20.9 GB 
[11/25 08:50:02 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8263 s / batch. (data: 2.93e-04). ETA=12:36:03, max mem: 20.9 GB 
[11/25 08:51:46 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8088 s / batch. (data: 3.16e-04). ETA=12:18:39, max mem: 20.9 GB 
[11/25 08:52:39 visual_prompt]: Epoch 1 / 100: avg data time: 1.94e-01, avg batch time: 1.0169, average train loss: 1.5403
[11/25 08:53:37 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3054, average loss: 1.5201
[11/25 08:53:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 08:53:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/25 08:55:21 visual_prompt]: 	Training 100/553. train loss: 0.7798,	0.8461 s / batch. (data: 2.07e-02). ETA=12:50:34, max mem: 20.9 GB 
[11/25 08:57:01 visual_prompt]: 	Training 200/553. train loss: 0.0007,	0.8200 s / batch. (data: 3.01e-04). ETA=12:25:27, max mem: 20.9 GB 
[11/25 08:58:45 visual_prompt]: 	Training 300/553. train loss: 0.8430,	1.2721 s / batch. (data: 4.44e-01). ETA=19:14:19, max mem: 20.9 GB 
[11/25 09:00:25 visual_prompt]: 	Training 400/553. train loss: 0.8061,	0.8400 s / batch. (data: 3.14e-04). ETA=12:40:50, max mem: 20.9 GB 
[11/25 09:02:08 visual_prompt]: 	Training 500/553. train loss: 0.6105,	0.8129 s / batch. (data: 5.41e-03). ETA=12:14:57, max mem: 20.9 GB 
[11/25 09:02:59 visual_prompt]: Epoch 2 / 100: avg data time: 1.93e-01, avg batch time: 1.0159, average train loss: 1.5226
[11/25 09:03:57 visual_prompt]: Inference (val):avg data time: 5.21e-05, avg batch time: 0.3117, average loss: 0.8135
[11/25 09:03:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.50	
[11/25 09:03:57 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/25 09:05:41 visual_prompt]: 	Training 100/553. train loss: 3.1812,	0.8240 s / batch. (data: 2.94e-04). ETA=12:22:53, max mem: 20.9 GB 
[11/25 09:07:24 visual_prompt]: 	Training 200/553. train loss: 2.0711,	0.8124 s / batch. (data: 5.36e-03). ETA=12:11:07, max mem: 20.9 GB 
[11/25 09:09:04 visual_prompt]: 	Training 300/553. train loss: 0.9746,	0.8273 s / batch. (data: 5.50e-03). ETA=12:23:03, max mem: 20.9 GB 
[11/25 09:10:46 visual_prompt]: 	Training 400/553. train loss: 0.0115,	0.8239 s / batch. (data: 3.62e-04). ETA=12:18:42, max mem: 20.9 GB 
[11/25 09:12:28 visual_prompt]: 	Training 500/553. train loss: 4.7185,	1.4819 s / batch. (data: 6.62e-01). ETA=22:06:10, max mem: 20.9 GB 
[11/25 09:13:19 visual_prompt]: Epoch 3 / 100: avg data time: 1.93e-01, avg batch time: 1.0168, average train loss: 2.5513
[11/25 09:14:17 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3088, average loss: 7.7699
[11/25 09:14:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.54	
[11/25 09:14:17 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/25 09:16:04 visual_prompt]: 	Training 100/553. train loss: 8.1876,	0.8400 s / batch. (data: 7.95e-03). ETA=12:29:34, max mem: 20.9 GB 
[11/25 09:17:45 visual_prompt]: 	Training 200/553. train loss: 2.3320,	0.8301 s / batch. (data: 7.95e-03). ETA=12:19:21, max mem: 20.9 GB 
[11/25 09:19:26 visual_prompt]: 	Training 300/553. train loss: 1.5272,	1.2040 s / batch. (data: 3.50e-01). ETA=17:50:22, max mem: 20.9 GB 
[11/25 09:21:03 visual_prompt]: 	Training 400/553. train loss: 2.3384,	0.8560 s / batch. (data: 1.19e-02). ETA=12:39:33, max mem: 20.9 GB 
[11/25 09:22:47 visual_prompt]: 	Training 500/553. train loss: 15.5396,	3.1987 s / batch. (data: 2.38e+00). ETA=1 day, 23:13:04, max mem: 20.9 GB 
[11/25 09:23:41 visual_prompt]: Epoch 4 / 100: avg data time: 1.95e-01, avg batch time: 1.0184, average train loss: 4.5100
[11/25 09:24:39 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3072, average loss: 4.4083
[11/25 09:24:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.16	
[11/25 09:24:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/25 09:26:22 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8186 s / batch. (data: 5.50e-03). ETA=12:02:54, max mem: 20.9 GB 
[11/25 09:28:03 visual_prompt]: 	Training 200/553. train loss: 6.5708,	1.3329 s / batch. (data: 5.08e-01). ETA=19:34:54, max mem: 20.9 GB 
[11/25 09:29:47 visual_prompt]: 	Training 300/553. train loss: 5.6033,	0.8339 s / batch. (data: 9.82e-03). ETA=12:13:37, max mem: 20.9 GB 
[11/25 09:31:27 visual_prompt]: 	Training 400/553. train loss: 16.1834,	0.8240 s / batch. (data: 3.03e-04). ETA=12:03:34, max mem: 20.9 GB 
[11/25 09:33:09 visual_prompt]: 	Training 500/553. train loss: 1.6618,	0.8252 s / batch. (data: 5.43e-03). ETA=12:03:15, max mem: 20.9 GB 
[11/25 09:34:03 visual_prompt]: Epoch 5 / 100: avg data time: 1.97e-01, avg batch time: 1.0207, average train loss: 5.5591
[11/25 09:35:01 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3069, average loss: 6.7012
[11/25 09:35:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.04	
[11/25 09:35:01 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/25 09:36:48 visual_prompt]: 	Training 100/553. train loss: 14.3782,	0.8162 s / batch. (data: 5.47e-03). ETA=11:53:17, max mem: 20.9 GB 
[11/25 09:38:29 visual_prompt]: 	Training 200/553. train loss: 7.5807,	0.8381 s / batch. (data: 5.56e-03). ETA=12:11:01, max mem: 20.9 GB 
[11/25 09:40:08 visual_prompt]: 	Training 300/553. train loss: 8.5515,	0.8336 s / batch. (data: 2.06e-02). ETA=12:05:43, max mem: 20.9 GB 
[11/25 09:41:53 visual_prompt]: 	Training 400/553. train loss: 6.8434,	0.8257 s / batch. (data: 3.21e-04). ETA=11:57:29, max mem: 20.9 GB 
[11/25 09:43:32 visual_prompt]: 	Training 500/553. train loss: 6.3768,	0.8360 s / batch. (data: 7.92e-03). ETA=12:04:59, max mem: 20.9 GB 
[11/25 09:44:24 visual_prompt]: Epoch 6 / 100: avg data time: 1.94e-01, avg batch time: 1.0185, average train loss: 7.4405
[11/25 09:45:22 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3079, average loss: 0.7440
[11/25 09:45:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.26	
[11/25 09:45:22 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/25 09:47:06 visual_prompt]: 	Training 100/553. train loss: 4.6580,	0.8343 s / batch. (data: 5.48e-03). ETA=12:01:27, max mem: 20.9 GB 
[11/25 09:48:48 visual_prompt]: 	Training 200/553. train loss: 6.2999,	0.8400 s / batch. (data: 5.44e-03). ETA=12:04:56, max mem: 20.9 GB 
[11/25 09:50:32 visual_prompt]: 	Training 300/553. train loss: 1.2756,	1.0880 s / batch. (data: 2.49e-01). ETA=15:37:09, max mem: 20.9 GB 
[11/25 09:52:14 visual_prompt]: 	Training 400/553. train loss: 2.8603,	1.7846 s / batch. (data: 9.49e-01). ETA=1 day, 1:34:11, max mem: 20.9 GB 
[11/25 09:53:54 visual_prompt]: 	Training 500/553. train loss: 7.7095,	0.8299 s / batch. (data: 9.91e-03). ETA=11:52:07, max mem: 20.9 GB 
[11/25 09:54:46 visual_prompt]: Epoch 7 / 100: avg data time: 1.94e-01, avg batch time: 1.0181, average train loss: 8.8655
[11/25 09:55:44 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3086, average loss: 0.8059
[11/25 09:55:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.19	
[11/25 09:55:44 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/25 09:57:27 visual_prompt]: 	Training 100/553. train loss: 5.4048,	0.8357 s / batch. (data: 1.20e-02). ETA=11:54:55, max mem: 20.9 GB 
[11/25 09:59:09 visual_prompt]: 	Training 200/553. train loss: 13.6669,	0.8282 s / batch. (data: 3.54e-04). ETA=11:47:10, max mem: 20.9 GB 
[11/25 10:00:51 visual_prompt]: 	Training 300/553. train loss: 3.8111,	0.8360 s / batch. (data: 7.91e-03). ETA=11:52:22, max mem: 20.9 GB 
[11/25 10:02:32 visual_prompt]: 	Training 400/553. train loss: 13.1761,	1.1355 s / batch. (data: 3.16e-01). ETA=16:05:42, max mem: 20.9 GB 
[11/25 10:04:13 visual_prompt]: 	Training 500/553. train loss: 58.6027,	1.6387 s / batch. (data: 8.19e-01). ETA=23:10:56, max mem: 20.9 GB 
[11/25 10:05:06 visual_prompt]: Epoch 8 / 100: avg data time: 1.94e-01, avg batch time: 1.0173, average train loss: 10.5169
[11/25 10:06:04 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3085, average loss: 7.4940
[11/25 10:06:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.82	
[11/25 10:06:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/25 10:07:49 visual_prompt]: 	Training 100/553. train loss: 11.2248,	0.8200 s / batch. (data: 3.02e-04). ETA=11:33:56, max mem: 20.9 GB 
[11/25 10:09:30 visual_prompt]: 	Training 200/553. train loss: 17.2262,	0.8285 s / batch. (data: 2.91e-04). ETA=11:39:44, max mem: 20.9 GB 
[11/25 10:11:13 visual_prompt]: 	Training 300/553. train loss: 1.2406,	1.9094 s / batch. (data: 1.09e+00). ETA=1 day, 2:49:31, max mem: 20.9 GB 
[11/25 10:12:55 visual_prompt]: 	Training 400/553. train loss: 14.1322,	0.8170 s / batch. (data: 3.80e-04). ETA=11:27:18, max mem: 20.9 GB 
[11/25 10:14:37 visual_prompt]: 	Training 500/553. train loss: 28.9069,	0.9613 s / batch. (data: 1.42e-01). ETA=13:27:08, max mem: 20.9 GB 
[11/25 10:15:28 visual_prompt]: Epoch 9 / 100: avg data time: 1.96e-01, avg batch time: 1.0197, average train loss: 14.5256
[11/25 10:16:26 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3070, average loss: 7.8257
[11/25 10:16:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.12	
[11/25 10:16:26 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/25 10:18:15 visual_prompt]: 	Training 100/553. train loss: 14.5590,	0.8448 s / batch. (data: 8.43e-04). ETA=11:47:10, max mem: 20.9 GB 
[11/25 10:19:53 visual_prompt]: 	Training 200/553. train loss: 1.0110,	0.8208 s / batch. (data: 5.45e-03). ETA=11:25:41, max mem: 20.9 GB 
[11/25 10:21:34 visual_prompt]: 	Training 300/553. train loss: 35.9825,	1.9741 s / batch. (data: 1.15e+00). ETA=1 day, 3:25:48, max mem: 20.9 GB 
[11/25 10:23:13 visual_prompt]: 	Training 400/553. train loss: 25.0067,	0.8236 s / batch. (data: 3.17e-04). ETA=11:25:17, max mem: 20.9 GB 
[11/25 10:24:55 visual_prompt]: 	Training 500/553. train loss: 3.2665,	0.8263 s / batch. (data: 1.02e-02). ETA=11:26:10, max mem: 20.9 GB 
[11/25 10:25:48 visual_prompt]: Epoch 10 / 100: avg data time: 1.93e-01, avg batch time: 1.0157, average train loss: 15.3921
[11/25 10:26:46 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3074, average loss: 8.6444
[11/25 10:26:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.01	
[11/25 10:26:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/25 10:28:33 visual_prompt]: 	Training 100/553. train loss: 2.8445,	0.8400 s / batch. (data: 7.93e-03). ETA=11:35:21, max mem: 20.9 GB 
[11/25 10:30:16 visual_prompt]: 	Training 200/553. train loss: 48.9576,	0.8508 s / batch. (data: 2.78e-04). ETA=11:42:52, max mem: 20.9 GB 
[11/25 10:31:57 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2298 s / batch. (data: 1.40e+00). ETA=1 day, 6:38:26, max mem: 20.9 GB 
[11/25 10:33:36 visual_prompt]: 	Training 400/553. train loss: 2.8517,	0.8480 s / batch. (data: 7.98e-03). ETA=11:37:46, max mem: 20.9 GB 
[11/25 10:35:16 visual_prompt]: 	Training 500/553. train loss: 48.7725,	0.8197 s / batch. (data: 5.60e-03). ETA=11:13:08, max mem: 20.9 GB 
[11/25 10:36:08 visual_prompt]: Epoch 11 / 100: avg data time: 1.91e-01, avg batch time: 1.0153, average train loss: 15.8937
[11/25 10:37:06 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3071, average loss: 2.6196
[11/25 10:37:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.89	
[11/25 10:37:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/25 10:38:52 visual_prompt]: 	Training 100/553. train loss: 16.8725,	0.9270 s / batch. (data: 9.04e-02). ETA=12:38:49, max mem: 20.9 GB 
[11/25 10:40:34 visual_prompt]: 	Training 200/553. train loss: 10.2299,	1.2223 s / batch. (data: 4.04e-01). ETA=16:38:33, max mem: 20.9 GB 
[11/25 10:42:13 visual_prompt]: 	Training 300/553. train loss: 23.4843,	0.8559 s / batch. (data: 1.19e-02). ETA=11:37:50, max mem: 20.9 GB 
[11/25 10:43:55 visual_prompt]: 	Training 400/553. train loss: 21.7349,	0.8187 s / batch. (data: 4.48e-04). ETA=11:06:05, max mem: 20.9 GB 
[11/25 10:45:37 visual_prompt]: 	Training 500/553. train loss: 6.1085,	0.8180 s / batch. (data: 4.10e-04). ETA=11:04:08, max mem: 20.9 GB 
[11/25 10:46:28 visual_prompt]: Epoch 12 / 100: avg data time: 1.94e-01, avg batch time: 1.0166, average train loss: 15.7183
[11/25 10:47:26 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3051, average loss: 9.6142
[11/25 10:47:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.14	
[11/25 10:47:26 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/25 10:49:13 visual_prompt]: 	Training 100/553. train loss: 5.4204,	0.8294 s / batch. (data: 1.06e-02). ETA=11:11:19, max mem: 20.9 GB 
[11/25 10:50:50 visual_prompt]: 	Training 200/553. train loss: 18.7901,	0.8165 s / batch. (data: 5.70e-04). ETA=10:59:31, max mem: 20.9 GB 
[11/25 10:52:32 visual_prompt]: 	Training 300/553. train loss: 4.6744,	1.8040 s / batch. (data: 9.82e-01). ETA=1 day, 0:14:08, max mem: 20.9 GB 
[11/25 10:54:12 visual_prompt]: 	Training 400/553. train loss: 22.9821,	0.8151 s / batch. (data: 3.20e-04). ETA=10:55:37, max mem: 20.9 GB 
[11/25 10:55:54 visual_prompt]: 	Training 500/553. train loss: 55.6965,	0.8001 s / batch. (data: 2.59e-04). ETA=10:42:13, max mem: 20.9 GB 
[11/25 10:56:46 visual_prompt]: Epoch 13 / 100: avg data time: 1.90e-01, avg batch time: 1.0133, average train loss: 18.4516
[11/25 10:57:44 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3076, average loss: 39.4750
[11/25 10:57:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.53	
[11/25 10:57:44 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/25 10:59:31 visual_prompt]: 	Training 100/553. train loss: 7.5923,	0.8335 s / batch. (data: 3.22e-04). ETA=11:06:58, max mem: 20.9 GB 
[11/25 11:01:12 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.1023 s / batch. (data: 2.74e-01). ETA=14:40:10, max mem: 20.9 GB 
[11/25 11:02:53 visual_prompt]: 	Training 300/553. train loss: 13.0603,	0.8389 s / batch. (data: 3.17e-04). ETA=11:08:30, max mem: 20.9 GB 
[11/25 11:04:33 visual_prompt]: 	Training 400/553. train loss: 15.3959,	0.8215 s / batch. (data: 3.98e-03). ETA=10:53:14, max mem: 20.9 GB 
[11/25 11:06:15 visual_prompt]: 	Training 500/553. train loss: 9.6030,	0.8346 s / batch. (data: 1.05e-02). ETA=11:02:15, max mem: 20.9 GB 
[11/25 11:07:06 visual_prompt]: Epoch 14 / 100: avg data time: 1.93e-01, avg batch time: 1.0159, average train loss: 17.0693
[11/25 11:08:04 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3056, average loss: 30.5229
[11/25 11:08:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.87	
[11/25 11:08:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/25 11:09:49 visual_prompt]: 	Training 100/553. train loss: 9.4049,	0.8296 s / batch. (data: 2.94e-04). ETA=10:56:12, max mem: 20.9 GB 
[11/25 11:11:29 visual_prompt]: 	Training 200/553. train loss: 97.4315,	0.8160 s / batch. (data: 2.94e-04). ETA=10:44:04, max mem: 20.9 GB 
[11/25 11:13:12 visual_prompt]: 	Training 300/553. train loss: 53.9615,	0.8222 s / batch. (data: 7.92e-04). ETA=10:47:34, max mem: 20.9 GB 
[11/25 11:14:50 visual_prompt]: 	Training 400/553. train loss: 5.0474,	1.0680 s / batch. (data: 2.54e-01). ETA=13:59:25, max mem: 20.9 GB 
[11/25 11:16:32 visual_prompt]: 	Training 500/553. train loss: 2.5343,	1.1387 s / batch. (data: 3.21e-01). ETA=14:53:04, max mem: 20.9 GB 
[11/25 11:17:26 visual_prompt]: Epoch 15 / 100: avg data time: 1.93e-01, avg batch time: 1.0152, average train loss: 15.8687
[11/25 11:18:24 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3077, average loss: 5.2078
[11/25 11:18:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.60	
[11/25 11:18:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/25 11:20:08 visual_prompt]: 	Training 100/553. train loss: 2.0542,	0.8063 s / batch. (data: 2.88e-04). ETA=10:30:18, max mem: 20.9 GB 
[11/25 11:21:49 visual_prompt]: 	Training 200/553. train loss: 7.6370,	0.8253 s / batch. (data: 5.42e-03). ETA=10:43:49, max mem: 20.9 GB 
[11/25 11:23:30 visual_prompt]: 	Training 300/553. train loss: 33.3452,	0.8352 s / batch. (data: 2.48e-02). ETA=10:50:09, max mem: 20.9 GB 
[11/25 11:25:11 visual_prompt]: 	Training 400/553. train loss: 29.9088,	0.8242 s / batch. (data: 6.93e-04). ETA=10:40:11, max mem: 20.9 GB 
[11/25 11:26:51 visual_prompt]: 	Training 500/553. train loss: 51.7052,	1.2680 s / batch. (data: 4.31e-01). ETA=16:22:48, max mem: 20.9 GB 
[11/25 11:27:44 visual_prompt]: Epoch 16 / 100: avg data time: 1.90e-01, avg batch time: 1.0139, average train loss: 16.8948
[11/25 11:28:42 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3058, average loss: 15.7914
[11/25 11:28:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.27	
[11/25 11:28:42 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/25 11:30:27 visual_prompt]: 	Training 100/553. train loss: 4.7427,	0.8240 s / batch. (data: 3.01e-04). ETA=10:36:32, max mem: 20.9 GB 
[11/25 11:32:09 visual_prompt]: 	Training 200/553. train loss: 38.9712,	0.8102 s / batch. (data: 3.95e-03). ETA=10:24:35, max mem: 20.9 GB 
[11/25 11:33:49 visual_prompt]: 	Training 300/553. train loss: 31.7634,	0.8291 s / batch. (data: 7.95e-03). ETA=10:37:44, max mem: 20.9 GB 
[11/25 11:35:30 visual_prompt]: 	Training 400/553. train loss: 12.3765,	1.2985 s / batch. (data: 4.78e-01). ETA=16:36:38, max mem: 20.9 GB 
[11/25 11:37:10 visual_prompt]: 	Training 500/553. train loss: 48.7165,	1.6920 s / batch. (data: 8.52e-01). ETA=21:35:52, max mem: 20.9 GB 
[11/25 11:38:04 visual_prompt]: Epoch 17 / 100: avg data time: 1.92e-01, avg batch time: 1.0147, average train loss: 15.6109
[11/25 11:39:01 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3071, average loss: 55.4827
[11/25 11:39:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.53	
[11/25 11:39:01 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/25 11:40:46 visual_prompt]: 	Training 100/553. train loss: 9.5563,	0.8362 s / batch. (data: 2.44e-02). ETA=10:38:19, max mem: 20.9 GB 
[11/25 11:42:30 visual_prompt]: 	Training 200/553. train loss: 3.4535,	0.8237 s / batch. (data: 3.06e-04). ETA=10:27:20, max mem: 20.9 GB 
[11/25 11:44:11 visual_prompt]: 	Training 300/553. train loss: 1.8644,	0.8360 s / batch. (data: 7.93e-03). ETA=10:35:19, max mem: 20.9 GB 
[11/25 11:45:52 visual_prompt]: 	Training 400/553. train loss: 1.3397,	0.8240 s / batch. (data: 3.08e-04). ETA=10:24:51, max mem: 20.9 GB 
[11/25 11:47:32 visual_prompt]: 	Training 500/553. train loss: 4.7031,	0.8200 s / batch. (data: 3.09e-04). ETA=10:20:26, max mem: 20.9 GB 
[11/25 11:48:23 visual_prompt]: Epoch 18 / 100: avg data time: 1.93e-01, avg batch time: 1.0158, average train loss: 14.6306
[11/25 11:49:21 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3064, average loss: 12.1063
[11/25 11:49:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.99	
[11/25 11:49:21 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/25 11:51:06 visual_prompt]: 	Training 100/553. train loss: 19.3821,	0.8541 s / batch. (data: 2.21e-02). ETA=10:44:05, max mem: 20.9 GB 
[11/25 11:52:47 visual_prompt]: 	Training 200/553. train loss: 1.5416,	0.8115 s / batch. (data: 3.03e-04). ETA=10:10:35, max mem: 20.9 GB 
[11/25 11:54:28 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8120 s / batch. (data: 3.11e-04). ETA=10:09:37, max mem: 20.9 GB 
[11/25 11:56:10 visual_prompt]: 	Training 400/553. train loss: 3.8163,	0.8164 s / batch. (data: 1.05e-02). ETA=10:11:31, max mem: 20.9 GB 
[11/25 11:57:47 visual_prompt]: 	Training 500/553. train loss: 1.2225,	0.8460 s / batch. (data: 9.99e-03). ETA=10:32:21, max mem: 20.9 GB 
[11/25 11:58:40 visual_prompt]: Epoch 19 / 100: avg data time: 1.88e-01, avg batch time: 1.0112, average train loss: 17.8823
[11/25 11:59:38 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3059, average loss: 38.2857
[11/25 11:59:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.57	
[11/25 11:59:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/25 12:01:22 visual_prompt]: 	Training 100/553. train loss: 8.3854,	0.8160 s / batch. (data: 3.31e-04). ETA=10:07:50, max mem: 20.9 GB 
[11/25 12:03:05 visual_prompt]: 	Training 200/553. train loss: 15.4685,	0.8299 s / batch. (data: 1.79e-02). ETA=10:16:49, max mem: 20.9 GB 
[11/25 12:04:46 visual_prompt]: 	Training 300/553. train loss: 7.9870,	0.8360 s / batch. (data: 3.03e-04). ETA=10:19:55, max mem: 20.9 GB 
[11/25 12:06:26 visual_prompt]: 	Training 400/553. train loss: 4.9703,	0.8146 s / batch. (data: 7.94e-03). ETA=10:02:43, max mem: 20.9 GB 
[11/25 12:08:06 visual_prompt]: 	Training 500/553. train loss: 7.5007,	0.8285 s / batch. (data: 5.42e-03). ETA=10:11:36, max mem: 20.9 GB 
[11/25 12:09:00 visual_prompt]: Epoch 20 / 100: avg data time: 1.92e-01, avg batch time: 1.0156, average train loss: 15.1892
[11/25 12:09:58 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3066, average loss: 4.2038
[11/25 12:09:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.69	
[11/25 12:09:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/25 12:11:46 visual_prompt]: 	Training 100/553. train loss: 34.4833,	0.8248 s / batch. (data: 5.46e-03). ETA=10:06:47, max mem: 20.9 GB 
[11/25 12:13:25 visual_prompt]: 	Training 200/553. train loss: 46.2701,	0.8270 s / batch. (data: 3.12e-04). ETA=10:06:59, max mem: 20.9 GB 
[11/25 12:15:06 visual_prompt]: 	Training 300/553. train loss: 20.9474,	1.2000 s / batch. (data: 3.71e-01). ETA=14:38:49, max mem: 20.9 GB 
[11/25 12:16:47 visual_prompt]: 	Training 400/553. train loss: 60.0103,	0.8488 s / batch. (data: 5.47e-03). ETA=10:20:11, max mem: 20.9 GB 
[11/25 12:18:28 visual_prompt]: 	Training 500/553. train loss: 14.5543,	0.8610 s / batch. (data: 2.10e-02). ETA=10:27:41, max mem: 20.9 GB 
[11/25 12:19:20 visual_prompt]: Epoch 21 / 100: avg data time: 1.92e-01, avg batch time: 1.0156, average train loss: 16.8275
[11/25 12:20:18 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3075, average loss: 16.6812
[11/25 12:20:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.55	
[11/25 12:20:18 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/25 12:22:02 visual_prompt]: 	Training 100/553. train loss: 5.9140,	0.8477 s / batch. (data: 2.06e-02). ETA=10:15:50, max mem: 20.9 GB 
[11/25 12:23:43 visual_prompt]: 	Training 200/553. train loss: 5.5664,	0.8280 s / batch. (data: 3.37e-04). ETA=10:00:07, max mem: 20.9 GB 
[11/25 12:25:21 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.0625 s / batch. (data: 2.31e-01). ETA=12:48:18, max mem: 20.9 GB 
[11/25 12:27:04 visual_prompt]: 	Training 400/553. train loss: 9.2249,	0.8320 s / batch. (data: 7.91e-03). ETA=10:00:13, max mem: 20.9 GB 
[11/25 12:28:45 visual_prompt]: 	Training 500/553. train loss: 17.4958,	0.8345 s / batch. (data: 1.04e-02). ETA=10:00:38, max mem: 20.9 GB 
[11/25 12:29:39 visual_prompt]: Epoch 22 / 100: avg data time: 1.91e-01, avg batch time: 1.0147, average train loss: 14.0558
[11/25 12:30:37 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3072, average loss: 25.2010
[11/25 12:30:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.91	
[11/25 12:30:37 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/25 12:32:23 visual_prompt]: 	Training 100/553. train loss: 2.0199,	1.2167 s / batch. (data: 3.81e-01). ETA=14:32:37, max mem: 20.9 GB 
[11/25 12:34:06 visual_prompt]: 	Training 200/553. train loss: 18.6884,	0.8735 s / batch. (data: 5.19e-02). ETA=10:25:02, max mem: 20.9 GB 
[11/25 12:35:49 visual_prompt]: 	Training 300/553. train loss: 11.0697,	0.8280 s / batch. (data: 8.40e-04). ETA=9:51:06, max mem: 20.9 GB 
[11/25 12:37:28 visual_prompt]: 	Training 400/553. train loss: 3.5827,	0.8240 s / batch. (data: 3.00e-04). ETA=9:46:53, max mem: 20.9 GB 
[11/25 12:39:07 visual_prompt]: 	Training 500/553. train loss: 3.3811,	0.8580 s / batch. (data: 1.05e-02). ETA=10:09:39, max mem: 20.9 GB 
[11/25 12:39:59 visual_prompt]: Epoch 23 / 100: avg data time: 1.94e-01, avg batch time: 1.0166, average train loss: 15.2128
[11/25 12:40:57 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3080, average loss: 10.3884
[11/25 12:40:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.68	
[11/25 12:40:57 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/25 12:42:40 visual_prompt]: 	Training 100/553. train loss: 5.0066,	0.8200 s / batch. (data: 1.15e-02). ETA=9:40:36, max mem: 20.9 GB 
[11/25 12:44:20 visual_prompt]: 	Training 200/553. train loss: 35.4084,	0.8376 s / batch. (data: 1.19e-02). ETA=9:51:38, max mem: 20.9 GB 
[11/25 12:46:02 visual_prompt]: 	Training 300/553. train loss: 17.1040,	1.0786 s / batch. (data: 2.45e-01). ETA=12:40:03, max mem: 20.9 GB 
[11/25 12:47:51 visual_prompt]: 	Training 400/553. train loss: 8.9767,	0.8280 s / batch. (data: 3.10e-04). ETA=9:42:05, max mem: 20.9 GB 
[11/25 12:49:33 visual_prompt]: 	Training 500/553. train loss: 4.3326,	0.8085 s / batch. (data: 3.15e-04). ETA=9:27:04, max mem: 20.9 GB 
[11/25 12:50:27 visual_prompt]: Epoch 24 / 100: avg data time: 2.05e-01, avg batch time: 1.0288, average train loss: 16.5487
[11/25 12:51:25 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3072, average loss: 10.6571
[11/25 12:51:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.46	
[11/25 12:51:25 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/25 12:53:13 visual_prompt]: 	Training 100/553. train loss: 18.5648,	0.8520 s / batch. (data: 5.42e-03). ETA=9:55:22, max mem: 20.9 GB 
[11/25 12:54:51 visual_prompt]: 	Training 200/553. train loss: 16.5736,	0.8320 s / batch. (data: 3.14e-04). ETA=9:40:00, max mem: 20.9 GB 
[11/25 12:56:32 visual_prompt]: 	Training 300/553. train loss: 13.8536,	0.8240 s / batch. (data: 3.52e-04). ETA=9:33:03, max mem: 20.9 GB 
[11/25 12:58:13 visual_prompt]: 	Training 400/553. train loss: 37.6028,	1.3640 s / batch. (data: 5.27e-01). ETA=15:46:19, max mem: 20.9 GB 
[11/25 12:59:54 visual_prompt]: 	Training 500/553. train loss: 9.4053,	1.6118 s / batch. (data: 7.91e-01). ETA=18:35:34, max mem: 20.9 GB 
[11/25 13:00:47 visual_prompt]: Epoch 25 / 100: avg data time: 1.93e-01, avg batch time: 1.0167, average train loss: 14.4637
[11/25 13:01:45 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3068, average loss: 13.4995
[11/25 13:01:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.05	
[11/25 13:01:45 visual_prompt]: Stopping early.
[11/25 13:01:45 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 13:01:45 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 13:01:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/25 13:01:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 13:01:45 visual_prompt]: Training with config:
[11/25 13:01:45 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/25 13:01:45 visual_prompt]: Loading training data...
[11/25 13:01:45 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 13:01:45 visual_prompt]: Loading validation data...
[11/25 13:01:45 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 13:01:45 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 13:01:48 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 13:01:48 visual_prompt]: tuned percent:0.525
[11/25 13:01:48 visual_prompt]: Device used for model: 0
[11/25 13:01:48 visual_prompt]: Setting up Evaluator...
[11/25 13:01:48 visual_prompt]: Setting up Trainer...
[11/25 13:01:48 visual_prompt]: 	Setting up the optimizer...
[11/25 13:01:48 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 13:03:33 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8240 s / batch. (data: 2.89e-04). ETA=12:38:05, max mem: 20.9 GB 
[11/25 13:05:12 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8320 s / batch. (data: 2.65e-04). ETA=12:44:05, max mem: 20.9 GB 
[11/25 13:06:56 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.2227 s / batch. (data: 3.77e-01). ETA=18:40:48, max mem: 20.9 GB 
[11/25 13:08:35 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8149 s / batch. (data: 2.97e-04). ETA=12:25:37, max mem: 20.9 GB 
[11/25 13:10:19 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8312 s / batch. (data: 8.53e-04). ETA=12:39:10, max mem: 20.9 GB 
[11/25 13:11:12 visual_prompt]: Epoch 1 / 100: avg data time: 1.94e-01, avg batch time: 1.0186, average train loss: 1.5403
[11/25 13:12:10 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3056, average loss: 1.5201
[11/25 13:12:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 13:12:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/25 13:13:54 visual_prompt]: 	Training 100/553. train loss: 0.9062,	1.4473 s / batch. (data: 6.30e-01). ETA=21:58:08, max mem: 20.9 GB 
[11/25 13:15:35 visual_prompt]: 	Training 200/553. train loss: 0.0004,	0.8110 s / batch. (data: 3.23e-04). ETA=12:17:16, max mem: 20.9 GB 
[11/25 13:17:18 visual_prompt]: 	Training 300/553. train loss: 2.3715,	1.1825 s / batch. (data: 3.51e-01). ETA=17:53:04, max mem: 20.9 GB 
[11/25 13:18:59 visual_prompt]: 	Training 400/553. train loss: 0.8650,	0.8279 s / batch. (data: 3.03e-04). ETA=12:29:55, max mem: 20.9 GB 
[11/25 13:20:40 visual_prompt]: 	Training 500/553. train loss: 0.7718,	0.8082 s / batch. (data: 3.05e-04). ETA=12:10:43, max mem: 20.9 GB 
[11/25 13:21:32 visual_prompt]: Epoch 2 / 100: avg data time: 1.93e-01, avg batch time: 1.0169, average train loss: 1.8079
[11/25 13:22:30 visual_prompt]: Inference (val):avg data time: 2.09e-04, avg batch time: 0.3071, average loss: 2.7599
[11/25 13:22:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.04	
[11/25 13:22:30 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/25 13:24:13 visual_prompt]: 	Training 100/553. train loss: 0.8204,	0.8240 s / batch. (data: 2.96e-04). ETA=12:22:53, max mem: 20.9 GB 
[11/25 13:25:57 visual_prompt]: 	Training 200/553. train loss: 0.8124,	2.1410 s / batch. (data: 1.31e+00). ETA=1 day, 8:06:42, max mem: 20.9 GB 
[11/25 13:27:37 visual_prompt]: 	Training 300/553. train loss: 1.1878,	0.8480 s / batch. (data: 3.16e-04). ETA=12:41:41, max mem: 20.9 GB 
[11/25 13:29:19 visual_prompt]: 	Training 400/553. train loss: 3.5394,	0.8309 s / batch. (data: 3.06e-04). ETA=12:24:58, max mem: 20.9 GB 
[11/25 13:31:02 visual_prompt]: 	Training 500/553. train loss: 0.9536,	1.2800 s / batch. (data: 4.64e-01). ETA=19:05:27, max mem: 20.9 GB 
[11/25 13:31:53 visual_prompt]: Epoch 3 / 100: avg data time: 1.91e-01, avg batch time: 1.0177, average train loss: 2.4124
[11/25 13:32:51 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3070, average loss: 3.8518
[11/25 13:32:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.31	
[11/25 13:32:51 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/25 13:34:37 visual_prompt]: 	Training 100/553. train loss: 5.8539,	0.8270 s / batch. (data: 3.43e-04). ETA=12:18:00, max mem: 20.9 GB 
[11/25 13:36:18 visual_prompt]: 	Training 200/553. train loss: 4.1352,	0.8230 s / batch. (data: 7.46e-03). ETA=12:13:04, max mem: 20.9 GB 
[11/25 13:38:00 visual_prompt]: 	Training 300/553. train loss: 1.8282,	1.6746 s / batch. (data: 8.52e-01). ETA=1 day, 0:48:46, max mem: 20.9 GB 
[11/25 13:39:37 visual_prompt]: 	Training 400/553. train loss: 22.1794,	1.3756 s / batch. (data: 5.48e-01). ETA=20:20:40, max mem: 20.9 GB 
[11/25 13:41:21 visual_prompt]: 	Training 500/553. train loss: 0.6721,	3.6956 s / batch. (data: 2.88e+00). ETA=2 days, 6:33:08, max mem: 20.9 GB 
[11/25 13:42:14 visual_prompt]: Epoch 4 / 100: avg data time: 1.93e-01, avg batch time: 1.0176, average train loss: 3.9943
[11/25 13:43:13 visual_prompt]: Inference (val):avg data time: 1.83e-04, avg batch time: 0.3069, average loss: 1.0283
[11/25 13:43:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.76	
[11/25 13:43:13 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/25 13:44:57 visual_prompt]: 	Training 100/553. train loss: 15.8534,	0.8229 s / batch. (data: 2.63e-04). ETA=12:06:45, max mem: 20.9 GB 
[11/25 13:46:38 visual_prompt]: 	Training 200/553. train loss: 0.5989,	1.0951 s / batch. (data: 2.74e-01). ETA=16:05:17, max mem: 20.9 GB 
[11/25 13:48:22 visual_prompt]: 	Training 300/553. train loss: 20.0197,	0.8256 s / batch. (data: 7.65e-03). ETA=12:06:19, max mem: 20.9 GB 
[11/25 13:50:02 visual_prompt]: 	Training 400/553. train loss: 3.1582,	0.8201 s / batch. (data: 2.81e-04). ETA=12:00:08, max mem: 20.9 GB 
[11/25 13:51:43 visual_prompt]: 	Training 500/553. train loss: 6.6953,	0.8160 s / batch. (data: 3.79e-04). ETA=11:55:13, max mem: 20.9 GB 
[11/25 13:52:37 visual_prompt]: Epoch 5 / 100: avg data time: 1.93e-01, avg batch time: 1.0180, average train loss: 5.6487
[11/25 13:53:34 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3067, average loss: 10.8251
[11/25 13:53:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.64	
[11/25 13:53:34 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/25 13:55:21 visual_prompt]: 	Training 100/553. train loss: 0.5371,	0.8237 s / batch. (data: 7.05e-04). ETA=11:59:50, max mem: 20.9 GB 
[11/25 13:57:02 visual_prompt]: 	Training 200/553. train loss: 22.6648,	0.8360 s / batch. (data: 3.14e-04). ETA=12:09:12, max mem: 20.9 GB 
[11/25 13:58:41 visual_prompt]: 	Training 300/553. train loss: 5.7618,	0.8203 s / batch. (data: 5.45e-03). ETA=11:54:09, max mem: 20.9 GB 
[11/25 14:00:27 visual_prompt]: 	Training 400/553. train loss: 2.8443,	0.8239 s / batch. (data: 3.33e-04). ETA=11:55:55, max mem: 20.9 GB 
[11/25 14:02:07 visual_prompt]: 	Training 500/553. train loss: 16.9213,	1.0390 s / batch. (data: 2.32e-01). ETA=15:01:03, max mem: 20.9 GB 
[11/25 14:02:58 visual_prompt]: Epoch 6 / 100: avg data time: 1.96e-01, avg batch time: 1.0196, average train loss: 7.5020
[11/25 14:03:56 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3070, average loss: 3.5944
[11/25 14:03:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.87	
[11/25 14:03:56 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/25 14:05:39 visual_prompt]: 	Training 100/553. train loss: 8.8278,	0.8600 s / batch. (data: 1.05e-02). ETA=12:23:38, max mem: 20.9 GB 
[11/25 14:07:21 visual_prompt]: 	Training 200/553. train loss: 3.8332,	0.8218 s / batch. (data: 3.45e-04). ETA=11:49:12, max mem: 20.9 GB 
[11/25 14:09:06 visual_prompt]: 	Training 300/553. train loss: 4.1304,	2.0587 s / batch. (data: 1.24e+00). ETA=1 day, 5:33:16, max mem: 20.9 GB 
[11/25 14:10:48 visual_prompt]: 	Training 400/553. train loss: 4.4043,	1.8956 s / batch. (data: 1.08e+00). ETA=1 day, 3:09:39, max mem: 20.9 GB 
[11/25 14:12:27 visual_prompt]: 	Training 500/553. train loss: 33.8506,	0.8178 s / batch. (data: 4.03e-04). ETA=11:41:40, max mem: 20.9 GB 
[11/25 14:13:19 visual_prompt]: Epoch 7 / 100: avg data time: 1.92e-01, avg batch time: 1.0166, average train loss: 9.9244
[11/25 14:14:16 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3066, average loss: 0.8425
[11/25 14:14:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.95	
[11/25 14:14:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/25 14:16:00 visual_prompt]: 	Training 100/553. train loss: 7.2772,	0.8400 s / batch. (data: 3.07e-04). ETA=11:58:35, max mem: 20.9 GB 
[11/25 14:17:43 visual_prompt]: 	Training 200/553. train loss: 4.0536,	0.8449 s / batch. (data: 1.05e-02). ETA=12:01:23, max mem: 20.9 GB 
[11/25 14:19:25 visual_prompt]: 	Training 300/553. train loss: 4.9816,	0.8271 s / batch. (data: 1.20e-02). ETA=11:44:48, max mem: 20.9 GB 
[11/25 14:21:06 visual_prompt]: 	Training 400/553. train loss: 9.5666,	1.0247 s / batch. (data: 1.92e-01). ETA=14:31:30, max mem: 20.9 GB 
[11/25 14:22:48 visual_prompt]: 	Training 500/553. train loss: 32.3815,	1.5360 s / batch. (data: 7.12e-01). ETA=21:43:45, max mem: 20.9 GB 
[11/25 14:23:41 visual_prompt]: Epoch 8 / 100: avg data time: 1.97e-01, avg batch time: 1.0211, average train loss: 9.7738
[11/25 14:24:39 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3062, average loss: 8.3319
[11/25 14:24:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.51	
[11/25 14:24:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/25 14:26:24 visual_prompt]: 	Training 100/553. train loss: 8.8853,	0.8192 s / batch. (data: 3.28e-03). ETA=11:33:17, max mem: 20.9 GB 
[11/25 14:28:04 visual_prompt]: 	Training 200/553. train loss: 10.7656,	0.8391 s / batch. (data: 1.05e-02). ETA=11:48:41, max mem: 20.9 GB 
[11/25 14:29:46 visual_prompt]: 	Training 300/553. train loss: 3.2274,	1.7533 s / batch. (data: 9.36e-01). ETA=1 day, 0:37:55, max mem: 20.9 GB 
[11/25 14:31:29 visual_prompt]: 	Training 400/553. train loss: 3.7891,	0.8440 s / batch. (data: 8.34e-04). ETA=11:50:00, max mem: 20.9 GB 
[11/25 14:33:11 visual_prompt]: 	Training 500/553. train loss: 5.6567,	1.0376 s / batch. (data: 2.01e-01). ETA=14:31:10, max mem: 20.9 GB 
[11/25 14:34:02 visual_prompt]: Epoch 9 / 100: avg data time: 1.93e-01, avg batch time: 1.0179, average train loss: 9.8596
[11/25 14:35:00 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3059, average loss: 9.2470
[11/25 14:35:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.70	
[11/25 14:35:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/25 14:36:48 visual_prompt]: 	Training 100/553. train loss: 79.6099,	0.8327 s / batch. (data: 1.05e-02). ETA=11:37:00, max mem: 20.9 GB 
[11/25 14:38:28 visual_prompt]: 	Training 200/553. train loss: 2.5538,	0.8440 s / batch. (data: 2.96e-04). ETA=11:45:03, max mem: 20.9 GB 
[11/25 14:40:08 visual_prompt]: 	Training 300/553. train loss: 30.6063,	1.0120 s / batch. (data: 1.77e-01). ETA=14:03:44, max mem: 20.9 GB 
[11/25 14:41:47 visual_prompt]: 	Training 400/553. train loss: 14.8067,	0.8301 s / batch. (data: 3.29e-04). ETA=11:30:43, max mem: 20.9 GB 
[11/25 14:43:29 visual_prompt]: 	Training 500/553. train loss: 11.6864,	0.8450 s / batch. (data: 1.71e-03). ETA=11:41:42, max mem: 20.9 GB 
[11/25 14:44:22 visual_prompt]: Epoch 10 / 100: avg data time: 1.92e-01, avg batch time: 1.0150, average train loss: 16.9430
[11/25 14:45:19 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3087, average loss: 3.1175
[11/25 14:45:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.73	
[11/25 14:45:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/25 14:47:07 visual_prompt]: 	Training 100/553. train loss: 8.8875,	0.8649 s / batch. (data: 8.21e-03). ETA=11:55:59, max mem: 20.9 GB 
[11/25 14:48:50 visual_prompt]: 	Training 200/553. train loss: 1.5306,	0.8614 s / batch. (data: 3.74e-02). ETA=11:51:39, max mem: 20.9 GB 
[11/25 14:50:30 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1989 s / batch. (data: 1.38e+00). ETA=1 day, 6:12:59, max mem: 20.9 GB 
[11/25 14:52:11 visual_prompt]: 	Training 400/553. train loss: 8.2091,	0.8182 s / batch. (data: 2.85e-04). ETA=11:13:14, max mem: 20.9 GB 
[11/25 14:53:51 visual_prompt]: 	Training 500/553. train loss: 4.4132,	0.8246 s / batch. (data: 5.42e-03). ETA=11:17:08, max mem: 20.9 GB 
[11/25 14:54:44 visual_prompt]: Epoch 11 / 100: avg data time: 1.96e-01, avg batch time: 1.0205, average train loss: 17.7631
[11/25 14:55:42 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3078, average loss: 2.8473
[11/25 14:55:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.31	
[11/25 14:55:42 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/25 14:57:28 visual_prompt]: 	Training 100/553. train loss: 6.9889,	0.8220 s / batch. (data: 3.36e-04). ETA=11:12:52, max mem: 20.9 GB 
[11/25 14:59:11 visual_prompt]: 	Training 200/553. train loss: 3.7040,	0.8240 s / batch. (data: 5.39e-03). ETA=11:13:10, max mem: 20.9 GB 
[11/25 15:00:51 visual_prompt]: 	Training 300/553. train loss: 6.6281,	0.8440 s / batch. (data: 1.05e-02). ETA=11:28:04, max mem: 20.9 GB 
[11/25 15:02:33 visual_prompt]: 	Training 400/553. train loss: 1.5824,	0.8155 s / batch. (data: 4.67e-04). ETA=11:03:32, max mem: 20.9 GB 
[11/25 15:04:14 visual_prompt]: 	Training 500/553. train loss: 67.3412,	0.8401 s / batch. (data: 1.25e-03). ETA=11:22:08, max mem: 20.9 GB 
[11/25 15:05:06 visual_prompt]: Epoch 12 / 100: avg data time: 1.96e-01, avg batch time: 1.0193, average train loss: 17.0899
[11/25 15:06:03 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3075, average loss: 4.5959
[11/25 15:06:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.78	
[11/25 15:06:03 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/25 15:07:49 visual_prompt]: 	Training 100/553. train loss: 5.7661,	0.8459 s / batch. (data: 9.07e-03). ETA=11:24:40, max mem: 20.9 GB 
[11/25 15:09:27 visual_prompt]: 	Training 200/553. train loss: 3.9897,	0.8213 s / batch. (data: 3.24e-04). ETA=11:03:24, max mem: 20.9 GB 
[11/25 15:11:10 visual_prompt]: 	Training 300/553. train loss: 12.3657,	1.7640 s / batch. (data: 9.53e-01). ETA=23:41:55, max mem: 20.9 GB 
[11/25 15:12:50 visual_prompt]: 	Training 400/553. train loss: 35.4608,	0.8545 s / batch. (data: 5.43e-03). ETA=11:27:23, max mem: 20.9 GB 
[11/25 15:14:32 visual_prompt]: 	Training 500/553. train loss: 4.5832,	0.8303 s / batch. (data: 3.17e-04). ETA=11:06:31, max mem: 20.9 GB 
[11/25 15:15:24 visual_prompt]: Epoch 13 / 100: avg data time: 1.90e-01, avg batch time: 1.0137, average train loss: 15.4765
[11/25 15:16:22 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3085, average loss: 11.7818
[11/25 15:16:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.04	
[11/25 15:16:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/25 15:18:08 visual_prompt]: 	Training 100/553. train loss: 21.9701,	0.8440 s / batch. (data: 7.96e-03). ETA=11:15:21, max mem: 20.9 GB 
[11/25 15:19:49 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.2238 s / batch. (data: 3.98e-01). ETA=16:17:15, max mem: 20.9 GB 
[11/25 15:21:30 visual_prompt]: 	Training 300/553. train loss: 42.3929,	0.8405 s / batch. (data: 1.26e-02). ETA=11:09:44, max mem: 20.9 GB 
[11/25 15:23:11 visual_prompt]: 	Training 400/553. train loss: 13.3594,	0.8187 s / batch. (data: 3.11e-04). ETA=10:50:59, max mem: 20.9 GB 
[11/25 15:24:52 visual_prompt]: 	Training 500/553. train loss: 32.8339,	0.8474 s / batch. (data: 2.09e-02). ETA=11:12:26, max mem: 20.9 GB 
[11/25 15:25:43 visual_prompt]: Epoch 14 / 100: avg data time: 1.91e-01, avg batch time: 1.0144, average train loss: 14.9519
[11/25 15:26:41 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3060, average loss: 8.8228
[11/25 15:26:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.32	
[11/25 15:26:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/25 15:28:26 visual_prompt]: 	Training 100/553. train loss: 8.3991,	0.8230 s / batch. (data: 5.43e-03). ETA=10:50:56, max mem: 20.9 GB 
[11/25 15:30:06 visual_prompt]: 	Training 200/553. train loss: 28.2339,	0.8762 s / batch. (data: 2.44e-02). ETA=11:31:33, max mem: 20.9 GB 
[11/25 15:31:48 visual_prompt]: 	Training 300/553. train loss: 34.7296,	0.8406 s / batch. (data: 2.54e-04). ETA=11:02:03, max mem: 20.9 GB 
[11/25 15:33:27 visual_prompt]: 	Training 400/553. train loss: 0.6504,	1.4328 s / batch. (data: 6.18e-01). ETA=18:46:07, max mem: 20.9 GB 
[11/25 15:35:09 visual_prompt]: 	Training 500/553. train loss: 9.5385,	1.5880 s / batch. (data: 7.33e-01). ETA=20:45:28, max mem: 20.9 GB 
[11/25 15:36:02 visual_prompt]: Epoch 15 / 100: avg data time: 1.90e-01, avg batch time: 1.0131, average train loss: 18.0007
[11/25 15:37:00 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3087, average loss: 21.3152
[11/25 15:37:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.15	
[11/25 15:37:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/25 15:38:44 visual_prompt]: 	Training 100/553. train loss: 24.1684,	0.8217 s / batch. (data: 1.02e-02). ETA=10:42:20, max mem: 20.9 GB 
[11/25 15:40:25 visual_prompt]: 	Training 200/553. train loss: 22.2943,	0.8355 s / batch. (data: 1.05e-02). ETA=10:51:47, max mem: 20.9 GB 
[11/25 15:42:07 visual_prompt]: 	Training 300/553. train loss: 19.5646,	0.8800 s / batch. (data: 3.07e-04). ETA=11:25:00, max mem: 20.9 GB 
[11/25 15:43:48 visual_prompt]: 	Training 400/553. train loss: 10.7294,	0.8292 s / batch. (data: 1.05e-02). ETA=10:44:02, max mem: 20.9 GB 
[11/25 15:45:28 visual_prompt]: 	Training 500/553. train loss: 7.4963,	0.8191 s / batch. (data: 3.54e-04). ETA=10:34:50, max mem: 20.9 GB 
[11/25 15:46:22 visual_prompt]: Epoch 16 / 100: avg data time: 1.93e-01, avg batch time: 1.0166, average train loss: 16.8012
[11/25 15:47:20 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3059, average loss: 1.8160
[11/25 15:47:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.65	
[11/25 15:47:20 visual_prompt]: Best epoch 16: best metric: -1.816
[11/25 15:47:20 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/25 15:49:05 visual_prompt]: 	Training 100/553. train loss: 3.3493,	0.8201 s / batch. (data: 3.51e-04). ETA=10:33:35, max mem: 20.9 GB 
[11/25 15:50:47 visual_prompt]: 	Training 200/553. train loss: 6.1398,	0.8226 s / batch. (data: 2.37e-04). ETA=10:34:06, max mem: 20.9 GB 
[11/25 15:52:28 visual_prompt]: 	Training 300/553. train loss: 0.5005,	0.8348 s / batch. (data: 2.96e-04). ETA=10:42:06, max mem: 20.9 GB 
[11/25 15:54:08 visual_prompt]: 	Training 400/553. train loss: 5.0081,	1.2400 s / batch. (data: 4.15e-01). ETA=15:51:45, max mem: 20.9 GB 
[11/25 15:55:49 visual_prompt]: 	Training 500/553. train loss: 4.7998,	1.7513 s / batch. (data: 9.35e-01). ETA=22:21:16, max mem: 20.9 GB 
[11/25 15:56:43 visual_prompt]: Epoch 17 / 100: avg data time: 1.93e-01, avg batch time: 1.0180, average train loss: 15.2727
[11/25 15:57:40 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3064, average loss: 18.4460
[11/25 15:57:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.13	
[11/25 15:57:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/25 15:59:26 visual_prompt]: 	Training 100/553. train loss: 28.6263,	0.8217 s / batch. (data: 3.07e-04). ETA=10:27:12, max mem: 20.9 GB 
[11/25 16:01:09 visual_prompt]: 	Training 200/553. train loss: 32.5979,	0.8520 s / batch. (data: 2.99e-04). ETA=10:48:56, max mem: 20.9 GB 
[11/25 16:02:51 visual_prompt]: 	Training 300/553. train loss: 8.1456,	0.8293 s / batch. (data: 2.95e-04). ETA=10:30:15, max mem: 20.9 GB 
[11/25 16:04:31 visual_prompt]: 	Training 400/553. train loss: 0.7908,	0.8611 s / batch. (data: 2.25e-02). ETA=10:52:57, max mem: 20.9 GB 
[11/25 16:06:11 visual_prompt]: 	Training 500/553. train loss: 7.3902,	0.8232 s / batch. (data: 3.79e-04). ETA=10:22:52, max mem: 20.9 GB 
[11/25 16:07:02 visual_prompt]: Epoch 18 / 100: avg data time: 1.92e-01, avg batch time: 1.0162, average train loss: 14.4648
[11/25 16:08:00 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3062, average loss: 4.8361
[11/25 16:08:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.09	
[11/25 16:08:00 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/25 16:09:45 visual_prompt]: 	Training 100/553. train loss: 2.9938,	1.2320 s / batch. (data: 4.06e-01). ETA=15:29:03, max mem: 20.9 GB 
[11/25 16:11:26 visual_prompt]: 	Training 200/553. train loss: 9.4797,	0.8200 s / batch. (data: 1.20e-02). ETA=10:16:58, max mem: 20.9 GB 
[11/25 16:13:08 visual_prompt]: 	Training 300/553. train loss: 67.5854,	0.8137 s / batch. (data: 2.97e-04). ETA=10:10:55, max mem: 20.9 GB 
[11/25 16:14:50 visual_prompt]: 	Training 400/553. train loss: 6.8751,	0.8160 s / batch. (data: 3.14e-04). ETA=10:11:15, max mem: 20.9 GB 
[11/25 16:16:28 visual_prompt]: 	Training 500/553. train loss: 5.4041,	0.8110 s / batch. (data: 3.14e-04). ETA=10:06:08, max mem: 20.9 GB 
[11/25 16:17:20 visual_prompt]: Epoch 19 / 100: avg data time: 1.90e-01, avg batch time: 1.0127, average train loss: 14.8815
[11/25 16:18:18 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3061, average loss: 20.3297
[11/25 16:18:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.58	
[11/25 16:18:18 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/25 16:20:02 visual_prompt]: 	Training 100/553. train loss: 3.2001,	0.8305 s / batch. (data: 3.13e-04). ETA=10:18:38, max mem: 20.9 GB 
[11/25 16:21:44 visual_prompt]: 	Training 200/553. train loss: 48.3757,	0.8362 s / batch. (data: 3.19e-04). ETA=10:21:30, max mem: 20.9 GB 
[11/25 16:23:25 visual_prompt]: 	Training 300/553. train loss: 2.0789,	0.8289 s / batch. (data: 1.56e-02). ETA=10:14:39, max mem: 20.9 GB 
[11/25 16:25:06 visual_prompt]: 	Training 400/553. train loss: 1.1873,	0.8319 s / batch. (data: 3.19e-04). ETA=10:15:29, max mem: 20.9 GB 
[11/25 16:26:46 visual_prompt]: 	Training 500/553. train loss: 11.6085,	0.8200 s / batch. (data: 7.95e-03). ETA=10:05:20, max mem: 20.9 GB 
[11/25 16:27:40 visual_prompt]: Epoch 20 / 100: avg data time: 1.92e-01, avg batch time: 1.0149, average train loss: 16.6486
[11/25 16:28:38 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3079, average loss: 26.0773
[11/25 16:28:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.82	
[11/25 16:28:38 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/25 16:30:24 visual_prompt]: 	Training 100/553. train loss: 11.1664,	0.8248 s / batch. (data: 5.45e-03). ETA=10:06:45, max mem: 20.9 GB 
[11/25 16:32:05 visual_prompt]: 	Training 200/553. train loss: 27.7361,	0.8086 s / batch. (data: 3.00e-04). ETA=9:53:28, max mem: 20.9 GB 
[11/25 16:33:45 visual_prompt]: 	Training 300/553. train loss: 31.1452,	1.3522 s / batch. (data: 5.34e-01). ETA=16:30:15, max mem: 20.9 GB 
[11/25 16:35:25 visual_prompt]: 	Training 400/553. train loss: 5.8189,	0.8297 s / batch. (data: 2.93e-04). ETA=10:06:12, max mem: 20.9 GB 
[11/25 16:37:08 visual_prompt]: 	Training 500/553. train loss: 26.2486,	0.8322 s / batch. (data: 5.41e-03). ETA=10:06:39, max mem: 20.9 GB 
[11/25 16:37:59 visual_prompt]: Epoch 21 / 100: avg data time: 1.93e-01, avg batch time: 1.0157, average train loss: 15.1601
[11/25 16:38:57 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3070, average loss: 8.8783
[11/25 16:38:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 51.47	
[11/25 16:38:57 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/25 16:40:41 visual_prompt]: 	Training 100/553. train loss: 46.0157,	0.8120 s / batch. (data: 2.99e-04). ETA=9:49:52, max mem: 20.9 GB 
[11/25 16:42:23 visual_prompt]: 	Training 200/553. train loss: 3.3613,	0.8240 s / batch. (data: 1.20e-02). ETA=9:57:14, max mem: 20.9 GB 
[11/25 16:44:02 visual_prompt]: 	Training 300/553. train loss: 0.0013,	0.8362 s / batch. (data: 1.22e-02). ETA=10:04:41, max mem: 20.9 GB 
[11/25 16:45:44 visual_prompt]: 	Training 400/553. train loss: 23.6424,	0.8238 s / batch. (data: 5.46e-03). ETA=9:54:18, max mem: 20.9 GB 
[11/25 16:47:25 visual_prompt]: 	Training 500/553. train loss: 4.7286,	0.8270 s / batch. (data: 1.05e-02). ETA=9:55:17, max mem: 20.9 GB 
[11/25 16:48:19 visual_prompt]: Epoch 22 / 100: avg data time: 1.92e-01, avg batch time: 1.0163, average train loss: 13.4969
[11/25 16:49:17 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3069, average loss: 11.3088
[11/25 16:49:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.83	
[11/25 16:49:17 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/25 16:51:03 visual_prompt]: 	Training 100/553. train loss: 2.1520,	0.8181 s / batch. (data: 5.47e-03). ETA=9:46:45, max mem: 20.9 GB 
[11/25 16:52:45 visual_prompt]: 	Training 200/553. train loss: 30.8022,	0.8307 s / batch. (data: 3.11e-04). ETA=9:54:26, max mem: 20.9 GB 
[11/25 16:54:28 visual_prompt]: 	Training 300/553. train loss: 8.1021,	0.8584 s / batch. (data: 1.56e-02). ETA=10:12:48, max mem: 20.9 GB 
[11/25 16:56:07 visual_prompt]: 	Training 400/553. train loss: 5.6357,	0.8234 s / batch. (data: 8.17e-04). ETA=9:46:25, max mem: 20.9 GB 
[11/25 16:57:50 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8140 s / batch. (data: 2.68e-04). ETA=9:38:22, max mem: 20.9 GB 
[11/25 16:58:43 visual_prompt]: Epoch 23 / 100: avg data time: 1.99e-01, avg batch time: 1.0224, average train loss: 16.6052
[11/25 16:59:40 visual_prompt]: Inference (val):avg data time: 2.58e-04, avg batch time: 0.3065, average loss: 17.7758
[11/25 16:59:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.79	
[11/25 16:59:40 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/25 17:01:23 visual_prompt]: 	Training 100/553. train loss: 23.6612,	0.8345 s / batch. (data: 2.96e-04). ETA=9:50:51, max mem: 20.9 GB 
[11/25 17:03:05 visual_prompt]: 	Training 200/553. train loss: 9.9993,	0.8200 s / batch. (data: 3.17e-04). ETA=9:39:12, max mem: 20.9 GB 
[11/25 17:04:46 visual_prompt]: 	Training 300/553. train loss: 4.3296,	1.0573 s / batch. (data: 2.10e-01). ETA=12:25:04, max mem: 20.9 GB 
[11/25 17:06:28 visual_prompt]: 	Training 400/553. train loss: 7.3109,	0.8325 s / batch. (data: 1.05e-02). ETA=9:45:16, max mem: 20.9 GB 
[11/25 17:08:10 visual_prompt]: 	Training 500/553. train loss: 12.1950,	0.8388 s / batch. (data: 2.57e-02). ETA=9:48:18, max mem: 20.9 GB 
[11/25 17:09:03 visual_prompt]: Epoch 24 / 100: avg data time: 1.94e-01, avg batch time: 1.0178, average train loss: 15.6678
[11/25 17:10:01 visual_prompt]: Inference (val):avg data time: 2.72e-04, avg batch time: 0.3077, average loss: 12.2164
[11/25 17:10:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.22	
[11/25 17:10:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/25 17:11:50 visual_prompt]: 	Training 100/553. train loss: 1.2045,	0.8360 s / batch. (data: 5.46e-03). ETA=9:44:12, max mem: 20.9 GB 
[11/25 17:13:28 visual_prompt]: 	Training 200/553. train loss: 7.4703,	0.9680 s / batch. (data: 1.44e-01). ETA=11:14:49, max mem: 20.9 GB 
[11/25 17:15:08 visual_prompt]: 	Training 300/553. train loss: 4.8980,	0.8880 s / batch. (data: 5.01e-02). ETA=10:17:35, max mem: 20.9 GB 
[11/25 17:16:50 visual_prompt]: 	Training 400/553. train loss: 1.9689,	1.2757 s / batch. (data: 4.64e-01). ETA=14:45:02, max mem: 20.9 GB 
[11/25 17:18:31 visual_prompt]: 	Training 500/553. train loss: 35.4260,	1.6240 s / batch. (data: 8.01e-01). ETA=18:44:00, max mem: 20.9 GB 
[11/25 17:19:24 visual_prompt]: Epoch 25 / 100: avg data time: 1.93e-01, avg batch time: 1.0173, average train loss: 15.9382
[11/25 17:20:22 visual_prompt]: Inference (val):avg data time: 1.45e-04, avg batch time: 0.3076, average loss: 8.8091
[11/25 17:20:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.45	
[11/25 17:20:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/25 17:22:07 visual_prompt]: 	Training 100/553. train loss: 2.1147,	0.8513 s / batch. (data: 1.93e-02). ETA=9:47:03, max mem: 20.9 GB 
[11/25 17:23:50 visual_prompt]: 	Training 200/553. train loss: 17.9493,	1.8390 s / batch. (data: 1.02e+00). ETA=21:05:05, max mem: 20.9 GB 
[11/25 17:25:32 visual_prompt]: 	Training 300/553. train loss: 0.0001,	0.8314 s / batch. (data: 8.08e-04). ETA=9:30:31, max mem: 20.9 GB 
[11/25 17:27:12 visual_prompt]: 	Training 400/553. train loss: 16.3151,	0.8320 s / batch. (data: 3.07e-04). ETA=9:29:34, max mem: 20.9 GB 
[11/25 17:28:51 visual_prompt]: 	Training 500/553. train loss: 10.0856,	0.8120 s / batch. (data: 2.97e-04). ETA=9:14:31, max mem: 20.9 GB 
[11/25 17:29:44 visual_prompt]: Epoch 26 / 100: avg data time: 1.91e-01, avg batch time: 1.0155, average train loss: 12.4177
[11/25 17:30:41 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3076, average loss: 10.5732
[11/25 17:30:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.24	
[11/25 17:30:41 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/25 17:32:27 visual_prompt]: 	Training 100/553. train loss: 2.7791,	0.8209 s / batch. (data: 3.10e-04). ETA=9:18:29, max mem: 20.9 GB 
[11/25 17:34:08 visual_prompt]: 	Training 200/553. train loss: 93.5871,	1.4909 s / batch. (data: 6.63e-01). ETA=16:51:52, max mem: 20.9 GB 
[11/25 17:35:48 visual_prompt]: 	Training 300/553. train loss: 2.9835,	0.8400 s / batch. (data: 3.12e-04). ETA=9:28:41, max mem: 20.9 GB 
[11/25 17:37:31 visual_prompt]: 	Training 400/553. train loss: 11.9248,	0.8527 s / batch. (data: 1.10e-02). ETA=9:35:54, max mem: 20.9 GB 
[11/25 17:39:13 visual_prompt]: 	Training 500/553. train loss: 10.5463,	0.8332 s / batch. (data: 8.16e-04). ETA=9:21:18, max mem: 20.9 GB 
[11/25 17:40:04 visual_prompt]: Epoch 27 / 100: avg data time: 1.94e-01, avg batch time: 1.0172, average train loss: 14.5366
[11/25 17:41:02 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3073, average loss: 13.4303
[11/25 17:41:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.34	
[11/25 17:41:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[11/25 17:42:46 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8560 s / batch. (data: 4.80e-04). ETA=9:34:29, max mem: 20.9 GB 
[11/25 17:44:28 visual_prompt]: 	Training 200/553. train loss: 6.1713,	0.8095 s / batch. (data: 3.41e-04). ETA=9:01:55, max mem: 20.9 GB 
[11/25 17:46:10 visual_prompt]: 	Training 300/553. train loss: 3.1755,	1.2581 s / batch. (data: 4.49e-01). ETA=14:00:10, max mem: 20.9 GB 
[11/25 17:47:50 visual_prompt]: 	Training 400/553. train loss: 91.5033,	0.8320 s / batch. (data: 7.93e-03). ETA=9:14:14, max mem: 20.9 GB 
[11/25 17:49:30 visual_prompt]: 	Training 500/553. train loss: 14.5788,	0.8560 s / batch. (data: 2.69e-04). ETA=9:28:47, max mem: 20.9 GB 
[11/25 17:50:23 visual_prompt]: Epoch 28 / 100: avg data time: 1.89e-01, avg batch time: 1.0144, average train loss: 16.3554
[11/25 17:51:21 visual_prompt]: Inference (val):avg data time: 3.58e-04, avg batch time: 0.3056, average loss: 1.2334
[11/25 17:51:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.91	
[11/25 17:51:21 visual_prompt]: Best epoch 28: best metric: -1.233
[11/25 17:51:21 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[11/25 17:53:12 visual_prompt]: 	Training 100/553. train loss: 0.0007,	0.8093 s / batch. (data: 3.23e-04). ETA=8:55:42, max mem: 20.9 GB 
[11/25 17:54:52 visual_prompt]: 	Training 200/553. train loss: 12.2392,	1.8200 s / batch. (data: 1.00e+00). ETA=20:01:40, max mem: 20.9 GB 
[11/25 17:56:31 visual_prompt]: 	Training 300/553. train loss: 7.0682,	0.8200 s / batch. (data: 3.39e-04). ETA=9:00:02, max mem: 20.9 GB 
[11/25 17:58:09 visual_prompt]: 	Training 400/553. train loss: 2.5316,	1.3083 s / batch. (data: 4.80e-01). ETA=14:19:27, max mem: 20.9 GB 
[11/25 17:59:50 visual_prompt]: 	Training 500/553. train loss: 6.7069,	0.8373 s / batch. (data: 3.21e-04). ETA=9:08:37, max mem: 20.9 GB 
[11/25 18:00:42 visual_prompt]: Epoch 29 / 100: avg data time: 1.92e-01, avg batch time: 1.0157, average train loss: 16.1975
[11/25 18:01:41 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3046, average loss: 42.7478
[11/25 18:01:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.75	
[11/25 18:01:41 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[11/25 18:03:24 visual_prompt]: 	Training 100/553. train loss: 12.7678,	0.8160 s / batch. (data: 3.07e-04). ETA=8:52:36, max mem: 20.9 GB 
[11/25 18:05:06 visual_prompt]: 	Training 200/553. train loss: 37.2850,	0.8133 s / batch. (data: 3.41e-04). ETA=8:49:30, max mem: 20.9 GB 
[11/25 18:06:46 visual_prompt]: 	Training 300/553. train loss: 30.1160,	1.4688 s / batch. (data: 6.60e-01). ETA=15:53:50, max mem: 20.9 GB 
[11/25 18:08:28 visual_prompt]: 	Training 400/553. train loss: 19.1996,	1.0465 s / batch. (data: 2.32e-01). ETA=11:17:49, max mem: 20.9 GB 
[11/25 18:10:08 visual_prompt]: 	Training 500/553. train loss: 8.1708,	1.5177 s / batch. (data: 6.93e-01). ETA=16:20:30, max mem: 20.9 GB 
[11/25 18:11:03 visual_prompt]: Epoch 30 / 100: avg data time: 1.93e-01, avg batch time: 1.0170, average train loss: 15.8050
[11/25 18:12:01 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3069, average loss: 7.5050
[11/25 18:12:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.89	
[11/25 18:12:01 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[11/25 18:13:48 visual_prompt]: 	Training 100/553. train loss: 5.6367,	0.8276 s / batch. (data: 5.44e-03). ETA=8:52:35, max mem: 20.9 GB 
[11/25 18:15:31 visual_prompt]: 	Training 200/553. train loss: 6.6392,	0.8476 s / batch. (data: 4.76e-03). ETA=9:04:01, max mem: 20.9 GB 
[11/25 18:17:09 visual_prompt]: 	Training 300/553. train loss: 41.8236,	0.8138 s / batch. (data: 2.99e-04). ETA=8:40:59, max mem: 20.9 GB 
[11/25 18:18:49 visual_prompt]: 	Training 400/553. train loss: 0.9660,	0.8522 s / batch. (data: 3.22e-02). ETA=9:04:07, max mem: 20.9 GB 
[11/25 18:20:31 visual_prompt]: 	Training 500/553. train loss: 3.8787,	0.8120 s / batch. (data: 2.89e-04). ETA=8:37:07, max mem: 20.9 GB 
[11/25 18:21:22 visual_prompt]: Epoch 31 / 100: avg data time: 1.90e-01, avg batch time: 1.0138, average train loss: 13.4559
[11/25 18:22:20 visual_prompt]: Inference (val):avg data time: 5.60e-05, avg batch time: 0.3085, average loss: 26.2762
[11/25 18:22:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 45.88	
[11/25 18:22:20 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[11/25 18:24:07 visual_prompt]: 	Training 100/553. train loss: 0.9339,	0.8316 s / batch. (data: 3.14e-04). ETA=8:47:29, max mem: 20.9 GB 
[11/25 18:25:47 visual_prompt]: 	Training 200/553. train loss: 30.8756,	0.8474 s / batch. (data: 5.43e-03). ETA=8:56:04, max mem: 20.9 GB 
[11/25 18:27:32 visual_prompt]: 	Training 300/553. train loss: 6.8317,	0.8212 s / batch. (data: 1.05e-02). ETA=8:38:07, max mem: 20.9 GB 
[11/25 18:29:13 visual_prompt]: 	Training 400/553. train loss: 3.5445,	0.8399 s / batch. (data: 7.88e-03). ETA=8:48:33, max mem: 20.9 GB 
[11/25 18:30:51 visual_prompt]: 	Training 500/553. train loss: 9.5365,	0.8298 s / batch. (data: 3.15e-04). ETA=8:40:47, max mem: 20.9 GB 
[11/25 18:31:42 visual_prompt]: Epoch 32 / 100: avg data time: 1.91e-01, avg batch time: 1.0150, average train loss: 12.0881
[11/25 18:32:39 visual_prompt]: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3060, average loss: 9.6817
[11/25 18:32:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.85	
[11/25 18:32:39 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[11/25 18:34:23 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.1588 s / batch. (data: 3.48e-01). ETA=12:04:21, max mem: 20.9 GB 
[11/25 18:36:06 visual_prompt]: 	Training 200/553. train loss: 16.7587,	1.4354 s / batch. (data: 6.17e-01). ETA=14:54:48, max mem: 20.9 GB 
[11/25 18:37:46 visual_prompt]: 	Training 300/553. train loss: 19.7131,	0.8360 s / batch. (data: 3.13e-04). ETA=8:39:46, max mem: 20.9 GB 
[11/25 18:39:28 visual_prompt]: 	Training 400/553. train loss: 7.7959,	0.8075 s / batch. (data: 2.92e-04). ETA=8:20:43, max mem: 20.9 GB 
[11/25 18:41:07 visual_prompt]: 	Training 500/553. train loss: 2.3726,	0.8227 s / batch. (data: 3.11e-04). ETA=8:28:45, max mem: 20.9 GB 
[11/25 18:42:00 visual_prompt]: Epoch 33 / 100: avg data time: 1.90e-01, avg batch time: 1.0138, average train loss: 14.4954
[11/25 18:42:58 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3064, average loss: 1.2949
[11/25 18:42:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 40.83	
[11/25 18:42:58 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[11/25 18:44:44 visual_prompt]: 	Training 100/553. train loss: 21.6753,	0.8160 s / batch. (data: 3.50e-04). ETA=8:22:31, max mem: 20.9 GB 
[11/25 18:46:23 visual_prompt]: 	Training 200/553. train loss: 4.5809,	0.8109 s / batch. (data: 3.15e-04). ETA=8:18:01, max mem: 20.9 GB 
[11/25 18:48:03 visual_prompt]: 	Training 300/553. train loss: 2.4369,	0.8192 s / batch. (data: 3.45e-04). ETA=8:21:45, max mem: 20.9 GB 
[11/25 18:49:46 visual_prompt]: 	Training 400/553. train loss: 0.8394,	0.8360 s / batch. (data: 3.05e-04). ETA=8:30:40, max mem: 20.9 GB 
[11/25 18:51:27 visual_prompt]: 	Training 500/553. train loss: 5.2098,	1.5320 s / batch. (data: 6.85e-01). ETA=15:33:15, max mem: 20.9 GB 
[11/25 18:52:19 visual_prompt]: Epoch 34 / 100: avg data time: 1.90e-01, avg batch time: 1.0137, average train loss: 15.0269
[11/25 18:53:17 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3071, average loss: 12.5809
[11/25 18:53:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.34	
[11/25 18:53:17 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[11/25 18:55:03 visual_prompt]: 	Training 100/553. train loss: 1.5603,	0.8240 s / batch. (data: 3.28e-04). ETA=8:19:51, max mem: 20.9 GB 
[11/25 18:56:46 visual_prompt]: 	Training 200/553. train loss: 7.1569,	0.8394 s / batch. (data: 3.52e-04). ETA=8:27:48, max mem: 20.9 GB 
[11/25 18:58:25 visual_prompt]: 	Training 300/553. train loss: 9.4010,	0.8312 s / batch. (data: 2.90e-04). ETA=8:21:26, max mem: 20.9 GB 
[11/25 19:00:04 visual_prompt]: 	Training 400/553. train loss: 3.3447,	0.8240 s / batch. (data: 3.30e-04). ETA=8:15:45, max mem: 20.9 GB 
[11/25 19:01:44 visual_prompt]: 	Training 500/553. train loss: 5.2290,	1.2320 s / batch. (data: 3.94e-01). ETA=12:19:08, max mem: 20.9 GB 
[11/25 19:02:37 visual_prompt]: Epoch 35 / 100: avg data time: 1.90e-01, avg batch time: 1.0138, average train loss: 13.1977
[11/25 19:03:35 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3069, average loss: 1.6425
[11/25 19:03:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.10	
[11/25 19:03:35 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[11/25 19:05:19 visual_prompt]: 	Training 100/553. train loss: 0.5133,	0.8400 s / batch. (data: 3.49e-04). ETA=8:21:49, max mem: 20.9 GB 
[11/25 19:07:01 visual_prompt]: 	Training 200/553. train loss: 56.9221,	0.8320 s / batch. (data: 1.19e-02). ETA=8:15:40, max mem: 20.9 GB 
[11/25 19:08:43 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8469 s / batch. (data: 1.19e-02). ETA=8:23:09, max mem: 20.9 GB 
[11/25 19:10:24 visual_prompt]: 	Training 400/553. train loss: 10.9672,	0.8252 s / batch. (data: 3.03e-04). ETA=8:08:52, max mem: 20.9 GB 
[11/25 19:12:06 visual_prompt]: 	Training 500/553. train loss: 10.8781,	1.0595 s / batch. (data: 2.26e-01). ETA=10:25:55, max mem: 20.9 GB 
[11/25 19:12:55 visual_prompt]: Epoch 36 / 100: avg data time: 1.90e-01, avg batch time: 1.0129, average train loss: 11.3525
[11/25 19:13:53 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3079, average loss: 42.5914
[11/25 19:13:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.18	
[11/25 19:13:53 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[11/25 19:15:39 visual_prompt]: 	Training 100/553. train loss: 4.1352,	0.8245 s / batch. (data: 1.56e-02). ETA=8:04:59, max mem: 20.9 GB 
[11/25 19:17:19 visual_prompt]: 	Training 200/553. train loss: 6.0755,	0.8197 s / batch. (data: 3.03e-04). ETA=8:00:45, max mem: 20.9 GB 
[11/25 19:19:00 visual_prompt]: 	Training 300/553. train loss: 57.1658,	1.6127 s / batch. (data: 7.80e-01). ETA=15:43:12, max mem: 20.9 GB 
[11/25 19:20:43 visual_prompt]: 	Training 400/553. train loss: 5.1942,	2.0295 s / batch. (data: 1.19e+00). ETA=19:43:35, max mem: 20.9 GB 
[11/25 19:22:21 visual_prompt]: 	Training 500/553. train loss: 6.9139,	1.2760 s / batch. (data: 4.37e-01). ETA=12:22:03, max mem: 20.9 GB 
[11/25 19:23:15 visual_prompt]: Epoch 37 / 100: avg data time: 1.92e-01, avg batch time: 1.0149, average train loss: 14.5513
[11/25 19:24:13 visual_prompt]: Inference (val):avg data time: 2.48e-04, avg batch time: 0.3080, average loss: 26.6352
[11/25 19:24:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.11	
[11/25 19:24:13 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[11/25 19:25:56 visual_prompt]: 	Training 100/553. train loss: 18.7491,	0.9617 s / batch. (data: 1.39e-01). ETA=9:16:47, max mem: 20.9 GB 
[11/25 19:27:38 visual_prompt]: 	Training 200/553. train loss: 3.7498,	1.1480 s / batch. (data: 3.29e-01). ETA=11:02:46, max mem: 20.9 GB 
[11/25 19:29:20 visual_prompt]: 	Training 300/553. train loss: 22.9882,	0.8120 s / batch. (data: 3.00e-04). ETA=7:47:24, max mem: 20.9 GB 
[11/25 19:30:59 visual_prompt]: 	Training 400/553. train loss: 0.0000,	1.0043 s / batch. (data: 1.97e-01). ETA=9:36:27, max mem: 20.9 GB 
[11/25 19:32:43 visual_prompt]: 	Training 500/553. train loss: 12.5993,	0.8320 s / batch. (data: 2.99e-04). ETA=7:56:10, max mem: 20.9 GB 
[11/25 19:33:34 visual_prompt]: Epoch 38 / 100: avg data time: 1.91e-01, avg batch time: 1.0148, average train loss: 13.2105
[11/25 19:34:32 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3069, average loss: 1.1450
[11/25 19:34:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.30	
[11/25 19:34:32 visual_prompt]: Best epoch 38: best metric: -1.145
[11/25 19:34:32 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[11/25 19:36:15 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8101 s / batch. (data: 3.00e-04). ETA=7:41:33, max mem: 20.9 GB 
[11/25 19:38:01 visual_prompt]: 	Training 200/553. train loss: 7.1017,	0.8280 s / batch. (data: 3.39e-04). ETA=7:50:22, max mem: 20.9 GB 
[11/25 19:39:45 visual_prompt]: 	Training 300/553. train loss: 19.4099,	0.8081 s / batch. (data: 2.69e-04). ETA=7:37:44, max mem: 20.9 GB 
[11/25 19:41:23 visual_prompt]: 	Training 400/553. train loss: 25.4852,	0.8400 s / batch. (data: 3.40e-04). ETA=7:54:22, max mem: 20.9 GB 
[11/25 19:43:05 visual_prompt]: 	Training 500/553. train loss: 7.9714,	1.7240 s / batch. (data: 9.16e-01). ETA=16:10:45, max mem: 20.9 GB 
[11/25 19:43:55 visual_prompt]: Epoch 39 / 100: avg data time: 1.93e-01, avg batch time: 1.0180, average train loss: 11.1693
[11/25 19:44:53 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3066, average loss: 11.1331
[11/25 19:44:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.75	
[11/25 19:44:53 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[11/25 19:46:38 visual_prompt]: 	Training 100/553. train loss: 12.8464,	0.8365 s / batch. (data: 4.59e-04). ETA=7:48:53, max mem: 20.9 GB 
[11/25 19:48:18 visual_prompt]: 	Training 200/553. train loss: 8.2352,	0.8072 s / batch. (data: 3.04e-04). ETA=7:31:09, max mem: 20.9 GB 
[11/25 19:50:01 visual_prompt]: 	Training 300/553. train loss: 8.4137,	0.8449 s / batch. (data: 8.28e-04). ETA=7:50:47, max mem: 20.9 GB 
[11/25 19:51:42 visual_prompt]: 	Training 400/553. train loss: 3.6512,	0.8080 s / batch. (data: 3.15e-04). ETA=7:28:52, max mem: 20.9 GB 
[11/25 19:53:22 visual_prompt]: 	Training 500/553. train loss: 17.7791,	0.8328 s / batch. (data: 2.91e-04). ETA=7:41:16, max mem: 20.9 GB 
[11/25 19:54:16 visual_prompt]: Epoch 40 / 100: avg data time: 1.94e-01, avg batch time: 1.0178, average train loss: 12.2651
[11/25 19:55:14 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3079, average loss: 7.9841
[11/25 19:55:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.35	
[11/25 19:55:14 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[11/25 19:57:04 visual_prompt]: 	Training 100/553. train loss: 4.6326,	0.8491 s / batch. (data: 3.84e-04). ETA=7:48:06, max mem: 20.9 GB 
[11/25 19:58:47 visual_prompt]: 	Training 200/553. train loss: 11.2113,	0.8212 s / batch. (data: 1.10e-03). ETA=7:31:22, max mem: 20.9 GB 
[11/25 20:00:27 visual_prompt]: 	Training 300/553. train loss: 74.7709,	0.8550 s / batch. (data: 3.14e-04). ETA=7:48:31, max mem: 20.9 GB 
[11/25 20:02:07 visual_prompt]: 	Training 400/553. train loss: 10.3165,	1.2204 s / batch. (data: 3.90e-01). ETA=11:06:45, max mem: 20.9 GB 
[11/25 20:03:44 visual_prompt]: 	Training 500/553. train loss: 5.7795,	0.8280 s / batch. (data: 3.11e-04). ETA=7:30:58, max mem: 20.9 GB 
[11/25 20:04:36 visual_prompt]: Epoch 41 / 100: avg data time: 1.94e-01, avg batch time: 1.0177, average train loss: 15.6686
[11/25 20:05:34 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3081, average loss: 1.5621
[11/25 20:05:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.75	
[11/25 20:05:34 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[11/25 20:07:18 visual_prompt]: 	Training 100/553. train loss: 40.2209,	0.8186 s / batch. (data: 7.93e-03). ETA=7:23:47, max mem: 20.9 GB 
[11/25 20:08:58 visual_prompt]: 	Training 200/553. train loss: 75.6881,	0.9013 s / batch. (data: 8.64e-02). ETA=8:07:07, max mem: 20.9 GB 
[11/25 20:10:40 visual_prompt]: 	Training 300/553. train loss: 19.6024,	0.8175 s / batch. (data: 3.17e-04). ETA=7:20:27, max mem: 20.9 GB 
[11/25 20:12:21 visual_prompt]: 	Training 400/553. train loss: 5.8587,	0.8098 s / batch. (data: 3.28e-04). ETA=7:14:56, max mem: 20.9 GB 
[11/25 20:14:01 visual_prompt]: 	Training 500/553. train loss: 104.2383,	0.8409 s / batch. (data: 1.94e-02). ETA=7:30:14, max mem: 20.9 GB 
[11/25 20:14:55 visual_prompt]: Epoch 42 / 100: avg data time: 1.90e-01, avg batch time: 1.0131, average train loss: 14.3884
[11/25 20:15:53 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3076, average loss: 36.3718
[11/25 20:15:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.19	
[11/25 20:15:53 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[11/25 20:17:39 visual_prompt]: 	Training 100/553. train loss: 4.2328,	0.8400 s / batch. (data: 3.24e-04). ETA=7:27:37, max mem: 20.9 GB 
[11/25 20:19:20 visual_prompt]: 	Training 200/553. train loss: 17.3884,	0.8292 s / batch. (data: 5.22e-03). ETA=7:20:29, max mem: 20.9 GB 
[11/25 20:20:58 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8152 s / batch. (data: 2.89e-04). ETA=7:11:42, max mem: 20.9 GB 
[11/25 20:22:38 visual_prompt]: 	Training 400/553. train loss: 3.4249,	0.8211 s / batch. (data: 2.96e-04). ETA=7:13:28, max mem: 20.9 GB 
[11/25 20:24:21 visual_prompt]: 	Training 500/553. train loss: 29.2855,	0.8189 s / batch. (data: 5.48e-03). ETA=7:10:55, max mem: 20.9 GB 
[11/25 20:25:14 visual_prompt]: Epoch 43 / 100: avg data time: 1.89e-01, avg batch time: 1.0141, average train loss: 12.4036
[11/25 20:26:11 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3081, average loss: 24.4953
[11/25 20:26:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.87	
[11/25 20:26:11 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[11/25 20:27:57 visual_prompt]: 	Training 100/553. train loss: 0.9345,	2.0480 s / batch. (data: 1.22e+00). ETA=17:52:30, max mem: 20.9 GB 
[11/25 20:29:40 visual_prompt]: 	Training 200/553. train loss: 2.4608,	0.8361 s / batch. (data: 2.81e-04). ETA=7:16:26, max mem: 20.9 GB 
[11/25 20:31:19 visual_prompt]: 	Training 300/553. train loss: 8.1179,	0.8113 s / batch. (data: 3.35e-04). ETA=7:02:09, max mem: 20.9 GB 
[11/25 20:32:59 visual_prompt]: 	Training 400/553. train loss: 0.3930,	0.8206 s / batch. (data: 7.94e-03). ETA=7:05:36, max mem: 20.9 GB 
[11/25 20:34:39 visual_prompt]: 	Training 500/553. train loss: 7.7799,	0.8400 s / batch. (data: 5.45e-03). ETA=7:14:17, max mem: 20.9 GB 
[11/25 20:35:33 visual_prompt]: Epoch 44 / 100: avg data time: 1.91e-01, avg batch time: 1.0148, average train loss: 12.4275
[11/25 20:36:30 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3082, average loss: 14.9444
[11/25 20:36:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.96	
[11/25 20:36:30 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[11/25 20:38:16 visual_prompt]: 	Training 100/553. train loss: 7.5158,	0.8360 s / batch. (data: 1.19e-02). ETA=7:10:04, max mem: 20.9 GB 
[11/25 20:39:53 visual_prompt]: 	Training 200/553. train loss: 9.6313,	0.8421 s / batch. (data: 2.21e-02). ETA=7:11:49, max mem: 20.9 GB 
[11/25 20:41:36 visual_prompt]: 	Training 300/553. train loss: 2.9258,	0.8188 s / batch. (data: 4.39e-04). ETA=6:58:32, max mem: 20.9 GB 
[11/25 20:43:14 visual_prompt]: 	Training 400/553. train loss: 4.2912,	0.8283 s / batch. (data: 8.25e-03). ETA=7:02:00, max mem: 20.9 GB 
[11/25 20:45:02 visual_prompt]: 	Training 500/553. train loss: 0.7238,	0.8440 s / batch. (data: 5.43e-03). ETA=7:08:35, max mem: 20.9 GB 
[11/25 20:45:54 visual_prompt]: Epoch 45 / 100: avg data time: 1.95e-01, avg batch time: 1.0185, average train loss: 9.5140
[11/25 20:46:52 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3081, average loss: 18.0233
[11/25 20:46:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.83	
[11/25 20:46:52 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[11/25 20:48:38 visual_prompt]: 	Training 100/553. train loss: 17.1797,	0.8197 s / batch. (data: 1.19e-02). ETA=6:54:07, max mem: 20.9 GB 
[11/25 20:50:20 visual_prompt]: 	Training 200/553. train loss: 8.9671,	0.8284 s / batch. (data: 1.05e-02). ETA=6:57:09, max mem: 20.9 GB 
[11/25 20:52:00 visual_prompt]: 	Training 300/553. train loss: 12.5220,	1.2334 s / batch. (data: 4.05e-01). ETA=10:19:04, max mem: 20.9 GB 
[11/25 20:53:41 visual_prompt]: 	Training 400/553. train loss: 1.9691,	0.8280 s / batch. (data: 3.11e-04). ETA=6:54:12, max mem: 20.9 GB 
[11/25 20:55:19 visual_prompt]: 	Training 500/553. train loss: 40.2306,	0.8559 s / batch. (data: 1.19e-02). ETA=7:06:45, max mem: 20.9 GB 
[11/25 20:56:13 visual_prompt]: Epoch 46 / 100: avg data time: 1.89e-01, avg batch time: 1.0140, average train loss: 8.7812
[11/25 20:57:11 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3076, average loss: 6.7670
[11/25 20:57:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.64	
[11/25 20:57:11 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[11/25 20:58:57 visual_prompt]: 	Training 100/553. train loss: 6.5043,	0.8160 s / batch. (data: 3.16e-04). ETA=6:44:46, max mem: 20.9 GB 
[11/25 21:00:34 visual_prompt]: 	Training 200/553. train loss: 19.0623,	0.9098 s / batch. (data: 9.68e-02). ETA=7:29:45, max mem: 20.9 GB 
[11/25 21:02:17 visual_prompt]: 	Training 300/553. train loss: 2.0360,	0.8436 s / batch. (data: 7.68e-04). ETA=6:55:39, max mem: 20.9 GB 
[11/25 21:03:58 visual_prompt]: 	Training 400/553. train loss: 17.8990,	0.8280 s / batch. (data: 2.99e-04). ETA=6:46:34, max mem: 20.9 GB 
[11/25 21:05:38 visual_prompt]: 	Training 500/553. train loss: 15.1629,	0.8546 s / batch. (data: 2.92e-04). ETA=6:58:14, max mem: 20.9 GB 
[11/25 21:06:32 visual_prompt]: Epoch 47 / 100: avg data time: 1.90e-01, avg batch time: 1.0156, average train loss: 10.7019
[11/25 21:07:30 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3079, average loss: 13.3165
[11/25 21:07:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.68	
[11/25 21:07:30 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[11/25 21:09:16 visual_prompt]: 	Training 100/553. train loss: 0.7990,	0.8280 s / batch. (data: 3.21e-04). ETA=6:43:05, max mem: 20.9 GB 
[11/25 21:10:57 visual_prompt]: 	Training 200/553. train loss: 3.6159,	0.8200 s / batch. (data: 3.29e-04). ETA=6:37:48, max mem: 20.9 GB 
[11/25 21:12:39 visual_prompt]: 	Training 300/553. train loss: 23.6609,	1.3646 s / batch. (data: 5.58e-01). ETA=10:59:46, max mem: 20.9 GB 
[11/25 21:14:17 visual_prompt]: 	Training 400/553. train loss: 16.5899,	0.8186 s / batch. (data: 3.30e-04). ETA=6:34:24, max mem: 20.9 GB 
[11/25 21:15:58 visual_prompt]: 	Training 500/553. train loss: 6.4613,	0.8175 s / batch. (data: 3.02e-04). ETA=6:32:31, max mem: 20.9 GB 
[11/25 21:16:51 visual_prompt]: Epoch 48 / 100: avg data time: 1.89e-01, avg batch time: 1.0136, average train loss: 8.3151
[11/25 21:17:49 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3074, average loss: 10.6992
[11/25 21:17:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.03	
[11/25 21:17:49 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[11/25 21:19:33 visual_prompt]: 	Training 100/553. train loss: 7.3680,	0.8134 s / batch. (data: 5.42e-03). ETA=6:28:28, max mem: 20.9 GB 
[11/25 21:21:13 visual_prompt]: 	Training 200/553. train loss: 4.8656,	0.8198 s / batch. (data: 5.42e-03). ETA=6:30:09, max mem: 20.9 GB 
[11/25 21:22:54 visual_prompt]: 	Training 300/553. train loss: 0.0002,	0.8189 s / batch. (data: 2.98e-04). ETA=6:28:21, max mem: 20.9 GB 
[11/25 21:24:37 visual_prompt]: 	Training 400/553. train loss: 1.1833,	0.8413 s / batch. (data: 2.97e-04). ETA=6:37:34, max mem: 20.9 GB 
[11/25 21:26:19 visual_prompt]: 	Training 500/553. train loss: 1.8481,	0.8360 s / batch. (data: 3.58e-04). ETA=6:33:41, max mem: 20.9 GB 
[11/25 21:27:11 visual_prompt]: Epoch 49 / 100: avg data time: 1.93e-01, avg batch time: 1.0172, average train loss: 7.7641
[11/25 21:28:09 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3079, average loss: 4.4623
[11/25 21:28:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 42.18	
[11/25 21:28:09 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[11/25 21:29:56 visual_prompt]: 	Training 100/553. train loss: 11.2692,	0.8330 s / batch. (data: 1.05e-02). ETA=6:30:11, max mem: 20.9 GB 
[11/25 21:31:37 visual_prompt]: 	Training 200/553. train loss: 6.6445,	0.8336 s / batch. (data: 1.36e-02). ETA=6:29:04, max mem: 20.9 GB 
[11/25 21:33:17 visual_prompt]: 	Training 300/553. train loss: 16.0209,	0.8216 s / batch. (data: 3.22e-04). ETA=6:22:05, max mem: 20.9 GB 
[11/25 21:34:55 visual_prompt]: 	Training 400/553. train loss: 44.0007,	0.8216 s / batch. (data: 2.88e-04). ETA=6:20:43, max mem: 20.9 GB 
[11/25 21:36:38 visual_prompt]: 	Training 500/553. train loss: 2.4115,	0.8080 s / batch. (data: 2.97e-04). ETA=6:13:03, max mem: 20.9 GB 
[11/25 21:37:30 visual_prompt]: Epoch 50 / 100: avg data time: 1.89e-01, avg batch time: 1.0129, average train loss: 9.6166
[11/25 21:38:27 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3065, average loss: 7.1900
[11/25 21:38:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.40	
[11/25 21:38:27 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[11/25 21:40:13 visual_prompt]: 	Training 100/553. train loss: 0.7930,	1.1598 s / batch. (data: 3.52e-01). ETA=8:52:32, max mem: 20.9 GB 
[11/25 21:41:54 visual_prompt]: 	Training 200/553. train loss: 1.1470,	0.8230 s / batch. (data: 2.90e-04). ETA=6:16:30, max mem: 20.9 GB 
[11/25 21:43:36 visual_prompt]: 	Training 300/553. train loss: 9.3558,	0.8089 s / batch. (data: 3.35e-04). ETA=6:08:42, max mem: 20.9 GB 
[11/25 21:45:18 visual_prompt]: 	Training 400/553. train loss: 2.7052,	1.5870 s / batch. (data: 7.68e-01). ETA=12:00:45, max mem: 20.9 GB 
[11/25 21:46:58 visual_prompt]: 	Training 500/553. train loss: 3.3948,	0.8400 s / batch. (data: 2.94e-04). ETA=6:20:05, max mem: 20.9 GB 
[11/25 21:47:50 visual_prompt]: Epoch 51 / 100: avg data time: 1.92e-01, avg batch time: 1.0164, average train loss: 8.6294
[11/25 21:48:47 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3062, average loss: 9.7046
[11/25 21:48:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.04	
[11/25 21:48:47 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[11/25 21:50:36 visual_prompt]: 	Training 100/553. train loss: 5.6225,	0.8370 s / batch. (data: 1.10e-02). ETA=6:16:35, max mem: 20.9 GB 
[11/25 21:52:16 visual_prompt]: 	Training 200/553. train loss: 2.4858,	0.8224 s / batch. (data: 3.03e-04). ETA=6:08:39, max mem: 20.9 GB 
[11/25 21:53:58 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8241 s / batch. (data: 3.24e-04). ETA=6:08:04, max mem: 20.9 GB 
[11/25 21:55:41 visual_prompt]: 	Training 400/553. train loss: 0.0000,	0.8368 s / batch. (data: 1.56e-02). ETA=6:12:21, max mem: 20.9 GB 
[11/25 21:57:17 visual_prompt]: 	Training 500/553. train loss: 4.0076,	0.8320 s / batch. (data: 3.07e-04). ETA=6:08:49, max mem: 20.9 GB 
[11/25 21:58:09 visual_prompt]: Epoch 52 / 100: avg data time: 1.92e-01, avg batch time: 1.0154, average train loss: 9.5048
[11/25 21:59:07 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3059, average loss: 33.6538
[11/25 21:59:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.28	
[11/25 21:59:07 visual_prompt]: Stopping early.
[11/25 21:59:07 visual_prompt]: Rank of current process: 0. World size: 1
[11/25 21:59:07 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/25 21:59:07 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/25 21:59:07 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/25 21:59:07 visual_prompt]: Training with config:
[11/25 21:59:07 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/25 21:59:07 visual_prompt]: Loading training data...
[11/25 21:59:07 visual_prompt]: Constructing mammo-cbis dataset train...
[11/25 21:59:07 visual_prompt]: Loading validation data...
[11/25 21:59:07 visual_prompt]: Constructing mammo-cbis dataset val...
[11/25 21:59:07 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/25 21:59:10 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/25 21:59:10 visual_prompt]: tuned percent:0.525
[11/25 21:59:10 visual_prompt]: Device used for model: 0
[11/25 21:59:10 visual_prompt]: Setting up Evaluator...
[11/25 21:59:10 visual_prompt]: Setting up Trainer...
[11/25 21:59:10 visual_prompt]: 	Setting up the optimizer...
[11/25 21:59:10 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/25 22:00:55 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8366 s / batch. (data: 3.13e-04). ETA=12:49:39, max mem: 20.9 GB 
[11/25 22:02:34 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8250 s / batch. (data: 1.64e-02). ETA=12:37:36, max mem: 20.9 GB 
[11/25 22:04:17 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3388 s / batch. (data: 5.11e-01). ETA=20:27:12, max mem: 20.9 GB 
[11/25 22:05:57 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8076 s / batch. (data: 3.09e-04). ETA=12:18:54, max mem: 20.9 GB 
[11/25 22:07:40 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8200 s / batch. (data: 7.85e-04). ETA=12:28:57, max mem: 20.9 GB 
[11/25 22:08:33 visual_prompt]: Epoch 1 / 100: avg data time: 1.96e-01, avg batch time: 1.0192, average train loss: 1.5403
[11/25 22:09:31 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3068, average loss: 1.5201
[11/25 22:09:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/25 22:09:31 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/25 22:11:16 visual_prompt]: 	Training 100/553. train loss: 3.5035,	1.2048 s / batch. (data: 3.83e-01). ETA=18:17:19, max mem: 20.9 GB 
[11/25 22:12:57 visual_prompt]: 	Training 200/553. train loss: 0.0003,	0.8089 s / batch. (data: 3.47e-04). ETA=12:15:24, max mem: 20.9 GB 
[11/25 22:14:40 visual_prompt]: 	Training 300/553. train loss: 0.8492,	1.2470 s / batch. (data: 4.34e-01). ETA=18:51:37, max mem: 20.9 GB 
[11/25 22:16:20 visual_prompt]: 	Training 400/553. train loss: 4.5294,	0.8280 s / batch. (data: 2.94e-04). ETA=12:29:59, max mem: 20.9 GB 
[11/25 22:18:03 visual_prompt]: 	Training 500/553. train loss: 0.4798,	0.8777 s / batch. (data: 5.92e-03). ETA=13:13:32, max mem: 20.9 GB 
[11/25 22:18:54 visual_prompt]: Epoch 2 / 100: avg data time: 1.95e-01, avg batch time: 1.0177, average train loss: 2.0718
[11/25 22:19:52 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.3079, average loss: 1.9715
[11/25 22:19:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.22	
[11/25 22:19:52 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/25 22:21:36 visual_prompt]: 	Training 100/553. train loss: 0.6295,	0.8136 s / batch. (data: 5.46e-03). ETA=12:13:33, max mem: 20.9 GB 
[11/25 22:23:18 visual_prompt]: 	Training 200/553. train loss: 0.9610,	1.4448 s / batch. (data: 6.38e-01). ETA=21:40:08, max mem: 20.9 GB 
[11/25 22:24:58 visual_prompt]: 	Training 300/553. train loss: 2.4749,	0.8200 s / batch. (data: 3.24e-04). ETA=12:16:32, max mem: 20.9 GB 
[11/25 22:26:40 visual_prompt]: 	Training 400/553. train loss: 0.2250,	0.8562 s / batch. (data: 1.64e-02). ETA=12:47:39, max mem: 20.9 GB 
[11/25 22:28:23 visual_prompt]: 	Training 500/553. train loss: 1.0738,	1.4082 s / batch. (data: 5.79e-01). ETA=21:00:09, max mem: 20.9 GB 
[11/25 22:29:14 visual_prompt]: Epoch 3 / 100: avg data time: 1.93e-01, avg batch time: 1.0163, average train loss: 2.3930
[11/25 22:30:12 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3069, average loss: 2.9331
[11/25 22:30:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.74	
[11/25 22:30:12 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/25 22:31:59 visual_prompt]: 	Training 100/553. train loss: 2.0854,	0.8368 s / batch. (data: 6.35e-03). ETA=12:26:43, max mem: 20.9 GB 
[11/25 22:33:40 visual_prompt]: 	Training 200/553. train loss: 1.2467,	0.8560 s / batch. (data: 3.11e-04). ETA=12:42:24, max mem: 20.9 GB 
[11/25 22:35:22 visual_prompt]: 	Training 300/553. train loss: 0.7352,	1.6720 s / batch. (data: 8.49e-01). ETA=1 day, 0:46:26, max mem: 20.9 GB 
[11/25 22:36:59 visual_prompt]: 	Training 400/553. train loss: 3.3312,	0.8293 s / batch. (data: 3.39e-04). ETA=12:15:54, max mem: 20.9 GB 
[11/25 22:38:42 visual_prompt]: 	Training 500/553. train loss: 13.1551,	3.4755 s / batch. (data: 2.66e+00). ETA=2 days, 3:18:12, max mem: 20.9 GB 
[11/25 22:39:36 visual_prompt]: Epoch 4 / 100: avg data time: 1.95e-01, avg batch time: 1.0186, average train loss: 3.2573
[11/25 22:40:34 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3059, average loss: 3.1964
[11/25 22:40:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.71	
[11/25 22:40:34 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/25 22:42:17 visual_prompt]: 	Training 100/553. train loss: 4.0519,	0.8400 s / batch. (data: 2.91e-04). ETA=12:21:48, max mem: 20.9 GB 
[11/25 22:43:58 visual_prompt]: 	Training 200/553. train loss: 8.2089,	0.9480 s / batch. (data: 1.06e-01). ETA=13:55:36, max mem: 20.9 GB 
[11/25 22:45:41 visual_prompt]: 	Training 300/553. train loss: 2.9179,	0.8534 s / batch. (data: 1.05e-02). ETA=12:30:50, max mem: 20.9 GB 
[11/25 22:47:21 visual_prompt]: 	Training 400/553. train loss: 3.8703,	0.8298 s / batch. (data: 5.42e-03). ETA=12:08:40, max mem: 20.9 GB 
[11/25 22:49:03 visual_prompt]: 	Training 500/553. train loss: 9.9950,	0.8295 s / batch. (data: 2.99e-04). ETA=12:07:01, max mem: 20.9 GB 
[11/25 22:49:56 visual_prompt]: Epoch 5 / 100: avg data time: 1.92e-01, avg batch time: 1.0166, average train loss: 6.9270
[11/25 22:50:54 visual_prompt]: Inference (val):avg data time: 3.07e-04, avg batch time: 0.3074, average loss: 4.2445
[11/25 22:50:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.30	
[11/25 22:50:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/25 22:52:41 visual_prompt]: 	Training 100/553. train loss: 4.6621,	0.8320 s / batch. (data: 3.05e-04). ETA=12:07:06, max mem: 20.9 GB 
[11/25 22:54:22 visual_prompt]: 	Training 200/553. train loss: 21.5458,	0.8292 s / batch. (data: 3.22e-04). ETA=12:03:15, max mem: 20.9 GB 
[11/25 22:56:01 visual_prompt]: 	Training 300/553. train loss: 1.0072,	0.8267 s / batch. (data: 5.43e-03). ETA=11:59:43, max mem: 20.9 GB 
[11/25 22:57:46 visual_prompt]: 	Training 400/553. train loss: 11.3812,	0.8594 s / batch. (data: 2.74e-02). ETA=12:26:44, max mem: 20.9 GB 
[11/25 22:59:24 visual_prompt]: 	Training 500/553. train loss: 8.4550,	0.8216 s / batch. (data: 3.29e-04). ETA=11:52:33, max mem: 20.9 GB 
[11/25 23:00:16 visual_prompt]: Epoch 6 / 100: avg data time: 1.91e-01, avg batch time: 1.0145, average train loss: 6.8165
[11/25 23:01:13 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3067, average loss: 2.8051
[11/25 23:01:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.58	
[11/25 23:01:13 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/25 23:02:56 visual_prompt]: 	Training 100/553. train loss: 6.3153,	0.8509 s / batch. (data: 1.09e-02). ETA=12:15:45, max mem: 20.9 GB 
[11/25 23:04:36 visual_prompt]: 	Training 200/553. train loss: 0.6921,	0.8333 s / batch. (data: 3.15e-04). ETA=11:59:12, max mem: 20.9 GB 
[11/25 23:06:20 visual_prompt]: 	Training 300/553. train loss: 3.9191,	1.9567 s / batch. (data: 1.14e+00). ETA=1 day, 4:05:27, max mem: 20.9 GB 
[11/25 23:08:01 visual_prompt]: 	Training 400/553. train loss: 2.8755,	1.8038 s / batch. (data: 9.76e-01). ETA=1 day, 1:50:42, max mem: 20.9 GB 
[11/25 23:09:40 visual_prompt]: 	Training 500/553. train loss: 3.8488,	0.8152 s / batch. (data: 3.05e-04). ETA=11:39:29, max mem: 20.9 GB 
[11/25 23:10:30 visual_prompt]: Epoch 7 / 100: avg data time: 1.83e-01, avg batch time: 1.0071, average train loss: 4.8161
[11/25 23:11:28 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3064, average loss: 4.5820
[11/25 23:11:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.39	
[11/25 23:11:28 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/25 23:13:10 visual_prompt]: 	Training 100/553. train loss: 3.1925,	0.8242 s / batch. (data: 3.43e-04). ETA=11:45:07, max mem: 20.9 GB 
[11/25 23:14:52 visual_prompt]: 	Training 200/553. train loss: 21.3880,	0.8137 s / batch. (data: 5.44e-03). ETA=11:34:44, max mem: 20.9 GB 
[11/25 23:16:33 visual_prompt]: 	Training 300/553. train loss: 1.0200,	0.8429 s / batch. (data: 1.19e-02). ETA=11:58:17, max mem: 20.9 GB 
[11/25 23:18:13 visual_prompt]: 	Training 400/553. train loss: 13.3662,	0.9880 s / batch. (data: 1.56e-01). ETA=14:00:16, max mem: 20.9 GB 
[11/25 23:19:54 visual_prompt]: 	Training 500/553. train loss: 0.2706,	1.5951 s / batch. (data: 7.88e-01). ETA=22:33:55, max mem: 20.9 GB 
[11/25 23:20:47 visual_prompt]: Epoch 8 / 100: avg data time: 1.86e-01, avg batch time: 1.0102, average train loss: 8.8876
[11/25 23:21:44 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3070, average loss: 1.0892
[11/25 23:21:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.81	
[11/25 23:21:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/25 23:23:28 visual_prompt]: 	Training 100/553. train loss: 0.0001,	0.8096 s / batch. (data: 5.07e-04). ETA=11:25:06, max mem: 20.9 GB 
[11/25 23:25:08 visual_prompt]: 	Training 200/553. train loss: 3.8978,	0.8360 s / batch. (data: 2.93e-04). ETA=11:46:04, max mem: 20.9 GB 
[11/25 23:26:49 visual_prompt]: 	Training 300/553. train loss: 3.0510,	1.7506 s / batch. (data: 9.12e-01). ETA=1 day, 0:35:40, max mem: 20.9 GB 
[11/25 23:28:31 visual_prompt]: 	Training 400/553. train loss: 1.7741,	0.8480 s / batch. (data: 7.86e-04). ETA=11:53:23, max mem: 20.9 GB 
[11/25 23:30:11 visual_prompt]: 	Training 500/553. train loss: 5.9535,	1.0419 s / batch. (data: 2.14e-01). ETA=14:34:46, max mem: 20.9 GB 
[11/25 23:31:02 visual_prompt]: Epoch 9 / 100: avg data time: 1.86e-01, avg batch time: 1.0084, average train loss: 7.6257
[11/25 23:31:59 visual_prompt]: Inference (val):avg data time: 1.73e-04, avg batch time: 0.3070, average loss: 15.7628
[11/25 23:31:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.76	
[11/25 23:31:59 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/25 23:33:46 visual_prompt]: 	Training 100/553. train loss: 19.4007,	0.8280 s / batch. (data: 3.22e-04). ETA=11:33:04, max mem: 20.9 GB 
[11/25 23:35:25 visual_prompt]: 	Training 200/553. train loss: 0.5049,	0.8240 s / batch. (data: 3.22e-04). ETA=11:28:22, max mem: 20.9 GB 
[11/25 23:37:04 visual_prompt]: 	Training 300/553. train loss: 6.9126,	1.7520 s / batch. (data: 9.26e-01). ETA=1 day, 0:20:40, max mem: 20.9 GB 
[11/25 23:38:42 visual_prompt]: 	Training 400/553. train loss: 13.4654,	0.8135 s / batch. (data: 3.19e-04). ETA=11:16:52, max mem: 20.9 GB 
[11/25 23:40:23 visual_prompt]: 	Training 500/553. train loss: 2.0865,	0.8360 s / batch. (data: 3.19e-04). ETA=11:34:11, max mem: 20.9 GB 
[11/25 23:41:16 visual_prompt]: Epoch 10 / 100: avg data time: 1.82e-01, avg batch time: 1.0060, average train loss: 10.7126
[11/25 23:42:13 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3067, average loss: 5.2410
[11/25 23:42:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.73	
[11/25 23:42:13 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/25 23:44:00 visual_prompt]: 	Training 100/553. train loss: 12.9361,	0.8160 s / batch. (data: 2.92e-04). ETA=11:15:30, max mem: 20.9 GB 
[11/25 23:45:41 visual_prompt]: 	Training 200/553. train loss: 5.3582,	0.8170 s / batch. (data: 3.15e-04). ETA=11:14:59, max mem: 20.9 GB 
[11/25 23:47:21 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1153 s / batch. (data: 1.30e+00). ETA=1 day, 5:04:04, max mem: 20.9 GB 
[11/25 23:48:59 visual_prompt]: 	Training 400/553. train loss: 7.4933,	0.8160 s / batch. (data: 3.01e-04). ETA=11:11:26, max mem: 20.9 GB 
[11/25 23:50:39 visual_prompt]: 	Training 500/553. train loss: 0.5588,	0.8320 s / batch. (data: 3.33e-04). ETA=11:23:12, max mem: 20.9 GB 
[11/25 23:51:30 visual_prompt]: Epoch 11 / 100: avg data time: 1.82e-01, avg batch time: 1.0068, average train loss: 8.1792
[11/25 23:52:28 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3056, average loss: 11.4596
[11/25 23:52:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.25	
[11/25 23:52:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/25 23:54:13 visual_prompt]: 	Training 100/553. train loss: 0.3006,	0.8280 s / batch. (data: 3.15e-04). ETA=11:17:47, max mem: 20.9 GB 
[11/25 23:55:55 visual_prompt]: 	Training 200/553. train loss: 5.7857,	0.8365 s / batch. (data: 3.16e-04). ETA=11:23:22, max mem: 20.9 GB 
[11/25 23:57:34 visual_prompt]: 	Training 300/553. train loss: 13.7108,	0.8069 s / batch. (data: 2.95e-04). ETA=10:57:51, max mem: 20.9 GB 
[11/25 23:59:15 visual_prompt]: 	Training 400/553. train loss: 19.6701,	0.8330 s / batch. (data: 8.99e-03). ETA=11:17:46, max mem: 20.9 GB 
[11/26 00:00:55 visual_prompt]: 	Training 500/553. train loss: 23.8542,	0.8280 s / batch. (data: 1.20e-02). ETA=11:12:17, max mem: 20.9 GB 
[11/26 00:01:47 visual_prompt]: Epoch 12 / 100: avg data time: 1.89e-01, avg batch time: 1.0112, average train loss: 15.1743
[11/26 00:02:44 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3062, average loss: 90.2101
[11/26 00:02:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.14	
[11/26 00:02:44 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/26 00:04:30 visual_prompt]: 	Training 100/553. train loss: 1.5743,	0.8400 s / batch. (data: 7.94e-03). ETA=11:19:54, max mem: 20.9 GB 
[11/26 00:06:07 visual_prompt]: 	Training 200/553. train loss: 3.3038,	0.8127 s / batch. (data: 2.91e-04). ETA=10:56:27, max mem: 20.9 GB 
[11/26 00:07:49 visual_prompt]: 	Training 300/553. train loss: 8.8213,	1.8077 s / batch. (data: 1.00e+00). ETA=1 day, 0:17:09, max mem: 20.9 GB 
[11/26 00:09:28 visual_prompt]: 	Training 400/553. train loss: 14.4938,	0.8466 s / batch. (data: 5.43e-03). ETA=11:20:58, max mem: 20.9 GB 
[11/26 00:11:09 visual_prompt]: 	Training 500/553. train loss: 3.1417,	0.8485 s / batch. (data: 3.71e-04). ETA=11:21:05, max mem: 20.9 GB 
[11/26 00:12:02 visual_prompt]: Epoch 13 / 100: avg data time: 1.85e-01, avg batch time: 1.0081, average train loss: 10.2507
[11/26 00:12:59 visual_prompt]: Inference (val):avg data time: 2.20e-04, avg batch time: 0.3073, average loss: 3.3825
[11/26 00:12:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.35	
[11/26 00:12:59 visual_prompt]: Best epoch 13: best metric: -3.383
[11/26 00:12:59 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/26 00:14:45 visual_prompt]: 	Training 100/553. train loss: 1.9361,	0.8266 s / batch. (data: 5.61e-03). ETA=11:01:24, max mem: 20.9 GB 
[11/26 00:16:26 visual_prompt]: 	Training 200/553. train loss: 0.0023,	1.4183 s / batch. (data: 5.77e-01). ETA=18:52:32, max mem: 20.9 GB 
[11/26 00:18:06 visual_prompt]: 	Training 300/553. train loss: 0.9843,	0.8487 s / batch. (data: 1.56e-02). ETA=11:16:19, max mem: 20.9 GB 
[11/26 00:19:46 visual_prompt]: 	Training 400/553. train loss: 11.0351,	0.8480 s / batch. (data: 1.19e-02). ETA=11:14:18, max mem: 20.9 GB 
[11/26 00:21:26 visual_prompt]: 	Training 500/553. train loss: 21.5263,	0.8541 s / batch. (data: 2.74e-02). ETA=11:17:46, max mem: 20.9 GB 
[11/26 00:22:18 visual_prompt]: Epoch 14 / 100: avg data time: 1.85e-01, avg batch time: 1.0093, average train loss: 9.0660
[11/26 00:23:15 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3043, average loss: 5.8439
[11/26 00:23:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.18	
[11/26 00:23:15 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/26 00:24:59 visual_prompt]: 	Training 100/553. train loss: 15.9422,	0.8400 s / batch. (data: 5.44e-03). ETA=11:04:24, max mem: 20.9 GB 
[11/26 00:26:38 visual_prompt]: 	Training 200/553. train loss: 24.2661,	0.8371 s / batch. (data: 1.20e-02). ETA=11:00:42, max mem: 20.9 GB 
[11/26 00:28:20 visual_prompt]: 	Training 300/553. train loss: 1.1437,	0.8561 s / batch. (data: 8.23e-04). ETA=11:14:19, max mem: 20.9 GB 
[11/26 00:29:58 visual_prompt]: 	Training 400/553. train loss: 1.0355,	0.8691 s / batch. (data: 2.11e-02). ETA=11:23:05, max mem: 20.9 GB 
[11/26 00:31:40 visual_prompt]: 	Training 500/553. train loss: 90.8781,	0.8142 s / batch. (data: 4.48e-04). ETA=10:38:35, max mem: 20.9 GB 
[11/26 00:32:33 visual_prompt]: Epoch 15 / 100: avg data time: 1.85e-01, avg batch time: 1.0083, average train loss: 17.6134
[11/26 00:33:30 visual_prompt]: Inference (val):avg data time: 1.90e-04, avg batch time: 0.3049, average loss: 23.9815
[11/26 00:33:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.29	
[11/26 00:33:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/26 00:35:14 visual_prompt]: 	Training 100/553. train loss: 25.2271,	0.8040 s / batch. (data: 2.76e-04). ETA=10:28:32, max mem: 20.9 GB 
[11/26 00:36:55 visual_prompt]: 	Training 200/553. train loss: 25.7270,	0.8373 s / batch. (data: 1.33e-02). ETA=10:53:11, max mem: 20.9 GB 
[11/26 00:38:35 visual_prompt]: 	Training 300/553. train loss: 57.3220,	0.8152 s / batch. (data: 2.89e-04). ETA=10:34:32, max mem: 20.9 GB 
[11/26 00:40:15 visual_prompt]: 	Training 400/553. train loss: 5.9560,	0.8385 s / batch. (data: 3.09e-04). ETA=10:51:16, max mem: 20.9 GB 
[11/26 00:41:55 visual_prompt]: 	Training 500/553. train loss: 8.8824,	1.1200 s / batch. (data: 2.77e-01). ETA=14:28:05, max mem: 20.9 GB 
[11/26 00:42:48 visual_prompt]: Epoch 16 / 100: avg data time: 1.84e-01, avg batch time: 1.0083, average train loss: 14.8550
[11/26 00:43:45 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3065, average loss: 13.6446
[11/26 00:43:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.17	
[11/26 00:43:45 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/26 00:45:28 visual_prompt]: 	Training 100/553. train loss: 26.1969,	0.8419 s / batch. (data: 1.05e-02). ETA=10:50:23, max mem: 20.9 GB 
[11/26 00:47:10 visual_prompt]: 	Training 200/553. train loss: 56.1493,	0.8200 s / batch. (data: 2.92e-04). ETA=10:32:06, max mem: 20.9 GB 
[11/26 00:48:50 visual_prompt]: 	Training 300/553. train loss: 1.6608,	0.8440 s / batch. (data: 2.84e-04). ETA=10:49:11, max mem: 20.9 GB 
[11/26 00:50:29 visual_prompt]: 	Training 400/553. train loss: 58.0613,	1.1725 s / batch. (data: 3.52e-01). ETA=14:59:55, max mem: 20.9 GB 
[11/26 00:52:10 visual_prompt]: 	Training 500/553. train loss: 10.7328,	1.8643 s / batch. (data: 1.05e+00). ETA=23:47:46, max mem: 20.9 GB 
[11/26 00:53:03 visual_prompt]: Epoch 17 / 100: avg data time: 1.85e-01, avg batch time: 1.0090, average train loss: 18.0756
[11/26 00:54:01 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3080, average loss: 18.8841
[11/26 00:54:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.52	
[11/26 00:54:01 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/26 00:55:45 visual_prompt]: 	Training 100/553. train loss: 27.9208,	0.8200 s / batch. (data: 3.79e-04). ETA=10:25:53, max mem: 20.9 GB 
[11/26 00:57:27 visual_prompt]: 	Training 200/553. train loss: 27.2477,	0.8277 s / batch. (data: 3.43e-04). ETA=10:30:26, max mem: 20.9 GB 
[11/26 00:59:08 visual_prompt]: 	Training 300/553. train loss: 2.7293,	0.8160 s / batch. (data: 3.03e-04). ETA=10:20:08, max mem: 20.9 GB 
[11/26 01:00:48 visual_prompt]: 	Training 400/553. train loss: 2.6912,	0.8200 s / batch. (data: 3.01e-04). ETA=10:21:49, max mem: 20.9 GB 
[11/26 01:02:27 visual_prompt]: 	Training 500/553. train loss: 39.4254,	0.8457 s / batch. (data: 2.07e-02). ETA=10:39:53, max mem: 20.9 GB 
[11/26 01:03:18 visual_prompt]: Epoch 18 / 100: avg data time: 1.85e-01, avg batch time: 1.0075, average train loss: 21.2450
[11/26 01:04:15 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3067, average loss: 20.1407
[11/26 01:04:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.29	
[11/26 01:04:15 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/26 01:05:59 visual_prompt]: 	Training 100/553. train loss: 1.8607,	1.0177 s / batch. (data: 2.10e-01). ETA=12:47:25, max mem: 20.9 GB 
[11/26 01:07:44 visual_prompt]: 	Training 200/553. train loss: 2.2904,	0.8295 s / batch. (data: 8.19e-04). ETA=10:24:08, max mem: 20.9 GB 
[11/26 01:09:23 visual_prompt]: 	Training 300/553. train loss: 7.7977,	0.8280 s / batch. (data: 7.94e-03). ETA=10:21:37, max mem: 20.9 GB 
[11/26 01:11:04 visual_prompt]: 	Training 400/553. train loss: 4.5759,	0.8338 s / batch. (data: 3.20e-04). ETA=10:24:34, max mem: 20.9 GB 
[11/26 01:12:41 visual_prompt]: 	Training 500/553. train loss: 25.7462,	0.8160 s / batch. (data: 3.18e-04). ETA=10:09:53, max mem: 20.9 GB 
[11/26 01:13:34 visual_prompt]: Epoch 19 / 100: avg data time: 1.86e-01, avg batch time: 1.0093, average train loss: 13.8323
[11/26 01:14:31 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3046, average loss: 33.4310
[11/26 01:14:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.05	
[11/26 01:14:31 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/26 01:16:14 visual_prompt]: 	Training 100/553. train loss: 5.0464,	0.8275 s / batch. (data: 5.30e-03). ETA=10:16:22, max mem: 20.9 GB 
[11/26 01:17:55 visual_prompt]: 	Training 200/553. train loss: 8.0982,	0.8240 s / batch. (data: 7.95e-03). ETA=10:12:25, max mem: 20.9 GB 
[11/26 01:19:35 visual_prompt]: 	Training 300/553. train loss: 6.0087,	0.8380 s / batch. (data: 2.94e-04). ETA=10:21:23, max mem: 20.9 GB 
[11/26 01:21:15 visual_prompt]: 	Training 400/553. train loss: 1.3213,	0.8315 s / batch. (data: 1.05e-02). ETA=10:15:11, max mem: 20.9 GB 
[11/26 01:22:54 visual_prompt]: 	Training 500/553. train loss: 14.1554,	0.8096 s / batch. (data: 2.93e-04). ETA=9:57:37, max mem: 20.9 GB 
[11/26 01:23:48 visual_prompt]: Epoch 20 / 100: avg data time: 1.83e-01, avg batch time: 1.0068, average train loss: 15.2412
[11/26 01:24:46 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3072, average loss: 41.8419
[11/26 01:24:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.45	
[11/26 01:24:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/26 01:26:32 visual_prompt]: 	Training 100/553. train loss: 1.8224,	0.8147 s / batch. (data: 5.47e-03). ETA=9:59:20, max mem: 20.9 GB 
[11/26 01:28:11 visual_prompt]: 	Training 200/553. train loss: 2.8603,	0.8320 s / batch. (data: 3.12e-04). ETA=10:10:40, max mem: 20.9 GB 
[11/26 01:29:51 visual_prompt]: 	Training 300/553. train loss: 6.2862,	1.0758 s / batch. (data: 2.54e-01). ETA=13:07:52, max mem: 20.9 GB 
[11/26 01:31:30 visual_prompt]: 	Training 400/553. train loss: 6.4808,	0.8419 s / batch. (data: 7.93e-03). ETA=10:15:10, max mem: 20.9 GB 
[11/26 01:33:11 visual_prompt]: 	Training 500/553. train loss: 13.3490,	0.8309 s / batch. (data: 5.43e-03). ETA=10:05:41, max mem: 20.9 GB 
[11/26 01:34:03 visual_prompt]: Epoch 21 / 100: avg data time: 1.84e-01, avg batch time: 1.0076, average train loss: 12.1980
[11/26 01:35:00 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3069, average loss: 3.1222
[11/26 01:35:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.84	
[11/26 01:35:00 visual_prompt]: Best epoch 21: best metric: -3.122
[11/26 01:35:00 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/26 01:36:43 visual_prompt]: 	Training 100/553. train loss: 0.8268,	0.8454 s / batch. (data: 1.05e-02). ETA=10:14:07, max mem: 20.9 GB 
[11/26 01:38:24 visual_prompt]: 	Training 200/553. train loss: 16.9778,	0.8200 s / batch. (data: 3.17e-04). ETA=9:54:19, max mem: 20.9 GB 
[11/26 01:40:03 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8290 s / batch. (data: 3.09e-04). ETA=9:59:28, max mem: 20.9 GB 
[11/26 01:41:43 visual_prompt]: 	Training 400/553. train loss: 1.0901,	0.8074 s / batch. (data: 2.91e-04). ETA=9:42:29, max mem: 20.9 GB 
[11/26 01:43:25 visual_prompt]: 	Training 500/553. train loss: 1.1772,	0.8104 s / batch. (data: 2.99e-04). ETA=9:43:19, max mem: 20.9 GB 
[11/26 01:44:19 visual_prompt]: Epoch 22 / 100: avg data time: 1.84e-01, avg batch time: 1.0091, average train loss: 7.3076
[11/26 01:45:16 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3065, average loss: 2.9483
[11/26 01:45:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.49	
[11/26 01:45:16 visual_prompt]: Best epoch 22: best metric: -2.948
[11/26 01:45:16 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/26 01:47:02 visual_prompt]: 	Training 100/553. train loss: 1.9362,	0.8200 s / batch. (data: 2.96e-04). ETA=9:48:07, max mem: 20.9 GB 
[11/26 01:48:43 visual_prompt]: 	Training 200/553. train loss: 60.2175,	0.8558 s / batch. (data: 3.35e-02). ETA=10:12:22, max mem: 20.9 GB 
[11/26 01:50:24 visual_prompt]: 	Training 300/553. train loss: 0.7793,	0.8613 s / batch. (data: 1.09e-02). ETA=10:14:53, max mem: 20.9 GB 
[11/26 01:52:03 visual_prompt]: 	Training 400/553. train loss: 8.3411,	0.8487 s / batch. (data: 8.27e-04). ETA=10:04:28, max mem: 20.9 GB 
[11/26 01:53:42 visual_prompt]: 	Training 500/553. train loss: 1.2086,	0.8221 s / batch. (data: 8.00e-03). ETA=9:44:07, max mem: 20.9 GB 
[11/26 01:54:33 visual_prompt]: Epoch 23 / 100: avg data time: 1.84e-01, avg batch time: 1.0072, average train loss: 12.7887
[11/26 01:55:31 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3068, average loss: 2.1401
[11/26 01:55:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.53	
[11/26 01:55:31 visual_prompt]: Best epoch 23: best metric: -2.140
[11/26 01:55:31 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/26 01:57:12 visual_prompt]: 	Training 100/553. train loss: 9.5365,	0.8413 s / batch. (data: 9.29e-03). ETA=9:55:40, max mem: 20.9 GB 
[11/26 01:58:53 visual_prompt]: 	Training 200/553. train loss: 9.0446,	0.8240 s / batch. (data: 2.93e-04). ETA=9:42:01, max mem: 20.9 GB 
[11/26 02:00:34 visual_prompt]: 	Training 300/553. train loss: 2.1329,	1.0922 s / batch. (data: 2.86e-01). ETA=12:49:39, max mem: 20.9 GB 
[11/26 02:02:14 visual_prompt]: 	Training 400/553. train loss: 12.1201,	0.8320 s / batch. (data: 7.96e-03). ETA=9:44:56, max mem: 20.9 GB 
[11/26 02:03:59 visual_prompt]: 	Training 500/553. train loss: 47.7156,	0.8240 s / batch. (data: 2.92e-04). ETA=9:37:55, max mem: 20.9 GB 
[11/26 02:04:52 visual_prompt]: Epoch 24 / 100: avg data time: 1.91e-01, avg batch time: 1.0145, average train loss: 12.5920
[11/26 02:05:49 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3058, average loss: 7.1258
[11/26 02:05:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.65	
[11/26 02:05:49 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/26 02:07:37 visual_prompt]: 	Training 100/553. train loss: 1.2235,	0.8400 s / batch. (data: 2.93e-04). ETA=9:46:57, max mem: 20.9 GB 
[11/26 02:09:14 visual_prompt]: 	Training 200/553. train loss: 8.7227,	0.8697 s / batch. (data: 2.16e-02). ETA=10:06:17, max mem: 20.9 GB 
[11/26 02:10:55 visual_prompt]: 	Training 300/553. train loss: 2.6724,	0.8073 s / batch. (data: 3.07e-04). ETA=9:21:25, max mem: 20.9 GB 
[11/26 02:12:35 visual_prompt]: 	Training 400/553. train loss: 1.7222,	1.3628 s / batch. (data: 5.57e-01). ETA=15:45:32, max mem: 20.9 GB 
[11/26 02:14:16 visual_prompt]: 	Training 500/553. train loss: 17.6020,	1.5760 s / batch. (data: 7.66e-01). ETA=18:10:48, max mem: 20.9 GB 
[11/26 02:15:08 visual_prompt]: Epoch 25 / 100: avg data time: 1.89e-01, avg batch time: 1.0113, average train loss: 16.1932
[11/26 02:16:06 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3061, average loss: 15.1664
[11/26 02:16:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.57	
[11/26 02:16:06 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/26 02:17:50 visual_prompt]: 	Training 100/553. train loss: 10.9355,	0.8280 s / batch. (data: 3.03e-04). ETA=9:30:58, max mem: 20.9 GB 
[11/26 02:19:31 visual_prompt]: 	Training 200/553. train loss: 48.2566,	1.7580 s / batch. (data: 9.42e-01). ETA=20:09:21, max mem: 20.9 GB 
[11/26 02:21:13 visual_prompt]: 	Training 300/553. train loss: 0.4749,	0.8339 s / batch. (data: 1.07e-02). ETA=9:32:15, max mem: 20.9 GB 
[11/26 02:22:52 visual_prompt]: 	Training 400/553. train loss: 1.9479,	0.8198 s / batch. (data: 7.95e-03). ETA=9:21:11, max mem: 20.9 GB 
[11/26 02:24:31 visual_prompt]: 	Training 500/553. train loss: 38.6370,	0.8380 s / batch. (data: 1.19e-02). ETA=9:32:18, max mem: 20.9 GB 
[11/26 02:25:22 visual_prompt]: Epoch 26 / 100: avg data time: 1.83e-01, avg batch time: 1.0065, average train loss: 9.0953
[11/26 02:26:20 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3058, average loss: 13.0650
[11/26 02:26:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.30	
[11/26 02:26:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/26 02:28:05 visual_prompt]: 	Training 100/553. train loss: 12.2426,	0.8349 s / batch. (data: 5.55e-03). ETA=9:28:01, max mem: 20.9 GB 
[11/26 02:29:44 visual_prompt]: 	Training 200/553. train loss: 53.9378,	1.3361 s / batch. (data: 5.21e-01). ETA=15:06:47, max mem: 20.9 GB 
[11/26 02:31:24 visual_prompt]: 	Training 300/553. train loss: 9.5595,	0.8200 s / batch. (data: 3.30e-04). ETA=9:15:09, max mem: 20.9 GB 
[11/26 02:33:06 visual_prompt]: 	Training 400/553. train loss: 0.7024,	0.8480 s / batch. (data: 8.39e-04). ETA=9:32:42, max mem: 20.9 GB 
[11/26 02:34:46 visual_prompt]: 	Training 500/553. train loss: 2.0934,	0.8360 s / batch. (data: 7.15e-04). ETA=9:23:12, max mem: 20.9 GB 
[11/26 02:35:37 visual_prompt]: Epoch 27 / 100: avg data time: 1.84e-01, avg batch time: 1.0069, average train loss: 9.9040
[11/26 02:36:34 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3073, average loss: 7.5517
[11/26 02:36:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.10	
[11/26 02:36:34 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[11/26 02:38:17 visual_prompt]: 	Training 100/553. train loss: 0.0000,	1.0135 s / batch. (data: 2.07e-01). ETA=11:20:13, max mem: 20.9 GB 
[11/26 02:39:58 visual_prompt]: 	Training 200/553. train loss: 24.6397,	0.8408 s / batch. (data: 2.06e-02). ETA=9:22:53, max mem: 20.9 GB 
[11/26 02:41:39 visual_prompt]: 	Training 300/553. train loss: 27.1145,	1.7200 s / batch. (data: 8.83e-01). ETA=19:08:38, max mem: 20.9 GB 
[11/26 02:43:19 visual_prompt]: 	Training 400/553. train loss: 6.8131,	0.8689 s / batch. (data: 5.06e-02). ETA=9:38:49, max mem: 20.9 GB 
[11/26 02:44:57 visual_prompt]: 	Training 500/553. train loss: 21.5959,	0.8194 s / batch. (data: 4.37e-04). ETA=9:04:26, max mem: 20.9 GB 
[11/26 02:45:50 visual_prompt]: Epoch 28 / 100: avg data time: 1.82e-01, avg batch time: 1.0058, average train loss: 11.7015
[11/26 02:46:48 visual_prompt]: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3055, average loss: 6.2508
[11/26 02:46:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.10	
[11/26 02:46:48 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[11/26 02:48:38 visual_prompt]: 	Training 100/553. train loss: 15.6510,	0.8240 s / batch. (data: 3.01e-04). ETA=9:05:26, max mem: 20.9 GB 
[11/26 02:50:17 visual_prompt]: 	Training 200/553. train loss: 8.5501,	1.7423 s / batch. (data: 9.32e-01). ETA=19:10:22, max mem: 20.9 GB 
[11/26 02:51:55 visual_prompt]: 	Training 300/553. train loss: 3.5969,	0.8360 s / batch. (data: 7.72e-04). ETA=9:10:34, max mem: 20.9 GB 
[11/26 02:53:32 visual_prompt]: 	Training 400/553. train loss: 21.7936,	1.3200 s / batch. (data: 4.95e-01). ETA=14:27:07, max mem: 20.9 GB 
[11/26 02:55:13 visual_prompt]: 	Training 500/553. train loss: 11.5920,	0.8476 s / batch. (data: 2.36e-02). ETA=9:15:24, max mem: 20.9 GB 
[11/26 02:56:06 visual_prompt]: Epoch 29 / 100: avg data time: 1.84e-01, avg batch time: 1.0088, average train loss: 8.9642
[11/26 02:57:03 visual_prompt]: Inference (val):avg data time: 2.16e-04, avg batch time: 0.3071, average loss: 2.9152
[11/26 02:57:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.47	
[11/26 02:57:03 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[11/26 02:58:46 visual_prompt]: 	Training 100/553. train loss: 3.1779,	0.8120 s / batch. (data: 3.03e-04). ETA=8:49:59, max mem: 20.9 GB 
[11/26 03:00:27 visual_prompt]: 	Training 200/553. train loss: 5.0161,	0.8250 s / batch. (data: 3.02e-04). ETA=8:57:05, max mem: 20.9 GB 
[11/26 03:02:06 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.2720 s / batch. (data: 4.38e-01). ETA=13:46:00, max mem: 20.9 GB 
[11/26 03:03:48 visual_prompt]: 	Training 400/553. train loss: 4.8886,	1.1908 s / batch. (data: 3.74e-01). ETA=12:51:17, max mem: 20.9 GB 
[11/26 03:05:27 visual_prompt]: 	Training 500/553. train loss: 2.4504,	1.5040 s / batch. (data: 6.79e-01). ETA=16:11:39, max mem: 20.9 GB 
[11/26 03:06:22 visual_prompt]: Epoch 30 / 100: avg data time: 1.87e-01, avg batch time: 1.0105, average train loss: 12.4585
[11/26 03:07:19 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3071, average loss: 1.8207
[11/26 03:07:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.72	
[11/26 03:07:19 visual_prompt]: Best epoch 30: best metric: -1.821
[11/26 03:07:19 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[11/26 03:09:05 visual_prompt]: 	Training 100/553. train loss: 9.1074,	0.8092 s / batch. (data: 2.96e-04). ETA=8:40:43, max mem: 20.9 GB 
[11/26 03:10:48 visual_prompt]: 	Training 200/553. train loss: 8.3460,	0.8241 s / batch. (data: 3.01e-04). ETA=8:48:54, max mem: 20.9 GB 
[11/26 03:12:25 visual_prompt]: 	Training 300/553. train loss: 14.9731,	0.8320 s / batch. (data: 3.22e-04). ETA=8:52:37, max mem: 20.9 GB 
[11/26 03:14:04 visual_prompt]: 	Training 400/553. train loss: 5.0719,	0.8360 s / batch. (data: 2.94e-04). ETA=8:53:48, max mem: 20.9 GB 
[11/26 03:15:45 visual_prompt]: 	Training 500/553. train loss: 2.2913,	0.8545 s / batch. (data: 2.48e-02). ETA=9:04:10, max mem: 20.9 GB 
[11/26 03:16:36 visual_prompt]: Epoch 31 / 100: avg data time: 1.83e-01, avg batch time: 1.0062, average train loss: 9.2928
[11/26 03:17:34 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3078, average loss: 3.2092
[11/26 03:17:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.53	
[11/26 03:17:34 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[11/26 03:19:19 visual_prompt]: 	Training 100/553. train loss: 0.6588,	0.8227 s / batch. (data: 2.94e-04). ETA=8:41:49, max mem: 20.9 GB 
[11/26 03:20:59 visual_prompt]: 	Training 200/553. train loss: 32.1023,	0.8354 s / batch. (data: 1.32e-02). ETA=8:48:29, max mem: 20.9 GB 
[11/26 03:22:42 visual_prompt]: 	Training 300/553. train loss: 7.0660,	0.8320 s / batch. (data: 7.94e-03). ETA=8:44:56, max mem: 20.9 GB 
[11/26 03:24:23 visual_prompt]: 	Training 400/553. train loss: 4.8223,	0.8180 s / batch. (data: 7.95e-03). ETA=8:34:45, max mem: 20.9 GB 
[11/26 03:26:00 visual_prompt]: 	Training 500/553. train loss: 10.7617,	0.8200 s / batch. (data: 3.05e-04). ETA=8:34:37, max mem: 20.9 GB 
[11/26 03:26:51 visual_prompt]: Epoch 32 / 100: avg data time: 1.85e-01, avg batch time: 1.0082, average train loss: 9.7804
[11/26 03:27:49 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3067, average loss: 3.2846
[11/26 03:27:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.36	
[11/26 03:27:49 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[11/26 03:29:31 visual_prompt]: 	Training 100/553. train loss: 34.7115,	0.8073 s / batch. (data: 3.06e-04). ETA=8:24:37, max mem: 20.9 GB 
[11/26 03:31:13 visual_prompt]: 	Training 200/553. train loss: 38.2422,	1.0956 s / batch. (data: 2.71e-01). ETA=11:22:58, max mem: 20.9 GB 
[11/26 03:32:53 visual_prompt]: 	Training 300/553. train loss: 14.4613,	0.8334 s / batch. (data: 3.02e-04). ETA=8:38:09, max mem: 20.9 GB 
[11/26 03:34:35 visual_prompt]: 	Training 400/553. train loss: 5.4463,	0.8320 s / batch. (data: 3.12e-04). ETA=8:35:53, max mem: 20.9 GB 
[11/26 03:36:14 visual_prompt]: 	Training 500/553. train loss: 7.4308,	0.8240 s / batch. (data: 4.25e-04). ETA=8:29:32, max mem: 20.9 GB 
[11/26 03:37:06 visual_prompt]: Epoch 33 / 100: avg data time: 1.85e-01, avg batch time: 1.0076, average train loss: 19.5149
[11/26 03:38:03 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3063, average loss: 24.6686
[11/26 03:38:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.60	
[11/26 03:38:03 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[11/26 03:39:49 visual_prompt]: 	Training 100/553. train loss: 1.7167,	0.8512 s / batch. (data: 9.73e-03). ETA=8:44:14, max mem: 20.9 GB 
[11/26 03:41:27 visual_prompt]: 	Training 200/553. train loss: 14.8736,	0.8280 s / batch. (data: 3.27e-04). ETA=8:28:32, max mem: 20.9 GB 
[11/26 03:43:07 visual_prompt]: 	Training 300/553. train loss: 20.2678,	0.8440 s / batch. (data: 4.64e-04). ETA=8:36:56, max mem: 20.9 GB 
[11/26 03:44:48 visual_prompt]: 	Training 400/553. train loss: 8.5092,	0.8465 s / batch. (data: 1.04e-02). ETA=8:37:03, max mem: 20.9 GB 
[11/26 03:46:29 visual_prompt]: 	Training 500/553. train loss: 3.7622,	1.5480 s / batch. (data: 7.15e-01). ETA=15:42:59, max mem: 20.9 GB 
[11/26 03:47:21 visual_prompt]: Epoch 34 / 100: avg data time: 1.84e-01, avg batch time: 1.0087, average train loss: 12.8303
[11/26 03:48:19 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3083, average loss: 1.4369
[11/26 03:48:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.19	
[11/26 03:48:19 visual_prompt]: Best epoch 34: best metric: -1.437
[11/26 03:48:19 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[11/26 03:50:05 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8214 s / batch. (data: 3.01e-04). ETA=8:18:17, max mem: 20.9 GB 
[11/26 03:51:46 visual_prompt]: 	Training 200/553. train loss: 27.4742,	0.8299 s / batch. (data: 3.10e-04). ETA=8:22:03, max mem: 20.9 GB 
[11/26 03:53:25 visual_prompt]: 	Training 300/553. train loss: 29.2407,	0.8097 s / batch. (data: 2.63e-04). ETA=8:08:31, max mem: 20.9 GB 
[11/26 03:55:04 visual_prompt]: 	Training 400/553. train loss: 15.7028,	0.8160 s / batch. (data: 2.94e-04). ETA=8:10:55, max mem: 20.9 GB 
[11/26 03:56:44 visual_prompt]: 	Training 500/553. train loss: 4.1077,	1.2675 s / batch. (data: 4.62e-01). ETA=12:40:28, max mem: 20.9 GB 
[11/26 03:57:36 visual_prompt]: Epoch 35 / 100: avg data time: 1.83e-01, avg batch time: 1.0077, average train loss: 11.6603
[11/26 03:58:34 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3082, average loss: 23.6302
[11/26 03:58:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.73	
[11/26 03:58:34 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[11/26 04:00:17 visual_prompt]: 	Training 100/553. train loss: 7.9954,	0.8280 s / batch. (data: 3.45e-04). ETA=8:14:39, max mem: 20.9 GB 
[11/26 04:01:58 visual_prompt]: 	Training 200/553. train loss: 13.3117,	0.8071 s / batch. (data: 3.11e-04). ETA=8:00:49, max mem: 20.9 GB 
[11/26 04:03:40 visual_prompt]: 	Training 300/553. train loss: 0.1916,	0.8244 s / batch. (data: 1.05e-02). ETA=8:09:44, max mem: 20.9 GB 
[11/26 04:05:19 visual_prompt]: 	Training 400/553. train loss: 0.6452,	0.8297 s / batch. (data: 1.05e-02). ETA=8:11:31, max mem: 20.9 GB 
[11/26 04:07:00 visual_prompt]: 	Training 500/553. train loss: 0.7493,	1.0178 s / batch. (data: 1.98e-01). ETA=10:01:17, max mem: 20.9 GB 
[11/26 04:07:50 visual_prompt]: Epoch 36 / 100: avg data time: 1.82e-01, avg batch time: 1.0058, average train loss: 8.5697
[11/26 04:08:47 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3079, average loss: 18.4189
[11/26 04:08:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.89	
[11/26 04:08:47 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[11/26 04:10:33 visual_prompt]: 	Training 100/553. train loss: 1.4602,	0.8321 s / batch. (data: 3.21e-04). ETA=8:09:27, max mem: 20.9 GB 
[11/26 04:12:12 visual_prompt]: 	Training 200/553. train loss: 15.3461,	0.8155 s / batch. (data: 2.72e-04). ETA=7:58:18, max mem: 20.9 GB 
[11/26 04:13:52 visual_prompt]: 	Training 300/553. train loss: 15.0805,	1.6731 s / batch. (data: 8.51e-01). ETA=16:18:31, max mem: 20.9 GB 
[11/26 04:15:35 visual_prompt]: 	Training 400/553. train loss: 8.8508,	1.8096 s / batch. (data: 9.93e-01). ETA=17:35:20, max mem: 20.9 GB 
[11/26 04:17:11 visual_prompt]: 	Training 500/553. train loss: 16.4601,	1.0475 s / batch. (data: 2.41e-01). ETA=10:09:10, max mem: 20.9 GB 
[11/26 04:18:05 visual_prompt]: Epoch 37 / 100: avg data time: 1.84e-01, avg batch time: 1.0075, average train loss: 10.1057
[11/26 04:19:02 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3077, average loss: 0.8617
[11/26 04:19:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/26 04:19:02 visual_prompt]: Best epoch 37: best metric: -0.862
[11/26 04:19:02 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[11/26 04:20:45 visual_prompt]: 	Training 100/553. train loss: 2.4563,	0.8952 s / batch. (data: 6.97e-02). ETA=8:38:19, max mem: 20.9 GB 
[11/26 04:22:26 visual_prompt]: 	Training 200/553. train loss: 1.2625,	1.2354 s / batch. (data: 4.24e-01). ETA=11:53:12, max mem: 20.9 GB 
[11/26 04:24:07 visual_prompt]: 	Training 300/553. train loss: 0.6433,	0.8261 s / batch. (data: 3.10e-04). ETA=7:55:31, max mem: 20.9 GB 
[11/26 04:25:46 visual_prompt]: 	Training 400/553. train loss: 0.0056,	0.8160 s / batch. (data: 2.85e-04). ETA=7:48:22, max mem: 20.9 GB 
[11/26 04:27:28 visual_prompt]: 	Training 500/553. train loss: 10.5995,	0.8447 s / batch. (data: 2.72e-04). ETA=8:03:26, max mem: 20.9 GB 
[11/26 04:28:19 visual_prompt]: Epoch 38 / 100: avg data time: 1.82e-01, avg batch time: 1.0060, average train loss: 9.1150
[11/26 04:29:16 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3054, average loss: 3.2880
[11/26 04:29:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.51	
[11/26 04:29:16 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[11/26 04:30:59 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8472 s / batch. (data: 2.06e-02). ETA=8:02:42, max mem: 20.9 GB 
[11/26 04:32:43 visual_prompt]: 	Training 200/553. train loss: 6.0318,	0.8140 s / batch. (data: 2.74e-04). ETA=7:42:25, max mem: 20.9 GB 
[11/26 04:34:26 visual_prompt]: 	Training 300/553. train loss: 1.6360,	0.8216 s / batch. (data: 3.01e-04). ETA=7:45:23, max mem: 20.9 GB 
[11/26 04:36:03 visual_prompt]: 	Training 400/553. train loss: 0.5821,	0.8214 s / batch. (data: 1.06e-02). ETA=7:43:52, max mem: 20.9 GB 
[11/26 04:37:44 visual_prompt]: 	Training 500/553. train loss: 1.3739,	1.7828 s / batch. (data: 9.55e-01). ETA=16:43:52, max mem: 20.9 GB 
[11/26 04:38:35 visual_prompt]: Epoch 39 / 100: avg data time: 1.86e-01, avg batch time: 1.0094, average train loss: 5.6180
[11/26 04:39:32 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3082, average loss: 1.1674
[11/26 04:39:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.95	
[11/26 04:39:32 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[11/26 04:41:18 visual_prompt]: 	Training 100/553. train loss: 16.0991,	0.8416 s / batch. (data: 2.91e-04). ETA=7:51:47, max mem: 20.9 GB 
[11/26 04:42:57 visual_prompt]: 	Training 200/553. train loss: 6.9398,	0.8402 s / batch. (data: 2.94e-04). ETA=7:49:34, max mem: 20.9 GB 
[11/26 04:44:39 visual_prompt]: 	Training 300/553. train loss: 4.2776,	0.8322 s / batch. (data: 8.90e-04). ETA=7:43:41, max mem: 20.9 GB 
[11/26 04:46:19 visual_prompt]: 	Training 400/553. train loss: 7.2788,	0.8316 s / batch. (data: 3.02e-04). ETA=7:41:59, max mem: 20.9 GB 
[11/26 04:47:58 visual_prompt]: 	Training 500/553. train loss: 0.1957,	0.8209 s / batch. (data: 5.40e-03). ETA=7:34:41, max mem: 20.9 GB 
[11/26 04:48:52 visual_prompt]: Epoch 40 / 100: avg data time: 1.87e-01, avg batch time: 1.0110, average train loss: 8.9193
[11/26 04:49:49 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3068, average loss: 6.3768
[11/26 04:49:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.94	
[11/26 04:49:49 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[11/26 04:51:37 visual_prompt]: 	Training 100/553. train loss: 2.3122,	0.8280 s / batch. (data: 2.85e-04). ETA=7:36:30, max mem: 20.9 GB 
[11/26 04:53:20 visual_prompt]: 	Training 200/553. train loss: 1.0709,	0.8537 s / batch. (data: 8.17e-04). ETA=7:49:14, max mem: 20.9 GB 
[11/26 04:54:59 visual_prompt]: 	Training 300/553. train loss: 1.7893,	0.8202 s / batch. (data: 3.31e-04). ETA=7:29:29, max mem: 20.9 GB 
[11/26 04:56:38 visual_prompt]: 	Training 400/553. train loss: 0.7781,	0.8214 s / batch. (data: 1.05e-02). ETA=7:28:46, max mem: 20.9 GB 
[11/26 04:58:16 visual_prompt]: 	Training 500/553. train loss: 2.0934,	0.8186 s / batch. (data: 3.19e-04). ETA=7:25:51, max mem: 20.9 GB 
[11/26 04:59:06 visual_prompt]: Epoch 41 / 100: avg data time: 1.82e-01, avg batch time: 1.0065, average train loss: 9.9561
[11/26 05:00:03 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3074, average loss: 6.7181
[11/26 05:00:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.14	
[11/26 05:00:03 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[11/26 05:01:46 visual_prompt]: 	Training 100/553. train loss: 2.5553,	0.8320 s / batch. (data: 2.66e-04). ETA=7:31:03, max mem: 20.9 GB 
[11/26 05:03:27 visual_prompt]: 	Training 200/553. train loss: 2.2946,	0.8221 s / batch. (data: 2.81e-04). ETA=7:24:18, max mem: 20.9 GB 
[11/26 05:05:08 visual_prompt]: 	Training 300/553. train loss: 44.8118,	0.8200 s / batch. (data: 2.99e-04). ETA=7:21:47, max mem: 20.9 GB 
[11/26 05:06:48 visual_prompt]: 	Training 400/553. train loss: 12.1536,	0.8163 s / batch. (data: 3.08e-04). ETA=7:18:25, max mem: 20.9 GB 
[11/26 05:08:27 visual_prompt]: 	Training 500/553. train loss: 0.0021,	1.1280 s / batch. (data: 2.99e-01). ETA=10:03:59, max mem: 20.9 GB 
[11/26 05:09:21 visual_prompt]: Epoch 42 / 100: avg data time: 1.84e-01, avg batch time: 1.0077, average train loss: 6.4853
[11/26 05:10:18 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3066, average loss: 7.4987
[11/26 05:10:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.33	
[11/26 05:10:18 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[11/26 05:12:05 visual_prompt]: 	Training 100/553. train loss: 0.5445,	0.8280 s / batch. (data: 3.08e-04). ETA=7:21:14, max mem: 20.9 GB 
[11/26 05:13:44 visual_prompt]: 	Training 200/553. train loss: 0.9264,	0.8200 s / batch. (data: 3.34e-04). ETA=7:15:35, max mem: 20.9 GB 
[11/26 05:15:22 visual_prompt]: 	Training 300/553. train loss: 5.0809,	0.8291 s / batch. (data: 2.89e-04). ETA=7:19:04, max mem: 20.9 GB 
[11/26 05:17:01 visual_prompt]: 	Training 400/553. train loss: 1.0758,	0.8209 s / batch. (data: 5.42e-03). ETA=7:13:19, max mem: 20.9 GB 
[11/26 05:18:42 visual_prompt]: 	Training 500/553. train loss: 1.8287,	0.8211 s / batch. (data: 3.26e-04). ETA=7:12:05, max mem: 20.9 GB 
[11/26 05:19:36 visual_prompt]: Epoch 43 / 100: avg data time: 1.85e-01, avg batch time: 1.0086, average train loss: 3.7523
[11/26 05:20:33 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3050, average loss: 1.0851
[11/26 05:20:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/26 05:20:33 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[11/26 05:22:18 visual_prompt]: 	Training 100/553. train loss: 1.5960,	1.0841 s / batch. (data: 2.59e-01). ETA=9:27:42, max mem: 20.9 GB 
[11/26 05:24:01 visual_prompt]: 	Training 200/553. train loss: 8.2660,	0.8217 s / batch. (data: 2.99e-04). ETA=7:08:57, max mem: 20.9 GB 
[11/26 05:25:39 visual_prompt]: 	Training 300/553. train loss: 4.5938,	0.8120 s / batch. (data: 2.96e-04). ETA=7:02:31, max mem: 20.9 GB 
[11/26 05:27:19 visual_prompt]: 	Training 400/553. train loss: 15.1157,	0.8320 s / batch. (data: 3.20e-04). ETA=7:11:32, max mem: 20.9 GB 
[11/26 05:28:59 visual_prompt]: 	Training 500/553. train loss: 3.6249,	0.8387 s / batch. (data: 1.05e-02). ETA=7:13:38, max mem: 20.9 GB 
[11/26 05:29:51 visual_prompt]: Epoch 44 / 100: avg data time: 1.85e-01, avg batch time: 1.0092, average train loss: 10.5993
[11/26 05:30:49 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3077, average loss: 16.9967
[11/26 05:30:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.53	
[11/26 05:30:49 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[11/26 05:32:35 visual_prompt]: 	Training 100/553. train loss: 10.2564,	0.8293 s / batch. (data: 5.43e-03). ETA=7:06:37, max mem: 20.9 GB 
[11/26 05:34:11 visual_prompt]: 	Training 200/553. train loss: 6.4706,	0.8320 s / batch. (data: 3.09e-04). ETA=7:06:38, max mem: 20.9 GB 
[11/26 05:35:53 visual_prompt]: 	Training 300/553. train loss: 8.1185,	0.8230 s / batch. (data: 5.45e-03). ETA=7:00:38, max mem: 20.9 GB 
[11/26 05:37:30 visual_prompt]: 	Training 400/553. train loss: 5.1428,	0.8166 s / batch. (data: 2.86e-04). ETA=6:56:01, max mem: 20.9 GB 
[11/26 05:39:14 visual_prompt]: 	Training 500/553. train loss: 4.8661,	0.8360 s / batch. (data: 7.94e-03). ETA=7:04:30, max mem: 20.9 GB 
[11/26 05:40:06 visual_prompt]: Epoch 45 / 100: avg data time: 1.83e-01, avg batch time: 1.0075, average train loss: 5.8682
[11/26 05:41:03 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3057, average loss: 1.5019
[11/26 05:41:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.04	
[11/26 05:41:03 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[11/26 05:42:49 visual_prompt]: 	Training 100/553. train loss: 4.5672,	1.3120 s / batch. (data: 4.86e-01). ETA=11:02:52, max mem: 20.9 GB 
[11/26 05:44:30 visual_prompt]: 	Training 200/553. train loss: 11.4203,	0.8083 s / batch. (data: 2.97e-04). ETA=6:47:02, max mem: 20.9 GB 
[11/26 05:46:09 visual_prompt]: 	Training 300/553. train loss: 3.6374,	0.8290 s / batch. (data: 2.87e-04). ETA=6:56:05, max mem: 20.9 GB 
[11/26 05:47:50 visual_prompt]: 	Training 400/553. train loss: 2.0307,	0.8162 s / batch. (data: 7.23e-04). ETA=6:48:18, max mem: 20.9 GB 
[11/26 05:49:27 visual_prompt]: 	Training 500/553. train loss: 31.1081,	0.8367 s / batch. (data: 1.55e-02). ETA=6:57:08, max mem: 20.9 GB 
[11/26 05:50:21 visual_prompt]: Epoch 46 / 100: avg data time: 1.84e-01, avg batch time: 1.0077, average train loss: 5.2327
[11/26 05:51:18 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3084, average loss: 3.8369
[11/26 05:51:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.58	
[11/26 05:51:18 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[11/26 05:53:03 visual_prompt]: 	Training 100/553. train loss: 7.1362,	0.8480 s / batch. (data: 3.29e-04). ETA=7:00:38, max mem: 20.9 GB 
[11/26 05:54:41 visual_prompt]: 	Training 200/553. train loss: 11.4090,	1.5601 s / batch. (data: 7.28e-01). ETA=12:51:16, max mem: 20.9 GB 
[11/26 05:56:22 visual_prompt]: 	Training 300/553. train loss: 4.9667,	0.8146 s / batch. (data: 2.72e-04). ETA=6:41:22, max mem: 20.9 GB 
[11/26 05:58:02 visual_prompt]: 	Training 400/553. train loss: 1.0379,	0.8160 s / batch. (data: 2.79e-04). ETA=6:40:41, max mem: 20.9 GB 
[11/26 05:59:41 visual_prompt]: 	Training 500/553. train loss: 12.5615,	0.8401 s / batch. (data: 8.10e-03). ETA=6:51:08, max mem: 20.9 GB 
[11/26 06:00:34 visual_prompt]: Epoch 47 / 100: avg data time: 1.82e-01, avg batch time: 1.0052, average train loss: 5.7454
[11/26 06:01:32 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3049, average loss: 0.6999
[11/26 06:01:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 43.50	rocauc: 56.76	
[11/26 06:01:32 visual_prompt]: Best epoch 47: best metric: -0.700
[11/26 06:01:32 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[11/26 06:03:17 visual_prompt]: 	Training 100/553. train loss: 3.1295,	0.8160 s / batch. (data: 3.03e-04). ETA=6:37:14, max mem: 20.9 GB 
[11/26 06:04:57 visual_prompt]: 	Training 200/553. train loss: 0.5968,	0.8289 s / batch. (data: 2.86e-04). ETA=6:42:09, max mem: 20.9 GB 
[11/26 06:06:39 visual_prompt]: 	Training 300/553. train loss: 5.2766,	1.5179 s / batch. (data: 6.96e-01). ETA=12:13:54, max mem: 20.9 GB 
[11/26 06:08:16 visual_prompt]: 	Training 400/553. train loss: 0.0635,	0.8283 s / batch. (data: 3.12e-04). ETA=6:39:05, max mem: 20.9 GB 
[11/26 06:09:56 visual_prompt]: 	Training 500/553. train loss: 5.1195,	0.8621 s / batch. (data: 3.41e-02). ETA=6:53:55, max mem: 20.9 GB 
[11/26 06:10:48 visual_prompt]: Epoch 48 / 100: avg data time: 1.82e-01, avg batch time: 1.0057, average train loss: 4.9506
[11/26 06:11:45 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3063, average loss: 11.4459
[11/26 06:11:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.97	
[11/26 06:11:45 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[11/26 06:13:29 visual_prompt]: 	Training 100/553. train loss: 4.8152,	0.8302 s / batch. (data: 2.95e-04). ETA=6:36:29, max mem: 20.9 GB 
[11/26 06:15:08 visual_prompt]: 	Training 200/553. train loss: 5.7341,	0.8146 s / batch. (data: 7.95e-03). ETA=6:27:40, max mem: 20.9 GB 
[11/26 06:16:49 visual_prompt]: 	Training 300/553. train loss: 7.1820,	0.8410 s / batch. (data: 2.89e-04). ETA=6:38:51, max mem: 20.9 GB 
[11/26 06:18:30 visual_prompt]: 	Training 400/553. train loss: 9.0521,	0.8200 s / batch. (data: 2.91e-04). ETA=6:27:31, max mem: 20.9 GB 
[11/26 06:20:11 visual_prompt]: 	Training 500/553. train loss: 0.6756,	0.8218 s / batch. (data: 1.05e-02). ETA=6:26:59, max mem: 20.9 GB 
[11/26 06:21:03 visual_prompt]: Epoch 49 / 100: avg data time: 1.84e-01, avg batch time: 1.0085, average train loss: 4.6919
[11/26 06:22:01 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3067, average loss: 1.8727
[11/26 06:22:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.81	
[11/26 06:22:01 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[11/26 06:23:46 visual_prompt]: 	Training 100/553. train loss: 0.7409,	0.8147 s / batch. (data: 2.94e-04). ETA=6:21:35, max mem: 20.9 GB 
[11/26 06:25:27 visual_prompt]: 	Training 200/553. train loss: 16.3499,	0.8230 s / batch. (data: 7.24e-04). ETA=6:24:06, max mem: 20.9 GB 
[11/26 06:27:06 visual_prompt]: 	Training 300/553. train loss: 3.4335,	0.8200 s / batch. (data: 2.87e-04). ETA=6:21:20, max mem: 20.9 GB 
[11/26 06:28:44 visual_prompt]: 	Training 400/553. train loss: 0.2178,	0.8360 s / batch. (data: 7.96e-03). ETA=6:27:24, max mem: 20.9 GB 
[11/26 06:30:26 visual_prompt]: 	Training 500/553. train loss: 7.9378,	0.8440 s / batch. (data: 7.91e-04). ETA=6:29:41, max mem: 20.9 GB 
[11/26 06:31:18 visual_prompt]: Epoch 50 / 100: avg data time: 1.85e-01, avg batch time: 1.0072, average train loss: 3.8544
[11/26 06:32:15 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3078, average loss: 1.1306
[11/26 06:32:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.34	
[11/26 06:32:15 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[11/26 06:34:00 visual_prompt]: 	Training 100/553. train loss: 3.7684,	1.1309 s / batch. (data: 2.96e-01). ETA=8:39:15, max mem: 20.9 GB 
[11/26 06:35:41 visual_prompt]: 	Training 200/553. train loss: 8.2651,	0.8410 s / batch. (data: 5.41e-03). ETA=6:24:45, max mem: 20.9 GB 
[11/26 06:37:22 visual_prompt]: 	Training 300/553. train loss: 0.5341,	1.5598 s / batch. (data: 7.55e-01). ETA=11:51:00, max mem: 20.9 GB 
[11/26 06:39:03 visual_prompt]: 	Training 400/553. train loss: 8.5789,	1.5400 s / batch. (data: 7.13e-01). ETA=11:39:25, max mem: 20.9 GB 
[11/26 06:40:42 visual_prompt]: 	Training 500/553. train loss: 0.8546,	0.8290 s / batch. (data: 7.52e-04). ETA=6:15:06, max mem: 20.9 GB 
[11/26 06:41:32 visual_prompt]: Epoch 51 / 100: avg data time: 1.84e-01, avg batch time: 1.0069, average train loss: 3.7384
[11/26 06:42:30 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3085, average loss: 0.6999
[11/26 06:42:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.23	
[11/26 06:42:30 visual_prompt]: Best epoch 51: best metric: -0.700
[11/26 06:42:30 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[11/26 06:44:18 visual_prompt]: 	Training 100/553. train loss: 11.8221,	0.8320 s / batch. (data: 3.01e-04). ETA=6:14:20, max mem: 20.9 GB 
[11/26 06:45:57 visual_prompt]: 	Training 200/553. train loss: 0.6621,	0.8530 s / batch. (data: 2.99e-04). ETA=6:22:22, max mem: 20.9 GB 
[11/26 06:47:38 visual_prompt]: 	Training 300/553. train loss: 0.3386,	0.8222 s / batch. (data: 8.06e-04). ETA=6:07:11, max mem: 20.9 GB 
[11/26 06:49:20 visual_prompt]: 	Training 400/553. train loss: 7.0128,	0.8062 s / batch. (data: 3.04e-04). ETA=5:58:41, max mem: 20.9 GB 
[11/26 06:50:55 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8151 s / batch. (data: 3.04e-04). ETA=6:01:19, max mem: 20.9 GB 
[11/26 06:51:46 visual_prompt]: Epoch 52 / 100: avg data time: 1.83e-01, avg batch time: 1.0063, average train loss: 8.5044
[11/26 06:52:44 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3059, average loss: 16.2109
[11/26 06:52:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.94	
[11/26 06:52:44 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[11/26 06:54:28 visual_prompt]: 	Training 100/553. train loss: 1.3408,	0.8085 s / batch. (data: 2.74e-04). ETA=5:56:21, max mem: 20.9 GB 
[11/26 06:56:08 visual_prompt]: 	Training 200/553. train loss: 0.2647,	0.8774 s / batch. (data: 1.60e-02). ETA=6:25:15, max mem: 20.9 GB 
[11/26 06:57:47 visual_prompt]: 	Training 300/553. train loss: 9.9964,	0.8155 s / batch. (data: 2.55e-04). ETA=5:56:41, max mem: 20.9 GB 
[11/26 06:59:30 visual_prompt]: 	Training 400/553. train loss: 1.2182,	0.8142 s / batch. (data: 7.95e-03). ETA=5:54:45, max mem: 20.9 GB 
[11/26 07:01:08 visual_prompt]: 	Training 500/553. train loss: 7.5149,	0.8080 s / batch. (data: 3.97e-04). ETA=5:50:43, max mem: 20.9 GB 
[11/26 07:02:01 visual_prompt]: Epoch 53 / 100: avg data time: 1.84e-01, avg batch time: 1.0071, average train loss: 5.3401
[11/26 07:02:58 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3072, average loss: 14.8889
[11/26 07:02:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.86	
[11/26 07:02:58 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[11/26 07:04:44 visual_prompt]: 	Training 100/553. train loss: 2.1315,	0.8327 s / batch. (data: 1.56e-02). ETA=5:59:18, max mem: 20.9 GB 
[11/26 07:06:25 visual_prompt]: 	Training 200/553. train loss: 10.6493,	0.8358 s / batch. (data: 1.05e-02). ETA=5:59:17, max mem: 20.9 GB 
[11/26 07:08:03 visual_prompt]: 	Training 300/553. train loss: 5.9574,	0.8186 s / batch. (data: 2.52e-04). ETA=5:50:31, max mem: 20.9 GB 
[11/26 07:09:43 visual_prompt]: 	Training 400/553. train loss: 4.9209,	0.8420 s / batch. (data: 1.06e-02). ETA=5:59:08, max mem: 20.9 GB 
[11/26 07:11:23 visual_prompt]: 	Training 500/553. train loss: 2.9053,	0.8385 s / batch. (data: 7.74e-04). ETA=5:56:14, max mem: 20.9 GB 
[11/26 07:12:16 visual_prompt]: Epoch 54 / 100: avg data time: 1.84e-01, avg batch time: 1.0084, average train loss: 5.2600
[11/26 07:13:13 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3049, average loss: 11.6650
[11/26 07:13:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.79	
[11/26 07:13:13 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[11/26 07:14:58 visual_prompt]: 	Training 100/553. train loss: 1.2028,	1.0216 s / batch. (data: 2.05e-01). ETA=7:11:24, max mem: 20.9 GB 
[11/26 07:16:36 visual_prompt]: 	Training 200/553. train loss: 16.6755,	0.8569 s / batch. (data: 2.06e-02). ETA=6:00:26, max mem: 20.9 GB 
[11/26 07:18:17 visual_prompt]: 	Training 300/553. train loss: 3.5134,	0.8320 s / batch. (data: 2.96e-04). ETA=5:48:34, max mem: 20.9 GB 
[11/26 07:19:56 visual_prompt]: 	Training 400/553. train loss: 10.3159,	0.8240 s / batch. (data: 3.37e-04). ETA=5:43:50, max mem: 20.9 GB 
[11/26 07:21:36 visual_prompt]: 	Training 500/553. train loss: 4.8559,	0.8657 s / batch. (data: 2.58e-02). ETA=5:59:49, max mem: 20.9 GB 
[11/26 07:22:29 visual_prompt]: Epoch 55 / 100: avg data time: 1.83e-01, avg batch time: 1.0050, average train loss: 5.5410
[11/26 07:23:27 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.3058, average loss: 9.3833
[11/26 07:23:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.64	
[11/26 07:23:27 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[11/26 07:25:12 visual_prompt]: 	Training 100/553. train loss: 1.1103,	0.8524 s / batch. (data: 2.03e-02). ETA=5:52:06, max mem: 20.9 GB 
[11/26 07:26:52 visual_prompt]: 	Training 200/553. train loss: 0.2164,	0.8520 s / batch. (data: 8.20e-04). ETA=5:50:31, max mem: 20.9 GB 
[11/26 07:28:33 visual_prompt]: 	Training 300/553. train loss: 1.0083,	0.8340 s / batch. (data: 3.86e-04). ETA=5:41:43, max mem: 20.9 GB 
[11/26 07:30:14 visual_prompt]: 	Training 400/553. train loss: 1.0088,	0.8290 s / batch. (data: 1.05e-02). ETA=5:38:18, max mem: 20.9 GB 
[11/26 07:31:54 visual_prompt]: 	Training 500/553. train loss: 4.3350,	2.3349 s / batch. (data: 1.52e+00). ETA=15:48:57, max mem: 20.9 GB 
[11/26 07:32:44 visual_prompt]: Epoch 56 / 100: avg data time: 1.85e-01, avg batch time: 1.0080, average train loss: 3.4236
[11/26 07:33:42 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3067, average loss: 4.8735
[11/26 07:33:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.04	
[11/26 07:33:42 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[11/26 07:35:29 visual_prompt]: 	Training 100/553. train loss: 0.6384,	0.8072 s / batch. (data: 2.83e-04). ETA=5:25:59, max mem: 20.9 GB 
[11/26 07:37:08 visual_prompt]: 	Training 200/553. train loss: 0.6226,	0.8189 s / batch. (data: 5.45e-03). ETA=5:29:22, max mem: 20.9 GB 
[11/26 07:38:47 visual_prompt]: 	Training 300/553. train loss: 0.0616,	0.8331 s / batch. (data: 2.88e-04). ETA=5:33:41, max mem: 20.9 GB 
[11/26 07:40:26 visual_prompt]: 	Training 400/553. train loss: 1.2673,	0.8150 s / batch. (data: 2.58e-04). ETA=5:25:04, max mem: 20.9 GB 
[11/26 07:42:04 visual_prompt]: 	Training 500/553. train loss: 1.4663,	0.8294 s / batch. (data: 2.78e-04). ETA=5:29:26, max mem: 20.9 GB 
[11/26 07:42:58 visual_prompt]: Epoch 57 / 100: avg data time: 1.82e-01, avg batch time: 1.0056, average train loss: 3.2758
[11/26 07:43:55 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3056, average loss: 6.1833
[11/26 07:43:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.85	
[11/26 07:43:55 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[11/26 07:45:39 visual_prompt]: 	Training 100/553. train loss: 1.7930,	1.0301 s / batch. (data: 2.06e-01). ETA=6:46:31, max mem: 20.9 GB 
[11/26 07:47:21 visual_prompt]: 	Training 200/553. train loss: 2.8647,	0.8360 s / batch. (data: 2.98e-04). ETA=5:28:32, max mem: 20.9 GB 
[11/26 07:49:03 visual_prompt]: 	Training 300/553. train loss: 3.5130,	0.8073 s / batch. (data: 1.07e-03). ETA=5:15:54, max mem: 20.9 GB 
[11/26 07:50:43 visual_prompt]: 	Training 400/553. train loss: 3.5694,	0.8338 s / batch. (data: 3.19e-04). ETA=5:24:52, max mem: 20.9 GB 
[11/26 07:52:22 visual_prompt]: 	Training 500/553. train loss: 4.1884,	0.8070 s / batch. (data: 2.80e-04). ETA=5:13:06, max mem: 20.9 GB 
[11/26 07:53:13 visual_prompt]: Epoch 58 / 100: avg data time: 1.84e-01, avg batch time: 1.0083, average train loss: 4.2337
[11/26 07:54:10 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3082, average loss: 1.2878
[11/26 07:54:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.34	
[11/26 07:54:11 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[11/26 07:55:56 visual_prompt]: 	Training 100/553. train loss: 1.5850,	0.8310 s / batch. (data: 2.86e-04). ETA=5:20:18, max mem: 20.9 GB 
[11/26 07:57:38 visual_prompt]: 	Training 200/553. train loss: 0.0001,	0.8320 s / batch. (data: 3.06e-04). ETA=5:19:17, max mem: 20.9 GB 
[11/26 07:59:18 visual_prompt]: 	Training 300/553. train loss: 6.9652,	0.8326 s / batch. (data: 7.59e-04). ETA=5:18:07, max mem: 20.9 GB 
[11/26 08:00:57 visual_prompt]: 	Training 400/553. train loss: 1.0044,	0.8546 s / batch. (data: 5.45e-03). ETA=5:25:07, max mem: 20.9 GB 
[11/26 08:02:40 visual_prompt]: 	Training 500/553. train loss: 4.3351,	0.8061 s / batch. (data: 2.52e-04). ETA=5:05:20, max mem: 20.9 GB 
[11/26 08:03:31 visual_prompt]: Epoch 59 / 100: avg data time: 1.87e-01, avg batch time: 1.0128, average train loss: 3.7559
[11/26 08:04:28 visual_prompt]: Inference (val):avg data time: 3.73e-04, avg batch time: 0.3068, average loss: 2.3761
[11/26 08:04:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.99	
[11/26 08:04:28 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[11/26 08:06:13 visual_prompt]: 	Training 100/553. train loss: 2.9310,	0.8200 s / batch. (data: 3.10e-04). ETA=5:08:29, max mem: 20.9 GB 
[11/26 08:07:53 visual_prompt]: 	Training 200/553. train loss: 3.4186,	0.8200 s / batch. (data: 3.02e-04). ETA=5:07:07, max mem: 20.9 GB 
[11/26 08:09:31 visual_prompt]: 	Training 300/553. train loss: 0.0000,	1.5730 s / batch. (data: 7.32e-01). ETA=9:46:32, max mem: 20.9 GB 
[11/26 08:11:13 visual_prompt]: 	Training 400/553. train loss: 0.8414,	1.0747 s / batch. (data: 2.54e-01). ETA=6:38:56, max mem: 20.9 GB 
[11/26 08:12:53 visual_prompt]: 	Training 500/553. train loss: 0.6160,	0.8440 s / batch. (data: 1.19e-02). ETA=5:11:54, max mem: 20.9 GB 
[11/26 08:13:46 visual_prompt]: Epoch 60 / 100: avg data time: 1.84e-01, avg batch time: 1.0079, average train loss: 2.8352
[11/26 08:14:43 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3068, average loss: 8.2320
[11/26 08:14:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.87	
[11/26 08:14:43 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[11/26 08:16:28 visual_prompt]: 	Training 100/553. train loss: 2.5247,	0.8480 s / batch. (data: 3.08e-04). ETA=5:11:13, max mem: 20.9 GB 
[11/26 08:18:09 visual_prompt]: 	Training 200/553. train loss: 2.5481,	1.5680 s / batch. (data: 7.39e-01). ETA=9:32:50, max mem: 20.9 GB 
[11/26 08:19:51 visual_prompt]: 	Training 300/553. train loss: 8.1168,	1.2680 s / batch. (data: 4.60e-01). ETA=7:41:07, max mem: 20.9 GB 
[11/26 08:21:28 visual_prompt]: 	Training 400/553. train loss: 0.9167,	0.8157 s / batch. (data: 2.92e-04). ETA=4:55:16, max mem: 20.9 GB 
[11/26 08:23:09 visual_prompt]: 	Training 500/553. train loss: 2.1288,	2.5538 s / batch. (data: 1.75e+00). ETA=15:20:14, max mem: 20.9 GB 
[11/26 08:24:00 visual_prompt]: Epoch 61 / 100: avg data time: 1.82e-01, avg batch time: 1.0069, average train loss: 5.1663
[11/26 08:24:58 visual_prompt]: Inference (val):avg data time: 5.65e-04, avg batch time: 0.3061, average loss: 4.0654
[11/26 08:24:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.58	
[11/26 08:24:58 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[11/26 08:26:42 visual_prompt]: 	Training 100/553. train loss: 0.6616,	0.8479 s / batch. (data: 4.18e-04). ETA=5:03:21, max mem: 20.9 GB 
[11/26 08:28:22 visual_prompt]: 	Training 200/553. train loss: 0.7322,	0.8728 s / batch. (data: 2.07e-02). ETA=5:10:48, max mem: 20.9 GB 
[11/26 08:30:02 visual_prompt]: 	Training 300/553. train loss: 3.3843,	0.8203 s / batch. (data: 8.70e-03). ETA=4:50:46, max mem: 20.9 GB 
[11/26 08:31:43 visual_prompt]: 	Training 400/553. train loss: 4.0434,	0.8360 s / batch. (data: 3.02e-04). ETA=4:54:55, max mem: 20.9 GB 
[11/26 08:33:21 visual_prompt]: 	Training 500/553. train loss: 2.6342,	0.8170 s / batch. (data: 5.45e-03). ETA=4:46:50, max mem: 20.9 GB 
[11/26 08:34:15 visual_prompt]: Epoch 62 / 100: avg data time: 1.84e-01, avg batch time: 1.0075, average train loss: 2.2123
[11/26 08:35:12 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3052, average loss: 6.2874
[11/26 08:35:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.54	
[11/26 08:35:12 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[11/26 08:37:02 visual_prompt]: 	Training 100/553. train loss: 6.2589,	0.8201 s / batch. (data: 2.31e-03). ETA=4:45:52, max mem: 20.9 GB 
[11/26 08:38:47 visual_prompt]: 	Training 200/553. train loss: 5.9336,	0.8400 s / batch. (data: 2.84e-04). ETA=4:51:23, max mem: 20.9 GB 
[11/26 08:40:25 visual_prompt]: 	Training 300/553. train loss: 0.8044,	0.8207 s / batch. (data: 2.99e-04). ETA=4:43:19, max mem: 20.9 GB 
[11/26 08:42:00 visual_prompt]: 	Training 400/553. train loss: 2.8658,	0.9039 s / batch. (data: 8.26e-02). ETA=5:10:33, max mem: 20.9 GB 
[11/26 08:43:39 visual_prompt]: 	Training 500/553. train loss: 0.8137,	0.8142 s / batch. (data: 5.43e-03). ETA=4:38:21, max mem: 20.9 GB 
[11/26 08:44:30 visual_prompt]: Epoch 63 / 100: avg data time: 1.84e-01, avg batch time: 1.0081, average train loss: 2.6216
[11/26 08:45:28 visual_prompt]: Inference (val):avg data time: 6.80e-05, avg batch time: 0.3126, average loss: 2.3884
[11/26 08:45:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.03	
[11/26 08:45:28 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[11/26 08:47:15 visual_prompt]: 	Training 100/553. train loss: 0.0482,	0.8257 s / batch. (data: 8.02e-04). ETA=4:40:11, max mem: 20.9 GB 
[11/26 08:48:57 visual_prompt]: 	Training 200/553. train loss: 1.9375,	0.8207 s / batch. (data: 2.79e-04). ETA=4:37:08, max mem: 20.9 GB 
[11/26 08:50:33 visual_prompt]: 	Training 300/553. train loss: 8.7293,	0.8360 s / batch. (data: 7.97e-03). ETA=4:40:54, max mem: 20.9 GB 
[11/26 08:52:13 visual_prompt]: 	Training 400/553. train loss: 5.6863,	0.9095 s / batch. (data: 9.50e-02). ETA=5:04:05, max mem: 20.9 GB 
[11/26 08:53:53 visual_prompt]: 	Training 500/553. train loss: 0.1400,	0.8271 s / batch. (data: 1.19e-02). ETA=4:35:10, max mem: 20.9 GB 
[11/26 08:54:45 visual_prompt]: Epoch 64 / 100: avg data time: 1.84e-01, avg batch time: 1.0082, average train loss: 4.3293
[11/26 08:55:43 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3065, average loss: 8.6779
[11/26 08:55:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.02	
[11/26 08:55:43 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[11/26 08:57:31 visual_prompt]: 	Training 100/553. train loss: 4.5883,	0.8316 s / batch. (data: 3.48e-04). ETA=4:34:31, max mem: 20.9 GB 
[11/26 08:59:12 visual_prompt]: 	Training 200/553. train loss: 0.8529,	1.7320 s / batch. (data: 9.17e-01). ETA=9:28:54, max mem: 20.9 GB 
[11/26 09:00:50 visual_prompt]: 	Training 300/553. train loss: 1.6087,	1.1332 s / batch. (data: 3.02e-01). ETA=6:10:20, max mem: 20.9 GB 
[11/26 09:02:30 visual_prompt]: 	Training 400/553. train loss: 2.3950,	0.8320 s / batch. (data: 7.85e-04). ETA=4:30:30, max mem: 20.9 GB 
[11/26 09:04:09 visual_prompt]: 	Training 500/553. train loss: 0.6915,	0.8298 s / batch. (data: 1.20e-02). ETA=4:28:25, max mem: 20.9 GB 
[11/26 09:05:00 visual_prompt]: Epoch 65 / 100: avg data time: 1.84e-01, avg batch time: 1.0079, average train loss: 2.8459
[11/26 09:05:58 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3064, average loss: 1.9032
[11/26 09:05:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.48	
[11/26 09:05:58 visual_prompt]: Stopping early.
[11/26 09:05:58 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 09:05:58 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 09:05:58 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/26 09:05:58 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 09:05:58 visual_prompt]: Training with config:
[11/26 09:05:58 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr5.0_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/26 09:05:58 visual_prompt]: Loading training data...
[11/26 09:05:58 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 09:05:58 visual_prompt]: Loading validation data...
[11/26 09:05:58 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 09:05:58 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 09:06:01 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 09:06:01 visual_prompt]: tuned percent:0.525
[11/26 09:06:01 visual_prompt]: Device used for model: 0
[11/26 09:06:01 visual_prompt]: Setting up Evaluator...
[11/26 09:06:01 visual_prompt]: Setting up Trainer...
[11/26 09:06:01 visual_prompt]: 	Setting up the optimizer...
[11/26 09:06:01 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 09:07:45 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8570 s / batch. (data: 1.06e-02). ETA=13:08:28, max mem: 20.9 GB 
[11/26 09:09:23 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8149 s / batch. (data: 2.91e-04). ETA=12:28:22, max mem: 20.9 GB 
[11/26 09:11:06 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.4713 s / batch. (data: 6.64e-01). ETA=22:28:39, max mem: 20.9 GB 
[11/26 09:12:44 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8119 s / batch. (data: 3.11e-04). ETA=12:22:53, max mem: 20.9 GB 
[11/26 09:14:27 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8324 s / batch. (data: 1.10e-02). ETA=12:40:16, max mem: 20.9 GB 
[11/26 09:15:20 visual_prompt]: Epoch 1 / 100: avg data time: 1.86e-01, avg batch time: 1.0101, average train loss: 1.5403
[11/26 09:16:17 visual_prompt]: Inference (val):avg data time: 1.61e-04, avg batch time: 0.3083, average loss: 1.5201
[11/26 09:16:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 09:16:17 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[11/26 09:18:00 visual_prompt]: 	Training 100/553. train loss: 3.0043,	1.4920 s / batch. (data: 6.81e-01). ETA=22:38:52, max mem: 20.9 GB 
[11/26 09:19:40 visual_prompt]: 	Training 200/553. train loss: 0.0004,	0.8320 s / batch. (data: 3.10e-04). ETA=12:36:22, max mem: 20.9 GB 
[11/26 09:21:22 visual_prompt]: 	Training 300/553. train loss: 1.0276,	1.1185 s / batch. (data: 3.02e-01). ETA=16:54:59, max mem: 20.9 GB 
[11/26 09:23:02 visual_prompt]: 	Training 400/553. train loss: 2.9529,	0.8092 s / batch. (data: 2.93e-04). ETA=12:12:59, max mem: 20.9 GB 
[11/26 09:24:43 visual_prompt]: 	Training 500/553. train loss: 1.0274,	0.8296 s / batch. (data: 4.63e-03). ETA=12:30:03, max mem: 20.9 GB 
[11/26 09:25:34 visual_prompt]: Epoch 2 / 100: avg data time: 1.85e-01, avg batch time: 1.0081, average train loss: 1.8527
[11/26 09:26:32 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3068, average loss: 3.8356
[11/26 09:26:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.38	
[11/26 09:26:32 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[11/26 09:28:14 visual_prompt]: 	Training 100/553. train loss: 4.4338,	0.8280 s / batch. (data: 3.08e-04). ETA=12:26:29, max mem: 20.9 GB 
[11/26 09:29:56 visual_prompt]: 	Training 200/553. train loss: 0.7001,	0.8160 s / batch. (data: 3.38e-04). ETA=12:14:19, max mem: 20.9 GB 
[11/26 09:31:35 visual_prompt]: 	Training 300/553. train loss: 2.6151,	0.8079 s / batch. (data: 3.07e-04). ETA=12:05:38, max mem: 20.9 GB 
[11/26 09:33:16 visual_prompt]: 	Training 400/553. train loss: 1.4173,	0.8251 s / batch. (data: 2.97e-04). ETA=12:19:44, max mem: 20.9 GB 
[11/26 09:34:58 visual_prompt]: 	Training 500/553. train loss: 1.0324,	1.1856 s / batch. (data: 3.61e-01). ETA=17:40:59, max mem: 20.9 GB 
[11/26 09:35:49 visual_prompt]: Epoch 3 / 100: avg data time: 1.83e-01, avg batch time: 1.0073, average train loss: 2.5510
[11/26 09:36:46 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3082, average loss: 1.8970
[11/26 09:36:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.23	
[11/26 09:36:46 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[11/26 09:38:33 visual_prompt]: 	Training 100/553. train loss: 2.2802,	0.8120 s / batch. (data: 2.96e-04). ETA=12:04:34, max mem: 20.9 GB 
[11/26 09:40:13 visual_prompt]: 	Training 200/553. train loss: 2.3283,	0.8448 s / batch. (data: 1.66e-02). ETA=12:32:26, max mem: 20.9 GB 
[11/26 09:41:54 visual_prompt]: 	Training 300/553. train loss: 1.8069,	1.6581 s / batch. (data: 8.52e-01). ETA=1 day, 0:34:06, max mem: 20.9 GB 
[11/26 09:43:29 visual_prompt]: 	Training 400/553. train loss: 1.2089,	1.3080 s / batch. (data: 4.96e-01). ETA=19:20:39, max mem: 20.9 GB 
[11/26 09:45:12 visual_prompt]: 	Training 500/553. train loss: 0.0000,	3.3262 s / batch. (data: 2.51e+00). ETA=2 days, 1:05:55, max mem: 20.9 GB 
[11/26 09:46:05 visual_prompt]: Epoch 4 / 100: avg data time: 1.85e-01, avg batch time: 1.0095, average train loss: 2.9854
[11/26 09:47:02 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3068, average loss: 12.4888
[11/26 09:47:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.00	
[11/26 09:47:02 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[11/26 09:48:45 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8660 s / batch. (data: 3.40e-02). ETA=12:44:46, max mem: 20.9 GB 
[11/26 09:50:25 visual_prompt]: 	Training 200/553. train loss: 6.0476,	1.0840 s / batch. (data: 2.63e-01). ETA=15:55:28, max mem: 20.9 GB 
[11/26 09:52:06 visual_prompt]: 	Training 300/553. train loss: 16.7818,	0.8351 s / batch. (data: 3.87e-04). ETA=12:14:41, max mem: 20.9 GB 
[11/26 09:53:45 visual_prompt]: 	Training 400/553. train loss: 16.1333,	0.8250 s / batch. (data: 3.04e-04). ETA=12:04:28, max mem: 20.9 GB 
[11/26 09:55:26 visual_prompt]: 	Training 500/553. train loss: 4.3187,	0.8377 s / batch. (data: 1.20e-02). ETA=12:14:15, max mem: 20.9 GB 
[11/26 09:56:19 visual_prompt]: Epoch 5 / 100: avg data time: 1.84e-01, avg batch time: 1.0069, average train loss: 5.7854
[11/26 09:57:16 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3063, average loss: 14.4886
[11/26 09:57:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.19	
[11/26 09:57:16 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[11/26 09:59:03 visual_prompt]: 	Training 100/553. train loss: 11.8735,	0.8285 s / batch. (data: 1.55e-02). ETA=12:04:04, max mem: 20.9 GB 
[11/26 10:00:42 visual_prompt]: 	Training 200/553. train loss: 8.3384,	0.8131 s / batch. (data: 5.41e-03). ETA=11:49:15, max mem: 20.9 GB 
[11/26 10:02:20 visual_prompt]: 	Training 300/553. train loss: 5.6462,	0.8111 s / batch. (data: 3.33e-04). ETA=11:46:05, max mem: 20.9 GB 
[11/26 10:04:04 visual_prompt]: 	Training 400/553. train loss: 4.8664,	0.8320 s / batch. (data: 3.11e-04). ETA=12:02:56, max mem: 20.9 GB 
[11/26 10:05:43 visual_prompt]: 	Training 500/553. train loss: 1.8894,	0.8313 s / batch. (data: 2.78e-04). ETA=12:00:58, max mem: 20.9 GB 
[11/26 10:06:35 visual_prompt]: Epoch 6 / 100: avg data time: 1.87e-01, avg batch time: 1.0105, average train loss: 6.9691
[11/26 10:07:33 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3074, average loss: 13.6566
[11/26 10:07:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.65	
[11/26 10:07:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[11/26 10:09:17 visual_prompt]: 	Training 100/553. train loss: 7.1549,	0.8465 s / batch. (data: 5.44e-03). ETA=12:11:57, max mem: 20.9 GB 
[11/26 10:10:57 visual_prompt]: 	Training 200/553. train loss: 2.6272,	0.8213 s / batch. (data: 3.03e-04). ETA=11:48:49, max mem: 20.9 GB 
[11/26 10:12:41 visual_prompt]: 	Training 300/553. train loss: 2.2763,	1.9680 s / batch. (data: 1.15e+00). ETA=1 day, 4:15:10, max mem: 20.9 GB 
[11/26 10:14:21 visual_prompt]: 	Training 400/553. train loss: 0.9221,	1.9551 s / batch. (data: 1.14e+00). ETA=1 day, 4:00:47, max mem: 20.9 GB 
[11/26 10:16:00 visual_prompt]: 	Training 500/553. train loss: 0.9348,	0.8280 s / batch. (data: 2.84e-04). ETA=11:50:27, max mem: 20.9 GB 
[11/26 10:16:51 visual_prompt]: Epoch 7 / 100: avg data time: 1.85e-01, avg batch time: 1.0094, average train loss: 6.1081
[11/26 10:17:48 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3069, average loss: 3.2896
[11/26 10:17:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.96	
[11/26 10:17:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[11/26 10:19:30 visual_prompt]: 	Training 100/553. train loss: 16.5645,	0.8311 s / batch. (data: 3.22e-04). ETA=11:51:00, max mem: 20.9 GB 
[11/26 10:21:12 visual_prompt]: 	Training 200/553. train loss: 1.7785,	0.8332 s / batch. (data: 5.44e-03). ETA=11:51:22, max mem: 20.9 GB 
[11/26 10:22:53 visual_prompt]: 	Training 300/553. train loss: 17.5561,	0.8247 s / batch. (data: 5.43e-03). ETA=11:42:47, max mem: 20.9 GB 
[11/26 10:24:33 visual_prompt]: 	Training 400/553. train loss: 4.2794,	0.8480 s / batch. (data: 7.95e-03). ETA=12:01:12, max mem: 20.9 GB 
[11/26 10:26:13 visual_prompt]: 	Training 500/553. train loss: 36.0258,	1.2560 s / batch. (data: 4.35e-01). ETA=17:46:06, max mem: 20.9 GB 
[11/26 10:27:06 visual_prompt]: Epoch 8 / 100: avg data time: 1.85e-01, avg batch time: 1.0090, average train loss: 7.0746
[11/26 10:28:04 visual_prompt]: Inference (val):avg data time: 2.50e-04, avg batch time: 0.3072, average loss: 1.8158
[11/26 10:28:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.71	
[11/26 10:28:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[11/26 10:29:48 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8441 s / batch. (data: 1.38e-02). ETA=11:54:19, max mem: 20.9 GB 
[11/26 10:31:27 visual_prompt]: 	Training 200/553. train loss: 1.3922,	0.8273 s / batch. (data: 1.19e-02). ETA=11:38:44, max mem: 20.9 GB 
[11/26 10:33:07 visual_prompt]: 	Training 300/553. train loss: 9.8045,	1.6960 s / batch. (data: 8.84e-01). ETA=23:49:36, max mem: 20.9 GB 
[11/26 10:34:49 visual_prompt]: 	Training 400/553. train loss: 16.5693,	0.8174 s / batch. (data: 1.05e-02). ETA=11:27:38, max mem: 20.9 GB 
[11/26 10:36:30 visual_prompt]: 	Training 500/553. train loss: 1.3801,	0.9911 s / batch. (data: 1.80e-01). ETA=13:52:09, max mem: 20.9 GB 
[11/26 10:37:21 visual_prompt]: Epoch 9 / 100: avg data time: 1.85e-01, avg batch time: 1.0079, average train loss: 5.9887
[11/26 10:38:18 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3060, average loss: 4.5217
[11/26 10:38:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.49	
[11/26 10:38:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[11/26 10:40:05 visual_prompt]: 	Training 100/553. train loss: 20.1672,	0.8222 s / batch. (data: 3.00e-04). ETA=11:28:11, max mem: 20.9 GB 
[11/26 10:41:45 visual_prompt]: 	Training 200/553. train loss: 7.2029,	0.8560 s / batch. (data: 2.87e-04). ETA=11:55:06, max mem: 20.9 GB 
[11/26 10:43:24 visual_prompt]: 	Training 300/553. train loss: 16.4779,	0.8325 s / batch. (data: 2.06e-02). ETA=11:34:04, max mem: 20.9 GB 
[11/26 10:45:02 visual_prompt]: 	Training 400/553. train loss: 4.8384,	0.8361 s / batch. (data: 3.10e-04). ETA=11:35:38, max mem: 20.9 GB 
[11/26 10:46:43 visual_prompt]: 	Training 500/553. train loss: 27.9044,	0.8230 s / batch. (data: 1.19e-02). ETA=11:23:23, max mem: 20.9 GB 
[11/26 10:47:36 visual_prompt]: Epoch 10 / 100: avg data time: 1.84e-01, avg batch time: 1.0073, average train loss: 9.6479
[11/26 10:48:33 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3078, average loss: 12.2237
[11/26 10:48:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.87	
[11/26 10:48:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[11/26 10:50:19 visual_prompt]: 	Training 100/553. train loss: 0.4391,	0.8154 s / batch. (data: 3.14e-04). ETA=11:14:59, max mem: 20.9 GB 
[11/26 10:52:01 visual_prompt]: 	Training 200/553. train loss: 0.8890,	0.8086 s / batch. (data: 2.90e-04). ETA=11:08:02, max mem: 20.9 GB 
[11/26 10:53:41 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.1252 s / batch. (data: 1.32e+00). ETA=1 day, 5:12:14, max mem: 20.9 GB 
[11/26 10:55:19 visual_prompt]: 	Training 400/553. train loss: 2.5199,	0.8240 s / batch. (data: 2.96e-04). ETA=11:18:01, max mem: 20.9 GB 
[11/26 10:56:58 visual_prompt]: 	Training 500/553. train loss: 3.1253,	0.8480 s / batch. (data: 3.02e-04). ETA=11:36:20, max mem: 20.9 GB 
[11/26 10:57:49 visual_prompt]: Epoch 11 / 100: avg data time: 1.81e-01, avg batch time: 1.0055, average train loss: 5.9548
[11/26 10:58:46 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3076, average loss: 8.7608
[11/26 10:58:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.98	
[11/26 10:58:46 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[11/26 11:00:33 visual_prompt]: 	Training 100/553. train loss: 4.0551,	0.8176 s / batch. (data: 3.19e-04). ETA=11:09:16, max mem: 20.9 GB 
[11/26 11:02:14 visual_prompt]: 	Training 200/553. train loss: 3.1000,	1.5056 s / batch. (data: 6.71e-01). ETA=20:30:00, max mem: 20.9 GB 
[11/26 11:03:53 visual_prompt]: 	Training 300/553. train loss: 2.7414,	0.8083 s / batch. (data: 3.04e-04). ETA=10:59:01, max mem: 20.9 GB 
[11/26 11:05:34 visual_prompt]: 	Training 400/553. train loss: 3.0529,	0.8163 s / batch. (data: 3.11e-04). ETA=11:04:10, max mem: 20.9 GB 
[11/26 11:07:14 visual_prompt]: 	Training 500/553. train loss: 39.8382,	0.8098 s / batch. (data: 2.95e-04). ETA=10:57:30, max mem: 20.9 GB 
[11/26 11:08:05 visual_prompt]: Epoch 12 / 100: avg data time: 1.87e-01, avg batch time: 1.0107, average train loss: 7.5559
[11/26 11:09:03 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3066, average loss: 7.9266
[11/26 11:09:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.23	
[11/26 11:09:03 visual_prompt]: Best epoch 12: best metric: -7.927
[11/26 11:09:03 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[11/26 11:10:49 visual_prompt]: 	Training 100/553. train loss: 1.6612,	0.8151 s / batch. (data: 5.34e-03). ETA=10:59:45, max mem: 20.9 GB 
[11/26 11:12:26 visual_prompt]: 	Training 200/553. train loss: 1.3221,	0.8196 s / batch. (data: 2.85e-04). ETA=11:02:02, max mem: 20.9 GB 
[11/26 11:14:07 visual_prompt]: 	Training 300/553. train loss: 1.4190,	1.8895 s / batch. (data: 1.08e+00). ETA=1 day, 1:23:03, max mem: 20.9 GB 
[11/26 11:15:45 visual_prompt]: 	Training 400/553. train loss: 28.4845,	0.8240 s / batch. (data: 2.93e-04). ETA=11:02:49, max mem: 20.9 GB 
[11/26 11:17:26 visual_prompt]: 	Training 500/553. train loss: 0.2549,	0.8480 s / batch. (data: 2.97e-04). ETA=11:20:43, max mem: 20.9 GB 
[11/26 11:18:19 visual_prompt]: Epoch 13 / 100: avg data time: 1.81e-01, avg batch time: 1.0048, average train loss: 6.1947
[11/26 11:19:16 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3066, average loss: 1.4006
[11/26 11:19:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.02	
[11/26 11:19:16 visual_prompt]: Best epoch 13: best metric: -1.401
[11/26 11:19:16 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[11/26 11:21:02 visual_prompt]: 	Training 100/553. train loss: 8.7893,	0.8477 s / batch. (data: 3.08e-04). ETA=11:18:20, max mem: 20.9 GB 
[11/26 11:22:42 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.1595 s / batch. (data: 3.39e-01). ETA=15:25:51, max mem: 20.9 GB 
[11/26 11:24:22 visual_prompt]: 	Training 300/553. train loss: 0.4008,	0.8194 s / batch. (data: 3.14e-04). ETA=10:52:57, max mem: 20.9 GB 
[11/26 11:26:02 visual_prompt]: 	Training 400/553. train loss: 2.3632,	0.8360 s / batch. (data: 3.00e-04). ETA=11:04:45, max mem: 20.9 GB 
[11/26 11:27:42 visual_prompt]: 	Training 500/553. train loss: 16.9422,	0.8068 s / batch. (data: 2.77e-04). ETA=10:40:11, max mem: 20.9 GB 
[11/26 11:28:32 visual_prompt]: Epoch 14 / 100: avg data time: 1.82e-01, avg batch time: 1.0060, average train loss: 4.8561
[11/26 11:29:30 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3060, average loss: 5.0464
[11/26 11:29:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.00	
[11/26 11:29:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[11/26 11:31:14 visual_prompt]: 	Training 100/553. train loss: 9.6415,	0.8480 s / batch. (data: 4.57e-04). ETA=11:10:42, max mem: 20.9 GB 
[11/26 11:32:52 visual_prompt]: 	Training 200/553. train loss: 0.2219,	0.8234 s / batch. (data: 1.05e-02). ETA=10:49:54, max mem: 20.9 GB 
[11/26 11:34:34 visual_prompt]: 	Training 300/553. train loss: 9.0622,	0.8320 s / batch. (data: 2.90e-04). ETA=10:55:17, max mem: 20.9 GB 
[11/26 11:36:11 visual_prompt]: 	Training 400/553. train loss: 7.6745,	1.0000 s / batch. (data: 1.81e-01). ETA=13:05:58, max mem: 20.9 GB 
[11/26 11:37:53 visual_prompt]: 	Training 500/553. train loss: 3.0960,	0.8166 s / batch. (data: 2.96e-04). ETA=10:40:27, max mem: 20.9 GB 
[11/26 11:38:46 visual_prompt]: Epoch 15 / 100: avg data time: 1.82e-01, avg batch time: 1.0053, average train loss: 8.3364
[11/26 11:39:44 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3069, average loss: 7.1108
[11/26 11:39:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.17	
[11/26 11:39:44 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[11/26 11:41:27 visual_prompt]: 	Training 100/553. train loss: 1.6939,	0.8280 s / batch. (data: 2.94e-04). ETA=10:47:17, max mem: 20.9 GB 
[11/26 11:43:08 visual_prompt]: 	Training 200/553. train loss: 35.5122,	0.8089 s / batch. (data: 3.06e-04). ETA=10:30:58, max mem: 20.9 GB 
[11/26 11:44:48 visual_prompt]: 	Training 300/553. train loss: 23.5092,	0.8287 s / batch. (data: 2.84e-04). ETA=10:45:06, max mem: 20.9 GB 
[11/26 11:46:28 visual_prompt]: 	Training 400/553. train loss: 0.4262,	0.8205 s / batch. (data: 6.85e-04). ETA=10:37:19, max mem: 20.9 GB 
[11/26 11:48:07 visual_prompt]: 	Training 500/553. train loss: 1.3423,	1.2874 s / batch. (data: 4.76e-01). ETA=16:37:51, max mem: 20.9 GB 
[11/26 11:49:00 visual_prompt]: Epoch 16 / 100: avg data time: 1.82e-01, avg batch time: 1.0059, average train loss: 6.4424
[11/26 11:49:58 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3053, average loss: 1.2462
[11/26 11:49:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 63.31	
[11/26 11:49:58 visual_prompt]: Best epoch 16: best metric: -1.246
[11/26 11:49:58 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[11/26 11:51:42 visual_prompt]: 	Training 100/553. train loss: 3.2804,	0.8120 s / batch. (data: 3.14e-04). ETA=10:27:17, max mem: 20.9 GB 
[11/26 11:53:23 visual_prompt]: 	Training 200/553. train loss: 15.9472,	0.8254 s / batch. (data: 2.84e-04). ETA=10:36:14, max mem: 20.9 GB 
[11/26 11:55:02 visual_prompt]: 	Training 300/553. train loss: 2.5579,	0.8186 s / batch. (data: 2.95e-04). ETA=10:29:37, max mem: 20.9 GB 
[11/26 11:56:42 visual_prompt]: 	Training 400/553. train loss: 2.8205,	1.1520 s / batch. (data: 3.19e-01). ETA=14:44:11, max mem: 20.9 GB 
[11/26 11:58:22 visual_prompt]: 	Training 500/553. train loss: 9.4810,	1.7000 s / batch. (data: 8.82e-01). ETA=21:41:57, max mem: 20.9 GB 
[11/26 11:59:15 visual_prompt]: Epoch 17 / 100: avg data time: 1.83e-01, avg batch time: 1.0069, average train loss: 5.8217
[11/26 12:00:13 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3056, average loss: 2.4304
[11/26 12:00:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 65.42	
[11/26 12:00:13 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[11/26 12:01:58 visual_prompt]: 	Training 100/553. train loss: 5.2362,	0.8329 s / batch. (data: 2.51e-04). ETA=10:35:47, max mem: 20.9 GB 
[11/26 12:03:40 visual_prompt]: 	Training 200/553. train loss: 10.0353,	0.8334 s / batch. (data: 5.89e-03). ETA=10:34:46, max mem: 20.9 GB 
[11/26 12:05:20 visual_prompt]: 	Training 300/553. train loss: 3.7636,	0.8078 s / batch. (data: 3.14e-04). ETA=10:13:52, max mem: 20.9 GB 
[11/26 12:07:00 visual_prompt]: 	Training 400/553. train loss: 2.8356,	0.8360 s / batch. (data: 2.97e-04). ETA=10:33:58, max mem: 20.9 GB 
[11/26 12:08:39 visual_prompt]: 	Training 500/553. train loss: 3.3602,	0.8481 s / batch. (data: 5.41e-03). ETA=10:41:43, max mem: 20.9 GB 
[11/26 12:09:30 visual_prompt]: Epoch 18 / 100: avg data time: 1.84e-01, avg batch time: 1.0083, average train loss: 4.8105
[11/26 12:10:28 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3061, average loss: 3.5719
[11/26 12:10:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.67	
[11/26 12:10:28 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[11/26 12:12:12 visual_prompt]: 	Training 100/553. train loss: 10.9852,	1.1113 s / batch. (data: 2.89e-01). ETA=13:58:01, max mem: 20.9 GB 
[11/26 12:13:53 visual_prompt]: 	Training 200/553. train loss: 2.1276,	0.8338 s / batch. (data: 1.38e-02). ETA=10:27:23, max mem: 20.9 GB 
[11/26 12:15:33 visual_prompt]: 	Training 300/553. train loss: 16.7522,	0.8176 s / batch. (data: 3.18e-04). ETA=10:13:50, max mem: 20.9 GB 
[11/26 12:17:15 visual_prompt]: 	Training 400/553. train loss: 0.8231,	0.8425 s / batch. (data: 1.44e-02). ETA=10:31:08, max mem: 20.9 GB 
[11/26 12:18:52 visual_prompt]: 	Training 500/553. train loss: 5.8514,	0.8179 s / batch. (data: 3.00e-04). ETA=10:11:18, max mem: 20.9 GB 
[11/26 12:19:43 visual_prompt]: Epoch 19 / 100: avg data time: 1.81e-01, avg batch time: 1.0043, average train loss: 5.4547
[11/26 12:20:41 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3063, average loss: 15.1166
[11/26 12:20:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.62	
[11/26 12:20:41 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[11/26 12:22:24 visual_prompt]: 	Training 100/553. train loss: 7.5258,	0.8280 s / batch. (data: 3.20e-04). ETA=10:16:45, max mem: 20.9 GB 
[11/26 12:24:06 visual_prompt]: 	Training 200/553. train loss: 0.0937,	0.8531 s / batch. (data: 4.18e-02). ETA=10:34:01, max mem: 20.9 GB 
[11/26 12:25:46 visual_prompt]: 	Training 300/553. train loss: 11.4970,	0.8200 s / batch. (data: 3.19e-04). ETA=10:08:03, max mem: 20.9 GB 
[11/26 12:27:26 visual_prompt]: 	Training 400/553. train loss: 7.0201,	0.8286 s / batch. (data: 1.05e-02). ETA=10:13:05, max mem: 20.9 GB 
[11/26 12:29:05 visual_prompt]: 	Training 500/553. train loss: 37.8318,	0.8221 s / batch. (data: 1.05e-02). ETA=10:06:53, max mem: 20.9 GB 
[11/26 12:29:59 visual_prompt]: Epoch 20 / 100: avg data time: 1.83e-01, avg batch time: 1.0090, average train loss: 7.4035
[11/26 12:30:56 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3062, average loss: 4.7180
[11/26 12:30:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.29	
[11/26 12:30:56 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[11/26 12:32:43 visual_prompt]: 	Training 100/553. train loss: 1.1744,	0.8265 s / batch. (data: 3.08e-04). ETA=10:08:01, max mem: 20.9 GB 
[11/26 12:34:22 visual_prompt]: 	Training 200/553. train loss: 15.9846,	0.8120 s / batch. (data: 2.81e-04). ETA=9:56:00, max mem: 20.9 GB 
[11/26 12:36:02 visual_prompt]: 	Training 300/553. train loss: 3.3645,	1.1363 s / batch. (data: 3.10e-01). ETA=13:52:08, max mem: 20.9 GB 
[11/26 12:37:41 visual_prompt]: 	Training 400/553. train loss: 11.6615,	0.8140 s / batch. (data: 2.74e-04). ETA=9:54:45, max mem: 20.9 GB 
[11/26 12:39:23 visual_prompt]: 	Training 500/553. train loss: 7.7146,	0.8320 s / batch. (data: 3.14e-04). ETA=10:06:31, max mem: 20.9 GB 
[11/26 12:40:14 visual_prompt]: Epoch 21 / 100: avg data time: 1.84e-01, avg batch time: 1.0088, average train loss: 7.2807
[11/26 12:41:11 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3070, average loss: 4.3996
[11/26 12:41:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 69.04	
[11/26 12:41:12 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[11/26 12:42:55 visual_prompt]: 	Training 100/553. train loss: 11.0785,	0.8200 s / batch. (data: 2.99e-04). ETA=9:55:41, max mem: 20.9 GB 
[11/26 12:44:35 visual_prompt]: 	Training 200/553. train loss: 3.7789,	0.8160 s / batch. (data: 3.02e-04). ETA=9:51:24, max mem: 20.9 GB 
[11/26 12:46:13 visual_prompt]: 	Training 300/553. train loss: 0.0005,	0.9280 s / batch. (data: 1.10e-01). ETA=11:11:04, max mem: 20.9 GB 
[11/26 12:47:55 visual_prompt]: 	Training 400/553. train loss: 1.1607,	0.8232 s / batch. (data: 1.55e-02). ETA=9:53:52, max mem: 20.9 GB 
[11/26 12:49:35 visual_prompt]: 	Training 500/553. train loss: 1.8572,	0.8366 s / batch. (data: 7.20e-03). ETA=10:02:11, max mem: 20.9 GB 
[11/26 12:50:29 visual_prompt]: Epoch 22 / 100: avg data time: 1.84e-01, avg batch time: 1.0078, average train loss: 5.3035
[11/26 12:51:26 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3077, average loss: 5.3951
[11/26 12:51:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 66.83	
[11/26 12:51:26 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[11/26 12:53:12 visual_prompt]: 	Training 100/553. train loss: 2.2837,	1.4456 s / batch. (data: 6.38e-01). ETA=17:16:49, max mem: 20.9 GB 
[11/26 12:54:53 visual_prompt]: 	Training 200/553. train loss: 3.1618,	0.9120 s / batch. (data: 8.43e-02). ETA=10:52:35, max mem: 20.9 GB 
[11/26 12:56:35 visual_prompt]: 	Training 300/553. train loss: 0.9660,	0.8244 s / batch. (data: 5.92e-03). ETA=9:48:32, max mem: 20.9 GB 
[11/26 12:58:13 visual_prompt]: 	Training 400/553. train loss: 0.7535,	0.8360 s / batch. (data: 8.13e-04). ETA=9:55:26, max mem: 20.9 GB 
[11/26 12:59:51 visual_prompt]: 	Training 500/553. train loss: 17.1139,	0.8200 s / batch. (data: 2.91e-04). ETA=9:42:39, max mem: 20.9 GB 
[11/26 13:00:43 visual_prompt]: Epoch 23 / 100: avg data time: 1.82e-01, avg batch time: 1.0063, average train loss: 6.0506
[11/26 13:01:40 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3053, average loss: 1.1173
[11/26 13:01:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 69.92	
[11/26 13:01:40 visual_prompt]: Best epoch 23: best metric: -1.117
[11/26 13:01:40 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[11/26 13:03:22 visual_prompt]: 	Training 100/553. train loss: 3.6433,	0.8200 s / batch. (data: 3.14e-04). ETA=9:40:34, max mem: 20.9 GB 
[11/26 13:05:02 visual_prompt]: 	Training 200/553. train loss: 2.6525,	0.8386 s / batch. (data: 7.54e-04). ETA=9:52:19, max mem: 20.9 GB 
[11/26 13:06:43 visual_prompt]: 	Training 300/553. train loss: 4.9809,	0.9160 s / batch. (data: 8.42e-02). ETA=10:45:30, max mem: 20.9 GB 
[11/26 13:08:23 visual_prompt]: 	Training 400/553. train loss: 0.7819,	0.8131 s / batch. (data: 5.43e-03). ETA=9:31:38, max mem: 20.9 GB 
[11/26 13:10:05 visual_prompt]: 	Training 500/553. train loss: 0.2565,	0.8186 s / batch. (data: 3.00e-04). ETA=9:34:07, max mem: 20.9 GB 
[11/26 13:10:58 visual_prompt]: Epoch 24 / 100: avg data time: 1.85e-01, avg batch time: 1.0092, average train loss: 4.8728
[11/26 13:11:56 visual_prompt]: Inference (val):avg data time: 1.95e-04, avg batch time: 0.3075, average loss: 8.6880
[11/26 13:11:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.08	
[11/26 13:11:56 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[11/26 13:13:43 visual_prompt]: 	Training 100/553. train loss: 11.0764,	0.8648 s / batch. (data: 4.69e-03). ETA=10:04:18, max mem: 20.9 GB 
[11/26 13:15:21 visual_prompt]: 	Training 200/553. train loss: 3.3035,	0.8240 s / batch. (data: 3.33e-04). ETA=9:34:25, max mem: 20.9 GB 
[11/26 13:17:01 visual_prompt]: 	Training 300/553. train loss: 4.5802,	0.8340 s / batch. (data: 3.10e-04). ETA=9:40:02, max mem: 20.9 GB 
[11/26 13:18:41 visual_prompt]: 	Training 400/553. train loss: 6.3984,	1.2620 s / batch. (data: 4.56e-01). ETA=14:35:34, max mem: 20.9 GB 
[11/26 13:20:23 visual_prompt]: 	Training 500/553. train loss: 2.0448,	1.6440 s / batch. (data: 8.11e-01). ETA=18:57:51, max mem: 20.9 GB 
[11/26 13:21:14 visual_prompt]: Epoch 25 / 100: avg data time: 1.85e-01, avg batch time: 1.0090, average train loss: 6.7018
[11/26 13:22:12 visual_prompt]: Inference (val):avg data time: 2.19e-04, avg batch time: 0.3097, average loss: 15.9009
[11/26 13:22:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.48	
[11/26 13:22:12 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[11/26 13:23:57 visual_prompt]: 	Training 100/553. train loss: 10.9048,	0.8280 s / batch. (data: 3.03e-04). ETA=9:30:57, max mem: 20.9 GB 
[11/26 13:25:38 visual_prompt]: 	Training 200/553. train loss: 16.0287,	1.8400 s / batch. (data: 1.02e+00). ETA=21:05:45, max mem: 20.9 GB 
[11/26 13:27:20 visual_prompt]: 	Training 300/553. train loss: 5.3045,	0.8376 s / batch. (data: 5.90e-03). ETA=9:34:48, max mem: 20.9 GB 
[11/26 13:28:59 visual_prompt]: 	Training 400/553. train loss: 1.7387,	0.8200 s / batch. (data: 2.91e-04). ETA=9:21:21, max mem: 20.9 GB 
[11/26 13:30:38 visual_prompt]: 	Training 500/553. train loss: 5.1397,	0.8407 s / batch. (data: 1.05e-02). ETA=9:34:09, max mem: 20.9 GB 
[11/26 13:31:29 visual_prompt]: Epoch 26 / 100: avg data time: 1.83e-01, avg batch time: 1.0074, average train loss: 5.4226
[11/26 13:32:27 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3073, average loss: 1.9294
[11/26 13:32:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 71.24	
[11/26 13:32:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[11/26 13:34:11 visual_prompt]: 	Training 100/553. train loss: 7.7995,	0.8092 s / batch. (data: 2.79e-04). ETA=9:10:33, max mem: 20.9 GB 
[11/26 13:35:52 visual_prompt]: 	Training 200/553. train loss: 17.9861,	1.2475 s / batch. (data: 4.38e-01). ETA=14:06:41, max mem: 20.9 GB 
[11/26 13:37:32 visual_prompt]: 	Training 300/553. train loss: 6.7700,	0.8342 s / batch. (data: 5.44e-03). ETA=9:24:46, max mem: 20.9 GB 
[11/26 13:39:13 visual_prompt]: 	Training 400/553. train loss: 9.3514,	0.8350 s / batch. (data: 8.13e-04). ETA=9:23:56, max mem: 20.9 GB 
[11/26 13:40:54 visual_prompt]: 	Training 500/553. train loss: 5.1421,	0.8153 s / batch. (data: 2.98e-04). ETA=9:09:16, max mem: 20.9 GB 
[11/26 13:41:44 visual_prompt]: Epoch 27 / 100: avg data time: 1.83e-01, avg batch time: 1.0074, average train loss: 5.3024
[11/26 13:42:41 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.3071, average loss: 2.4663
[11/26 13:42:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 72.00	
[11/26 13:42:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[11/26 13:44:25 visual_prompt]: 	Training 100/553. train loss: 3.7675,	0.8240 s / batch. (data: 3.31e-04). ETA=9:13:01, max mem: 20.9 GB 
[11/26 13:46:06 visual_prompt]: 	Training 200/553. train loss: 23.0866,	0.8202 s / batch. (data: 8.73e-03). ETA=9:09:04, max mem: 20.9 GB 
[11/26 13:47:47 visual_prompt]: 	Training 300/553. train loss: 22.1772,	1.4890 s / batch. (data: 6.81e-01). ETA=16:34:23, max mem: 20.9 GB 
[11/26 13:49:26 visual_prompt]: 	Training 400/553. train loss: 19.7659,	0.8279 s / batch. (data: 4.45e-04). ETA=9:11:30, max mem: 20.9 GB 
[11/26 13:51:06 visual_prompt]: 	Training 500/553. train loss: 0.9137,	0.8293 s / batch. (data: 2.93e-04). ETA=9:11:01, max mem: 20.9 GB 
[11/26 13:51:59 visual_prompt]: Epoch 28 / 100: avg data time: 1.83e-01, avg batch time: 1.0085, average train loss: 5.5347
[11/26 13:52:57 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.3078, average loss: 1.6315
[11/26 13:52:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.93	
[11/26 13:52:57 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[11/26 13:54:47 visual_prompt]: 	Training 100/553. train loss: 0.6146,	0.8320 s / batch. (data: 2.99e-04). ETA=9:10:43, max mem: 20.9 GB 
[11/26 13:56:27 visual_prompt]: 	Training 200/553. train loss: 22.7147,	1.9176 s / batch. (data: 1.09e+00). ETA=21:06:06, max mem: 20.9 GB 
[11/26 13:58:04 visual_prompt]: 	Training 300/553. train loss: 14.2245,	0.8432 s / batch. (data: 7.82e-04). ETA=9:15:20, max mem: 20.9 GB 
[11/26 13:59:41 visual_prompt]: 	Training 400/553. train loss: 11.3983,	1.2550 s / batch. (data: 4.40e-01). ETA=13:44:26, max mem: 20.9 GB 
[11/26 14:01:22 visual_prompt]: 	Training 500/553. train loss: 14.5313,	0.8361 s / batch. (data: 1.35e-02). ETA=9:07:50, max mem: 20.9 GB 
[11/26 14:02:14 visual_prompt]: Epoch 29 / 100: avg data time: 1.83e-01, avg batch time: 1.0077, average train loss: 6.5519
[11/26 14:03:11 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3063, average loss: 5.4480
[11/26 14:03:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 71.10	
[11/26 14:03:11 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[11/26 14:04:54 visual_prompt]: 	Training 100/553. train loss: 4.4012,	0.8283 s / batch. (data: 2.86e-04). ETA=9:00:38, max mem: 20.9 GB 
[11/26 14:06:35 visual_prompt]: 	Training 200/553. train loss: 2.6239,	0.8549 s / batch. (data: 1.08e-02). ETA=9:16:34, max mem: 20.9 GB 
[11/26 14:08:14 visual_prompt]: 	Training 300/553. train loss: 0.0003,	1.2201 s / batch. (data: 3.92e-01). ETA=13:12:18, max mem: 20.9 GB 
[11/26 14:09:56 visual_prompt]: 	Training 400/553. train loss: 9.0745,	1.2720 s / batch. (data: 4.26e-01). ETA=13:43:53, max mem: 20.9 GB 
[11/26 14:11:36 visual_prompt]: 	Training 500/553. train loss: 1.3446,	1.4915 s / batch. (data: 6.79e-01). ETA=16:03:36, max mem: 20.9 GB 
[11/26 14:12:30 visual_prompt]: Epoch 30 / 100: avg data time: 1.85e-01, avg batch time: 1.0108, average train loss: 4.8709
[11/26 14:13:28 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3070, average loss: 1.7702
[11/26 14:13:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 71.46	
[11/26 14:13:28 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[11/26 14:15:14 visual_prompt]: 	Training 100/553. train loss: 3.3388,	0.8200 s / batch. (data: 3.93e-03). ETA=8:47:40, max mem: 20.9 GB 
[11/26 14:16:55 visual_prompt]: 	Training 200/553. train loss: 5.7669,	0.8099 s / batch. (data: 2.87e-04). ETA=8:39:50, max mem: 20.9 GB 
[11/26 14:18:33 visual_prompt]: 	Training 300/553. train loss: 1.8027,	0.8194 s / batch. (data: 1.20e-02). ETA=8:44:34, max mem: 20.9 GB 
[11/26 14:20:12 visual_prompt]: 	Training 400/553. train loss: 0.7635,	1.4351 s / batch. (data: 5.89e-01). ETA=15:16:17, max mem: 20.9 GB 
[11/26 14:21:52 visual_prompt]: 	Training 500/553. train loss: 4.7992,	0.8688 s / batch. (data: 2.47e-02). ETA=9:13:15, max mem: 20.9 GB 
[11/26 14:22:43 visual_prompt]: Epoch 31 / 100: avg data time: 1.80e-01, avg batch time: 1.0045, average train loss: 5.5309
[11/26 14:23:41 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3060, average loss: 2.6270
[11/26 14:23:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 73.17	
[11/26 14:23:41 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[11/26 14:25:26 visual_prompt]: 	Training 100/553. train loss: 3.3297,	0.8560 s / batch. (data: 7.84e-04). ETA=9:02:58, max mem: 20.9 GB 
[11/26 14:27:06 visual_prompt]: 	Training 200/553. train loss: 0.2150,	0.8429 s / batch. (data: 1.20e-02). ETA=8:53:13, max mem: 20.9 GB 
[11/26 14:28:49 visual_prompt]: 	Training 300/553. train loss: 12.3047,	0.8087 s / batch. (data: 3.00e-04). ETA=8:30:14, max mem: 20.9 GB 
[11/26 14:30:30 visual_prompt]: 	Training 400/553. train loss: 4.1105,	0.8147 s / batch. (data: 6.57e-03). ETA=8:32:39, max mem: 20.9 GB 
[11/26 14:32:07 visual_prompt]: 	Training 500/553. train loss: 3.4288,	0.8240 s / batch. (data: 5.45e-03). ETA=8:37:08, max mem: 20.9 GB 
[11/26 14:32:57 visual_prompt]: Epoch 32 / 100: avg data time: 1.84e-01, avg batch time: 1.0066, average train loss: 4.1184
[11/26 14:33:55 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3080, average loss: 2.0776
[11/26 14:33:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 68.07	
[11/26 14:33:55 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[11/26 14:35:38 visual_prompt]: 	Training 100/553. train loss: 1.1823,	1.1986 s / batch. (data: 3.80e-01). ETA=12:29:11, max mem: 20.9 GB 
[11/26 14:37:20 visual_prompt]: 	Training 200/553. train loss: 3.7484,	1.4366 s / batch. (data: 6.12e-01). ETA=14:55:35, max mem: 20.9 GB 
[11/26 14:38:59 visual_prompt]: 	Training 300/553. train loss: 2.3543,	0.8226 s / batch. (data: 2.95e-04). ETA=8:31:25, max mem: 20.9 GB 
[11/26 14:40:40 visual_prompt]: 	Training 400/553. train loss: 1.3910,	0.8368 s / batch. (data: 7.97e-03). ETA=8:38:51, max mem: 20.9 GB 
[11/26 14:42:20 visual_prompt]: 	Training 500/553. train loss: 5.4955,	0.8240 s / batch. (data: 1.01e-03). ETA=8:29:34, max mem: 20.9 GB 
[11/26 14:43:12 visual_prompt]: Epoch 33 / 100: avg data time: 1.83e-01, avg batch time: 1.0070, average train loss: 5.7833
[11/26 14:44:09 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3061, average loss: 3.3489
[11/26 14:44:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 71.64	
[11/26 14:44:09 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[11/26 14:45:55 visual_prompt]: 	Training 100/553. train loss: 4.8498,	0.8524 s / batch. (data: 2.05e-02). ETA=8:44:56, max mem: 20.9 GB 
[11/26 14:47:33 visual_prompt]: 	Training 200/553. train loss: 13.0847,	0.8266 s / batch. (data: 1.05e-02). ETA=8:27:39, max mem: 20.9 GB 
[11/26 14:49:13 visual_prompt]: 	Training 300/553. train loss: 1.1124,	0.8160 s / batch. (data: 2.99e-04). ETA=8:19:49, max mem: 20.9 GB 
[11/26 14:50:54 visual_prompt]: 	Training 400/553. train loss: 0.3108,	0.8110 s / batch. (data: 2.97e-04). ETA=8:15:25, max mem: 20.9 GB 
[11/26 14:52:34 visual_prompt]: 	Training 500/553. train loss: 2.7579,	1.5018 s / batch. (data: 6.62e-01). ETA=15:14:53, max mem: 20.9 GB 
[11/26 14:53:26 visual_prompt]: Epoch 34 / 100: avg data time: 1.83e-01, avg batch time: 1.0061, average train loss: 4.3912
[11/26 14:54:23 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3098, average loss: 2.7150
[11/26 14:54:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 74.03	
[11/26 14:54:23 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[11/26 14:56:09 visual_prompt]: 	Training 100/553. train loss: 1.7249,	0.8304 s / batch. (data: 3.20e-04). ETA=8:23:43, max mem: 20.9 GB 
[11/26 14:57:50 visual_prompt]: 	Training 200/553. train loss: 6.6556,	0.8360 s / batch. (data: 3.27e-04). ETA=8:25:45, max mem: 20.9 GB 
[11/26 14:59:28 visual_prompt]: 	Training 300/553. train loss: 13.3379,	0.8160 s / batch. (data: 2.85e-04). ETA=8:12:18, max mem: 20.9 GB 
[11/26 15:01:07 visual_prompt]: 	Training 400/553. train loss: 14.0743,	0.8596 s / batch. (data: 3.92e-02). ETA=8:37:09, max mem: 20.9 GB 
[11/26 15:02:46 visual_prompt]: 	Training 500/553. train loss: 2.7219,	1.1946 s / batch. (data: 3.76e-01). ETA=11:56:43, max mem: 20.9 GB 
[11/26 15:03:39 visual_prompt]: Epoch 35 / 100: avg data time: 1.81e-01, avg batch time: 1.0048, average train loss: 5.7482
[11/26 15:04:36 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3068, average loss: 6.5095
[11/26 15:04:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 71.10	
[11/26 15:04:36 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[11/26 15:06:19 visual_prompt]: 	Training 100/553. train loss: 14.5993,	0.8240 s / batch. (data: 3.39e-04). ETA=8:12:15, max mem: 20.9 GB 
[11/26 15:08:00 visual_prompt]: 	Training 200/553. train loss: 6.4547,	0.8320 s / batch. (data: 3.15e-04). ETA=8:15:40, max mem: 20.9 GB 
[11/26 15:09:42 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8196 s / batch. (data: 1.16e-02). ETA=8:06:55, max mem: 20.9 GB 
[11/26 15:11:22 visual_prompt]: 	Training 400/553. train loss: 2.0341,	0.8223 s / batch. (data: 3.12e-04). ETA=8:07:08, max mem: 20.9 GB 
[11/26 15:13:03 visual_prompt]: 	Training 500/553. train loss: 4.2121,	0.9364 s / batch. (data: 1.30e-01). ETA=9:13:10, max mem: 20.9 GB 
[11/26 15:13:52 visual_prompt]: Epoch 36 / 100: avg data time: 1.81e-01, avg batch time: 1.0050, average train loss: 5.4749
[11/26 15:14:49 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3062, average loss: 11.8365
[11/26 15:14:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.14	
[11/26 15:14:49 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[11/26 15:16:34 visual_prompt]: 	Training 100/553. train loss: 2.6765,	0.8458 s / batch. (data: 6.70e-03). ETA=8:17:28, max mem: 20.9 GB 
[11/26 15:18:14 visual_prompt]: 	Training 200/553. train loss: 4.3420,	0.8273 s / batch. (data: 2.94e-04). ETA=8:05:14, max mem: 20.9 GB 
[11/26 15:19:54 visual_prompt]: 	Training 300/553. train loss: 2.0901,	1.5225 s / batch. (data: 6.94e-01). ETA=14:50:28, max mem: 20.9 GB 
[11/26 15:21:37 visual_prompt]: 	Training 400/553. train loss: 2.6582,	2.0133 s / batch. (data: 1.20e+00). ETA=19:34:08, max mem: 20.9 GB 
[11/26 15:23:13 visual_prompt]: 	Training 500/553. train loss: 6.6427,	1.0695 s / batch. (data: 2.22e-01). ETA=10:21:58, max mem: 20.9 GB 
[11/26 15:24:07 visual_prompt]: Epoch 37 / 100: avg data time: 1.85e-01, avg batch time: 1.0081, average train loss: 5.3824
[11/26 15:25:04 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3052, average loss: 3.0341
[11/26 15:25:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 72.71	
[11/26 15:25:04 visual_prompt]: Stopping early.
[11/26 15:25:05 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 15:25:05 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 15:25:05 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/26 15:25:05 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 15:25:05 visual_prompt]: Training with config:
[11/26 15:25:05 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/26 15:25:05 visual_prompt]: Loading training data...
[11/26 15:25:05 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 15:25:05 visual_prompt]: Loading validation data...
[11/26 15:25:05 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 15:25:05 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 15:25:07 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 15:25:07 visual_prompt]: tuned percent:0.525
[11/26 15:25:07 visual_prompt]: Device used for model: 0
[11/26 15:25:07 visual_prompt]: Setting up Evaluator...
[11/26 15:25:07 visual_prompt]: Setting up Trainer...
[11/26 15:25:07 visual_prompt]: 	Setting up the optimizer...
[11/26 15:25:08 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 15:26:51 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8200 s / batch. (data: 2.99e-04). ETA=12:34:23, max mem: 20.9 GB 
[11/26 15:28:30 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8199 s / batch. (data: 4.17e-04). ETA=12:32:58, max mem: 20.9 GB 
[11/26 15:30:13 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.9668 s / batch. (data: 1.14e+00). ETA=1 day, 6:02:52, max mem: 20.9 GB 
[11/26 15:31:51 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8280 s / batch. (data: 7.95e-03). ETA=12:37:35, max mem: 20.9 GB 
[11/26 15:33:34 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8218 s / batch. (data: 2.83e-04). ETA=12:30:34, max mem: 20.9 GB 
[11/26 15:34:27 visual_prompt]: Epoch 1 / 100: avg data time: 1.87e-01, avg batch time: 1.0108, average train loss: 1.5403
[11/26 15:35:24 visual_prompt]: Inference (val):avg data time: 2.10e-04, avg batch time: 0.3079, average loss: 1.5201
[11/26 15:35:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 15:35:24 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/26 15:37:07 visual_prompt]: 	Training 100/553. train loss: 1.3178,	0.8279 s / batch. (data: 1.05e-02). ETA=12:34:04, max mem: 20.9 GB 
[11/26 15:38:47 visual_prompt]: 	Training 200/553. train loss: 0.2776,	0.8320 s / batch. (data: 3.35e-04). ETA=12:36:23, max mem: 20.9 GB 
[11/26 15:40:29 visual_prompt]: 	Training 300/553. train loss: 1.1687,	1.1058 s / batch. (data: 2.73e-01). ETA=16:43:26, max mem: 20.9 GB 
[11/26 15:42:08 visual_prompt]: 	Training 400/553. train loss: 1.4125,	0.8190 s / batch. (data: 3.18e-04). ETA=12:21:50, max mem: 20.9 GB 
[11/26 15:43:50 visual_prompt]: 	Training 500/553. train loss: 0.5618,	0.8271 s / batch. (data: 6.92e-03). ETA=12:27:49, max mem: 20.9 GB 
[11/26 15:44:41 visual_prompt]: Epoch 2 / 100: avg data time: 1.83e-01, avg batch time: 1.0065, average train loss: 1.0792
[11/26 15:45:38 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3081, average loss: 2.9495
[11/26 15:45:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.70	
[11/26 15:45:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/26 15:47:21 visual_prompt]: 	Training 100/553. train loss: 1.1325,	1.0200 s / batch. (data: 1.80e-01). ETA=15:19:35, max mem: 20.9 GB 
[11/26 15:49:02 visual_prompt]: 	Training 200/553. train loss: 0.7032,	0.8120 s / batch. (data: 2.91e-04). ETA=12:10:42, max mem: 20.9 GB 
[11/26 15:50:41 visual_prompt]: 	Training 300/553. train loss: 0.8751,	0.8520 s / batch. (data: 1.19e-02). ETA=12:45:17, max mem: 20.9 GB 
[11/26 15:52:22 visual_prompt]: 	Training 400/553. train loss: 0.4530,	0.8166 s / batch. (data: 3.01e-04). ETA=12:12:08, max mem: 20.9 GB 
[11/26 15:54:04 visual_prompt]: 	Training 500/553. train loss: 0.7412,	1.3520 s / batch. (data: 5.31e-01). ETA=20:09:54, max mem: 20.9 GB 
[11/26 15:54:55 visual_prompt]: Epoch 3 / 100: avg data time: 1.82e-01, avg batch time: 1.0064, average train loss: 1.4825
[11/26 15:55:52 visual_prompt]: Inference (val):avg data time: 3.86e-04, avg batch time: 0.3065, average loss: 1.3046
[11/26 15:55:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.65	
[11/26 15:55:52 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/26 15:57:38 visual_prompt]: 	Training 100/553. train loss: 2.1249,	0.8680 s / batch. (data: 2.96e-04). ETA=12:54:33, max mem: 20.9 GB 
[11/26 15:59:18 visual_prompt]: 	Training 200/553. train loss: 3.7853,	0.8172 s / batch. (data: 5.44e-03). ETA=12:07:50, max mem: 20.9 GB 
[11/26 16:00:59 visual_prompt]: 	Training 300/553. train loss: 0.7608,	1.5646 s / batch. (data: 7.42e-01). ETA=23:10:57, max mem: 20.9 GB 
[11/26 16:02:34 visual_prompt]: 	Training 400/553. train loss: 0.5434,	1.0840 s / batch. (data: 2.60e-01). ETA=16:01:54, max mem: 20.9 GB 
[11/26 16:04:17 visual_prompt]: 	Training 500/553. train loss: 0.2530,	3.6694 s / batch. (data: 2.85e+00). ETA=2 days, 6:09:52, max mem: 20.9 GB 
[11/26 16:05:10 visual_prompt]: Epoch 4 / 100: avg data time: 1.86e-01, avg batch time: 1.0085, average train loss: 2.4336
[11/26 16:06:07 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3065, average loss: 0.9127
[11/26 16:06:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.69	
[11/26 16:06:07 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/26 16:07:50 visual_prompt]: 	Training 100/553. train loss: 0.0051,	0.8498 s / batch. (data: 1.05e-02). ETA=12:30:31, max mem: 20.9 GB 
[11/26 16:09:30 visual_prompt]: 	Training 200/553. train loss: 1.9763,	1.3831 s / batch. (data: 5.71e-01). ETA=20:19:09, max mem: 20.9 GB 
[11/26 16:11:12 visual_prompt]: 	Training 300/553. train loss: 1.9640,	0.8454 s / batch. (data: 9.34e-03). ETA=12:23:44, max mem: 20.9 GB 
[11/26 16:12:51 visual_prompt]: 	Training 400/553. train loss: 0.7200,	0.8304 s / batch. (data: 2.98e-04). ETA=12:09:11, max mem: 20.9 GB 
[11/26 16:14:31 visual_prompt]: 	Training 500/553. train loss: 0.5754,	0.8247 s / batch. (data: 7.96e-03). ETA=12:02:49, max mem: 20.9 GB 
[11/26 16:15:25 visual_prompt]: Epoch 5 / 100: avg data time: 1.84e-01, avg batch time: 1.0080, average train loss: 2.6534
[11/26 16:16:22 visual_prompt]: Inference (val):avg data time: 2.19e-04, avg batch time: 0.3075, average loss: 5.0650
[11/26 16:16:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.42	
[11/26 16:16:22 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/26 16:18:08 visual_prompt]: 	Training 100/553. train loss: 2.6954,	0.8404 s / batch. (data: 8.16e-04). ETA=12:14:24, max mem: 20.9 GB 
[11/26 16:19:48 visual_prompt]: 	Training 200/553. train loss: 6.5801,	0.8320 s / batch. (data: 3.07e-04). ETA=12:05:41, max mem: 20.9 GB 
[11/26 16:21:27 visual_prompt]: 	Training 300/553. train loss: 5.0656,	0.8349 s / batch. (data: 1.05e-02). ETA=12:06:52, max mem: 20.9 GB 
[11/26 16:23:11 visual_prompt]: 	Training 400/553. train loss: 1.3784,	0.8226 s / batch. (data: 7.94e-03). ETA=11:54:45, max mem: 20.9 GB 
[11/26 16:24:49 visual_prompt]: 	Training 500/553. train loss: 0.1031,	0.8320 s / batch. (data: 7.99e-03). ETA=12:01:33, max mem: 20.9 GB 
[11/26 16:25:41 visual_prompt]: Epoch 6 / 100: avg data time: 1.88e-01, avg batch time: 1.0111, average train loss: 3.5055
[11/26 16:26:39 visual_prompt]: Inference (val):avg data time: 2.19e-04, avg batch time: 0.3067, average loss: 6.6380
[11/26 16:26:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 37.33	
[11/26 16:26:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/26 16:28:22 visual_prompt]: 	Training 100/553. train loss: 12.1488,	0.8480 s / batch. (data: 7.95e-03). ETA=12:13:15, max mem: 20.9 GB 
[11/26 16:30:03 visual_prompt]: 	Training 200/553. train loss: 1.0307,	0.8258 s / batch. (data: 3.22e-04). ETA=11:52:41, max mem: 20.9 GB 
[11/26 16:31:47 visual_prompt]: 	Training 300/553. train loss: 3.5153,	1.8548 s / batch. (data: 1.02e+00). ETA=1 day, 2:37:38, max mem: 20.9 GB 
[11/26 16:33:27 visual_prompt]: 	Training 400/553. train loss: 2.3256,	1.9269 s / batch. (data: 1.11e+00). ETA=1 day, 3:36:33, max mem: 20.9 GB 
[11/26 16:35:05 visual_prompt]: 	Training 500/553. train loss: 3.2859,	0.8279 s / batch. (data: 3.09e-04). ETA=11:50:23, max mem: 20.9 GB 
[11/26 16:35:56 visual_prompt]: Epoch 7 / 100: avg data time: 1.84e-01, avg batch time: 1.0071, average train loss: 4.0773
[11/26 16:36:54 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3066, average loss: 2.2820
[11/26 16:36:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.19	
[11/26 16:36:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/26 16:38:36 visual_prompt]: 	Training 100/553. train loss: 2.5110,	0.8360 s / batch. (data: 3.33e-04). ETA=11:55:10, max mem: 20.9 GB 
[11/26 16:40:17 visual_prompt]: 	Training 200/553. train loss: 3.3374,	1.0360 s / batch. (data: 2.07e-01). ETA=14:44:32, max mem: 20.9 GB 
[11/26 16:41:59 visual_prompt]: 	Training 300/553. train loss: 2.0184,	0.8231 s / batch. (data: 3.06e-04). ETA=11:41:24, max mem: 20.9 GB 
[11/26 16:43:40 visual_prompt]: 	Training 400/553. train loss: 0.7383,	1.0508 s / batch. (data: 2.23e-01). ETA=14:53:38, max mem: 20.9 GB 
[11/26 16:45:20 visual_prompt]: 	Training 500/553. train loss: 2.7546,	1.4027 s / batch. (data: 5.73e-01). ETA=19:50:38, max mem: 20.9 GB 
[11/26 16:46:13 visual_prompt]: Epoch 8 / 100: avg data time: 1.87e-01, avg batch time: 1.0106, average train loss: 5.1077
[11/26 16:47:10 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3064, average loss: 0.8597
[11/26 16:47:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.15	
[11/26 16:47:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/26 16:48:54 visual_prompt]: 	Training 100/553. train loss: 4.1649,	0.8085 s / batch. (data: 2.93e-04). ETA=11:24:11, max mem: 20.9 GB 
[11/26 16:50:33 visual_prompt]: 	Training 200/553. train loss: 0.6673,	0.8176 s / batch. (data: 2.86e-04). ETA=11:30:31, max mem: 20.9 GB 
[11/26 16:52:15 visual_prompt]: 	Training 300/553. train loss: 7.3893,	1.7014 s / batch. (data: 8.83e-01). ETA=23:54:09, max mem: 20.9 GB 
[11/26 16:53:55 visual_prompt]: 	Training 400/553. train loss: 0.6317,	0.8096 s / batch. (data: 2.55e-04). ETA=11:21:07, max mem: 20.9 GB 
[11/26 16:55:37 visual_prompt]: 	Training 500/553. train loss: 3.4955,	0.9321 s / batch. (data: 1.21e-01). ETA=13:02:33, max mem: 20.9 GB 
[11/26 16:56:28 visual_prompt]: Epoch 9 / 100: avg data time: 1.86e-01, avg batch time: 1.0090, average train loss: 5.4163
[11/26 16:57:25 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3085, average loss: 1.2256
[11/26 16:57:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.59	
[11/26 16:57:25 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/26 16:59:12 visual_prompt]: 	Training 100/553. train loss: 28.6720,	0.8280 s / batch. (data: 3.00e-04). ETA=11:33:04, max mem: 20.9 GB 
[11/26 17:00:51 visual_prompt]: 	Training 200/553. train loss: 5.4005,	0.8320 s / batch. (data: 5.45e-03). ETA=11:35:01, max mem: 20.9 GB 
[11/26 17:02:30 visual_prompt]: 	Training 300/553. train loss: 17.1056,	0.8177 s / batch. (data: 1.05e-02). ETA=11:21:45, max mem: 20.9 GB 
[11/26 17:04:09 visual_prompt]: 	Training 400/553. train loss: 6.5896,	0.8560 s / batch. (data: 3.14e-03). ETA=11:52:14, max mem: 20.9 GB 
[11/26 17:05:50 visual_prompt]: 	Training 500/553. train loss: 0.4526,	1.0560 s / batch. (data: 2.40e-01). ETA=14:36:52, max mem: 20.9 GB 
[11/26 17:06:43 visual_prompt]: Epoch 10 / 100: avg data time: 1.84e-01, avg batch time: 1.0085, average train loss: 7.0842
[11/26 17:07:40 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3080, average loss: 1.9893
[11/26 17:07:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.73	
[11/26 17:07:40 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/26 17:09:27 visual_prompt]: 	Training 100/553. train loss: 2.3298,	0.8328 s / batch. (data: 2.90e-04). ETA=11:29:22, max mem: 20.9 GB 
[11/26 17:11:08 visual_prompt]: 	Training 200/553. train loss: 27.7675,	0.8280 s / batch. (data: 2.86e-04). ETA=11:24:05, max mem: 20.9 GB 
[11/26 17:12:48 visual_prompt]: 	Training 300/553. train loss: 34.4139,	2.0383 s / batch. (data: 1.23e+00). ETA=1 day, 4:00:35, max mem: 20.9 GB 
[11/26 17:14:26 visual_prompt]: 	Training 400/553. train loss: 3.9413,	0.8436 s / batch. (data: 2.17e-02). ETA=11:34:08, max mem: 20.9 GB 
[11/26 17:16:06 visual_prompt]: 	Training 500/553. train loss: 1.0671,	0.8240 s / batch. (data: 3.04e-04). ETA=11:16:40, max mem: 20.9 GB 
[11/26 17:16:57 visual_prompt]: Epoch 11 / 100: avg data time: 1.83e-01, avg batch time: 1.0063, average train loss: 7.7834
[11/26 17:17:54 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3071, average loss: 0.8970
[11/26 17:17:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.27	
[11/26 17:17:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/26 17:19:40 visual_prompt]: 	Training 100/553. train loss: 8.2900,	0.8155 s / batch. (data: 8.75e-03). ETA=11:07:34, max mem: 20.9 GB 
[11/26 17:21:21 visual_prompt]: 	Training 200/553. train loss: 4.5576,	1.2320 s / batch. (data: 3.94e-01). ETA=16:46:29, max mem: 20.9 GB 
[11/26 17:23:00 visual_prompt]: 	Training 300/553. train loss: 6.9376,	0.8492 s / batch. (data: 2.52e-02). ETA=11:32:22, max mem: 20.9 GB 
[11/26 17:24:40 visual_prompt]: 	Training 400/553. train loss: 4.4879,	0.8080 s / batch. (data: 3.24e-04). ETA=10:57:22, max mem: 20.9 GB 
[11/26 17:26:21 visual_prompt]: 	Training 500/553. train loss: 0.7148,	0.8532 s / batch. (data: 5.42e-03). ETA=11:32:45, max mem: 20.9 GB 
[11/26 17:27:12 visual_prompt]: Epoch 12 / 100: avg data time: 1.84e-01, avg batch time: 1.0076, average train loss: 7.2357
[11/26 17:28:09 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3071, average loss: 3.4477
[11/26 17:28:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.88	
[11/26 17:28:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/26 17:29:55 visual_prompt]: 	Training 100/553. train loss: 3.1823,	0.8706 s / batch. (data: 2.27e-02). ETA=11:44:40, max mem: 20.9 GB 
[11/26 17:31:32 visual_prompt]: 	Training 200/553. train loss: 5.5554,	0.8082 s / batch. (data: 3.00e-04). ETA=10:52:46, max mem: 20.9 GB 
[11/26 17:33:14 visual_prompt]: 	Training 300/553. train loss: 0.9770,	1.8723 s / batch. (data: 1.05e+00). ETA=1 day, 1:09:09, max mem: 20.9 GB 
[11/26 17:34:53 visual_prompt]: 	Training 400/553. train loss: 24.2262,	0.8400 s / batch. (data: 7.95e-03). ETA=11:15:42, max mem: 20.9 GB 
[11/26 17:36:34 visual_prompt]: 	Training 500/553. train loss: 4.8191,	0.8160 s / batch. (data: 3.16e-04). ETA=10:55:02, max mem: 20.9 GB 
[11/26 17:37:26 visual_prompt]: Epoch 13 / 100: avg data time: 1.84e-01, avg batch time: 1.0074, average train loss: 7.4968
[11/26 17:38:24 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3073, average loss: 4.0477
[11/26 17:38:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.12	
[11/26 17:38:24 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/26 17:40:09 visual_prompt]: 	Training 100/553. train loss: 3.8470,	0.8161 s / batch. (data: 3.03e-04). ETA=10:53:03, max mem: 20.9 GB 
[11/26 17:41:48 visual_prompt]: 	Training 200/553. train loss: 31.0490,	1.1609 s / batch. (data: 3.41e-01). ETA=15:26:57, max mem: 20.9 GB 
[11/26 17:43:29 visual_prompt]: 	Training 300/553. train loss: 3.5751,	0.8246 s / batch. (data: 5.76e-03). ETA=10:57:06, max mem: 20.9 GB 
[11/26 17:45:08 visual_prompt]: 	Training 400/553. train loss: 1.4584,	0.8456 s / batch. (data: 2.92e-04). ETA=11:12:23, max mem: 20.9 GB 
[11/26 17:46:49 visual_prompt]: 	Training 500/553. train loss: 5.5519,	0.8360 s / batch. (data: 7.95e-03). ETA=11:03:23, max mem: 20.9 GB 
[11/26 17:47:39 visual_prompt]: Epoch 14 / 100: avg data time: 1.81e-01, avg batch time: 1.0049, average train loss: 6.7657
[11/26 17:48:37 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3066, average loss: 1.5707
[11/26 17:48:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.87	
[11/26 17:48:37 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/26 17:50:21 visual_prompt]: 	Training 100/553. train loss: 8.5192,	0.8380 s / batch. (data: 3.04e-02). ETA=11:02:49, max mem: 20.9 GB 
[11/26 17:51:59 visual_prompt]: 	Training 200/553. train loss: 7.0463,	0.8125 s / batch. (data: 2.87e-04). ETA=10:41:16, max mem: 20.9 GB 
[11/26 17:53:41 visual_prompt]: 	Training 300/553. train loss: 14.5544,	0.8181 s / batch. (data: 3.05e-04). ETA=10:44:21, max mem: 20.9 GB 
[11/26 17:55:19 visual_prompt]: 	Training 400/553. train loss: 6.5952,	0.9440 s / batch. (data: 9.50e-02). ETA=12:21:56, max mem: 20.9 GB 
[11/26 17:57:00 visual_prompt]: 	Training 500/553. train loss: 8.2057,	0.8722 s / batch. (data: 5.44e-03). ETA=11:24:04, max mem: 20.9 GB 
[11/26 17:57:53 visual_prompt]: Epoch 15 / 100: avg data time: 1.82e-01, avg batch time: 1.0049, average train loss: 8.7889
[11/26 17:58:50 visual_prompt]: Inference (val):avg data time: 4.78e-04, avg batch time: 0.3087, average loss: 2.6206
[11/26 17:58:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.29	
[11/26 17:58:50 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/26 18:00:33 visual_prompt]: 	Training 100/553. train loss: 2.7006,	0.8264 s / batch. (data: 3.00e-04). ETA=10:46:03, max mem: 20.9 GB 
[11/26 18:02:13 visual_prompt]: 	Training 200/553. train loss: 13.8312,	0.8400 s / batch. (data: 3.00e-04). ETA=10:55:15, max mem: 20.9 GB 
[11/26 18:03:54 visual_prompt]: 	Training 300/553. train loss: 1.7735,	0.8200 s / batch. (data: 3.08e-04). ETA=10:38:17, max mem: 20.9 GB 
[11/26 18:05:34 visual_prompt]: 	Training 400/553. train loss: 2.1459,	0.8178 s / batch. (data: 1.04e-02). ETA=10:35:15, max mem: 20.9 GB 
[11/26 18:07:13 visual_prompt]: 	Training 500/553. train loss: 13.0191,	1.0040 s / batch. (data: 1.69e-01). ETA=12:58:10, max mem: 20.9 GB 
[11/26 18:08:06 visual_prompt]: Epoch 16 / 100: avg data time: 1.83e-01, avg batch time: 1.0056, average train loss: 8.9952
[11/26 18:09:04 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3065, average loss: 15.6493
[11/26 18:09:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.58	
[11/26 18:09:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/26 18:10:47 visual_prompt]: 	Training 100/553. train loss: 3.8247,	0.8315 s / batch. (data: 2.92e-04). ETA=10:42:23, max mem: 20.9 GB 
[11/26 18:12:28 visual_prompt]: 	Training 200/553. train loss: 1.4259,	0.8160 s / batch. (data: 4.26e-04). ETA=10:28:59, max mem: 20.9 GB 
[11/26 18:14:07 visual_prompt]: 	Training 300/553. train loss: 10.5930,	0.8120 s / batch. (data: 2.74e-04). ETA=10:24:36, max mem: 20.9 GB 
[11/26 18:15:48 visual_prompt]: 	Training 400/553. train loss: 14.6420,	1.1720 s / batch. (data: 3.49e-01). ETA=14:59:33, max mem: 20.9 GB 
[11/26 18:17:28 visual_prompt]: 	Training 500/553. train loss: 5.6674,	1.7610 s / batch. (data: 9.43e-01). ETA=22:28:41, max mem: 20.9 GB 
[11/26 18:18:21 visual_prompt]: Epoch 17 / 100: avg data time: 1.85e-01, avg batch time: 1.0085, average train loss: 7.7397
[11/26 18:19:19 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3083, average loss: 1.6430
[11/26 18:19:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.38	
[11/26 18:19:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/26 18:21:03 visual_prompt]: 	Training 100/553. train loss: 17.1106,	0.8440 s / batch. (data: 1.20e-02). ETA=10:44:14, max mem: 20.9 GB 
[11/26 18:22:46 visual_prompt]: 	Training 200/553. train loss: 8.3440,	0.8457 s / batch. (data: 7.14e-04). ETA=10:44:05, max mem: 20.9 GB 
[11/26 18:24:25 visual_prompt]: 	Training 300/553. train loss: 0.9150,	0.8216 s / batch. (data: 3.09e-04). ETA=10:24:22, max mem: 20.9 GB 
[11/26 18:26:06 visual_prompt]: 	Training 400/553. train loss: 4.5661,	0.8080 s / batch. (data: 3.09e-04). ETA=10:12:42, max mem: 20.9 GB 
[11/26 18:27:45 visual_prompt]: 	Training 500/553. train loss: 0.8066,	0.8063 s / batch. (data: 3.01e-04). ETA=10:10:06, max mem: 20.9 GB 
[11/26 18:28:36 visual_prompt]: Epoch 18 / 100: avg data time: 1.82e-01, avg batch time: 1.0069, average train loss: 6.6641
[11/26 18:29:33 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3072, average loss: 4.8269
[11/26 18:29:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.34	
[11/26 18:29:33 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/26 18:31:17 visual_prompt]: 	Training 100/553. train loss: 30.8825,	0.8160 s / batch. (data: 2.92e-04). ETA=10:15:20, max mem: 20.9 GB 
[11/26 18:32:58 visual_prompt]: 	Training 200/553. train loss: 9.3120,	0.8302 s / batch. (data: 3.07e-04). ETA=10:24:41, max mem: 20.9 GB 
[11/26 18:34:38 visual_prompt]: 	Training 300/553. train loss: 0.0079,	0.8323 s / batch. (data: 1.05e-02). ETA=10:24:52, max mem: 20.9 GB 
[11/26 18:36:19 visual_prompt]: 	Training 400/553. train loss: 5.2936,	0.8258 s / batch. (data: 3.08e-04). ETA=10:18:36, max mem: 20.9 GB 
[11/26 18:37:55 visual_prompt]: 	Training 500/553. train loss: 12.5619,	0.8510 s / batch. (data: 1.10e-02). ETA=10:36:05, max mem: 20.9 GB 
[11/26 18:38:48 visual_prompt]: Epoch 19 / 100: avg data time: 1.79e-01, avg batch time: 1.0030, average train loss: 7.0264
[11/26 18:39:46 visual_prompt]: Inference (val):avg data time: 2.14e-04, avg batch time: 0.3175, average loss: 26.7086
[11/26 18:39:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.51	
[11/26 18:39:46 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/26 18:41:29 visual_prompt]: 	Training 100/553. train loss: 0.7665,	0.8320 s / batch. (data: 3.12e-04). ETA=10:19:44, max mem: 20.9 GB 
[11/26 18:43:10 visual_prompt]: 	Training 200/553. train loss: 1.3021,	0.8307 s / batch. (data: 1.47e-02). ETA=10:17:23, max mem: 20.9 GB 
[11/26 18:44:50 visual_prompt]: 	Training 300/553. train loss: 2.7288,	0.8626 s / batch. (data: 3.27e-04). ETA=10:39:37, max mem: 20.9 GB 
[11/26 18:46:30 visual_prompt]: 	Training 400/553. train loss: 0.7817,	0.8230 s / batch. (data: 7.94e-03). ETA=10:08:54, max mem: 20.9 GB 
[11/26 18:48:09 visual_prompt]: 	Training 500/553. train loss: 8.7915,	0.8239 s / batch. (data: 4.46e-04). ETA=10:08:13, max mem: 20.9 GB 
[11/26 18:49:03 visual_prompt]: Epoch 20 / 100: avg data time: 1.84e-01, avg batch time: 1.0079, average train loss: 7.5887
[11/26 18:50:01 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3066, average loss: 5.7085
[11/26 18:50:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.41	
[11/26 18:50:01 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/26 18:51:47 visual_prompt]: 	Training 100/553. train loss: 25.1690,	0.8309 s / batch. (data: 5.48e-03). ETA=10:11:15, max mem: 20.9 GB 
[11/26 18:53:26 visual_prompt]: 	Training 200/553. train loss: 19.9447,	0.8213 s / batch. (data: 1.19e-02). ETA=10:02:51, max mem: 20.9 GB 
[11/26 18:55:06 visual_prompt]: 	Training 300/553. train loss: 59.5267,	1.2114 s / batch. (data: 3.74e-01). ETA=14:47:07, max mem: 20.9 GB 
[11/26 18:56:45 visual_prompt]: 	Training 400/553. train loss: 2.0890,	0.8480 s / batch. (data: 2.85e-04). ETA=10:19:36, max mem: 20.9 GB 
[11/26 18:58:26 visual_prompt]: 	Training 500/553. train loss: 11.5200,	0.8067 s / batch. (data: 3.01e-04). ETA=9:48:04, max mem: 20.9 GB 
[11/26 18:59:18 visual_prompt]: Epoch 21 / 100: avg data time: 1.83e-01, avg batch time: 1.0072, average train loss: 8.5933
[11/26 19:00:15 visual_prompt]: Inference (val):avg data time: 2.93e-04, avg batch time: 0.3088, average loss: 11.3172
[11/26 19:00:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.94	
[11/26 19:00:15 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/26 19:01:59 visual_prompt]: 	Training 100/553. train loss: 3.0353,	0.8619 s / batch. (data: 2.98e-02). ETA=10:26:05, max mem: 20.9 GB 
[11/26 19:03:38 visual_prompt]: 	Training 200/553. train loss: 0.6691,	0.8296 s / batch. (data: 2.89e-04). ETA=10:01:16, max mem: 20.9 GB 
[11/26 19:05:16 visual_prompt]: 	Training 300/553. train loss: 4.7423,	0.8360 s / batch. (data: 3.19e-04). ETA=10:04:31, max mem: 20.9 GB 
[11/26 19:06:57 visual_prompt]: 	Training 400/553. train loss: 11.0472,	0.8240 s / batch. (data: 2.95e-04). ETA=9:54:30, max mem: 20.9 GB 
[11/26 19:08:37 visual_prompt]: 	Training 500/553. train loss: 3.0639,	0.8302 s / batch. (data: 1.01e-02). ETA=9:57:32, max mem: 20.9 GB 
[11/26 19:09:31 visual_prompt]: Epoch 22 / 100: avg data time: 1.81e-01, avg batch time: 1.0046, average train loss: 6.5814
[11/26 19:10:28 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3072, average loss: 4.6498
[11/26 19:10:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.24	
[11/26 19:10:28 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/26 19:12:13 visual_prompt]: 	Training 100/553. train loss: 8.9007,	0.8131 s / batch. (data: 5.43e-03). ETA=9:43:11, max mem: 20.9 GB 
[11/26 19:13:54 visual_prompt]: 	Training 200/553. train loss: 0.6759,	0.8320 s / batch. (data: 7.55e-04). ETA=9:55:22, max mem: 20.9 GB 
[11/26 19:15:36 visual_prompt]: 	Training 300/553. train loss: 4.5584,	0.8320 s / batch. (data: 7.57e-04). ETA=9:53:57, max mem: 20.9 GB 
[11/26 19:17:14 visual_prompt]: 	Training 400/553. train loss: 2.1760,	0.8080 s / batch. (data: 2.74e-04). ETA=9:35:29, max mem: 20.9 GB 
[11/26 19:18:52 visual_prompt]: 	Training 500/553. train loss: 30.7379,	0.8560 s / batch. (data: 1.20e-02). ETA=10:08:14, max mem: 20.9 GB 
[11/26 19:19:44 visual_prompt]: Epoch 23 / 100: avg data time: 1.82e-01, avg batch time: 1.0057, average train loss: 7.8208
[11/26 19:20:42 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3082, average loss: 6.2168
[11/26 19:20:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.19	
[11/26 19:20:42 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/26 19:22:23 visual_prompt]: 	Training 100/553. train loss: 0.4136,	0.8240 s / batch. (data: 5.45e-03). ETA=9:43:25, max mem: 20.9 GB 
[11/26 19:24:03 visual_prompt]: 	Training 200/553. train loss: 1.0560,	0.8400 s / batch. (data: 8.94e-04). ETA=9:53:18, max mem: 20.9 GB 
[11/26 19:25:43 visual_prompt]: 	Training 300/553. train loss: 18.3757,	1.0944 s / batch. (data: 2.76e-01). ETA=12:51:10, max mem: 20.9 GB 
[11/26 19:27:24 visual_prompt]: 	Training 400/553. train loss: 14.4224,	0.8339 s / batch. (data: 3.04e-04). ETA=9:46:15, max mem: 20.9 GB 
[11/26 19:29:05 visual_prompt]: 	Training 500/553. train loss: 1.6670,	0.8343 s / batch. (data: 3.07e-04). ETA=9:45:08, max mem: 20.9 GB 
[11/26 19:29:59 visual_prompt]: Epoch 24 / 100: avg data time: 1.84e-01, avg batch time: 1.0070, average train loss: 7.9286
[11/26 19:30:56 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3074, average loss: 0.6890
[11/26 19:30:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[11/26 19:30:56 visual_prompt]: Best epoch 24: best metric: -0.689
[11/26 19:30:56 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/26 19:32:43 visual_prompt]: 	Training 100/553. train loss: 3.0734,	0.8320 s / batch. (data: 3.40e-04). ETA=9:41:24, max mem: 20.9 GB 
[11/26 19:34:20 visual_prompt]: 	Training 200/553. train loss: 8.3300,	0.9860 s / batch. (data: 1.63e-01). ETA=11:27:24, max mem: 20.9 GB 
[11/26 19:36:00 visual_prompt]: 	Training 300/553. train loss: 16.9463,	0.8442 s / batch. (data: 3.39e-02). ETA=9:47:04, max mem: 20.9 GB 
[11/26 19:37:40 visual_prompt]: 	Training 400/553. train loss: 2.5061,	1.1960 s / batch. (data: 3.79e-01). ETA=13:49:45, max mem: 20.9 GB 
[11/26 19:39:21 visual_prompt]: 	Training 500/553. train loss: 3.7947,	1.5534 s / batch. (data: 7.38e-01). ETA=17:55:09, max mem: 20.9 GB 
[11/26 19:40:12 visual_prompt]: Epoch 25 / 100: avg data time: 1.83e-01, avg batch time: 1.0063, average train loss: 5.8817
[11/26 19:41:10 visual_prompt]: Inference (val):avg data time: 2.15e-04, avg batch time: 0.3076, average loss: 5.3982
[11/26 19:41:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.12	
[11/26 19:41:10 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/26 19:42:54 visual_prompt]: 	Training 100/553. train loss: 2.4094,	0.8241 s / batch. (data: 5.40e-03). ETA=9:28:17, max mem: 20.9 GB 
[11/26 19:44:35 visual_prompt]: 	Training 200/553. train loss: 18.5704,	1.7823 s / batch. (data: 9.76e-01). ETA=20:26:03, max mem: 20.9 GB 
[11/26 19:46:16 visual_prompt]: 	Training 300/553. train loss: 1.3351,	0.8070 s / batch. (data: 3.17e-04). ETA=9:13:47, max mem: 20.9 GB 
[11/26 19:47:55 visual_prompt]: 	Training 400/553. train loss: 13.1680,	0.8333 s / batch. (data: 5.40e-03). ETA=9:30:27, max mem: 20.9 GB 
[11/26 19:49:34 visual_prompt]: 	Training 500/553. train loss: 0.8563,	0.8376 s / batch. (data: 1.05e-02). ETA=9:32:01, max mem: 20.9 GB 
[11/26 19:50:26 visual_prompt]: Epoch 26 / 100: avg data time: 1.83e-01, avg batch time: 1.0058, average train loss: 7.3773
[11/26 19:51:23 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3053, average loss: 10.7353
[11/26 19:51:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.40	
[11/26 19:51:23 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/26 19:53:08 visual_prompt]: 	Training 100/553. train loss: 8.1316,	0.8174 s / batch. (data: 3.27e-04). ETA=9:16:09, max mem: 20.9 GB 
[11/26 19:54:47 visual_prompt]: 	Training 200/553. train loss: 17.1043,	1.3921 s / batch. (data: 5.84e-01). ETA=15:44:47, max mem: 20.9 GB 
[11/26 19:56:28 visual_prompt]: 	Training 300/553. train loss: 6.4966,	0.8264 s / batch. (data: 3.26e-04). ETA=9:19:29, max mem: 20.9 GB 
[11/26 19:58:09 visual_prompt]: 	Training 400/553. train loss: 0.7360,	0.8360 s / batch. (data: 7.87e-04). ETA=9:24:36, max mem: 20.9 GB 
[11/26 19:59:50 visual_prompt]: 	Training 500/553. train loss: 0.7005,	0.8279 s / batch. (data: 2.97e-04). ETA=9:17:47, max mem: 20.9 GB 
[11/26 20:00:40 visual_prompt]: Epoch 27 / 100: avg data time: 1.83e-01, avg batch time: 1.0069, average train loss: 7.5118
[11/26 20:01:37 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3065, average loss: 5.4981
[11/26 20:01:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.93	
[11/26 20:01:37 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[11/26 20:03:20 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.9363 s / batch. (data: 1.23e-01). ETA=10:28:24, max mem: 20.9 GB 
[11/26 20:05:01 visual_prompt]: 	Training 200/553. train loss: 0.5731,	0.8131 s / batch. (data: 5.43e-03). ETA=9:04:20, max mem: 20.9 GB 
[11/26 20:06:42 visual_prompt]: 	Training 300/553. train loss: 5.0490,	1.5720 s / batch. (data: 7.58e-01). ETA=17:29:49, max mem: 20.9 GB 
[11/26 20:08:22 visual_prompt]: 	Training 400/553. train loss: 19.3130,	0.8303 s / batch. (data: 7.99e-04). ETA=9:13:06, max mem: 20.9 GB 
[11/26 20:10:01 visual_prompt]: 	Training 500/553. train loss: 0.3478,	0.8060 s / batch. (data: 2.98e-04). ETA=8:55:33, max mem: 20.9 GB 
[11/26 20:10:53 visual_prompt]: Epoch 28 / 100: avg data time: 1.80e-01, avg batch time: 1.0041, average train loss: 7.7986
[11/26 20:11:50 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3069, average loss: 2.5523
[11/26 20:11:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.50	
[11/26 20:11:50 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[11/26 20:13:40 visual_prompt]: 	Training 100/553. train loss: 8.1445,	0.8412 s / batch. (data: 3.34e-04). ETA=9:16:49, max mem: 20.9 GB 
[11/26 20:15:19 visual_prompt]: 	Training 200/553. train loss: 19.1221,	1.8280 s / batch. (data: 9.97e-01). ETA=20:06:57, max mem: 20.9 GB 
[11/26 20:16:58 visual_prompt]: 	Training 300/553. train loss: 0.5006,	0.8560 s / batch. (data: 8.04e-04). ETA=9:23:44, max mem: 20.9 GB 
[11/26 20:18:34 visual_prompt]: 	Training 400/553. train loss: 28.6541,	1.1240 s / batch. (data: 2.79e-01). ETA=12:18:23, max mem: 20.9 GB 
[11/26 20:20:15 visual_prompt]: 	Training 500/553. train loss: 18.4631,	0.8068 s / batch. (data: 3.07e-04). ETA=8:48:40, max mem: 20.9 GB 
[11/26 20:21:07 visual_prompt]: Epoch 29 / 100: avg data time: 1.82e-01, avg batch time: 1.0064, average train loss: 7.6487
[11/26 20:22:05 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3056, average loss: 7.2246
[11/26 20:22:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.46	
[11/26 20:22:05 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[11/26 20:23:47 visual_prompt]: 	Training 100/553. train loss: 10.0702,	0.8359 s / batch. (data: 3.00e-04). ETA=9:05:36, max mem: 20.9 GB 
[11/26 20:25:28 visual_prompt]: 	Training 200/553. train loss: 6.5274,	0.8189 s / batch. (data: 3.31e-04). ETA=8:53:07, max mem: 20.9 GB 
[11/26 20:27:07 visual_prompt]: 	Training 300/553. train loss: 0.0001,	1.7937 s / batch. (data: 9.57e-01). ETA=19:24:48, max mem: 20.9 GB 
[11/26 20:28:49 visual_prompt]: 	Training 400/553. train loss: 9.2713,	0.9665 s / batch. (data: 1.62e-01). ETA=10:25:59, max mem: 20.9 GB 
[11/26 20:30:28 visual_prompt]: 	Training 500/553. train loss: 0.9887,	1.5000 s / batch. (data: 6.61e-01). ETA=16:09:03, max mem: 20.9 GB 
[11/26 20:31:22 visual_prompt]: Epoch 30 / 100: avg data time: 1.85e-01, avg batch time: 1.0079, average train loss: 6.2163
[11/26 20:32:19 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3068, average loss: 13.2799
[11/26 20:32:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.08	
[11/26 20:32:19 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[11/26 20:34:05 visual_prompt]: 	Training 100/553. train loss: 7.1848,	0.8261 s / batch. (data: 5.42e-03). ETA=8:51:37, max mem: 20.9 GB 
[11/26 20:35:47 visual_prompt]: 	Training 200/553. train loss: 1.2936,	0.8180 s / batch. (data: 5.41e-03). ETA=8:45:00, max mem: 20.9 GB 
[11/26 20:37:25 visual_prompt]: 	Training 300/553. train loss: 7.0803,	0.8193 s / batch. (data: 3.15e-04). ETA=8:44:30, max mem: 20.9 GB 
[11/26 20:39:05 visual_prompt]: 	Training 400/553. train loss: 2.0222,	0.9960 s / batch. (data: 1.63e-01). ETA=10:35:56, max mem: 20.9 GB 
[11/26 20:40:45 visual_prompt]: 	Training 500/553. train loss: 5.0059,	0.8120 s / batch. (data: 3.04e-04). ETA=8:37:05, max mem: 20.9 GB 
[11/26 20:41:37 visual_prompt]: Epoch 31 / 100: avg data time: 1.85e-01, avg batch time: 1.0087, average train loss: 6.7407
[11/26 20:42:35 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3070, average loss: 12.3639
[11/26 20:42:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[11/26 20:42:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[11/26 20:44:20 visual_prompt]: 	Training 100/553. train loss: 0.7040,	0.8301 s / batch. (data: 5.41e-03). ETA=8:46:30, max mem: 20.9 GB 
[11/26 20:46:00 visual_prompt]: 	Training 200/553. train loss: 3.9722,	0.8348 s / batch. (data: 1.25e-02). ETA=8:48:06, max mem: 20.9 GB 
[11/26 20:47:42 visual_prompt]: 	Training 300/553. train loss: 12.6734,	0.8400 s / batch. (data: 7.95e-03). ETA=8:50:00, max mem: 20.9 GB 
[11/26 20:49:23 visual_prompt]: 	Training 400/553. train loss: 14.6780,	0.8149 s / batch. (data: 2.99e-04). ETA=8:32:48, max mem: 20.9 GB 
[11/26 20:51:00 visual_prompt]: 	Training 500/553. train loss: 1.0126,	0.8378 s / batch. (data: 3.17e-04). ETA=8:45:50, max mem: 20.9 GB 
[11/26 20:51:51 visual_prompt]: Epoch 32 / 100: avg data time: 1.82e-01, avg batch time: 1.0054, average train loss: 6.8522
[11/26 20:52:48 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3049, average loss: 1.7569
[11/26 20:52:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.47	
[11/26 20:52:48 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[11/26 20:54:30 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8240 s / batch. (data: 3.03e-04). ETA=8:35:03, max mem: 20.9 GB 
[11/26 20:56:13 visual_prompt]: 	Training 200/553. train loss: 22.1620,	1.5025 s / batch. (data: 6.95e-01). ETA=15:36:37, max mem: 20.9 GB 
[11/26 20:57:52 visual_prompt]: 	Training 300/553. train loss: 9.6480,	0.8154 s / batch. (data: 2.94e-04). ETA=8:26:56, max mem: 20.9 GB 
[11/26 20:59:33 visual_prompt]: 	Training 400/553. train loss: 3.1186,	0.8349 s / batch. (data: 1.05e-02). ETA=8:37:40, max mem: 20.9 GB 
[11/26 21:01:13 visual_prompt]: 	Training 500/553. train loss: 0.9272,	0.8720 s / batch. (data: 3.31e-04). ETA=8:59:13, max mem: 20.9 GB 
[11/26 21:02:04 visual_prompt]: Epoch 33 / 100: avg data time: 1.82e-01, avg batch time: 1.0054, average train loss: 7.0732
[11/26 21:03:01 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3071, average loss: 2.8704
[11/26 21:03:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/26 21:03:01 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[11/26 21:04:47 visual_prompt]: 	Training 100/553. train loss: 0.9340,	0.8139 s / batch. (data: 3.50e-04). ETA=8:21:14, max mem: 20.9 GB 
[11/26 21:06:25 visual_prompt]: 	Training 200/553. train loss: 7.4274,	0.8233 s / batch. (data: 5.43e-03). ETA=8:25:39, max mem: 20.9 GB 
[11/26 21:08:04 visual_prompt]: 	Training 300/553. train loss: 9.3686,	0.8160 s / batch. (data: 3.21e-04). ETA=8:19:48, max mem: 20.9 GB 
[11/26 21:09:45 visual_prompt]: 	Training 400/553. train loss: 0.9182,	0.8212 s / batch. (data: 2.98e-04). ETA=8:21:36, max mem: 20.9 GB 
[11/26 21:11:25 visual_prompt]: 	Training 500/553. train loss: 2.1674,	1.5516 s / batch. (data: 7.19e-01). ETA=15:45:13, max mem: 20.9 GB 
[11/26 21:12:17 visual_prompt]: Epoch 34 / 100: avg data time: 1.80e-01, avg batch time: 1.0041, average train loss: 5.9662
[11/26 21:13:14 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3067, average loss: 4.5517
[11/26 21:13:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.40	
[11/26 21:13:14 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[11/26 21:15:00 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8284 s / batch. (data: 2.95e-04). ETA=8:22:30, max mem: 20.9 GB 
[11/26 21:16:40 visual_prompt]: 	Training 200/553. train loss: 0.9434,	0.8354 s / batch. (data: 3.22e-04). ETA=8:25:23, max mem: 20.9 GB 
[11/26 21:18:19 visual_prompt]: 	Training 300/553. train loss: 3.9342,	0.8081 s / batch. (data: 3.06e-04). ETA=8:07:29, max mem: 20.9 GB 
[11/26 21:19:57 visual_prompt]: 	Training 400/553. train loss: 3.4352,	0.8980 s / batch. (data: 7.60e-02). ETA=9:00:17, max mem: 20.9 GB 
[11/26 21:21:37 visual_prompt]: 	Training 500/553. train loss: 33.6728,	1.2459 s / batch. (data: 3.89e-01). ETA=12:27:31, max mem: 20.9 GB 
[11/26 21:22:29 visual_prompt]: Epoch 35 / 100: avg data time: 1.80e-01, avg batch time: 1.0037, average train loss: 6.2769
[11/26 21:23:26 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3052, average loss: 6.2577
[11/26 21:23:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.13	
[11/26 21:23:27 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[11/26 21:25:10 visual_prompt]: 	Training 100/553. train loss: 23.1203,	0.8361 s / batch. (data: 3.22e-04). ETA=8:19:28, max mem: 20.9 GB 
[11/26 21:26:51 visual_prompt]: 	Training 200/553. train loss: 25.4850,	0.8063 s / batch. (data: 2.79e-04). ETA=8:00:21, max mem: 20.9 GB 
[11/26 21:28:33 visual_prompt]: 	Training 300/553. train loss: 0.0047,	0.8261 s / batch. (data: 3.56e-04). ETA=8:10:45, max mem: 20.9 GB 
[11/26 21:30:12 visual_prompt]: 	Training 400/553. train loss: 3.0664,	0.8210 s / batch. (data: 3.18e-04). ETA=8:06:24, max mem: 20.9 GB 
[11/26 21:31:53 visual_prompt]: 	Training 500/553. train loss: 6.5926,	1.0760 s / batch. (data: 2.32e-01). ETA=10:35:39, max mem: 20.9 GB 
[11/26 21:32:43 visual_prompt]: Epoch 36 / 100: avg data time: 1.82e-01, avg batch time: 1.0053, average train loss: 7.1413
[11/26 21:33:40 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3065, average loss: 13.5050
[11/26 21:33:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.78	
[11/26 21:33:40 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[11/26 21:35:25 visual_prompt]: 	Training 100/553. train loss: 6.9923,	0.8301 s / batch. (data: 3.09e-04). ETA=8:08:14, max mem: 20.9 GB 
[11/26 21:37:03 visual_prompt]: 	Training 200/553. train loss: 11.8993,	0.8579 s / batch. (data: 2.18e-02). ETA=8:23:10, max mem: 20.9 GB 
[11/26 21:38:45 visual_prompt]: 	Training 300/553. train loss: 19.4043,	1.5796 s / batch. (data: 7.71e-01). ETA=15:23:51, max mem: 20.9 GB 
[11/26 21:40:28 visual_prompt]: 	Training 400/553. train loss: 43.1729,	1.8188 s / batch. (data: 1.01e+00). ETA=17:40:44, max mem: 20.9 GB 
[11/26 21:42:05 visual_prompt]: 	Training 500/553. train loss: 16.0144,	0.9220 s / batch. (data: 7.90e-02). ETA=8:56:12, max mem: 20.9 GB 
[11/26 21:42:58 visual_prompt]: Epoch 37 / 100: avg data time: 1.85e-01, avg batch time: 1.0088, average train loss: 6.2848
[11/26 21:43:56 visual_prompt]: Inference (val):avg data time: 1.76e-04, avg batch time: 0.3075, average loss: 1.5466
[11/26 21:43:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.55	
[11/26 21:43:56 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[11/26 21:45:38 visual_prompt]: 	Training 100/553. train loss: 1.1445,	0.9776 s / batch. (data: 1.63e-01). ETA=9:25:59, max mem: 20.9 GB 
[11/26 21:47:19 visual_prompt]: 	Training 200/553. train loss: 4.3302,	0.8429 s / batch. (data: 1.57e-02). ETA=8:06:36, max mem: 20.9 GB 
[11/26 21:49:02 visual_prompt]: 	Training 300/553. train loss: 1.0383,	0.8069 s / batch. (data: 2.93e-04). ETA=7:44:30, max mem: 20.9 GB 
[11/26 21:50:38 visual_prompt]: 	Training 400/553. train loss: 31.5564,	0.8166 s / batch. (data: 8.89e-03). ETA=7:48:44, max mem: 20.9 GB 
[11/26 21:52:21 visual_prompt]: 	Training 500/553. train loss: 42.8701,	0.8450 s / batch. (data: 1.71e-02). ETA=8:03:36, max mem: 20.9 GB 
[11/26 21:53:12 visual_prompt]: Epoch 38 / 100: avg data time: 1.83e-01, avg batch time: 1.0057, average train loss: 7.3106
[11/26 21:54:09 visual_prompt]: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3071, average loss: 7.7741
[11/26 21:54:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.80	
[11/26 21:54:09 visual_prompt]: Stopping early.
[11/26 21:54:09 visual_prompt]: Rank of current process: 0. World size: 1
[11/26 21:54:09 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/26 21:54:09 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/26 21:54:09 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/26 21:54:09 visual_prompt]: Training with config:
[11/26 21:54:09 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/26 21:54:09 visual_prompt]: Loading training data...
[11/26 21:54:09 visual_prompt]: Constructing mammo-cbis dataset train...
[11/26 21:54:09 visual_prompt]: Loading validation data...
[11/26 21:54:09 visual_prompt]: Constructing mammo-cbis dataset val...
[11/26 21:54:09 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/26 21:54:12 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/26 21:54:12 visual_prompt]: tuned percent:0.525
[11/26 21:54:12 visual_prompt]: Device used for model: 0
[11/26 21:54:12 visual_prompt]: Setting up Evaluator...
[11/26 21:54:12 visual_prompt]: Setting up Trainer...
[11/26 21:54:12 visual_prompt]: 	Setting up the optimizer...
[11/26 21:54:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/26 21:55:55 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8280 s / batch. (data: 2.74e-04). ETA=12:41:44, max mem: 20.9 GB 
[11/26 21:57:34 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8088 s / batch. (data: 2.94e-04). ETA=12:22:45, max mem: 20.9 GB 
[11/26 21:59:17 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.9200 s / batch. (data: 9.61e-02). ETA=14:03:20, max mem: 20.9 GB 
[11/26 22:00:56 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8205 s / batch. (data: 9.92e-03). ETA=12:30:44, max mem: 20.9 GB 
[11/26 22:02:38 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8240 s / batch. (data: 2.99e-04). ETA=12:32:35, max mem: 20.9 GB 
[11/26 22:03:31 visual_prompt]: Epoch 1 / 100: avg data time: 1.87e-01, avg batch time: 1.0107, average train loss: 1.5403
[11/26 22:04:28 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3056, average loss: 1.5201
[11/26 22:04:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/26 22:04:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/26 22:06:12 visual_prompt]: 	Training 100/553. train loss: 1.1712,	1.0144 s / batch. (data: 1.86e-01). ETA=15:23:52, max mem: 20.9 GB 
[11/26 22:07:51 visual_prompt]: 	Training 200/553. train loss: 0.0979,	0.8400 s / batch. (data: 3.39e-04). ETA=12:43:39, max mem: 20.9 GB 
[11/26 22:09:34 visual_prompt]: 	Training 300/553. train loss: 1.4903,	1.0081 s / batch. (data: 1.72e-01). ETA=15:14:45, max mem: 20.9 GB 
[11/26 22:11:13 visual_prompt]: 	Training 400/553. train loss: 0.5685,	0.8080 s / batch. (data: 2.68e-04). ETA=12:11:54, max mem: 20.9 GB 
[11/26 22:12:55 visual_prompt]: 	Training 500/553. train loss: 0.7332,	0.8215 s / batch. (data: 2.90e-04). ETA=12:22:42, max mem: 20.9 GB 
[11/26 22:13:46 visual_prompt]: Epoch 2 / 100: avg data time: 1.84e-01, avg batch time: 1.0078, average train loss: 1.2897
[11/26 22:14:43 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3073, average loss: 2.6171
[11/26 22:14:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.83	
[11/26 22:14:43 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/26 22:16:26 visual_prompt]: 	Training 100/553. train loss: 1.3173,	0.8338 s / batch. (data: 9.72e-03). ETA=12:31:42, max mem: 20.9 GB 
[11/26 22:18:08 visual_prompt]: 	Training 200/553. train loss: 0.7036,	0.8192 s / batch. (data: 2.94e-04). ETA=12:17:11, max mem: 20.9 GB 
[11/26 22:19:47 visual_prompt]: 	Training 300/553. train loss: 1.2212,	0.8520 s / batch. (data: 1.19e-02). ETA=12:45:18, max mem: 20.9 GB 
[11/26 22:21:28 visual_prompt]: 	Training 400/553. train loss: 0.3644,	0.8337 s / batch. (data: 3.12e-04). ETA=12:27:29, max mem: 20.9 GB 
[11/26 22:23:10 visual_prompt]: 	Training 500/553. train loss: 0.7960,	1.3760 s / batch. (data: 5.33e-01). ETA=20:31:22, max mem: 20.9 GB 
[11/26 22:24:00 visual_prompt]: Epoch 3 / 100: avg data time: 1.84e-01, avg batch time: 1.0071, average train loss: 1.3528
[11/26 22:24:58 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3089, average loss: 1.8503
[11/26 22:24:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.62	
[11/26 22:24:58 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/26 22:26:44 visual_prompt]: 	Training 100/553. train loss: 1.2718,	0.8200 s / batch. (data: 2.90e-04). ETA=12:11:43, max mem: 20.9 GB 
[11/26 22:28:24 visual_prompt]: 	Training 200/553. train loss: 0.9991,	0.8105 s / batch. (data: 2.73e-04). ETA=12:01:52, max mem: 20.9 GB 
[11/26 22:30:05 visual_prompt]: 	Training 300/553. train loss: 0.6045,	1.5476 s / batch. (data: 7.41e-01). ETA=22:55:48, max mem: 20.9 GB 
[11/26 22:31:41 visual_prompt]: 	Training 400/553. train loss: 1.0981,	1.2687 s / batch. (data: 4.36e-01). ETA=18:45:49, max mem: 20.9 GB 
[11/26 22:33:23 visual_prompt]: 	Training 500/553. train loss: 2.7090,	3.5804 s / batch. (data: 2.78e+00). ETA=2 days, 4:51:03, max mem: 20.9 GB 
[11/26 22:34:16 visual_prompt]: Epoch 4 / 100: avg data time: 1.86e-01, avg batch time: 1.0098, average train loss: 1.2842
[11/26 22:35:14 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3069, average loss: 1.5540
[11/26 22:35:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.66	
[11/26 22:35:14 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/26 22:36:56 visual_prompt]: 	Training 100/553. train loss: 16.0523,	0.8075 s / batch. (data: 4.35e-04). ETA=11:53:08, max mem: 20.9 GB 
[11/26 22:38:37 visual_prompt]: 	Training 200/553. train loss: 2.0396,	1.3704 s / batch. (data: 5.52e-01). ETA=20:07:58, max mem: 20.9 GB 
[11/26 22:40:18 visual_prompt]: 	Training 300/553. train loss: 0.7127,	0.8222 s / batch. (data: 1.19e-02). ETA=12:03:21, max mem: 20.9 GB 
[11/26 22:41:57 visual_prompt]: 	Training 400/553. train loss: 5.3987,	0.8259 s / batch. (data: 2.87e-04). ETA=12:05:15, max mem: 20.9 GB 
[11/26 22:43:37 visual_prompt]: 	Training 500/553. train loss: 2.3383,	0.8470 s / batch. (data: 5.41e-03). ETA=12:22:22, max mem: 20.9 GB 
[11/26 22:44:30 visual_prompt]: Epoch 5 / 100: avg data time: 1.83e-01, avg batch time: 1.0065, average train loss: 2.3967
[11/26 22:45:28 visual_prompt]: Inference (val):avg data time: 1.87e-04, avg batch time: 0.3045, average loss: 1.1091
[11/26 22:45:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.43	
[11/26 22:45:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/26 22:47:14 visual_prompt]: 	Training 100/553. train loss: 3.2001,	0.8403 s / batch. (data: 5.41e-03). ETA=12:14:22, max mem: 20.9 GB 
[11/26 22:48:53 visual_prompt]: 	Training 200/553. train loss: 12.0924,	0.8279 s / batch. (data: 4.85e-04). ETA=12:02:09, max mem: 20.9 GB 
[11/26 22:50:32 visual_prompt]: 	Training 300/553. train loss: 0.6165,	0.8258 s / batch. (data: 3.04e-04). ETA=11:58:55, max mem: 20.9 GB 
[11/26 22:52:16 visual_prompt]: 	Training 400/553. train loss: 9.0347,	0.8381 s / batch. (data: 1.01e-02). ETA=12:08:16, max mem: 20.9 GB 
[11/26 22:53:55 visual_prompt]: 	Training 500/553. train loss: 0.9203,	0.8321 s / batch. (data: 3.75e-04). ETA=12:01:37, max mem: 20.9 GB 
[11/26 22:54:47 visual_prompt]: Epoch 6 / 100: avg data time: 1.88e-01, avg batch time: 1.0104, average train loss: 3.5688
[11/26 22:55:44 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3056, average loss: 2.7631
[11/26 22:55:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.84	
[11/26 22:55:44 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/26 22:57:27 visual_prompt]: 	Training 100/553. train loss: 3.1863,	0.8530 s / batch. (data: 2.09e-02). ETA=12:17:33, max mem: 20.9 GB 
[11/26 22:59:08 visual_prompt]: 	Training 200/553. train loss: 1.2053,	0.8415 s / batch. (data: 3.24e-04). ETA=12:06:16, max mem: 20.9 GB 
[11/26 23:00:51 visual_prompt]: 	Training 300/553. train loss: 2.6570,	1.8977 s / batch. (data: 1.08e+00). ETA=1 day, 3:14:35, max mem: 20.9 GB 
[11/26 23:02:31 visual_prompt]: 	Training 400/553. train loss: 1.3255,	1.8719 s / batch. (data: 1.05e+00). ETA=1 day, 2:49:14, max mem: 20.9 GB 
[11/26 23:04:09 visual_prompt]: 	Training 500/553. train loss: 2.3669,	0.8211 s / batch. (data: 3.28e-04). ETA=11:44:33, max mem: 20.9 GB 
[11/26 23:05:01 visual_prompt]: Epoch 7 / 100: avg data time: 1.83e-01, avg batch time: 1.0066, average train loss: 4.0666
[11/26 23:05:58 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3059, average loss: 5.2876
[11/26 23:05:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.18	
[11/26 23:05:58 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/26 23:07:40 visual_prompt]: 	Training 100/553. train loss: 2.6818,	0.8215 s / batch. (data: 3.05e-04). ETA=11:42:45, max mem: 20.9 GB 
[11/26 23:09:22 visual_prompt]: 	Training 200/553. train loss: 0.5606,	0.8257 s / batch. (data: 1.12e-02). ETA=11:44:59, max mem: 20.9 GB 
[11/26 23:11:03 visual_prompt]: 	Training 300/553. train loss: 3.3904,	0.8322 s / batch. (data: 3.46e-03). ETA=11:49:11, max mem: 20.9 GB 
[11/26 23:12:44 visual_prompt]: 	Training 400/553. train loss: 3.0755,	0.8206 s / batch. (data: 2.98e-04). ETA=11:37:54, max mem: 20.9 GB 
[11/26 23:14:24 visual_prompt]: 	Training 500/553. train loss: 18.5463,	1.4358 s / batch. (data: 6.03e-01). ETA=20:18:43, max mem: 20.9 GB 
[11/26 23:15:17 visual_prompt]: Epoch 8 / 100: avg data time: 1.86e-01, avg batch time: 1.0095, average train loss: 4.4663
[11/26 23:16:14 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3066, average loss: 1.9721
[11/26 23:16:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.10	
[11/26 23:16:14 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/26 23:17:58 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8425 s / batch. (data: 1.05e-02). ETA=11:52:57, max mem: 20.9 GB 
[11/26 23:19:37 visual_prompt]: 	Training 200/553. train loss: 0.9589,	0.8200 s / batch. (data: 2.98e-04). ETA=11:32:34, max mem: 20.9 GB 
[11/26 23:21:18 visual_prompt]: 	Training 300/553. train loss: 11.3712,	1.7534 s / batch. (data: 9.32e-01). ETA=1 day, 0:38:02, max mem: 20.9 GB 
[11/26 23:23:00 visual_prompt]: 	Training 400/553. train loss: 2.1421,	0.8147 s / batch. (data: 2.82e-04). ETA=11:25:23, max mem: 20.9 GB 
[11/26 23:24:40 visual_prompt]: 	Training 500/553. train loss: 2.3915,	0.9960 s / batch. (data: 1.58e-01). ETA=13:56:14, max mem: 20.9 GB 
[11/26 23:25:31 visual_prompt]: Epoch 9 / 100: avg data time: 1.84e-01, avg batch time: 1.0074, average train loss: 4.8825
[11/26 23:26:28 visual_prompt]: Inference (val):avg data time: 3.11e-04, avg batch time: 0.3052, average loss: 0.7921
[11/26 23:26:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 40.65	rocauc: 41.97	
[11/26 23:26:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/26 23:28:15 visual_prompt]: 	Training 100/553. train loss: 5.6283,	0.8440 s / batch. (data: 7.71e-04). ETA=11:46:29, max mem: 20.9 GB 
[11/26 23:29:54 visual_prompt]: 	Training 200/553. train loss: 1.0567,	0.8280 s / batch. (data: 3.23e-04). ETA=11:31:42, max mem: 20.9 GB 
[11/26 23:31:34 visual_prompt]: 	Training 300/553. train loss: 7.1913,	2.2602 s / batch. (data: 1.45e+00). ETA=1 day, 7:24:21, max mem: 20.9 GB 
[11/26 23:33:11 visual_prompt]: 	Training 400/553. train loss: 9.4222,	0.9533 s / batch. (data: 1.32e-01). ETA=13:13:12, max mem: 20.9 GB 
[11/26 23:34:53 visual_prompt]: 	Training 500/553. train loss: 12.0612,	0.8222 s / batch. (data: 5.54e-03). ETA=11:22:42, max mem: 20.9 GB 
[11/26 23:35:45 visual_prompt]: Epoch 10 / 100: avg data time: 1.84e-01, avg batch time: 1.0072, average train loss: 6.9467
[11/26 23:36:43 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3047, average loss: 4.1296
[11/26 23:36:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.30	
[11/26 23:36:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/26 23:38:29 visual_prompt]: 	Training 100/553. train loss: 3.6753,	0.8240 s / batch. (data: 3.76e-04). ETA=11:22:07, max mem: 20.9 GB 
[11/26 23:40:11 visual_prompt]: 	Training 200/553. train loss: 4.9292,	0.8076 s / batch. (data: 3.17e-04). ETA=11:07:14, max mem: 20.9 GB 
[11/26 23:41:51 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.0902 s / batch. (data: 1.28e+00). ETA=1 day, 4:43:24, max mem: 20.9 GB 
[11/26 23:43:29 visual_prompt]: 	Training 400/553. train loss: 0.7602,	0.8210 s / batch. (data: 2.99e-04). ETA=11:15:35, max mem: 20.9 GB 
[11/26 23:45:08 visual_prompt]: 	Training 500/553. train loss: 3.7707,	0.8415 s / batch. (data: 3.04e-04). ETA=11:31:01, max mem: 20.9 GB 
[11/26 23:45:59 visual_prompt]: Epoch 11 / 100: avg data time: 1.82e-01, avg batch time: 1.0057, average train loss: 6.0666
[11/26 23:46:57 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3062, average loss: 1.8684
[11/26 23:46:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.24	
[11/26 23:46:57 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/26 23:48:42 visual_prompt]: 	Training 100/553. train loss: 2.4200,	0.8400 s / batch. (data: 4.53e-04). ETA=11:27:38, max mem: 20.9 GB 
[11/26 23:50:23 visual_prompt]: 	Training 200/553. train loss: 2.7326,	1.3654 s / batch. (data: 5.51e-01). ETA=18:35:25, max mem: 20.9 GB 
[11/26 23:52:02 visual_prompt]: 	Training 300/553. train loss: 10.3288,	0.8449 s / batch. (data: 3.07e-04). ETA=11:28:48, max mem: 20.9 GB 
[11/26 23:53:42 visual_prompt]: 	Training 400/553. train loss: 6.6342,	0.8240 s / batch. (data: 3.69e-04). ETA=11:10:24, max mem: 20.9 GB 
[11/26 23:55:23 visual_prompt]: 	Training 500/553. train loss: 3.1335,	0.8360 s / batch. (data: 5.43e-03). ETA=11:18:47, max mem: 20.9 GB 
[11/26 23:56:14 visual_prompt]: Epoch 12 / 100: avg data time: 1.84e-01, avg batch time: 1.0078, average train loss: 6.9875
[11/26 23:57:11 visual_prompt]: Inference (val):avg data time: 1.07e-04, avg batch time: 0.3057, average loss: 1.9784
[11/26 23:57:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.19	
[11/26 23:57:11 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/26 23:58:57 visual_prompt]: 	Training 100/553. train loss: 6.9440,	0.8400 s / batch. (data: 3.28e-04). ETA=11:19:53, max mem: 20.9 GB 
[11/27 00:00:34 visual_prompt]: 	Training 200/553. train loss: 3.8198,	0.8240 s / batch. (data: 3.82e-04). ETA=11:05:33, max mem: 20.9 GB 
[11/27 00:02:16 visual_prompt]: 	Training 300/553. train loss: 3.7713,	1.8280 s / batch. (data: 9.88e-01). ETA=1 day, 0:33:28, max mem: 20.9 GB 
[11/27 00:03:54 visual_prompt]: 	Training 400/553. train loss: 33.9767,	0.8160 s / batch. (data: 3.22e-04). ETA=10:56:23, max mem: 20.9 GB 
[11/27 00:05:36 visual_prompt]: 	Training 500/553. train loss: 18.2886,	0.8513 s / batch. (data: 1.13e-02). ETA=11:23:23, max mem: 20.9 GB 
[11/27 00:06:28 visual_prompt]: Epoch 13 / 100: avg data time: 1.83e-01, avg batch time: 1.0070, average train loss: 8.3601
[11/27 00:07:26 visual_prompt]: Inference (val):avg data time: 5.45e-05, avg batch time: 0.3154, average loss: 2.4024
[11/27 00:07:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.31	
[11/27 00:07:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/27 00:09:12 visual_prompt]: 	Training 100/553. train loss: 5.6784,	0.8360 s / batch. (data: 3.02e-04). ETA=11:08:57, max mem: 20.9 GB 
[11/27 00:10:51 visual_prompt]: 	Training 200/553. train loss: 3.5062,	1.0481 s / batch. (data: 2.10e-01). ETA=13:56:53, max mem: 20.9 GB 
[11/27 00:12:31 visual_prompt]: 	Training 300/553. train loss: 2.7180,	0.8261 s / batch. (data: 5.44e-03). ETA=10:58:14, max mem: 20.9 GB 
[11/27 00:14:10 visual_prompt]: 	Training 400/553. train loss: 2.3671,	0.8480 s / batch. (data: 1.20e-02). ETA=11:14:18, max mem: 20.9 GB 
[11/27 00:15:51 visual_prompt]: 	Training 500/553. train loss: 3.3707,	0.8468 s / batch. (data: 5.43e-03). ETA=11:11:58, max mem: 20.9 GB 
[11/27 00:16:42 visual_prompt]: Epoch 14 / 100: avg data time: 1.82e-01, avg batch time: 1.0050, average train loss: 7.4798
[11/27 00:17:40 visual_prompt]: Inference (val):avg data time: 6.08e-04, avg batch time: 0.3081, average loss: 6.0543
[11/27 00:17:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.13	
[11/27 00:17:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/27 00:19:24 visual_prompt]: 	Training 100/553. train loss: 7.5818,	0.8314 s / batch. (data: 8.85e-03). ETA=10:57:37, max mem: 20.9 GB 
[11/27 00:21:03 visual_prompt]: 	Training 200/553. train loss: 37.2050,	0.8480 s / batch. (data: 7.94e-03). ETA=11:09:19, max mem: 20.9 GB 
[11/27 00:22:45 visual_prompt]: 	Training 300/553. train loss: 0.9968,	0.8243 s / batch. (data: 5.92e-03). ETA=10:49:14, max mem: 20.9 GB 
[11/27 00:24:23 visual_prompt]: 	Training 400/553. train loss: 15.8997,	0.8305 s / batch. (data: 1.05e-02). ETA=10:52:43, max mem: 20.9 GB 
[11/27 00:26:04 visual_prompt]: 	Training 500/553. train loss: 0.9633,	0.8240 s / batch. (data: 7.95e-03). ETA=10:46:15, max mem: 20.9 GB 
[11/27 00:26:57 visual_prompt]: Epoch 15 / 100: avg data time: 1.85e-01, avg batch time: 1.0077, average train loss: 7.0170
[11/27 00:27:55 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3070, average loss: 79.1104
[11/27 00:27:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.74	
[11/27 00:27:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/27 00:29:38 visual_prompt]: 	Training 100/553. train loss: 12.6038,	0.8400 s / batch. (data: 3.06e-04). ETA=10:56:40, max mem: 20.9 GB 
[11/27 00:31:18 visual_prompt]: 	Training 200/553. train loss: 41.1485,	0.8251 s / batch. (data: 3.21e-04). ETA=10:43:37, max mem: 20.9 GB 
[11/27 00:32:59 visual_prompt]: 	Training 300/553. train loss: 0.7949,	0.8400 s / batch. (data: 7.94e-03). ETA=10:53:53, max mem: 20.9 GB 
[11/27 00:34:39 visual_prompt]: 	Training 400/553. train loss: 1.5314,	0.8256 s / batch. (data: 7.75e-04). ETA=10:41:16, max mem: 20.9 GB 
[11/27 00:36:18 visual_prompt]: 	Training 500/553. train loss: 3.2104,	1.5566 s / batch. (data: 7.51e-01). ETA=20:06:29, max mem: 20.9 GB 
[11/27 00:37:11 visual_prompt]: Epoch 16 / 100: avg data time: 1.81e-01, avg batch time: 1.0057, average train loss: 6.9581
[11/27 00:38:08 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3068, average loss: 6.3874
[11/27 00:38:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.45	
[11/27 00:38:08 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/27 00:39:51 visual_prompt]: 	Training 100/553. train loss: 3.1416,	0.8320 s / batch. (data: 2.84e-04). ETA=10:42:45, max mem: 20.9 GB 
[11/27 00:41:32 visual_prompt]: 	Training 200/553. train loss: 1.7053,	0.8320 s / batch. (data: 2.63e-04). ETA=10:41:22, max mem: 20.9 GB 
[11/27 00:43:12 visual_prompt]: 	Training 300/553. train loss: 2.6357,	0.8360 s / batch. (data: 1.19e-02). ETA=10:43:01, max mem: 20.9 GB 
[11/27 00:44:51 visual_prompt]: 	Training 400/553. train loss: 7.6595,	1.1028 s / batch. (data: 2.96e-01). ETA=14:06:24, max mem: 20.9 GB 
[11/27 00:46:31 visual_prompt]: 	Training 500/553. train loss: 3.5254,	1.5650 s / batch. (data: 7.35e-01). ETA=19:58:36, max mem: 20.9 GB 
[11/27 00:47:25 visual_prompt]: Epoch 17 / 100: avg data time: 1.83e-01, avg batch time: 1.0067, average train loss: 6.7164
[11/27 00:48:22 visual_prompt]: Inference (val):avg data time: 2.16e-04, avg batch time: 0.3057, average loss: 1.1450
[11/27 00:48:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.21	
[11/27 00:48:22 visual_prompt]: Best epoch 17: best metric: -1.145
[11/27 00:48:22 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/27 00:50:06 visual_prompt]: 	Training 100/553. train loss: 18.2026,	0.8138 s / batch. (data: 2.91e-04). ETA=10:21:09, max mem: 20.9 GB 
[11/27 00:51:49 visual_prompt]: 	Training 200/553. train loss: 4.7025,	0.8160 s / batch. (data: 2.98e-04). ETA=10:21:30, max mem: 20.9 GB 
[11/27 00:53:29 visual_prompt]: 	Training 300/553. train loss: 1.9836,	0.8320 s / batch. (data: 3.27e-04). ETA=10:32:18, max mem: 20.9 GB 
[11/27 00:55:09 visual_prompt]: 	Training 400/553. train loss: 5.2338,	0.8075 s / batch. (data: 2.95e-04). ETA=10:12:20, max mem: 20.9 GB 
[11/27 00:56:48 visual_prompt]: 	Training 500/553. train loss: 8.3156,	0.8123 s / batch. (data: 4.92e-03). ETA=10:14:35, max mem: 20.9 GB 
[11/27 00:57:39 visual_prompt]: Epoch 18 / 100: avg data time: 1.83e-01, avg batch time: 1.0059, average train loss: 7.3623
[11/27 00:58:36 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3073, average loss: 9.6024
[11/27 00:58:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.61	
[11/27 00:58:36 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/27 01:00:20 visual_prompt]: 	Training 100/553. train loss: 0.8589,	1.2360 s / batch. (data: 3.99e-01). ETA=15:32:03, max mem: 20.9 GB 
[11/27 01:02:01 visual_prompt]: 	Training 200/553. train loss: 2.2295,	0.8292 s / batch. (data: 2.81e-04). ETA=10:23:55, max mem: 20.9 GB 
[11/27 01:03:41 visual_prompt]: 	Training 300/553. train loss: 17.0633,	0.9007 s / batch. (data: 8.43e-02). ETA=11:16:10, max mem: 20.9 GB 
[11/27 01:05:22 visual_prompt]: 	Training 400/553. train loss: 2.2336,	0.8231 s / batch. (data: 5.98e-03). ETA=10:16:35, max mem: 20.9 GB 
[11/27 01:06:58 visual_prompt]: 	Training 500/553. train loss: 7.0014,	0.8320 s / batch. (data: 3.05e-04). ETA=10:21:52, max mem: 20.9 GB 
[11/27 01:07:50 visual_prompt]: Epoch 19 / 100: avg data time: 1.79e-01, avg batch time: 1.0025, average train loss: 6.6630
[11/27 01:08:48 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3062, average loss: 1.8048
[11/27 01:08:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.98	
[11/27 01:08:48 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/27 01:10:30 visual_prompt]: 	Training 100/553. train loss: 1.3331,	0.8307 s / batch. (data: 3.25e-04). ETA=10:18:44, max mem: 20.9 GB 
[11/27 01:12:12 visual_prompt]: 	Training 200/553. train loss: 3.7003,	0.8279 s / batch. (data: 3.91e-04). ETA=10:15:18, max mem: 20.9 GB 
[11/27 01:13:52 visual_prompt]: 	Training 300/553. train loss: 0.5766,	0.8240 s / batch. (data: 3.13e-04). ETA=10:11:02, max mem: 20.9 GB 
[11/27 01:15:32 visual_prompt]: 	Training 400/553. train loss: 2.2929,	0.8293 s / batch. (data: 3.09e-04). ETA=10:13:36, max mem: 20.9 GB 
[11/27 01:17:11 visual_prompt]: 	Training 500/553. train loss: 0.8760,	0.8320 s / batch. (data: 8.07e-03). ETA=10:14:10, max mem: 20.9 GB 
[11/27 01:18:04 visual_prompt]: Epoch 20 / 100: avg data time: 1.84e-01, avg batch time: 1.0060, average train loss: 7.9536
[11/27 01:19:01 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3068, average loss: 0.7894
[11/27 01:19:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.55	
[11/27 01:19:01 visual_prompt]: Best epoch 20: best metric: -0.789
[11/27 01:19:01 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/27 01:20:48 visual_prompt]: 	Training 100/553. train loss: 3.4122,	0.8555 s / batch. (data: 4.18e-02). ETA=10:29:21, max mem: 20.9 GB 
[11/27 01:22:28 visual_prompt]: 	Training 200/553. train loss: 15.1083,	0.8227 s / batch. (data: 7.94e-03). ETA=10:03:53, max mem: 20.9 GB 
[11/27 01:24:07 visual_prompt]: 	Training 300/553. train loss: 36.6907,	1.0236 s / batch. (data: 2.12e-01). ETA=12:29:38, max mem: 20.9 GB 
[11/27 01:25:47 visual_prompt]: 	Training 400/553. train loss: 22.1858,	0.8170 s / batch. (data: 2.87e-04). ETA=9:56:57, max mem: 20.9 GB 
[11/27 01:27:28 visual_prompt]: 	Training 500/553. train loss: 1.8073,	0.8422 s / batch. (data: 4.30e-04). ETA=10:13:55, max mem: 20.9 GB 
[11/27 01:28:19 visual_prompt]: Epoch 21 / 100: avg data time: 1.85e-01, avg batch time: 1.0090, average train loss: 6.9093
[11/27 01:29:17 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3057, average loss: 6.4880
[11/27 01:29:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.66	
[11/27 01:29:17 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/27 01:31:00 visual_prompt]: 	Training 100/553. train loss: 3.0801,	0.8179 s / batch. (data: 3.11e-04). ETA=9:54:10, max mem: 20.9 GB 
[11/27 01:32:41 visual_prompt]: 	Training 200/553. train loss: 7.0918,	0.8084 s / batch. (data: 2.91e-04). ETA=9:45:54, max mem: 20.9 GB 
[11/27 01:34:19 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8143 s / batch. (data: 3.16e-04). ETA=9:48:51, max mem: 20.9 GB 
[11/27 01:36:01 visual_prompt]: 	Training 400/553. train loss: 20.0441,	0.8185 s / batch. (data: 3.03e-04). ETA=9:50:31, max mem: 20.9 GB 
[11/27 01:37:40 visual_prompt]: 	Training 500/553. train loss: 2.1695,	0.8379 s / batch. (data: 1.53e-02). ETA=10:03:07, max mem: 20.9 GB 
[11/27 01:38:34 visual_prompt]: Epoch 22 / 100: avg data time: 1.84e-01, avg batch time: 1.0072, average train loss: 7.2443
[11/27 01:39:31 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3055, average loss: 6.5371
[11/27 01:39:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.52	
[11/27 01:39:31 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/27 01:41:16 visual_prompt]: 	Training 100/553. train loss: 0.7161,	0.8121 s / batch. (data: 3.94e-04). ETA=9:42:28, max mem: 20.9 GB 
[11/27 01:42:57 visual_prompt]: 	Training 200/553. train loss: 0.8525,	0.8114 s / batch. (data: 2.88e-04). ETA=9:40:36, max mem: 20.9 GB 
[11/27 01:44:40 visual_prompt]: 	Training 300/553. train loss: 1.7063,	0.8220 s / batch. (data: 5.39e-03). ETA=9:46:50, max mem: 20.9 GB 
[11/27 01:46:18 visual_prompt]: 	Training 400/553. train loss: 7.9508,	0.8391 s / batch. (data: 5.87e-03). ETA=9:57:38, max mem: 20.9 GB 
[11/27 01:47:56 visual_prompt]: 	Training 500/553. train loss: 3.5188,	0.8480 s / batch. (data: 2.93e-04). ETA=10:02:33, max mem: 20.9 GB 
[11/27 01:48:48 visual_prompt]: Epoch 23 / 100: avg data time: 1.84e-01, avg batch time: 1.0072, average train loss: 6.2641
[11/27 01:49:46 visual_prompt]: Inference (val):avg data time: 2.09e-04, avg batch time: 0.3050, average loss: 9.3899
[11/27 01:49:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.78	
[11/27 01:49:46 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/27 01:51:27 visual_prompt]: 	Training 100/553. train loss: 0.9012,	0.8240 s / batch. (data: 3.79e-04). ETA=9:43:22, max mem: 20.9 GB 
[11/27 01:53:07 visual_prompt]: 	Training 200/553. train loss: 1.0382,	0.8086 s / batch. (data: 2.99e-04). ETA=9:31:08, max mem: 20.9 GB 
[11/27 01:54:47 visual_prompt]: 	Training 300/553. train loss: 5.4717,	1.0066 s / batch. (data: 1.87e-01). ETA=11:49:19, max mem: 20.9 GB 
[11/27 01:56:28 visual_prompt]: 	Training 400/553. train loss: 4.3375,	0.8278 s / batch. (data: 5.43e-03). ETA=9:41:57, max mem: 20.9 GB 
[11/27 01:58:10 visual_prompt]: 	Training 500/553. train loss: 11.1186,	0.8360 s / batch. (data: 3.16e-04). ETA=9:46:19, max mem: 20.9 GB 
[11/27 01:59:03 visual_prompt]: Epoch 24 / 100: avg data time: 1.84e-01, avg batch time: 1.0073, average train loss: 7.0337
[11/27 02:00:00 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3078, average loss: 6.6399
[11/27 02:00:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.51	
[11/27 02:00:00 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/27 02:01:47 visual_prompt]: 	Training 100/553. train loss: 4.3921,	0.8182 s / batch. (data: 3.01e-04). ETA=9:31:43, max mem: 20.9 GB 
[11/27 02:03:24 visual_prompt]: 	Training 200/553. train loss: 1.0410,	0.8289 s / batch. (data: 1.05e-02). ETA=9:37:49, max mem: 20.9 GB 
[11/27 02:05:04 visual_prompt]: 	Training 300/553. train loss: 3.6681,	0.8303 s / batch. (data: 8.63e-03). ETA=9:37:26, max mem: 20.9 GB 
[11/27 02:06:44 visual_prompt]: 	Training 400/553. train loss: 0.9376,	1.3480 s / batch. (data: 5.18e-01). ETA=15:35:14, max mem: 20.9 GB 
[11/27 02:08:24 visual_prompt]: 	Training 500/553. train loss: 4.8600,	1.5440 s / batch. (data: 7.15e-01). ETA=17:48:39, max mem: 20.9 GB 
[11/27 02:09:16 visual_prompt]: Epoch 25 / 100: avg data time: 1.81e-01, avg batch time: 1.0049, average train loss: 4.7027
[11/27 02:10:13 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3065, average loss: 8.6686
[11/27 02:10:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.27	
[11/27 02:10:13 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/27 02:11:58 visual_prompt]: 	Training 100/553. train loss: 13.1341,	0.8382 s / batch. (data: 1.60e-02). ETA=9:37:58, max mem: 20.9 GB 
[11/27 02:13:39 visual_prompt]: 	Training 200/553. train loss: 3.6936,	1.7779 s / batch. (data: 9.63e-01). ETA=20:23:03, max mem: 20.9 GB 
[11/27 02:15:21 visual_prompt]: 	Training 300/553. train loss: 5.4963,	0.8440 s / batch. (data: 2.86e-04). ETA=9:39:12, max mem: 20.9 GB 
[11/27 02:16:59 visual_prompt]: 	Training 400/553. train loss: 2.1597,	0.8226 s / batch. (data: 1.55e-02). ETA=9:23:08, max mem: 20.9 GB 
[11/27 02:18:37 visual_prompt]: 	Training 500/553. train loss: 12.5324,	0.8216 s / batch. (data: 2.73e-04). ETA=9:21:06, max mem: 20.9 GB 
[11/27 02:19:30 visual_prompt]: Epoch 26 / 100: avg data time: 1.82e-01, avg batch time: 1.0061, average train loss: 4.8536
[11/27 02:20:27 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3080, average loss: 8.5867
[11/27 02:20:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.31	
[11/27 02:20:27 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/27 02:22:13 visual_prompt]: 	Training 100/553. train loss: 1.0692,	0.8329 s / batch. (data: 5.43e-03). ETA=9:26:41, max mem: 20.9 GB 
[11/27 02:23:52 visual_prompt]: 	Training 200/553. train loss: 10.7444,	1.1461 s / batch. (data: 3.36e-01). ETA=12:57:50, max mem: 20.9 GB 
[11/27 02:25:32 visual_prompt]: 	Training 300/553. train loss: 7.6850,	0.8491 s / batch. (data: 3.15e-04). ETA=9:34:52, max mem: 20.9 GB 
[11/27 02:27:13 visual_prompt]: 	Training 400/553. train loss: 2.4567,	0.8219 s / batch. (data: 2.95e-04). ETA=9:15:06, max mem: 20.9 GB 
[11/27 02:28:54 visual_prompt]: 	Training 500/553. train loss: 0.8005,	0.8201 s / batch. (data: 7.75e-04). ETA=9:12:31, max mem: 20.9 GB 
[11/27 02:29:44 visual_prompt]: Epoch 27 / 100: avg data time: 1.84e-01, avg batch time: 1.0068, average train loss: 5.8253
[11/27 02:30:41 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3063, average loss: 11.6900
[11/27 02:30:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.86	
[11/27 02:30:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[11/27 02:32:25 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8480 s / batch. (data: 3.94e-03). ETA=9:29:06, max mem: 20.9 GB 
[11/27 02:34:05 visual_prompt]: 	Training 200/553. train loss: 0.6749,	0.8232 s / batch. (data: 3.06e-04). ETA=9:11:06, max mem: 20.9 GB 
[11/27 02:35:46 visual_prompt]: 	Training 300/553. train loss: 1.5065,	1.5600 s / batch. (data: 7.47e-01). ETA=17:21:47, max mem: 20.9 GB 
[11/27 02:37:25 visual_prompt]: 	Training 400/553. train loss: 19.6463,	0.8187 s / batch. (data: 3.11e-04). ETA=9:05:24, max mem: 20.9 GB 
[11/27 02:39:04 visual_prompt]: 	Training 500/553. train loss: 15.3082,	0.8099 s / batch. (data: 3.06e-04). ETA=8:58:08, max mem: 20.9 GB 
[11/27 02:39:57 visual_prompt]: Epoch 28 / 100: avg data time: 1.81e-01, avg batch time: 1.0041, average train loss: 7.0804
[11/27 02:40:54 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3068, average loss: 7.1892
[11/27 02:40:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.42	
[11/27 02:40:54 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[11/27 02:42:45 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8200 s / batch. (data: 2.92e-04). ETA=9:02:47, max mem: 20.9 GB 
[11/27 02:44:24 visual_prompt]: 	Training 200/553. train loss: 4.4561,	1.8448 s / batch. (data: 1.03e+00). ETA=20:18:05, max mem: 20.9 GB 
[11/27 02:46:03 visual_prompt]: 	Training 300/553. train loss: 0.8421,	0.8280 s / batch. (data: 3.32e-04). ETA=9:05:18, max mem: 20.9 GB 
[11/27 02:47:39 visual_prompt]: 	Training 400/553. train loss: 10.4524,	0.8137 s / batch. (data: 3.73e-04). ETA=8:54:32, max mem: 20.9 GB 
[11/27 02:49:20 visual_prompt]: 	Training 500/553. train loss: 1.1496,	0.8334 s / batch. (data: 3.24e-04). ETA=9:06:07, max mem: 20.9 GB 
[11/27 02:50:12 visual_prompt]: Epoch 29 / 100: avg data time: 1.85e-01, avg batch time: 1.0090, average train loss: 5.8846
[11/27 02:51:10 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3063, average loss: 2.1267
[11/27 02:51:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.62	
[11/27 02:51:10 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[11/27 02:52:52 visual_prompt]: 	Training 100/553. train loss: 1.9249,	0.8200 s / batch. (data: 3.48e-04). ETA=8:55:15, max mem: 20.9 GB 
[11/27 02:54:34 visual_prompt]: 	Training 200/553. train loss: 0.9541,	0.8290 s / batch. (data: 5.40e-03). ETA=8:59:44, max mem: 20.9 GB 
[11/27 02:56:12 visual_prompt]: 	Training 300/553. train loss: 0.8238,	0.8145 s / batch. (data: 3.05e-04). ETA=8:48:55, max mem: 20.9 GB 
[11/27 02:57:54 visual_prompt]: 	Training 400/553. train loss: 3.9622,	1.2406 s / batch. (data: 4.03e-01). ETA=13:23:34, max mem: 20.9 GB 
[11/27 02:59:33 visual_prompt]: 	Training 500/553. train loss: 10.2939,	1.5690 s / batch. (data: 7.37e-01). ETA=16:53:38, max mem: 20.9 GB 
[11/27 03:00:27 visual_prompt]: Epoch 30 / 100: avg data time: 1.84e-01, avg batch time: 1.0076, average train loss: 5.9626
[11/27 03:01:24 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3046, average loss: 8.9855
[11/27 03:01:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.66	
[11/27 03:01:24 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[11/27 03:03:10 visual_prompt]: 	Training 100/553. train loss: 3.8355,	0.8560 s / batch. (data: 3.02e-04). ETA=9:10:49, max mem: 20.9 GB 
[11/27 03:04:52 visual_prompt]: 	Training 200/553. train loss: 6.6480,	0.8074 s / batch. (data: 3.01e-04). ETA=8:38:12, max mem: 20.9 GB 
[11/27 03:06:29 visual_prompt]: 	Training 300/553. train loss: 5.1967,	0.8328 s / batch. (data: 1.19e-02). ETA=8:53:07, max mem: 20.9 GB 
[11/27 03:08:08 visual_prompt]: 	Training 400/553. train loss: 5.0988,	1.1508 s / batch. (data: 3.10e-01). ETA=12:14:48, max mem: 20.9 GB 
[11/27 03:09:49 visual_prompt]: 	Training 500/553. train loss: 15.8001,	0.8288 s / batch. (data: 5.42e-03). ETA=8:47:49, max mem: 20.9 GB 
[11/27 03:10:40 visual_prompt]: Epoch 31 / 100: avg data time: 1.82e-01, avg batch time: 1.0057, average train loss: 6.7488
[11/27 03:11:38 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3074, average loss: 5.5210
[11/27 03:11:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.26	
[11/27 03:11:38 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[11/27 03:13:23 visual_prompt]: 	Training 100/553. train loss: 1.5124,	0.8243 s / batch. (data: 5.49e-03). ETA=8:42:50, max mem: 20.9 GB 
[11/27 03:15:03 visual_prompt]: 	Training 200/553. train loss: 18.4035,	0.8269 s / batch. (data: 7.99e-03). ETA=8:43:06, max mem: 20.9 GB 
[11/27 03:16:46 visual_prompt]: 	Training 300/553. train loss: 5.6368,	0.8182 s / batch. (data: 3.11e-04). ETA=8:36:16, max mem: 20.9 GB 
[11/27 03:18:27 visual_prompt]: 	Training 400/553. train loss: 13.7159,	0.8200 s / batch. (data: 2.89e-04). ETA=8:36:01, max mem: 20.9 GB 
[11/27 03:20:04 visual_prompt]: 	Training 500/553. train loss: 16.9340,	0.8313 s / batch. (data: 2.89e-04). ETA=8:41:45, max mem: 20.9 GB 
[11/27 03:20:55 visual_prompt]: Epoch 32 / 100: avg data time: 1.84e-01, avg batch time: 1.0066, average train loss: 6.1576
[11/27 03:21:52 visual_prompt]: Inference (val):avg data time: 7.23e-04, avg batch time: 0.3070, average loss: 5.3873
[11/27 03:21:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.64	
[11/27 03:21:52 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[11/27 03:23:34 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8096 s / batch. (data: 2.99e-04). ETA=8:26:04, max mem: 20.9 GB 
[11/27 03:25:17 visual_prompt]: 	Training 200/553. train loss: 1.8919,	1.2708 s / batch. (data: 4.33e-01). ETA=13:12:13, max mem: 20.9 GB 
[11/27 03:26:56 visual_prompt]: 	Training 300/553. train loss: 12.5559,	0.8248 s / batch. (data: 5.44e-03). ETA=8:32:49, max mem: 20.9 GB 
[11/27 03:28:38 visual_prompt]: 	Training 400/553. train loss: 7.1457,	0.8149 s / batch. (data: 7.96e-03). ETA=8:25:18, max mem: 20.9 GB 
[11/27 03:30:17 visual_prompt]: 	Training 500/553. train loss: 5.0127,	0.8320 s / batch. (data: 3.17e-04). ETA=8:34:30, max mem: 20.9 GB 
[11/27 03:31:09 visual_prompt]: Epoch 33 / 100: avg data time: 1.83e-01, avg batch time: 1.0066, average train loss: 8.0582
[11/27 03:32:06 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3067, average loss: 5.3979
[11/27 03:32:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.53	
[11/27 03:32:06 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[11/27 03:33:52 visual_prompt]: 	Training 100/553. train loss: 1.6507,	0.8261 s / batch. (data: 5.52e-03). ETA=8:28:46, max mem: 20.9 GB 
[11/27 03:35:30 visual_prompt]: 	Training 200/553. train loss: 4.2793,	0.8219 s / batch. (data: 1.05e-02). ETA=8:24:46, max mem: 20.9 GB 
[11/27 03:37:09 visual_prompt]: 	Training 300/553. train loss: 11.0636,	0.8224 s / batch. (data: 3.19e-04). ETA=8:23:42, max mem: 20.9 GB 
[11/27 03:38:50 visual_prompt]: 	Training 400/553. train loss: 0.8228,	0.8253 s / batch. (data: 3.10e-04). ETA=8:24:09, max mem: 20.9 GB 
[11/27 03:40:30 visual_prompt]: 	Training 500/553. train loss: 1.5929,	1.4975 s / batch. (data: 6.83e-01). ETA=15:12:13, max mem: 20.9 GB 
[11/27 03:41:22 visual_prompt]: Epoch 34 / 100: avg data time: 1.83e-01, avg batch time: 1.0048, average train loss: 6.2087
[11/27 03:42:20 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3076, average loss: 5.2972
[11/27 03:42:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/27 03:42:20 visual_prompt]: Stopping early.
[11/27 03:42:20 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 03:42:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 03:42:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/27 03:42:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 03:42:20 visual_prompt]: Training with config:
[11/27 03:42:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/27 03:42:20 visual_prompt]: Loading training data...
[11/27 03:42:20 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 03:42:20 visual_prompt]: Loading validation data...
[11/27 03:42:20 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 03:42:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 03:42:22 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 03:42:22 visual_prompt]: tuned percent:0.525
[11/27 03:42:22 visual_prompt]: Device used for model: 0
[11/27 03:42:22 visual_prompt]: Setting up Evaluator...
[11/27 03:42:22 visual_prompt]: Setting up Trainer...
[11/27 03:42:22 visual_prompt]: 	Setting up the optimizer...
[11/27 03:42:22 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 03:44:06 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8143 s / batch. (data: 5.44e-03). ETA=12:29:08, max mem: 20.9 GB 
[11/27 03:45:45 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8147 s / batch. (data: 3.18e-04). ETA=12:28:12, max mem: 20.9 GB 
[11/27 03:47:28 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.2048 s / batch. (data: 3.92e-01). ETA=18:24:21, max mem: 20.9 GB 
[11/27 03:49:06 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8228 s / batch. (data: 3.11e-04). ETA=12:32:51, max mem: 20.9 GB 
[11/27 03:50:48 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8407 s / batch. (data: 8.17e-04). ETA=12:47:51, max mem: 20.9 GB 
[11/27 03:51:41 visual_prompt]: Epoch 1 / 100: avg data time: 1.85e-01, avg batch time: 1.0099, average train loss: 1.5403
[11/27 03:52:38 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3072, average loss: 1.5201
[11/27 03:52:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 03:52:38 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/27 03:54:22 visual_prompt]: 	Training 100/553. train loss: 1.2142,	0.8440 s / batch. (data: 1.60e-02). ETA=12:48:41, max mem: 20.9 GB 
[11/27 03:56:01 visual_prompt]: 	Training 200/553. train loss: 0.0920,	0.8288 s / batch. (data: 2.08e-02). ETA=12:33:31, max mem: 20.9 GB 
[11/27 03:57:45 visual_prompt]: 	Training 300/553. train loss: 1.6199,	1.2000 s / batch. (data: 3.72e-01). ETA=18:08:53, max mem: 20.9 GB 
[11/27 03:59:23 visual_prompt]: 	Training 400/553. train loss: 1.0970,	0.8280 s / batch. (data: 3.19e-04). ETA=12:29:58, max mem: 20.9 GB 
[11/27 04:01:05 visual_prompt]: 	Training 500/553. train loss: 0.5494,	0.8305 s / batch. (data: 3.08e-04). ETA=12:30:49, max mem: 20.9 GB 
[11/27 04:01:56 visual_prompt]: Epoch 2 / 100: avg data time: 1.84e-01, avg batch time: 1.0086, average train loss: 1.3718
[11/27 04:02:53 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3070, average loss: 4.0659
[11/27 04:02:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.87	
[11/27 04:02:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/27 04:04:36 visual_prompt]: 	Training 100/553. train loss: 0.9441,	0.8229 s / batch. (data: 5.60e-03). ETA=12:21:54, max mem: 20.9 GB 
[11/27 04:06:17 visual_prompt]: 	Training 200/553. train loss: 0.7023,	0.8142 s / batch. (data: 2.93e-04). ETA=12:12:40, max mem: 20.9 GB 
[11/27 04:07:56 visual_prompt]: 	Training 300/553. train loss: 1.4231,	0.8280 s / batch. (data: 2.93e-04). ETA=12:23:43, max mem: 20.9 GB 
[11/27 04:09:38 visual_prompt]: 	Training 400/553. train loss: 4.3751,	0.8252 s / batch. (data: 7.94e-03). ETA=12:19:52, max mem: 20.9 GB 
[11/27 04:11:19 visual_prompt]: 	Training 500/553. train loss: 0.7843,	1.1582 s / batch. (data: 3.52e-01). ETA=17:16:27, max mem: 20.9 GB 
[11/27 04:12:09 visual_prompt]: Epoch 3 / 100: avg data time: 1.81e-01, avg batch time: 1.0048, average train loss: 1.6100
[11/27 04:13:06 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3059, average loss: 1.9609
[11/27 04:13:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.84	
[11/27 04:13:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/27 04:14:52 visual_prompt]: 	Training 100/553. train loss: 2.2299,	0.8280 s / batch. (data: 5.41e-03). ETA=12:18:51, max mem: 20.9 GB 
[11/27 04:16:32 visual_prompt]: 	Training 200/553. train loss: 1.0641,	0.8280 s / batch. (data: 7.94e-03). ETA=12:17:28, max mem: 20.9 GB 
[11/27 04:18:13 visual_prompt]: 	Training 300/553. train loss: 0.8041,	1.5749 s / batch. (data: 7.48e-01). ETA=23:20:09, max mem: 20.9 GB 
[11/27 04:19:49 visual_prompt]: 	Training 400/553. train loss: 2.0559,	0.8429 s / batch. (data: 5.45e-03). ETA=12:27:57, max mem: 20.9 GB 
[11/27 04:21:31 visual_prompt]: 	Training 500/553. train loss: 0.0003,	3.4176 s / batch. (data: 2.57e+00). ETA=2 days, 2:26:54, max mem: 20.9 GB 
[11/27 04:22:25 visual_prompt]: Epoch 4 / 100: avg data time: 1.86e-01, avg batch time: 1.0093, average train loss: 2.5073
[11/27 04:23:22 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3060, average loss: 1.3046
[11/27 04:23:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.22	
[11/27 04:23:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/27 04:25:04 visual_prompt]: 	Training 100/553. train loss: 0.0006,	0.8153 s / batch. (data: 5.42e-03). ETA=11:59:59, max mem: 20.9 GB 
[11/27 04:26:45 visual_prompt]: 	Training 200/553. train loss: 3.6976,	1.3495 s / batch. (data: 5.38e-01). ETA=19:49:32, max mem: 20.9 GB 
[11/27 04:28:26 visual_prompt]: 	Training 300/553. train loss: 8.8004,	0.8170 s / batch. (data: 8.94e-03). ETA=11:58:47, max mem: 20.9 GB 
[11/27 04:30:06 visual_prompt]: 	Training 400/553. train loss: 0.6081,	0.8432 s / batch. (data: 1.05e-02). ETA=12:20:24, max mem: 20.9 GB 
[11/27 04:31:47 visual_prompt]: 	Training 500/553. train loss: 1.3376,	0.8424 s / batch. (data: 3.15e-04). ETA=12:18:18, max mem: 20.9 GB 
[11/27 04:32:40 visual_prompt]: Epoch 5 / 100: avg data time: 1.84e-01, avg batch time: 1.0083, average train loss: 2.8567
[11/27 04:33:37 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3080, average loss: 5.3650
[11/27 04:33:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.52	
[11/27 04:33:37 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/27 04:35:23 visual_prompt]: 	Training 100/553. train loss: 1.1663,	0.8120 s / batch. (data: 2.90e-04). ETA=11:49:37, max mem: 20.9 GB 
[11/27 04:37:03 visual_prompt]: 	Training 200/553. train loss: 3.3154,	0.8644 s / batch. (data: 1.05e-02). ETA=12:33:59, max mem: 20.9 GB 
[11/27 04:38:40 visual_prompt]: 	Training 300/553. train loss: 0.7684,	0.8457 s / batch. (data: 9.68e-03). ETA=12:16:16, max mem: 20.9 GB 
[11/27 04:40:25 visual_prompt]: 	Training 400/553. train loss: 1.2629,	0.8240 s / batch. (data: 5.42e-03). ETA=11:56:00, max mem: 20.9 GB 
[11/27 04:42:04 visual_prompt]: 	Training 500/553. train loss: 5.1409,	0.8311 s / batch. (data: 1.55e-02). ETA=12:00:46, max mem: 20.9 GB 
[11/27 04:42:56 visual_prompt]: Epoch 6 / 100: avg data time: 1.87e-01, avg batch time: 1.0108, average train loss: 2.0619
[11/27 04:43:53 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3054, average loss: 0.9227
[11/27 04:43:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.91	
[11/27 04:43:53 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/27 04:45:36 visual_prompt]: 	Training 100/553. train loss: 4.5842,	0.8225 s / batch. (data: 5.41e-03). ETA=11:51:12, max mem: 20.9 GB 
[11/27 04:47:16 visual_prompt]: 	Training 200/553. train loss: 0.7425,	0.8218 s / batch. (data: 5.43e-03). ETA=11:49:12, max mem: 20.9 GB 
[11/27 04:48:59 visual_prompt]: 	Training 300/553. train loss: 1.6967,	2.0079 s / batch. (data: 1.17e+00). ETA=1 day, 4:49:34, max mem: 20.9 GB 
[11/27 04:50:39 visual_prompt]: 	Training 400/553. train loss: 0.6390,	1.8508 s / batch. (data: 1.01e+00). ETA=1 day, 2:31:05, max mem: 20.9 GB 
[11/27 04:52:17 visual_prompt]: 	Training 500/553. train loss: 0.6944,	0.8203 s / batch. (data: 1.05e-02). ETA=11:43:49, max mem: 20.9 GB 
[11/27 04:53:09 visual_prompt]: Epoch 7 / 100: avg data time: 1.81e-01, avg batch time: 1.0046, average train loss: 2.1026
[11/27 04:54:06 visual_prompt]: Inference (val):avg data time: 3.44e-04, avg batch time: 0.3065, average loss: 1.9892
[11/27 04:54:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.76	
[11/27 04:54:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/27 04:55:49 visual_prompt]: 	Training 100/553. train loss: 8.7400,	0.8085 s / batch. (data: 2.74e-04). ETA=11:31:38, max mem: 20.9 GB 
[11/27 04:57:31 visual_prompt]: 	Training 200/553. train loss: 11.3453,	0.8334 s / batch. (data: 3.32e-04). ETA=11:51:32, max mem: 20.9 GB 
[11/27 04:59:12 visual_prompt]: 	Training 300/553. train loss: 2.3540,	0.8110 s / batch. (data: 2.90e-04). ETA=11:31:06, max mem: 20.9 GB 
[11/27 05:00:52 visual_prompt]: 	Training 400/553. train loss: 3.7570,	1.0307 s / batch. (data: 2.12e-01). ETA=14:36:33, max mem: 20.9 GB 
[11/27 05:02:33 visual_prompt]: 	Training 500/553. train loss: 6.8638,	1.4116 s / batch. (data: 6.00e-01). ETA=19:58:09, max mem: 20.9 GB 
[11/27 05:03:25 visual_prompt]: Epoch 8 / 100: avg data time: 1.85e-01, avg batch time: 1.0101, average train loss: 5.9094
[11/27 05:04:22 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3079, average loss: 3.1904
[11/27 05:04:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.02	
[11/27 05:04:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/27 05:06:06 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8360 s / batch. (data: 1.20e-02). ETA=11:47:28, max mem: 20.9 GB 
[11/27 05:07:45 visual_prompt]: 	Training 200/553. train loss: 0.5124,	0.8140 s / batch. (data: 2.87e-04). ETA=11:27:31, max mem: 20.9 GB 
[11/27 05:09:26 visual_prompt]: 	Training 300/553. train loss: 2.3567,	1.7440 s / batch. (data: 9.19e-01). ETA=1 day, 0:30:05, max mem: 20.9 GB 
[11/27 05:11:08 visual_prompt]: 	Training 400/553. train loss: 2.7290,	0.8440 s / batch. (data: 2.98e-04). ETA=11:50:01, max mem: 20.9 GB 
[11/27 05:12:49 visual_prompt]: 	Training 500/553. train loss: 0.4908,	1.0222 s / batch. (data: 2.08e-01). ETA=14:18:12, max mem: 20.9 GB 
[11/27 05:13:40 visual_prompt]: Epoch 9 / 100: avg data time: 1.86e-01, avg batch time: 1.0092, average train loss: 2.7112
[11/27 05:14:38 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3072, average loss: 0.8254
[11/27 05:14:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.13	
[11/27 05:14:38 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/27 05:16:24 visual_prompt]: 	Training 100/553. train loss: 11.5821,	0.8280 s / batch. (data: 3.03e-04). ETA=11:33:04, max mem: 20.9 GB 
[11/27 05:18:03 visual_prompt]: 	Training 200/553. train loss: 0.5792,	0.8401 s / batch. (data: 2.63e-04). ETA=11:41:49, max mem: 20.9 GB 
[11/27 05:19:42 visual_prompt]: 	Training 300/553. train loss: 1.7898,	0.8131 s / batch. (data: 7.94e-03). ETA=11:17:55, max mem: 20.9 GB 
[11/27 05:21:20 visual_prompt]: 	Training 400/553. train loss: 4.9888,	0.8222 s / batch. (data: 3.41e-04). ETA=11:24:09, max mem: 20.9 GB 
[11/27 05:23:01 visual_prompt]: 	Training 500/553. train loss: 1.6759,	0.8650 s / batch. (data: 2.09e-02). ETA=11:58:15, max mem: 20.9 GB 
[11/27 05:23:55 visual_prompt]: Epoch 10 / 100: avg data time: 1.85e-01, avg batch time: 1.0070, average train loss: 4.3058
[11/27 05:24:52 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3075, average loss: 1.0591
[11/27 05:24:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.98	
[11/27 05:24:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/27 05:26:38 visual_prompt]: 	Training 100/553. train loss: 5.5329,	0.8320 s / batch. (data: 2.58e-04). ETA=11:28:44, max mem: 20.9 GB 
[11/27 05:28:20 visual_prompt]: 	Training 200/553. train loss: 8.8807,	0.8469 s / batch. (data: 8.19e-04). ETA=11:39:42, max mem: 20.9 GB 
[11/27 05:30:00 visual_prompt]: 	Training 300/553. train loss: 0.0029,	2.1320 s / batch. (data: 1.30e+00). ETA=1 day, 5:17:49, max mem: 20.9 GB 
[11/27 05:31:38 visual_prompt]: 	Training 400/553. train loss: 2.8477,	0.8141 s / batch. (data: 5.40e-03). ETA=11:09:54, max mem: 20.9 GB 
[11/27 05:33:17 visual_prompt]: 	Training 500/553. train loss: 2.4086,	0.8181 s / batch. (data: 1.05e-02). ETA=11:11:46, max mem: 20.9 GB 
[11/27 05:34:08 visual_prompt]: Epoch 11 / 100: avg data time: 1.82e-01, avg batch time: 1.0060, average train loss: 2.4816
[11/27 05:35:05 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3059, average loss: 4.5119
[11/27 05:35:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.09	
[11/27 05:35:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/27 05:36:51 visual_prompt]: 	Training 100/553. train loss: 1.4814,	0.8320 s / batch. (data: 3.26e-04). ETA=11:21:04, max mem: 20.9 GB 
[11/27 05:38:32 visual_prompt]: 	Training 200/553. train loss: 1.5926,	0.8298 s / batch. (data: 3.09e-04). ETA=11:17:56, max mem: 20.9 GB 
[11/27 05:40:11 visual_prompt]: 	Training 300/553. train loss: 4.6174,	0.8298 s / batch. (data: 1.59e-02). ETA=11:16:31, max mem: 20.9 GB 
[11/27 05:41:51 visual_prompt]: 	Training 400/553. train loss: 8.3687,	0.8160 s / batch. (data: 3.02e-04). ETA=11:03:54, max mem: 20.9 GB 
[11/27 05:43:31 visual_prompt]: 	Training 500/553. train loss: 23.9205,	0.8200 s / batch. (data: 7.98e-03). ETA=11:05:47, max mem: 20.9 GB 
[11/27 05:44:22 visual_prompt]: Epoch 12 / 100: avg data time: 1.83e-01, avg batch time: 1.0070, average train loss: 7.8640
[11/27 05:45:20 visual_prompt]: Inference (val):avg data time: 2.22e-04, avg batch time: 0.3060, average loss: 0.8345
[11/27 05:45:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.96	
[11/27 05:45:20 visual_prompt]: Best epoch 12: best metric: -0.834
[11/27 05:45:20 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/27 05:47:06 visual_prompt]: 	Training 100/553. train loss: 4.5378,	0.8200 s / batch. (data: 3.06e-04). ETA=11:03:43, max mem: 20.9 GB 
[11/27 05:48:43 visual_prompt]: 	Training 200/553. train loss: 6.0635,	0.8466 s / batch. (data: 2.06e-02). ETA=11:23:49, max mem: 20.9 GB 
[11/27 05:50:24 visual_prompt]: 	Training 300/553. train loss: 3.3955,	1.7657 s / batch. (data: 9.44e-01). ETA=23:43:15, max mem: 20.9 GB 
[11/27 05:52:03 visual_prompt]: 	Training 400/553. train loss: 10.6050,	0.8396 s / batch. (data: 1.06e-02). ETA=11:15:20, max mem: 20.9 GB 
[11/27 05:53:44 visual_prompt]: 	Training 500/553. train loss: 9.7557,	0.8240 s / batch. (data: 2.78e-04). ETA=11:01:26, max mem: 20.9 GB 
[11/27 05:54:37 visual_prompt]: Epoch 13 / 100: avg data time: 1.83e-01, avg batch time: 1.0067, average train loss: 6.9726
[11/27 05:55:34 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3055, average loss: 1.6762
[11/27 05:55:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.13	
[11/27 05:55:34 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/27 05:57:20 visual_prompt]: 	Training 100/553. train loss: 21.1755,	0.8420 s / batch. (data: 1.40e-02). ETA=11:13:47, max mem: 20.9 GB 
[11/27 05:59:00 visual_prompt]: 	Training 200/553. train loss: 0.0000,	1.3206 s / batch. (data: 5.15e-01). ETA=17:34:32, max mem: 20.9 GB 
[11/27 06:00:40 visual_prompt]: 	Training 300/553. train loss: 2.2619,	0.8160 s / batch. (data: 7.96e-03). ETA=10:50:13, max mem: 20.9 GB 
[11/27 06:02:19 visual_prompt]: 	Training 400/553. train loss: 2.6300,	0.8331 s / batch. (data: 2.89e-04). ETA=11:02:27, max mem: 20.9 GB 
[11/27 06:03:59 visual_prompt]: 	Training 500/553. train loss: 4.8886,	0.8108 s / batch. (data: 5.42e-03). ETA=10:43:20, max mem: 20.9 GB 
[11/27 06:04:51 visual_prompt]: Epoch 14 / 100: avg data time: 1.84e-01, avg batch time: 1.0065, average train loss: 5.9034
[11/27 06:05:48 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3063, average loss: 5.0850
[11/27 06:05:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.40	
[11/27 06:05:48 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/27 06:07:32 visual_prompt]: 	Training 100/553. train loss: 6.5107,	0.8320 s / batch. (data: 2.84e-04). ETA=10:58:05, max mem: 20.9 GB 
[11/27 06:09:10 visual_prompt]: 	Training 200/553. train loss: 1.9310,	0.8200 s / batch. (data: 2.93e-04). ETA=10:47:13, max mem: 20.9 GB 
[11/27 06:10:53 visual_prompt]: 	Training 300/553. train loss: 13.6328,	0.8396 s / batch. (data: 2.06e-02). ETA=11:01:16, max mem: 20.9 GB 
[11/27 06:12:30 visual_prompt]: 	Training 400/553. train loss: 5.5660,	0.9947 s / batch. (data: 1.74e-01). ETA=13:01:48, max mem: 20.9 GB 
[11/27 06:14:11 visual_prompt]: 	Training 500/553. train loss: 13.2362,	0.8400 s / batch. (data: 3.04e-04). ETA=10:58:49, max mem: 20.9 GB 
[11/27 06:15:04 visual_prompt]: Epoch 15 / 100: avg data time: 1.82e-01, avg batch time: 1.0047, average train loss: 8.9296
[11/27 06:16:01 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3084, average loss: 0.7932
[11/27 06:16:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.85	
[11/27 06:16:01 visual_prompt]: Best epoch 15: best metric: -0.793
[11/27 06:16:01 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/27 06:17:44 visual_prompt]: 	Training 100/553. train loss: 6.6863,	0.8146 s / batch. (data: 2.82e-04). ETA=10:36:49, max mem: 20.9 GB 
[11/27 06:19:24 visual_prompt]: 	Training 200/553. train loss: 0.8400,	0.8219 s / batch. (data: 2.91e-04). ETA=10:41:10, max mem: 20.9 GB 
[11/27 06:21:04 visual_prompt]: 	Training 300/553. train loss: 1.1524,	0.8349 s / batch. (data: 2.96e-04). ETA=10:49:54, max mem: 20.9 GB 
[11/27 06:22:45 visual_prompt]: 	Training 400/553. train loss: 5.8702,	0.8202 s / batch. (data: 2.97e-04). ETA=10:37:06, max mem: 20.9 GB 
[11/27 06:24:24 visual_prompt]: 	Training 500/553. train loss: 10.1401,	1.1954 s / batch. (data: 3.90e-01). ETA=15:26:31, max mem: 20.9 GB 
[11/27 06:25:17 visual_prompt]: Epoch 16 / 100: avg data time: 1.81e-01, avg batch time: 1.0049, average train loss: 4.0607
[11/27 06:26:14 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3076, average loss: 1.6048
[11/27 06:26:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.91	
[11/27 06:26:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/27 06:27:58 visual_prompt]: 	Training 100/553. train loss: 7.7257,	0.8154 s / batch. (data: 7.92e-03). ETA=10:29:56, max mem: 20.9 GB 
[11/27 06:29:39 visual_prompt]: 	Training 200/553. train loss: 2.0183,	0.8218 s / batch. (data: 2.68e-04). ETA=10:33:31, max mem: 20.9 GB 
[11/27 06:31:18 visual_prompt]: 	Training 300/553. train loss: 6.1852,	0.8454 s / batch. (data: 7.93e-03). ETA=10:50:18, max mem: 20.9 GB 
[11/27 06:32:58 visual_prompt]: 	Training 400/553. train loss: 0.6028,	1.2533 s / batch. (data: 4.37e-01). ETA=16:01:56, max mem: 20.9 GB 
[11/27 06:34:38 visual_prompt]: 	Training 500/553. train loss: 0.8317,	1.6436 s / batch. (data: 8.37e-01). ETA=20:58:46, max mem: 20.9 GB 
[11/27 06:35:31 visual_prompt]: Epoch 17 / 100: avg data time: 1.83e-01, avg batch time: 1.0070, average train loss: 5.6851
[11/27 06:36:28 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3071, average loss: 3.2075
[11/27 06:36:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.88	
[11/27 06:36:28 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/27 06:38:13 visual_prompt]: 	Training 100/553. train loss: 8.7789,	0.8289 s / batch. (data: 3.16e-04). ETA=10:32:43, max mem: 20.9 GB 
[11/27 06:39:56 visual_prompt]: 	Training 200/553. train loss: 6.0809,	0.8320 s / batch. (data: 3.05e-04). ETA=10:33:40, max mem: 20.9 GB 
[11/27 06:41:36 visual_prompt]: 	Training 300/553. train loss: 2.3445,	0.8250 s / batch. (data: 2.99e-04). ETA=10:26:58, max mem: 20.9 GB 
[11/27 06:43:16 visual_prompt]: 	Training 400/553. train loss: 4.4757,	0.8240 s / batch. (data: 2.80e-04). ETA=10:24:51, max mem: 20.9 GB 
[11/27 06:44:55 visual_prompt]: 	Training 500/553. train loss: 1.6489,	0.8208 s / batch. (data: 2.81e-04). ETA=10:21:05, max mem: 20.9 GB 
[11/27 06:45:46 visual_prompt]: Epoch 18 / 100: avg data time: 1.85e-01, avg batch time: 1.0082, average train loss: 4.2608
[11/27 06:46:43 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3062, average loss: 2.0046
[11/27 06:46:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.59	
[11/27 06:46:43 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/27 06:48:27 visual_prompt]: 	Training 100/553. train loss: 1.2388,	0.8556 s / batch. (data: 1.05e-02). ETA=10:45:11, max mem: 20.9 GB 
[11/27 06:50:08 visual_prompt]: 	Training 200/553. train loss: 2.4127,	0.8280 s / batch. (data: 7.46e-04). ETA=10:22:59, max mem: 20.9 GB 
[11/27 06:51:48 visual_prompt]: 	Training 300/553. train loss: 8.9535,	0.8425 s / batch. (data: 2.06e-02). ETA=10:32:30, max mem: 20.9 GB 
[11/27 06:53:30 visual_prompt]: 	Training 400/553. train loss: 2.3611,	0.8190 s / batch. (data: 7.88e-04). ETA=10:13:31, max mem: 20.9 GB 
[11/27 06:55:05 visual_prompt]: 	Training 500/553. train loss: 0.6586,	0.8200 s / batch. (data: 5.44e-03). ETA=10:12:51, max mem: 20.9 GB 
[11/27 06:55:58 visual_prompt]: Epoch 19 / 100: avg data time: 1.79e-01, avg batch time: 1.0023, average train loss: 2.8640
[11/27 06:56:55 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3058, average loss: 8.3813
[11/27 06:56:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.01	
[11/27 06:56:55 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/27 06:58:38 visual_prompt]: 	Training 100/553. train loss: 0.6519,	0.8280 s / batch. (data: 2.88e-04). ETA=10:16:45, max mem: 20.9 GB 
[11/27 07:00:20 visual_prompt]: 	Training 200/553. train loss: 1.2496,	0.8360 s / batch. (data: 3.40e-04). ETA=10:21:19, max mem: 20.9 GB 
[11/27 07:02:00 visual_prompt]: 	Training 300/553. train loss: 0.8050,	0.8320 s / batch. (data: 8.03e-04). ETA=10:16:57, max mem: 20.9 GB 
[11/27 07:03:40 visual_prompt]: 	Training 400/553. train loss: 1.9979,	0.8417 s / batch. (data: 5.45e-03). ETA=10:22:43, max mem: 20.9 GB 
[11/27 07:05:19 visual_prompt]: 	Training 500/553. train loss: 1.2681,	0.8211 s / batch. (data: 3.02e-04). ETA=10:06:09, max mem: 20.9 GB 
[11/27 07:06:13 visual_prompt]: Epoch 20 / 100: avg data time: 1.85e-01, avg batch time: 1.0077, average train loss: 5.0237
[11/27 07:07:10 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3075, average loss: 3.8102
[11/27 07:07:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.32	
[11/27 07:07:10 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/27 07:08:57 visual_prompt]: 	Training 100/553. train loss: 7.4528,	0.8160 s / batch. (data: 3.00e-04). ETA=10:00:18, max mem: 20.9 GB 
[11/27 07:10:35 visual_prompt]: 	Training 200/553. train loss: 5.4756,	0.8549 s / batch. (data: 3.34e-02). ETA=10:27:29, max mem: 20.9 GB 
[11/27 07:12:16 visual_prompt]: 	Training 300/553. train loss: 21.7955,	0.9848 s / batch. (data: 1.74e-01). ETA=12:01:13, max mem: 20.9 GB 
[11/27 07:13:54 visual_prompt]: 	Training 400/553. train loss: 6.5294,	0.8419 s / batch. (data: 9.81e-03). ETA=10:15:07, max mem: 20.9 GB 
[11/27 07:15:36 visual_prompt]: 	Training 500/553. train loss: 5.4555,	0.8274 s / batch. (data: 1.05e-02). ETA=10:03:09, max mem: 20.9 GB 
[11/27 07:16:27 visual_prompt]: Epoch 21 / 100: avg data time: 1.84e-01, avg batch time: 1.0067, average train loss: 5.0771
[11/27 07:17:24 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3056, average loss: 0.6898
[11/27 07:17:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.28	
[11/27 07:17:24 visual_prompt]: Best epoch 21: best metric: -0.690
[11/27 07:17:24 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/27 07:19:07 visual_prompt]: 	Training 100/553. train loss: 1.6547,	0.8193 s / batch. (data: 5.42e-03). ETA=9:55:11, max mem: 20.9 GB 
[11/27 07:20:47 visual_prompt]: 	Training 200/553. train loss: 4.8469,	0.8736 s / batch. (data: 3.02e-04). ETA=10:33:11, max mem: 20.9 GB 
[11/27 07:22:25 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8066 s / batch. (data: 3.00e-04). ETA=9:43:16, max mem: 20.9 GB 
[11/27 07:24:07 visual_prompt]: 	Training 400/553. train loss: 21.3525,	0.8480 s / batch. (data: 3.47e-04). ETA=10:11:48, max mem: 20.9 GB 
[11/27 07:25:46 visual_prompt]: 	Training 500/553. train loss: 3.4759,	0.8354 s / batch. (data: 5.43e-03). ETA=10:01:16, max mem: 20.9 GB 
[11/27 07:26:40 visual_prompt]: Epoch 22 / 100: avg data time: 1.83e-01, avg batch time: 1.0058, average train loss: 6.3588
[11/27 07:27:38 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3074, average loss: 1.1448
[11/27 07:27:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.40	
[11/27 07:27:38 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/27 07:29:23 visual_prompt]: 	Training 100/553. train loss: 1.8340,	0.8337 s / batch. (data: 2.57e-02). ETA=9:57:56, max mem: 20.9 GB 
[11/27 07:31:03 visual_prompt]: 	Training 200/553. train loss: 1.3014,	0.8901 s / batch. (data: 8.34e-02). ETA=10:36:56, max mem: 20.9 GB 
[11/27 07:32:46 visual_prompt]: 	Training 300/553. train loss: 1.6809,	0.8280 s / batch. (data: 7.88e-04). ETA=9:51:05, max mem: 20.9 GB 
[11/27 07:34:23 visual_prompt]: 	Training 400/553. train loss: 2.2285,	0.8360 s / batch. (data: 3.24e-04). ETA=9:55:25, max mem: 20.9 GB 
[11/27 07:36:02 visual_prompt]: 	Training 500/553. train loss: 0.0010,	0.8260 s / batch. (data: 1.20e-02). ETA=9:46:54, max mem: 20.9 GB 
[11/27 07:36:54 visual_prompt]: Epoch 23 / 100: avg data time: 1.83e-01, avg batch time: 1.0049, average train loss: 3.0160
[11/27 07:37:51 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3064, average loss: 1.4121
[11/27 07:37:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.34	
[11/27 07:37:51 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/27 07:39:32 visual_prompt]: 	Training 100/553. train loss: 5.5223,	0.8244 s / batch. (data: 1.05e-02). ETA=9:43:40, max mem: 20.9 GB 
[11/27 07:41:12 visual_prompt]: 	Training 200/553. train loss: 0.9238,	0.8280 s / batch. (data: 1.19e-02). ETA=9:44:51, max mem: 20.9 GB 
[11/27 07:42:52 visual_prompt]: 	Training 300/553. train loss: 2.2984,	1.1291 s / batch. (data: 3.09e-01). ETA=13:15:39, max mem: 20.9 GB 
[11/27 07:44:33 visual_prompt]: 	Training 400/553. train loss: 2.7778,	0.8200 s / batch. (data: 2.98e-04). ETA=9:36:28, max mem: 20.9 GB 
[11/27 07:46:15 visual_prompt]: 	Training 500/553. train loss: 8.6358,	0.8079 s / batch. (data: 2.76e-04). ETA=9:26:38, max mem: 20.9 GB 
[11/27 07:47:08 visual_prompt]: Epoch 24 / 100: avg data time: 1.84e-01, avg batch time: 1.0067, average train loss: 4.2892
[11/27 07:48:05 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3078, average loss: 15.8930
[11/27 07:48:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.24	
[11/27 07:48:05 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/27 07:49:52 visual_prompt]: 	Training 100/553. train loss: 16.2428,	0.8080 s / batch. (data: 3.02e-04). ETA=9:24:38, max mem: 20.9 GB 
[11/27 07:51:29 visual_prompt]: 	Training 200/553. train loss: 2.1483,	0.8378 s / batch. (data: 1.57e-02). ETA=9:44:03, max mem: 20.9 GB 
[11/27 07:53:09 visual_prompt]: 	Training 300/553. train loss: 3.0458,	0.8358 s / batch. (data: 3.03e-04). ETA=9:41:17, max mem: 20.9 GB 
[11/27 07:54:49 visual_prompt]: 	Training 400/553. train loss: 3.4441,	1.3021 s / batch. (data: 4.80e-01). ETA=15:03:24, max mem: 20.9 GB 
[11/27 07:56:29 visual_prompt]: 	Training 500/553. train loss: 6.8680,	1.1900 s / batch. (data: 3.54e-01). ETA=13:43:38, max mem: 20.9 GB 
[11/27 07:57:22 visual_prompt]: Epoch 25 / 100: avg data time: 1.83e-01, avg batch time: 1.0071, average train loss: 6.1061
[11/27 07:58:19 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3069, average loss: 9.2456
[11/27 07:58:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.45	
[11/27 07:58:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/27 08:00:03 visual_prompt]: 	Training 100/553. train loss: 10.9275,	0.8360 s / batch. (data: 3.11e-04). ETA=9:36:29, max mem: 20.9 GB 
[11/27 08:01:45 visual_prompt]: 	Training 200/553. train loss: 27.0044,	1.8168 s / batch. (data: 1.01e+00). ETA=20:49:46, max mem: 20.9 GB 
[11/27 08:03:26 visual_prompt]: 	Training 300/553. train loss: 0.0091,	0.8185 s / batch. (data: 7.68e-04). ETA=9:21:42, max mem: 20.9 GB 
[11/27 08:05:04 visual_prompt]: 	Training 400/553. train loss: 0.7513,	0.8320 s / batch. (data: 7.97e-03). ETA=9:29:34, max mem: 20.9 GB 
[11/27 08:06:43 visual_prompt]: 	Training 500/553. train loss: 1.5391,	0.8120 s / batch. (data: 2.33e-04). ETA=9:14:32, max mem: 20.9 GB 
[11/27 08:07:36 visual_prompt]: Epoch 26 / 100: avg data time: 1.83e-01, avg batch time: 1.0065, average train loss: 3.9692
[11/27 08:08:33 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3075, average loss: 1.9148
[11/27 08:08:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.57	
[11/27 08:08:33 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/27 08:10:18 visual_prompt]: 	Training 100/553. train loss: 7.9751,	0.8560 s / batch. (data: 5.43e-03). ETA=9:42:23, max mem: 20.9 GB 
[11/27 08:11:58 visual_prompt]: 	Training 200/553. train loss: 4.2559,	1.3285 s / batch. (data: 5.21e-01). ETA=15:01:38, max mem: 20.9 GB 
[11/27 08:13:37 visual_prompt]: 	Training 300/553. train loss: 3.7082,	0.8280 s / batch. (data: 7.97e-03). ETA=9:20:34, max mem: 20.9 GB 
[11/27 08:15:19 visual_prompt]: 	Training 400/553. train loss: 12.4739,	0.8357 s / batch. (data: 5.97e-03). ETA=9:24:24, max mem: 20.9 GB 
[11/27 08:17:00 visual_prompt]: 	Training 500/553. train loss: 2.9141,	0.8419 s / batch. (data: 7.16e-04). ETA=9:27:09, max mem: 20.9 GB 
[11/27 08:17:50 visual_prompt]: Epoch 27 / 100: avg data time: 1.84e-01, avg batch time: 1.0077, average train loss: 5.5327
[11/27 08:18:48 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3069, average loss: 2.9857
[11/27 08:18:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.25	
[11/27 08:18:48 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[11/27 08:20:30 visual_prompt]: 	Training 100/553. train loss: 0.0005,	0.9466 s / batch. (data: 1.40e-01). ETA=10:35:19, max mem: 20.9 GB 
[11/27 08:22:11 visual_prompt]: 	Training 200/553. train loss: 14.3116,	0.8113 s / batch. (data: 3.18e-04). ETA=9:03:08, max mem: 20.9 GB 
[11/27 08:23:52 visual_prompt]: 	Training 300/553. train loss: 3.0044,	1.2990 s / batch. (data: 4.75e-01). ETA=14:27:29, max mem: 20.9 GB 
[11/27 08:25:32 visual_prompt]: 	Training 400/553. train loss: 7.5925,	0.8160 s / batch. (data: 7.96e-03). ETA=9:03:34, max mem: 20.9 GB 
[11/27 08:27:11 visual_prompt]: 	Training 500/553. train loss: 0.0967,	0.8334 s / batch. (data: 7.59e-04). ETA=9:13:48, max mem: 20.9 GB 
[11/27 08:28:03 visual_prompt]: Epoch 28 / 100: avg data time: 1.81e-01, avg batch time: 1.0041, average train loss: 3.8640
[11/27 08:29:00 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3058, average loss: 0.9024
[11/27 08:29:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.51	
[11/27 08:29:00 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[11/27 08:30:50 visual_prompt]: 	Training 100/553. train loss: 3.2007,	0.8342 s / batch. (data: 1.05e-02). ETA=9:12:10, max mem: 20.9 GB 
[11/27 08:32:30 visual_prompt]: 	Training 200/553. train loss: 3.3687,	1.8950 s / batch. (data: 1.08e+00). ETA=20:51:14, max mem: 20.9 GB 
[11/27 08:34:08 visual_prompt]: 	Training 300/553. train loss: 3.0620,	0.8257 s / batch. (data: 1.05e-03). ETA=9:03:47, max mem: 20.9 GB 
[11/27 08:35:44 visual_prompt]: 	Training 400/553. train loss: 5.5722,	1.3120 s / batch. (data: 4.89e-01). ETA=14:21:54, max mem: 20.9 GB 
[11/27 08:37:25 visual_prompt]: 	Training 500/553. train loss: 5.5548,	0.8280 s / batch. (data: 3.40e-04). ETA=9:02:33, max mem: 20.9 GB 
[11/27 08:38:17 visual_prompt]: Epoch 29 / 100: avg data time: 1.82e-01, avg batch time: 1.0059, average train loss: 4.2439
[11/27 08:39:14 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3057, average loss: 5.3311
[11/27 08:39:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.96	
[11/27 08:39:14 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[11/27 08:40:56 visual_prompt]: 	Training 100/553. train loss: 1.5523,	0.8200 s / batch. (data: 7.94e-03). ETA=8:55:14, max mem: 20.9 GB 
[11/27 08:42:38 visual_prompt]: 	Training 200/553. train loss: 2.2251,	0.8160 s / batch. (data: 5.40e-03). ETA=8:51:15, max mem: 20.9 GB 
[11/27 08:44:16 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.9800 s / batch. (data: 1.50e-01). ETA=10:36:23, max mem: 20.9 GB 
[11/27 08:45:57 visual_prompt]: 	Training 400/553. train loss: 0.8466,	1.0760 s / batch. (data: 2.55e-01). ETA=11:36:56, max mem: 20.9 GB 
[11/27 08:47:37 visual_prompt]: 	Training 500/553. train loss: 1.1982,	1.4738 s / batch. (data: 6.52e-01). ETA=15:52:09, max mem: 20.9 GB 
[11/27 08:48:31 visual_prompt]: Epoch 30 / 100: avg data time: 1.84e-01, avg batch time: 1.0070, average train loss: 2.4570
[11/27 08:49:28 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3084, average loss: 0.8049
[11/27 08:49:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.99	
[11/27 08:49:28 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[11/27 08:51:13 visual_prompt]: 	Training 100/553. train loss: 1.4897,	0.8219 s / batch. (data: 2.85e-04). ETA=8:48:53, max mem: 20.9 GB 
[11/27 08:52:54 visual_prompt]: 	Training 200/553. train loss: 14.7473,	0.8480 s / batch. (data: 1.08e-02). ETA=9:04:16, max mem: 20.9 GB 
[11/27 08:54:33 visual_prompt]: 	Training 300/553. train loss: 4.5189,	0.8400 s / batch. (data: 7.93e-03). ETA=8:57:44, max mem: 20.9 GB 
[11/27 08:56:12 visual_prompt]: 	Training 400/553. train loss: 1.6754,	1.5760 s / batch. (data: 7.54e-01). ETA=16:46:16, max mem: 20.9 GB 
[11/27 08:57:52 visual_prompt]: 	Training 500/553. train loss: 0.7275,	0.8333 s / batch. (data: 2.72e-02). ETA=8:50:40, max mem: 20.9 GB 
[11/27 08:58:43 visual_prompt]: Epoch 31 / 100: avg data time: 1.81e-01, avg batch time: 1.0040, average train loss: 3.5094
[11/27 08:59:41 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3070, average loss: 0.9829
[11/27 08:59:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.52	
[11/27 08:59:41 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[11/27 09:01:26 visual_prompt]: 	Training 100/553. train loss: 0.6376,	0.8103 s / batch. (data: 3.13e-04). ETA=8:33:57, max mem: 20.9 GB 
[11/27 09:03:06 visual_prompt]: 	Training 200/553. train loss: 0.6640,	0.8317 s / batch. (data: 5.40e-03). ETA=8:46:09, max mem: 20.9 GB 
[11/27 09:04:49 visual_prompt]: 	Training 300/553. train loss: 2.4566,	0.8127 s / batch. (data: 5.41e-03). ETA=8:32:44, max mem: 20.9 GB 
[11/27 09:06:29 visual_prompt]: 	Training 400/553. train loss: 9.4297,	0.8130 s / batch. (data: 5.44e-03). ETA=8:31:37, max mem: 20.9 GB 
[11/27 09:08:06 visual_prompt]: 	Training 500/553. train loss: 1.0623,	0.8177 s / batch. (data: 9.61e-03). ETA=8:33:11, max mem: 20.9 GB 
[11/27 09:08:57 visual_prompt]: Epoch 32 / 100: avg data time: 1.82e-01, avg batch time: 1.0055, average train loss: 4.0424
[11/27 09:09:54 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3079, average loss: 4.6429
[11/27 09:09:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.22	
[11/27 09:09:54 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[11/27 09:11:36 visual_prompt]: 	Training 100/553. train loss: 0.0045,	0.8080 s / batch. (data: 3.01e-04). ETA=8:25:04, max mem: 20.9 GB 
[11/27 09:13:19 visual_prompt]: 	Training 200/553. train loss: 7.5363,	1.4120 s / batch. (data: 5.92e-01). ETA=14:40:14, max mem: 20.9 GB 
[11/27 09:14:58 visual_prompt]: 	Training 300/553. train loss: 1.1980,	0.8400 s / batch. (data: 2.57e-04). ETA=8:42:15, max mem: 20.9 GB 
[11/27 09:16:39 visual_prompt]: 	Training 400/553. train loss: 1.8828,	0.8071 s / batch. (data: 3.03e-04). ETA=8:20:25, max mem: 20.9 GB 
[11/27 09:18:19 visual_prompt]: 	Training 500/553. train loss: 1.0904,	0.8280 s / batch. (data: 3.09e-04). ETA=8:32:02, max mem: 20.9 GB 
[11/27 09:19:10 visual_prompt]: Epoch 33 / 100: avg data time: 1.82e-01, avg batch time: 1.0046, average train loss: 4.0622
[11/27 09:20:07 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3059, average loss: 9.3758
[11/27 09:20:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.62	
[11/27 09:20:07 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[11/27 09:21:52 visual_prompt]: 	Training 100/553. train loss: 7.7062,	0.8223 s / batch. (data: 7.92e-03). ETA=8:26:24, max mem: 20.9 GB 
[11/27 09:23:30 visual_prompt]: 	Training 200/553. train loss: 1.5949,	0.8290 s / batch. (data: 5.42e-03). ETA=8:29:08, max mem: 20.9 GB 
[11/27 09:25:09 visual_prompt]: 	Training 300/553. train loss: 3.7869,	0.8200 s / batch. (data: 7.94e-03). ETA=8:22:15, max mem: 20.9 GB 
[11/27 09:26:51 visual_prompt]: 	Training 400/553. train loss: 1.8363,	0.8313 s / batch. (data: 5.42e-03). ETA=8:27:48, max mem: 20.9 GB 
[11/27 09:28:31 visual_prompt]: 	Training 500/553. train loss: 0.7041,	1.4807 s / batch. (data: 6.47e-01). ETA=15:02:01, max mem: 20.9 GB 
[11/27 09:29:23 visual_prompt]: Epoch 34 / 100: avg data time: 1.82e-01, avg batch time: 1.0056, average train loss: 4.8986
[11/27 09:30:20 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3056, average loss: 12.1164
[11/27 09:30:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.31	
[11/27 09:30:20 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[11/27 09:32:06 visual_prompt]: 	Training 100/553. train loss: 6.5282,	0.8161 s / batch. (data: 2.80e-04). ETA=8:15:03, max mem: 20.9 GB 
[11/27 09:33:47 visual_prompt]: 	Training 200/553. train loss: 1.3176,	0.8200 s / batch. (data: 3.14e-04). ETA=8:16:04, max mem: 20.9 GB 
[11/27 09:35:25 visual_prompt]: 	Training 300/553. train loss: 0.7490,	0.8283 s / batch. (data: 2.91e-04). ETA=8:19:42, max mem: 20.9 GB 
[11/27 09:37:04 visual_prompt]: 	Training 400/553. train loss: 3.2232,	0.9292 s / batch. (data: 1.13e-01). ETA=9:19:03, max mem: 20.9 GB 
[11/27 09:38:44 visual_prompt]: 	Training 500/553. train loss: 0.7879,	1.2118 s / batch. (data: 4.03e-01). ETA=12:07:03, max mem: 20.9 GB 
[11/27 09:39:37 visual_prompt]: Epoch 35 / 100: avg data time: 1.83e-01, avg batch time: 1.0058, average train loss: 3.0271
[11/27 09:40:34 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3063, average loss: 2.3186
[11/27 09:40:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.02	
[11/27 09:40:34 visual_prompt]: Stopping early.
[11/27 09:40:34 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 09:40:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 09:40:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/27 09:40:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 09:40:34 visual_prompt]: Training with config:
[11/27 09:40:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr2.5_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/27 09:40:34 visual_prompt]: Loading training data...
[11/27 09:40:34 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 09:40:34 visual_prompt]: Loading validation data...
[11/27 09:40:34 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 09:40:34 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 09:40:37 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 09:40:37 visual_prompt]: tuned percent:0.525
[11/27 09:40:37 visual_prompt]: Device used for model: 0
[11/27 09:40:37 visual_prompt]: Setting up Evaluator...
[11/27 09:40:37 visual_prompt]: Setting up Trainer...
[11/27 09:40:37 visual_prompt]: 	Setting up the optimizer...
[11/27 09:40:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 09:42:21 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8400 s / batch. (data: 3.07e-04). ETA=12:52:47, max mem: 20.9 GB 
[11/27 09:43:59 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8200 s / batch. (data: 3.10e-04). ETA=12:33:01, max mem: 20.9 GB 
[11/27 09:45:42 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3051 s / batch. (data: 4.92e-01). ETA=19:56:20, max mem: 20.9 GB 
[11/27 09:47:21 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8218 s / batch. (data: 7.96e-03). ETA=12:31:56, max mem: 20.9 GB 
[11/27 09:49:04 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8302 s / batch. (data: 1.20e-02). ETA=12:38:13, max mem: 20.9 GB 
[11/27 09:49:56 visual_prompt]: Epoch 1 / 100: avg data time: 1.87e-01, avg batch time: 1.0105, average train loss: 1.5403
[11/27 09:51:03 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3058, average loss: 1.5201
[11/27 09:51:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 09:51:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[11/27 09:52:46 visual_prompt]: 	Training 100/553. train loss: 1.2469,	0.8363 s / batch. (data: 1.56e-02). ETA=12:41:42, max mem: 20.9 GB 
[11/27 09:54:26 visual_prompt]: 	Training 200/553. train loss: 0.1243,	0.8440 s / batch. (data: 3.08e-04). ETA=12:47:17, max mem: 20.9 GB 
[11/27 09:56:08 visual_prompt]: 	Training 300/553. train loss: 1.6256,	1.0640 s / batch. (data: 2.43e-01). ETA=16:05:33, max mem: 20.9 GB 
[11/27 09:57:48 visual_prompt]: 	Training 400/553. train loss: 1.0894,	0.8341 s / batch. (data: 2.99e-04). ETA=12:35:32, max mem: 20.9 GB 
[11/27 09:59:29 visual_prompt]: 	Training 500/553. train loss: 0.5472,	0.8424 s / batch. (data: 2.87e-04). ETA=12:41:35, max mem: 20.9 GB 
[11/27 10:00:20 visual_prompt]: Epoch 2 / 100: avg data time: 1.84e-01, avg batch time: 1.0078, average train loss: 1.3787
[11/27 10:01:18 visual_prompt]: Inference (val):avg data time: 1.70e-04, avg batch time: 0.3070, average loss: 4.4359
[11/27 10:01:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.97	
[11/27 10:01:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[11/27 10:03:00 visual_prompt]: 	Training 100/553. train loss: 3.5525,	1.2954 s / batch. (data: 4.71e-01). ETA=19:27:51, max mem: 20.9 GB 
[11/27 10:04:41 visual_prompt]: 	Training 200/553. train loss: 0.7205,	0.8222 s / batch. (data: 2.60e-04). ETA=12:19:53, max mem: 20.9 GB 
[11/27 10:06:21 visual_prompt]: 	Training 300/553. train loss: 1.3486,	0.8280 s / batch. (data: 2.87e-04). ETA=12:23:44, max mem: 20.9 GB 
[11/27 10:08:07 visual_prompt]: 	Training 400/553. train loss: 5.5128,	0.8520 s / batch. (data: 5.43e-03). ETA=12:43:51, max mem: 20.9 GB 
[11/27 10:09:48 visual_prompt]: 	Training 500/553. train loss: 0.8341,	1.2920 s / batch. (data: 4.58e-01). ETA=19:16:15, max mem: 20.9 GB 
[11/27 10:10:39 visual_prompt]: Epoch 3 / 100: avg data time: 1.92e-01, avg batch time: 1.0158, average train loss: 1.7772
[11/27 10:11:37 visual_prompt]: Inference (val):avg data time: 2.69e-04, avg batch time: 0.3057, average loss: 2.0011
[11/27 10:11:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.93	
[11/27 10:11:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[11/27 10:13:24 visual_prompt]: 	Training 100/553. train loss: 1.9634,	0.8520 s / batch. (data: 3.21e-04). ETA=12:40:16, max mem: 20.9 GB 
[11/27 10:15:06 visual_prompt]: 	Training 200/553. train loss: 1.2338,	0.8384 s / batch. (data: 9.10e-03). ETA=12:26:44, max mem: 20.9 GB 
[11/27 10:16:49 visual_prompt]: 	Training 300/553. train loss: 0.9824,	0.8360 s / batch. (data: 2.92e-04). ETA=12:23:14, max mem: 20.9 GB 
[11/27 10:18:26 visual_prompt]: 	Training 400/553. train loss: 0.5550,	0.8088 s / batch. (data: 3.09e-04). ETA=11:57:38, max mem: 20.9 GB 
[11/27 10:20:07 visual_prompt]: 	Training 500/553. train loss: 1.8908,	2.3729 s / batch. (data: 1.53e+00). ETA=1 day, 11:01:38, max mem: 20.9 GB 
[11/27 10:21:07 visual_prompt]: Epoch 4 / 100: avg data time: 2.07e-01, avg batch time: 1.0314, average train loss: 1.6000
[11/27 10:22:05 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3055, average loss: 0.9835
[11/27 10:22:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.53	
[11/27 10:22:05 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[11/27 10:23:48 visual_prompt]: 	Training 100/553. train loss: 0.3163,	0.8320 s / batch. (data: 7.93e-03). ETA=12:14:44, max mem: 20.9 GB 
[11/27 10:25:28 visual_prompt]: 	Training 200/553. train loss: 3.4402,	1.2920 s / batch. (data: 4.54e-01). ETA=18:58:51, max mem: 20.9 GB 
[11/27 10:27:10 visual_prompt]: 	Training 300/553. train loss: 11.1062,	0.8400 s / batch. (data: 7.95e-03). ETA=12:19:00, max mem: 20.9 GB 
[11/27 10:28:49 visual_prompt]: 	Training 400/553. train loss: 4.8068,	0.8160 s / batch. (data: 2.48e-04). ETA=11:56:35, max mem: 20.9 GB 
[11/27 10:30:30 visual_prompt]: 	Training 500/553. train loss: 3.0015,	0.8240 s / batch. (data: 1.20e-02). ETA=12:02:11, max mem: 20.9 GB 
[11/27 10:31:23 visual_prompt]: Epoch 5 / 100: avg data time: 1.85e-01, avg batch time: 1.0088, average train loss: 3.1452
[11/27 10:32:20 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3066, average loss: 7.8595
[11/27 10:32:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.86	
[11/27 10:32:20 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[11/27 10:34:06 visual_prompt]: 	Training 100/553. train loss: 1.0219,	0.8326 s / batch. (data: 3.05e-04). ETA=12:07:39, max mem: 20.9 GB 
[11/27 10:35:47 visual_prompt]: 	Training 200/553. train loss: 6.9314,	0.8540 s / batch. (data: 2.21e-02). ETA=12:24:56, max mem: 20.9 GB 
[11/27 10:37:25 visual_prompt]: 	Training 300/553. train loss: 2.0502,	0.8258 s / batch. (data: 1.05e-02). ETA=11:58:54, max mem: 20.9 GB 
[11/27 10:39:10 visual_prompt]: 	Training 400/553. train loss: 2.9298,	0.8440 s / batch. (data: 7.94e-03). ETA=12:13:21, max mem: 20.9 GB 
[11/27 10:40:48 visual_prompt]: 	Training 500/553. train loss: 8.1810,	0.8108 s / batch. (data: 3.02e-04). ETA=11:43:09, max mem: 20.9 GB 
[11/27 10:41:41 visual_prompt]: Epoch 6 / 100: avg data time: 1.90e-01, avg batch time: 1.0133, average train loss: 3.0157
[11/27 10:42:38 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3096, average loss: 1.0766
[11/27 10:42:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.61	
[11/27 10:42:38 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[11/27 10:44:21 visual_prompt]: 	Training 100/553. train loss: 4.1645,	0.8120 s / batch. (data: 4.17e-04). ETA=11:42:06, max mem: 20.9 GB 
[11/27 10:46:01 visual_prompt]: 	Training 200/553. train loss: 0.8076,	1.0524 s / batch. (data: 2.43e-01). ETA=15:08:13, max mem: 20.9 GB 
[11/27 10:47:45 visual_prompt]: 	Training 300/553. train loss: 1.0695,	1.1755 s / batch. (data: 3.32e-01). ETA=16:52:31, max mem: 20.9 GB 
[11/27 10:49:25 visual_prompt]: 	Training 400/553. train loss: 0.9894,	1.7702 s / batch. (data: 9.26e-01). ETA=1 day, 1:21:51, max mem: 20.9 GB 
[11/27 10:51:04 visual_prompt]: 	Training 500/553. train loss: 1.4231,	0.8226 s / batch. (data: 3.20e-04). ETA=11:45:48, max mem: 20.9 GB 
[11/27 10:51:56 visual_prompt]: Epoch 7 / 100: avg data time: 1.84e-01, avg batch time: 1.0076, average train loss: 2.4139
[11/27 10:52:53 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3069, average loss: 1.2369
[11/27 10:52:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.16	
[11/27 10:52:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[11/27 10:54:35 visual_prompt]: 	Training 100/553. train loss: 3.9532,	0.8240 s / batch. (data: 3.41e-04). ETA=11:44:55, max mem: 20.9 GB 
[11/27 10:56:17 visual_prompt]: 	Training 200/553. train loss: 4.5629,	0.8145 s / batch. (data: 3.27e-04). ETA=11:35:28, max mem: 20.9 GB 
[11/27 10:57:58 visual_prompt]: 	Training 300/553. train loss: 10.5372,	0.8068 s / batch. (data: 3.07e-04). ETA=11:27:31, max mem: 20.9 GB 
[11/27 10:59:39 visual_prompt]: 	Training 400/553. train loss: 2.3542,	0.8142 s / batch. (data: 7.94e-03). ETA=11:32:27, max mem: 20.9 GB 
[11/27 11:01:19 visual_prompt]: 	Training 500/553. train loss: 6.8852,	1.4580 s / batch. (data: 6.52e-01). ETA=20:37:33, max mem: 20.9 GB 
[11/27 11:02:12 visual_prompt]: Epoch 8 / 100: avg data time: 1.88e-01, avg batch time: 1.0102, average train loss: 3.2039
[11/27 11:03:09 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3066, average loss: 0.7341
[11/27 11:03:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.82	
[11/27 11:03:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[11/27 11:04:54 visual_prompt]: 	Training 100/553. train loss: 0.0003,	0.8113 s / batch. (data: 2.93e-04). ETA=11:26:33, max mem: 20.9 GB 
[11/27 11:06:33 visual_prompt]: 	Training 200/553. train loss: 3.1278,	0.8385 s / batch. (data: 1.21e-02). ETA=11:48:10, max mem: 20.9 GB 
[11/27 11:08:13 visual_prompt]: 	Training 300/553. train loss: 0.8057,	1.5490 s / batch. (data: 7.25e-01). ETA=21:45:39, max mem: 20.9 GB 
[11/27 11:09:56 visual_prompt]: 	Training 400/553. train loss: 1.2837,	0.8231 s / batch. (data: 8.82e-04). ETA=11:32:27, max mem: 20.9 GB 
[11/27 11:11:36 visual_prompt]: 	Training 500/553. train loss: 0.6201,	0.9239 s / batch. (data: 1.11e-01). ETA=12:55:40, max mem: 20.9 GB 
[11/27 11:12:27 visual_prompt]: Epoch 9 / 100: avg data time: 1.86e-01, avg batch time: 1.0087, average train loss: 2.7151
[11/27 11:13:25 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3075, average loss: 0.7821
[11/27 11:13:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.84	
[11/27 11:13:25 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[11/27 11:15:11 visual_prompt]: 	Training 100/553. train loss: 6.0433,	0.8280 s / batch. (data: 1.19e-02). ETA=11:33:03, max mem: 20.9 GB 
[11/27 11:16:51 visual_prompt]: 	Training 200/553. train loss: 0.9900,	0.8440 s / batch. (data: 3.17e-04). ETA=11:45:03, max mem: 20.9 GB 
[11/27 11:18:31 visual_prompt]: 	Training 300/553. train loss: 2.5430,	0.8400 s / batch. (data: 1.59e-02). ETA=11:40:21, max mem: 20.9 GB 
[11/27 11:20:08 visual_prompt]: 	Training 400/553. train loss: 1.1189,	0.9369 s / batch. (data: 1.09e-01). ETA=12:59:33, max mem: 20.9 GB 
[11/27 11:21:50 visual_prompt]: 	Training 500/553. train loss: 0.8032,	0.8120 s / batch. (data: 3.16e-04). ETA=11:14:15, max mem: 20.9 GB 
[11/27 11:22:43 visual_prompt]: Epoch 10 / 100: avg data time: 1.85e-01, avg batch time: 1.0087, average train loss: 3.6532
[11/27 11:23:40 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3067, average loss: 0.7692
[11/27 11:23:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.96	
[11/27 11:23:40 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[11/27 11:25:27 visual_prompt]: 	Training 100/553. train loss: 1.9915,	0.8119 s / batch. (data: 2.69e-04). ETA=11:12:05, max mem: 20.9 GB 
[11/27 11:27:09 visual_prompt]: 	Training 200/553. train loss: 1.4055,	0.8235 s / batch. (data: 7.92e-04). ETA=11:20:22, max mem: 20.9 GB 
[11/27 11:28:48 visual_prompt]: 	Training 300/553. train loss: 0.0000,	2.2366 s / batch. (data: 1.41e+00). ETA=1 day, 6:44:02, max mem: 20.9 GB 
[11/27 11:30:27 visual_prompt]: 	Training 400/553. train loss: 3.1107,	0.8068 s / batch. (data: 2.91e-04). ETA=11:03:50, max mem: 20.9 GB 
[11/27 11:32:06 visual_prompt]: 	Training 500/553. train loss: 2.3858,	0.8400 s / batch. (data: 1.20e-02). ETA=11:29:46, max mem: 20.9 GB 
[11/27 11:32:58 visual_prompt]: Epoch 11 / 100: avg data time: 1.85e-01, avg batch time: 1.0081, average train loss: 2.7395
[11/27 11:33:55 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3061, average loss: 2.1884
[11/27 11:33:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.54	
[11/27 11:33:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[11/27 11:35:42 visual_prompt]: 	Training 100/553. train loss: 0.7223,	0.8624 s / batch. (data: 4.16e-02). ETA=11:45:57, max mem: 20.9 GB 
[11/27 11:37:23 visual_prompt]: 	Training 200/553. train loss: 2.2740,	0.8522 s / batch. (data: 3.28e-04). ETA=11:36:13, max mem: 20.9 GB 
[11/27 11:39:02 visual_prompt]: 	Training 300/553. train loss: 8.0912,	0.8120 s / batch. (data: 2.98e-04). ETA=11:02:00, max mem: 20.9 GB 
[11/27 11:40:42 visual_prompt]: 	Training 400/553. train loss: 1.6407,	0.8267 s / batch. (data: 7.84e-03). ETA=11:12:38, max mem: 20.9 GB 
[11/27 11:42:23 visual_prompt]: 	Training 500/553. train loss: 12.1122,	0.8187 s / batch. (data: 2.82e-03). ETA=11:04:44, max mem: 20.9 GB 
[11/27 11:43:14 visual_prompt]: Epoch 12 / 100: avg data time: 1.87e-01, avg batch time: 1.0107, average train loss: 2.7763
[11/27 11:44:12 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.3054, average loss: 7.2946
[11/27 11:44:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.00	
[11/27 11:44:12 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[11/27 11:45:58 visual_prompt]: 	Training 100/553. train loss: 1.2997,	0.8132 s / batch. (data: 5.42e-03). ETA=10:58:12, max mem: 20.9 GB 
[11/27 11:47:35 visual_prompt]: 	Training 200/553. train loss: 0.6424,	0.8621 s / batch. (data: 1.05e-02). ETA=11:36:19, max mem: 20.9 GB 
[11/27 11:49:17 visual_prompt]: 	Training 300/553. train loss: 1.9891,	1.9526 s / batch. (data: 1.12e+00). ETA=1 day, 2:13:57, max mem: 20.9 GB 
[11/27 11:50:56 visual_prompt]: 	Training 400/553. train loss: 1.8692,	0.8520 s / batch. (data: 1.20e-02). ETA=11:25:19, max mem: 20.9 GB 
[11/27 11:52:38 visual_prompt]: 	Training 500/553. train loss: 5.3363,	0.8217 s / batch. (data: 2.95e-04). ETA=10:59:38, max mem: 20.9 GB 
[11/27 11:53:30 visual_prompt]: Epoch 13 / 100: avg data time: 1.85e-01, avg batch time: 1.0087, average train loss: 3.2620
[11/27 11:54:27 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3070, average loss: 2.3255
[11/27 11:54:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.87	
[11/27 11:54:27 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[11/27 11:56:12 visual_prompt]: 	Training 100/553. train loss: 0.6281,	0.8400 s / batch. (data: 5.39e-03). ETA=11:12:09, max mem: 20.9 GB 
[11/27 11:57:53 visual_prompt]: 	Training 200/553. train loss: 0.0045,	1.0400 s / batch. (data: 2.17e-01). ETA=13:50:27, max mem: 20.9 GB 
[11/27 11:59:33 visual_prompt]: 	Training 300/553. train loss: 1.0539,	0.8216 s / batch. (data: 1.05e-02). ETA=10:54:40, max mem: 20.9 GB 
[11/27 12:01:13 visual_prompt]: 	Training 400/553. train loss: 0.6801,	0.8483 s / batch. (data: 2.80e-04). ETA=11:14:31, max mem: 20.9 GB 
[11/27 12:02:54 visual_prompt]: 	Training 500/553. train loss: 2.5838,	0.8152 s / batch. (data: 2.60e-04). ETA=10:46:51, max mem: 20.9 GB 
[11/27 12:03:45 visual_prompt]: Epoch 14 / 100: avg data time: 1.85e-01, avg batch time: 1.0084, average train loss: 2.5301
[11/27 12:04:42 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3076, average loss: 0.6842
[11/27 12:04:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.50	
[11/27 12:04:42 visual_prompt]: Best epoch 14: best metric: -0.684
[11/27 12:04:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[11/27 12:06:27 visual_prompt]: 	Training 100/553. train loss: 4.3418,	0.8318 s / batch. (data: 3.14e-04). ETA=10:57:57, max mem: 20.9 GB 
[11/27 12:08:06 visual_prompt]: 	Training 200/553. train loss: 1.6729,	0.8294 s / batch. (data: 1.05e-02). ETA=10:54:37, max mem: 20.9 GB 
[11/27 12:09:48 visual_prompt]: 	Training 300/553. train loss: 2.9823,	0.8293 s / batch. (data: 3.19e-04). ETA=10:53:09, max mem: 20.9 GB 
[11/27 12:11:26 visual_prompt]: 	Training 400/553. train loss: 0.5084,	1.3280 s / batch. (data: 4.88e-01). ETA=17:23:45, max mem: 20.9 GB 
[11/27 12:13:07 visual_prompt]: 	Training 500/553. train loss: 1.0066,	0.8321 s / batch. (data: 7.94e-03). ETA=10:52:34, max mem: 20.9 GB 
[11/27 12:14:00 visual_prompt]: Epoch 15 / 100: avg data time: 1.83e-01, avg batch time: 1.0072, average train loss: 3.7963
[11/27 12:14:57 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3062, average loss: 3.8092
[11/27 12:14:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.50	
[11/27 12:14:57 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[11/27 12:16:40 visual_prompt]: 	Training 100/553. train loss: 5.3203,	0.8261 s / batch. (data: 5.07e-04). ETA=10:45:46, max mem: 20.9 GB 
[11/27 12:18:21 visual_prompt]: 	Training 200/553. train loss: 0.4610,	0.8066 s / batch. (data: 3.07e-04). ETA=10:29:10, max mem: 20.9 GB 
[11/27 12:20:02 visual_prompt]: 	Training 300/553. train loss: 3.5131,	0.8600 s / batch. (data: 3.14e-04). ETA=11:09:26, max mem: 20.9 GB 
[11/27 12:21:42 visual_prompt]: 	Training 400/553. train loss: 7.2067,	0.8240 s / batch. (data: 8.03e-04). ETA=10:40:01, max mem: 20.9 GB 
[11/27 12:23:22 visual_prompt]: 	Training 500/553. train loss: 1.3315,	1.5179 s / batch. (data: 7.12e-01). ETA=19:36:29, max mem: 20.9 GB 
[11/27 12:24:15 visual_prompt]: Epoch 16 / 100: avg data time: 1.86e-01, avg batch time: 1.0082, average train loss: 2.9806
[11/27 12:25:12 visual_prompt]: Inference (val):avg data time: 2.76e-04, avg batch time: 0.3061, average loss: 0.7077
[11/27 12:25:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.44	
[11/27 12:25:12 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[11/27 12:26:56 visual_prompt]: 	Training 100/553. train loss: 6.2435,	0.8415 s / batch. (data: 3.10e-04). ETA=10:50:04, max mem: 20.9 GB 
[11/27 12:28:38 visual_prompt]: 	Training 200/553. train loss: 8.1347,	0.8320 s / batch. (data: 2.98e-04). ETA=10:41:21, max mem: 20.9 GB 
[11/27 12:30:17 visual_prompt]: 	Training 300/553. train loss: 12.6241,	0.8204 s / batch. (data: 8.69e-03). ETA=10:31:03, max mem: 20.9 GB 
[11/27 12:31:58 visual_prompt]: 	Training 400/553. train loss: 3.3180,	1.2320 s / batch. (data: 4.14e-01). ETA=15:45:36, max mem: 20.9 GB 
[11/27 12:33:37 visual_prompt]: 	Training 500/553. train loss: 2.3273,	1.5857 s / batch. (data: 7.64e-01). ETA=20:14:25, max mem: 20.9 GB 
[11/27 12:34:30 visual_prompt]: Epoch 17 / 100: avg data time: 1.84e-01, avg batch time: 1.0081, average train loss: 3.0233
[11/27 12:35:28 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3048, average loss: 3.4167
[11/27 12:35:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.38	
[11/27 12:35:28 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[11/27 12:37:12 visual_prompt]: 	Training 100/553. train loss: 1.2508,	0.8305 s / batch. (data: 8.11e-04). ETA=10:33:54, max mem: 20.9 GB 
[11/27 12:38:55 visual_prompt]: 	Training 200/553. train loss: 5.2008,	0.8574 s / batch. (data: 5.44e-03). ETA=10:53:00, max mem: 20.9 GB 
[11/27 12:40:36 visual_prompt]: 	Training 300/553. train loss: 1.5172,	0.8334 s / batch. (data: 2.99e-04). ETA=10:33:21, max mem: 20.9 GB 
[11/27 12:42:15 visual_prompt]: 	Training 400/553. train loss: 2.1699,	0.8121 s / batch. (data: 4.49e-03). ETA=10:15:48, max mem: 20.9 GB 
[11/27 12:43:55 visual_prompt]: 	Training 500/553. train loss: 2.3278,	0.8160 s / batch. (data: 2.96e-04). ETA=10:17:26, max mem: 20.9 GB 
[11/27 12:44:46 visual_prompt]: Epoch 18 / 100: avg data time: 1.85e-01, avg batch time: 1.0087, average train loss: 3.2838
[11/27 12:45:43 visual_prompt]: Inference (val):avg data time: 4.60e-04, avg batch time: 0.3078, average loss: 3.8529
[11/27 12:45:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.17	
[11/27 12:45:43 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[11/27 12:47:27 visual_prompt]: 	Training 100/553. train loss: 1.1073,	0.8188 s / batch. (data: 3.14e-04). ETA=10:17:27, max mem: 20.9 GB 
[11/27 12:49:09 visual_prompt]: 	Training 200/553. train loss: 0.5272,	0.8069 s / batch. (data: 2.47e-04). ETA=10:07:09, max mem: 20.9 GB 
[11/27 12:50:50 visual_prompt]: 	Training 300/553. train loss: 0.7648,	0.8131 s / batch. (data: 2.82e-04). ETA=10:10:25, max mem: 20.9 GB 
[11/27 12:52:31 visual_prompt]: 	Training 400/553. train loss: 0.9542,	0.8280 s / batch. (data: 7.61e-04). ETA=10:20:15, max mem: 20.9 GB 
[11/27 12:54:07 visual_prompt]: 	Training 500/553. train loss: 1.7440,	0.8183 s / batch. (data: 1.03e-02). ETA=10:11:39, max mem: 20.9 GB 
[11/27 12:55:00 visual_prompt]: Epoch 19 / 100: avg data time: 1.82e-01, avg batch time: 1.0057, average train loss: 2.5396
[11/27 12:55:57 visual_prompt]: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3078, average loss: 8.6541
[11/27 12:55:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 64.80	
[11/27 12:55:57 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[11/27 12:57:39 visual_prompt]: 	Training 100/553. train loss: 1.4667,	0.8276 s / batch. (data: 2.80e-04). ETA=10:16:29, max mem: 20.9 GB 
[11/27 12:59:21 visual_prompt]: 	Training 200/553. train loss: 0.4625,	0.8469 s / batch. (data: 1.05e-02). ETA=10:29:25, max mem: 20.9 GB 
[11/27 13:01:02 visual_prompt]: 	Training 300/553. train loss: 11.2653,	0.8390 s / batch. (data: 5.42e-03). ETA=10:22:09, max mem: 20.9 GB 
[11/27 13:02:41 visual_prompt]: 	Training 400/553. train loss: 2.3421,	0.8373 s / batch. (data: 2.13e-02). ETA=10:19:30, max mem: 20.9 GB 
[11/27 13:04:21 visual_prompt]: 	Training 500/553. train loss: 2.3442,	0.8313 s / batch. (data: 8.80e-03). ETA=10:13:41, max mem: 20.9 GB 
[11/27 13:05:16 visual_prompt]: Epoch 20 / 100: avg data time: 1.85e-01, avg batch time: 1.0098, average train loss: 3.0663
[11/27 13:06:13 visual_prompt]: Inference (val):avg data time: 3.80e-04, avg batch time: 0.3053, average loss: 1.1841
[11/27 13:06:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 66.69	
[11/27 13:06:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[11/27 13:08:00 visual_prompt]: 	Training 100/553. train loss: 4.4882,	0.8307 s / batch. (data: 5.46e-03). ETA=10:11:08, max mem: 20.9 GB 
[11/27 13:09:39 visual_prompt]: 	Training 200/553. train loss: 0.4055,	0.8086 s / batch. (data: 2.67e-04). ETA=9:53:30, max mem: 20.9 GB 
[11/27 13:11:19 visual_prompt]: 	Training 300/553. train loss: 11.5161,	0.8328 s / batch. (data: 8.79e-03). ETA=10:09:54, max mem: 20.9 GB 
[11/27 13:12:58 visual_prompt]: 	Training 400/553. train loss: 3.4505,	0.8257 s / batch. (data: 2.92e-04). ETA=10:03:18, max mem: 20.9 GB 
[11/27 13:14:40 visual_prompt]: 	Training 500/553. train loss: 4.3983,	0.8224 s / batch. (data: 2.96e-04). ETA=9:59:31, max mem: 20.9 GB 
[11/27 13:15:32 visual_prompt]: Epoch 21 / 100: avg data time: 1.86e-01, avg batch time: 1.0102, average train loss: 3.2743
[11/27 13:16:30 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3055, average loss: 1.0218
[11/27 13:16:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 64.63	
[11/27 13:16:30 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[11/27 13:18:13 visual_prompt]: 	Training 100/553. train loss: 2.2802,	0.8286 s / batch. (data: 1.07e-02). ETA=10:01:56, max mem: 20.9 GB 
[11/27 13:19:53 visual_prompt]: 	Training 200/553. train loss: 2.4901,	0.8218 s / batch. (data: 3.07e-04). ETA=9:55:39, max mem: 20.9 GB 
[11/27 13:21:31 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.8280 s / batch. (data: 3.16e-04). ETA=9:58:44, max mem: 20.9 GB 
[11/27 13:23:13 visual_prompt]: 	Training 400/553. train loss: 0.4690,	0.8160 s / batch. (data: 7.94e-03). ETA=9:48:43, max mem: 20.9 GB 
[11/27 13:24:53 visual_prompt]: 	Training 500/553. train loss: 2.7315,	0.8492 s / batch. (data: 1.19e-02). ETA=10:11:15, max mem: 20.9 GB 
[11/27 13:25:47 visual_prompt]: Epoch 22 / 100: avg data time: 1.84e-01, avg batch time: 1.0074, average train loss: 2.5661
[11/27 13:26:44 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3052, average loss: 2.9954
[11/27 13:26:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.53	
[11/27 13:26:44 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[11/27 13:28:30 visual_prompt]: 	Training 100/553. train loss: 3.3062,	0.8694 s / batch. (data: 5.08e-02). ETA=10:23:32, max mem: 20.9 GB 
[11/27 13:30:11 visual_prompt]: 	Training 200/553. train loss: 7.6764,	0.8183 s / batch. (data: 3.02e-04). ETA=9:45:34, max mem: 20.9 GB 
[11/27 13:31:53 visual_prompt]: 	Training 300/553. train loss: 2.3995,	0.8362 s / batch. (data: 7.90e-04). ETA=9:56:56, max mem: 20.9 GB 
[11/27 13:33:32 visual_prompt]: 	Training 400/553. train loss: 0.3571,	0.8209 s / batch. (data: 8.14e-04). ETA=9:44:40, max mem: 20.9 GB 
[11/27 13:35:10 visual_prompt]: 	Training 500/553. train loss: 0.0222,	0.8261 s / batch. (data: 2.98e-04). ETA=9:47:01, max mem: 20.9 GB 
[11/27 13:36:02 visual_prompt]: Epoch 23 / 100: avg data time: 1.85e-01, avg batch time: 1.0084, average train loss: 2.4915
[11/27 13:36:59 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3069, average loss: 1.4835
[11/27 13:36:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 66.13	
[11/27 13:36:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[11/27 13:38:41 visual_prompt]: 	Training 100/553. train loss: 2.2103,	0.8231 s / batch. (data: 4.20e-04). ETA=9:42:45, max mem: 20.9 GB 
[11/27 13:40:20 visual_prompt]: 	Training 200/553. train loss: 2.7661,	0.8188 s / batch. (data: 2.99e-04). ETA=9:38:20, max mem: 20.9 GB 
[11/27 13:42:02 visual_prompt]: 	Training 300/553. train loss: 0.9720,	1.0440 s / batch. (data: 2.11e-01). ETA=12:15:42, max mem: 20.9 GB 
[11/27 13:43:43 visual_prompt]: 	Training 400/553. train loss: 3.7028,	0.8323 s / batch. (data: 1.20e-02). ETA=9:45:07, max mem: 20.9 GB 
[11/27 13:45:25 visual_prompt]: 	Training 500/553. train loss: 1.4318,	0.8060 s / batch. (data: 3.11e-04). ETA=9:25:17, max mem: 20.9 GB 
[11/27 13:46:18 visual_prompt]: Epoch 24 / 100: avg data time: 1.86e-01, avg batch time: 1.0095, average train loss: 3.6031
[11/27 13:47:15 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3060, average loss: 6.2080
[11/27 13:47:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.38	
[11/27 13:47:15 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[11/27 13:49:03 visual_prompt]: 	Training 100/553. train loss: 1.1305,	0.8137 s / batch. (data: 4.52e-04). ETA=9:28:36, max mem: 20.9 GB 
[11/27 13:50:40 visual_prompt]: 	Training 200/553. train loss: 2.1433,	0.8081 s / batch. (data: 2.97e-04). ETA=9:23:20, max mem: 20.9 GB 
[11/27 13:52:26 visual_prompt]: 	Training 300/553. train loss: 1.3771,	0.8417 s / batch. (data: 7.66e-04). ETA=9:45:23, max mem: 20.9 GB 
[11/27 13:54:07 visual_prompt]: 	Training 400/553. train loss: 2.0029,	1.2295 s / batch. (data: 3.87e-01). ETA=14:12:59, max mem: 20.9 GB 
[11/27 13:55:47 visual_prompt]: 	Training 500/553. train loss: 3.5644,	1.5840 s / batch. (data: 7.58e-01). ETA=18:16:20, max mem: 20.9 GB 
[11/27 13:56:40 visual_prompt]: Epoch 25 / 100: avg data time: 1.97e-01, avg batch time: 1.0205, average train loss: 2.5629
[11/27 13:57:37 visual_prompt]: Inference (val):avg data time: 4.34e-04, avg batch time: 0.3062, average loss: 11.4234
[11/27 13:57:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.88	
[11/27 13:57:37 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[11/27 13:59:22 visual_prompt]: 	Training 100/553. train loss: 1.9308,	0.8301 s / batch. (data: 7.94e-03). ETA=9:32:25, max mem: 20.9 GB 
[11/27 14:01:04 visual_prompt]: 	Training 200/553. train loss: 5.0691,	1.7940 s / batch. (data: 9.89e-01). ETA=20:34:05, max mem: 20.9 GB 
[11/27 14:02:46 visual_prompt]: 	Training 300/553. train loss: 0.0269,	0.8378 s / batch. (data: 3.19e-04). ETA=9:34:57, max mem: 20.9 GB 
[11/27 14:04:25 visual_prompt]: 	Training 400/553. train loss: 1.0313,	0.8200 s / batch. (data: 7.92e-03). ETA=9:21:20, max mem: 20.9 GB 
[11/27 14:06:03 visual_prompt]: 	Training 500/553. train loss: 1.3334,	0.8164 s / batch. (data: 5.39e-03). ETA=9:17:33, max mem: 20.9 GB 
[11/27 14:06:55 visual_prompt]: Epoch 26 / 100: avg data time: 1.86e-01, avg batch time: 1.0085, average train loss: 3.2823
[11/27 14:07:53 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3072, average loss: 1.6341
[11/27 14:07:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 69.17	
[11/27 14:07:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[11/27 14:09:39 visual_prompt]: 	Training 100/553. train loss: 0.1812,	0.8309 s / batch. (data: 1.20e-02). ETA=9:25:19, max mem: 20.9 GB 
[11/27 14:11:18 visual_prompt]: 	Training 200/553. train loss: 8.5601,	1.1160 s / batch. (data: 2.93e-01). ETA=12:37:25, max mem: 20.9 GB 
[11/27 14:12:59 visual_prompt]: 	Training 300/553. train loss: 3.8854,	0.8240 s / batch. (data: 7.95e-03). ETA=9:17:52, max mem: 20.9 GB 
[11/27 14:14:40 visual_prompt]: 	Training 400/553. train loss: 0.5293,	0.8240 s / batch. (data: 8.13e-04). ETA=9:16:29, max mem: 20.9 GB 
[11/27 14:16:21 visual_prompt]: 	Training 500/553. train loss: 2.6149,	0.8102 s / batch. (data: 3.09e-04). ETA=9:05:51, max mem: 20.9 GB 
[11/27 14:17:12 visual_prompt]: Epoch 27 / 100: avg data time: 1.87e-01, avg batch time: 1.0113, average train loss: 2.6397
[11/27 14:18:10 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3074, average loss: 0.7752
[11/27 14:18:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.10	
[11/27 14:18:10 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[11/27 14:19:53 visual_prompt]: 	Training 100/553. train loss: 1.3799,	0.8233 s / batch. (data: 3.10e-04). ETA=9:12:33, max mem: 20.9 GB 
[11/27 14:21:34 visual_prompt]: 	Training 200/553. train loss: 10.3189,	0.8271 s / batch. (data: 1.19e-02). ETA=9:13:44, max mem: 20.9 GB 
[11/27 14:23:15 visual_prompt]: 	Training 300/553. train loss: 0.6879,	1.6214 s / batch. (data: 8.01e-01). ETA=18:02:46, max mem: 20.9 GB 
[11/27 14:24:54 visual_prompt]: 	Training 400/553. train loss: 0.7422,	0.8104 s / batch. (data: 4.30e-04). ETA=8:59:50, max mem: 20.9 GB 
[11/27 14:26:34 visual_prompt]: 	Training 500/553. train loss: 7.5876,	0.8423 s / batch. (data: 1.03e-02). ETA=9:19:42, max mem: 20.9 GB 
[11/27 14:27:26 visual_prompt]: Epoch 28 / 100: avg data time: 1.84e-01, avg batch time: 1.0062, average train loss: 2.5582
[11/27 14:28:24 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3074, average loss: 1.6362
[11/27 14:28:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 69.12	
[11/27 14:28:24 visual_prompt]: Stopping early.
[11/27 14:28:24 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 14:28:24 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 14:28:24 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/27 14:28:24 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 14:28:24 visual_prompt]: Training with config:
[11/27 14:28:24 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/27 14:28:24 visual_prompt]: Loading training data...
[11/27 14:28:24 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 14:28:24 visual_prompt]: Loading validation data...
[11/27 14:28:24 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 14:28:24 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 14:28:27 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 14:28:27 visual_prompt]: tuned percent:0.525
[11/27 14:28:27 visual_prompt]: Device used for model: 0
[11/27 14:28:27 visual_prompt]: Setting up Evaluator...
[11/27 14:28:27 visual_prompt]: Setting up Trainer...
[11/27 14:28:27 visual_prompt]: 	Setting up the optimizer...
[11/27 14:28:27 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 14:30:11 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8176 s / batch. (data: 3.14e-04). ETA=12:32:12, max mem: 20.9 GB 
[11/27 14:31:49 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8181 s / batch. (data: 2.81e-04). ETA=12:31:18, max mem: 20.9 GB 
[11/27 14:33:33 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3344 s / batch. (data: 5.13e-01). ETA=20:23:11, max mem: 20.9 GB 
[11/27 14:35:11 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8298 s / batch. (data: 1.19e-02). ETA=12:39:13, max mem: 20.9 GB 
[11/27 14:36:54 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8075 s / batch. (data: 3.14e-04). ETA=12:17:33, max mem: 20.9 GB 
[11/27 14:37:47 visual_prompt]: Epoch 1 / 100: avg data time: 1.88e-01, avg batch time: 1.0115, average train loss: 1.5403
[11/27 14:38:44 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3067, average loss: 1.5201
[11/27 14:38:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 14:38:44 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/27 14:40:27 visual_prompt]: 	Training 100/553. train loss: 0.7319,	0.8760 s / batch. (data: 6.84e-02). ETA=13:17:52, max mem: 20.9 GB 
[11/27 14:42:07 visual_prompt]: 	Training 200/553. train loss: 0.0588,	0.8200 s / batch. (data: 3.23e-04). ETA=12:25:28, max mem: 20.9 GB 
[11/27 14:43:49 visual_prompt]: 	Training 300/553. train loss: 0.7024,	1.0296 s / batch. (data: 2.06e-01). ETA=15:34:16, max mem: 20.9 GB 
[11/27 14:45:29 visual_prompt]: 	Training 400/553. train loss: 1.2103,	0.8302 s / batch. (data: 1.01e-02). ETA=12:31:57, max mem: 20.9 GB 
[11/27 14:47:11 visual_prompt]: 	Training 500/553. train loss: 0.6010,	0.8473 s / batch. (data: 2.34e-02). ETA=12:46:02, max mem: 20.9 GB 
[11/27 14:48:02 visual_prompt]: Epoch 2 / 100: avg data time: 1.86e-01, avg batch time: 1.0083, average train loss: 0.9073
[11/27 14:48:59 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3078, average loss: 1.4984
[11/27 14:48:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.17	
[11/27 14:48:59 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/27 14:50:42 visual_prompt]: 	Training 100/553. train loss: 1.1307,	0.8329 s / batch. (data: 5.45e-03). ETA=12:30:56, max mem: 20.9 GB 
[11/27 14:52:23 visual_prompt]: 	Training 200/553. train loss: 0.7188,	2.1062 s / batch. (data: 1.28e+00). ETA=1 day, 7:35:19, max mem: 20.9 GB 
[11/27 14:54:02 visual_prompt]: 	Training 300/553. train loss: 1.0751,	0.8430 s / batch. (data: 1.05e-02). ETA=12:37:13, max mem: 20.9 GB 
[11/27 14:55:43 visual_prompt]: 	Training 400/553. train loss: 1.2364,	0.8491 s / batch. (data: 1.10e-02). ETA=12:41:15, max mem: 20.9 GB 
[11/27 14:57:25 visual_prompt]: 	Training 500/553. train loss: 0.7146,	1.2243 s / batch. (data: 4.04e-01). ETA=18:15:36, max mem: 20.9 GB 
[11/27 14:58:16 visual_prompt]: Epoch 3 / 100: avg data time: 1.83e-01, avg batch time: 1.0061, average train loss: 1.0137
[11/27 14:59:13 visual_prompt]: Inference (val):avg data time: 4.54e-04, avg batch time: 0.3057, average loss: 0.7381
[11/27 14:59:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.10	
[11/27 14:59:13 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/27 15:00:58 visual_prompt]: 	Training 100/553. train loss: 0.7098,	0.8546 s / batch. (data: 1.05e-02). ETA=12:42:35, max mem: 20.9 GB 
[11/27 15:02:39 visual_prompt]: 	Training 200/553. train loss: 0.6045,	0.8560 s / batch. (data: 7.95e-03). ETA=12:42:25, max mem: 20.9 GB 
[11/27 15:04:19 visual_prompt]: 	Training 300/553. train loss: 0.6077,	0.8129 s / batch. (data: 3.04e-04). ETA=12:02:39, max mem: 20.9 GB 
[11/27 15:05:56 visual_prompt]: 	Training 400/553. train loss: 0.5648,	1.3320 s / batch. (data: 5.17e-01). ETA=19:41:56, max mem: 20.9 GB 
[11/27 15:07:38 visual_prompt]: 	Training 500/553. train loss: 1.8408,	3.5951 s / batch. (data: 2.79e+00). ETA=2 days, 5:04:08, max mem: 20.9 GB 
[11/27 15:08:32 visual_prompt]: Epoch 4 / 100: avg data time: 1.87e-01, avg batch time: 1.0098, average train loss: 1.0741
[11/27 15:09:29 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3071, average loss: 0.6947
[11/27 15:09:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.97	
[11/27 15:09:29 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/27 15:11:13 visual_prompt]: 	Training 100/553. train loss: 3.1885,	0.8403 s / batch. (data: 2.81e-02). ETA=12:22:04, max mem: 20.9 GB 
[11/27 15:12:53 visual_prompt]: 	Training 200/553. train loss: 0.6860,	1.3560 s / batch. (data: 5.12e-01). ETA=19:55:16, max mem: 20.9 GB 
[11/27 15:14:34 visual_prompt]: 	Training 300/553. train loss: 0.9858,	0.8271 s / batch. (data: 5.42e-03). ETA=12:07:42, max mem: 20.9 GB 
[11/27 15:16:13 visual_prompt]: 	Training 400/553. train loss: 2.0841,	0.8240 s / batch. (data: 3.06e-04). ETA=12:03:34, max mem: 20.9 GB 
[11/27 15:17:54 visual_prompt]: 	Training 500/553. train loss: 0.5765,	0.8464 s / batch. (data: 1.20e-02). ETA=12:21:49, max mem: 20.9 GB 
[11/27 15:18:47 visual_prompt]: Epoch 5 / 100: avg data time: 1.85e-01, avg batch time: 1.0082, average train loss: 1.3406
[11/27 15:19:44 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3074, average loss: 1.1708
[11/27 15:19:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.42	
[11/27 15:19:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/27 15:21:30 visual_prompt]: 	Training 100/553. train loss: 1.4281,	0.8440 s / batch. (data: 7.62e-04). ETA=12:17:35, max mem: 20.9 GB 
[11/27 15:23:10 visual_prompt]: 	Training 200/553. train loss: 0.3868,	0.8544 s / batch. (data: 3.02e-04). ETA=12:25:15, max mem: 20.9 GB 
[11/27 15:24:49 visual_prompt]: 	Training 300/553. train loss: 0.5484,	0.8258 s / batch. (data: 3.18e-04). ETA=11:58:57, max mem: 20.9 GB 
[11/27 15:26:33 visual_prompt]: 	Training 400/553. train loss: 1.1725,	0.8285 s / batch. (data: 5.79e-03). ETA=11:59:52, max mem: 20.9 GB 
[11/27 15:28:12 visual_prompt]: 	Training 500/553. train loss: 1.1503,	0.8310 s / batch. (data: 3.34e-04). ETA=12:00:41, max mem: 20.9 GB 
[11/27 15:29:04 visual_prompt]: Epoch 6 / 100: avg data time: 1.87e-01, avg batch time: 1.0112, average train loss: 1.4767
[11/27 15:30:02 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3061, average loss: 0.7122
[11/27 15:30:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.23	
[11/27 15:30:02 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/27 15:31:44 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8221 s / batch. (data: 1.52e-02). ETA=11:50:52, max mem: 20.9 GB 
[11/27 15:33:26 visual_prompt]: 	Training 200/553. train loss: 0.5725,	0.8259 s / batch. (data: 5.44e-03). ETA=11:52:48, max mem: 20.9 GB 
[11/27 15:35:08 visual_prompt]: 	Training 300/553. train loss: 1.1049,	0.8108 s / batch. (data: 3.24e-04). ETA=11:38:26, max mem: 20.9 GB 
[11/27 15:36:49 visual_prompt]: 	Training 400/553. train loss: 0.8149,	1.6599 s / batch. (data: 8.22e-01). ETA=23:47:02, max mem: 20.9 GB 
[11/27 15:38:28 visual_prompt]: 	Training 500/553. train loss: 2.5716,	0.8360 s / batch. (data: 5.46e-03). ETA=11:57:17, max mem: 20.9 GB 
[11/27 15:39:19 visual_prompt]: Epoch 7 / 100: avg data time: 1.83e-01, avg batch time: 1.0062, average train loss: 1.7214
[11/27 15:40:16 visual_prompt]: Inference (val):avg data time: 2.49e-04, avg batch time: 0.3068, average loss: 2.4383
[11/27 15:40:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.17	
[11/27 15:40:16 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/27 15:41:58 visual_prompt]: 	Training 100/553. train loss: 1.4058,	0.8112 s / batch. (data: 3.01e-04). ETA=11:33:55, max mem: 20.9 GB 
[11/27 15:43:40 visual_prompt]: 	Training 200/553. train loss: 1.2943,	0.8395 s / batch. (data: 1.05e-02). ETA=11:56:45, max mem: 20.9 GB 
[11/27 15:45:21 visual_prompt]: 	Training 300/553. train loss: 1.1910,	0.8260 s / batch. (data: 1.80e-02). ETA=11:43:51, max mem: 20.9 GB 
[11/27 15:47:02 visual_prompt]: 	Training 400/553. train loss: 1.4699,	0.9811 s / batch. (data: 1.55e-01). ETA=13:54:23, max mem: 20.9 GB 
[11/27 15:48:43 visual_prompt]: 	Training 500/553. train loss: 2.6714,	1.4921 s / batch. (data: 6.66e-01). ETA=21:06:29, max mem: 20.9 GB 
[11/27 15:49:35 visual_prompt]: Epoch 8 / 100: avg data time: 1.87e-01, avg batch time: 1.0105, average train loss: 1.7855
[11/27 15:50:32 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3072, average loss: 1.4725
[11/27 15:50:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.95	
[11/27 15:50:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/27 15:52:17 visual_prompt]: 	Training 100/553. train loss: 0.1074,	0.8200 s / batch. (data: 2.97e-04). ETA=11:33:54, max mem: 20.9 GB 
[11/27 15:53:56 visual_prompt]: 	Training 200/553. train loss: 0.8484,	0.8400 s / batch. (data: 7.94e-03). ETA=11:49:28, max mem: 20.9 GB 
[11/27 15:55:37 visual_prompt]: 	Training 300/553. train loss: 0.5530,	1.8901 s / batch. (data: 1.08e+00). ETA=1 day, 2:33:12, max mem: 20.9 GB 
[11/27 15:57:17 visual_prompt]: 	Training 400/553. train loss: 1.3158,	0.8208 s / batch. (data: 4.52e-04). ETA=11:30:32, max mem: 20.9 GB 
[11/27 15:58:59 visual_prompt]: 	Training 500/553. train loss: 1.6353,	1.0505 s / batch. (data: 2.12e-01). ETA=14:41:59, max mem: 20.9 GB 
[11/27 15:59:50 visual_prompt]: Epoch 9 / 100: avg data time: 1.86e-01, avg batch time: 1.0089, average train loss: 2.0420
[11/27 16:00:48 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3067, average loss: 2.6569
[11/27 16:00:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.63	
[11/27 16:00:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/27 16:02:36 visual_prompt]: 	Training 100/553. train loss: 4.6758,	0.8261 s / batch. (data: 3.22e-04). ETA=11:31:27, max mem: 20.9 GB 
[11/27 16:04:14 visual_prompt]: 	Training 200/553. train loss: 4.1651,	0.8209 s / batch. (data: 5.42e-03). ETA=11:25:47, max mem: 20.9 GB 
[11/27 16:05:53 visual_prompt]: 	Training 300/553. train loss: 1.9748,	1.4275 s / batch. (data: 6.00e-01). ETA=19:50:08, max mem: 20.9 GB 
[11/27 16:07:32 visual_prompt]: 	Training 400/553. train loss: 0.8070,	0.8344 s / batch. (data: 5.44e-03). ETA=11:34:14, max mem: 20.9 GB 
[11/27 16:09:13 visual_prompt]: 	Training 500/553. train loss: 0.5712,	0.8428 s / batch. (data: 2.15e-02). ETA=11:39:52, max mem: 20.9 GB 
[11/27 16:10:06 visual_prompt]: Epoch 10 / 100: avg data time: 1.85e-01, avg batch time: 1.0083, average train loss: 2.7223
[11/27 16:11:03 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3077, average loss: 0.9930
[11/27 16:11:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.69	
[11/27 16:11:03 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/27 16:12:50 visual_prompt]: 	Training 100/553. train loss: 1.3468,	0.8320 s / batch. (data: 3.03e-04). ETA=11:28:45, max mem: 20.9 GB 
[11/27 16:14:32 visual_prompt]: 	Training 200/553. train loss: 7.5440,	0.8466 s / batch. (data: 5.41e-03). ETA=11:39:27, max mem: 20.9 GB 
[11/27 16:16:11 visual_prompt]: 	Training 300/553. train loss: 0.0002,	2.2330 s / batch. (data: 1.42e+00). ETA=1 day, 6:41:06, max mem: 20.9 GB 
[11/27 16:17:50 visual_prompt]: 	Training 400/553. train loss: 5.0701,	0.8445 s / batch. (data: 2.85e-02). ETA=11:34:54, max mem: 20.9 GB 
[11/27 16:19:29 visual_prompt]: 	Training 500/553. train loss: 4.2936,	0.8280 s / batch. (data: 3.37e-04). ETA=11:19:55, max mem: 20.9 GB 
[11/27 16:20:20 visual_prompt]: Epoch 11 / 100: avg data time: 1.83e-01, avg batch time: 1.0062, average train loss: 2.7813
[11/27 16:21:21 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3067, average loss: 0.7190
[11/27 16:21:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.25	
[11/27 16:21:21 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/27 16:23:07 visual_prompt]: 	Training 100/553. train loss: 1.2974,	0.9960 s / batch. (data: 1.73e-01). ETA=13:35:20, max mem: 20.9 GB 
[11/27 16:24:50 visual_prompt]: 	Training 200/553. train loss: 1.2570,	0.8395 s / batch. (data: 3.19e-04). ETA=11:25:51, max mem: 20.9 GB 
[11/27 16:26:29 visual_prompt]: 	Training 300/553. train loss: 1.0898,	0.8080 s / batch. (data: 2.73e-04). ETA=10:58:44, max mem: 20.9 GB 
[11/27 16:28:11 visual_prompt]: 	Training 400/553. train loss: 2.4149,	0.8231 s / batch. (data: 3.19e-04). ETA=11:09:40, max mem: 20.9 GB 
[11/27 16:29:51 visual_prompt]: 	Training 500/553. train loss: 0.8156,	0.8229 s / batch. (data: 2.94e-04). ETA=11:08:07, max mem: 20.9 GB 
[11/27 16:30:42 visual_prompt]: Epoch 12 / 100: avg data time: 1.91e-01, avg batch time: 1.0136, average train loss: 2.2124
[11/27 16:31:39 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3058, average loss: 1.9518
[11/27 16:31:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 42.65	
[11/27 16:31:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/27 16:33:25 visual_prompt]: 	Training 100/553. train loss: 1.6673,	0.8106 s / batch. (data: 3.16e-04). ETA=10:56:05, max mem: 20.9 GB 
[11/27 16:35:03 visual_prompt]: 	Training 200/553. train loss: 1.0907,	0.8442 s / batch. (data: 2.84e-04). ETA=11:21:51, max mem: 20.9 GB 
[11/27 16:36:43 visual_prompt]: 	Training 300/553. train loss: 11.3819,	1.7097 s / batch. (data: 8.74e-01). ETA=22:58:09, max mem: 20.9 GB 
[11/27 16:38:22 visual_prompt]: 	Training 400/553. train loss: 0.8201,	0.8120 s / batch. (data: 3.02e-04). ETA=10:53:10, max mem: 20.9 GB 
[11/27 16:40:03 visual_prompt]: 	Training 500/553. train loss: 4.5766,	0.8108 s / batch. (data: 2.54e-04). ETA=10:50:50, max mem: 20.9 GB 
[11/27 16:40:55 visual_prompt]: Epoch 13 / 100: avg data time: 1.83e-01, avg batch time: 1.0061, average train loss: 3.3906
[11/27 16:41:53 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3080, average loss: 1.8345
[11/27 16:41:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 54.85	
[11/27 16:41:53 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/27 16:43:38 visual_prompt]: 	Training 100/553. train loss: 1.4706,	0.8223 s / batch. (data: 3.46e-04). ETA=10:57:59, max mem: 20.9 GB 
[11/27 16:45:18 visual_prompt]: 	Training 200/553. train loss: 0.0002,	1.2325 s / batch. (data: 4.12e-01). ETA=16:24:10, max mem: 20.9 GB 
[11/27 16:46:58 visual_prompt]: 	Training 300/553. train loss: 0.9847,	0.8330 s / batch. (data: 1.56e-02). ETA=11:03:48, max mem: 20.9 GB 
[11/27 16:48:38 visual_prompt]: 	Training 400/553. train loss: 0.6098,	0.8107 s / batch. (data: 2.94e-04). ETA=10:44:40, max mem: 20.9 GB 
[11/27 16:50:19 visual_prompt]: 	Training 500/553. train loss: 0.8445,	0.8357 s / batch. (data: 2.93e-04). ETA=11:03:08, max mem: 20.9 GB 
[11/27 16:51:09 visual_prompt]: Epoch 14 / 100: avg data time: 1.83e-01, avg batch time: 1.0064, average train loss: 2.9013
[11/27 16:52:07 visual_prompt]: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3067, average loss: 2.6851
[11/27 16:52:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.51	
[11/27 16:52:07 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/27 16:53:51 visual_prompt]: 	Training 100/553. train loss: 0.7502,	0.8481 s / batch. (data: 3.32e-04). ETA=11:10:50, max mem: 20.9 GB 
[11/27 16:55:30 visual_prompt]: 	Training 200/553. train loss: 6.2084,	0.8480 s / batch. (data: 2.81e-04). ETA=11:09:18, max mem: 20.9 GB 
[11/27 16:57:13 visual_prompt]: 	Training 300/553. train loss: 6.2141,	0.8375 s / batch. (data: 8.19e-04). ETA=10:59:38, max mem: 20.9 GB 
[11/27 16:58:50 visual_prompt]: 	Training 400/553. train loss: 4.9723,	1.0073 s / batch. (data: 1.93e-01). ETA=13:11:43, max mem: 20.9 GB 
[11/27 17:00:31 visual_prompt]: 	Training 500/553. train loss: 0.5792,	0.8411 s / batch. (data: 1.56e-02). ETA=10:59:38, max mem: 20.9 GB 
[11/27 17:01:24 visual_prompt]: Epoch 15 / 100: avg data time: 1.85e-01, avg batch time: 1.0079, average train loss: 2.8800
[11/27 17:02:22 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3054, average loss: 1.1222
[11/27 17:02:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.96	
[11/27 17:02:22 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/27 17:04:05 visual_prompt]: 	Training 100/553. train loss: 0.9188,	0.8457 s / batch. (data: 2.86e-04). ETA=11:01:06, max mem: 20.9 GB 
[11/27 17:05:46 visual_prompt]: 	Training 200/553. train loss: 1.6138,	0.8113 s / batch. (data: 2.85e-04). ETA=10:32:54, max mem: 20.9 GB 
[11/27 17:07:26 visual_prompt]: 	Training 300/553. train loss: 2.6054,	0.8074 s / batch. (data: 2.40e-04). ETA=10:28:28, max mem: 20.9 GB 
[11/27 17:09:16 visual_prompt]: 	Training 400/553. train loss: 11.0380,	0.8182 s / batch. (data: 8.14e-04). ETA=10:35:30, max mem: 20.9 GB 
[11/27 17:10:56 visual_prompt]: 	Training 500/553. train loss: 1.0261,	1.0600 s / batch. (data: 2.43e-01). ETA=13:41:35, max mem: 20.9 GB 
[11/27 17:11:49 visual_prompt]: Epoch 16 / 100: avg data time: 2.02e-01, avg batch time: 1.0250, average train loss: 3.0492
[11/27 17:12:46 visual_prompt]: Inference (val):avg data time: 3.77e-05, avg batch time: 0.3057, average loss: 1.8886
[11/27 17:12:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.51	
[11/27 17:12:46 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/27 17:14:30 visual_prompt]: 	Training 100/553. train loss: 3.0164,	0.8204 s / batch. (data: 7.95e-03). ETA=10:33:47, max mem: 20.9 GB 
[11/27 17:16:12 visual_prompt]: 	Training 200/553. train loss: 0.7377,	0.8080 s / batch. (data: 3.19e-04). ETA=10:22:51, max mem: 20.9 GB 
[11/27 17:17:52 visual_prompt]: 	Training 300/553. train loss: 2.3622,	0.8181 s / batch. (data: 7.97e-03). ETA=10:29:16, max mem: 20.9 GB 
[11/27 17:19:32 visual_prompt]: 	Training 400/553. train loss: 3.6778,	1.1919 s / batch. (data: 3.73e-01). ETA=15:14:51, max mem: 20.9 GB 
[11/27 17:21:11 visual_prompt]: 	Training 500/553. train loss: 2.8617,	1.6274 s / batch. (data: 7.87e-01). ETA=20:46:23, max mem: 20.9 GB 
[11/27 17:22:04 visual_prompt]: Epoch 17 / 100: avg data time: 1.86e-01, avg batch time: 1.0092, average train loss: 3.0789
[11/27 17:23:02 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3070, average loss: 2.8243
[11/27 17:23:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.42	
[11/27 17:23:02 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/27 17:24:47 visual_prompt]: 	Training 100/553. train loss: 2.2617,	0.8191 s / batch. (data: 2.87e-03). ETA=10:25:13, max mem: 20.9 GB 
[11/27 17:26:29 visual_prompt]: 	Training 200/553. train loss: 1.3063,	0.8240 s / batch. (data: 3.18e-04). ETA=10:27:35, max mem: 20.9 GB 
[11/27 17:28:09 visual_prompt]: 	Training 300/553. train loss: 1.5358,	0.8315 s / batch. (data: 1.14e-02). ETA=10:31:54, max mem: 20.9 GB 
[11/27 17:29:50 visual_prompt]: 	Training 400/553. train loss: 4.9620,	0.8223 s / batch. (data: 3.07e-04). ETA=10:23:33, max mem: 20.9 GB 
[11/27 17:31:29 visual_prompt]: 	Training 500/553. train loss: 1.2236,	0.8221 s / batch. (data: 2.93e-04). ETA=10:22:03, max mem: 20.9 GB 
[11/27 17:32:20 visual_prompt]: Epoch 18 / 100: avg data time: 1.85e-01, avg batch time: 1.0095, average train loss: 3.1646
[11/27 17:33:18 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3079, average loss: 2.9035
[11/27 17:33:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.02	
[11/27 17:33:18 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[11/27 17:35:02 visual_prompt]: 	Training 100/553. train loss: 1.2002,	0.8299 s / batch. (data: 1.19e-02). ETA=10:25:49, max mem: 20.9 GB 
[11/27 17:36:43 visual_prompt]: 	Training 200/553. train loss: 1.4388,	0.8333 s / batch. (data: 5.40e-03). ETA=10:26:59, max mem: 20.9 GB 
[11/27 17:38:23 visual_prompt]: 	Training 300/553. train loss: 12.4888,	0.8292 s / batch. (data: 3.05e-04). ETA=10:22:33, max mem: 20.9 GB 
[11/27 17:40:06 visual_prompt]: 	Training 400/553. train loss: 1.7336,	0.8386 s / batch. (data: 8.18e-04). ETA=10:28:11, max mem: 20.9 GB 
[11/27 17:41:42 visual_prompt]: 	Training 500/553. train loss: 0.7673,	0.8160 s / batch. (data: 2.99e-04). ETA=10:09:55, max mem: 20.9 GB 
[11/27 17:42:34 visual_prompt]: Epoch 19 / 100: avg data time: 1.82e-01, avg batch time: 1.0053, average train loss: 2.8470
[11/27 17:43:31 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.3068, average loss: 7.4800
[11/27 17:43:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.49	
[11/27 17:43:31 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[11/27 17:45:14 visual_prompt]: 	Training 100/553. train loss: 4.2764,	0.8400 s / batch. (data: 3.13e-04). ETA=10:25:42, max mem: 20.9 GB 
[11/27 17:46:55 visual_prompt]: 	Training 200/553. train loss: 0.8867,	0.8360 s / batch. (data: 7.95e-03). ETA=10:21:19, max mem: 20.9 GB 
[11/27 17:48:36 visual_prompt]: 	Training 300/553. train loss: 0.9475,	0.8415 s / batch. (data: 3.06e-04). ETA=10:23:59, max mem: 20.9 GB 
[11/27 17:50:15 visual_prompt]: 	Training 400/553. train loss: 0.5581,	0.8240 s / batch. (data: 3.21e-04). ETA=10:09:40, max mem: 20.9 GB 
[11/27 17:51:55 visual_prompt]: 	Training 500/553. train loss: 3.0633,	0.8209 s / batch. (data: 1.01e-02). ETA=10:06:00, max mem: 20.9 GB 
[11/27 17:52:49 visual_prompt]: Epoch 20 / 100: avg data time: 1.85e-01, avg batch time: 1.0089, average train loss: 2.7796
[11/27 17:53:48 visual_prompt]: Inference (val):avg data time: 3.00e-04, avg batch time: 0.3049, average loss: 0.8203
[11/27 17:53:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.48	
[11/27 17:53:48 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[11/27 17:55:35 visual_prompt]: 	Training 100/553. train loss: 4.5754,	0.8266 s / batch. (data: 6.55e-03). ETA=10:08:05, max mem: 20.9 GB 
[11/27 17:57:14 visual_prompt]: 	Training 200/553. train loss: 4.4108,	0.8207 s / batch. (data: 2.93e-04). ETA=10:02:23, max mem: 20.9 GB 
[11/27 17:58:54 visual_prompt]: 	Training 300/553. train loss: 3.5463,	0.8640 s / batch. (data: 4.31e-02). ETA=10:32:44, max mem: 20.9 GB 
[11/27 18:00:32 visual_prompt]: 	Training 400/553. train loss: 5.4283,	0.8217 s / batch. (data: 3.23e-04). ETA=10:00:24, max mem: 20.9 GB 
[11/27 18:02:15 visual_prompt]: 	Training 500/553. train loss: 1.6179,	0.8400 s / batch. (data: 3.22e-04). ETA=10:12:21, max mem: 20.9 GB 
[11/27 18:03:06 visual_prompt]: Epoch 21 / 100: avg data time: 1.86e-01, avg batch time: 1.0092, average train loss: 2.9148
[11/27 18:04:03 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3052, average loss: 15.3549
[11/27 18:04:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.30	
[11/27 18:04:03 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[11/27 18:05:47 visual_prompt]: 	Training 100/553. train loss: 1.5224,	0.8080 s / batch. (data: 3.20e-04). ETA=9:46:57, max mem: 20.9 GB 
[11/27 18:07:27 visual_prompt]: 	Training 200/553. train loss: 13.7409,	0.8116 s / batch. (data: 5.38e-03). ETA=9:48:14, max mem: 20.9 GB 
[11/27 18:09:05 visual_prompt]: 	Training 300/553. train loss: 0.0334,	0.8803 s / batch. (data: 7.36e-02). ETA=10:36:32, max mem: 20.9 GB 
[11/27 18:10:47 visual_prompt]: 	Training 400/553. train loss: 1.1310,	0.8165 s / batch. (data: 5.42e-03). ETA=9:49:04, max mem: 20.9 GB 
[11/27 18:12:27 visual_prompt]: 	Training 500/553. train loss: 22.3371,	0.8385 s / batch. (data: 1.05e-02). ETA=10:03:30, max mem: 20.9 GB 
[11/27 18:13:21 visual_prompt]: Epoch 22 / 100: avg data time: 1.85e-01, avg batch time: 1.0080, average train loss: 3.2062
[11/27 18:14:18 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3073, average loss: 4.0187
[11/27 18:14:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.98	
[11/27 18:14:18 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[11/27 18:16:04 visual_prompt]: 	Training 100/553. train loss: 4.3435,	0.8560 s / batch. (data: 5.46e-03). ETA=10:13:56, max mem: 20.9 GB 
[11/27 18:17:45 visual_prompt]: 	Training 200/553. train loss: 0.8062,	0.8112 s / batch. (data: 3.15e-04). ETA=9:40:29, max mem: 20.9 GB 
[11/27 18:19:28 visual_prompt]: 	Training 300/553. train loss: 0.5328,	0.8321 s / batch. (data: 8.89e-04). ETA=9:54:02, max mem: 20.9 GB 
[11/27 18:21:06 visual_prompt]: 	Training 400/553. train loss: 2.7154,	0.8278 s / batch. (data: 3.06e-04). ETA=9:49:36, max mem: 20.9 GB 
[11/27 18:22:44 visual_prompt]: 	Training 500/553. train loss: 1.2378,	0.8162 s / batch. (data: 3.00e-04). ETA=9:39:58, max mem: 20.9 GB 
[11/27 18:23:36 visual_prompt]: Epoch 23 / 100: avg data time: 1.84e-01, avg batch time: 1.0082, average train loss: 2.7538
[11/27 18:24:34 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.3058, average loss: 0.8995
[11/27 18:24:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.44	
[11/27 18:24:34 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[11/27 18:26:16 visual_prompt]: 	Training 100/553. train loss: 1.7059,	0.8519 s / batch. (data: 1.99e-02). ETA=10:03:11, max mem: 20.9 GB 
[11/27 18:27:55 visual_prompt]: 	Training 200/553. train loss: 2.1721,	0.8412 s / batch. (data: 7.99e-04). ETA=9:54:09, max mem: 20.9 GB 
[11/27 18:29:36 visual_prompt]: 	Training 300/553. train loss: 5.1876,	1.0825 s / batch. (data: 2.69e-01). ETA=12:42:50, max mem: 20.9 GB 
[11/27 18:31:16 visual_prompt]: 	Training 400/553. train loss: 0.9383,	0.8174 s / batch. (data: 3.02e-04). ETA=9:34:39, max mem: 20.9 GB 
[11/27 18:32:59 visual_prompt]: 	Training 500/553. train loss: 0.6994,	0.8320 s / batch. (data: 3.04e-04). ETA=9:43:31, max mem: 20.9 GB 
[11/27 18:33:52 visual_prompt]: Epoch 24 / 100: avg data time: 1.86e-01, avg batch time: 1.0089, average train loss: 2.7766
[11/27 18:34:49 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3073, average loss: 0.6933
[11/27 18:34:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 51.58	
[11/27 18:34:49 visual_prompt]: Best epoch 24: best metric: -0.693
[11/27 18:34:49 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[11/27 18:36:37 visual_prompt]: 	Training 100/553. train loss: 0.9825,	0.8147 s / batch. (data: 2.84e-04). ETA=9:29:18, max mem: 20.9 GB 
[11/27 18:38:14 visual_prompt]: 	Training 200/553. train loss: 2.4278,	1.1063 s / batch. (data: 2.61e-01). ETA=12:51:15, max mem: 20.9 GB 
[11/27 18:39:54 visual_prompt]: 	Training 300/553. train loss: 0.6883,	0.8228 s / batch. (data: 3.18e-04). ETA=9:32:14, max mem: 20.9 GB 
[11/27 18:41:34 visual_prompt]: 	Training 400/553. train loss: 10.6437,	1.1451 s / batch. (data: 3.28e-01). ETA=13:14:29, max mem: 20.9 GB 
[11/27 18:43:14 visual_prompt]: 	Training 500/553. train loss: 1.9651,	1.3360 s / batch. (data: 5.02e-01). ETA=15:24:41, max mem: 20.9 GB 
[11/27 18:44:07 visual_prompt]: Epoch 25 / 100: avg data time: 1.85e-01, avg batch time: 1.0089, average train loss: 2.5519
[11/27 18:45:05 visual_prompt]: Inference (val):avg data time: 2.93e-04, avg batch time: 0.3064, average loss: 13.9499
[11/27 18:45:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.81	
[11/27 18:45:05 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[11/27 18:46:49 visual_prompt]: 	Training 100/553. train loss: 1.1473,	0.8528 s / batch. (data: 3.68e-02). ETA=9:48:05, max mem: 20.9 GB 
[11/27 18:48:31 visual_prompt]: 	Training 200/553. train loss: 0.6540,	1.6582 s / batch. (data: 8.52e-01). ETA=19:00:40, max mem: 20.9 GB 
[11/27 18:50:13 visual_prompt]: 	Training 300/553. train loss: 0.3755,	0.8440 s / batch. (data: 7.92e-04). ETA=9:39:11, max mem: 20.9 GB 
[11/27 18:51:51 visual_prompt]: 	Training 400/553. train loss: 3.4730,	0.8440 s / batch. (data: 7.94e-03). ETA=9:37:47, max mem: 20.9 GB 
[11/27 18:53:29 visual_prompt]: 	Training 500/553. train loss: 0.6244,	0.8508 s / batch. (data: 1.89e-02). ETA=9:41:02, max mem: 20.9 GB 
[11/27 18:54:22 visual_prompt]: Epoch 26 / 100: avg data time: 1.83e-01, avg batch time: 1.0067, average train loss: 2.1947
[11/27 18:55:19 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3073, average loss: 3.6868
[11/27 18:55:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.12	
[11/27 18:55:19 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[11/27 18:57:04 visual_prompt]: 	Training 100/553. train loss: 1.7660,	0.8439 s / batch. (data: 7.89e-03). ETA=9:34:11, max mem: 20.9 GB 
[11/27 18:58:44 visual_prompt]: 	Training 200/553. train loss: 2.8494,	1.3022 s / batch. (data: 4.96e-01). ETA=14:43:50, max mem: 20.9 GB 
[11/27 19:00:24 visual_prompt]: 	Training 300/553. train loss: 5.5061,	0.8320 s / batch. (data: 3.05e-04). ETA=9:23:17, max mem: 20.9 GB 
[11/27 19:02:05 visual_prompt]: 	Training 400/553. train loss: 1.2970,	0.8400 s / batch. (data: 3.07e-04). ETA=9:27:18, max mem: 20.9 GB 
[11/27 19:03:46 visual_prompt]: 	Training 500/553. train loss: 0.5721,	0.8311 s / batch. (data: 2.87e-04). ETA=9:19:53, max mem: 20.9 GB 
[11/27 19:04:36 visual_prompt]: Epoch 27 / 100: avg data time: 1.84e-01, avg batch time: 1.0071, average train loss: 2.6540
[11/27 19:05:33 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3077, average loss: 3.5891
[11/27 19:05:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.28	
[11/27 19:05:33 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[11/27 19:07:17 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8120 s / batch. (data: 2.62e-04). ETA=9:05:00, max mem: 20.9 GB 
[11/27 19:08:57 visual_prompt]: 	Training 200/553. train loss: 0.6356,	0.8157 s / batch. (data: 3.44e-04). ETA=9:06:06, max mem: 20.9 GB 
[11/27 19:10:38 visual_prompt]: 	Training 300/553. train loss: 2.6102,	1.6558 s / batch. (data: 8.26e-01). ETA=18:25:47, max mem: 20.9 GB 
[11/27 19:12:18 visual_prompt]: 	Training 400/553. train loss: 3.8450,	0.8440 s / batch. (data: 2.75e-04). ETA=9:22:14, max mem: 20.9 GB 
[11/27 19:13:57 visual_prompt]: 	Training 500/553. train loss: 0.6316,	0.8141 s / batch. (data: 2.92e-04). ETA=9:00:56, max mem: 20.9 GB 
[11/27 19:14:50 visual_prompt]: Epoch 28 / 100: avg data time: 1.84e-01, avg batch time: 1.0062, average train loss: 2.5020
[11/27 19:15:48 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3058, average loss: 2.3042
[11/27 19:15:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.51	
[11/27 19:15:48 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[11/27 19:17:38 visual_prompt]: 	Training 100/553. train loss: 3.3383,	0.8207 s / batch. (data: 2.84e-04). ETA=9:03:14, max mem: 20.9 GB 
[11/27 19:19:18 visual_prompt]: 	Training 200/553. train loss: 1.6855,	1.9301 s / batch. (data: 1.11e+00). ETA=21:14:23, max mem: 20.9 GB 
[11/27 19:20:56 visual_prompt]: 	Training 300/553. train loss: 0.6887,	0.8110 s / batch. (data: 3.20e-04). ETA=8:54:06, max mem: 20.9 GB 
[11/27 19:22:33 visual_prompt]: 	Training 400/553. train loss: 6.9906,	0.8894 s / batch. (data: 7.71e-02). ETA=9:44:15, max mem: 20.9 GB 
[11/27 19:24:14 visual_prompt]: 	Training 500/553. train loss: 2.3491,	0.8506 s / batch. (data: 1.56e-02). ETA=9:17:23, max mem: 20.9 GB 
[11/27 19:25:06 visual_prompt]: Epoch 29 / 100: avg data time: 1.86e-01, avg batch time: 1.0094, average train loss: 2.7083
[11/27 19:26:03 visual_prompt]: Inference (val):avg data time: 3.88e-05, avg batch time: 0.3053, average loss: 2.2601
[11/27 19:26:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.89	
[11/27 19:26:03 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[11/27 19:27:45 visual_prompt]: 	Training 100/553. train loss: 3.3582,	0.8179 s / batch. (data: 3.92e-04). ETA=8:53:52, max mem: 20.9 GB 
[11/27 19:29:27 visual_prompt]: 	Training 200/553. train loss: 0.6117,	0.8380 s / batch. (data: 5.43e-03). ETA=9:05:36, max mem: 20.9 GB 
[11/27 19:31:06 visual_prompt]: 	Training 300/553. train loss: 0.0323,	1.9758 s / batch. (data: 1.16e+00). ETA=21:23:03, max mem: 20.9 GB 
[11/27 19:32:47 visual_prompt]: 	Training 400/553. train loss: 0.7059,	1.1121 s / batch. (data: 3.00e-01). ETA=12:00:18, max mem: 20.9 GB 
[11/27 19:34:27 visual_prompt]: 	Training 500/553. train loss: 4.8829,	1.4960 s / batch. (data: 6.78e-01). ETA=16:06:30, max mem: 20.9 GB 
[11/27 19:35:21 visual_prompt]: Epoch 30 / 100: avg data time: 1.85e-01, avg batch time: 1.0080, average train loss: 2.6531
[11/27 19:36:18 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3070, average loss: 2.0257
[11/27 19:36:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.52	
[11/27 19:36:18 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[11/27 19:38:04 visual_prompt]: 	Training 100/553. train loss: 0.9641,	0.8320 s / batch. (data: 7.93e-03). ETA=8:55:22, max mem: 20.9 GB 
[11/27 19:39:46 visual_prompt]: 	Training 200/553. train loss: 3.9513,	0.8240 s / batch. (data: 2.81e-04). ETA=8:48:52, max mem: 20.9 GB 
[11/27 19:41:24 visual_prompt]: 	Training 300/553. train loss: 10.7418,	0.8280 s / batch. (data: 3.08e-04). ETA=8:50:03, max mem: 20.9 GB 
[11/27 19:43:04 visual_prompt]: 	Training 400/553. train loss: 6.2864,	1.4859 s / batch. (data: 6.65e-01). ETA=15:48:45, max mem: 20.9 GB 
[11/27 19:44:44 visual_prompt]: 	Training 500/553. train loss: 0.9899,	0.8102 s / batch. (data: 2.99e-04). ETA=8:35:56, max mem: 20.9 GB 
[11/27 19:45:35 visual_prompt]: Epoch 31 / 100: avg data time: 1.82e-01, avg batch time: 1.0058, average train loss: 2.2740
[11/27 19:46:32 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3058, average loss: 0.7490
[11/27 19:46:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.76	
[11/27 19:46:32 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[11/27 19:48:18 visual_prompt]: 	Training 100/553. train loss: 0.6049,	0.8351 s / batch. (data: 8.09e-04). ETA=8:49:42, max mem: 20.9 GB 
[11/27 19:49:58 visual_prompt]: 	Training 200/553. train loss: 0.5663,	0.8316 s / batch. (data: 1.05e-02). ETA=8:46:05, max mem: 20.9 GB 
[11/27 19:51:40 visual_prompt]: 	Training 300/553. train loss: 7.0590,	0.8200 s / batch. (data: 3.24e-04). ETA=8:37:22, max mem: 20.9 GB 
[11/27 19:53:21 visual_prompt]: 	Training 400/553. train loss: 0.7622,	0.8440 s / batch. (data: 2.77e-04). ETA=8:51:07, max mem: 20.9 GB 
[11/27 19:54:59 visual_prompt]: 	Training 500/553. train loss: 0.6783,	0.8160 s / batch. (data: 4.24e-04). ETA=8:32:07, max mem: 20.9 GB 
[11/27 19:55:49 visual_prompt]: Epoch 32 / 100: avg data time: 1.84e-01, avg batch time: 1.0071, average train loss: 2.2939
[11/27 19:56:45 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3077, average loss: 2.3057
[11/27 19:56:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 44.82	
[11/27 19:56:45 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[11/27 19:58:23 visual_prompt]: 	Training 100/553. train loss: 0.0002,	0.8076 s / batch. (data: 3.03e-04). ETA=8:24:47, max mem: 20.9 GB 
[11/27 20:00:00 visual_prompt]: 	Training 200/553. train loss: 2.4458,	1.2535 s / batch. (data: 4.26e-01). ETA=13:01:26, max mem: 20.9 GB 
[11/27 20:01:35 visual_prompt]: 	Training 300/553. train loss: 0.7954,	0.8360 s / batch. (data: 2.87e-04). ETA=8:39:46, max mem: 20.9 GB 
[11/27 20:03:12 visual_prompt]: 	Training 400/553. train loss: 1.3548,	0.8197 s / batch. (data: 2.91e-04). ETA=8:28:14, max mem: 20.9 GB 
[11/27 20:04:47 visual_prompt]: 	Training 500/553. train loss: 0.6328,	0.8240 s / batch. (data: 1.19e-02). ETA=8:29:32, max mem: 20.9 GB 
[11/27 20:05:36 visual_prompt]: Epoch 33 / 100: avg data time: 1.37e-01, avg batch time: 0.9604, average train loss: 2.1522
[11/27 20:06:31 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3065, average loss: 0.6910
[11/27 20:06:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.83	
[11/27 20:06:31 visual_prompt]: Best epoch 33: best metric: -0.691
[11/27 20:06:31 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[11/27 20:08:11 visual_prompt]: 	Training 100/553. train loss: 0.7138,	0.8120 s / batch. (data: 3.20e-04). ETA=8:20:03, max mem: 20.9 GB 
[11/27 20:09:45 visual_prompt]: 	Training 200/553. train loss: 3.4925,	0.8200 s / batch. (data: 3.14e-04). ETA=8:23:37, max mem: 20.9 GB 
[11/27 20:11:20 visual_prompt]: 	Training 300/553. train loss: 2.9661,	0.8377 s / batch. (data: 9.66e-03). ETA=8:33:07, max mem: 20.9 GB 
[11/27 20:12:57 visual_prompt]: 	Training 400/553. train loss: 1.7802,	0.8256 s / batch. (data: 2.96e-04). ETA=8:24:19, max mem: 20.9 GB 
[11/27 20:14:32 visual_prompt]: 	Training 500/553. train loss: 0.8462,	1.3736 s / batch. (data: 5.53e-01). ETA=13:56:46, max mem: 20.9 GB 
[11/27 20:15:22 visual_prompt]: Epoch 34 / 100: avg data time: 1.39e-01, avg batch time: 0.9615, average train loss: 2.6615
[11/27 20:16:17 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3059, average loss: 1.1740
[11/27 20:16:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.93	
[11/27 20:16:17 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[11/27 20:17:59 visual_prompt]: 	Training 100/553. train loss: 3.9830,	0.8152 s / batch. (data: 7.95e-03). ETA=8:14:32, max mem: 20.9 GB 
[11/27 20:19:37 visual_prompt]: 	Training 200/553. train loss: 1.6856,	0.8370 s / batch. (data: 1.05e-02). ETA=8:26:20, max mem: 20.9 GB 
[11/27 20:21:11 visual_prompt]: 	Training 300/553. train loss: 1.6449,	0.8215 s / batch. (data: 5.38e-03). ETA=8:15:38, max mem: 20.9 GB 
[11/27 20:22:46 visual_prompt]: 	Training 400/553. train loss: 0.6149,	0.8339 s / batch. (data: 5.44e-03). ETA=8:21:41, max mem: 20.9 GB 
[11/27 20:24:22 visual_prompt]: 	Training 500/553. train loss: 0.6931,	1.1813 s / batch. (data: 3.60e-01). ETA=11:48:44, max mem: 20.9 GB 
[11/27 20:25:13 visual_prompt]: Epoch 35 / 100: avg data time: 1.46e-01, avg batch time: 0.9688, average train loss: 1.8298
[11/27 20:26:08 visual_prompt]: Inference (val):avg data time: 1.89e-04, avg batch time: 0.3059, average loss: 1.8427
[11/27 20:26:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.98	
[11/27 20:26:08 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[11/27 20:27:47 visual_prompt]: 	Training 100/553. train loss: 1.1378,	0.8173 s / batch. (data: 3.29e-04). ETA=8:08:17, max mem: 20.9 GB 
[11/27 20:29:24 visual_prompt]: 	Training 200/553. train loss: 1.7021,	0.8169 s / batch. (data: 2.81e-04). ETA=8:06:38, max mem: 20.9 GB 
[11/27 20:31:03 visual_prompt]: 	Training 300/553. train loss: 3.7361,	0.8274 s / batch. (data: 5.42e-03). ETA=8:11:34, max mem: 20.9 GB 
[11/27 20:32:38 visual_prompt]: 	Training 400/553. train loss: 1.4654,	0.8067 s / batch. (data: 2.98e-04). ETA=7:57:52, max mem: 20.9 GB 
[11/27 20:34:15 visual_prompt]: 	Training 500/553. train loss: 0.5981,	0.9195 s / batch. (data: 7.53e-02). ETA=9:03:11, max mem: 20.9 GB 
[11/27 20:35:03 visual_prompt]: Epoch 36 / 100: avg data time: 1.44e-01, avg batch time: 0.9674, average train loss: 2.2093
[11/27 20:35:58 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3057, average loss: 5.9647
[11/27 20:35:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.15	
[11/27 20:35:58 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[11/27 20:37:38 visual_prompt]: 	Training 100/553. train loss: 2.6822,	0.8211 s / batch. (data: 3.22e-04). ETA=8:02:57, max mem: 20.9 GB 
[11/27 20:39:15 visual_prompt]: 	Training 200/553. train loss: 0.6890,	0.8077 s / batch. (data: 2.97e-04). ETA=7:53:46, max mem: 20.9 GB 
[11/27 20:40:51 visual_prompt]: 	Training 300/553. train loss: 1.9766,	1.4240 s / batch. (data: 6.12e-01). ETA=13:52:50, max mem: 20.9 GB 
[11/27 20:42:29 visual_prompt]: 	Training 400/553. train loss: 1.1788,	1.9186 s / batch. (data: 1.11e+00). ETA=18:38:55, max mem: 20.9 GB 
[11/27 20:44:02 visual_prompt]: 	Training 500/553. train loss: 4.2658,	1.0356 s / batch. (data: 2.29e-01). ETA=10:02:12, max mem: 20.9 GB 
[11/27 20:44:54 visual_prompt]: Epoch 37 / 100: avg data time: 1.46e-01, avg batch time: 0.9693, average train loss: 2.0978
[11/27 20:45:49 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3101, average loss: 1.5988
[11/27 20:45:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.77	
[11/27 20:45:49 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[11/27 20:47:28 visual_prompt]: 	Training 100/553. train loss: 0.8851,	0.8070 s / batch. (data: 3.02e-04). ETA=7:47:13, max mem: 20.9 GB 
[11/27 20:49:06 visual_prompt]: 	Training 200/553. train loss: 2.0352,	1.4988 s / batch. (data: 6.82e-01). ETA=14:25:18, max mem: 20.9 GB 
[11/27 20:50:42 visual_prompt]: 	Training 300/553. train loss: 1.5409,	0.8072 s / batch. (data: 2.93e-04). ETA=7:44:38, max mem: 20.9 GB 
[11/27 20:52:17 visual_prompt]: 	Training 400/553. train loss: 0.6111,	0.8420 s / batch. (data: 1.19e-02). ETA=8:03:16, max mem: 20.9 GB 
[11/27 20:53:56 visual_prompt]: 	Training 500/553. train loss: 22.3139,	0.8520 s / batch. (data: 1.14e-03). ETA=8:07:36, max mem: 20.9 GB 
[11/27 20:54:44 visual_prompt]: Epoch 38 / 100: avg data time: 1.44e-01, avg batch time: 0.9671, average train loss: 2.1677
[11/27 20:55:39 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3070, average loss: 4.7694
[11/27 20:55:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.79	
[11/27 20:55:39 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[11/27 20:57:19 visual_prompt]: 	Training 100/553. train loss: 0.0604,	0.8655 s / batch. (data: 5.47e-03). ETA=8:13:08, max mem: 20.9 GB 
[11/27 20:58:59 visual_prompt]: 	Training 200/553. train loss: 0.6759,	0.8258 s / batch. (data: 8.33e-04). ETA=7:49:08, max mem: 20.9 GB 
[11/27 21:00:38 visual_prompt]: 	Training 300/553. train loss: 3.5685,	0.8178 s / batch. (data: 2.60e-04). ETA=7:43:14, max mem: 20.9 GB 
[11/27 21:02:11 visual_prompt]: 	Training 400/553. train loss: 2.7708,	1.0027 s / batch. (data: 1.81e-01). ETA=9:26:17, max mem: 20.9 GB 
[11/27 21:03:48 visual_prompt]: 	Training 500/553. train loss: 1.4081,	1.7811 s / batch. (data: 9.68e-01). ETA=16:42:55, max mem: 20.9 GB 
[11/27 21:04:37 visual_prompt]: Epoch 39 / 100: avg data time: 1.48e-01, avg batch time: 0.9714, average train loss: 1.9088
[11/27 21:05:32 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3057, average loss: 0.8640
[11/27 21:05:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.66	
[11/27 21:05:32 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[11/27 21:07:13 visual_prompt]: 	Training 100/553. train loss: 4.1731,	0.8220 s / batch. (data: 2.97e-04). ETA=7:40:45, max mem: 20.9 GB 
[11/27 21:08:48 visual_prompt]: 	Training 200/553. train loss: 1.3754,	0.8294 s / batch. (data: 7.94e-03). ETA=7:43:30, max mem: 20.9 GB 
[11/27 21:10:26 visual_prompt]: 	Training 300/553. train loss: 5.2486,	0.8500 s / batch. (data: 7.95e-04). ETA=7:53:39, max mem: 20.9 GB 
[11/27 21:12:02 visual_prompt]: 	Training 400/553. train loss: 0.7807,	0.8450 s / batch. (data: 8.59e-04). ETA=7:49:27, max mem: 20.9 GB 
[11/27 21:13:38 visual_prompt]: 	Training 500/553. train loss: 1.5303,	0.8181 s / batch. (data: 1.19e-02). ETA=7:33:09, max mem: 20.9 GB 
[11/27 21:14:30 visual_prompt]: Epoch 40 / 100: avg data time: 1.49e-01, avg batch time: 0.9726, average train loss: 2.1051
[11/27 21:15:25 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3068, average loss: 0.8930
[11/27 21:15:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.54	
[11/27 21:15:25 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[11/27 21:17:09 visual_prompt]: 	Training 100/553. train loss: 0.6960,	0.8236 s / batch. (data: 5.55e-03). ETA=7:34:03, max mem: 20.9 GB 
[11/27 21:18:48 visual_prompt]: 	Training 200/553. train loss: 1.1739,	0.8240 s / batch. (data: 3.05e-04). ETA=7:32:54, max mem: 20.9 GB 
[11/27 21:20:23 visual_prompt]: 	Training 300/553. train loss: 2.3497,	0.8526 s / batch. (data: 3.46e-02). ETA=7:47:14, max mem: 20.9 GB 
[11/27 21:21:58 visual_prompt]: 	Training 400/553. train loss: 0.9638,	0.8219 s / batch. (data: 3.00e-04). ETA=7:29:03, max mem: 20.9 GB 
[11/27 21:23:33 visual_prompt]: 	Training 500/553. train loss: 2.8615,	0.8353 s / batch. (data: 5.88e-03). ETA=7:34:57, max mem: 20.9 GB 
[11/27 21:24:21 visual_prompt]: Epoch 41 / 100: avg data time: 1.45e-01, avg batch time: 0.9688, average train loss: 1.9298
[11/27 21:25:16 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3058, average loss: 1.0720
[11/27 21:25:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 55.11	
[11/27 21:25:16 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[11/27 21:26:54 visual_prompt]: 	Training 100/553. train loss: 2.6607,	0.8204 s / batch. (data: 2.84e-04). ETA=7:24:46, max mem: 20.9 GB 
[11/27 21:28:32 visual_prompt]: 	Training 200/553. train loss: 3.9794,	0.8399 s / batch. (data: 5.43e-03). ETA=7:33:55, max mem: 20.9 GB 
[11/27 21:30:08 visual_prompt]: 	Training 300/553. train loss: 0.8868,	0.8064 s / batch. (data: 2.97e-04). ETA=7:14:29, max mem: 20.9 GB 
[11/27 21:31:44 visual_prompt]: 	Training 400/553. train loss: 0.9817,	0.8240 s / batch. (data: 3.18e-04). ETA=7:22:34, max mem: 20.9 GB 
[11/27 21:33:20 visual_prompt]: 	Training 500/553. train loss: 2.0830,	0.8320 s / batch. (data: 3.05e-04). ETA=7:25:29, max mem: 20.9 GB 
[11/27 21:34:11 visual_prompt]: Epoch 42 / 100: avg data time: 1.46e-01, avg batch time: 0.9679, average train loss: 2.3765
[11/27 21:35:06 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3075, average loss: 2.6654
[11/27 21:35:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.41	
[11/27 21:35:06 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[11/27 21:36:48 visual_prompt]: 	Training 100/553. train loss: 0.6919,	0.8069 s / batch. (data: 2.92e-04). ETA=7:09:58, max mem: 20.9 GB 
[11/27 21:38:24 visual_prompt]: 	Training 200/553. train loss: 4.3715,	0.8360 s / batch. (data: 5.46e-03). ETA=7:24:06, max mem: 20.9 GB 
[11/27 21:39:58 visual_prompt]: 	Training 300/553. train loss: 3.3153,	0.8367 s / batch. (data: 7.96e-03). ETA=7:23:05, max mem: 20.9 GB 
[11/27 21:41:33 visual_prompt]: 	Training 400/553. train loss: 0.5495,	0.8091 s / batch. (data: 2.71e-04). ETA=7:07:08, max mem: 20.9 GB 
[11/27 21:43:11 visual_prompt]: 	Training 500/553. train loss: 2.3719,	0.8332 s / batch. (data: 2.57e-02). ETA=7:18:28, max mem: 20.9 GB 
[11/27 21:44:03 visual_prompt]: Epoch 43 / 100: avg data time: 1.48e-01, avg batch time: 0.9715, average train loss: 2.1711
[11/27 21:44:58 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3067, average loss: 1.0611
[11/27 21:44:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.77	
[11/27 21:44:58 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[11/27 21:46:39 visual_prompt]: 	Training 100/553. train loss: 0.6536,	1.2552 s / batch. (data: 4.48e-01). ETA=10:57:18, max mem: 20.9 GB 
[11/27 21:48:17 visual_prompt]: 	Training 200/553. train loss: 1.2334,	0.8361 s / batch. (data: 2.65e-04). ETA=7:16:26, max mem: 20.9 GB 
[11/27 21:49:51 visual_prompt]: 	Training 300/553. train loss: 0.8066,	0.8400 s / batch. (data: 1.20e-02). ETA=7:17:06, max mem: 20.9 GB 
[11/27 21:51:26 visual_prompt]: 	Training 400/553. train loss: 1.1292,	0.8307 s / batch. (data: 5.12e-03). ETA=7:10:51, max mem: 20.9 GB 
[11/27 21:53:02 visual_prompt]: 	Training 500/553. train loss: 1.5931,	0.8069 s / batch. (data: 2.96e-04). ETA=6:57:10, max mem: 20.9 GB 
[11/27 21:53:53 visual_prompt]: Epoch 44 / 100: avg data time: 1.42e-01, avg batch time: 0.9659, average train loss: 2.0941
[11/27 21:54:48 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.3061, average loss: 3.0344
[11/27 21:54:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.45	
[11/27 21:54:48 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[11/27 21:56:30 visual_prompt]: 	Training 100/553. train loss: 2.6431,	0.8280 s / batch. (data: 7.95e-03). ETA=7:05:58, max mem: 20.9 GB 
[11/27 21:58:01 visual_prompt]: 	Training 200/553. train loss: 0.6821,	0.9391 s / batch. (data: 1.13e-01). ETA=8:01:35, max mem: 20.9 GB 
[11/27 21:59:40 visual_prompt]: 	Training 300/553. train loss: 6.4774,	0.8389 s / batch. (data: 3.10e-04). ETA=7:08:46, max mem: 20.9 GB 
[11/27 22:01:13 visual_prompt]: 	Training 400/553. train loss: 2.2171,	0.8075 s / batch. (data: 2.96e-04). ETA=6:51:23, max mem: 20.9 GB 
[11/27 22:02:51 visual_prompt]: 	Training 500/553. train loss: 0.6980,	0.8216 s / batch. (data: 3.40e-04). ETA=6:57:11, max mem: 20.9 GB 
[11/27 22:03:41 visual_prompt]: Epoch 45 / 100: avg data time: 1.41e-01, avg batch time: 0.9643, average train loss: 1.9767
[11/27 22:04:36 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3063, average loss: 1.0924
[11/27 22:04:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.41	
[11/27 22:04:36 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[11/27 22:06:16 visual_prompt]: 	Training 100/553. train loss: 3.2458,	0.8330 s / batch. (data: 1.59e-02). ETA=7:00:52, max mem: 20.9 GB 
[11/27 22:07:53 visual_prompt]: 	Training 200/553. train loss: 2.2857,	0.9010 s / batch. (data: 7.87e-04). ETA=7:33:42, max mem: 20.9 GB 
[11/27 22:09:28 visual_prompt]: 	Training 300/553. train loss: 1.5727,	0.8360 s / batch. (data: 2.78e-04). ETA=6:59:37, max mem: 20.9 GB 
[11/27 22:11:04 visual_prompt]: 	Training 400/553. train loss: 0.7820,	0.8202 s / batch. (data: 1.05e-02). ETA=6:50:17, max mem: 20.9 GB 
[11/27 22:12:36 visual_prompt]: 	Training 500/553. train loss: 5.5864,	0.8145 s / batch. (data: 2.52e-04). ETA=6:46:06, max mem: 20.9 GB 
[11/27 22:13:28 visual_prompt]: Epoch 46 / 100: avg data time: 1.39e-01, avg batch time: 0.9619, average train loss: 1.7843
[11/27 22:14:22 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3063, average loss: 1.3203
[11/27 22:14:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.42	
[11/27 22:14:22 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[11/27 22:16:04 visual_prompt]: 	Training 100/553. train loss: 3.5818,	0.8114 s / batch. (data: 5.42e-03). ETA=6:42:27, max mem: 20.9 GB 
[11/27 22:17:36 visual_prompt]: 	Training 200/553. train loss: 0.7208,	0.8200 s / batch. (data: 3.03e-04). ETA=6:45:22, max mem: 20.9 GB 
[11/27 22:19:13 visual_prompt]: 	Training 300/553. train loss: 3.1863,	0.8400 s / batch. (data: 8.10e-04). ETA=6:53:51, max mem: 20.9 GB 
[11/27 22:20:50 visual_prompt]: 	Training 400/553. train loss: 0.4958,	0.8200 s / batch. (data: 2.88e-04). ETA=6:42:38, max mem: 20.9 GB 
[11/27 22:22:24 visual_prompt]: 	Training 500/553. train loss: 1.8249,	0.8481 s / batch. (data: 7.94e-03). ETA=6:55:00, max mem: 20.9 GB 
[11/27 22:23:16 visual_prompt]: Epoch 47 / 100: avg data time: 1.40e-01, avg batch time: 0.9644, average train loss: 2.1250
[11/27 22:24:10 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3074, average loss: 0.7435
[11/27 22:24:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.28	
[11/27 22:24:10 visual_prompt]: Stopping early.
[11/27 22:24:10 visual_prompt]: Rank of current process: 0. World size: 1
[11/27 22:24:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/27 22:24:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/27 22:24:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/27 22:24:10 visual_prompt]: Training with config:
[11/27 22:24:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/27 22:24:10 visual_prompt]: Loading training data...
[11/27 22:24:10 visual_prompt]: Constructing mammo-cbis dataset train...
[11/27 22:24:10 visual_prompt]: Loading validation data...
[11/27 22:24:10 visual_prompt]: Constructing mammo-cbis dataset val...
[11/27 22:24:10 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/27 22:24:13 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/27 22:24:13 visual_prompt]: tuned percent:0.525
[11/27 22:24:13 visual_prompt]: Device used for model: 0
[11/27 22:24:13 visual_prompt]: Setting up Evaluator...
[11/27 22:24:13 visual_prompt]: Setting up Trainer...
[11/27 22:24:13 visual_prompt]: 	Setting up the optimizer...
[11/27 22:24:13 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/27 22:25:52 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8549 s / batch. (data: 5.41e-03). ETA=13:06:28, max mem: 20.9 GB 
[11/27 22:27:26 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8218 s / batch. (data: 5.42e-03). ETA=12:34:40, max mem: 20.9 GB 
[11/27 22:29:04 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.6560 s / batch. (data: 8.39e-01). ETA=1 day, 1:18:00, max mem: 20.9 GB 
[11/27 22:30:38 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8120 s / batch. (data: 2.85e-04). ETA=12:22:59, max mem: 20.9 GB 
[11/27 22:32:16 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8172 s / batch. (data: 3.14e-04). ETA=12:26:22, max mem: 20.9 GB 
[11/27 22:33:06 visual_prompt]: Epoch 1 / 100: avg data time: 1.41e-01, avg batch time: 0.9644, average train loss: 1.5403
[11/27 22:34:01 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3079, average loss: 1.5201
[11/27 22:34:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/27 22:34:01 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/27 22:35:41 visual_prompt]: 	Training 100/553. train loss: 0.7431,	2.3040 s / batch. (data: 1.47e+00). ETA=1 day, 10:58:25, max mem: 20.9 GB 
[11/27 22:37:16 visual_prompt]: 	Training 200/553. train loss: 0.0229,	0.8263 s / batch. (data: 3.16e-04). ETA=12:31:11, max mem: 20.9 GB 
[11/27 22:38:53 visual_prompt]: 	Training 300/553. train loss: 0.7115,	0.9786 s / batch. (data: 1.61e-01). ETA=14:48:00, max mem: 20.9 GB 
[11/27 22:40:28 visual_prompt]: 	Training 400/553. train loss: 1.0806,	0.8291 s / batch. (data: 1.05e-02). ETA=12:30:58, max mem: 20.9 GB 
[11/27 22:42:05 visual_prompt]: 	Training 500/553. train loss: 0.6006,	0.8360 s / batch. (data: 2.79e-04). ETA=12:35:50, max mem: 20.9 GB 
[11/27 22:42:54 visual_prompt]: Epoch 2 / 100: avg data time: 1.40e-01, avg batch time: 0.9638, average train loss: 0.9692
[11/27 22:43:48 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3060, average loss: 1.2669
[11/27 22:43:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.96	
[11/27 22:43:48 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/27 22:45:27 visual_prompt]: 	Training 100/553. train loss: 1.2298,	0.8440 s / batch. (data: 2.94e-04). ETA=12:40:55, max mem: 20.9 GB 
[11/27 22:47:04 visual_prompt]: 	Training 200/553. train loss: 0.9757,	0.8311 s / batch. (data: 5.41e-03). ETA=12:27:52, max mem: 20.9 GB 
[11/27 22:48:38 visual_prompt]: 	Training 300/553. train loss: 0.7386,	0.8280 s / batch. (data: 2.75e-04). ETA=12:23:44, max mem: 20.9 GB 
[11/27 22:50:14 visual_prompt]: 	Training 400/553. train loss: 3.2934,	0.8280 s / batch. (data: 7.94e-03). ETA=12:22:21, max mem: 20.9 GB 
[11/27 22:51:51 visual_prompt]: 	Training 500/553. train loss: 0.6973,	1.0462 s / batch. (data: 2.25e-01). ETA=15:36:16, max mem: 20.9 GB 
[11/27 22:52:40 visual_prompt]: Epoch 3 / 100: avg data time: 1.37e-01, avg batch time: 0.9610, average train loss: 0.9728
[11/27 22:53:35 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3049, average loss: 0.8207
[11/27 22:53:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.52	
[11/27 22:53:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/27 22:55:15 visual_prompt]: 	Training 100/553. train loss: 0.7498,	0.8344 s / batch. (data: 3.01e-04). ETA=12:24:36, max mem: 20.9 GB 
[11/27 22:56:52 visual_prompt]: 	Training 200/553. train loss: 0.6275,	0.8301 s / batch. (data: 3.12e-04). ETA=12:19:21, max mem: 20.9 GB 
[11/27 22:58:29 visual_prompt]: 	Training 300/553. train loss: 0.6057,	1.0000 s / batch. (data: 1.78e-01). ETA=14:49:01, max mem: 20.9 GB 
[11/27 23:00:01 visual_prompt]: 	Training 400/553. train loss: 0.5973,	1.1780 s / batch. (data: 3.59e-01). ETA=17:25:19, max mem: 20.9 GB 
[11/27 23:01:39 visual_prompt]: 	Training 500/553. train loss: 0.3909,	3.2583 s / batch. (data: 2.45e+00). ETA=2 days, 0:05:46, max mem: 20.9 GB 
[11/27 23:02:31 visual_prompt]: Epoch 4 / 100: avg data time: 1.47e-01, avg batch time: 0.9695, average train loss: 1.0162
[11/27 23:03:26 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3047, average loss: 0.6778
[11/27 23:03:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 58.80	
[11/27 23:03:26 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/27 23:05:04 visual_prompt]: 	Training 100/553. train loss: 3.2099,	0.8080 s / batch. (data: 3.18e-04). ETA=11:53:33, max mem: 20.9 GB 
[11/27 23:06:41 visual_prompt]: 	Training 200/553. train loss: 0.9763,	1.1997 s / batch. (data: 3.76e-01). ETA=17:37:27, max mem: 20.9 GB 
[11/27 23:08:19 visual_prompt]: 	Training 300/553. train loss: 2.7767,	0.8360 s / batch. (data: 7.75e-04). ETA=12:15:29, max mem: 20.9 GB 
[11/27 23:09:54 visual_prompt]: 	Training 400/553. train loss: 2.3731,	0.8120 s / batch. (data: 2.89e-04). ETA=11:53:00, max mem: 20.9 GB 
[11/27 23:11:31 visual_prompt]: 	Training 500/553. train loss: 0.6466,	0.8430 s / batch. (data: 2.69e-02). ETA=12:18:52, max mem: 20.9 GB 
[11/27 23:12:22 visual_prompt]: Epoch 5 / 100: avg data time: 1.45e-01, avg batch time: 0.9690, average train loss: 1.1417
[11/27 23:13:17 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.3056, average loss: 2.2109
[11/27 23:13:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.25	
[11/27 23:13:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/27 23:14:58 visual_prompt]: 	Training 100/553. train loss: 1.4888,	0.8388 s / batch. (data: 7.74e-04). ETA=12:13:01, max mem: 20.9 GB 
[11/27 23:16:34 visual_prompt]: 	Training 200/553. train loss: 2.5385,	0.8295 s / batch. (data: 5.43e-03). ETA=12:03:29, max mem: 20.9 GB 
[11/27 23:18:09 visual_prompt]: 	Training 300/553. train loss: 2.6572,	0.8360 s / batch. (data: 2.99e-04). ETA=12:07:47, max mem: 20.9 GB 
[11/27 23:19:49 visual_prompt]: 	Training 400/553. train loss: 2.0656,	0.8360 s / batch. (data: 3.08e-04). ETA=12:06:25, max mem: 20.9 GB 
[11/27 23:21:24 visual_prompt]: 	Training 500/553. train loss: 2.2061,	0.8420 s / batch. (data: 5.43e-03). ETA=12:10:12, max mem: 20.9 GB 
[11/27 23:22:14 visual_prompt]: Epoch 6 / 100: avg data time: 1.47e-01, avg batch time: 0.9709, average train loss: 1.4448
[11/27 23:23:09 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.3073, average loss: 0.7829
[11/27 23:23:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.25	
[11/27 23:23:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/27 23:24:48 visual_prompt]: 	Training 100/553. train loss: 2.1696,	0.8425 s / batch. (data: 1.05e-02). ETA=12:08:30, max mem: 20.9 GB 
[11/27 23:26:24 visual_prompt]: 	Training 200/553. train loss: 0.5902,	1.5840 s / batch. (data: 7.66e-01). ETA=22:47:02, max mem: 20.9 GB 
[11/27 23:28:03 visual_prompt]: 	Training 300/553. train loss: 1.1093,	1.5890 s / batch. (data: 7.60e-01). ETA=22:48:43, max mem: 20.9 GB 
[11/27 23:29:40 visual_prompt]: 	Training 400/553. train loss: 1.1678,	1.8200 s / batch. (data: 9.77e-01). ETA=1 day, 2:04:38, max mem: 20.9 GB 
[11/27 23:31:15 visual_prompt]: 	Training 500/553. train loss: 0.6711,	0.8410 s / batch. (data: 1.05e-02). ETA=12:01:36, max mem: 20.9 GB 
[11/27 23:32:04 visual_prompt]: Epoch 7 / 100: avg data time: 1.44e-01, avg batch time: 0.9680, average train loss: 1.4688
[11/27 23:32:59 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3051, average loss: 0.9110
[11/27 23:32:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.41	
[11/27 23:32:59 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/27 23:34:38 visual_prompt]: 	Training 100/553. train loss: 3.7135,	0.8127 s / batch. (data: 5.43e-03). ETA=11:35:17, max mem: 20.9 GB 
[11/27 23:36:16 visual_prompt]: 	Training 200/553. train loss: 0.6631,	0.8155 s / batch. (data: 5.43e-03). ETA=11:36:17, max mem: 20.9 GB 
[11/27 23:37:53 visual_prompt]: 	Training 300/553. train loss: 1.7093,	0.8439 s / batch. (data: 2.84e-04). ETA=11:59:08, max mem: 20.9 GB 
[11/27 23:39:29 visual_prompt]: 	Training 400/553. train loss: 3.7947,	0.8160 s / batch. (data: 2.93e-04). ETA=11:33:58, max mem: 20.9 GB 
[11/27 23:41:06 visual_prompt]: 	Training 500/553. train loss: 4.6051,	1.2970 s / batch. (data: 4.91e-01). ETA=18:20:52, max mem: 20.9 GB 
[11/27 23:41:57 visual_prompt]: Epoch 8 / 100: avg data time: 1.48e-01, avg batch time: 0.9714, average train loss: 1.9594
[11/27 23:42:52 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3054, average loss: 0.8549
[11/27 23:42:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[11/27 23:42:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/27 23:44:32 visual_prompt]: 	Training 100/553. train loss: 0.9613,	0.8206 s / batch. (data: 3.01e-04). ETA=11:34:24, max mem: 20.9 GB 
[11/27 23:46:07 visual_prompt]: 	Training 200/553. train loss: 0.5460,	0.8240 s / batch. (data: 2.86e-04). ETA=11:35:58, max mem: 20.9 GB 
[11/27 23:47:44 visual_prompt]: 	Training 300/553. train loss: 0.7530,	1.5776 s / batch. (data: 7.68e-01). ETA=22:09:49, max mem: 20.9 GB 
[11/27 23:49:22 visual_prompt]: 	Training 400/553. train loss: 0.6412,	0.8374 s / batch. (data: 1.05e-02). ETA=11:44:28, max mem: 20.9 GB 
[11/27 23:50:58 visual_prompt]: 	Training 500/553. train loss: 0.6179,	0.8409 s / batch. (data: 3.56e-02). ETA=11:46:00, max mem: 20.9 GB 
[11/27 23:51:48 visual_prompt]: Epoch 9 / 100: avg data time: 1.47e-01, avg batch time: 0.9698, average train loss: 1.6057
[11/27 23:52:43 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3049, average loss: 0.8593
[11/27 23:52:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.55	
[11/27 23:52:43 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/27 23:54:26 visual_prompt]: 	Training 100/553. train loss: 4.1423,	0.8249 s / batch. (data: 5.40e-03). ETA=11:30:26, max mem: 20.9 GB 
[11/27 23:56:01 visual_prompt]: 	Training 200/553. train loss: 0.9189,	0.8240 s / batch. (data: 2.99e-04). ETA=11:28:21, max mem: 20.9 GB 
[11/27 23:57:37 visual_prompt]: 	Training 300/553. train loss: 2.8276,	1.7734 s / batch. (data: 9.63e-01). ETA=1 day, 0:38:32, max mem: 20.9 GB 
[11/27 23:59:10 visual_prompt]: 	Training 400/553. train loss: 1.9134,	0.8403 s / batch. (data: 3.17e-04). ETA=11:39:12, max mem: 20.9 GB 
[11/28 00:00:48 visual_prompt]: 	Training 500/553. train loss: 0.9033,	0.8302 s / batch. (data: 6.48e-03). ETA=11:29:23, max mem: 20.9 GB 
[11/28 00:01:39 visual_prompt]: Epoch 10 / 100: avg data time: 1.45e-01, avg batch time: 0.9685, average train loss: 2.1337
[11/28 00:02:34 visual_prompt]: Inference (val):avg data time: 2.19e-04, avg batch time: 0.3068, average loss: 1.3788
[11/28 00:02:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.90	
[11/28 00:02:34 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/28 00:04:16 visual_prompt]: 	Training 100/553. train loss: 2.5256,	0.8160 s / batch. (data: 2.75e-04). ETA=11:15:31, max mem: 20.9 GB 
[11/28 00:05:54 visual_prompt]: 	Training 200/553. train loss: 1.4719,	0.8399 s / batch. (data: 4.49e-04). ETA=11:33:55, max mem: 20.9 GB 
[11/28 00:07:32 visual_prompt]: 	Training 300/553. train loss: 0.0326,	2.3560 s / batch. (data: 1.54e+00). ETA=1 day, 8:22:31, max mem: 20.9 GB 
[11/28 00:09:11 visual_prompt]: 	Training 400/553. train loss: 1.0791,	0.8113 s / batch. (data: 3.05e-04). ETA=11:07:32, max mem: 20.9 GB 
[11/28 00:10:52 visual_prompt]: 	Training 500/553. train loss: 2.7138,	0.8365 s / batch. (data: 3.10e-04). ETA=11:26:56, max mem: 20.9 GB 
[11/28 00:11:43 visual_prompt]: Epoch 11 / 100: avg data time: 1.71e-01, avg batch time: 0.9937, average train loss: 2.4819
[11/28 00:12:44 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3057, average loss: 1.0858
[11/28 00:12:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.37	
[11/28 00:12:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/28 00:14:50 visual_prompt]: 	Training 100/553. train loss: 0.6847,	0.8200 s / batch. (data: 2.82e-04). ETA=11:11:16, max mem: 20.9 GB 
[11/28 00:16:31 visual_prompt]: 	Training 200/553. train loss: 0.5447,	1.6397 s / batch. (data: 8.30e-01). ETA=22:19:35, max mem: 20.9 GB 
[11/28 00:18:06 visual_prompt]: 	Training 300/553. train loss: 1.9428,	0.8180 s / batch. (data: 5.42e-03). ETA=11:06:52, max mem: 20.9 GB 
[11/28 00:19:42 visual_prompt]: 	Training 400/553. train loss: 1.3645,	0.8272 s / batch. (data: 2.07e-02). ETA=11:13:02, max mem: 20.9 GB 
[11/28 00:21:18 visual_prompt]: 	Training 500/553. train loss: 5.1667,	0.8320 s / batch. (data: 3.03e-04). ETA=11:15:32, max mem: 20.9 GB 
[11/28 00:22:07 visual_prompt]: Epoch 12 / 100: avg data time: 1.95e-01, avg batch time: 1.0182, average train loss: 1.8395
[11/28 00:23:02 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3041, average loss: 1.0636
[11/28 00:23:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.40	
[11/28 00:23:02 visual_prompt]: Best epoch 12: best metric: -1.064
[11/28 00:23:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/28 00:24:42 visual_prompt]: 	Training 100/553. train loss: 0.5006,	0.8191 s / batch. (data: 3.08e-04). ETA=11:02:57, max mem: 20.9 GB 
[11/28 00:26:21 visual_prompt]: 	Training 200/553. train loss: 0.7680,	0.8280 s / batch. (data: 5.45e-03). ETA=11:08:47, max mem: 20.9 GB 
[11/28 00:28:03 visual_prompt]: 	Training 300/553. train loss: 0.7476,	1.8253 s / batch. (data: 9.92e-01). ETA=1 day, 0:31:17, max mem: 20.9 GB 
[11/28 00:29:42 visual_prompt]: 	Training 400/553. train loss: 1.3625,	0.8233 s / batch. (data: 5.45e-03). ETA=11:02:13, max mem: 20.9 GB 
[11/28 00:31:24 visual_prompt]: 	Training 500/553. train loss: 3.0766,	0.8310 s / batch. (data: 7.09e-04). ETA=11:07:05, max mem: 20.9 GB 
[11/28 00:32:16 visual_prompt]: Epoch 13 / 100: avg data time: 1.80e-01, avg batch time: 1.0021, average train loss: 2.0376
[11/28 00:33:13 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3059, average loss: 0.8844
[11/28 00:33:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.78	
[11/28 00:33:13 visual_prompt]: Best epoch 13: best metric: -0.884
[11/28 00:33:13 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/28 00:35:04 visual_prompt]: 	Training 100/553. train loss: 8.4550,	0.8222 s / batch. (data: 2.93e-04). ETA=10:57:54, max mem: 20.9 GB 
[11/28 00:36:40 visual_prompt]: 	Training 200/553. train loss: 0.1337,	0.8240 s / batch. (data: 3.02e-04). ETA=10:57:59, max mem: 20.9 GB 
[11/28 00:38:17 visual_prompt]: 	Training 300/553. train loss: 0.7719,	0.8403 s / batch. (data: 1.56e-02). ETA=11:09:36, max mem: 20.9 GB 
[11/28 00:39:52 visual_prompt]: 	Training 400/553. train loss: 0.6683,	0.8280 s / batch. (data: 2.98e-04). ETA=10:58:25, max mem: 20.9 GB 
[11/28 00:41:29 visual_prompt]: 	Training 500/553. train loss: 3.9133,	0.8080 s / batch. (data: 3.12e-04). ETA=10:41:08, max mem: 20.9 GB 
[11/28 00:42:19 visual_prompt]: Epoch 14 / 100: avg data time: 1.62e-01, avg batch time: 0.9859, average train loss: 1.7534
[11/28 00:43:14 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3080, average loss: 1.6784
[11/28 00:43:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.75	
[11/28 00:43:14 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/28 00:44:55 visual_prompt]: 	Training 100/553. train loss: 0.9750,	0.8440 s / batch. (data: 3.02e-04). ETA=11:07:33, max mem: 20.9 GB 
[11/28 00:46:30 visual_prompt]: 	Training 200/553. train loss: 12.8571,	0.8227 s / batch. (data: 2.30e-04). ETA=10:49:20, max mem: 20.9 GB 
[11/28 00:48:08 visual_prompt]: 	Training 300/553. train loss: 1.9245,	0.8280 s / batch. (data: 7.71e-04). ETA=10:52:09, max mem: 20.9 GB 
[11/28 00:49:42 visual_prompt]: 	Training 400/553. train loss: 0.6197,	1.0000 s / batch. (data: 1.74e-01). ETA=13:05:57, max mem: 20.9 GB 
[11/28 00:51:20 visual_prompt]: 	Training 500/553. train loss: 1.6630,	0.8520 s / batch. (data: 1.20e-02). ETA=11:08:13, max mem: 20.9 GB 
[11/28 00:52:12 visual_prompt]: Epoch 15 / 100: avg data time: 1.46e-01, avg batch time: 0.9721, average train loss: 2.6104
[11/28 00:53:07 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3066, average loss: 2.2748
[11/28 00:53:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.09	
[11/28 00:53:07 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/28 00:54:46 visual_prompt]: 	Training 100/553. train loss: 0.5710,	0.8293 s / batch. (data: 3.07e-04). ETA=10:48:18, max mem: 20.9 GB 
[11/28 00:56:24 visual_prompt]: 	Training 200/553. train loss: 2.6514,	0.8200 s / batch. (data: 2.90e-04). ETA=10:39:39, max mem: 20.9 GB 
[11/28 00:57:59 visual_prompt]: 	Training 300/553. train loss: 5.6168,	0.8281 s / batch. (data: 1.19e-02). ETA=10:44:37, max mem: 20.9 GB 
[11/28 00:59:37 visual_prompt]: 	Training 400/553. train loss: 4.9681,	0.8459 s / batch. (data: 7.57e-04). ETA=10:57:01, max mem: 20.9 GB 
[11/28 01:01:13 visual_prompt]: 	Training 500/553. train loss: 0.8915,	1.1320 s / batch. (data: 3.13e-01). ETA=14:37:22, max mem: 20.9 GB 
[11/28 01:02:04 visual_prompt]: Epoch 16 / 100: avg data time: 1.48e-01, avg batch time: 0.9711, average train loss: 2.6295
[11/28 01:02:59 visual_prompt]: Inference (val):avg data time: 4.41e-05, avg batch time: 0.3047, average loss: 0.7354
[11/28 01:02:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.72	
[11/28 01:02:59 visual_prompt]: Best epoch 16: best metric: -0.735
[11/28 01:02:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/28 01:04:39 visual_prompt]: 	Training 100/553. train loss: 0.5410,	0.8508 s / batch. (data: 5.96e-03). ETA=10:57:16, max mem: 20.9 GB 
[11/28 01:06:17 visual_prompt]: 	Training 200/553. train loss: 0.8182,	0.8320 s / batch. (data: 2.83e-04). ETA=10:41:21, max mem: 20.9 GB 
[11/28 01:07:52 visual_prompt]: 	Training 300/553. train loss: 1.0049,	0.8346 s / batch. (data: 2.94e-04). ETA=10:41:56, max mem: 20.9 GB 
[11/28 01:09:29 visual_prompt]: 	Training 400/553. train loss: 6.2289,	1.1600 s / batch. (data: 3.28e-01). ETA=14:50:19, max mem: 20.9 GB 
[11/28 01:11:07 visual_prompt]: 	Training 500/553. train loss: 4.5001,	1.6139 s / batch. (data: 8.07e-01). ETA=20:36:02, max mem: 20.9 GB 
[11/28 01:11:58 visual_prompt]: Epoch 17 / 100: avg data time: 1.50e-01, avg batch time: 0.9752, average train loss: 2.3484
[11/28 01:12:54 visual_prompt]: Inference (val):avg data time: 5.24e-05, avg batch time: 0.3139, average loss: 3.8605
[11/28 01:12:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.36	
[11/28 01:12:54 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/28 01:14:34 visual_prompt]: 	Training 100/553. train loss: 5.0259,	0.8160 s / batch. (data: 2.87e-04). ETA=10:22:51, max mem: 20.9 GB 
[11/28 01:16:14 visual_prompt]: 	Training 200/553. train loss: 16.7930,	0.8096 s / batch. (data: 3.87e-04). ETA=10:16:36, max mem: 20.9 GB 
[11/28 01:17:50 visual_prompt]: 	Training 300/553. train loss: 0.7122,	0.8216 s / batch. (data: 3.01e-04). ETA=10:24:24, max mem: 20.9 GB 
[11/28 01:19:26 visual_prompt]: 	Training 400/553. train loss: 1.0092,	0.8146 s / batch. (data: 2.89e-04). ETA=10:17:42, max mem: 20.9 GB 
[11/28 01:21:02 visual_prompt]: 	Training 500/553. train loss: 0.7725,	0.8320 s / batch. (data: 2.96e-04). ETA=10:29:32, max mem: 20.9 GB 
[11/28 01:21:53 visual_prompt]: Epoch 18 / 100: avg data time: 1.50e-01, avg batch time: 0.9741, average train loss: 3.1650
[11/28 01:22:48 visual_prompt]: Inference (val):avg data time: 5.98e-05, avg batch time: 0.3163, average loss: 0.9248
[11/28 01:22:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.60	
[11/28 01:22:48 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[11/28 01:24:29 visual_prompt]: 	Training 100/553. train loss: 0.6262,	1.0851 s / batch. (data: 2.60e-01). ETA=13:38:15, max mem: 20.9 GB 
[11/28 01:26:06 visual_prompt]: 	Training 200/553. train loss: 1.7280,	0.8434 s / batch. (data: 2.67e-02). ETA=10:34:34, max mem: 20.9 GB 
[11/28 01:27:43 visual_prompt]: 	Training 300/553. train loss: 5.0892,	0.8360 s / batch. (data: 3.25e-04). ETA=10:27:40, max mem: 20.9 GB 
[11/28 01:29:21 visual_prompt]: 	Training 400/553. train loss: 1.3262,	0.8338 s / batch. (data: 5.84e-03). ETA=10:24:37, max mem: 20.9 GB 
[11/28 01:30:54 visual_prompt]: 	Training 500/553. train loss: 1.1437,	0.8066 s / batch. (data: 2.79e-04). ETA=10:02:51, max mem: 20.9 GB 
[11/28 01:31:44 visual_prompt]: Epoch 19 / 100: avg data time: 1.45e-01, avg batch time: 0.9684, average train loss: 1.7645
[11/28 01:32:39 visual_prompt]: Inference (val):avg data time: 2.78e-04, avg batch time: 0.3082, average loss: 3.4530
[11/28 01:32:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.46	
[11/28 01:32:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[11/28 01:34:18 visual_prompt]: 	Training 100/553. train loss: 0.5834,	0.8240 s / batch. (data: 3.19e-04). ETA=10:13:47, max mem: 20.9 GB 
[11/28 01:35:56 visual_prompt]: 	Training 200/553. train loss: 0.6940,	0.8214 s / batch. (data: 3.20e-04). ETA=10:10:29, max mem: 20.9 GB 
[11/28 01:37:34 visual_prompt]: 	Training 300/553. train loss: 1.6089,	0.8353 s / batch. (data: 7.69e-04). ETA=10:19:24, max mem: 20.9 GB 
[11/28 01:39:11 visual_prompt]: 	Training 400/553. train loss: 0.6775,	0.8520 s / batch. (data: 7.94e-03). ETA=10:30:21, max mem: 20.9 GB 
[11/28 01:40:48 visual_prompt]: 	Training 500/553. train loss: 1.3922,	0.8200 s / batch. (data: 2.95e-04). ETA=10:05:20, max mem: 20.9 GB 
[11/28 01:41:40 visual_prompt]: Epoch 20 / 100: avg data time: 1.50e-01, avg batch time: 0.9779, average train loss: 2.1300
[11/28 01:42:34 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3068, average loss: 0.7118
[11/28 01:42:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.56	
[11/28 01:42:34 visual_prompt]: Best epoch 20: best metric: -0.712
[11/28 01:42:34 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[11/28 01:44:16 visual_prompt]: 	Training 100/553. train loss: 0.5810,	0.8440 s / batch. (data: 3.16e-04). ETA=10:20:53, max mem: 20.9 GB 
[11/28 01:45:51 visual_prompt]: 	Training 200/553. train loss: 6.4126,	0.8560 s / batch. (data: 4.40e-04). ETA=10:28:17, max mem: 20.9 GB 
[11/28 01:47:26 visual_prompt]: 	Training 300/553. train loss: 0.8785,	0.8469 s / batch. (data: 1.08e-02). ETA=10:20:10, max mem: 20.9 GB 
[11/28 01:49:02 visual_prompt]: 	Training 400/553. train loss: 3.0139,	0.8160 s / batch. (data: 2.91e-04). ETA=9:56:13, max mem: 20.9 GB 
[11/28 01:50:39 visual_prompt]: 	Training 500/553. train loss: 2.0082,	0.8253 s / batch. (data: 2.83e-04). ETA=10:01:39, max mem: 20.9 GB 
[11/28 01:51:28 visual_prompt]: Epoch 21 / 100: avg data time: 1.41e-01, avg batch time: 0.9656, average train loss: 2.1354
[11/28 01:52:23 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3085, average loss: 1.3491
[11/28 01:52:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.65	
[11/28 01:52:23 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[11/28 01:54:02 visual_prompt]: 	Training 100/553. train loss: 1.5063,	0.8428 s / batch. (data: 2.45e-02). ETA=10:12:14, max mem: 20.9 GB 
[11/28 01:55:38 visual_prompt]: 	Training 200/553. train loss: 0.6319,	0.8071 s / batch. (data: 4.37e-04). ETA=9:44:58, max mem: 20.9 GB 
[11/28 01:57:12 visual_prompt]: 	Training 300/553. train loss: 0.0657,	0.8073 s / batch. (data: 2.71e-04). ETA=9:43:46, max mem: 20.9 GB 
[11/28 01:58:49 visual_prompt]: 	Training 400/553. train loss: 10.1777,	0.8076 s / batch. (data: 2.69e-04). ETA=9:42:37, max mem: 20.9 GB 
[11/28 02:00:25 visual_prompt]: 	Training 500/553. train loss: 0.8846,	0.8205 s / batch. (data: 3.24e-04). ETA=9:50:36, max mem: 20.9 GB 
[11/28 02:01:16 visual_prompt]: Epoch 22 / 100: avg data time: 1.40e-01, avg batch time: 0.9639, average train loss: 1.8319
[11/28 02:02:11 visual_prompt]: Inference (val):avg data time: 2.71e-04, avg batch time: 0.3058, average loss: 3.1889
[11/28 02:02:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.64	
[11/28 02:02:11 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[11/28 02:03:52 visual_prompt]: 	Training 100/553. train loss: 1.2534,	0.8269 s / batch. (data: 1.03e-02). ETA=9:53:04, max mem: 20.9 GB 
[11/28 02:05:28 visual_prompt]: 	Training 200/553. train loss: 0.5634,	0.8278 s / batch. (data: 7.00e-03). ETA=9:52:20, max mem: 20.9 GB 
[11/28 02:07:06 visual_prompt]: 	Training 300/553. train loss: 0.5942,	0.8360 s / batch. (data: 3.01e-04). ETA=9:56:49, max mem: 20.9 GB 
[11/28 02:08:40 visual_prompt]: 	Training 400/553. train loss: 1.5386,	0.8498 s / batch. (data: 5.98e-03). ETA=10:05:16, max mem: 20.9 GB 
[11/28 02:10:14 visual_prompt]: 	Training 500/553. train loss: 0.0000,	0.8333 s / batch. (data: 2.94e-04). ETA=9:52:06, max mem: 20.9 GB 
[11/28 02:11:05 visual_prompt]: Epoch 23 / 100: avg data time: 1.43e-01, avg batch time: 0.9657, average train loss: 2.2013
[11/28 02:11:59 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3058, average loss: 2.4110
[11/28 02:11:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.72	
[11/28 02:11:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[11/28 02:13:36 visual_prompt]: 	Training 100/553. train loss: 6.5537,	0.8544 s / batch. (data: 1.04e-02). ETA=10:04:56, max mem: 20.9 GB 
[11/28 02:15:12 visual_prompt]: 	Training 200/553. train loss: 0.6106,	0.8176 s / batch. (data: 7.61e-04). ETA=9:37:31, max mem: 20.9 GB 
[11/28 02:16:47 visual_prompt]: 	Training 300/553. train loss: 1.3971,	0.8492 s / batch. (data: 1.60e-02). ETA=9:58:25, max mem: 20.9 GB 
[11/28 02:18:24 visual_prompt]: 	Training 400/553. train loss: 0.6552,	0.8222 s / batch. (data: 7.95e-03). ETA=9:38:03, max mem: 20.9 GB 
[11/28 02:20:02 visual_prompt]: 	Training 500/553. train loss: 0.9322,	0.8060 s / batch. (data: 3.13e-04). ETA=9:25:16, max mem: 20.9 GB 
[11/28 02:20:53 visual_prompt]: Epoch 24 / 100: avg data time: 1.41e-01, avg batch time: 0.9647, average train loss: 2.2871
[11/28 02:21:48 visual_prompt]: Inference (val):avg data time: 4.38e-04, avg batch time: 0.3147, average loss: 0.7321
[11/28 02:21:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.61	
[11/28 02:21:48 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[11/28 02:23:30 visual_prompt]: 	Training 100/553. train loss: 2.2837,	0.8360 s / batch. (data: 2.96e-04). ETA=9:44:11, max mem: 20.9 GB 
[11/28 02:25:03 visual_prompt]: 	Training 200/553. train loss: 1.1692,	0.8240 s / batch. (data: 1.19e-02). ETA=9:34:26, max mem: 20.9 GB 
[11/28 02:26:39 visual_prompt]: 	Training 300/553. train loss: 1.5923,	0.8061 s / batch. (data: 2.35e-04). ETA=9:20:37, max mem: 20.9 GB 
[11/28 02:28:15 visual_prompt]: 	Training 400/553. train loss: 1.2355,	1.1017 s / batch. (data: 2.90e-01). ETA=12:44:21, max mem: 20.9 GB 
[11/28 02:29:52 visual_prompt]: 	Training 500/553. train loss: 0.7242,	0.8165 s / batch. (data: 3.15e-04). ETA=9:25:06, max mem: 20.9 GB 
[11/28 02:30:42 visual_prompt]: Epoch 25 / 100: avg data time: 1.42e-01, avg batch time: 0.9644, average train loss: 1.6836
[11/28 02:31:36 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3070, average loss: 3.6299
[11/28 02:31:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.64	
[11/28 02:31:36 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[11/28 02:33:16 visual_prompt]: 	Training 100/553. train loss: 2.0128,	0.8479 s / batch. (data: 1.60e-02). ETA=9:44:43, max mem: 20.9 GB 
[11/28 02:34:54 visual_prompt]: 	Training 200/553. train loss: 11.8396,	1.7280 s / batch. (data: 9.19e-01). ETA=19:48:44, max mem: 20.9 GB 
[11/28 02:36:31 visual_prompt]: 	Training 300/553. train loss: 0.1117,	0.8200 s / batch. (data: 2.85e-04). ETA=9:22:43, max mem: 20.9 GB 
[11/28 02:38:06 visual_prompt]: 	Training 400/553. train loss: 0.5824,	0.8224 s / batch. (data: 3.41e-04). ETA=9:22:58, max mem: 20.9 GB 
[11/28 02:39:40 visual_prompt]: 	Training 500/553. train loss: 5.2843,	0.8640 s / batch. (data: 7.62e-04). ETA=9:50:00, max mem: 20.9 GB 
[11/28 02:40:30 visual_prompt]: Epoch 26 / 100: avg data time: 1.40e-01, avg batch time: 0.9650, average train loss: 2.4047
[11/28 02:41:24 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3055, average loss: 0.8601
[11/28 02:41:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.71	
[11/28 02:41:24 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[11/28 02:43:05 visual_prompt]: 	Training 100/553. train loss: 0.6568,	0.8178 s / batch. (data: 7.96e-03). ETA=9:16:24, max mem: 20.9 GB 
[11/28 02:44:40 visual_prompt]: 	Training 200/553. train loss: 5.6375,	0.9422 s / batch. (data: 1.15e-01). ETA=10:39:26, max mem: 20.9 GB 
[11/28 02:46:16 visual_prompt]: 	Training 300/553. train loss: 2.6742,	0.8151 s / batch. (data: 3.15e-04). ETA=9:11:49, max mem: 20.9 GB 
[11/28 02:47:53 visual_prompt]: 	Training 400/553. train loss: 22.6630,	0.8214 s / batch. (data: 7.81e-04). ETA=9:14:44, max mem: 20.9 GB 
[11/28 02:49:29 visual_prompt]: 	Training 500/553. train loss: 0.7050,	0.8325 s / batch. (data: 7.86e-04). ETA=9:20:49, max mem: 20.9 GB 
[11/28 02:50:17 visual_prompt]: Epoch 27 / 100: avg data time: 1.41e-01, avg batch time: 0.9634, average train loss: 2.5300
[11/28 02:51:14 visual_prompt]: Inference (val):avg data time: 5.09e-05, avg batch time: 0.3193, average loss: 4.6716
[11/28 02:51:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.49	
[11/28 02:51:14 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[11/28 02:52:53 visual_prompt]: 	Training 100/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.14e-04). ETA=9:21:04, max mem: 20.9 GB 
[11/28 02:54:29 visual_prompt]: 	Training 200/553. train loss: 0.6092,	0.8139 s / batch. (data: 2.85e-04). ETA=9:04:52, max mem: 20.9 GB 
[11/28 02:56:06 visual_prompt]: 	Training 300/553. train loss: 1.4800,	1.3302 s / batch. (data: 5.05e-01). ETA=14:48:19, max mem: 20.9 GB 
[11/28 02:57:41 visual_prompt]: 	Training 400/553. train loss: 3.4777,	0.8259 s / batch. (data: 2.92e-04). ETA=9:10:08, max mem: 20.9 GB 
[11/28 02:59:16 visual_prompt]: 	Training 500/553. train loss: 3.5916,	0.8182 s / batch. (data: 2.88e-04). ETA=9:03:39, max mem: 20.9 GB 
[11/28 03:00:07 visual_prompt]: Epoch 28 / 100: avg data time: 1.39e-01, avg batch time: 0.9638, average train loss: 2.3100
[11/28 03:01:01 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3052, average loss: 0.7591
[11/28 03:01:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.65	
[11/28 03:01:01 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[11/28 03:02:47 visual_prompt]: 	Training 100/553. train loss: 1.9452,	0.8255 s / batch. (data: 6.89e-04). ETA=9:06:25, max mem: 20.9 GB 
[11/28 03:04:22 visual_prompt]: 	Training 200/553. train loss: 0.9163,	1.8612 s / batch. (data: 1.04e+00). ETA=20:28:53, max mem: 20.9 GB 
[11/28 03:05:56 visual_prompt]: 	Training 300/553. train loss: 1.0176,	0.8176 s / batch. (data: 3.28e-04). ETA=8:58:26, max mem: 20.9 GB 
[11/28 03:07:30 visual_prompt]: 	Training 400/553. train loss: 2.4156,	1.2400 s / batch. (data: 4.11e-01). ETA=13:34:35, max mem: 20.9 GB 
[11/28 03:09:06 visual_prompt]: 	Training 500/553. train loss: 2.2743,	0.8240 s / batch. (data: 3.06e-04). ETA=8:59:55, max mem: 20.9 GB 
[11/28 03:09:55 visual_prompt]: Epoch 29 / 100: avg data time: 1.43e-01, avg batch time: 0.9659, average train loss: 1.7621
[11/28 03:10:50 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.3064, average loss: 1.4689
[11/28 03:10:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.46	
[11/28 03:10:50 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[11/28 03:12:29 visual_prompt]: 	Training 100/553. train loss: 1.3579,	0.8160 s / batch. (data: 2.83e-04). ETA=8:52:37, max mem: 20.9 GB 
[11/28 03:14:06 visual_prompt]: 	Training 200/553. train loss: 1.5283,	0.8155 s / batch. (data: 1.05e-02). ETA=8:50:57, max mem: 20.9 GB 
[11/28 03:15:40 visual_prompt]: 	Training 300/553. train loss: 0.0000,	0.9839 s / batch. (data: 1.73e-01). ETA=10:38:56, max mem: 20.9 GB 
[11/28 03:17:17 visual_prompt]: 	Training 400/553. train loss: 0.8073,	1.1001 s / batch. (data: 2.75e-01). ETA=11:52:31, max mem: 20.9 GB 
[11/28 03:18:52 visual_prompt]: 	Training 500/553. train loss: 2.9417,	1.4144 s / batch. (data: 5.93e-01). ETA=15:13:44, max mem: 20.9 GB 
[11/28 03:19:43 visual_prompt]: Epoch 30 / 100: avg data time: 1.40e-01, avg batch time: 0.9638, average train loss: 1.7273
[11/28 03:20:38 visual_prompt]: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3069, average loss: 0.7019
[11/28 03:20:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.84	
[11/28 03:20:38 visual_prompt]: Best epoch 30: best metric: -0.702
[11/28 03:20:38 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[11/28 03:22:19 visual_prompt]: 	Training 100/553. train loss: 2.5419,	0.8253 s / batch. (data: 7.95e-03). ETA=8:51:06, max mem: 20.9 GB 
[11/28 03:23:56 visual_prompt]: 	Training 200/553. train loss: 1.6897,	0.8181 s / batch. (data: 2.94e-04). ETA=8:45:05, max mem: 20.9 GB 
[11/28 03:25:29 visual_prompt]: 	Training 300/553. train loss: 3.9643,	0.8458 s / batch. (data: 1.06e-02). ETA=9:01:28, max mem: 20.9 GB 
[11/28 03:27:05 visual_prompt]: 	Training 400/553. train loss: 1.2880,	1.1776 s / batch. (data: 3.43e-01). ETA=12:31:53, max mem: 20.9 GB 
[11/28 03:28:40 visual_prompt]: 	Training 500/553. train loss: 0.6039,	0.8160 s / batch. (data: 2.78e-04). ETA=8:39:39, max mem: 20.9 GB 
[11/28 03:29:30 visual_prompt]: Epoch 31 / 100: avg data time: 1.39e-01, avg batch time: 0.9624, average train loss: 2.2001
[11/28 03:30:25 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3063, average loss: 0.8232
[11/28 03:30:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.39	
[11/28 03:30:25 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[11/28 03:32:05 visual_prompt]: 	Training 100/553. train loss: 0.9455,	0.8366 s / batch. (data: 2.82e-04). ETA=8:50:37, max mem: 20.9 GB 
[11/28 03:33:41 visual_prompt]: 	Training 200/553. train loss: 1.8196,	0.8480 s / batch. (data: 7.36e-04). ETA=8:56:27, max mem: 20.9 GB 
[11/28 03:35:20 visual_prompt]: 	Training 300/553. train loss: 2.4443,	0.8250 s / batch. (data: 2.81e-04). ETA=8:40:33, max mem: 20.9 GB 
[11/28 03:36:56 visual_prompt]: 	Training 400/553. train loss: 0.7317,	0.8440 s / batch. (data: 3.03e-04). ETA=8:51:08, max mem: 20.9 GB 
[11/28 03:38:29 visual_prompt]: 	Training 500/553. train loss: 1.1630,	0.8259 s / batch. (data: 5.40e-03). ETA=8:38:22, max mem: 20.9 GB 
[11/28 03:39:17 visual_prompt]: Epoch 32 / 100: avg data time: 1.39e-01, avg batch time: 0.9629, average train loss: 1.5822
[11/28 03:40:12 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3063, average loss: 2.1389
[11/28 03:40:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.80	
[11/28 03:40:12 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[11/28 03:41:50 visual_prompt]: 	Training 100/553. train loss: 0.0169,	1.1381 s / batch. (data: 3.09e-01). ETA=11:51:23, max mem: 20.9 GB 
[11/28 03:43:28 visual_prompt]: 	Training 200/553. train loss: 2.2373,	1.1240 s / batch. (data: 3.05e-01). ETA=11:40:42, max mem: 20.9 GB 
[11/28 03:45:03 visual_prompt]: 	Training 300/553. train loss: 0.6407,	0.8399 s / batch. (data: 2.79e-04). ETA=8:42:10, max mem: 20.9 GB 
[11/28 03:46:40 visual_prompt]: 	Training 400/553. train loss: 3.1459,	0.8161 s / batch. (data: 2.99e-04). ETA=8:26:02, max mem: 20.9 GB 
[11/28 03:48:15 visual_prompt]: 	Training 500/553. train loss: 1.8285,	0.8256 s / batch. (data: 3.20e-04). ETA=8:30:31, max mem: 20.9 GB 
[11/28 03:49:04 visual_prompt]: Epoch 33 / 100: avg data time: 1.39e-01, avg batch time: 0.9624, average train loss: 2.2029
[11/28 03:49:59 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3067, average loss: 1.7899
[11/28 03:49:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.28	
[11/28 03:49:59 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[11/28 03:51:41 visual_prompt]: 	Training 100/553. train loss: 1.7802,	0.9120 s / batch. (data: 8.36e-02). ETA=9:21:38, max mem: 20.9 GB 
[11/28 03:53:16 visual_prompt]: 	Training 200/553. train loss: 0.7059,	0.8440 s / batch. (data: 1.20e-02). ETA=8:38:22, max mem: 20.9 GB 
[11/28 03:54:51 visual_prompt]: 	Training 300/553. train loss: 0.6871,	0.8120 s / batch. (data: 2.92e-04). ETA=8:17:21, max mem: 20.9 GB 
[11/28 03:56:28 visual_prompt]: 	Training 400/553. train loss: 8.1180,	0.8051 s / batch. (data: 2.96e-04). ETA=8:11:46, max mem: 20.9 GB 
[11/28 03:58:04 visual_prompt]: 	Training 500/553. train loss: 1.1805,	1.3280 s / batch. (data: 5.16e-01). ETA=13:29:00, max mem: 20.9 GB 
[11/28 03:58:55 visual_prompt]: Epoch 34 / 100: avg data time: 1.43e-01, avg batch time: 0.9692, average train loss: 2.4220
[11/28 03:59:51 visual_prompt]: Inference (val):avg data time: 4.87e-04, avg batch time: 0.3189, average loss: 0.6973
[11/28 03:59:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.53	
[11/28 03:59:51 visual_prompt]: Best epoch 34: best metric: -0.697
[11/28 03:59:51 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[11/28 04:01:32 visual_prompt]: 	Training 100/553. train loss: 3.9472,	0.8320 s / batch. (data: 7.93e-03). ETA=8:24:43, max mem: 20.9 GB 
[11/28 04:03:10 visual_prompt]: 	Training 200/553. train loss: 0.7024,	0.8200 s / batch. (data: 3.02e-04). ETA=8:16:03, max mem: 20.9 GB 
[11/28 04:04:44 visual_prompt]: 	Training 300/553. train loss: 3.0088,	0.8188 s / batch. (data: 5.88e-04). ETA=8:13:59, max mem: 20.9 GB 
[11/28 04:06:20 visual_prompt]: 	Training 400/553. train loss: 3.9492,	0.8320 s / batch. (data: 5.42e-03). ETA=8:20:33, max mem: 20.9 GB 
[11/28 04:07:54 visual_prompt]: 	Training 500/553. train loss: 0.7027,	1.0720 s / batch. (data: 2.32e-01). ETA=10:43:09, max mem: 20.9 GB 
[11/28 04:08:45 visual_prompt]: Epoch 35 / 100: avg data time: 1.43e-01, avg batch time: 0.9669, average train loss: 1.8787
[11/28 04:09:40 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3077, average loss: 4.9512
[11/28 04:09:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.78	
[11/28 04:09:40 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[11/28 04:11:20 visual_prompt]: 	Training 100/553. train loss: 1.8079,	0.8246 s / batch. (data: 2.29e-03). ETA=8:12:38, max mem: 20.9 GB 
[11/28 04:12:57 visual_prompt]: 	Training 200/553. train loss: 1.8903,	0.8160 s / batch. (data: 3.21e-04). ETA=8:06:07, max mem: 20.9 GB 
[11/28 04:14:35 visual_prompt]: 	Training 300/553. train loss: 0.0066,	0.8068 s / batch. (data: 2.87e-04). ETA=7:59:17, max mem: 20.9 GB 
[11/28 04:16:11 visual_prompt]: 	Training 400/553. train loss: 0.7187,	0.8275 s / batch. (data: 1.05e-02). ETA=8:10:14, max mem: 20.9 GB 
[11/28 04:17:49 visual_prompt]: 	Training 500/553. train loss: 1.8388,	0.9320 s / batch. (data: 1.07e-01). ETA=9:10:35, max mem: 20.9 GB 
[11/28 04:18:37 visual_prompt]: Epoch 36 / 100: avg data time: 1.44e-01, avg batch time: 0.9702, average train loss: 2.0146
[11/28 04:19:32 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3072, average loss: 1.5923
[11/28 04:19:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.21	
[11/28 04:19:32 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[11/28 04:21:12 visual_prompt]: 	Training 100/553. train loss: 0.5348,	0.8075 s / batch. (data: 3.06e-04). ETA=7:54:57, max mem: 20.9 GB 
[11/28 04:22:48 visual_prompt]: 	Training 200/553. train loss: 0.8555,	0.8614 s / batch. (data: 2.55e-02). ETA=8:25:15, max mem: 20.9 GB 
[11/28 04:24:23 visual_prompt]: 	Training 300/553. train loss: 3.8998,	1.1720 s / batch. (data: 3.45e-01). ETA=11:25:28, max mem: 20.9 GB 
[11/28 04:26:01 visual_prompt]: 	Training 400/553. train loss: 0.7419,	1.8760 s / batch. (data: 1.05e+00). ETA=18:14:05, max mem: 20.9 GB 
[11/28 04:27:34 visual_prompt]: 	Training 500/553. train loss: 1.4059,	1.1137 s / batch. (data: 2.93e-01). ETA=10:47:39, max mem: 20.9 GB 
[11/28 04:28:26 visual_prompt]: Epoch 37 / 100: avg data time: 1.43e-01, avg batch time: 0.9657, average train loss: 1.9589
[11/28 04:29:21 visual_prompt]: Inference (val):avg data time: 5.09e-05, avg batch time: 0.3083, average loss: 0.7793
[11/28 04:29:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.97	
[11/28 04:29:21 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[11/28 04:30:59 visual_prompt]: 	Training 100/553. train loss: 0.5645,	0.8262 s / batch. (data: 7.94e-03). ETA=7:58:22, max mem: 20.9 GB 
[11/28 04:32:36 visual_prompt]: 	Training 200/553. train loss: 1.0203,	0.8160 s / batch. (data: 7.93e-03). ETA=7:51:06, max mem: 20.9 GB 
[11/28 04:34:13 visual_prompt]: 	Training 300/553. train loss: 2.2659,	0.8160 s / batch. (data: 3.02e-04). ETA=7:49:44, max mem: 20.9 GB 
[11/28 04:35:47 visual_prompt]: 	Training 400/553. train loss: 0.4156,	0.8361 s / batch. (data: 2.21e-02). ETA=7:59:55, max mem: 20.9 GB 
[11/28 04:37:25 visual_prompt]: 	Training 500/553. train loss: 0.5712,	0.8064 s / batch. (data: 3.16e-04). ETA=7:41:31, max mem: 20.9 GB 
[11/28 04:38:14 visual_prompt]: Epoch 38 / 100: avg data time: 1.39e-01, avg batch time: 0.9636, average train loss: 1.9836
[11/28 04:39:08 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.3050, average loss: 2.0595
[11/28 04:39:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.40	
[11/28 04:39:08 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[11/28 04:40:46 visual_prompt]: 	Training 100/553. train loss: 0.0233,	0.8407 s / batch. (data: 7.89e-03). ETA=7:58:59, max mem: 20.9 GB 
[11/28 04:42:26 visual_prompt]: 	Training 200/553. train loss: 2.2022,	0.8104 s / batch. (data: 5.40e-03). ETA=7:40:22, max mem: 20.9 GB 
[11/28 04:44:04 visual_prompt]: 	Training 300/553. train loss: 1.7887,	0.8308 s / batch. (data: 1.05e-02). ETA=7:50:36, max mem: 20.9 GB 
[11/28 04:45:39 visual_prompt]: 	Training 400/553. train loss: 1.1589,	1.3051 s / batch. (data: 4.76e-01). ETA=12:17:06, max mem: 20.9 GB 
[11/28 04:47:14 visual_prompt]: 	Training 500/553. train loss: 1.4047,	1.3427 s / batch. (data: 5.07e-01). ETA=12:36:05, max mem: 20.9 GB 
[11/28 04:48:03 visual_prompt]: Epoch 39 / 100: avg data time: 1.44e-01, avg batch time: 0.9669, average train loss: 1.9424
[11/28 04:48:57 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3065, average loss: 0.6912
[11/28 04:48:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 49.58	
[11/28 04:48:57 visual_prompt]: Best epoch 39: best metric: -0.691
[11/28 04:48:57 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[11/28 04:50:38 visual_prompt]: 	Training 100/553. train loss: 2.9837,	0.8133 s / batch. (data: 5.40e-03). ETA=7:35:55, max mem: 20.9 GB 
[11/28 04:52:14 visual_prompt]: 	Training 200/553. train loss: 2.2992,	0.8389 s / batch. (data: 1.05e-02). ETA=7:48:50, max mem: 20.9 GB 
[11/28 04:53:49 visual_prompt]: 	Training 300/553. train loss: 3.2350,	0.8520 s / batch. (data: 7.94e-03). ETA=7:54:44, max mem: 20.9 GB 
[11/28 04:55:27 visual_prompt]: 	Training 400/553. train loss: 3.3173,	0.8214 s / batch. (data: 3.01e-04). ETA=7:36:20, max mem: 20.9 GB 
[11/28 04:57:03 visual_prompt]: 	Training 500/553. train loss: 1.2729,	0.8345 s / batch. (data: 2.83e-04). ETA=7:42:12, max mem: 20.9 GB 
[11/28 04:57:54 visual_prompt]: Epoch 40 / 100: avg data time: 1.45e-01, avg batch time: 0.9700, average train loss: 2.1331
[11/28 04:58:49 visual_prompt]: Inference (val):avg data time: 5.41e-05, avg batch time: 0.3089, average loss: 0.6875
[11/28 04:58:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.70	
[11/28 04:58:49 visual_prompt]: Best epoch 40: best metric: -0.687
[11/28 04:58:49 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[11/28 05:00:33 visual_prompt]: 	Training 100/553. train loss: 1.0026,	0.8212 s / batch. (data: 5.42e-03). ETA=7:32:46, max mem: 20.9 GB 
[11/28 05:02:11 visual_prompt]: 	Training 200/553. train loss: 0.5577,	0.8175 s / batch. (data: 1.05e-02). ETA=7:29:21, max mem: 20.9 GB 
[11/28 05:03:46 visual_prompt]: 	Training 300/553. train loss: 0.8985,	0.8141 s / batch. (data: 2.92e-04). ETA=7:26:08, max mem: 20.9 GB 
[11/28 05:05:22 visual_prompt]: 	Training 400/553. train loss: 0.7687,	0.8240 s / batch. (data: 2.74e-04). ETA=7:30:10, max mem: 20.9 GB 
[11/28 05:06:55 visual_prompt]: 	Training 500/553. train loss: 0.9609,	0.8480 s / batch. (data: 3.01e-04). ETA=7:41:52, max mem: 20.9 GB 
[11/28 05:07:42 visual_prompt]: Epoch 41 / 100: avg data time: 1.41e-01, avg batch time: 0.9643, average train loss: 1.8076
[11/28 05:08:37 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3061, average loss: 0.8254
[11/28 05:08:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.25	
[11/28 05:08:37 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[11/28 05:10:15 visual_prompt]: 	Training 100/553. train loss: 3.5121,	0.8138 s / batch. (data: 2.86e-04). ETA=7:21:10, max mem: 20.9 GB 
[11/28 05:11:52 visual_prompt]: 	Training 200/553. train loss: 2.8598,	0.8146 s / batch. (data: 6.86e-03). ETA=7:20:13, max mem: 20.9 GB 
[11/28 05:13:29 visual_prompt]: 	Training 300/553. train loss: 1.5575,	0.8120 s / batch. (data: 2.89e-04). ETA=7:17:29, max mem: 20.9 GB 
[11/28 05:15:06 visual_prompt]: 	Training 400/553. train loss: 0.8109,	0.8321 s / batch. (data: 5.42e-03). ETA=7:26:57, max mem: 20.9 GB 
[11/28 05:16:41 visual_prompt]: 	Training 500/553. train loss: 1.9449,	0.8402 s / batch. (data: 2.21e-02). ETA=7:29:53, max mem: 20.9 GB 
[11/28 05:17:33 visual_prompt]: Epoch 42 / 100: avg data time: 1.42e-01, avg batch time: 0.9697, average train loss: 2.8876
[11/28 05:18:28 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3076, average loss: 3.6646
[11/28 05:18:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.18	
[11/28 05:18:28 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[11/28 05:20:10 visual_prompt]: 	Training 100/553. train loss: 0.5826,	0.8280 s / batch. (data: 7.95e-03). ETA=7:21:14, max mem: 20.9 GB 
[11/28 05:21:45 visual_prompt]: 	Training 200/553. train loss: 2.6250,	0.8120 s / batch. (data: 2.62e-04). ETA=7:11:22, max mem: 20.9 GB 
[11/28 05:23:19 visual_prompt]: 	Training 300/553. train loss: 0.8087,	0.8292 s / batch. (data: 1.19e-02). ETA=7:19:07, max mem: 20.9 GB 
[11/28 05:24:53 visual_prompt]: 	Training 400/553. train loss: 0.8170,	0.8430 s / batch. (data: 5.42e-03). ETA=7:25:02, max mem: 20.9 GB 
[11/28 05:26:30 visual_prompt]: 	Training 500/553. train loss: 2.3906,	0.8344 s / batch. (data: 1.55e-02). ETA=7:19:06, max mem: 20.9 GB 
[11/28 05:27:22 visual_prompt]: Epoch 43 / 100: avg data time: 1.42e-01, avg batch time: 0.9648, average train loss: 1.7912
[11/28 05:28:16 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.3066, average loss: 0.7125
[11/28 05:28:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.82	
[11/28 05:28:16 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[11/28 05:29:57 visual_prompt]: 	Training 100/553. train loss: 0.6620,	0.8325 s / batch. (data: 5.45e-03). ETA=7:15:59, max mem: 20.9 GB 
[11/28 05:31:34 visual_prompt]: 	Training 200/553. train loss: 1.5963,	0.8229 s / batch. (data: 2.72e-04). ETA=7:09:34, max mem: 20.9 GB 
[11/28 05:33:08 visual_prompt]: 	Training 300/553. train loss: 1.1343,	0.8185 s / batch. (data: 7.94e-03). ETA=7:05:52, max mem: 20.9 GB 
[11/28 05:34:43 visual_prompt]: 	Training 400/553. train loss: 0.7961,	0.8240 s / batch. (data: 2.70e-04). ETA=7:07:24, max mem: 20.9 GB 
[11/28 05:36:20 visual_prompt]: 	Training 500/553. train loss: 1.4189,	0.8177 s / batch. (data: 3.24e-04). ETA=7:02:44, max mem: 20.9 GB 
[11/28 05:37:11 visual_prompt]: Epoch 44 / 100: avg data time: 1.43e-01, avg batch time: 0.9666, average train loss: 1.7323
[11/28 05:38:05 visual_prompt]: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3068, average loss: 3.6322
[11/28 05:38:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.28	
[11/28 05:38:05 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[11/28 05:39:47 visual_prompt]: 	Training 100/553. train loss: 1.9493,	0.8310 s / batch. (data: 6.26e-03). ETA=7:07:32, max mem: 20.9 GB 
[11/28 05:41:19 visual_prompt]: 	Training 200/553. train loss: 1.9173,	0.8424 s / batch. (data: 1.04e-02). ETA=7:11:58, max mem: 20.9 GB 
[11/28 05:42:56 visual_prompt]: 	Training 300/553. train loss: 4.5470,	0.8496 s / batch. (data: 1.05e-02). ETA=7:14:16, max mem: 20.9 GB 
[11/28 05:44:30 visual_prompt]: 	Training 400/553. train loss: 0.9918,	0.8240 s / batch. (data: 2.98e-04). ETA=6:59:47, max mem: 20.9 GB 
[11/28 05:46:08 visual_prompt]: 	Training 500/553. train loss: 0.5472,	0.8390 s / batch. (data: 1.10e-02). ETA=7:06:02, max mem: 20.9 GB 
[11/28 05:46:57 visual_prompt]: Epoch 45 / 100: avg data time: 1.39e-01, avg batch time: 0.9621, average train loss: 1.2542
[11/28 05:47:52 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3055, average loss: 2.2121
[11/28 05:47:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.60	
[11/28 05:47:52 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[11/28 05:49:33 visual_prompt]: 	Training 100/553. train loss: 1.0741,	1.2115 s / batch. (data: 3.91e-01). ETA=10:12:06, max mem: 20.9 GB 
[11/28 05:51:10 visual_prompt]: 	Training 200/553. train loss: 1.3387,	0.8235 s / batch. (data: 7.78e-04). ETA=6:54:42, max mem: 20.9 GB 
[11/28 05:52:44 visual_prompt]: 	Training 300/553. train loss: 1.0882,	0.8071 s / batch. (data: 2.75e-04). ETA=6:45:04, max mem: 20.9 GB 
[11/28 05:54:22 visual_prompt]: 	Training 400/553. train loss: 1.5967,	0.8223 s / batch. (data: 2.80e-04). ETA=6:51:21, max mem: 20.9 GB 
[11/28 05:55:53 visual_prompt]: 	Training 500/553. train loss: 1.9018,	0.8484 s / batch. (data: 2.44e-02). ETA=7:03:00, max mem: 20.9 GB 
[11/28 05:56:46 visual_prompt]: Epoch 46 / 100: avg data time: 1.43e-01, avg batch time: 0.9649, average train loss: 1.1625
[11/28 05:57:41 visual_prompt]: Inference (val):avg data time: 1.98e-04, avg batch time: 0.3053, average loss: 0.7888
[11/28 05:57:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.65	
[11/28 05:57:41 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[11/28 05:59:23 visual_prompt]: 	Training 100/553. train loss: 3.3990,	0.8120 s / batch. (data: 3.11e-04). ETA=6:42:46, max mem: 20.9 GB 
[11/28 06:00:56 visual_prompt]: 	Training 200/553. train loss: 2.1580,	1.2588 s / batch. (data: 4.53e-01). ETA=10:22:19, max mem: 20.9 GB 
[11/28 06:02:34 visual_prompt]: 	Training 300/553. train loss: 2.5841,	0.8203 s / batch. (data: 2.39e-04). ETA=6:44:09, max mem: 20.9 GB 
[11/28 06:04:12 visual_prompt]: 	Training 400/553. train loss: 1.3300,	0.8181 s / batch. (data: 2.89e-04). ETA=6:41:42, max mem: 20.9 GB 
[11/28 06:05:47 visual_prompt]: 	Training 500/553. train loss: 1.0709,	0.8306 s / batch. (data: 7.25e-04). ETA=6:46:28, max mem: 20.9 GB 
[11/28 06:06:39 visual_prompt]: Epoch 47 / 100: avg data time: 1.50e-01, avg batch time: 0.9726, average train loss: 1.8019
[11/28 06:07:34 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3059, average loss: 1.6289
[11/28 06:07:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.55	
[11/28 06:07:34 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[11/28 06:09:16 visual_prompt]: 	Training 100/553. train loss: 1.5418,	0.8400 s / batch. (data: 2.83e-04). ETA=6:48:55, max mem: 20.9 GB 
[11/28 06:10:53 visual_prompt]: 	Training 200/553. train loss: 1.7255,	0.8320 s / batch. (data: 3.21e-04). ETA=6:43:38, max mem: 20.9 GB 
[11/28 06:12:31 visual_prompt]: 	Training 300/553. train loss: 0.7735,	1.6146 s / batch. (data: 8.09e-01). ETA=13:00:38, max mem: 20.9 GB 
[11/28 06:14:05 visual_prompt]: 	Training 400/553. train loss: 0.0003,	0.8182 s / batch. (data: 5.57e-03). ETA=6:34:14, max mem: 20.9 GB 
[11/28 06:15:41 visual_prompt]: 	Training 500/553. train loss: 0.8329,	0.8120 s / batch. (data: 3.13e-04). ETA=6:29:53, max mem: 20.9 GB 
[11/28 06:16:33 visual_prompt]: Epoch 48 / 100: avg data time: 1.50e-01, avg batch time: 0.9751, average train loss: 1.3961
[11/28 06:17:29 visual_prompt]: Inference (val):avg data time: 2.07e-04, avg batch time: 0.3131, average loss: 1.5958
[11/28 06:17:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.42	
[11/28 06:17:29 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[11/28 06:19:09 visual_prompt]: 	Training 100/553. train loss: 0.7296,	0.8360 s / batch. (data: 2.90e-04). ETA=6:39:16, max mem: 20.9 GB 
[11/28 06:20:45 visual_prompt]: 	Training 200/553. train loss: 1.0577,	0.8061 s / batch. (data: 3.02e-04). ETA=6:23:38, max mem: 20.9 GB 
[11/28 06:22:22 visual_prompt]: 	Training 300/553. train loss: 3.9160,	0.8080 s / batch. (data: 3.00e-04). ETA=6:23:12, max mem: 20.9 GB 
[11/28 06:24:00 visual_prompt]: 	Training 400/553. train loss: 0.8221,	0.8209 s / batch. (data: 2.92e-04). ETA=6:27:56, max mem: 20.9 GB 
[11/28 06:25:37 visual_prompt]: 	Training 500/553. train loss: 0.5410,	0.8063 s / batch. (data: 3.45e-04). ETA=6:19:43, max mem: 20.9 GB 
[11/28 06:26:28 visual_prompt]: Epoch 49 / 100: avg data time: 1.52e-01, avg batch time: 0.9744, average train loss: 1.3092
[11/28 06:27:23 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.3084, average loss: 1.1443
[11/28 06:27:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.26	
[11/28 06:27:23 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[11/28 06:29:05 visual_prompt]: 	Training 100/553. train loss: 0.7521,	0.8167 s / batch. (data: 1.19e-02). ETA=6:22:33, max mem: 20.9 GB 
[11/28 06:30:42 visual_prompt]: 	Training 200/553. train loss: 4.1074,	0.8097 s / batch. (data: 2.69e-04). ETA=6:17:55, max mem: 20.9 GB 
[11/28 06:32:17 visual_prompt]: 	Training 300/553. train loss: 0.7680,	0.8190 s / batch. (data: 2.61e-04). ETA=6:20:52, max mem: 20.9 GB 
[11/28 06:33:52 visual_prompt]: 	Training 400/553. train loss: 1.1112,	0.8233 s / batch. (data: 5.41e-03). ETA=6:21:30, max mem: 20.9 GB 
[11/28 06:35:29 visual_prompt]: 	Training 500/553. train loss: 3.2906,	0.8325 s / batch. (data: 7.57e-04). ETA=6:24:23, max mem: 20.9 GB 
[11/28 06:36:20 visual_prompt]: Epoch 50 / 100: avg data time: 1.47e-01, avg batch time: 0.9708, average train loss: 1.3716
[11/28 06:37:15 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3073, average loss: 1.5215
[11/28 06:37:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.37	
[11/28 06:37:15 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[11/28 06:38:55 visual_prompt]: 	Training 100/553. train loss: 0.7612,	0.9642 s / batch. (data: 1.47e-01). ETA=7:22:44, max mem: 20.9 GB 
[11/28 06:40:32 visual_prompt]: 	Training 200/553. train loss: 1.4728,	0.8120 s / batch. (data: 2.81e-04). ETA=6:11:29, max mem: 20.9 GB 
[11/28 06:42:09 visual_prompt]: 	Training 300/553. train loss: 1.2629,	0.8153 s / batch. (data: 5.43e-03). ETA=6:11:38, max mem: 20.9 GB 
[11/28 06:43:45 visual_prompt]: 	Training 400/553. train loss: 2.5016,	1.3406 s / batch. (data: 5.10e-01). ETA=10:08:51, max mem: 20.9 GB 
[11/28 06:45:20 visual_prompt]: 	Training 500/553. train loss: 0.6205,	0.8560 s / batch. (data: 7.86e-04). ETA=6:27:20, max mem: 20.9 GB 
[11/28 06:46:08 visual_prompt]: Epoch 51 / 100: avg data time: 1.41e-01, avg batch time: 0.9645, average train loss: 1.2642
[11/28 06:47:03 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3070, average loss: 0.7854
[11/28 06:47:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.11	
[11/28 06:47:03 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[11/28 06:48:47 visual_prompt]: 	Training 100/553. train loss: 2.8758,	0.8364 s / batch. (data: 2.78e-04). ETA=6:16:21, max mem: 20.9 GB 
[11/28 06:50:22 visual_prompt]: 	Training 200/553. train loss: 0.7447,	0.8097 s / batch. (data: 3.41e-04). ETA=6:02:57, max mem: 20.9 GB 
[11/28 06:51:59 visual_prompt]: 	Training 300/553. train loss: 1.9115,	0.8193 s / batch. (data: 2.79e-04). ETA=6:05:56, max mem: 20.9 GB 
[11/28 06:53:39 visual_prompt]: 	Training 400/553. train loss: 0.0999,	0.8255 s / batch. (data: 3.25e-04). ETA=6:07:19, max mem: 20.9 GB 
[11/28 06:55:11 visual_prompt]: 	Training 500/553. train loss: 1.4158,	0.8280 s / batch. (data: 2.88e-04). ETA=6:07:02, max mem: 20.9 GB 
[11/28 06:55:59 visual_prompt]: Epoch 52 / 100: avg data time: 1.42e-01, avg batch time: 0.9689, average train loss: 1.6700
[11/28 06:56:54 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3069, average loss: 1.0005
[11/28 06:56:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.52	
[11/28 06:56:54 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[11/28 06:58:34 visual_prompt]: 	Training 100/553. train loss: 2.1284,	0.8169 s / batch. (data: 2.88e-04). ETA=6:00:01, max mem: 20.9 GB 
[11/28 07:00:10 visual_prompt]: 	Training 200/553. train loss: 0.5101,	0.8382 s / batch. (data: 5.38e-03). ETA=6:08:01, max mem: 20.9 GB 
[11/28 07:01:45 visual_prompt]: 	Training 300/553. train loss: 0.8134,	0.8429 s / batch. (data: 2.87e-02). ETA=6:08:41, max mem: 20.9 GB 
[11/28 07:03:23 visual_prompt]: 	Training 400/553. train loss: 0.8493,	0.8266 s / batch. (data: 3.05e-04). ETA=6:00:10, max mem: 20.9 GB 
[11/28 07:04:57 visual_prompt]: 	Training 500/553. train loss: 2.9513,	0.8560 s / batch. (data: 2.99e-04). ETA=6:11:33, max mem: 20.9 GB 
[11/28 07:05:49 visual_prompt]: Epoch 53 / 100: avg data time: 1.44e-01, avg batch time: 0.9671, average train loss: 1.3348
[11/28 07:06:44 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3055, average loss: 8.3878
[11/28 07:06:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 41.33	
[11/28 07:06:44 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[11/28 07:08:27 visual_prompt]: 	Training 100/553. train loss: 0.7603,	0.8055 s / batch. (data: 2.84e-04). ETA=5:47:34, max mem: 20.9 GB 
[11/28 07:10:04 visual_prompt]: 	Training 200/553. train loss: 0.6690,	0.8065 s / batch. (data: 3.05e-04). ETA=5:46:39, max mem: 20.9 GB 
[11/28 07:11:38 visual_prompt]: 	Training 300/553. train loss: 1.4122,	0.8409 s / batch. (data: 3.08e-04). ETA=6:00:03, max mem: 20.9 GB 
[11/28 07:13:14 visual_prompt]: 	Training 400/553. train loss: 2.2229,	0.8386 s / batch. (data: 1.06e-02). ETA=5:57:40, max mem: 20.9 GB 
[11/28 07:14:50 visual_prompt]: 	Training 500/553. train loss: 0.5575,	0.8413 s / batch. (data: 7.57e-04). ETA=5:57:26, max mem: 20.9 GB 
[11/28 07:15:40 visual_prompt]: Epoch 54 / 100: avg data time: 1.45e-01, avg batch time: 0.9700, average train loss: 1.5471
[11/28 07:16:35 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3067, average loss: 3.9482
[11/28 07:16:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.72	
[11/28 07:16:35 visual_prompt]: Stopping early.
[11/28 07:16:35 visual_prompt]: Rank of current process: 0. World size: 1
[11/28 07:16:35 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 07:16:35 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/28 07:16:35 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/28 07:16:35 visual_prompt]: Training with config:
[11/28 07:16:35 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/28 07:16:35 visual_prompt]: Loading training data...
[11/28 07:16:35 visual_prompt]: Constructing mammo-cbis dataset train...
[11/28 07:16:35 visual_prompt]: Loading validation data...
[11/28 07:16:35 visual_prompt]: Constructing mammo-cbis dataset val...
[11/28 07:16:35 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/28 07:16:38 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/28 07:16:38 visual_prompt]: tuned percent:0.525
[11/28 07:16:38 visual_prompt]: Device used for model: 0
[11/28 07:16:38 visual_prompt]: Setting up Evaluator...
[11/28 07:16:38 visual_prompt]: Setting up Trainer...
[11/28 07:16:38 visual_prompt]: 	Setting up the optimizer...
[11/28 07:16:38 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/28 07:18:17 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8390 s / batch. (data: 5.41e-03). ETA=12:51:50, max mem: 20.9 GB 
[11/28 07:19:52 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8280 s / batch. (data: 2.68e-04). ETA=12:40:23, max mem: 20.9 GB 
[11/28 07:21:30 visual_prompt]: 	Training 300/553. train loss: 1.3905,	0.8280 s / batch. (data: 4.07e-04). ETA=12:38:59, max mem: 20.9 GB 
[11/28 07:23:05 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8232 s / batch. (data: 2.77e-04). ETA=12:33:11, max mem: 20.9 GB 
[11/28 07:24:43 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8214 s / batch. (data: 1.17e-02). ETA=12:30:12, max mem: 20.9 GB 
[11/28 07:25:34 visual_prompt]: Epoch 1 / 100: avg data time: 1.45e-01, avg batch time: 0.9684, average train loss: 1.5403
[11/28 07:26:29 visual_prompt]: Inference (val):avg data time: 5.14e-05, avg batch time: 0.3132, average loss: 1.5201
[11/28 07:26:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/28 07:26:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/28 07:28:08 visual_prompt]: 	Training 100/553. train loss: 0.7444,	1.0746 s / batch. (data: 2.48e-01). ETA=16:18:41, max mem: 20.9 GB 
[11/28 07:29:44 visual_prompt]: 	Training 200/553. train loss: 0.0235,	0.8446 s / batch. (data: 2.68e-02). ETA=12:47:49, max mem: 20.9 GB 
[11/28 07:31:21 visual_prompt]: 	Training 300/553. train loss: 0.7303,	0.9432 s / batch. (data: 1.35e-01). ETA=14:15:55, max mem: 20.9 GB 
[11/28 07:32:56 visual_prompt]: 	Training 400/553. train loss: 1.0344,	0.8146 s / batch. (data: 2.69e-04). ETA=12:17:48, max mem: 20.9 GB 
[11/28 07:34:33 visual_prompt]: 	Training 500/553. train loss: 0.6514,	0.8602 s / batch. (data: 1.40e-02). ETA=12:57:45, max mem: 20.9 GB 
[11/28 07:35:23 visual_prompt]: Epoch 2 / 100: avg data time: 1.42e-01, avg batch time: 0.9652, average train loss: 0.9801
[11/28 07:36:17 visual_prompt]: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3069, average loss: 1.2716
[11/28 07:36:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.56	
[11/28 07:36:17 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/28 07:37:56 visual_prompt]: 	Training 100/553. train loss: 1.4377,	0.8557 s / batch. (data: 1.05e-02). ETA=12:51:27, max mem: 20.9 GB 
[11/28 07:39:33 visual_prompt]: 	Training 200/553. train loss: 0.8695,	0.8360 s / batch. (data: 7.94e-03). ETA=12:32:19, max mem: 20.9 GB 
[11/28 07:41:08 visual_prompt]: 	Training 300/553. train loss: 0.6381,	0.8256 s / batch. (data: 2.93e-04). ETA=12:21:33, max mem: 20.9 GB 
[11/28 07:42:46 visual_prompt]: 	Training 400/553. train loss: 3.7800,	0.8095 s / batch. (data: 2.90e-04). ETA=12:05:44, max mem: 20.9 GB 
[11/28 07:44:23 visual_prompt]: 	Training 500/553. train loss: 0.7114,	1.1942 s / batch. (data: 3.73e-01). ETA=17:48:42, max mem: 20.9 GB 
[11/28 07:45:12 visual_prompt]: Epoch 3 / 100: avg data time: 1.42e-01, avg batch time: 0.9674, average train loss: 1.0415
[11/28 07:46:08 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3060, average loss: 0.7156
[11/28 07:46:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 44.31	rocauc: 59.24	
[11/28 07:46:08 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/28 07:47:50 visual_prompt]: 	Training 100/553. train loss: 0.7622,	0.8172 s / batch. (data: 2.89e-04). ETA=12:09:12, max mem: 20.9 GB 
[11/28 07:49:28 visual_prompt]: 	Training 200/553. train loss: 0.5349,	0.8320 s / batch. (data: 3.16e-04). ETA=12:21:03, max mem: 20.9 GB 
[11/28 07:51:05 visual_prompt]: 	Training 300/553. train loss: 0.5132,	1.4780 s / batch. (data: 6.58e-01). ETA=21:53:57, max mem: 20.9 GB 
[11/28 07:52:37 visual_prompt]: 	Training 400/553. train loss: 1.0355,	1.1134 s / batch. (data: 2.94e-01). ETA=16:27:56, max mem: 20.9 GB 
[11/28 07:54:17 visual_prompt]: 	Training 500/553. train loss: 0.2275,	3.3993 s / batch. (data: 2.59e+00). ETA=2 days, 2:10:44, max mem: 20.9 GB 
[11/28 07:55:09 visual_prompt]: Epoch 4 / 100: avg data time: 1.54e-01, avg batch time: 0.9774, average train loss: 1.0871
[11/28 07:56:04 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3055, average loss: 1.6708
[11/28 07:56:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.13	
[11/28 07:56:04 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/28 07:57:43 visual_prompt]: 	Training 100/553. train loss: 2.6226,	0.8198 s / batch. (data: 1.34e-02). ETA=12:04:00, max mem: 20.9 GB 
[11/28 07:59:19 visual_prompt]: 	Training 200/553. train loss: 1.5608,	0.9297 s / batch. (data: 1.12e-01). ETA=13:39:30, max mem: 20.9 GB 
[11/28 08:00:57 visual_prompt]: 	Training 300/553. train loss: 1.2797,	0.8520 s / batch. (data: 2.87e-04). ETA=12:29:34, max mem: 20.9 GB 
[11/28 08:02:32 visual_prompt]: 	Training 400/553. train loss: 2.5061,	0.8280 s / batch. (data: 2.83e-04). ETA=12:07:04, max mem: 20.9 GB 
[11/28 08:04:08 visual_prompt]: 	Training 500/553. train loss: 1.0235,	0.8199 s / batch. (data: 2.72e-04). ETA=11:58:38, max mem: 20.9 GB 
[11/28 08:04:58 visual_prompt]: Epoch 5 / 100: avg data time: 1.41e-01, avg batch time: 0.9659, average train loss: 1.4804
[11/28 08:05:53 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3065, average loss: 3.6272
[11/28 08:05:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.28	
[11/28 08:05:53 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/28 08:07:34 visual_prompt]: 	Training 100/553. train loss: 0.5887,	0.8311 s / batch. (data: 5.39e-03). ETA=12:06:19, max mem: 20.9 GB 
[11/28 08:09:09 visual_prompt]: 	Training 200/553. train loss: 3.9305,	0.8324 s / batch. (data: 1.05e-02). ETA=12:06:04, max mem: 20.9 GB 
[11/28 08:10:44 visual_prompt]: 	Training 300/553. train loss: 1.8119,	0.8219 s / batch. (data: 3.00e-04). ETA=11:55:31, max mem: 20.9 GB 
[11/28 08:12:24 visual_prompt]: 	Training 400/553. train loss: 1.6268,	0.8571 s / batch. (data: 1.30e-02). ETA=12:24:42, max mem: 20.9 GB 
[11/28 08:13:59 visual_prompt]: 	Training 500/553. train loss: 3.0176,	0.8393 s / batch. (data: 3.11e-04). ETA=12:07:53, max mem: 20.9 GB 
[11/28 08:14:49 visual_prompt]: Epoch 6 / 100: avg data time: 1.46e-01, avg batch time: 0.9689, average train loss: 1.3675
[11/28 08:15:43 visual_prompt]: Inference (val):avg data time: 6.77e-04, avg batch time: 0.3067, average loss: 0.9233
[11/28 08:15:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.50	
[11/28 08:15:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/28 08:17:21 visual_prompt]: 	Training 100/553. train loss: 1.1389,	0.8383 s / batch. (data: 3.14e-04). ETA=12:04:53, max mem: 20.9 GB 
[11/28 08:18:59 visual_prompt]: 	Training 200/553. train loss: 0.5374,	0.8206 s / batch. (data: 2.83e-04). ETA=11:48:14, max mem: 20.9 GB 
[11/28 08:20:38 visual_prompt]: 	Training 300/553. train loss: 0.6987,	1.7160 s / batch. (data: 8.75e-01). ETA=1 day, 0:38:08, max mem: 20.9 GB 
[11/28 08:22:15 visual_prompt]: 	Training 400/553. train loss: 0.8380,	1.9004 s / batch. (data: 1.08e+00). ETA=1 day, 3:13:44, max mem: 20.9 GB 
[11/28 08:23:50 visual_prompt]: 	Training 500/553. train loss: 1.3916,	0.8509 s / batch. (data: 1.57e-02). ETA=12:10:06, max mem: 20.9 GB 
[11/28 08:24:38 visual_prompt]: Epoch 7 / 100: avg data time: 1.43e-01, avg batch time: 0.9669, average train loss: 1.1268
[11/28 08:25:33 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3064, average loss: 0.6818
[11/28 08:25:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 61.98	
[11/28 08:25:33 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/28 08:27:11 visual_prompt]: 	Training 100/553. train loss: 2.6913,	0.8210 s / batch. (data: 2.96e-04). ETA=11:42:22, max mem: 20.9 GB 
[11/28 08:28:49 visual_prompt]: 	Training 200/553. train loss: 1.1695,	0.8160 s / batch. (data: 3.19e-04). ETA=11:36:42, max mem: 20.9 GB 
[11/28 08:30:25 visual_prompt]: 	Training 300/553. train loss: 0.5651,	0.8256 s / batch. (data: 2.69e-04). ETA=11:43:31, max mem: 20.9 GB 
[11/28 08:32:02 visual_prompt]: 	Training 400/553. train loss: 0.8290,	0.8367 s / batch. (data: 2.29e-02). ETA=11:51:37, max mem: 20.9 GB 
[11/28 08:33:38 visual_prompt]: 	Training 500/553. train loss: 1.5774,	1.2751 s / batch. (data: 4.44e-01). ETA=18:02:19, max mem: 20.9 GB 
[11/28 08:34:29 visual_prompt]: Epoch 8 / 100: avg data time: 1.45e-01, avg batch time: 0.9692, average train loss: 1.4651
[11/28 08:35:24 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3067, average loss: 1.0596
[11/28 08:35:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.58	
[11/28 08:35:24 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/28 08:37:03 visual_prompt]: 	Training 100/553. train loss: 0.0075,	0.8196 s / batch. (data: 3.08e-04). ETA=11:33:37, max mem: 20.9 GB 
[11/28 08:38:38 visual_prompt]: 	Training 200/553. train loss: 0.5798,	0.8220 s / batch. (data: 5.41e-03). ETA=11:34:16, max mem: 20.9 GB 
[11/28 08:40:14 visual_prompt]: 	Training 300/553. train loss: 0.9115,	1.5800 s / batch. (data: 7.42e-01). ETA=22:11:52, max mem: 20.9 GB 
[11/28 08:41:51 visual_prompt]: 	Training 400/553. train loss: 1.0908,	0.8163 s / batch. (data: 4.80e-04). ETA=11:26:41, max mem: 20.9 GB 
[11/28 08:43:28 visual_prompt]: 	Training 500/553. train loss: 0.8527,	0.9577 s / batch. (data: 1.39e-01). ETA=13:24:05, max mem: 20.9 GB 
[11/28 08:44:17 visual_prompt]: Epoch 9 / 100: avg data time: 1.42e-01, avg batch time: 0.9646, average train loss: 1.1514
[11/28 08:45:12 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.3056, average loss: 0.8764
[11/28 08:45:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.72	
[11/28 08:45:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/28 08:46:53 visual_prompt]: 	Training 100/553. train loss: 2.6237,	0.8438 s / batch. (data: 3.04e-04). ETA=11:46:15, max mem: 20.9 GB 
[11/28 08:48:29 visual_prompt]: 	Training 200/553. train loss: 1.0801,	0.8480 s / batch. (data: 1.19e-02). ETA=11:48:22, max mem: 20.9 GB 
[11/28 08:50:04 visual_prompt]: 	Training 300/553. train loss: 0.8928,	1.1961 s / batch. (data: 3.89e-01). ETA=16:37:12, max mem: 20.9 GB 
[11/28 08:51:40 visual_prompt]: 	Training 400/553. train loss: 3.2987,	0.8204 s / batch. (data: 2.95e-04). ETA=11:22:37, max mem: 20.9 GB 
[11/28 08:53:17 visual_prompt]: 	Training 500/553. train loss: 0.9837,	0.8146 s / batch. (data: 3.47e-04). ETA=11:16:27, max mem: 20.9 GB 
[11/28 08:54:08 visual_prompt]: Epoch 10 / 100: avg data time: 1.47e-01, avg batch time: 0.9701, average train loss: 1.7488
[11/28 08:55:03 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.3053, average loss: 0.7217
[11/28 08:55:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.37	
[11/28 08:55:03 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/28 08:56:45 visual_prompt]: 	Training 100/553. train loss: 0.7583,	0.8280 s / batch. (data: 2.81e-04). ETA=11:25:27, max mem: 20.9 GB 
[11/28 08:58:23 visual_prompt]: 	Training 200/553. train loss: 0.4333,	0.8048 s / batch. (data: 3.12e-04). ETA=11:04:51, max mem: 20.9 GB 
[11/28 08:59:59 visual_prompt]: 	Training 300/553. train loss: 0.0232,	2.0623 s / batch. (data: 1.18e+00). ETA=1 day, 4:20:21, max mem: 20.9 GB 
[11/28 09:01:34 visual_prompt]: 	Training 400/553. train loss: 0.6679,	0.8059 s / batch. (data: 2.88e-04). ETA=11:03:08, max mem: 20.9 GB 
[11/28 09:03:09 visual_prompt]: 	Training 500/553. train loss: 0.6420,	0.8720 s / batch. (data: 2.93e-04). ETA=11:56:03, max mem: 20.9 GB 
[11/28 09:03:59 visual_prompt]: Epoch 11 / 100: avg data time: 1.46e-01, avg batch time: 0.9685, average train loss: 1.3496
[11/28 09:04:54 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3070, average loss: 2.7328
[11/28 09:04:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.00	
[11/28 09:04:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/28 09:06:36 visual_prompt]: 	Training 100/553. train loss: 0.7005,	0.8797 s / batch. (data: 6.69e-02). ETA=12:00:08, max mem: 20.9 GB 
[11/28 09:08:12 visual_prompt]: 	Training 200/553. train loss: 0.5731,	0.8650 s / batch. (data: 3.29e-02). ETA=11:46:37, max mem: 20.9 GB 
[11/28 09:09:48 visual_prompt]: 	Training 300/553. train loss: 0.7164,	0.8308 s / batch. (data: 1.57e-02). ETA=11:17:20, max mem: 20.9 GB 
[11/28 09:11:25 visual_prompt]: 	Training 400/553. train loss: 1.8411,	0.8587 s / batch. (data: 1.46e-02). ETA=11:38:37, max mem: 20.9 GB 
[11/28 09:13:00 visual_prompt]: 	Training 500/553. train loss: 7.3812,	0.8093 s / batch. (data: 2.60e-04). ETA=10:57:08, max mem: 20.9 GB 
[11/28 09:13:49 visual_prompt]: Epoch 12 / 100: avg data time: 1.44e-01, avg batch time: 0.9670, average train loss: 1.7641
[11/28 09:14:44 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.3067, average loss: 5.1254
[11/28 09:14:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.50	
[11/28 09:14:44 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/28 09:16:26 visual_prompt]: 	Training 100/553. train loss: 1.0007,	0.8428 s / batch. (data: 6.74e-03). ETA=11:22:08, max mem: 20.9 GB 
[11/28 09:17:59 visual_prompt]: 	Training 200/553. train loss: 0.7884,	0.8123 s / batch. (data: 5.43e-03). ETA=10:56:06, max mem: 20.9 GB 
[11/28 09:19:35 visual_prompt]: 	Training 300/553. train loss: 1.5693,	1.5180 s / batch. (data: 6.90e-01). ETA=20:23:38, max mem: 20.9 GB 
[11/28 09:21:10 visual_prompt]: 	Training 400/553. train loss: 2.8366,	0.8280 s / batch. (data: 4.09e-04). ETA=11:06:02, max mem: 20.9 GB 
[11/28 09:22:47 visual_prompt]: 	Training 500/553. train loss: 3.0108,	0.8139 s / batch. (data: 2.93e-04). ETA=10:53:19, max mem: 20.9 GB 
[11/28 09:23:38 visual_prompt]: Epoch 13 / 100: avg data time: 1.41e-01, avg batch time: 0.9644, average train loss: 1.9001
[11/28 09:24:32 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3062, average loss: 1.2067
[11/28 09:24:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.46	
[11/28 09:24:32 visual_prompt]: Best epoch 13: best metric: -1.207
[11/28 09:24:32 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/28 09:26:13 visual_prompt]: 	Training 100/553. train loss: 1.5523,	0.8430 s / batch. (data: 5.41e-03). ETA=11:14:34, max mem: 20.9 GB 
[11/28 09:27:48 visual_prompt]: 	Training 200/553. train loss: 0.0387,	0.8840 s / batch. (data: 4.44e-02). ETA=11:45:55, max mem: 20.9 GB 
[11/28 09:29:25 visual_prompt]: 	Training 300/553. train loss: 0.9315,	0.8280 s / batch. (data: 5.43e-03). ETA=10:59:47, max mem: 20.9 GB 
[11/28 09:31:01 visual_prompt]: 	Training 400/553. train loss: 0.7454,	0.8280 s / batch. (data: 2.93e-04). ETA=10:58:23, max mem: 20.9 GB 
[11/28 09:32:37 visual_prompt]: 	Training 500/553. train loss: 1.9779,	0.8169 s / batch. (data: 2.94e-04). ETA=10:48:11, max mem: 20.9 GB 
[11/28 09:33:26 visual_prompt]: Epoch 14 / 100: avg data time: 1.43e-01, avg batch time: 0.9655, average train loss: 1.2273
[11/28 09:34:22 visual_prompt]: Inference (val):avg data time: 4.93e-05, avg batch time: 0.3139, average loss: 0.7427
[11/28 09:34:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.20	
[11/28 09:34:22 visual_prompt]: Best epoch 14: best metric: -0.743
[11/28 09:34:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/28 09:36:02 visual_prompt]: 	Training 100/553. train loss: 1.0834,	0.8240 s / batch. (data: 3.06e-04). ETA=10:51:44, max mem: 20.9 GB 
[11/28 09:37:38 visual_prompt]: 	Training 200/553. train loss: 4.6378,	0.8600 s / batch. (data: 3.07e-04). ETA=11:18:48, max mem: 20.9 GB 
[11/28 09:39:17 visual_prompt]: 	Training 300/553. train loss: 4.5705,	0.8592 s / batch. (data: 6.83e-04). ETA=11:16:45, max mem: 20.9 GB 
[11/28 09:40:51 visual_prompt]: 	Training 400/553. train loss: 0.6600,	0.8452 s / batch. (data: 3.48e-02). ETA=11:04:17, max mem: 20.9 GB 
[11/28 09:42:29 visual_prompt]: 	Training 500/553. train loss: 1.6384,	0.8560 s / batch. (data: 3.12e-04). ETA=11:11:22, max mem: 20.9 GB 
[11/28 09:43:20 visual_prompt]: Epoch 15 / 100: avg data time: 1.48e-01, avg batch time: 0.9731, average train loss: 2.1151
[11/28 09:44:15 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3073, average loss: 1.9401
[11/28 09:44:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.86	
[11/28 09:44:15 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/28 09:45:55 visual_prompt]: 	Training 100/553. train loss: 0.5644,	0.8254 s / batch. (data: 2.81e-04). ETA=10:45:15, max mem: 20.9 GB 
[11/28 09:47:34 visual_prompt]: 	Training 200/553. train loss: 2.3663,	0.8185 s / batch. (data: 2.87e-04). ETA=10:38:31, max mem: 20.9 GB 
[11/28 09:49:11 visual_prompt]: 	Training 300/553. train loss: 1.1997,	0.8320 s / batch. (data: 3.35e-04). ETA=10:47:37, max mem: 20.9 GB 
[11/28 09:50:48 visual_prompt]: 	Training 400/553. train loss: 1.6563,	0.8542 s / batch. (data: 5.92e-03). ETA=11:03:32, max mem: 20.9 GB 
[11/28 09:52:24 visual_prompt]: 	Training 500/553. train loss: 0.5822,	1.4328 s / batch. (data: 6.16e-01). ETA=18:30:30, max mem: 20.9 GB 
[11/28 09:53:15 visual_prompt]: Epoch 16 / 100: avg data time: 1.50e-01, avg batch time: 0.9756, average train loss: 1.4400
[11/28 09:54:10 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3058, average loss: 0.7450
[11/28 09:54:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.94	
[11/28 09:54:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/28 09:55:51 visual_prompt]: 	Training 100/553. train loss: 1.6136,	0.8280 s / batch. (data: 5.43e-03). ETA=10:39:39, max mem: 20.9 GB 
[11/28 09:57:28 visual_prompt]: 	Training 200/553. train loss: 3.2109,	0.8440 s / batch. (data: 1.20e-02). ETA=10:50:37, max mem: 20.9 GB 
[11/28 09:59:04 visual_prompt]: 	Training 300/553. train loss: 2.5880,	0.8104 s / batch. (data: 5.41e-03). ETA=10:23:22, max mem: 20.9 GB 
[11/28 10:00:40 visual_prompt]: 	Training 400/553. train loss: 0.9729,	1.0935 s / batch. (data: 2.62e-01). ETA=13:59:16, max mem: 20.9 GB 
[11/28 10:02:17 visual_prompt]: 	Training 500/553. train loss: 1.1869,	1.3975 s / batch. (data: 5.93e-01). ETA=17:50:17, max mem: 20.9 GB 
[11/28 10:03:09 visual_prompt]: Epoch 17 / 100: avg data time: 1.52e-01, avg batch time: 0.9741, average train loss: 1.4579
[11/28 10:04:04 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3060, average loss: 1.4424
[11/28 10:04:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 65.36	
[11/28 10:04:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/28 10:05:45 visual_prompt]: 	Training 100/553. train loss: 0.6193,	0.8145 s / batch. (data: 7.95e-03). ETA=10:21:44, max mem: 20.9 GB 
[11/28 10:07:24 visual_prompt]: 	Training 200/553. train loss: 1.5820,	0.8092 s / batch. (data: 3.08e-04). ETA=10:16:18, max mem: 20.9 GB 
[11/28 10:09:00 visual_prompt]: 	Training 300/553. train loss: 0.5427,	0.8240 s / batch. (data: 2.93e-04). ETA=10:26:13, max mem: 20.9 GB 
[11/28 10:10:36 visual_prompt]: 	Training 400/553. train loss: 1.4705,	0.8258 s / batch. (data: 2.81e-04). ETA=10:26:11, max mem: 20.9 GB 
[11/28 10:12:12 visual_prompt]: 	Training 500/553. train loss: 0.7507,	0.8120 s / batch. (data: 2.96e-04). ETA=10:14:23, max mem: 20.9 GB 
[11/28 10:13:03 visual_prompt]: Epoch 18 / 100: avg data time: 1.50e-01, avg batch time: 0.9732, average train loss: 1.3950
[11/28 10:13:58 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3059, average loss: 0.8686
[11/28 10:13:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.83	
[11/28 10:13:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[11/28 10:15:38 visual_prompt]: 	Training 100/553. train loss: 0.5782,	0.8412 s / batch. (data: 9.12e-03). ETA=10:34:20, max mem: 20.9 GB 
[11/28 10:17:15 visual_prompt]: 	Training 200/553. train loss: 0.7036,	0.8259 s / batch. (data: 4.14e-03). ETA=10:21:26, max mem: 20.9 GB 
[11/28 10:18:51 visual_prompt]: 	Training 300/553. train loss: 3.1288,	0.8480 s / batch. (data: 2.86e-04). ETA=10:36:38, max mem: 20.9 GB 
[11/28 10:20:29 visual_prompt]: 	Training 400/553. train loss: 0.5579,	0.8208 s / batch. (data: 6.64e-04). ETA=10:14:49, max mem: 20.9 GB 
[11/28 10:22:01 visual_prompt]: 	Training 500/553. train loss: 0.6502,	0.8440 s / batch. (data: 5.41e-03). ETA=10:30:50, max mem: 20.9 GB 
[11/28 10:22:51 visual_prompt]: Epoch 19 / 100: avg data time: 1.40e-01, avg batch time: 0.9634, average train loss: 1.2007
[11/28 10:23:45 visual_prompt]: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3078, average loss: 2.6943
[11/28 10:23:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.90	
[11/28 10:23:45 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[11/28 10:25:24 visual_prompt]: 	Training 100/553. train loss: 0.5632,	0.8159 s / batch. (data: 4.58e-04). ETA=10:07:46, max mem: 20.9 GB 
[11/28 10:27:01 visual_prompt]: 	Training 200/553. train loss: 0.8049,	0.8711 s / batch. (data: 9.49e-03). ETA=10:47:26, max mem: 20.9 GB 
[11/28 10:28:37 visual_prompt]: 	Training 300/553. train loss: 3.2648,	0.8220 s / batch. (data: 8.10e-04). ETA=10:09:35, max mem: 20.9 GB 
[11/28 10:30:12 visual_prompt]: 	Training 400/553. train loss: 0.8737,	0.8260 s / batch. (data: 5.45e-03). ETA=10:11:08, max mem: 20.9 GB 
[11/28 10:31:47 visual_prompt]: 	Training 500/553. train loss: 1.3796,	0.8138 s / batch. (data: 2.84e-04). ETA=10:00:47, max mem: 20.9 GB 
[11/28 10:32:39 visual_prompt]: Epoch 20 / 100: avg data time: 1.41e-01, avg batch time: 0.9648, average train loss: 1.4975
[11/28 10:33:34 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3132, average loss: 0.8022
[11/28 10:33:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.56	
[11/28 10:33:34 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[11/28 10:35:16 visual_prompt]: 	Training 100/553. train loss: 1.7491,	0.8240 s / batch. (data: 1.69e-03). ETA=10:06:10, max mem: 20.9 GB 
[11/28 10:36:51 visual_prompt]: 	Training 200/553. train loss: 1.3258,	0.8180 s / batch. (data: 2.91e-04). ETA=10:00:26, max mem: 20.9 GB 
[11/28 10:38:27 visual_prompt]: 	Training 300/553. train loss: 1.9252,	0.8951 s / batch. (data: 7.42e-02). ETA=10:55:29, max mem: 20.9 GB 
[11/28 10:40:03 visual_prompt]: 	Training 400/553. train loss: 1.4301,	0.8120 s / batch. (data: 2.91e-04). ETA=9:53:18, max mem: 20.9 GB 
[11/28 10:41:40 visual_prompt]: 	Training 500/553. train loss: 0.6992,	0.8241 s / batch. (data: 7.91e-03). ETA=10:00:45, max mem: 20.9 GB 
[11/28 10:42:29 visual_prompt]: Epoch 21 / 100: avg data time: 1.45e-01, avg batch time: 0.9673, average train loss: 1.3935
[11/28 10:43:24 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.3084, average loss: 1.4628
[11/28 10:43:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.11	
[11/28 10:43:24 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[11/28 10:45:04 visual_prompt]: 	Training 100/553. train loss: 0.7020,	0.8217 s / batch. (data: 1.19e-02). ETA=9:56:55, max mem: 20.9 GB 
[11/28 10:46:40 visual_prompt]: 	Training 200/553. train loss: 0.6173,	0.8187 s / batch. (data: 2.82e-04). ETA=9:53:21, max mem: 20.9 GB 
[11/28 10:48:14 visual_prompt]: 	Training 300/553. train loss: 0.0023,	0.8173 s / batch. (data: 2.78e-04). ETA=9:50:58, max mem: 20.9 GB 
[11/28 10:49:51 visual_prompt]: 	Training 400/553. train loss: 0.7821,	0.8280 s / batch. (data: 7.99e-03). ETA=9:57:23, max mem: 20.9 GB 
[11/28 10:51:28 visual_prompt]: 	Training 500/553. train loss: 0.5836,	0.8144 s / batch. (data: 5.41e-03). ETA=9:46:12, max mem: 20.9 GB 
[11/28 10:52:20 visual_prompt]: Epoch 22 / 100: avg data time: 1.46e-01, avg batch time: 0.9686, average train loss: 1.7404
[11/28 10:53:15 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3053, average loss: 0.6882
[11/28 10:53:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.23	
[11/28 10:53:15 visual_prompt]: Best epoch 22: best metric: -0.688
[11/28 10:53:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[11/28 10:54:56 visual_prompt]: 	Training 100/553. train loss: 0.7659,	0.8386 s / batch. (data: 2.66e-02). ETA=10:01:29, max mem: 20.9 GB 
[11/28 10:56:32 visual_prompt]: 	Training 200/553. train loss: 1.9202,	0.8209 s / batch. (data: 5.45e-03). ETA=9:47:24, max mem: 20.9 GB 
[11/28 10:58:10 visual_prompt]: 	Training 300/553. train loss: 0.6428,	0.8379 s / batch. (data: 1.10e-02). ETA=9:58:11, max mem: 20.9 GB 
[11/28 10:59:43 visual_prompt]: 	Training 400/553. train loss: 0.9562,	0.8472 s / batch. (data: 1.05e-02). ETA=10:03:22, max mem: 20.9 GB 
[11/28 11:01:18 visual_prompt]: 	Training 500/553. train loss: 0.7178,	0.8324 s / batch. (data: 3.09e-04). ETA=9:51:28, max mem: 20.9 GB 
[11/28 11:02:08 visual_prompt]: Epoch 23 / 100: avg data time: 1.40e-01, avg batch time: 0.9634, average train loss: 1.2195
[11/28 11:03:03 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3086, average loss: 0.7635
[11/28 11:03:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.16	
[11/28 11:03:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[11/28 11:04:41 visual_prompt]: 	Training 100/553. train loss: 3.1249,	0.8249 s / batch. (data: 2.99e-04). ETA=9:44:02, max mem: 20.9 GB 
[11/28 11:06:18 visual_prompt]: 	Training 200/553. train loss: 0.9423,	0.8600 s / batch. (data: 2.78e-04). ETA=10:07:28, max mem: 20.9 GB 
[11/28 11:07:55 visual_prompt]: 	Training 300/553. train loss: 0.7538,	0.8903 s / batch. (data: 4.06e-02). ETA=10:27:21, max mem: 20.9 GB 
[11/28 11:09:31 visual_prompt]: 	Training 400/553. train loss: 1.4100,	0.8360 s / batch. (data: 2.97e-04). ETA=9:47:43, max mem: 20.9 GB 
[11/28 11:11:10 visual_prompt]: 	Training 500/553. train loss: 1.5143,	0.8240 s / batch. (data: 3.14e-04). ETA=9:37:54, max mem: 20.9 GB 
[11/28 11:12:02 visual_prompt]: Epoch 24 / 100: avg data time: 1.50e-01, avg batch time: 0.9746, average train loss: 1.4541
[11/28 11:12:57 visual_prompt]: Inference (val):avg data time: 2.41e-04, avg batch time: 0.3091, average loss: 2.3475
[11/28 11:12:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.11	
[11/28 11:12:57 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[11/28 11:14:40 visual_prompt]: 	Training 100/553. train loss: 1.6118,	0.8183 s / batch. (data: 3.02e-04). ETA=9:31:49, max mem: 20.9 GB 
[11/28 11:16:14 visual_prompt]: 	Training 200/553. train loss: 1.5407,	0.8218 s / batch. (data: 2.99e-04). ETA=9:32:56, max mem: 20.9 GB 
[11/28 11:17:54 visual_prompt]: 	Training 300/553. train loss: 2.3506,	0.8060 s / batch. (data: 4.41e-04). ETA=9:20:31, max mem: 20.9 GB 
[11/28 11:19:30 visual_prompt]: 	Training 400/553. train loss: 0.6104,	1.0230 s / batch. (data: 1.95e-01). ETA=11:49:44, max mem: 20.9 GB 
[11/28 11:21:06 visual_prompt]: 	Training 500/553. train loss: 0.6991,	0.8206 s / batch. (data: 2.82e-04). ETA=9:27:56, max mem: 20.9 GB 
[11/28 11:21:58 visual_prompt]: Epoch 25 / 100: avg data time: 1.54e-01, avg batch time: 0.9771, average train loss: 1.9322
[11/28 11:22:53 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3061, average loss: 4.2951
[11/28 11:22:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.22	
[11/28 11:22:53 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[11/28 11:24:34 visual_prompt]: 	Training 100/553. train loss: 1.3717,	0.8200 s / batch. (data: 3.85e-04). ETA=9:25:27, max mem: 20.9 GB 
[11/28 11:26:12 visual_prompt]: 	Training 200/553. train loss: 4.0646,	1.4910 s / batch. (data: 6.86e-01). ETA=17:05:43, max mem: 20.9 GB 
[11/28 11:27:51 visual_prompt]: 	Training 300/553. train loss: 0.0216,	0.8317 s / batch. (data: 5.92e-03). ETA=9:30:45, max mem: 20.9 GB 
[11/28 11:29:28 visual_prompt]: 	Training 400/553. train loss: 2.7905,	0.8430 s / batch. (data: 5.38e-03). ETA=9:37:04, max mem: 20.9 GB 
[11/28 11:31:03 visual_prompt]: 	Training 500/553. train loss: 1.7160,	0.8400 s / batch. (data: 7.96e-03). ETA=9:33:38, max mem: 20.9 GB 
[11/28 11:31:53 visual_prompt]: Epoch 26 / 100: avg data time: 1.49e-01, avg batch time: 0.9772, average train loss: 1.4642
[11/28 11:32:48 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3056, average loss: 0.6935
[11/28 11:32:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 58.71	
[11/28 11:32:48 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[11/28 11:34:31 visual_prompt]: 	Training 100/553. train loss: 1.0322,	1.6479 s / batch. (data: 2.97e-02). ETA=18:41:11, max mem: 20.9 GB 
[11/28 11:36:06 visual_prompt]: 	Training 200/553. train loss: 4.8499,	0.8171 s / batch. (data: 2.69e-04). ETA=9:14:32, max mem: 20.9 GB 
[11/28 11:37:43 visual_prompt]: 	Training 300/553. train loss: 0.8878,	0.8179 s / batch. (data: 3.14e-04). ETA=9:13:46, max mem: 20.9 GB 
[11/28 11:39:21 visual_prompt]: 	Training 400/553. train loss: 0.6557,	0.8310 s / batch. (data: 8.69e-04). ETA=9:21:13, max mem: 20.9 GB 
[11/28 11:40:57 visual_prompt]: 	Training 500/553. train loss: 0.9570,	0.8400 s / batch. (data: 7.70e-04). ETA=9:25:54, max mem: 20.9 GB 
[11/28 11:41:46 visual_prompt]: Epoch 27 / 100: avg data time: 1.46e-01, avg batch time: 0.9728, average train loss: 1.2158
[11/28 11:42:41 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.3059, average loss: 0.6990
[11/28 11:42:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.84	
[11/28 11:42:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[11/28 11:44:20 visual_prompt]: 	Training 100/553. train loss: 1.4399,	0.8395 s / batch. (data: 1.38e-02). ETA=9:23:26, max mem: 20.9 GB 
[11/28 11:45:57 visual_prompt]: 	Training 200/553. train loss: 4.6834,	0.8516 s / batch. (data: 2.35e-02). ETA=9:30:06, max mem: 20.9 GB 
[11/28 11:47:32 visual_prompt]: 	Training 300/553. train loss: 0.4759,	1.3280 s / batch. (data: 5.11e-01). ETA=14:46:51, max mem: 20.9 GB 
[11/28 11:49:07 visual_prompt]: 	Training 400/553. train loss: 0.7004,	0.8233 s / batch. (data: 7.94e-03). ETA=9:08:26, max mem: 20.9 GB 
[11/28 11:50:41 visual_prompt]: 	Training 500/553. train loss: 2.9759,	0.8419 s / batch. (data: 2.75e-04). ETA=9:19:25, max mem: 20.9 GB 
[11/28 11:51:32 visual_prompt]: Epoch 28 / 100: avg data time: 1.38e-01, avg batch time: 0.9603, average train loss: 1.2730
[11/28 11:52:27 visual_prompt]: Inference (val):avg data time: 2.79e-04, avg batch time: 0.3065, average loss: 0.6871
[11/28 11:52:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.59	
[11/28 11:52:27 visual_prompt]: Best epoch 28: best metric: -0.687
[11/28 11:52:27 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[11/28 11:54:13 visual_prompt]: 	Training 100/553. train loss: 1.8330,	0.8200 s / batch. (data: 2.67e-04). ETA=9:02:49, max mem: 20.9 GB 
[11/28 11:55:49 visual_prompt]: 	Training 200/553. train loss: 0.7838,	1.6895 s / batch. (data: 8.76e-01). ETA=18:35:32, max mem: 20.9 GB 
[11/28 11:57:22 visual_prompt]: 	Training 300/553. train loss: 0.8803,	0.8240 s / batch. (data: 3.17e-04). ETA=9:02:41, max mem: 20.9 GB 
[11/28 11:58:59 visual_prompt]: 	Training 400/553. train loss: 1.4903,	1.0400 s / batch. (data: 2.23e-01). ETA=11:23:14, max mem: 20.9 GB 
[11/28 12:00:40 visual_prompt]: 	Training 500/553. train loss: 0.8697,	0.8344 s / batch. (data: 3.28e-04). ETA=9:06:43, max mem: 20.9 GB 
[11/28 12:01:32 visual_prompt]: Epoch 29 / 100: avg data time: 1.61e-01, avg batch time: 0.9854, average train loss: 1.2836
[11/28 12:02:28 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3052, average loss: 1.4331
[11/28 12:02:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.20	
[11/28 12:02:28 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[11/28 12:04:09 visual_prompt]: 	Training 100/553. train loss: 0.8284,	0.8120 s / batch. (data: 3.04e-04). ETA=8:49:58, max mem: 20.9 GB 
[11/28 12:05:49 visual_prompt]: 	Training 200/553. train loss: 0.9494,	0.8210 s / batch. (data: 5.46e-03). ETA=8:54:30, max mem: 20.9 GB 
[11/28 12:07:33 visual_prompt]: 	Training 300/553. train loss: 0.0349,	2.0162 s / batch. (data: 1.20e+00). ETA=21:49:15, max mem: 20.9 GB 
[11/28 12:09:19 visual_prompt]: 	Training 400/553. train loss: 0.7079,	1.2061 s / batch. (data: 3.95e-01). ETA=13:01:11, max mem: 20.9 GB 
[11/28 12:11:02 visual_prompt]: 	Training 500/553. train loss: 0.5656,	1.8969 s / batch. (data: 1.08e+00). ETA=20:25:27, max mem: 20.9 GB 
[11/28 12:11:58 visual_prompt]: Epoch 30 / 100: avg data time: 2.07e-01, avg batch time: 1.0301, average train loss: 1.1249
[11/28 12:12:58 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.3073, average loss: 0.7190
[11/28 12:12:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.52	
[11/28 12:12:58 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[11/28 12:14:47 visual_prompt]: 	Training 100/553. train loss: 0.9726,	0.8520 s / batch. (data: 1.19e-02). ETA=9:08:13, max mem: 20.9 GB 
[11/28 12:16:33 visual_prompt]: 	Training 200/553. train loss: 2.1719,	0.8080 s / batch. (data: 2.99e-04). ETA=8:38:37, max mem: 20.9 GB 
[11/28 12:18:14 visual_prompt]: 	Training 300/553. train loss: 1.9726,	0.8227 s / batch. (data: 1.56e-02). ETA=8:46:40, max mem: 20.9 GB 
[11/28 12:19:56 visual_prompt]: 	Training 400/553. train loss: 0.5531,	1.0898 s / batch. (data: 2.81e-01). ETA=11:35:49, max mem: 20.9 GB 
[11/28 12:21:40 visual_prompt]: 	Training 500/553. train loss: 0.8325,	0.8098 s / batch. (data: 3.18e-04). ETA=8:35:40, max mem: 20.9 GB 
[11/28 12:22:34 visual_prompt]: Epoch 31 / 100: avg data time: 2.18e-01, avg batch time: 1.0420, average train loss: 1.2540
[11/28 12:23:34 visual_prompt]: Inference (val):avg data time: 5.87e-05, avg batch time: 0.3124, average loss: 0.7238
[11/28 12:23:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.56	
[11/28 12:23:34 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[11/28 12:25:23 visual_prompt]: 	Training 100/553. train loss: 0.6575,	0.8320 s / batch. (data: 4.26e-04). ETA=8:47:43, max mem: 20.9 GB 
[11/28 12:27:06 visual_prompt]: 	Training 200/553. train loss: 0.5992,	0.8399 s / batch. (data: 5.62e-03). ETA=8:51:21, max mem: 20.9 GB 
[11/28 12:28:54 visual_prompt]: 	Training 300/553. train loss: 1.3649,	0.8121 s / batch. (data: 3.27e-04). ETA=8:32:21, max mem: 20.9 GB 
[11/28 12:30:37 visual_prompt]: 	Training 400/553. train loss: 1.0292,	0.8089 s / batch. (data: 3.14e-04). ETA=8:29:02, max mem: 20.9 GB 
[11/28 12:32:18 visual_prompt]: 	Training 500/553. train loss: 0.7414,	0.8464 s / batch. (data: 2.24e-02). ETA=8:51:12, max mem: 20.9 GB 
[11/28 12:33:10 visual_prompt]: Epoch 32 / 100: avg data time: 2.21e-01, avg batch time: 1.0423, average train loss: 1.0378
[11/28 12:34:10 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.3046, average loss: 1.1125
[11/28 12:34:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.76	
[11/28 12:34:10 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[11/28 12:35:56 visual_prompt]: 	Training 100/553. train loss: 0.1837,	1.1475 s / batch. (data: 3.29e-01). ETA=11:57:14, max mem: 20.9 GB 
[11/28 12:37:42 visual_prompt]: 	Training 200/553. train loss: 2.0197,	1.5485 s / batch. (data: 7.37e-01). ETA=16:05:21, max mem: 20.9 GB 
[11/28 12:39:26 visual_prompt]: 	Training 300/553. train loss: 0.6138,	0.8520 s / batch. (data: 3.39e-04). ETA=8:49:42, max mem: 20.9 GB 
[11/28 12:41:11 visual_prompt]: 	Training 400/553. train loss: 1.0919,	0.8320 s / batch. (data: 3.04e-04). ETA=8:35:54, max mem: 20.9 GB 
[11/28 12:42:53 visual_prompt]: 	Training 500/553. train loss: 0.8687,	0.8808 s / batch. (data: 6.68e-02). ETA=9:04:39, max mem: 20.9 GB 
[11/28 12:43:47 visual_prompt]: Epoch 33 / 100: avg data time: 2.20e-01, avg batch time: 1.0433, average train loss: 1.4306
[11/28 12:44:46 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3049, average loss: 0.7960
[11/28 12:44:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.11	
[11/28 12:44:46 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[11/28 12:46:36 visual_prompt]: 	Training 100/553. train loss: 1.9688,	1.0068 s / batch. (data: 1.60e-01). ETA=10:20:01, max mem: 20.9 GB 
[11/28 12:48:18 visual_prompt]: 	Training 200/553. train loss: 1.7306,	0.8349 s / batch. (data: 7.97e-03). ETA=8:32:47, max mem: 20.9 GB 
[11/28 12:50:00 visual_prompt]: 	Training 300/553. train loss: 0.7064,	0.8601 s / batch. (data: 4.68e-02). ETA=8:46:47, max mem: 20.9 GB 
[11/28 12:51:45 visual_prompt]: 	Training 400/553. train loss: 3.1435,	0.8400 s / batch. (data: 3.02e-04). ETA=8:33:06, max mem: 20.9 GB 
[11/28 12:53:29 visual_prompt]: 	Training 500/553. train loss: 0.5808,	1.7406 s / batch. (data: 9.29e-01). ETA=17:40:21, max mem: 20.9 GB 
[11/28 12:54:23 visual_prompt]: Epoch 34 / 100: avg data time: 2.21e-01, avg batch time: 1.0420, average train loss: 1.4370
[11/28 12:55:22 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.3047, average loss: 0.7245
[11/28 12:55:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.37	
[11/28 12:55:22 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[11/28 12:57:12 visual_prompt]: 	Training 100/553. train loss: 0.3701,	0.8240 s / batch. (data: 3.37e-04). ETA=8:19:51, max mem: 20.9 GB 
[11/28 12:58:57 visual_prompt]: 	Training 200/553. train loss: 1.1650,	0.8427 s / batch. (data: 1.06e-02). ETA=8:29:49, max mem: 20.9 GB 
[11/28 13:00:39 visual_prompt]: 	Training 300/553. train loss: 0.6866,	0.8050 s / batch. (data: 3.76e-04). ETA=8:05:39, max mem: 20.9 GB 
[11/28 13:02:23 visual_prompt]: 	Training 400/553. train loss: 2.5259,	0.8901 s / batch. (data: 8.51e-02). ETA=8:55:30, max mem: 20.9 GB 
[11/28 13:04:06 visual_prompt]: 	Training 500/553. train loss: 0.6820,	1.0202 s / batch. (data: 1.90e-01). ETA=10:12:03, max mem: 20.9 GB 
[11/28 13:05:00 visual_prompt]: Epoch 35 / 100: avg data time: 2.23e-01, avg batch time: 1.0450, average train loss: 1.4965
[11/28 13:06:00 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.3054, average loss: 4.1198
[11/28 13:06:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.79	
[11/28 13:06:00 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[11/28 13:07:46 visual_prompt]: 	Training 100/553. train loss: 2.8035,	0.8240 s / batch. (data: 4.04e-04). ETA=8:12:14, max mem: 20.9 GB 
[11/28 13:09:31 visual_prompt]: 	Training 200/553. train loss: 2.2939,	0.8280 s / batch. (data: 3.20e-04). ETA=8:13:18, max mem: 20.9 GB 
[11/28 13:11:17 visual_prompt]: 	Training 300/553. train loss: 0.0959,	0.8280 s / batch. (data: 3.00e-04). ETA=8:11:55, max mem: 20.9 GB 
[11/28 13:13:01 visual_prompt]: 	Training 400/553. train loss: 2.7227,	0.8341 s / batch. (data: 1.05e-02). ETA=8:14:09, max mem: 20.9 GB 
[11/28 13:14:46 visual_prompt]: 	Training 500/553. train loss: 4.2440,	1.1640 s / batch. (data: 3.44e-01). ETA=11:27:38, max mem: 20.9 GB 
[11/28 13:15:38 visual_prompt]: Epoch 36 / 100: avg data time: 2.22e-01, avg batch time: 1.0442, average train loss: 1.7918
[11/28 13:16:37 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.3064, average loss: 1.7960
[11/28 13:16:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.40	
[11/28 13:16:37 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[11/28 13:18:25 visual_prompt]: 	Training 100/553. train loss: 0.5411,	0.8059 s / batch. (data: 3.41e-04). ETA=7:54:00, max mem: 20.9 GB 
[11/28 13:20:09 visual_prompt]: 	Training 200/553. train loss: 1.6018,	0.8240 s / batch. (data: 3.09e-04). ETA=8:03:19, max mem: 20.9 GB 
[11/28 13:21:53 visual_prompt]: 	Training 300/553. train loss: 4.2549,	1.7732 s / batch. (data: 9.63e-01). ETA=17:17:03, max mem: 20.9 GB 
[11/28 13:23:41 visual_prompt]: 	Training 400/553. train loss: 1.0434,	1.5783 s / batch. (data: 7.74e-01). ETA=15:20:27, max mem: 20.9 GB 
[11/28 13:25:21 visual_prompt]: 	Training 500/553. train loss: 1.9251,	1.2465 s / batch. (data: 4.37e-01). ETA=12:04:53, max mem: 20.9 GB 
[11/28 13:26:16 visual_prompt]: Epoch 37 / 100: avg data time: 2.25e-01, avg batch time: 1.0470, average train loss: 1.4903
[11/28 13:27:16 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.3061, average loss: 1.1598
[11/28 13:27:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.07	
[11/28 13:27:16 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[11/28 13:29:02 visual_prompt]: 	Training 100/553. train loss: 0.8273,	0.8254 s / batch. (data: 3.09e-04). ETA=7:57:54, max mem: 20.9 GB 
[11/28 13:30:47 visual_prompt]: 	Training 200/553. train loss: 0.7999,	1.5484 s / batch. (data: 7.28e-01). ETA=14:53:53, max mem: 20.9 GB 
[11/28 13:32:33 visual_prompt]: 	Training 300/553. train loss: 1.0808,	0.8158 s / batch. (data: 3.01e-04). ETA=7:49:35, max mem: 20.9 GB 
[11/28 13:34:15 visual_prompt]: 	Training 400/553. train loss: 0.0853,	0.8157 s / batch. (data: 4.17e-04). ETA=7:48:12, max mem: 20.9 GB 
[11/28 13:36:02 visual_prompt]: 	Training 500/553. train loss: 0.7028,	0.8207 s / batch. (data: 4.23e-04). ETA=7:49:42, max mem: 20.9 GB 
[11/28 13:36:55 visual_prompt]: Epoch 38 / 100: avg data time: 2.23e-01, avg batch time: 1.0466, average train loss: 1.4671
[11/28 13:37:55 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.3058, average loss: 2.2161
[11/28 13:37:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.28	
[11/28 13:37:55 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[11/28 13:39:42 visual_prompt]: 	Training 100/553. train loss: 0.3800,	0.8217 s / batch. (data: 3.70e-04). ETA=7:48:10, max mem: 20.9 GB 
[11/28 13:41:31 visual_prompt]: 	Training 200/553. train loss: 1.0805,	0.8147 s / batch. (data: 4.43e-04). ETA=7:42:49, max mem: 20.9 GB 
[11/28 13:43:18 visual_prompt]: 	Training 300/553. train loss: 5.5154,	0.8180 s / batch. (data: 7.98e-03). ETA=7:43:20, max mem: 20.9 GB 
[11/28 13:44:59 visual_prompt]: 	Training 400/553. train loss: 1.1481,	0.9004 s / batch. (data: 7.50e-02). ETA=8:28:30, max mem: 20.9 GB 
[11/28 13:46:43 visual_prompt]: 	Training 500/553. train loss: 0.7273,	1.9561 s / batch. (data: 1.15e+00). ETA=18:21:28, max mem: 20.9 GB 
[11/28 13:47:34 visual_prompt]: Epoch 39 / 100: avg data time: 2.26e-01, avg batch time: 1.0476, average train loss: 1.3382
[11/28 13:48:34 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.3062, average loss: 1.5693
[11/28 13:48:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.72	
[11/28 13:48:34 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[11/28 13:50:24 visual_prompt]: 	Training 100/553. train loss: 1.3164,	0.8354 s / batch. (data: 5.44e-03). ETA=7:48:18, max mem: 20.9 GB 
[11/28 13:52:07 visual_prompt]: 	Training 200/553. train loss: 2.5917,	0.8338 s / batch. (data: 1.05e-02). ETA=7:45:58, max mem: 20.9 GB 
[11/28 13:53:52 visual_prompt]: 	Training 300/553. train loss: 2.0996,	0.8323 s / batch. (data: 3.46e-03). ETA=7:43:46, max mem: 20.9 GB 
[11/28 13:55:37 visual_prompt]: 	Training 400/553. train loss: 1.2853,	0.8362 s / batch. (data: 5.94e-03). ETA=7:44:31, max mem: 20.9 GB 
[11/28 13:57:21 visual_prompt]: 	Training 500/553. train loss: 0.0488,	0.8061 s / batch. (data: 3.03e-04). ETA=7:26:28, max mem: 20.9 GB 
[11/28 13:58:17 visual_prompt]: Epoch 40 / 100: avg data time: 2.31e-01, avg batch time: 1.0545, average train loss: 1.4414
[11/28 13:59:17 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3060, average loss: 0.7913
[11/28 13:59:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.57	
[11/28 13:59:17 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[11/28 14:01:10 visual_prompt]: 	Training 100/553. train loss: 1.1697,	0.8238 s / batch. (data: 5.52e-03). ETA=7:34:10, max mem: 20.9 GB 
[11/28 14:02:57 visual_prompt]: 	Training 200/553. train loss: 3.3325,	0.8355 s / batch. (data: 1.19e-03). ETA=7:39:15, max mem: 20.9 GB 
[11/28 14:04:40 visual_prompt]: 	Training 300/553. train loss: 1.0434,	0.8431 s / batch. (data: 1.05e-02). ETA=7:41:59, max mem: 20.9 GB 
[11/28 14:06:24 visual_prompt]: 	Training 400/553. train loss: 0.7200,	0.8267 s / batch. (data: 3.02e-04). ETA=7:31:39, max mem: 20.9 GB 
[11/28 14:08:05 visual_prompt]: 	Training 500/553. train loss: 0.6739,	0.8236 s / batch. (data: 3.07e-04). ETA=7:28:34, max mem: 20.9 GB 
[11/28 14:08:57 visual_prompt]: Epoch 41 / 100: avg data time: 2.25e-01, avg batch time: 1.0472, average train loss: 1.7543
[11/28 14:09:57 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3054, average loss: 1.0916
[11/28 14:09:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.43	
[11/28 14:09:57 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[11/28 14:11:43 visual_prompt]: 	Training 100/553. train loss: 3.8636,	0.8241 s / batch. (data: 5.41e-03). ETA=7:26:44, max mem: 20.9 GB 
[11/28 14:13:27 visual_prompt]: 	Training 200/553. train loss: 4.4115,	0.8147 s / batch. (data: 4.11e-04). ETA=7:20:18, max mem: 20.9 GB 
[11/28 14:15:11 visual_prompt]: 	Training 300/553. train loss: 0.7441,	0.8240 s / batch. (data: 4.22e-04). ETA=7:23:56, max mem: 20.9 GB 
[11/28 14:16:56 visual_prompt]: 	Training 400/553. train loss: 0.6954,	0.8107 s / batch. (data: 5.60e-03). ETA=7:15:26, max mem: 20.9 GB 
[11/28 14:18:39 visual_prompt]: 	Training 500/553. train loss: 0.5426,	0.8120 s / batch. (data: 3.23e-04). ETA=7:14:47, max mem: 20.9 GB 
[11/28 14:19:34 visual_prompt]: Epoch 42 / 100: avg data time: 2.22e-01, avg batch time: 1.0437, average train loss: 1.3538
[11/28 14:20:33 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3047, average loss: 1.4551
[11/28 14:20:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.13	
[11/28 14:20:33 visual_prompt]: Stopping early.
[11/28 14:20:34 visual_prompt]: Rank of current process: 0. World size: 1
[11/28 14:20:34 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 14:20:34 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/28 14:20:34 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/28 14:20:34 visual_prompt]: Training with config:
[11/28 14:20:34 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr1.0_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/28 14:20:34 visual_prompt]: Loading training data...
[11/28 14:20:34 visual_prompt]: Constructing mammo-cbis dataset train...
[11/28 14:20:34 visual_prompt]: Loading validation data...
[11/28 14:20:34 visual_prompt]: Constructing mammo-cbis dataset val...
[11/28 14:20:34 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/28 14:20:37 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/28 14:20:37 visual_prompt]: tuned percent:0.525
[11/28 14:20:37 visual_prompt]: Device used for model: 0
[11/28 14:20:37 visual_prompt]: Setting up Evaluator...
[11/28 14:20:37 visual_prompt]: Setting up Trainer...
[11/28 14:20:37 visual_prompt]: 	Setting up the optimizer...
[11/28 14:20:37 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/28 14:22:25 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8400 s / batch. (data: 1.06e-02). ETA=12:52:47, max mem: 20.9 GB 
[11/28 14:24:08 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8276 s / batch. (data: 3.69e-04). ETA=12:39:59, max mem: 20.9 GB 
[11/28 14:25:54 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.0734 s / batch. (data: 2.33e-01). ETA=16:23:56, max mem: 20.9 GB 
[11/28 14:27:36 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8212 s / batch. (data: 1.19e-02). ETA=12:31:26, max mem: 20.9 GB 
[11/28 14:29:22 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8081 s / batch. (data: 3.09e-04). ETA=12:18:05, max mem: 20.9 GB 
[11/28 14:30:17 visual_prompt]: Epoch 1 / 100: avg data time: 2.27e-01, avg batch time: 1.0485, average train loss: 1.5403
[11/28 14:31:17 visual_prompt]: Inference (val):avg data time: 2.24e-04, avg batch time: 0.3063, average loss: 1.5201
[11/28 14:31:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/28 14:31:17 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[11/28 14:33:05 visual_prompt]: 	Training 100/553. train loss: 0.7442,	0.8320 s / batch. (data: 4.22e-04). ETA=12:37:46, max mem: 20.9 GB 
[11/28 14:34:48 visual_prompt]: 	Training 200/553. train loss: 0.0236,	0.8122 s / batch. (data: 5.46e-03). ETA=12:18:22, max mem: 20.9 GB 
[11/28 14:36:34 visual_prompt]: 	Training 300/553. train loss: 0.7331,	1.1440 s / batch. (data: 3.17e-01). ETA=17:18:05, max mem: 20.9 GB 
[11/28 14:38:16 visual_prompt]: 	Training 400/553. train loss: 1.0297,	0.8339 s / batch. (data: 4.57e-04). ETA=12:35:18, max mem: 20.9 GB 
[11/28 14:40:02 visual_prompt]: 	Training 500/553. train loss: 0.6588,	0.8240 s / batch. (data: 3.56e-04). ETA=12:24:59, max mem: 20.9 GB 
[11/28 14:40:55 visual_prompt]: Epoch 2 / 100: avg data time: 2.24e-01, avg batch time: 1.0464, average train loss: 0.9814
[11/28 14:41:55 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3055, average loss: 1.2745
[11/28 14:41:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.69	
[11/28 14:41:55 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[11/28 14:43:42 visual_prompt]: 	Training 100/553. train loss: 1.4617,	0.9946 s / batch. (data: 1.79e-01). ETA=14:56:42, max mem: 20.9 GB 
[11/28 14:45:27 visual_prompt]: 	Training 200/553. train loss: 0.8717,	1.9600 s / batch. (data: 1.15e+00). ETA=1 day, 5:23:48, max mem: 20.9 GB 
[11/28 14:47:10 visual_prompt]: 	Training 300/553. train loss: 0.6260,	0.8264 s / batch. (data: 6.32e-03). ETA=12:22:19, max mem: 20.9 GB 
[11/28 14:48:56 visual_prompt]: 	Training 400/553. train loss: 3.8072,	0.8080 s / batch. (data: 3.45e-04). ETA=12:04:23, max mem: 20.9 GB 
[11/28 14:50:42 visual_prompt]: 	Training 500/553. train loss: 0.7178,	1.4922 s / batch. (data: 6.75e-01). ETA=22:15:23, max mem: 20.9 GB 
[11/28 14:51:35 visual_prompt]: Epoch 3 / 100: avg data time: 2.25e-01, avg batch time: 1.0475, average train loss: 1.0506
[11/28 14:52:35 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3064, average loss: 0.7089
[11/28 14:52:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 59.41	
[11/28 14:52:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[11/28 14:54:24 visual_prompt]: 	Training 100/553. train loss: 0.7360,	0.8604 s / batch. (data: 2.05e-02). ETA=12:47:44, max mem: 20.9 GB 
[11/28 14:56:09 visual_prompt]: 	Training 200/553. train loss: 0.5707,	0.8320 s / batch. (data: 4.36e-04). ETA=12:21:00, max mem: 20.9 GB 
[11/28 14:57:53 visual_prompt]: 	Training 300/553. train loss: 0.6907,	1.7726 s / batch. (data: 9.65e-01). ETA=1 day, 2:15:50, max mem: 20.9 GB 
[11/28 14:59:33 visual_prompt]: 	Training 400/553. train loss: 0.9989,	0.9600 s / batch. (data: 1.49e-01). ETA=14:11:51, max mem: 20.9 GB 
[11/28 15:01:19 visual_prompt]: 	Training 500/553. train loss: 0.1562,	3.1476 s / batch. (data: 2.34e+00). ETA=1 day, 22:27:49, max mem: 20.9 GB 
[11/28 15:02:15 visual_prompt]: Epoch 4 / 100: avg data time: 2.28e-01, avg batch time: 1.0495, average train loss: 1.1074
[11/28 15:03:15 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3064, average loss: 1.4242
[11/28 15:03:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.75	
[11/28 15:03:15 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[11/28 15:05:03 visual_prompt]: 	Training 100/553. train loss: 2.8956,	0.8441 s / batch. (data: 5.55e-03). ETA=12:25:26, max mem: 20.9 GB 
[11/28 15:06:47 visual_prompt]: 	Training 200/553. train loss: 1.7048,	1.4035 s / batch. (data: 5.82e-01). ETA=20:37:10, max mem: 20.9 GB 
[11/28 15:08:32 visual_prompt]: 	Training 300/553. train loss: 3.8031,	0.8320 s / batch. (data: 3.43e-04). ETA=12:12:01, max mem: 20.9 GB 
[11/28 15:10:15 visual_prompt]: 	Training 400/553. train loss: 2.9447,	0.8268 s / batch. (data: 5.42e-03). ETA=12:06:01, max mem: 20.9 GB 
[11/28 15:11:59 visual_prompt]: 	Training 500/553. train loss: 1.0125,	0.8280 s / batch. (data: 2.82e-04). ETA=12:05:45, max mem: 20.9 GB 
[11/28 15:12:55 visual_prompt]: Epoch 5 / 100: avg data time: 2.25e-01, avg batch time: 1.0475, average train loss: 1.3246
[11/28 15:13:55 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.3055, average loss: 1.9187
[11/28 15:13:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.00	
[11/28 15:13:55 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[11/28 15:15:45 visual_prompt]: 	Training 100/553. train loss: 0.7886,	0.8256 s / batch. (data: 1.20e-02). ETA=12:01:29, max mem: 20.9 GB 
[11/28 15:17:29 visual_prompt]: 	Training 200/553. train loss: 4.9891,	0.8255 s / batch. (data: 5.55e-03). ETA=12:00:03, max mem: 20.9 GB 
[11/28 15:19:11 visual_prompt]: 	Training 300/553. train loss: 0.6830,	0.8091 s / batch. (data: 3.54e-04). ETA=11:44:22, max mem: 20.9 GB 
[11/28 15:20:59 visual_prompt]: 	Training 400/553. train loss: 0.6291,	0.8205 s / batch. (data: 3.25e-04). ETA=11:52:58, max mem: 20.9 GB 
[11/28 15:22:42 visual_prompt]: 	Training 500/553. train loss: 3.6114,	0.8299 s / batch. (data: 1.05e-02). ETA=11:59:45, max mem: 20.9 GB 
[11/28 15:23:36 visual_prompt]: Epoch 6 / 100: avg data time: 2.29e-01, avg batch time: 1.0508, average train loss: 1.4182
[11/28 15:24:36 visual_prompt]: Inference (val):avg data time: 4.59e-05, avg batch time: 0.3060, average loss: 1.3055
[11/28 15:24:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 64.08	
[11/28 15:24:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[11/28 15:26:23 visual_prompt]: 	Training 100/553. train loss: 1.0775,	0.8131 s / batch. (data: 7.94e-03). ETA=11:43:05, max mem: 20.9 GB 
[11/28 15:28:07 visual_prompt]: 	Training 200/553. train loss: 0.6079,	0.8358 s / batch. (data: 1.09e-02). ETA=12:01:20, max mem: 20.9 GB 
[11/28 15:29:54 visual_prompt]: 	Training 300/553. train loss: 0.7782,	2.1816 s / batch. (data: 1.36e+00). ETA=1 day, 7:19:09, max mem: 20.9 GB 
[11/28 15:31:38 visual_prompt]: 	Training 400/553. train loss: 0.8291,	2.1200 s / batch. (data: 1.29e+00). ETA=1 day, 6:22:33, max mem: 20.9 GB 
[11/28 15:33:19 visual_prompt]: 	Training 500/553. train loss: 1.8916,	0.8266 s / batch. (data: 5.45e-03). ETA=11:49:17, max mem: 20.9 GB 
[11/28 15:34:13 visual_prompt]: Epoch 7 / 100: avg data time: 2.21e-01, avg batch time: 1.0433, average train loss: 1.3133
[11/28 15:35:13 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3065, average loss: 0.6972
[11/28 15:35:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 64.01	
[11/28 15:35:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[11/28 15:36:59 visual_prompt]: 	Training 100/553. train loss: 3.1443,	0.8322 s / batch. (data: 5.46e-03). ETA=11:51:57, max mem: 20.9 GB 
[11/28 15:38:45 visual_prompt]: 	Training 200/553. train loss: 1.8945,	0.8355 s / batch. (data: 5.45e-03). ETA=11:53:23, max mem: 20.9 GB 
[11/28 15:40:30 visual_prompt]: 	Training 300/553. train loss: 3.2202,	0.8161 s / batch. (data: 2.99e-04). ETA=11:35:25, max mem: 20.9 GB 
[11/28 15:42:16 visual_prompt]: 	Training 400/553. train loss: 0.7549,	0.8400 s / batch. (data: 7.94e-03). ETA=11:54:26, max mem: 20.9 GB 
[11/28 15:44:00 visual_prompt]: 	Training 500/553. train loss: 2.4947,	1.6803 s / batch. (data: 8.59e-01). ETA=23:46:14, max mem: 20.9 GB 
[11/28 15:44:54 visual_prompt]: Epoch 8 / 100: avg data time: 2.28e-01, avg batch time: 1.0511, average train loss: 1.8896
[11/28 15:45:54 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.3071, average loss: 1.0552
[11/28 15:45:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.93	
[11/28 15:45:54 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[11/28 15:47:43 visual_prompt]: 	Training 100/553. train loss: 0.0021,	0.8520 s / batch. (data: 7.94e-03). ETA=12:01:01, max mem: 20.9 GB 
[11/28 15:49:28 visual_prompt]: 	Training 200/553. train loss: 0.5775,	0.8222 s / batch. (data: 4.28e-04). ETA=11:34:27, max mem: 20.9 GB 
[11/28 15:51:13 visual_prompt]: 	Training 300/553. train loss: 1.3700,	1.7586 s / batch. (data: 9.40e-01). ETA=1 day, 0:42:24, max mem: 20.9 GB 
[11/28 15:52:58 visual_prompt]: 	Training 400/553. train loss: 1.1137,	0.8400 s / batch. (data: 2.90e-04). ETA=11:46:41, max mem: 20.9 GB 
[11/28 15:54:44 visual_prompt]: 	Training 500/553. train loss: 1.7083,	1.0325 s / batch. (data: 2.07e-01). ETA=14:26:51, max mem: 20.9 GB 
[11/28 15:55:37 visual_prompt]: Epoch 9 / 100: avg data time: 2.31e-01, avg batch time: 1.0538, average train loss: 1.4716
[11/28 15:56:37 visual_prompt]: Inference (val):avg data time: 5.36e-05, avg batch time: 0.3066, average loss: 1.2125
[11/28 15:56:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.94	
[11/28 15:56:37 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[11/28 15:58:28 visual_prompt]: 	Training 100/553. train loss: 3.3886,	0.8282 s / batch. (data: 1.92e-04). ETA=11:33:15, max mem: 20.9 GB 
[11/28 16:00:12 visual_prompt]: 	Training 200/553. train loss: 0.6082,	0.8308 s / batch. (data: 5.41e-03). ETA=11:34:04, max mem: 20.9 GB 
[11/28 16:01:55 visual_prompt]: 	Training 300/553. train loss: 0.6965,	0.8280 s / batch. (data: 7.95e-03). ETA=11:30:21, max mem: 20.9 GB 
[11/28 16:03:36 visual_prompt]: 	Training 400/553. train loss: 1.5656,	0.9194 s / batch. (data: 7.56e-02). ETA=12:45:00, max mem: 20.9 GB 
[11/28 16:05:23 visual_prompt]: 	Training 500/553. train loss: 0.6589,	1.2882 s / batch. (data: 4.76e-01). ETA=17:49:43, max mem: 20.9 GB 
[11/28 16:06:18 visual_prompt]: Epoch 10 / 100: avg data time: 2.28e-01, avg batch time: 1.0496, average train loss: 2.1908
[11/28 16:07:18 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3072, average loss: 1.2408
[11/28 16:07:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.24	
[11/28 16:07:18 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[11/28 16:09:08 visual_prompt]: 	Training 100/553. train loss: 0.5043,	0.8253 s / batch. (data: 9.24e-03). ETA=11:23:13, max mem: 20.9 GB 
[11/28 16:10:54 visual_prompt]: 	Training 200/553. train loss: 1.3819,	0.8320 s / batch. (data: 4.91e-04). ETA=11:27:19, max mem: 20.9 GB 
[11/28 16:12:39 visual_prompt]: 	Training 300/553. train loss: 0.1835,	2.2785 s / batch. (data: 1.47e+00). ETA=1 day, 7:18:37, max mem: 20.9 GB 
[11/28 16:14:23 visual_prompt]: 	Training 400/553. train loss: 0.8650,	0.8401 s / batch. (data: 7.33e-04). ETA=11:31:14, max mem: 20.9 GB 
[11/28 16:16:05 visual_prompt]: 	Training 500/553. train loss: 1.4664,	0.8421 s / batch. (data: 1.07e-02). ETA=11:31:31, max mem: 20.9 GB 
[11/28 16:16:59 visual_prompt]: Epoch 11 / 100: avg data time: 2.27e-01, avg batch time: 1.0505, average train loss: 1.3125
[11/28 16:17:59 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3045, average loss: 0.6698
[11/28 16:17:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 63.50	
[11/28 16:17:59 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[11/28 16:19:49 visual_prompt]: 	Training 100/553. train loss: 1.0039,	1.0440 s / batch. (data: 2.32e-01). ETA=14:14:35, max mem: 20.9 GB 
[11/28 16:21:35 visual_prompt]: 	Training 200/553. train loss: 1.3046,	1.2710 s / batch. (data: 4.54e-01). ETA=17:18:18, max mem: 20.9 GB 
[11/28 16:23:18 visual_prompt]: 	Training 300/553. train loss: 2.3753,	0.8120 s / batch. (data: 3.22e-04). ETA=11:02:02, max mem: 20.9 GB 
[11/28 16:25:02 visual_prompt]: 	Training 400/553. train loss: 2.2551,	0.8386 s / batch. (data: 4.44e-04). ETA=11:22:19, max mem: 20.9 GB 
[11/28 16:26:47 visual_prompt]: 	Training 500/553. train loss: 7.8036,	0.8120 s / batch. (data: 2.84e-04). ETA=10:59:20, max mem: 20.9 GB 
[11/28 16:27:40 visual_prompt]: Epoch 12 / 100: avg data time: 2.27e-01, avg batch time: 1.0506, average train loss: 1.5844
[11/28 16:28:40 visual_prompt]: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3071, average loss: 4.4920
[11/28 16:28:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.22	
[11/28 16:28:40 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[11/28 16:30:30 visual_prompt]: 	Training 100/553. train loss: 0.9622,	0.8283 s / batch. (data: 3.09e-04). ETA=11:10:25, max mem: 20.9 GB 
[11/28 16:32:11 visual_prompt]: 	Training 200/553. train loss: 0.6140,	0.8059 s / batch. (data: 4.33e-04). ETA=10:50:58, max mem: 20.9 GB 
[11/28 16:33:57 visual_prompt]: 	Training 300/553. train loss: 0.6270,	2.0317 s / batch. (data: 1.20e+00). ETA=1 day, 3:17:41, max mem: 20.9 GB 
[11/28 16:35:41 visual_prompt]: 	Training 400/553. train loss: 0.5155,	0.8090 s / batch. (data: 4.33e-04). ETA=10:50:47, max mem: 20.9 GB 
[11/28 16:37:26 visual_prompt]: 	Training 500/553. train loss: 3.0772,	0.8138 s / batch. (data: 3.37e-04). ETA=10:53:14, max mem: 20.9 GB 
[11/28 16:38:20 visual_prompt]: Epoch 13 / 100: avg data time: 2.27e-01, avg batch time: 1.0498, average train loss: 1.5807
[11/28 16:39:21 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.3053, average loss: 0.7782
[11/28 16:39:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 62.49	
[11/28 16:39:21 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[11/28 16:41:11 visual_prompt]: 	Training 100/553. train loss: 0.9836,	0.8480 s / batch. (data: 4.15e-04). ETA=11:18:31, max mem: 20.9 GB 
[11/28 16:42:55 visual_prompt]: 	Training 200/553. train loss: 0.0590,	1.4680 s / batch. (data: 6.30e-01). ETA=19:32:11, max mem: 20.9 GB 
[11/28 16:44:39 visual_prompt]: 	Training 300/553. train loss: 0.4164,	0.9533 s / batch. (data: 1.24e-01). ETA=12:39:36, max mem: 20.9 GB 
[11/28 16:46:23 visual_prompt]: 	Training 400/553. train loss: 0.8832,	0.8262 s / batch. (data: 5.59e-03). ETA=10:56:57, max mem: 20.9 GB 
[11/28 16:48:07 visual_prompt]: 	Training 500/553. train loss: 4.5468,	0.8215 s / batch. (data: 3.11e-04). ETA=10:51:52, max mem: 20.9 GB 
[11/28 16:49:01 visual_prompt]: Epoch 14 / 100: avg data time: 2.27e-01, avg batch time: 1.0502, average train loss: 1.4852
[11/28 16:50:02 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3074, average loss: 0.9987
[11/28 16:50:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 51.22	rocauc: 66.16	
[11/28 16:50:02 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[11/28 16:51:51 visual_prompt]: 	Training 100/553. train loss: 1.3619,	0.8440 s / batch. (data: 4.41e-04). ETA=11:07:32, max mem: 20.9 GB 
[11/28 16:53:34 visual_prompt]: 	Training 200/553. train loss: 0.2890,	0.8600 s / batch. (data: 1.59e-02). ETA=11:18:46, max mem: 20.9 GB 
[11/28 16:55:21 visual_prompt]: 	Training 300/553. train loss: 1.6031,	0.8206 s / batch. (data: 3.44e-04). ETA=10:46:19, max mem: 20.9 GB 
[11/28 16:57:02 visual_prompt]: 	Training 400/553. train loss: 1.2904,	0.9716 s / batch. (data: 1.54e-01). ETA=12:43:36, max mem: 20.9 GB 
[11/28 16:58:47 visual_prompt]: 	Training 500/553. train loss: 0.8546,	0.8068 s / batch. (data: 3.68e-04). ETA=10:32:46, max mem: 20.9 GB 
[11/28 16:59:43 visual_prompt]: Epoch 15 / 100: avg data time: 2.27e-01, avg batch time: 1.0503, average train loss: 1.8137
[11/28 17:00:44 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.3063, average loss: 1.5462
[11/28 17:00:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.27	
[11/28 17:00:44 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[11/28 17:02:31 visual_prompt]: 	Training 100/553. train loss: 1.7917,	0.8160 s / batch. (data: 3.86e-04). ETA=10:37:52, max mem: 20.9 GB 
[11/28 17:04:16 visual_prompt]: 	Training 200/553. train loss: 0.9200,	0.8294 s / batch. (data: 5.72e-03). ETA=10:46:59, max mem: 20.9 GB 
[11/28 17:06:02 visual_prompt]: 	Training 300/553. train loss: 2.8723,	0.8313 s / batch. (data: 1.08e-03). ETA=10:47:05, max mem: 20.9 GB 
[11/28 17:07:46 visual_prompt]: 	Training 400/553. train loss: 1.8306,	0.8160 s / batch. (data: 3.04e-04). ETA=10:33:51, max mem: 20.9 GB 
[11/28 17:09:30 visual_prompt]: 	Training 500/553. train loss: 0.5853,	1.7440 s / batch. (data: 9.19e-01). ETA=22:31:45, max mem: 20.9 GB 
[11/28 17:10:25 visual_prompt]: Epoch 16 / 100: avg data time: 2.28e-01, avg batch time: 1.0504, average train loss: 1.3513
[11/28 17:11:25 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3063, average loss: 0.8820
[11/28 17:11:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 67.44	
[11/28 17:11:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[11/28 17:13:13 visual_prompt]: 	Training 100/553. train loss: 0.9979,	0.8200 s / batch. (data: 3.29e-04). ETA=10:33:27, max mem: 20.9 GB 
[11/28 17:15:00 visual_prompt]: 	Training 200/553. train loss: 5.0759,	0.8343 s / batch. (data: 1.06e-02). ETA=10:43:10, max mem: 20.9 GB 
[11/28 17:16:46 visual_prompt]: 	Training 300/553. train loss: 1.2097,	0.8280 s / batch. (data: 3.40e-04). ETA=10:36:53, max mem: 20.9 GB 
[11/28 17:18:31 visual_prompt]: 	Training 400/553. train loss: 0.6966,	1.5640 s / batch. (data: 7.46e-01). ETA=20:00:26, max mem: 20.9 GB 
[11/28 17:20:15 visual_prompt]: 	Training 500/553. train loss: 0.9693,	1.6240 s / batch. (data: 7.94e-01). ETA=20:43:46, max mem: 20.9 GB 
[11/28 17:21:10 visual_prompt]: Epoch 17 / 100: avg data time: 2.35e-01, avg batch time: 1.0579, average train loss: 1.5792
[11/28 17:22:10 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3083, average loss: 0.6952
[11/28 17:22:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 69.91	
[11/28 17:22:10 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[11/28 17:23:58 visual_prompt]: 	Training 100/553. train loss: 0.2510,	0.8434 s / batch. (data: 3.34e-04). ETA=10:43:48, max mem: 20.9 GB 
[11/28 17:25:46 visual_prompt]: 	Training 200/553. train loss: 2.5037,	0.8102 s / batch. (data: 3.68e-04). ETA=10:17:07, max mem: 20.9 GB 
[11/28 17:27:30 visual_prompt]: 	Training 300/553. train loss: 0.5159,	0.8061 s / batch. (data: 3.03e-04). ETA=10:12:35, max mem: 20.9 GB 
[11/28 17:29:14 visual_prompt]: 	Training 400/553. train loss: 1.4378,	0.8118 s / batch. (data: 3.93e-04). ETA=10:15:35, max mem: 20.9 GB 
[11/28 17:30:57 visual_prompt]: 	Training 500/553. train loss: 1.6077,	0.8242 s / batch. (data: 7.93e-03). ETA=10:23:37, max mem: 20.9 GB 
[11/28 17:31:49 visual_prompt]: Epoch 18 / 100: avg data time: 2.26e-01, avg batch time: 1.0485, average train loss: 1.5942
[11/28 17:32:49 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3055, average loss: 0.9914
[11/28 17:32:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.78	
[11/28 17:32:49 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[11/28 17:34:37 visual_prompt]: 	Training 100/553. train loss: 0.8349,	1.4144 s / batch. (data: 5.73e-01). ETA=17:46:36, max mem: 20.9 GB 
[11/28 17:36:23 visual_prompt]: 	Training 200/553. train loss: 0.4752,	0.8265 s / batch. (data: 5.41e-03). ETA=10:21:54, max mem: 20.9 GB 
[11/28 17:38:06 visual_prompt]: 	Training 300/553. train loss: 2.0306,	0.8302 s / batch. (data: 1.05e-02). ETA=10:23:18, max mem: 20.9 GB 
[11/28 17:39:52 visual_prompt]: 	Training 400/553. train loss: 0.4793,	0.8277 s / batch. (data: 1.10e-02). ETA=10:20:00, max mem: 20.9 GB 
[11/28 17:41:32 visual_prompt]: 	Training 500/553. train loss: 0.9482,	0.8280 s / batch. (data: 3.80e-04). ETA=10:18:52, max mem: 20.9 GB 
[11/28 17:42:26 visual_prompt]: Epoch 19 / 100: avg data time: 2.20e-01, avg batch time: 1.0429, average train loss: 1.2877
[11/28 17:43:26 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3058, average loss: 3.3269
[11/28 17:43:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 68.71	
[11/28 17:43:26 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[11/28 17:45:13 visual_prompt]: 	Training 100/553. train loss: 0.6429,	0.8320 s / batch. (data: 7.95e-03). ETA=10:19:44, max mem: 20.9 GB 
[11/28 17:46:58 visual_prompt]: 	Training 200/553. train loss: 0.4377,	0.8354 s / batch. (data: 3.81e-04). ETA=10:20:51, max mem: 20.9 GB 
[11/28 17:48:42 visual_prompt]: 	Training 300/553. train loss: 4.3347,	0.8383 s / batch. (data: 2.97e-04). ETA=10:21:37, max mem: 20.9 GB 
[11/28 17:50:27 visual_prompt]: 	Training 400/553. train loss: 0.5259,	0.8240 s / batch. (data: 5.54e-03). ETA=10:09:40, max mem: 20.9 GB 
[11/28 17:52:10 visual_prompt]: 	Training 500/553. train loss: 1.7503,	0.8160 s / batch. (data: 4.38e-04). ETA=10:02:22, max mem: 20.9 GB 
[11/28 17:53:05 visual_prompt]: Epoch 20 / 100: avg data time: 2.25e-01, avg batch time: 1.0475, average train loss: 1.6240
[11/28 17:54:05 visual_prompt]: Inference (val):avg data time: 4.50e-04, avg batch time: 0.3060, average loss: 1.1141
[11/28 17:54:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 67.32	
[11/28 17:54:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[11/28 17:55:56 visual_prompt]: 	Training 100/553. train loss: 0.4350,	0.8180 s / batch. (data: 3.96e-04). ETA=10:01:48, max mem: 20.9 GB 
[11/28 17:57:39 visual_prompt]: 	Training 200/553. train loss: 0.1791,	0.8203 s / batch. (data: 1.07e-02). ETA=10:02:06, max mem: 20.9 GB 
[11/28 17:59:23 visual_prompt]: 	Training 300/553. train loss: 4.4692,	1.3265 s / batch. (data: 5.07e-01). ETA=16:11:24, max mem: 20.9 GB 
[11/28 18:01:04 visual_prompt]: 	Training 400/553. train loss: 4.0793,	0.8303 s / batch. (data: 5.26e-04). ETA=10:06:39, max mem: 20.9 GB 
[11/28 18:02:50 visual_prompt]: 	Training 500/553. train loss: 1.5386,	0.8230 s / batch. (data: 5.47e-03). ETA=9:59:58, max mem: 20.9 GB 
[11/28 18:03:43 visual_prompt]: Epoch 21 / 100: avg data time: 2.23e-01, avg batch time: 1.0452, average train loss: 1.3973
[11/28 18:04:43 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.3081, average loss: 0.7684
[11/28 18:04:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 69.66	
[11/28 18:04:43 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[11/28 18:06:32 visual_prompt]: 	Training 100/553. train loss: 1.9408,	0.8241 s / batch. (data: 5.55e-03). ETA=9:58:38, max mem: 20.9 GB 
[11/28 18:08:16 visual_prompt]: 	Training 200/553. train loss: 0.4545,	0.8162 s / batch. (data: 2.92e-04). ETA=9:51:35, max mem: 20.9 GB 
[11/28 18:09:59 visual_prompt]: 	Training 300/553. train loss: 0.1374,	0.8521 s / batch. (data: 3.11e-04). ETA=10:16:08, max mem: 20.9 GB 
[11/28 18:11:44 visual_prompt]: 	Training 400/553. train loss: 0.6139,	0.8280 s / batch. (data: 7.93e-03). ETA=9:57:19, max mem: 20.9 GB 
[11/28 18:13:29 visual_prompt]: 	Training 500/553. train loss: 1.2890,	0.8601 s / batch. (data: 3.07e-04). ETA=10:19:03, max mem: 20.9 GB 
[11/28 18:14:24 visual_prompt]: Epoch 22 / 100: avg data time: 2.26e-01, avg batch time: 1.0494, average train loss: 1.2825
[11/28 18:15:24 visual_prompt]: Inference (val):avg data time: 4.13e-05, avg batch time: 0.3055, average loss: 1.7864
[11/28 18:15:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 70.46	
[11/28 18:15:24 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[11/28 18:17:13 visual_prompt]: 	Training 100/553. train loss: 1.4544,	0.8143 s / batch. (data: 3.97e-04). ETA=9:44:01, max mem: 20.9 GB 
[11/28 18:18:58 visual_prompt]: 	Training 200/553. train loss: 2.4655,	1.1186 s / batch. (data: 3.03e-01). ETA=13:20:27, max mem: 20.9 GB 
[11/28 18:20:44 visual_prompt]: 	Training 300/553. train loss: 0.5708,	0.8440 s / batch. (data: 1.00e-03). ETA=10:02:30, max mem: 20.9 GB 
[11/28 18:22:26 visual_prompt]: 	Training 400/553. train loss: 0.6146,	0.8372 s / batch. (data: 1.07e-03). ETA=9:56:18, max mem: 20.9 GB 
[11/28 18:24:09 visual_prompt]: 	Training 500/553. train loss: 0.9569,	0.8134 s / batch. (data: 7.89e-03). ETA=9:37:58, max mem: 20.9 GB 
[11/28 18:25:04 visual_prompt]: Epoch 23 / 100: avg data time: 2.26e-01, avg batch time: 1.0485, average train loss: 1.2626
[11/28 18:26:04 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3069, average loss: 0.9033
[11/28 18:26:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.60	rocauc: 69.94	
[11/28 18:26:04 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[11/28 18:27:49 visual_prompt]: 	Training 100/553. train loss: 0.4386,	0.8327 s / batch. (data: 1.56e-02). ETA=9:49:34, max mem: 20.9 GB 
[11/28 18:29:32 visual_prompt]: 	Training 200/553. train loss: 1.4972,	0.8349 s / batch. (data: 4.12e-04). ETA=9:49:42, max mem: 20.9 GB 
[11/28 18:31:17 visual_prompt]: 	Training 300/553. train loss: 0.7559,	0.9670 s / batch. (data: 1.61e-01). ETA=11:21:26, max mem: 20.9 GB 
[11/28 18:33:02 visual_prompt]: 	Training 400/553. train loss: 1.0931,	0.8249 s / batch. (data: 3.78e-04). ETA=9:39:53, max mem: 20.9 GB 
[11/28 18:34:47 visual_prompt]: 	Training 500/553. train loss: 0.9253,	0.8197 s / batch. (data: 3.92e-04). ETA=9:34:52, max mem: 20.9 GB 
[11/28 18:35:42 visual_prompt]: Epoch 24 / 100: avg data time: 2.24e-01, avg batch time: 1.0462, average train loss: 1.4870
[11/28 18:36:42 visual_prompt]: Inference (val):avg data time: 3.65e-04, avg batch time: 0.3061, average loss: 1.2143
[11/28 18:36:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 69.19	
[11/28 18:36:42 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[11/28 18:38:34 visual_prompt]: 	Training 100/553. train loss: 2.9093,	0.8278 s / batch. (data: 8.01e-03). ETA=9:38:26, max mem: 20.9 GB 
[11/28 18:40:15 visual_prompt]: 	Training 200/553. train loss: 1.0024,	0.8480 s / batch. (data: 3.15e-04). ETA=9:51:10, max mem: 20.9 GB 
[11/28 18:41:57 visual_prompt]: 	Training 300/553. train loss: 0.7224,	0.9000 s / batch. (data: 6.84e-02). ETA=10:25:55, max mem: 20.9 GB 
[11/28 18:43:41 visual_prompt]: 	Training 400/553. train loss: 0.4022,	1.4640 s / batch. (data: 6.50e-01). ETA=16:55:44, max mem: 20.9 GB 
[11/28 18:45:27 visual_prompt]: 	Training 500/553. train loss: 0.8279,	1.7481 s / batch. (data: 9.19e-01). ETA=20:09:53, max mem: 20.9 GB 
[11/28 18:46:20 visual_prompt]: Epoch 25 / 100: avg data time: 2.23e-01, avg batch time: 1.0456, average train loss: 1.5533
[11/28 18:47:20 visual_prompt]: Inference (val):avg data time: 2.22e-04, avg batch time: 0.3071, average loss: 1.8714
[11/28 18:47:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 69.58	
[11/28 18:47:20 visual_prompt]: Stopping early.
[11/28 18:47:20 visual_prompt]: Rank of current process: 0. World size: 1
[11/28 18:47:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/28 18:47:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/28 18:47:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/28 18:47:20 visual_prompt]: Training with config:
[11/28 18:47:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.5_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/28 18:47:20 visual_prompt]: Loading training data...
[11/28 18:47:20 visual_prompt]: Constructing mammo-cbis dataset train...
[11/28 18:47:20 visual_prompt]: Loading validation data...
[11/28 18:47:20 visual_prompt]: Constructing mammo-cbis dataset val...
[11/28 18:47:20 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/28 18:47:23 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/28 18:47:23 visual_prompt]: tuned percent:0.525
[11/28 18:47:23 visual_prompt]: Device used for model: 0
[11/28 18:47:23 visual_prompt]: Setting up Evaluator...
[11/28 18:47:23 visual_prompt]: Setting up Trainer...
[11/28 18:47:23 visual_prompt]: 	Setting up the optimizer...
[11/28 18:47:24 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/28 18:49:12 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8537 s / batch. (data: 2.90e-02). ETA=13:05:23, max mem: 20.9 GB 
[11/28 18:50:53 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8097 s / batch. (data: 3.20e-04). ETA=12:23:36, max mem: 20.9 GB 
[11/28 18:52:40 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.3000 s / batch. (data: 4.65e-01). ETA=19:51:39, max mem: 20.9 GB 
[11/28 18:54:22 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8166 s / batch. (data: 3.71e-04). ETA=12:27:11, max mem: 20.9 GB 
[11/28 18:56:08 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8200 s / batch. (data: 3.04e-04). ETA=12:28:55, max mem: 20.9 GB 
[11/28 18:57:03 visual_prompt]: Epoch 1 / 100: avg data time: 2.24e-01, avg batch time: 1.0470, average train loss: 1.5403
[11/28 18:58:03 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.3079, average loss: 1.5201
[11/28 18:58:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/28 18:58:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[11/28 18:59:51 visual_prompt]: 	Training 100/553. train loss: 0.7700,	0.9462 s / batch. (data: 1.14e-01). ETA=14:21:44, max mem: 20.9 GB 
[11/28 19:01:34 visual_prompt]: 	Training 200/553. train loss: 0.1688,	0.8292 s / batch. (data: 4.12e-04). ETA=12:33:47, max mem: 20.9 GB 
[11/28 19:03:20 visual_prompt]: 	Training 300/553. train loss: 0.9670,	1.1881 s / batch. (data: 3.60e-01). ETA=17:58:07, max mem: 20.9 GB 
[11/28 19:05:03 visual_prompt]: 	Training 400/553. train loss: 1.0565,	0.8320 s / batch. (data: 4.64e-04). ETA=12:33:36, max mem: 20.9 GB 
[11/28 19:06:47 visual_prompt]: 	Training 500/553. train loss: 0.6524,	0.8063 s / batch. (data: 3.05e-04). ETA=12:08:57, max mem: 20.9 GB 
[11/28 19:07:40 visual_prompt]: Epoch 2 / 100: avg data time: 2.22e-01, avg batch time: 1.0447, average train loss: 0.8609
[11/28 19:08:40 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3058, average loss: 0.7371
[11/28 19:08:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.87	
[11/28 19:08:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[11/28 19:10:26 visual_prompt]: 	Training 100/553. train loss: 0.7855,	0.8307 s / batch. (data: 3.69e-04). ETA=12:28:54, max mem: 20.9 GB 
[11/28 19:12:11 visual_prompt]: 	Training 200/553. train loss: 0.8414,	0.8141 s / batch. (data: 7.95e-03). ETA=12:12:39, max mem: 20.9 GB 
[11/28 19:13:54 visual_prompt]: 	Training 300/553. train loss: 0.6527,	0.8178 s / batch. (data: 3.97e-04). ETA=12:14:36, max mem: 20.9 GB 
[11/28 19:15:38 visual_prompt]: 	Training 400/553. train loss: 0.9671,	0.8503 s / batch. (data: 6.25e-03). ETA=12:42:19, max mem: 20.9 GB 
[11/28 19:17:24 visual_prompt]: 	Training 500/553. train loss: 0.7584,	1.4526 s / batch. (data: 6.48e-01). ETA=21:39:55, max mem: 20.9 GB 
[11/28 19:18:16 visual_prompt]: Epoch 3 / 100: avg data time: 2.19e-01, avg batch time: 1.0420, average train loss: 0.7676
[11/28 19:19:16 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3055, average loss: 0.7342
[11/28 19:19:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.38	
[11/28 19:19:16 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[11/28 19:21:05 visual_prompt]: 	Training 100/553. train loss: 0.8624,	0.8320 s / batch. (data: 3.26e-04). ETA=12:22:28, max mem: 20.9 GB 
[11/28 19:22:49 visual_prompt]: 	Training 200/553. train loss: 0.9227,	0.8263 s / batch. (data: 1.20e-02). ETA=12:15:59, max mem: 20.9 GB 
[11/28 19:24:33 visual_prompt]: 	Training 300/553. train loss: 0.7043,	1.4080 s / batch. (data: 5.78e-01). ETA=20:51:43, max mem: 20.9 GB 
[11/28 19:26:13 visual_prompt]: 	Training 400/553. train loss: 0.6346,	1.3608 s / batch. (data: 5.36e-01). ETA=20:07:29, max mem: 20.9 GB 
[11/28 19:28:00 visual_prompt]: 	Training 500/553. train loss: 0.1718,	3.5000 s / batch. (data: 2.67e+00). ETA=2 days, 3:39:55, max mem: 20.9 GB 
[11/28 19:28:55 visual_prompt]: Epoch 4 / 100: avg data time: 2.24e-01, avg batch time: 1.0464, average train loss: 0.9653
[11/28 19:29:54 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3055, average loss: 1.0900
[11/28 19:29:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.41	
[11/28 19:29:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[11/28 19:31:41 visual_prompt]: 	Training 100/553. train loss: 2.2221,	0.8200 s / batch. (data: 2.90e-04). ETA=12:04:10, max mem: 20.9 GB 
[11/28 19:33:25 visual_prompt]: 	Training 200/553. train loss: 0.6535,	1.2990 s / batch. (data: 4.81e-01). ETA=19:05:01, max mem: 20.9 GB 
[11/28 19:35:10 visual_prompt]: 	Training 300/553. train loss: 1.4512,	0.8407 s / batch. (data: 4.10e-04). ETA=12:19:41, max mem: 20.9 GB 
[11/28 19:36:54 visual_prompt]: 	Training 400/553. train loss: 2.0240,	0.8425 s / batch. (data: 3.12e-04). ETA=12:19:47, max mem: 20.9 GB 
[11/28 19:38:38 visual_prompt]: 	Training 500/553. train loss: 0.5817,	0.8600 s / batch. (data: 2.90e-04). ETA=12:33:46, max mem: 20.9 GB 
[11/28 19:39:33 visual_prompt]: Epoch 5 / 100: avg data time: 2.24e-01, avg batch time: 1.0459, average train loss: 0.9193
[11/28 19:40:33 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3057, average loss: 1.2579
[11/28 19:40:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.52	
[11/28 19:40:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[11/28 19:42:22 visual_prompt]: 	Training 100/553. train loss: 0.7583,	0.8240 s / batch. (data: 1.15e-03). ETA=12:00:04, max mem: 20.9 GB 
[11/28 19:44:05 visual_prompt]: 	Training 200/553. train loss: 2.2897,	0.8320 s / batch. (data: 5.65e-03). ETA=12:05:41, max mem: 20.9 GB 
[11/28 19:45:48 visual_prompt]: 	Training 300/553. train loss: 0.5654,	0.8373 s / batch. (data: 4.20e-04). ETA=12:08:54, max mem: 20.9 GB 
[11/28 19:47:36 visual_prompt]: 	Training 400/553. train loss: 0.5669,	0.8258 s / batch. (data: 6.39e-03). ETA=11:57:30, max mem: 20.9 GB 
[11/28 19:49:18 visual_prompt]: 	Training 500/553. train loss: 0.7552,	0.8319 s / batch. (data: 7.39e-03). ETA=12:01:30, max mem: 20.9 GB 
[11/28 19:50:11 visual_prompt]: Epoch 6 / 100: avg data time: 2.23e-01, avg batch time: 1.0460, average train loss: 0.9855
[11/28 19:51:11 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3072, average loss: 0.9076
[11/28 19:51:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.66	
[11/28 19:51:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[11/28 19:52:58 visual_prompt]: 	Training 100/553. train loss: 2.4193,	0.8280 s / batch. (data: 3.01e-04). ETA=11:56:00, max mem: 20.9 GB 
[11/28 19:54:42 visual_prompt]: 	Training 200/553. train loss: 0.5644,	0.8440 s / batch. (data: 3.17e-04). ETA=12:08:25, max mem: 20.9 GB 
[11/28 19:56:30 visual_prompt]: 	Training 300/553. train loss: 0.5432,	2.0485 s / batch. (data: 1.24e+00). ETA=1 day, 5:24:31, max mem: 20.9 GB 
[11/28 19:58:14 visual_prompt]: 	Training 400/553. train loss: 0.5758,	2.3200 s / batch. (data: 1.50e+00). ETA=1 day, 9:14:31, max mem: 20.9 GB 
[11/28 19:59:57 visual_prompt]: 	Training 500/553. train loss: 1.1166,	0.8217 s / batch. (data: 1.20e-02). ETA=11:45:03, max mem: 20.9 GB 
[11/28 20:00:50 visual_prompt]: Epoch 7 / 100: avg data time: 2.23e-01, avg batch time: 1.0465, average train loss: 1.0383
[11/28 20:01:50 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.3071, average loss: 0.7151
[11/28 20:01:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.52	
[11/28 20:01:50 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[11/28 20:03:36 visual_prompt]: 	Training 100/553. train loss: 2.1714,	1.3320 s / batch. (data: 4.92e-01). ETA=18:59:30, max mem: 20.9 GB 
[11/28 20:05:21 visual_prompt]: 	Training 200/553. train loss: 0.6697,	0.8215 s / batch. (data: 3.14e-04). ETA=11:41:24, max mem: 20.9 GB 
[11/28 20:07:08 visual_prompt]: 	Training 300/553. train loss: 0.6618,	0.8107 s / batch. (data: 3.83e-04). ETA=11:30:48, max mem: 20.9 GB 
[11/28 20:08:53 visual_prompt]: 	Training 400/553. train loss: 0.6970,	1.2347 s / batch. (data: 4.02e-01). ETA=17:30:04, max mem: 20.9 GB 
[11/28 20:10:39 visual_prompt]: 	Training 500/553. train loss: 3.6742,	1.5662 s / batch. (data: 7.38e-01). ETA=22:09:24, max mem: 20.9 GB 
[11/28 20:11:34 visual_prompt]: Epoch 8 / 100: avg data time: 2.34e-01, avg batch time: 1.0558, average train loss: 1.1705
[11/28 20:12:33 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3063, average loss: 0.9263
[11/28 20:12:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.42	
[11/28 20:12:33 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[11/28 20:14:21 visual_prompt]: 	Training 100/553. train loss: 0.3339,	0.8067 s / batch. (data: 4.13e-04). ETA=11:22:42, max mem: 20.9 GB 
[11/28 20:16:04 visual_prompt]: 	Training 200/553. train loss: 0.8443,	0.8160 s / batch. (data: 4.60e-04). ETA=11:29:09, max mem: 20.9 GB 
[11/28 20:17:48 visual_prompt]: 	Training 300/553. train loss: 0.6550,	1.8590 s / batch. (data: 1.05e+00). ETA=1 day, 2:07:01, max mem: 20.9 GB 
[11/28 20:19:33 visual_prompt]: 	Training 400/553. train loss: 0.9674,	0.8560 s / batch. (data: 4.01e-04). ETA=12:00:05, max mem: 20.9 GB 
[11/28 20:21:17 visual_prompt]: 	Training 500/553. train loss: 0.7006,	1.1400 s / batch. (data: 3.05e-01). ETA=15:57:06, max mem: 20.9 GB 
[11/28 20:22:10 visual_prompt]: Epoch 9 / 100: avg data time: 2.21e-01, avg batch time: 1.0433, average train loss: 1.0496
[11/28 20:23:11 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3085, average loss: 0.6961
[11/28 20:23:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.60	
[11/28 20:23:11 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[11/28 20:25:02 visual_prompt]: 	Training 100/553. train loss: 3.6842,	0.8194 s / batch. (data: 3.60e-04). ETA=11:25:51, max mem: 20.9 GB 
[11/28 20:26:45 visual_prompt]: 	Training 200/553. train loss: 2.0461,	0.8617 s / batch. (data: 2.57e-02). ETA=11:59:50, max mem: 20.9 GB 
[11/28 20:28:29 visual_prompt]: 	Training 300/553. train loss: 3.3785,	0.8320 s / batch. (data: 3.12e-04). ETA=11:33:38, max mem: 20.9 GB 
[11/28 20:30:09 visual_prompt]: 	Training 400/553. train loss: 0.9255,	1.0641 s / batch. (data: 2.50e-01). ETA=14:45:24, max mem: 20.9 GB 
[11/28 20:31:55 visual_prompt]: 	Training 500/553. train loss: 0.5967,	1.0145 s / batch. (data: 2.09e-01). ETA=14:02:23, max mem: 20.9 GB 
[11/28 20:32:49 visual_prompt]: Epoch 10 / 100: avg data time: 2.25e-01, avg batch time: 1.0463, average train loss: 1.3934
[11/28 20:33:49 visual_prompt]: Inference (val):avg data time: 3.14e-04, avg batch time: 0.3074, average loss: 0.7303
[11/28 20:33:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.89	
[11/28 20:33:49 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[11/28 20:35:41 visual_prompt]: 	Training 100/553. train loss: 0.8313,	0.8042 s / batch. (data: 3.71e-04). ETA=11:05:43, max mem: 20.9 GB 
[11/28 20:37:28 visual_prompt]: 	Training 200/553. train loss: 0.6824,	0.8306 s / batch. (data: 9.61e-04). ETA=11:26:11, max mem: 20.9 GB 
[11/28 20:39:11 visual_prompt]: 	Training 300/553. train loss: 0.1487,	2.2683 s / batch. (data: 1.44e+00). ETA=1 day, 7:10:10, max mem: 20.9 GB 
[11/28 20:40:55 visual_prompt]: 	Training 400/553. train loss: 0.6376,	0.8186 s / batch. (data: 2.96e-04). ETA=11:13:33, max mem: 20.9 GB 
[11/28 20:42:38 visual_prompt]: 	Training 500/553. train loss: 1.5787,	0.8136 s / batch. (data: 3.24e-04). ETA=11:08:06, max mem: 20.9 GB 
[11/28 20:43:31 visual_prompt]: Epoch 11 / 100: avg data time: 2.29e-01, avg batch time: 1.0523, average train loss: 1.1421
[11/28 20:44:31 visual_prompt]: Inference (val):avg data time: 1.46e-04, avg batch time: 0.3076, average loss: 1.3272
[11/28 20:44:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.44	
[11/28 20:44:31 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[11/28 20:46:20 visual_prompt]: 	Training 100/553. train loss: 0.7520,	0.8320 s / batch. (data: 4.54e-04). ETA=11:21:05, max mem: 20.9 GB 
[11/28 20:48:05 visual_prompt]: 	Training 200/553. train loss: 0.8649,	1.0092 s / batch. (data: 1.76e-01). ETA=13:44:30, max mem: 20.9 GB 
[11/28 20:49:48 visual_prompt]: 	Training 300/553. train loss: 1.3444,	0.8077 s / batch. (data: 2.78e-04). ETA=10:58:30, max mem: 20.9 GB 
[11/28 20:51:33 visual_prompt]: 	Training 400/553. train loss: 0.7673,	0.8293 s / batch. (data: 1.05e-02). ETA=11:14:45, max mem: 20.9 GB 
[11/28 20:53:18 visual_prompt]: 	Training 500/553. train loss: 1.1665,	0.8420 s / batch. (data: 1.05e-02). ETA=11:23:38, max mem: 20.9 GB 
[11/28 20:54:12 visual_prompt]: Epoch 12 / 100: avg data time: 2.27e-01, avg batch time: 1.0503, average train loss: 1.3947
[11/28 20:55:11 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3060, average loss: 0.8203
[11/28 20:55:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.77	
[11/28 20:55:11 visual_prompt]: Best epoch 12: best metric: -0.820
[11/28 20:55:11 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[11/28 20:57:01 visual_prompt]: 	Training 100/553. train loss: 0.7714,	1.0922 s / batch. (data: 2.87e-01). ETA=14:43:59, max mem: 20.9 GB 
[11/28 20:58:44 visual_prompt]: 	Training 200/553. train loss: 0.7119,	0.8064 s / batch. (data: 3.11e-04). ETA=10:51:20, max mem: 20.9 GB 
[11/28 21:00:31 visual_prompt]: 	Training 300/553. train loss: 0.6117,	1.8800 s / batch. (data: 1.06e+00). ETA=1 day, 1:15:23, max mem: 20.9 GB 
[11/28 21:02:14 visual_prompt]: 	Training 400/553. train loss: 3.0195,	0.8079 s / batch. (data: 4.13e-04). ETA=10:49:54, max mem: 20.9 GB 
[11/28 21:04:00 visual_prompt]: 	Training 500/553. train loss: 2.4177,	0.8281 s / batch. (data: 2.84e-04). ETA=11:04:42, max mem: 20.9 GB 
[11/28 21:04:54 visual_prompt]: Epoch 13 / 100: avg data time: 2.32e-01, avg batch time: 1.0540, average train loss: 1.3389
[11/28 21:05:54 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.3049, average loss: 0.8071
[11/28 21:05:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 45.86	
[11/28 21:05:54 visual_prompt]: Best epoch 13: best metric: -0.807
[11/28 21:05:54 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[11/28 21:07:43 visual_prompt]: 	Training 100/553. train loss: 0.5203,	0.8240 s / batch. (data: 1.23e-03). ETA=10:59:20, max mem: 20.9 GB 
[11/28 21:09:29 visual_prompt]: 	Training 200/553. train loss: 0.1359,	1.5267 s / batch. (data: 7.20e-01). ETA=20:19:04, max mem: 20.9 GB 
[11/28 21:11:12 visual_prompt]: 	Training 300/553. train loss: 0.7159,	0.8242 s / batch. (data: 1.47e-02). ETA=10:56:43, max mem: 20.9 GB 
[11/28 21:12:56 visual_prompt]: 	Training 400/553. train loss: 0.4982,	0.8423 s / batch. (data: 1.02e-02). ETA=11:09:48, max mem: 20.9 GB 
[11/28 21:14:41 visual_prompt]: 	Training 500/553. train loss: 1.7130,	0.8333 s / batch. (data: 1.19e-02). ETA=11:01:13, max mem: 20.9 GB 
[11/28 21:15:36 visual_prompt]: Epoch 14 / 100: avg data time: 2.29e-01, avg batch time: 1.0525, average train loss: 1.2353
[11/28 21:16:36 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.3140, average loss: 0.8232
[11/28 21:16:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.43	
[11/28 21:16:36 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[11/28 21:18:25 visual_prompt]: 	Training 100/553. train loss: 0.7051,	0.8568 s / batch. (data: 3.28e-02). ETA=11:17:42, max mem: 20.9 GB 
[11/28 21:20:08 visual_prompt]: 	Training 200/553. train loss: 5.6507,	0.8360 s / batch. (data: 4.26e-04). ETA=10:59:50, max mem: 20.9 GB 
[11/28 21:21:54 visual_prompt]: 	Training 300/553. train loss: 1.0801,	0.8360 s / batch. (data: 4.02e-04). ETA=10:58:25, max mem: 20.9 GB 
[11/28 21:23:36 visual_prompt]: 	Training 400/553. train loss: 1.5442,	1.4819 s / batch. (data: 6.31e-01). ETA=19:24:43, max mem: 20.9 GB 
[11/28 21:25:22 visual_prompt]: 	Training 500/553. train loss: 0.6505,	0.8240 s / batch. (data: 3.95e-04). ETA=10:46:15, max mem: 20.9 GB 
[11/28 21:26:16 visual_prompt]: Epoch 15 / 100: avg data time: 2.24e-01, avg batch time: 1.0482, average train loss: 1.3959
[11/28 21:27:16 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.3048, average loss: 1.1552
[11/28 21:27:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.79	
[11/28 21:27:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[11/28 21:29:05 visual_prompt]: 	Training 100/553. train loss: 3.9407,	0.8320 s / batch. (data: 7.95e-03). ETA=10:50:24, max mem: 20.9 GB 
[11/28 21:30:49 visual_prompt]: 	Training 200/553. train loss: 2.2138,	0.8294 s / batch. (data: 9.35e-03). ETA=10:47:01, max mem: 20.9 GB 
[11/28 21:32:34 visual_prompt]: 	Training 300/553. train loss: 0.6724,	0.8158 s / batch. (data: 4.25e-04). ETA=10:35:03, max mem: 20.9 GB 
[11/28 21:34:19 visual_prompt]: 	Training 400/553. train loss: 1.2174,	0.8428 s / batch. (data: 9.35e-04). ETA=10:54:37, max mem: 20.9 GB 
[11/28 21:36:02 visual_prompt]: 	Training 500/553. train loss: 0.8232,	1.2426 s / batch. (data: 4.13e-01). ETA=16:03:08, max mem: 20.9 GB 
[11/28 21:36:57 visual_prompt]: Epoch 16 / 100: avg data time: 2.27e-01, avg batch time: 1.0514, average train loss: 1.4743
[11/28 21:37:57 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3054, average loss: 0.6890
[11/28 21:37:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.91	
[11/28 21:37:57 visual_prompt]: Best epoch 16: best metric: -0.689
[11/28 21:37:57 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[11/28 21:39:44 visual_prompt]: 	Training 100/553. train loss: 0.8575,	0.8698 s / batch. (data: 1.56e-02). ETA=11:11:56, max mem: 20.9 GB 
[11/28 21:41:29 visual_prompt]: 	Training 200/553. train loss: 3.3039,	0.8149 s / batch. (data: 3.79e-04). ETA=10:28:12, max mem: 20.9 GB 
[11/28 21:43:14 visual_prompt]: 	Training 300/553. train loss: 0.9985,	0.8050 s / batch. (data: 3.08e-04). ETA=10:19:11, max mem: 20.9 GB 
[11/28 21:44:58 visual_prompt]: 	Training 400/553. train loss: 2.4057,	1.6121 s / batch. (data: 7.80e-01). ETA=20:37:20, max mem: 20.9 GB 
[11/28 21:46:42 visual_prompt]: 	Training 500/553. train loss: 0.5941,	1.3267 s / batch. (data: 4.89e-01). ETA=16:56:03, max mem: 20.9 GB 
[11/28 21:47:37 visual_prompt]: Epoch 17 / 100: avg data time: 2.26e-01, avg batch time: 1.0494, average train loss: 1.4583
[11/28 21:48:38 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3059, average loss: 1.1845
[11/28 21:48:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.50	
[11/28 21:48:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[11/28 21:50:26 visual_prompt]: 	Training 100/553. train loss: 0.8823,	0.8237 s / batch. (data: 7.61e-03). ETA=10:28:43, max mem: 20.9 GB 
[11/28 21:52:14 visual_prompt]: 	Training 200/553. train loss: 2.5699,	0.8480 s / batch. (data: 1.08e-03). ETA=10:45:51, max mem: 20.9 GB 
[11/28 21:53:59 visual_prompt]: 	Training 300/553. train loss: 0.5949,	0.8480 s / batch. (data: 3.24e-04). ETA=10:44:28, max mem: 20.9 GB 
[11/28 21:55:44 visual_prompt]: 	Training 400/553. train loss: 0.9589,	0.8336 s / batch. (data: 1.19e-02). ETA=10:32:08, max mem: 20.9 GB 
[11/28 21:57:26 visual_prompt]: 	Training 500/553. train loss: 0.9227,	0.8390 s / batch. (data: 3.30e-04). ETA=10:34:48, max mem: 20.9 GB 
[11/28 21:58:20 visual_prompt]: Epoch 18 / 100: avg data time: 2.29e-01, avg batch time: 1.0520, average train loss: 1.4059
[11/28 21:59:19 visual_prompt]: Inference (val):avg data time: 3.96e-05, avg batch time: 0.3081, average loss: 1.4272
[11/28 21:59:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.14	
[11/28 21:59:19 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[11/28 22:01:07 visual_prompt]: 	Training 100/553. train loss: 0.5746,	0.8257 s / batch. (data: 4.35e-04). ETA=10:22:39, max mem: 20.9 GB 
[11/28 22:02:53 visual_prompt]: 	Training 200/553. train loss: 0.7530,	0.8200 s / batch. (data: 3.26e-04). ETA=10:16:59, max mem: 20.9 GB 
[11/28 22:04:37 visual_prompt]: 	Training 300/553. train loss: 0.3898,	0.8319 s / batch. (data: 4.53e-04). ETA=10:24:34, max mem: 20.9 GB 
[11/28 22:06:23 visual_prompt]: 	Training 400/553. train loss: 0.9271,	0.8321 s / batch. (data: 3.02e-04). ETA=10:23:18, max mem: 20.9 GB 
[11/28 22:08:02 visual_prompt]: 	Training 500/553. train loss: 0.5652,	0.8208 s / batch. (data: 4.25e-04). ETA=10:13:28, max mem: 20.9 GB 
[11/28 22:08:58 visual_prompt]: Epoch 19 / 100: avg data time: 2.24e-01, avg batch time: 1.0465, average train loss: 1.3115
[11/28 22:09:58 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3075, average loss: 5.8312
[11/28 22:09:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.63	
[11/28 22:09:58 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[11/28 22:11:43 visual_prompt]: 	Training 100/553. train loss: 0.7514,	0.8186 s / batch. (data: 3.38e-04). ETA=10:09:46, max mem: 20.9 GB 
[11/28 22:13:29 visual_prompt]: 	Training 200/553. train loss: 0.6134,	0.8273 s / batch. (data: 1.07e-02). ETA=10:14:51, max mem: 20.9 GB 
[11/28 22:15:14 visual_prompt]: 	Training 300/553. train loss: 0.6458,	0.8091 s / batch. (data: 3.43e-04). ETA=9:59:57, max mem: 20.9 GB 
[11/28 22:17:00 visual_prompt]: 	Training 400/553. train loss: 1.0217,	0.8440 s / batch. (data: 3.60e-04). ETA=10:24:27, max mem: 20.9 GB 
[11/28 22:18:44 visual_prompt]: 	Training 500/553. train loss: 1.3167,	0.8360 s / batch. (data: 3.99e-03). ETA=10:17:09, max mem: 20.9 GB 
[11/28 22:19:46 visual_prompt]: Epoch 20 / 100: avg data time: 2.40e-01, avg batch time: 1.0629, average train loss: 1.2826
[11/28 22:20:45 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.3058, average loss: 0.6973
[11/28 22:20:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.44	
[11/28 22:20:45 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[11/28 22:22:36 visual_prompt]: 	Training 100/553. train loss: 1.3835,	0.8232 s / batch. (data: 5.53e-03). ETA=10:05:34, max mem: 20.9 GB 
[11/28 22:24:20 visual_prompt]: 	Training 200/553. train loss: 1.9942,	0.8341 s / batch. (data: 5.49e-03). ETA=10:12:12, max mem: 20.9 GB 
[11/28 22:26:03 visual_prompt]: 	Training 300/553. train loss: 1.2501,	0.8338 s / batch. (data: 5.45e-03). ETA=10:10:36, max mem: 20.9 GB 
[11/28 22:27:48 visual_prompt]: 	Training 400/553. train loss: 1.1687,	0.8187 s / batch. (data: 5.44e-03). ETA=9:58:09, max mem: 20.9 GB 
[11/28 22:29:34 visual_prompt]: 	Training 500/553. train loss: 1.5510,	0.8106 s / batch. (data: 4.08e-04). ETA=9:50:56, max mem: 20.9 GB 
[11/28 22:30:28 visual_prompt]: Epoch 21 / 100: avg data time: 2.31e-01, avg batch time: 1.0527, average train loss: 1.3929
[11/28 22:31:27 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3052, average loss: 0.8058
[11/28 22:31:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.46	
[11/28 22:31:27 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[11/28 22:33:15 visual_prompt]: 	Training 100/553. train loss: 0.7388,	0.8272 s / batch. (data: 5.42e-03). ETA=10:00:53, max mem: 20.9 GB 
[11/28 22:35:00 visual_prompt]: 	Training 200/553. train loss: 0.6400,	0.8223 s / batch. (data: 2.98e-04). ETA=9:55:59, max mem: 20.9 GB 
[11/28 22:36:43 visual_prompt]: 	Training 300/553. train loss: 0.2082,	0.8535 s / batch. (data: 1.60e-02). ETA=10:17:12, max mem: 20.9 GB 
[11/28 22:38:28 visual_prompt]: 	Training 400/553. train loss: 3.0826,	0.8138 s / batch. (data: 2.87e-04). ETA=9:47:08, max mem: 20.9 GB 
[11/28 22:40:12 visual_prompt]: 	Training 500/553. train loss: 0.8451,	0.8400 s / batch. (data: 3.17e-04). ETA=10:04:36, max mem: 20.9 GB 
[11/28 22:41:08 visual_prompt]: Epoch 22 / 100: avg data time: 2.27e-01, avg batch time: 1.0491, average train loss: 1.4194
[11/28 22:42:08 visual_prompt]: Inference (val):avg data time: 5.32e-05, avg batch time: 0.3122, average loss: 1.3268
[11/28 22:42:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.88	
[11/28 22:42:08 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[11/28 22:43:58 visual_prompt]: 	Training 100/553. train loss: 0.9541,	0.8280 s / batch. (data: 3.15e-04). ETA=9:53:53, max mem: 20.9 GB 
[11/28 22:45:43 visual_prompt]: 	Training 200/553. train loss: 0.5850,	1.1532 s / batch. (data: 3.35e-01). ETA=13:45:09, max mem: 20.9 GB 
[11/28 22:47:30 visual_prompt]: 	Training 300/553. train loss: 0.5825,	0.8373 s / batch. (data: 5.43e-03). ETA=9:57:45, max mem: 20.9 GB 
[11/28 22:49:12 visual_prompt]: 	Training 400/553. train loss: 0.9522,	0.8064 s / batch. (data: 4.10e-04). ETA=9:34:19, max mem: 20.9 GB 
[11/28 22:50:53 visual_prompt]: 	Training 500/553. train loss: 0.2300,	0.8227 s / batch. (data: 4.49e-04). ETA=9:44:34, max mem: 20.9 GB 
[11/28 22:51:48 visual_prompt]: Epoch 23 / 100: avg data time: 2.24e-01, avg batch time: 1.0472, average train loss: 1.2357
[11/28 22:52:47 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.3078, average loss: 0.9518
[11/28 22:52:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.56	
[11/28 22:52:47 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[11/28 22:54:33 visual_prompt]: 	Training 100/553. train loss: 1.1287,	0.8086 s / batch. (data: 3.11e-04). ETA=9:32:29, max mem: 20.9 GB 
[11/28 22:56:18 visual_prompt]: 	Training 200/553. train loss: 0.8943,	0.8446 s / batch. (data: 1.20e-02). ETA=9:56:34, max mem: 20.9 GB 
[11/28 22:58:03 visual_prompt]: 	Training 300/553. train loss: 0.8214,	1.1523 s / batch. (data: 3.27e-01). ETA=13:32:01, max mem: 20.9 GB 
[11/28 22:59:48 visual_prompt]: 	Training 400/553. train loss: 0.5857,	0.8343 s / batch. (data: 1.08e-02). ETA=9:46:33, max mem: 20.9 GB 
[11/28 23:01:35 visual_prompt]: 	Training 500/553. train loss: 0.7096,	0.8200 s / batch. (data: 3.55e-04). ETA=9:35:05, max mem: 20.9 GB 
[11/28 23:02:30 visual_prompt]: Epoch 24 / 100: avg data time: 2.32e-01, avg batch time: 1.0538, average train loss: 1.4594
[11/28 23:03:30 visual_prompt]: Inference (val):avg data time: 1.59e-04, avg batch time: 0.3049, average loss: 0.6917
[11/28 23:03:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.10	
[11/28 23:03:30 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[11/28 23:05:22 visual_prompt]: 	Training 100/553. train loss: 0.8401,	0.8215 s / batch. (data: 2.96e-04). ETA=9:34:05, max mem: 20.9 GB 
[11/28 23:07:02 visual_prompt]: 	Training 200/553. train loss: 0.6188,	1.1320 s / batch. (data: 3.20e-01). ETA=13:09:11, max mem: 20.9 GB 
[11/28 23:08:47 visual_prompt]: 	Training 300/553. train loss: 1.1996,	0.8126 s / batch. (data: 5.43e-03). ETA=9:25:09, max mem: 20.9 GB 
[11/28 23:10:33 visual_prompt]: 	Training 400/553. train loss: 0.6409,	1.9038 s / batch. (data: 1.06e+00). ETA=22:00:49, max mem: 20.9 GB 
[11/28 23:12:18 visual_prompt]: 	Training 500/553. train loss: 1.3159,	1.6227 s / batch. (data: 7.91e-01). ETA=18:43:07, max mem: 20.9 GB 
[11/28 23:13:12 visual_prompt]: Epoch 25 / 100: avg data time: 2.31e-01, avg batch time: 1.0530, average train loss: 1.3519
[11/28 23:14:12 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3057, average loss: 1.5980
[11/28 23:14:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.26	
[11/28 23:14:12 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[11/28 23:16:01 visual_prompt]: 	Training 100/553. train loss: 2.1431,	0.8196 s / batch. (data: 4.35e-04). ETA=9:25:11, max mem: 20.9 GB 
[11/28 23:17:47 visual_prompt]: 	Training 200/553. train loss: 0.6065,	2.1491 s / batch. (data: 1.33e+00). ETA=1 day, 0:38:24, max mem: 20.9 GB 
[11/28 23:19:32 visual_prompt]: 	Training 300/553. train loss: 1.4141,	0.8061 s / batch. (data: 3.27e-04). ETA=9:13:10, max mem: 20.9 GB 
[11/28 23:21:15 visual_prompt]: 	Training 400/553. train loss: 0.6268,	0.8440 s / batch. (data: 1.20e-02). ETA=9:37:45, max mem: 20.9 GB 
[11/28 23:22:57 visual_prompt]: 	Training 500/553. train loss: 1.7169,	0.8247 s / batch. (data: 7.96e-03). ETA=9:23:11, max mem: 20.9 GB 
[11/28 23:23:51 visual_prompt]: Epoch 26 / 100: avg data time: 2.25e-01, avg batch time: 1.0475, average train loss: 1.1621
[11/28 23:24:51 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.3057, average loss: 1.3134
[11/28 23:24:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.72	
[11/28 23:24:51 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[11/28 23:26:42 visual_prompt]: 	Training 100/553. train loss: 0.9940,	0.8426 s / batch. (data: 1.59e-02). ETA=9:33:17, max mem: 20.9 GB 
[11/28 23:28:26 visual_prompt]: 	Training 200/553. train loss: 4.5847,	1.5756 s / batch. (data: 7.29e-01). ETA=17:49:21, max mem: 20.9 GB 
[11/28 23:30:11 visual_prompt]: 	Training 300/553. train loss: 1.9995,	0.8239 s / batch. (data: 4.70e-04). ETA=9:17:50, max mem: 20.9 GB 
[11/28 23:31:56 visual_prompt]: 	Training 400/553. train loss: 0.6904,	0.8275 s / batch. (data: 4.18e-04). ETA=9:18:51, max mem: 20.9 GB 
[11/28 23:33:41 visual_prompt]: 	Training 500/553. train loss: 0.5809,	0.8235 s / batch. (data: 3.40e-04). ETA=9:14:48, max mem: 20.9 GB 
[11/28 23:34:33 visual_prompt]: Epoch 27 / 100: avg data time: 2.28e-01, avg batch time: 1.0512, average train loss: 1.3266
[11/28 23:35:33 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3064, average loss: 1.2304
[11/28 23:35:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.60	
[11/28 23:35:33 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[11/28 23:37:21 visual_prompt]: 	Training 100/553. train loss: 0.0578,	0.8075 s / batch. (data: 4.39e-04). ETA=9:01:55, max mem: 20.9 GB 
[11/28 23:39:05 visual_prompt]: 	Training 200/553. train loss: 3.9587,	0.8284 s / batch. (data: 3.89e-04). ETA=9:14:34, max mem: 20.9 GB 
[11/28 23:40:52 visual_prompt]: 	Training 300/553. train loss: 0.9422,	1.6960 s / batch. (data: 8.60e-01). ETA=18:52:36, max mem: 20.9 GB 
[11/28 23:42:35 visual_prompt]: 	Training 400/553. train loss: 1.7290,	0.8304 s / batch. (data: 1.06e-02). ETA=9:13:09, max mem: 20.9 GB 
[11/28 23:44:17 visual_prompt]: 	Training 500/553. train loss: 0.0043,	0.8404 s / batch. (data: 4.02e-04). ETA=9:18:27, max mem: 20.9 GB 
[11/28 23:45:12 visual_prompt]: Epoch 28 / 100: avg data time: 2.24e-01, avg batch time: 1.0470, average train loss: 1.4301
[11/28 23:46:12 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3047, average loss: 0.7544
[11/28 23:46:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.30	
[11/28 23:46:12 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[11/28 23:48:06 visual_prompt]: 	Training 100/553. train loss: 0.3826,	0.8401 s / batch. (data: 2.97e-04). ETA=9:16:03, max mem: 20.9 GB 
[11/28 23:49:49 visual_prompt]: 	Training 200/553. train loss: 1.1219,	1.7379 s / batch. (data: 9.28e-01). ETA=19:07:29, max mem: 20.9 GB 
[11/28 23:51:31 visual_prompt]: 	Training 300/553. train loss: 1.3036,	0.8360 s / batch. (data: 3.03e-04). ETA=9:10:37, max mem: 20.9 GB 
[11/28 23:53:12 visual_prompt]: 	Training 400/553. train loss: 2.4949,	1.3575 s / batch. (data: 5.44e-01). ETA=14:51:47, max mem: 20.9 GB 
[11/28 23:54:55 visual_prompt]: 	Training 500/553. train loss: 1.1566,	0.8280 s / batch. (data: 3.36e-04). ETA=9:02:33, max mem: 20.9 GB 
[11/28 23:55:49 visual_prompt]: Epoch 29 / 100: avg data time: 2.21e-01, avg batch time: 1.0438, average train loss: 1.3371
[11/28 23:56:49 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.3061, average loss: 2.8916
[11/28 23:56:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 38.71	
[11/28 23:56:49 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[11/28 23:58:35 visual_prompt]: 	Training 100/553. train loss: 2.1121,	0.8069 s / batch. (data: 3.43e-04). ETA=8:46:39, max mem: 20.9 GB 
[11/29 00:00:21 visual_prompt]: 	Training 200/553. train loss: 0.8000,	0.8602 s / batch. (data: 8.18e-03). ETA=9:20:02, max mem: 20.9 GB 
[11/29 00:02:03 visual_prompt]: 	Training 300/553. train loss: 0.4499,	1.9246 s / batch. (data: 1.09e+00). ETA=20:49:47, max mem: 20.9 GB 
[11/29 00:03:49 visual_prompt]: 	Training 400/553. train loss: 0.8217,	1.3864 s / batch. (data: 5.43e-01). ETA=14:57:59, max mem: 20.9 GB 
[11/29 00:05:32 visual_prompt]: 	Training 500/553. train loss: 0.8218,	1.7825 s / batch. (data: 9.53e-01). ETA=19:11:35, max mem: 20.9 GB 
[11/29 00:06:28 visual_prompt]: Epoch 30 / 100: avg data time: 2.22e-01, avg batch time: 1.0460, average train loss: 1.3557
[11/29 00:07:27 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.3059, average loss: 0.7049
[11/29 00:07:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.05	
[11/29 00:07:27 visual_prompt]: Stopping early.
[11/29 00:07:27 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 00:07:27 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 00:07:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/29 00:07:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 00:07:27 visual_prompt]: Training with config:
[11/29 00:07:27 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.5_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/29 00:07:27 visual_prompt]: Loading training data...
[11/29 00:07:27 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 00:07:27 visual_prompt]: Loading validation data...
[11/29 00:07:27 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 00:07:27 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/29 00:07:30 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/29 00:07:30 visual_prompt]: tuned percent:0.525
[11/29 00:07:30 visual_prompt]: Device used for model: 0
[11/29 00:07:30 visual_prompt]: Setting up Evaluator...
[11/29 00:07:30 visual_prompt]: Setting up Trainer...
[11/29 00:07:30 visual_prompt]: 	Setting up the optimizer...
[11/29 00:07:30 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 00:09:18 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8268 s / batch. (data: 6.87e-03). ETA=12:40:41, max mem: 20.9 GB 
[11/29 00:11:03 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8055 s / batch. (data: 3.08e-04). ETA=12:19:43, max mem: 20.9 GB 
[11/29 00:12:50 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.7840 s / batch. (data: 9.67e-01). ETA=1 day, 3:15:17, max mem: 20.9 GB 
[11/29 00:14:33 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8055 s / batch. (data: 3.15e-04). ETA=12:17:03, max mem: 20.9 GB 
[11/29 00:16:19 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8664 s / batch. (data: 1.07e-03). ETA=13:11:16, max mem: 20.9 GB 
[11/29 00:17:14 visual_prompt]: Epoch 1 / 100: avg data time: 2.32e-01, avg batch time: 1.0547, average train loss: 1.5403
[11/29 00:18:13 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.3052, average loss: 1.5201
[11/29 00:18:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/29 00:18:13 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[11/29 00:20:01 visual_prompt]: 	Training 100/553. train loss: 0.7742,	0.8399 s / batch. (data: 4.12e-04). ETA=12:45:00, max mem: 20.9 GB 
[11/29 00:21:44 visual_prompt]: 	Training 200/553. train loss: 0.1321,	0.8400 s / batch. (data: 3.04e-04). ETA=12:43:39, max mem: 20.9 GB 
[11/29 00:23:30 visual_prompt]: 	Training 300/553. train loss: 0.9265,	1.1800 s / batch. (data: 3.70e-01). ETA=17:50:48, max mem: 20.9 GB 
[11/29 00:25:12 visual_prompt]: 	Training 400/553. train loss: 1.5258,	0.8301 s / batch. (data: 1.05e-02). ETA=12:31:51, max mem: 20.9 GB 
[11/29 00:26:57 visual_prompt]: 	Training 500/553. train loss: 0.5747,	0.8186 s / batch. (data: 2.91e-04). ETA=12:20:07, max mem: 20.9 GB 
[11/29 00:27:50 visual_prompt]: Epoch 2 / 100: avg data time: 2.20e-01, avg batch time: 1.0420, average train loss: 0.9021
[11/29 00:28:49 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.3059, average loss: 1.0807
[11/29 00:28:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.77	
[11/29 00:28:49 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[11/29 00:30:35 visual_prompt]: 	Training 100/553. train loss: 0.7503,	0.8320 s / batch. (data: 3.33e-04). ETA=12:30:08, max mem: 20.9 GB 
[11/29 00:32:20 visual_prompt]: 	Training 200/553. train loss: 0.7792,	1.4760 s / batch. (data: 6.60e-01). ETA=22:08:13, max mem: 20.9 GB 
[11/29 00:34:03 visual_prompt]: 	Training 300/553. train loss: 0.5905,	0.8179 s / batch. (data: 3.69e-04). ETA=12:14:40, max mem: 20.9 GB 
[11/29 00:35:48 visual_prompt]: 	Training 400/553. train loss: 1.6339,	0.9362 s / batch. (data: 3.37e-02). ETA=13:59:23, max mem: 20.9 GB 
[11/29 00:37:34 visual_prompt]: 	Training 500/553. train loss: 0.8624,	1.3840 s / batch. (data: 5.57e-01). ETA=20:38:32, max mem: 20.9 GB 
[11/29 00:38:27 visual_prompt]: Epoch 3 / 100: avg data time: 2.22e-01, avg batch time: 1.0447, average train loss: 0.8498
[11/29 00:39:27 visual_prompt]: Inference (val):avg data time: 4.03e-05, avg batch time: 0.3061, average loss: 0.7327
[11/29 00:39:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.58	
[11/29 00:39:27 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[11/29 00:41:16 visual_prompt]: 	Training 100/553. train loss: 0.8313,	0.8339 s / batch. (data: 7.97e-03). ETA=12:24:08, max mem: 20.9 GB 
[11/29 00:43:01 visual_prompt]: 	Training 200/553. train loss: 1.4157,	0.8335 s / batch. (data: 1.19e-02). ETA=12:22:24, max mem: 20.9 GB 
[11/29 00:44:48 visual_prompt]: 	Training 300/553. train loss: 0.7560,	2.5080 s / batch. (data: 1.68e+00). ETA=1 day, 13:09:38, max mem: 20.9 GB 
[11/29 00:46:27 visual_prompt]: 	Training 400/553. train loss: 0.6816,	1.5980 s / batch. (data: 7.82e-01). ETA=23:37:58, max mem: 20.9 GB 
[11/29 00:48:14 visual_prompt]: 	Training 500/553. train loss: 0.6022,	3.5073 s / batch. (data: 2.70e+00). ETA=2 days, 3:46:19, max mem: 20.9 GB 
[11/29 00:49:08 visual_prompt]: Epoch 4 / 100: avg data time: 2.30e-01, avg batch time: 1.0513, average train loss: 0.9386
[11/29 00:50:08 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.3060, average loss: 1.0216
[11/29 00:50:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.34	
[11/29 00:50:08 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[11/29 00:51:54 visual_prompt]: 	Training 100/553. train loss: 0.6818,	0.8239 s / batch. (data: 7.93e-03). ETA=12:07:39, max mem: 20.9 GB 
[11/29 00:53:39 visual_prompt]: 	Training 200/553. train loss: 0.6144,	1.4200 s / batch. (data: 6.04e-01). ETA=20:51:39, max mem: 20.9 GB 
[11/29 00:55:24 visual_prompt]: 	Training 300/553. train loss: 1.4252,	0.8472 s / batch. (data: 1.11e-02). ETA=12:25:23, max mem: 20.9 GB 
[11/29 00:57:07 visual_prompt]: 	Training 400/553. train loss: 1.0239,	0.8160 s / batch. (data: 4.18e-04). ETA=11:56:33, max mem: 20.9 GB 
[11/29 00:58:52 visual_prompt]: 	Training 500/553. train loss: 0.5849,	0.8062 s / batch. (data: 3.89e-04). ETA=11:46:38, max mem: 20.9 GB 
[11/29 00:59:46 visual_prompt]: Epoch 5 / 100: avg data time: 2.23e-01, avg batch time: 1.0463, average train loss: 0.8772
[11/29 01:00:46 visual_prompt]: Inference (val):avg data time: 4.41e-05, avg batch time: 0.3060, average loss: 1.6199
[11/29 01:00:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.20	
[11/29 01:00:46 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[11/29 01:02:36 visual_prompt]: 	Training 100/553. train loss: 0.5677,	0.8360 s / batch. (data: 9.63e-04). ETA=12:10:34, max mem: 20.9 GB 
[11/29 01:04:21 visual_prompt]: 	Training 200/553. train loss: 0.9987,	0.8264 s / batch. (data: 4.01e-04). ETA=12:00:51, max mem: 20.9 GB 
[11/29 01:06:04 visual_prompt]: 	Training 300/553. train loss: 0.5979,	0.8320 s / batch. (data: 4.02e-04). ETA=12:04:18, max mem: 20.9 GB 
[11/29 01:07:52 visual_prompt]: 	Training 400/553. train loss: 0.6772,	0.8241 s / batch. (data: 3.21e-04). ETA=11:56:02, max mem: 20.9 GB 
[11/29 01:09:35 visual_prompt]: 	Training 500/553. train loss: 0.9436,	0.8201 s / batch. (data: 3.73e-04). ETA=11:51:11, max mem: 20.9 GB 
[11/29 01:10:28 visual_prompt]: Epoch 6 / 100: avg data time: 2.30e-01, avg batch time: 1.0525, average train loss: 0.9098
[11/29 01:11:28 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3060, average loss: 1.0490
[11/29 01:11:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.71	
[11/29 01:11:28 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[11/29 01:13:15 visual_prompt]: 	Training 100/553. train loss: 2.2867,	0.8160 s / batch. (data: 3.41e-04). ETA=11:45:37, max mem: 20.9 GB 
[11/29 01:14:58 visual_prompt]: 	Training 200/553. train loss: 0.5891,	1.0240 s / batch. (data: 1.96e-01). ETA=14:43:44, max mem: 20.9 GB 
[11/29 01:16:46 visual_prompt]: 	Training 300/553. train loss: 0.5617,	1.9560 s / batch. (data: 1.12e+00). ETA=1 day, 4:04:47, max mem: 20.9 GB 
[11/29 01:18:30 visual_prompt]: 	Training 400/553. train loss: 0.5960,	2.1770 s / batch. (data: 1.34e+00). ETA=1 day, 7:11:36, max mem: 20.9 GB 
[11/29 01:20:12 visual_prompt]: 	Training 500/553. train loss: 1.2674,	0.8298 s / batch. (data: 5.50e-03). ETA=11:51:58, max mem: 20.9 GB 
[11/29 01:21:05 visual_prompt]: Epoch 7 / 100: avg data time: 2.20e-01, avg batch time: 1.0426, average train loss: 0.9300
[11/29 01:22:05 visual_prompt]: Inference (val):avg data time: 2.64e-04, avg batch time: 0.3062, average loss: 0.7171
[11/29 01:22:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.19	
[11/29 01:22:05 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[11/29 01:23:50 visual_prompt]: 	Training 100/553. train loss: 0.7912,	0.8380 s / batch. (data: 1.00e-02). ETA=11:56:55, max mem: 20.9 GB 
[11/29 01:25:36 visual_prompt]: 	Training 200/553. train loss: 0.7433,	0.8064 s / batch. (data: 3.20e-04). ETA=11:28:29, max mem: 20.9 GB 
[11/29 01:27:21 visual_prompt]: 	Training 300/553. train loss: 0.7953,	0.8109 s / batch. (data: 5.58e-03). ETA=11:31:01, max mem: 20.9 GB 
[11/29 01:29:05 visual_prompt]: 	Training 400/553. train loss: 0.7159,	1.0861 s / batch. (data: 2.24e-01). ETA=15:23:44, max mem: 20.9 GB 
[11/29 01:30:50 visual_prompt]: 	Training 500/553. train loss: 1.2213,	1.5119 s / batch. (data: 6.82e-01). ETA=21:23:20, max mem: 20.9 GB 
[11/29 01:31:44 visual_prompt]: Epoch 8 / 100: avg data time: 2.23e-01, avg batch time: 1.0478, average train loss: 0.9477
[11/29 01:32:44 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.3061, average loss: 1.1595
[11/29 01:32:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.49	
[11/29 01:32:44 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[11/29 01:34:32 visual_prompt]: 	Training 100/553. train loss: 0.1267,	0.8206 s / batch. (data: 2.85e-04). ETA=11:34:27, max mem: 20.9 GB 
[11/29 01:36:15 visual_prompt]: 	Training 200/553. train loss: 0.7180,	0.8115 s / batch. (data: 5.74e-03). ETA=11:25:25, max mem: 20.9 GB 
[11/29 01:38:00 visual_prompt]: 	Training 300/553. train loss: 0.5589,	2.0058 s / batch. (data: 1.19e+00). ETA=1 day, 4:10:46, max mem: 20.9 GB 
[11/29 01:39:46 visual_prompt]: 	Training 400/553. train loss: 0.6192,	0.8163 s / batch. (data: 1.05e-02). ETA=11:26:43, max mem: 20.9 GB 
[11/29 01:41:30 visual_prompt]: 	Training 500/553. train loss: 0.6840,	1.1782 s / batch. (data: 3.44e-01). ETA=16:29:13, max mem: 20.9 GB 
[11/29 01:42:23 visual_prompt]: Epoch 9 / 100: avg data time: 2.24e-01, avg batch time: 1.0469, average train loss: 0.9406
[11/29 01:43:22 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.3064, average loss: 0.6886
[11/29 01:43:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.76	
[11/29 01:43:22 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[11/29 01:45:13 visual_prompt]: 	Training 100/553. train loss: 1.4953,	0.8320 s / batch. (data: 1.20e-02). ETA=11:36:27, max mem: 20.9 GB 
[11/29 01:46:56 visual_prompt]: 	Training 200/553. train loss: 0.7161,	0.8360 s / batch. (data: 1.19e-02). ETA=11:38:21, max mem: 20.9 GB 
[11/29 01:48:39 visual_prompt]: 	Training 300/553. train loss: 0.5708,	0.8236 s / batch. (data: 7.98e-03). ETA=11:26:38, max mem: 20.9 GB 
[11/29 01:50:20 visual_prompt]: 	Training 400/553. train loss: 0.8661,	0.9427 s / batch. (data: 1.11e-01). ETA=13:04:22, max mem: 20.9 GB 
[11/29 01:52:05 visual_prompt]: 	Training 500/553. train loss: 0.5701,	1.1609 s / batch. (data: 3.56e-01). ETA=16:04:00, max mem: 20.9 GB 
[11/29 01:52:59 visual_prompt]: Epoch 10 / 100: avg data time: 2.20e-01, avg batch time: 1.0429, average train loss: 1.1865
[11/29 01:53:59 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.3080, average loss: 0.6896
[11/29 01:53:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.88	
[11/29 01:53:59 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[11/29 01:55:49 visual_prompt]: 	Training 100/553. train loss: 1.0289,	0.8240 s / batch. (data: 4.57e-04). ETA=11:22:07, max mem: 20.9 GB 
[11/29 01:57:35 visual_prompt]: 	Training 200/553. train loss: 1.3758,	0.8226 s / batch. (data: 5.43e-03). ETA=11:19:35, max mem: 20.9 GB 
[11/29 01:59:18 visual_prompt]: 	Training 300/553. train loss: 0.1677,	2.2519 s / batch. (data: 1.41e+00). ETA=1 day, 6:56:42, max mem: 20.9 GB 
[11/29 02:01:01 visual_prompt]: 	Training 400/553. train loss: 0.6386,	0.8222 s / batch. (data: 3.72e-04). ETA=11:16:30, max mem: 20.9 GB 
[11/29 02:02:44 visual_prompt]: 	Training 500/553. train loss: 0.8986,	0.8400 s / batch. (data: 1.19e-02). ETA=11:29:46, max mem: 20.9 GB 
[11/29 02:03:38 visual_prompt]: Epoch 11 / 100: avg data time: 2.23e-01, avg batch time: 1.0458, average train loss: 0.9486
[11/29 02:04:37 visual_prompt]: Inference (val):avg data time: 2.24e-04, avg batch time: 0.3061, average loss: 0.7804
[11/29 02:04:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.26	
[11/29 02:04:37 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[11/29 02:06:27 visual_prompt]: 	Training 100/553. train loss: 0.8570,	1.1891 s / batch. (data: 3.29e-01). ETA=16:13:25, max mem: 20.9 GB 
[11/29 02:08:12 visual_prompt]: 	Training 200/553. train loss: 0.5702,	0.8320 s / batch. (data: 3.44e-04). ETA=11:19:42, max mem: 20.9 GB 
[11/29 02:09:55 visual_prompt]: 	Training 300/553. train loss: 1.0247,	0.8229 s / batch. (data: 3.12e-04). ETA=11:10:55, max mem: 20.9 GB 
[11/29 02:11:40 visual_prompt]: 	Training 400/553. train loss: 0.7868,	0.8320 s / batch. (data: 4.37e-04). ETA=11:16:54, max mem: 20.9 GB 
[11/29 02:13:26 visual_prompt]: 	Training 500/553. train loss: 3.7251,	0.8163 s / batch. (data: 8.51e-04). ETA=11:02:45, max mem: 20.9 GB 
[11/29 02:14:19 visual_prompt]: Epoch 12 / 100: avg data time: 2.29e-01, avg batch time: 1.0520, average train loss: 1.0984
[11/29 02:15:19 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.3067, average loss: 1.8046
[11/29 02:15:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.26	
[11/29 02:15:19 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[11/29 02:17:08 visual_prompt]: 	Training 100/553. train loss: 0.6844,	0.8080 s / batch. (data: 3.55e-04). ETA=10:54:01, max mem: 20.9 GB 
[11/29 02:18:50 visual_prompt]: 	Training 200/553. train loss: 0.7033,	0.8120 s / batch. (data: 4.04e-04). ETA=10:55:51, max mem: 20.9 GB 
[11/29 02:20:35 visual_prompt]: 	Training 300/553. train loss: 0.9773,	2.0089 s / batch. (data: 1.20e+00). ETA=1 day, 2:59:18, max mem: 20.9 GB 
[11/29 02:22:17 visual_prompt]: 	Training 400/553. train loss: 3.7853,	0.8360 s / batch. (data: 3.19e-04). ETA=11:12:29, max mem: 20.9 GB 
[11/29 02:24:03 visual_prompt]: 	Training 500/553. train loss: 1.1160,	0.8336 s / batch. (data: 7.87e-04). ETA=11:09:09, max mem: 20.9 GB 
[11/29 02:24:57 visual_prompt]: Epoch 13 / 100: avg data time: 2.25e-01, avg batch time: 1.0462, average train loss: 1.2169
[11/29 02:25:57 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.3052, average loss: 0.9227
[11/29 02:25:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.10	
[11/29 02:25:57 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[11/29 02:27:46 visual_prompt]: 	Training 100/553. train loss: 0.9061,	0.8560 s / batch. (data: 5.45e-03). ETA=11:24:58, max mem: 20.9 GB 
[11/29 02:29:30 visual_prompt]: 	Training 200/553. train loss: 0.0361,	1.5375 s / batch. (data: 7.28e-01). ETA=20:27:43, max mem: 20.9 GB 
[11/29 02:31:13 visual_prompt]: 	Training 300/553. train loss: 0.6723,	0.9088 s / batch. (data: 8.27e-02). ETA=12:04:08, max mem: 20.9 GB 
[11/29 02:32:57 visual_prompt]: 	Training 400/553. train loss: 0.6289,	0.8560 s / batch. (data: 3.40e-04). ETA=11:20:40, max mem: 20.9 GB 
[11/29 02:34:40 visual_prompt]: 	Training 500/553. train loss: 1.1140,	0.8299 s / batch. (data: 5.45e-03). ETA=10:58:30, max mem: 20.9 GB 
[11/29 02:35:34 visual_prompt]: Epoch 14 / 100: avg data time: 2.21e-01, avg batch time: 1.0441, average train loss: 1.2065
[11/29 02:36:34 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3094, average loss: 0.6908
[11/29 02:36:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 63.98	
[11/29 02:36:34 visual_prompt]: Best epoch 14: best metric: -0.691
[11/29 02:36:34 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[11/29 02:38:22 visual_prompt]: 	Training 100/553. train loss: 0.8596,	1.2560 s / batch. (data: 4.32e-01). ETA=16:33:27, max mem: 20.9 GB 
[11/29 02:40:05 visual_prompt]: 	Training 200/553. train loss: 4.4659,	0.8063 s / batch. (data: 4.68e-04). ETA=10:36:23, max mem: 20.9 GB 
[11/29 02:41:51 visual_prompt]: 	Training 300/553. train loss: 0.6629,	0.8218 s / batch. (data: 4.61e-04). ETA=10:47:17, max mem: 20.9 GB 
[11/29 02:43:33 visual_prompt]: 	Training 400/553. train loss: 1.2398,	1.0500 s / batch. (data: 2.00e-01). ETA=13:45:18, max mem: 20.9 GB 
[11/29 02:45:19 visual_prompt]: 	Training 500/553. train loss: 0.5912,	0.8359 s / batch. (data: 1.19e-02). ETA=10:55:37, max mem: 20.9 GB 
[11/29 02:46:14 visual_prompt]: Epoch 15 / 100: avg data time: 2.25e-01, avg batch time: 1.0486, average train loss: 1.2034
[11/29 02:47:14 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3061, average loss: 1.0720
[11/29 02:47:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.47	
[11/29 02:47:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[11/29 02:49:02 visual_prompt]: 	Training 100/553. train loss: 0.7801,	0.8290 s / batch. (data: 5.54e-03). ETA=10:48:04, max mem: 20.9 GB 
[11/29 02:50:47 visual_prompt]: 	Training 200/553. train loss: 1.8266,	0.8560 s / batch. (data: 5.01e-03). ETA=11:07:46, max mem: 20.9 GB 
[11/29 02:52:31 visual_prompt]: 	Training 300/553. train loss: 1.1724,	0.8400 s / batch. (data: 8.03e-04). ETA=10:53:53, max mem: 20.9 GB 
[11/29 02:54:14 visual_prompt]: 	Training 400/553. train loss: 0.9592,	0.8430 s / batch. (data: 9.06e-04). ETA=10:54:45, max mem: 20.9 GB 
[11/29 02:55:57 visual_prompt]: 	Training 500/553. train loss: 0.6301,	1.3602 s / batch. (data: 5.56e-01). ETA=17:34:17, max mem: 20.9 GB 
[11/29 02:56:52 visual_prompt]: Epoch 16 / 100: avg data time: 2.22e-01, avg batch time: 1.0446, average train loss: 0.9768
[11/29 02:57:51 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3059, average loss: 0.8237
[11/29 02:57:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.02	
[11/29 02:57:51 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[11/29 02:59:38 visual_prompt]: 	Training 100/553. train loss: 0.9535,	0.8401 s / batch. (data: 3.17e-04). ETA=10:48:58, max mem: 20.9 GB 
[11/29 03:01:24 visual_prompt]: 	Training 200/553. train loss: 2.9774,	0.8186 s / batch. (data: 2.78e-04). ETA=10:31:01, max mem: 20.9 GB 
[11/29 03:03:08 visual_prompt]: 	Training 300/553. train loss: 1.5363,	0.8128 s / batch. (data: 7.94e-03). ETA=10:25:14, max mem: 20.9 GB 
[11/29 03:04:52 visual_prompt]: 	Training 400/553. train loss: 0.6225,	1.3506 s / batch. (data: 5.13e-01). ETA=17:16:39, max mem: 20.9 GB 
[11/29 03:06:36 visual_prompt]: 	Training 500/553. train loss: 1.1883,	1.9291 s / batch. (data: 1.11e+00). ETA=1 day, 0:37:25, max mem: 20.9 GB 
[11/29 03:07:31 visual_prompt]: Epoch 17 / 100: avg data time: 2.25e-01, avg batch time: 1.0480, average train loss: 1.1318
[11/29 03:08:31 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.3056, average loss: 0.8863
[11/29 03:08:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.78	
[11/29 03:08:31 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[11/29 03:10:19 visual_prompt]: 	Training 100/553. train loss: 0.7490,	0.8504 s / batch. (data: 2.24e-02). ETA=10:49:08, max mem: 20.9 GB 
[11/29 03:12:06 visual_prompt]: 	Training 200/553. train loss: 0.6182,	0.8273 s / batch. (data: 2.89e-04). ETA=10:30:06, max mem: 20.9 GB 
[11/29 03:13:51 visual_prompt]: 	Training 300/553. train loss: 0.6582,	0.8201 s / batch. (data: 3.06e-04). ETA=10:23:17, max mem: 20.9 GB 
[11/29 03:15:35 visual_prompt]: 	Training 400/553. train loss: 2.0270,	0.8147 s / batch. (data: 7.94e-03). ETA=10:17:47, max mem: 20.9 GB 
[11/29 03:17:17 visual_prompt]: 	Training 500/553. train loss: 0.7826,	1.4604 s / batch. (data: 6.22e-01). ETA=18:25:00, max mem: 20.9 GB 
[11/29 03:18:10 visual_prompt]: Epoch 18 / 100: avg data time: 2.26e-01, avg batch time: 1.0478, average train loss: 1.3795
[11/29 03:19:10 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.3076, average loss: 0.6943
[11/29 03:19:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.74	
[11/29 03:19:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[11/29 03:20:59 visual_prompt]: 	Training 100/553. train loss: 0.6935,	1.5898 s / batch. (data: 7.79e-01). ETA=19:58:50, max mem: 20.9 GB 
[11/29 03:22:44 visual_prompt]: 	Training 200/553. train loss: 0.7440,	0.8346 s / batch. (data: 5.75e-04). ETA=10:28:00, max mem: 20.9 GB 
[11/29 03:24:27 visual_prompt]: 	Training 300/553. train loss: 0.3663,	0.8161 s / batch. (data: 9.47e-03). ETA=10:12:43, max mem: 20.9 GB 
[11/29 03:26:11 visual_prompt]: 	Training 400/553. train loss: 0.6592,	0.8360 s / batch. (data: 3.49e-04). ETA=10:26:15, max mem: 20.9 GB 
[11/29 03:27:54 visual_prompt]: 	Training 500/553. train loss: 0.8467,	0.8200 s / batch. (data: 3.76e-04). ETA=10:12:52, max mem: 20.9 GB 
[11/29 03:28:48 visual_prompt]: Epoch 19 / 100: avg data time: 2.23e-01, avg batch time: 1.0450, average train loss: 1.1908
[11/29 03:29:47 visual_prompt]: Inference (val):avg data time: 6.03e-05, avg batch time: 0.3073, average loss: 4.5945
[11/29 03:29:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.63	
[11/29 03:29:47 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[11/29 03:31:34 visual_prompt]: 	Training 100/553. train loss: 0.6789,	0.8201 s / batch. (data: 1.09e-02). ETA=10:10:54, max mem: 20.9 GB 
[11/29 03:33:20 visual_prompt]: 	Training 200/553. train loss: 0.6506,	0.8263 s / batch. (data: 5.46e-03). ETA=10:14:08, max mem: 20.9 GB 
[11/29 03:35:04 visual_prompt]: 	Training 300/553. train loss: 0.6597,	0.8392 s / batch. (data: 1.65e-02). ETA=10:22:18, max mem: 20.9 GB 
[11/29 03:36:49 visual_prompt]: 	Training 400/553. train loss: 0.7239,	0.8525 s / batch. (data: 2.74e-02). ETA=10:30:46, max mem: 20.9 GB 
[11/29 03:38:33 visual_prompt]: 	Training 500/553. train loss: 0.7731,	0.8457 s / batch. (data: 5.55e-03). ETA=10:24:20, max mem: 20.9 GB 
[11/29 03:39:29 visual_prompt]: Epoch 20 / 100: avg data time: 2.30e-01, avg batch time: 1.0518, average train loss: 1.4464
[11/29 03:40:29 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.3053, average loss: 0.8541
[11/29 03:40:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.39	
[11/29 03:40:29 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[11/29 03:42:20 visual_prompt]: 	Training 100/553. train loss: 1.5329,	0.8431 s / batch. (data: 3.46e-04). ETA=10:20:15, max mem: 20.9 GB 
[11/29 03:44:03 visual_prompt]: 	Training 200/553. train loss: 0.8960,	0.8249 s / batch. (data: 5.51e-03). ETA=10:05:29, max mem: 20.9 GB 
[11/29 03:45:47 visual_prompt]: 	Training 300/553. train loss: 3.4251,	1.0410 s / batch. (data: 2.29e-01). ETA=12:42:22, max mem: 20.9 GB 
[11/29 03:47:29 visual_prompt]: 	Training 400/553. train loss: 1.6836,	0.8202 s / batch. (data: 3.86e-04). ETA=9:59:19, max mem: 20.9 GB 
[11/29 03:49:14 visual_prompt]: 	Training 500/553. train loss: 0.7203,	0.8542 s / batch. (data: 5.44e-03). ETA=10:22:42, max mem: 20.9 GB 
[11/29 03:50:07 visual_prompt]: Epoch 21 / 100: avg data time: 2.24e-01, avg batch time: 1.0458, average train loss: 1.2872
[11/29 03:51:07 visual_prompt]: Inference (val):avg data time: 4.89e-05, avg batch time: 0.3057, average loss: 0.8163
[11/29 03:51:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.15	
[11/29 03:51:07 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[11/29 03:52:55 visual_prompt]: 	Training 100/553. train loss: 1.2997,	0.8240 s / batch. (data: 2.99e-04). ETA=9:58:37, max mem: 20.9 GB 
[11/29 03:54:39 visual_prompt]: 	Training 200/553. train loss: 0.6311,	0.8120 s / batch. (data: 3.43e-04). ETA=9:48:30, max mem: 20.9 GB 
[11/29 03:56:22 visual_prompt]: 	Training 300/553. train loss: 0.2748,	0.8080 s / batch. (data: 3.09e-04). ETA=9:44:17, max mem: 20.9 GB 
[11/29 03:58:07 visual_prompt]: 	Training 400/553. train loss: 0.8418,	0.8280 s / batch. (data: 8.00e-03). ETA=9:57:22, max mem: 20.9 GB 
[11/29 03:59:51 visual_prompt]: 	Training 500/553. train loss: 0.5765,	0.8400 s / batch. (data: 3.96e-03). ETA=10:04:37, max mem: 20.9 GB 
[11/29 04:00:46 visual_prompt]: Epoch 22 / 100: avg data time: 2.23e-01, avg batch time: 1.0465, average train loss: 1.2951
[11/29 04:01:45 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3075, average loss: 0.6884
[11/29 04:01:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.37	
[11/29 04:01:45 visual_prompt]: Best epoch 22: best metric: -0.688
[11/29 04:01:45 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[11/29 04:03:35 visual_prompt]: 	Training 100/553. train loss: 1.0762,	0.8214 s / batch. (data: 3.58e-04). ETA=9:49:07, max mem: 20.9 GB 
[11/29 04:05:19 visual_prompt]: 	Training 200/553. train loss: 2.0692,	0.8059 s / batch. (data: 3.09e-04). ETA=9:36:41, max mem: 20.9 GB 
[11/29 04:07:05 visual_prompt]: 	Training 300/553. train loss: 0.6199,	0.8127 s / batch. (data: 3.01e-04). ETA=9:40:11, max mem: 20.9 GB 
[11/29 04:08:47 visual_prompt]: 	Training 400/553. train loss: 0.7393,	0.8301 s / batch. (data: 2.85e-04). ETA=9:51:13, max mem: 20.9 GB 
[11/29 04:10:29 visual_prompt]: 	Training 500/553. train loss: 0.1916,	0.8262 s / batch. (data: 1.19e-02). ETA=9:47:04, max mem: 20.9 GB 
[11/29 04:11:24 visual_prompt]: Epoch 23 / 100: avg data time: 2.24e-01, avg batch time: 1.0462, average train loss: 1.0443
[11/29 04:12:23 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3084, average loss: 1.1503
[11/29 04:12:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.70	
[11/29 04:12:23 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[11/29 04:14:08 visual_prompt]: 	Training 100/553. train loss: 1.8344,	1.0280 s / batch. (data: 1.87e-01). ETA=12:07:51, max mem: 20.9 GB 
[11/29 04:15:52 visual_prompt]: 	Training 200/553. train loss: 1.3425,	0.8443 s / batch. (data: 1.57e-02). ETA=9:56:23, max mem: 20.9 GB 
[11/29 04:17:36 visual_prompt]: 	Training 300/553. train loss: 0.7916,	1.0960 s / batch. (data: 2.50e-01). ETA=12:52:20, max mem: 20.9 GB 
[11/29 04:19:21 visual_prompt]: 	Training 400/553. train loss: 0.5746,	0.8420 s / batch. (data: 2.20e-02). ETA=9:51:57, max mem: 20.9 GB 
[11/29 04:21:07 visual_prompt]: 	Training 500/553. train loss: 1.3705,	0.8242 s / batch. (data: 3.15e-04). ETA=9:38:03, max mem: 20.9 GB 
[11/29 04:22:02 visual_prompt]: Epoch 24 / 100: avg data time: 2.24e-01, avg batch time: 1.0463, average train loss: 1.4314
[11/29 04:23:02 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3062, average loss: 1.9105
[11/29 04:23:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.09	
[11/29 04:23:02 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[11/29 04:24:53 visual_prompt]: 	Training 100/553. train loss: 1.8765,	0.8124 s / batch. (data: 3.09e-04). ETA=9:27:42, max mem: 20.9 GB 
[11/29 04:26:34 visual_prompt]: 	Training 200/553. train loss: 0.6737,	0.8520 s / batch. (data: 3.23e-04). ETA=9:53:57, max mem: 20.9 GB 
[11/29 04:28:17 visual_prompt]: 	Training 300/553. train loss: 0.7955,	0.8200 s / batch. (data: 2.69e-04). ETA=9:30:18, max mem: 20.9 GB 
[11/29 04:30:01 visual_prompt]: 	Training 400/553. train loss: 0.5555,	1.3440 s / batch. (data: 5.12e-01). ETA=15:32:28, max mem: 20.9 GB 
[11/29 04:31:45 visual_prompt]: 	Training 500/553. train loss: 0.7783,	1.7300 s / batch. (data: 9.11e-01). ETA=19:57:22, max mem: 20.9 GB 
[11/29 04:32:39 visual_prompt]: Epoch 25 / 100: avg data time: 2.21e-01, avg batch time: 1.0439, average train loss: 1.1830
[11/29 04:33:39 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3054, average loss: 2.2345
[11/29 04:33:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.31	
[11/29 04:33:39 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[11/29 04:35:26 visual_prompt]: 	Training 100/553. train loss: 0.6476,	0.8503 s / batch. (data: 1.05e-02). ETA=9:46:22, max mem: 20.9 GB 
[11/29 04:37:11 visual_prompt]: 	Training 200/553. train loss: 2.7143,	1.8977 s / batch. (data: 1.09e+00). ETA=21:45:26, max mem: 20.9 GB 
[11/29 04:38:57 visual_prompt]: 	Training 300/553. train loss: 1.5424,	0.8072 s / batch. (data: 3.47e-04). ETA=9:13:55, max mem: 20.9 GB 
[11/29 04:40:39 visual_prompt]: 	Training 400/553. train loss: 2.0193,	0.8200 s / batch. (data: 3.66e-04). ETA=9:21:20, max mem: 20.9 GB 
[11/29 04:42:21 visual_prompt]: 	Training 500/553. train loss: 1.8188,	0.8180 s / batch. (data: 3.79e-04). ETA=9:18:37, max mem: 20.9 GB 
[11/29 04:43:15 visual_prompt]: Epoch 26 / 100: avg data time: 2.20e-01, avg batch time: 1.0421, average train loss: 1.1821
[11/29 04:44:14 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.3069, average loss: 0.6895
[11/29 04:44:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.57	
[11/29 04:44:14 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[11/29 04:46:02 visual_prompt]: 	Training 100/553. train loss: 0.5583,	0.8636 s / batch. (data: 2.77e-02). ETA=9:47:34, max mem: 20.9 GB 
[11/29 04:47:46 visual_prompt]: 	Training 200/553. train loss: 4.4085,	1.6781 s / batch. (data: 8.68e-01). ETA=18:58:56, max mem: 20.9 GB 
[11/29 04:49:30 visual_prompt]: 	Training 300/553. train loss: 1.0742,	0.8284 s / batch. (data: 8.34e-03). ETA=9:20:50, max mem: 20.9 GB 
[11/29 04:51:15 visual_prompt]: 	Training 400/553. train loss: 1.0918,	0.8120 s / batch. (data: 4.28e-04). ETA=9:08:24, max mem: 20.9 GB 
[11/29 04:52:59 visual_prompt]: 	Training 500/553. train loss: 0.8502,	0.8182 s / batch. (data: 3.44e-04). ETA=9:11:13, max mem: 20.9 GB 
[11/29 04:53:51 visual_prompt]: Epoch 27 / 100: avg data time: 2.22e-01, avg batch time: 1.0432, average train loss: 1.2770
[11/29 04:54:51 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.3062, average loss: 1.3220
[11/29 04:54:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.40	
[11/29 04:54:51 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[11/29 04:56:39 visual_prompt]: 	Training 100/553. train loss: 0.8539,	0.8220 s / batch. (data: 5.42e-03). ETA=9:11:40, max mem: 20.9 GB 
[11/29 04:58:24 visual_prompt]: 	Training 200/553. train loss: 1.2059,	0.8130 s / batch. (data: 7.95e-03). ETA=9:04:16, max mem: 20.9 GB 
[11/29 05:00:09 visual_prompt]: 	Training 300/553. train loss: 0.5361,	1.7698 s / batch. (data: 9.58e-01). ETA=19:41:54, max mem: 20.9 GB 
[11/29 05:01:52 visual_prompt]: 	Training 400/553. train loss: 1.3699,	0.8400 s / batch. (data: 7.89e-04). ETA=9:19:34, max mem: 20.9 GB 
[11/29 05:03:34 visual_prompt]: 	Training 500/553. train loss: 2.3905,	0.8238 s / batch. (data: 5.66e-03). ETA=9:07:23, max mem: 20.9 GB 
[11/29 05:04:30 visual_prompt]: Epoch 28 / 100: avg data time: 2.24e-01, avg batch time: 1.0460, average train loss: 1.1732
[11/29 05:05:29 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3074, average loss: 0.6995
[11/29 05:05:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.27	
[11/29 05:05:29 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[11/29 05:07:25 visual_prompt]: 	Training 100/553. train loss: 1.0199,	0.8102 s / batch. (data: 2.92e-04). ETA=8:56:16, max mem: 20.9 GB 
[11/29 05:09:08 visual_prompt]: 	Training 200/553. train loss: 0.9454,	2.0820 s / batch. (data: 1.26e+00). ETA=22:54:42, max mem: 20.9 GB 
[11/29 05:10:50 visual_prompt]: 	Training 300/553. train loss: 0.9287,	0.8280 s / batch. (data: 4.25e-04). ETA=9:05:19, max mem: 20.9 GB 
[11/29 05:12:30 visual_prompt]: 	Training 400/553. train loss: 2.0555,	1.2955 s / batch. (data: 4.89e-01). ETA=14:11:02, max mem: 20.9 GB 
[11/29 05:14:15 visual_prompt]: 	Training 500/553. train loss: 1.2652,	0.8129 s / batch. (data: 7.96e-03). ETA=8:52:39, max mem: 20.9 GB 
[11/29 05:15:08 visual_prompt]: Epoch 29 / 100: avg data time: 2.25e-01, avg batch time: 1.0470, average train loss: 1.3232
[11/29 05:16:08 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.3048, average loss: 1.3718
[11/29 05:16:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.95	
[11/29 05:16:08 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[11/29 05:17:55 visual_prompt]: 	Training 100/553. train loss: 0.7049,	0.8206 s / batch. (data: 3.88e-04). ETA=8:55:36, max mem: 20.9 GB 
[11/29 05:19:41 visual_prompt]: 	Training 200/553. train loss: 0.9930,	0.8133 s / batch. (data: 3.21e-04). ETA=8:49:29, max mem: 20.9 GB 
[11/29 05:21:23 visual_prompt]: 	Training 300/553. train loss: 0.1686,	0.9160 s / batch. (data: 8.44e-02). ETA=9:54:48, max mem: 20.9 GB 
[11/29 05:23:10 visual_prompt]: 	Training 400/553. train loss: 0.8886,	1.4320 s / batch. (data: 6.15e-01). ETA=15:27:30, max mem: 20.9 GB 
[11/29 05:24:53 visual_prompt]: 	Training 500/553. train loss: 0.7784,	1.5520 s / batch. (data: 7.15e-01). ETA=16:42:40, max mem: 20.9 GB 
[11/29 05:25:49 visual_prompt]: Epoch 30 / 100: avg data time: 2.28e-01, avg batch time: 1.0510, average train loss: 0.9486
[11/29 05:26:49 visual_prompt]: Inference (val):avg data time: 4.94e-04, avg batch time: 0.3141, average loss: 0.6880
[11/29 05:26:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.49	
[11/29 05:26:49 visual_prompt]: Best epoch 30: best metric: -0.688
[11/29 05:26:49 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[11/29 05:28:39 visual_prompt]: 	Training 100/553. train loss: 0.9790,	0.8275 s / batch. (data: 7.95e-03). ETA=8:52:30, max mem: 20.9 GB 
[11/29 05:30:24 visual_prompt]: 	Training 200/553. train loss: 1.7139,	0.8361 s / batch. (data: 1.20e-02). ETA=8:56:36, max mem: 20.9 GB 
[11/29 05:32:05 visual_prompt]: 	Training 300/553. train loss: 0.8648,	0.8120 s / batch. (data: 3.27e-04). ETA=8:39:48, max mem: 20.9 GB 
[11/29 05:33:47 visual_prompt]: 	Training 400/553. train loss: 0.5830,	0.8124 s / batch. (data: 3.88e-04). ETA=8:38:41, max mem: 20.9 GB 
[11/29 05:35:32 visual_prompt]: 	Training 500/553. train loss: 0.5741,	0.8480 s / batch. (data: 4.35e-04). ETA=9:00:00, max mem: 20.9 GB 
[11/29 05:36:25 visual_prompt]: Epoch 31 / 100: avg data time: 2.19e-01, avg batch time: 1.0415, average train loss: 1.0939
[11/29 05:37:24 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3052, average loss: 1.6096
[11/29 05:37:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.88	
[11/29 05:37:24 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[11/29 05:39:13 visual_prompt]: 	Training 100/553. train loss: 0.5697,	0.8317 s / batch. (data: 7.93e-04). ETA=8:47:31, max mem: 20.9 GB 
[11/29 05:40:57 visual_prompt]: 	Training 200/553. train loss: 0.6193,	0.8294 s / batch. (data: 5.57e-03). ETA=8:44:42, max mem: 20.9 GB 
[11/29 05:42:44 visual_prompt]: 	Training 300/553. train loss: 0.9712,	0.8220 s / batch. (data: 4.33e-04). ETA=8:38:40, max mem: 20.9 GB 
[11/29 05:44:28 visual_prompt]: 	Training 400/553. train loss: 0.9305,	0.8576 s / batch. (data: 2.56e-02). ETA=8:59:41, max mem: 20.9 GB 
[11/29 05:46:08 visual_prompt]: 	Training 500/553. train loss: 0.6974,	0.8120 s / batch. (data: 3.60e-04). ETA=8:29:37, max mem: 20.9 GB 
[11/29 05:47:00 visual_prompt]: Epoch 32 / 100: avg data time: 2.17e-01, avg batch time: 1.0403, average train loss: 0.8809
[11/29 05:47:59 visual_prompt]: Inference (val):avg data time: 3.67e-05, avg batch time: 0.3064, average loss: 0.7373
[11/29 05:47:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 49.74	
[11/29 05:47:59 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[11/29 05:49:46 visual_prompt]: 	Training 100/553. train loss: 0.0345,	0.8320 s / batch. (data: 3.35e-04). ETA=8:40:03, max mem: 20.9 GB 
[11/29 05:51:32 visual_prompt]: 	Training 200/553. train loss: 3.4234,	1.6088 s / batch. (data: 8.04e-01). ETA=16:42:53, max mem: 20.9 GB 
[11/29 05:53:14 visual_prompt]: 	Training 300/553. train loss: 0.6207,	0.8339 s / batch. (data: 3.06e-04). ETA=8:38:29, max mem: 20.9 GB 
[11/29 05:54:59 visual_prompt]: 	Training 400/553. train loss: 0.9057,	0.8480 s / batch. (data: 1.19e-02). ETA=8:45:48, max mem: 20.9 GB 
[11/29 05:56:42 visual_prompt]: 	Training 500/553. train loss: 0.5455,	0.8376 s / batch. (data: 5.44e-03). ETA=8:37:59, max mem: 20.9 GB 
[11/29 05:57:35 visual_prompt]: Epoch 33 / 100: avg data time: 2.19e-01, avg batch time: 1.0415, average train loss: 1.1526
[11/29 05:58:35 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3054, average loss: 1.1265
[11/29 05:58:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.90	
[11/29 05:58:35 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[11/29 06:00:25 visual_prompt]: 	Training 100/553. train loss: 0.9483,	0.8360 s / batch. (data: 4.18e-04). ETA=8:34:50, max mem: 20.9 GB 
[11/29 06:02:06 visual_prompt]: 	Training 200/553. train loss: 0.8462,	1.3639 s / batch. (data: 5.20e-01). ETA=13:57:41, max mem: 20.9 GB 
[11/29 06:03:48 visual_prompt]: 	Training 300/553. train loss: 0.9869,	0.8600 s / batch. (data: 3.87e-02). ETA=8:46:46, max mem: 20.9 GB 
[11/29 06:05:33 visual_prompt]: 	Training 400/553. train loss: 0.8354,	0.8303 s / batch. (data: 1.20e-02). ETA=8:27:11, max mem: 20.9 GB 
[11/29 06:07:17 visual_prompt]: 	Training 500/553. train loss: 0.6489,	1.6409 s / batch. (data: 8.22e-01). ETA=16:39:35, max mem: 20.9 GB 
[11/29 06:08:10 visual_prompt]: Epoch 34 / 100: avg data time: 2.17e-01, avg batch time: 1.0394, average train loss: 1.0422
[11/29 06:09:09 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3067, average loss: 0.6918
[11/29 06:09:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.79	
[11/29 06:09:09 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[11/29 06:10:58 visual_prompt]: 	Training 100/553. train loss: 1.3668,	0.8280 s / batch. (data: 3.18e-04). ETA=8:22:17, max mem: 20.9 GB 
[11/29 06:12:44 visual_prompt]: 	Training 200/553. train loss: 1.0887,	0.8280 s / batch. (data: 3.82e-04). ETA=8:20:54, max mem: 20.9 GB 
[11/29 06:14:25 visual_prompt]: 	Training 300/553. train loss: 1.1288,	0.8320 s / batch. (data: 3.25e-04). ETA=8:21:57, max mem: 20.9 GB 
[11/29 06:16:08 visual_prompt]: 	Training 400/553. train loss: 1.5932,	0.8101 s / batch. (data: 3.34e-04). ETA=8:07:24, max mem: 20.9 GB 
[11/29 06:17:51 visual_prompt]: 	Training 500/553. train loss: 0.7116,	1.4461 s / batch. (data: 6.41e-01). ETA=14:27:35, max mem: 20.9 GB 
[11/29 06:18:45 visual_prompt]: Epoch 35 / 100: avg data time: 2.20e-01, avg batch time: 1.0418, average train loss: 1.0545
[11/29 06:19:45 visual_prompt]: Inference (val):avg data time: 4.43e-05, avg batch time: 0.3066, average loss: 2.2827
[11/29 06:19:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.91	
[11/29 06:19:45 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[11/29 06:21:32 visual_prompt]: 	Training 100/553. train loss: 0.6815,	0.8350 s / batch. (data: 3.22e-04). ETA=8:18:48, max mem: 20.9 GB 
[11/29 06:23:16 visual_prompt]: 	Training 200/553. train loss: 2.7106,	0.9465 s / batch. (data: 1.25e-01). ETA=9:23:52, max mem: 20.9 GB 
[11/29 06:25:02 visual_prompt]: 	Training 300/553. train loss: 0.0736,	0.8200 s / batch. (data: 7.96e-03). ETA=8:07:09, max mem: 20.9 GB 
[11/29 06:26:45 visual_prompt]: 	Training 400/553. train loss: 0.8018,	0.8200 s / batch. (data: 2.95e-04). ETA=8:05:47, max mem: 20.9 GB 
[11/29 06:28:30 visual_prompt]: 	Training 500/553. train loss: 0.7004,	0.8371 s / batch. (data: 1.69e-02). ETA=8:14:31, max mem: 20.9 GB 
[11/29 06:29:20 visual_prompt]: Epoch 36 / 100: avg data time: 2.18e-01, avg batch time: 1.0403, average train loss: 1.1824
[11/29 06:30:20 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3078, average loss: 2.6534
[11/29 06:30:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 40.09	
[11/29 06:30:20 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[11/29 06:32:08 visual_prompt]: 	Training 100/553. train loss: 1.0975,	0.8070 s / batch. (data: 3.34e-04). ETA=7:54:40, max mem: 20.9 GB 
[11/29 06:33:50 visual_prompt]: 	Training 200/553. train loss: 0.7176,	0.8374 s / batch. (data: 1.05e-02). ETA=8:11:09, max mem: 20.9 GB 
[11/29 06:35:35 visual_prompt]: 	Training 300/553. train loss: 1.9940,	1.8107 s / batch. (data: 9.64e-01). ETA=17:38:59, max mem: 20.9 GB 
[11/29 06:37:20 visual_prompt]: 	Training 400/553. train loss: 0.6749,	2.0119 s / batch. (data: 1.19e+00). ETA=19:33:21, max mem: 20.9 GB 
[11/29 06:39:01 visual_prompt]: 	Training 500/553. train loss: 1.3532,	1.3235 s / batch. (data: 4.92e-01). ETA=12:49:38, max mem: 20.9 GB 
[11/29 06:39:56 visual_prompt]: Epoch 37 / 100: avg data time: 2.20e-01, avg batch time: 1.0424, average train loss: 1.2433
[11/29 06:40:57 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.3052, average loss: 0.7133
[11/29 06:40:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.38	
[11/29 06:40:57 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[11/29 06:42:43 visual_prompt]: 	Training 100/553. train loss: 0.5807,	0.9040 s / batch. (data: 5.98e-02). ETA=8:43:23, max mem: 20.9 GB 
[11/29 06:44:27 visual_prompt]: 	Training 200/553. train loss: 0.8502,	0.8273 s / batch. (data: 1.06e-02). ETA=7:57:35, max mem: 20.9 GB 
[11/29 06:46:12 visual_prompt]: 	Training 300/553. train loss: 0.8143,	0.8180 s / batch. (data: 3.40e-04). ETA=7:50:53, max mem: 20.9 GB 
[11/29 06:47:54 visual_prompt]: 	Training 400/553. train loss: 0.6667,	1.1416 s / batch. (data: 3.22e-01). ETA=10:55:14, max mem: 20.9 GB 
[11/29 06:49:40 visual_prompt]: 	Training 500/553. train loss: 1.4159,	0.8219 s / batch. (data: 3.03e-04). ETA=7:50:23, max mem: 20.9 GB 
[11/29 06:50:33 visual_prompt]: Epoch 38 / 100: avg data time: 2.19e-01, avg batch time: 1.0418, average train loss: 1.0341
[11/29 06:51:32 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3052, average loss: 1.5270
[11/29 06:51:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.81	
[11/29 06:51:32 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[11/29 06:53:19 visual_prompt]: 	Training 100/553. train loss: 0.0641,	0.8120 s / batch. (data: 3.10e-04). ETA=7:42:40, max mem: 20.9 GB 
[11/29 06:55:06 visual_prompt]: 	Training 200/553. train loss: 1.1026,	0.8313 s / batch. (data: 1.53e-02). ETA=7:52:17, max mem: 20.9 GB 
[11/29 06:56:53 visual_prompt]: 	Training 300/553. train loss: 1.9701,	0.8347 s / batch. (data: 1.05e-02). ETA=7:52:48, max mem: 20.9 GB 
[11/29 06:58:35 visual_prompt]: 	Training 400/553. train loss: 0.6570,	0.8334 s / batch. (data: 3.36e-04). ETA=7:50:40, max mem: 20.9 GB 
[11/29 07:00:19 visual_prompt]: 	Training 500/553. train loss: 0.5838,	1.9866 s / batch. (data: 1.18e+00). ETA=18:38:38, max mem: 20.9 GB 
[11/29 07:01:11 visual_prompt]: Epoch 39 / 100: avg data time: 2.23e-01, avg batch time: 1.0458, average train loss: 0.9864
[11/29 07:02:10 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.3048, average loss: 0.9107
[11/29 07:02:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.25	
[11/29 07:02:10 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[11/29 07:03:59 visual_prompt]: 	Training 100/553. train loss: 1.6172,	0.8263 s / batch. (data: 5.50e-03). ETA=7:43:09, max mem: 20.9 GB 
[11/29 07:05:41 visual_prompt]: 	Training 200/553. train loss: 2.2256,	0.8360 s / batch. (data: 3.24e-04). ETA=7:47:14, max mem: 20.9 GB 
[11/29 07:07:27 visual_prompt]: 	Training 300/553. train loss: 2.0742,	0.8145 s / batch. (data: 7.95e-03). ETA=7:33:50, max mem: 20.9 GB 
[11/29 07:09:12 visual_prompt]: 	Training 400/553. train loss: 0.7869,	0.8171 s / batch. (data: 5.50e-03). ETA=7:33:57, max mem: 20.9 GB 
[11/29 07:10:55 visual_prompt]: 	Training 500/553. train loss: 0.5239,	0.8240 s / batch. (data: 3.01e-04). ETA=7:36:24, max mem: 20.9 GB 
[11/29 07:11:50 visual_prompt]: Epoch 40 / 100: avg data time: 2.27e-01, avg batch time: 1.0496, average train loss: 1.2190
[11/29 07:12:50 visual_prompt]: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3070, average loss: 0.6916
[11/29 07:12:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 54.74	
[11/29 07:12:50 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[11/29 07:14:43 visual_prompt]: 	Training 100/553. train loss: 1.1397,	0.8482 s / batch. (data: 7.95e-03). ETA=7:47:38, max mem: 20.9 GB 
[11/29 07:16:30 visual_prompt]: 	Training 200/553. train loss: 1.6828,	0.8460 s / batch. (data: 7.32e-04). ETA=7:45:02, max mem: 20.9 GB 
[11/29 07:18:13 visual_prompt]: 	Training 300/553. train loss: 0.7830,	0.8425 s / batch. (data: 1.04e-02). ETA=7:41:41, max mem: 20.9 GB 
[11/29 07:19:56 visual_prompt]: 	Training 400/553. train loss: 0.8210,	0.8745 s / batch. (data: 1.58e-02). ETA=7:57:47, max mem: 20.9 GB 
[11/29 07:21:36 visual_prompt]: 	Training 500/553. train loss: 0.7016,	0.8174 s / batch. (data: 1.12e-02). ETA=7:25:11, max mem: 20.9 GB 
[11/29 07:22:29 visual_prompt]: Epoch 41 / 100: avg data time: 2.22e-01, avg batch time: 1.0461, average train loss: 1.2086
[11/29 07:23:28 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.3057, average loss: 1.9049
[11/29 07:23:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.74	
[11/29 07:23:28 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[11/29 07:25:15 visual_prompt]: 	Training 100/553. train loss: 1.2751,	0.8200 s / batch. (data: 3.17e-04). ETA=7:24:30, max mem: 20.9 GB 
[11/29 07:26:58 visual_prompt]: 	Training 200/553. train loss: 4.0036,	0.8328 s / batch. (data: 2.38e-02). ETA=7:30:04, max mem: 20.9 GB 
[11/29 07:28:43 visual_prompt]: 	Training 300/553. train loss: 0.6465,	0.8440 s / batch. (data: 1.20e-02). ETA=7:34:45, max mem: 20.9 GB 
[11/29 07:30:27 visual_prompt]: 	Training 400/553. train loss: 0.7100,	0.8163 s / batch. (data: 5.43e-03). ETA=7:18:26, max mem: 20.9 GB 
[11/29 07:32:11 visual_prompt]: 	Training 500/553. train loss: 1.0262,	0.8351 s / batch. (data: 4.87e-04). ETA=7:27:10, max mem: 20.9 GB 
[11/29 07:33:06 visual_prompt]: Epoch 42 / 100: avg data time: 2.23e-01, avg batch time: 1.0441, average train loss: 1.3379
[11/29 07:34:05 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3050, average loss: 0.7310
[11/29 07:34:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.59	
[11/29 07:34:05 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[11/29 07:35:54 visual_prompt]: 	Training 100/553. train loss: 0.5841,	0.8101 s / batch. (data: 2.91e-04). ETA=7:11:41, max mem: 20.9 GB 
[11/29 07:37:37 visual_prompt]: 	Training 200/553. train loss: 0.7191,	0.8381 s / batch. (data: 1.57e-02). ETA=7:25:13, max mem: 20.9 GB 
[11/29 07:39:18 visual_prompt]: 	Training 300/553. train loss: 1.3362,	0.8261 s / batch. (data: 4.65e-04). ETA=7:17:28, max mem: 20.9 GB 
[11/29 07:41:01 visual_prompt]: 	Training 400/553. train loss: 0.6597,	0.8240 s / batch. (data: 7.93e-03). ETA=7:14:58, max mem: 20.9 GB 
[11/29 07:42:46 visual_prompt]: 	Training 500/553. train loss: 0.6791,	0.8173 s / batch. (data: 4.06e-04). ETA=7:10:04, max mem: 20.9 GB 
[11/29 07:43:41 visual_prompt]: Epoch 43 / 100: avg data time: 2.18e-01, avg batch time: 1.0413, average train loss: 1.1429
[11/29 07:44:41 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.3069, average loss: 0.8117
[11/29 07:44:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.13	
[11/29 07:44:41 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[11/29 07:46:29 visual_prompt]: 	Training 100/553. train loss: 0.5694,	0.8346 s / batch. (data: 3.11e-04). ETA=7:17:03, max mem: 20.9 GB 
[11/29 07:48:14 visual_prompt]: 	Training 200/553. train loss: 0.8311,	0.8210 s / batch. (data: 3.77e-04). ETA=7:08:34, max mem: 20.9 GB 
[11/29 07:49:55 visual_prompt]: 	Training 300/553. train loss: 0.6640,	0.8095 s / batch. (data: 3.13e-04). ETA=7:01:12, max mem: 20.9 GB 
[11/29 07:51:37 visual_prompt]: 	Training 400/553. train loss: 0.7252,	0.8228 s / batch. (data: 7.94e-03). ETA=7:06:46, max mem: 20.9 GB 
[11/29 07:53:20 visual_prompt]: 	Training 500/553. train loss: 0.6982,	0.8509 s / batch. (data: 9.56e-03). ETA=7:19:56, max mem: 20.9 GB 
[11/29 07:54:15 visual_prompt]: Epoch 44 / 100: avg data time: 2.09e-01, avg batch time: 1.0379, average train loss: 1.0470
[11/29 07:55:14 visual_prompt]: Inference (val):avg data time: 3.92e-05, avg batch time: 0.3087, average loss: 1.0421
[11/29 07:55:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.24	
[11/29 07:55:14 visual_prompt]: Stopping early.
[11/29 07:55:14 visual_prompt]: Rank of current process: 0. World size: 1
[11/29 07:55:14 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/29 07:55:14 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/29 07:55:14 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/29 07:55:14 visual_prompt]: Training with config:
[11/29 07:55:14 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr0.5_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/29 07:55:14 visual_prompt]: Loading training data...
[11/29 07:55:14 visual_prompt]: Constructing mammo-cbis dataset train...
[11/29 07:55:14 visual_prompt]: Loading validation data...
[11/29 07:55:14 visual_prompt]: Constructing mammo-cbis dataset val...
[11/29 07:55:14 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 2501, 768])
load_pretrained: grid-size from 14 to 50
[11/29 07:55:17 visual_prompt]: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/29 07:55:17 visual_prompt]: tuned percent:0.525
[11/29 07:55:17 visual_prompt]: Device used for model: 0
[11/29 07:55:17 visual_prompt]: Setting up Evaluator...
[11/29 07:55:17 visual_prompt]: Setting up Trainer...
[11/29 07:55:17 visual_prompt]: 	Setting up the optimizer...
[11/29 07:55:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/29 07:57:04 visual_prompt]: 	Training 100/553. train loss: 2.1087,	0.8600 s / batch. (data: 3.78e-04). ETA=13:11:12, max mem: 20.9 GB 
[11/29 07:58:46 visual_prompt]: 	Training 200/553. train loss: 1.3578,	0.8382 s / batch. (data: 1.05e-02). ETA=12:49:44, max mem: 20.9 GB 
[11/29 08:00:32 visual_prompt]: 	Training 300/553. train loss: 1.3905,	1.2760 s / batch. (data: 4.51e-01). ETA=19:29:40, max mem: 20.9 GB 
[11/29 08:02:14 visual_prompt]: 	Training 400/553. train loss: 0.0383,	0.8154 s / batch. (data: 4.08e-04). ETA=12:26:04, max mem: 20.9 GB 
[11/29 08:04:00 visual_prompt]: 	Training 500/553. train loss: 0.9538,	0.8530 s / batch. (data: 8.00e-04). ETA=12:59:02, max mem: 20.9 GB 
[11/29 08:04:54 visual_prompt]: Epoch 1 / 100: avg data time: 2.18e-01, avg batch time: 1.0424, average train loss: 1.5403
[11/29 08:05:53 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.3082, average loss: 1.5201
[11/29 08:05:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/29 08:05:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[11/29 08:07:40 visual_prompt]: 	Training 100/553. train loss: 0.7679,	1.3079 s / batch. (data: 4.87e-01). ETA=19:51:13, max mem: 20.9 GB 
[11/29 08:09:23 visual_prompt]: 	Training 200/553. train loss: 0.0953,	0.8418 s / batch. (data: 3.81e-04). ETA=12:45:16, max mem: 20.9 GB 
[11/29 08:11:08 visual_prompt]: 	Training 300/553. train loss: 0.8468,	1.1485 s / batch. (data: 3.19e-01). ETA=17:22:13, max mem: 20.9 GB 
[11/29 08:12:50 visual_prompt]: 	Training 400/553. train loss: 1.5797,	0.8280 s / batch. (data: 3.18e-04). ETA=12:29:59, max mem: 20.9 GB 
[11/29 08:14:34 visual_prompt]: 	Training 500/553. train loss: 0.5381,	0.8360 s / batch. (data: 3.07e-04). ETA=12:35:50, max mem: 20.9 GB 
[11/29 08:15:27 visual_prompt]: Epoch 2 / 100: avg data time: 2.14e-01, avg batch time: 1.0377, average train loss: 0.9151
[11/29 08:16:26 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3072, average loss: 1.2192
[11/29 08:16:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.14	
[11/29 08:16:26 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[11/29 08:18:13 visual_prompt]: 	Training 100/553. train loss: 0.7689,	0.8320 s / batch. (data: 1.19e-02). ETA=12:30:06, max mem: 20.9 GB 
[11/29 08:19:57 visual_prompt]: 	Training 200/553. train loss: 0.8610,	0.8138 s / batch. (data: 3.28e-04). ETA=12:12:18, max mem: 20.9 GB 
[11/29 08:21:40 visual_prompt]: 	Training 300/553. train loss: 0.6059,	0.8216 s / batch. (data: 3.46e-04). ETA=12:17:56, max mem: 20.9 GB 
[11/29 08:23:24 visual_prompt]: 	Training 400/553. train loss: 1.8079,	0.8240 s / batch. (data: 7.93e-03). ETA=12:18:46, max mem: 20.9 GB 
[11/29 08:25:09 visual_prompt]: 	Training 500/553. train loss: 0.8238,	1.4080 s / batch. (data: 5.66e-01). ETA=21:00:00, max mem: 20.9 GB 
[11/29 08:26:01 visual_prompt]: Epoch 3 / 100: avg data time: 2.16e-01, avg batch time: 1.0395, average train loss: 0.8850
[11/29 08:27:01 visual_prompt]: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3072, average loss: 0.7381
[11/29 08:27:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.87	
[11/29 08:27:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[11/29 08:28:50 visual_prompt]: 	Training 100/553. train loss: 0.8136,	0.8280 s / batch. (data: 3.24e-04). ETA=12:18:52, max mem: 20.9 GB 
[11/29 08:30:34 visual_prompt]: 	Training 200/553. train loss: 0.5487,	0.8445 s / batch. (data: 2.04e-02). ETA=12:32:08, max mem: 20.9 GB 
[11/29 08:32:18 visual_prompt]: 	Training 300/553. train loss: 0.9565,	1.4766 s / batch. (data: 6.49e-01). ETA=21:52:42, max mem: 20.9 GB 
[11/29 08:33:56 visual_prompt]: 	Training 400/553. train loss: 1.2003,	1.4880 s / batch. (data: 6.57e-01). ETA=22:00:23, max mem: 20.9 GB 
[11/29 08:35:41 visual_prompt]: 	Training 500/553. train loss: 0.2921,	3.7046 s / batch. (data: 2.90e+00). ETA=2 days, 6:41:04, max mem: 20.9 GB 
[11/29 08:36:37 visual_prompt]: Epoch 4 / 100: avg data time: 2.18e-01, avg batch time: 1.0411, average train loss: 0.9867
[11/29 08:37:36 visual_prompt]: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3058, average loss: 1.5527
[11/29 08:37:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.95	
[11/29 08:37:36 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[11/29 08:39:22 visual_prompt]: 	Training 100/553. train loss: 2.0901,	0.8176 s / batch. (data: 2.97e-04). ETA=12:02:05, max mem: 20.9 GB 
[11/29 08:41:06 visual_prompt]: 	Training 200/553. train loss: 0.8485,	1.6047 s / batch. (data: 7.98e-01). ETA=23:34:28, max mem: 20.9 GB 
[11/29 08:42:50 visual_prompt]: 	Training 300/553. train loss: 1.4419,	0.8571 s / batch. (data: 1.32e-02). ETA=12:34:02, max mem: 20.9 GB 
[11/29 08:44:32 visual_prompt]: 	Training 400/553. train loss: 1.5371,	0.8417 s / batch. (data: 3.10e-04). ETA=12:19:09, max mem: 20.9 GB 
[11/29 08:46:16 visual_prompt]: 	Training 500/553. train loss: 0.5277,	0.8148 s / batch. (data: 2.99e-04). ETA=11:54:10, max mem: 20.9 GB 
[11/29 08:47:11 visual_prompt]: Epoch 5 / 100: avg data time: 2.15e-01, avg batch time: 1.0389, average train loss: 1.0075
[11/29 08:48:10 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.3066, average loss: 1.4005
[11/29 08:48:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.86	
[11/29 08:48:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[11/29 08:49:59 visual_prompt]: 	Training 100/553. train loss: 0.6448,	0.8240 s / batch. (data: 7.91e-04). ETA=12:00:07, max mem: 20.9 GB 
[11/29 08:51:42 visual_prompt]: 	Training 200/553. train loss: 2.5418,	0.8436 s / batch. (data: 1.56e-02). ETA=12:15:51, max mem: 20.9 GB 
[11/29 08:53:24 visual_prompt]: 	Training 300/553. train loss: 0.5570,	0.8309 s / batch. (data: 1.06e-02). ETA=12:03:21, max mem: 20.9 GB 
[11/29 08:55:11 visual_prompt]: 	Training 400/553. train loss: 0.8850,	0.8226 s / batch. (data: 3.49e-04). ETA=11:54:46, max mem: 20.9 GB 
[11/29 08:56:52 visual_prompt]: 	Training 500/553. train loss: 1.2508,	0.8134 s / batch. (data: 5.45e-03). ETA=11:45:23, max mem: 20.9 GB 
[11/29 08:57:46 visual_prompt]: Epoch 6 / 100: avg data time: 2.17e-01, avg batch time: 1.0409, average train loss: 1.0922
[11/29 08:58:46 visual_prompt]: Inference (val):avg data time: 1.63e-04, avg batch time: 0.3085, average loss: 1.2903
[11/29 08:58:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.99	
[11/29 08:58:46 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[11/29 09:00:33 visual_prompt]: 	Training 100/553. train loss: 2.6519,	0.8075 s / batch. (data: 3.17e-04). ETA=11:38:15, max mem: 20.9 GB 
[11/29 09:02:15 visual_prompt]: 	Training 200/553. train loss: 0.5422,	0.8610 s / batch. (data: 1.05e-02). ETA=12:23:06, max mem: 20.9 GB 
[11/29 09:04:02 visual_prompt]: 	Training 300/553. train loss: 0.7642,	2.0716 s / batch. (data: 1.26e+00). ETA=1 day, 5:44:22, max mem: 20.9 GB 
[11/29 09:05:45 visual_prompt]: 	Training 400/553. train loss: 0.5958,	2.0440 s / batch. (data: 1.21e+00). ETA=1 day, 5:17:14, max mem: 20.9 GB 
[11/29 09:07:27 visual_prompt]: 	Training 500/553. train loss: 1.4260,	0.8142 s / batch. (data: 3.57e-04). ETA=11:38:34, max mem: 20.9 GB 
[11/29 09:08:20 visual_prompt]: Epoch 7 / 100: avg data time: 2.14e-01, avg batch time: 1.0378, average train loss: 0.9920
[11/29 09:09:20 visual_prompt]: Inference (val):avg data time: 3.90e-05, avg batch time: 0.3040, average loss: 0.6865
[11/29 09:09:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 63.35	
[11/29 09:09:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[11/29 09:11:05 visual_prompt]: 	Training 100/553. train loss: 1.1387,	0.8254 s / batch. (data: 9.34e-03). ETA=11:46:07, max mem: 20.9 GB 
[11/29 09:12:51 visual_prompt]: 	Training 200/553. train loss: 1.0875,	0.8400 s / batch. (data: 3.31e-04). ETA=11:57:13, max mem: 20.9 GB 
[11/29 09:14:35 visual_prompt]: 	Training 300/553. train loss: 1.8679,	0.8165 s / batch. (data: 3.02e-04). ETA=11:35:45, max mem: 20.9 GB 
[11/29 09:16:18 visual_prompt]: 	Training 400/553. train loss: 0.7211,	0.8360 s / batch. (data: 3.66e-04). ETA=11:50:58, max mem: 20.9 GB 
[11/29 09:18:02 visual_prompt]: 	Training 500/553. train loss: 1.6014,	1.2361 s / batch. (data: 3.98e-01). ETA=17:29:10, max mem: 20.9 GB 
[11/29 09:18:57 visual_prompt]: Epoch 8 / 100: avg data time: 2.21e-01, avg batch time: 1.0436, average train loss: 1.1053
[11/29 09:19:57 visual_prompt]: Inference (val):avg data time: 4.45e-05, avg batch time: 0.3061, average loss: 0.6950
[11/29 09:19:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.56	rocauc: 60.88	
[11/29 09:19:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[11/29 09:21:45 visual_prompt]: 	Training 100/553. train loss: 0.1361,	0.8461 s / batch. (data: 1.01e-02). ETA=11:56:02, max mem: 20.9 GB 
[11/29 09:23:28 visual_prompt]: 	Training 200/553. train loss: 0.7272,	0.8240 s / batch. (data: 1.19e-02). ETA=11:35:58, max mem: 20.9 GB 
[11/29 09:25:12 visual_prompt]: 	Training 300/553. train loss: 0.7011,	1.2817 s / batch. (data: 4.63e-01). ETA=18:00:21, max mem: 20.9 GB 
[11/29 09:26:57 visual_prompt]: 	Training 400/553. train loss: 0.6741,	0.8434 s / batch. (data: 2.24e-02). ETA=11:49:33, max mem: 20.9 GB 
[11/29 09:28:42 visual_prompt]: 	Training 500/553. train loss: 1.0512,	1.1331 s / batch. (data: 3.11e-01). ETA=15:51:20, max mem: 20.9 GB 
[11/29 09:29:34 visual_prompt]: Epoch 9 / 100: avg data time: 2.18e-01, avg batch time: 1.0426, average train loss: 0.8821
[11/29 09:30:34 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.3075, average loss: 1.2095
[11/29 09:30:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.04	
[11/29 09:30:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[11/29 09:32:25 visual_prompt]: 	Training 100/553. train loss: 1.7716,	0.8092 s / batch. (data: 3.72e-04). ETA=11:17:21, max mem: 20.9 GB 
[11/29 09:34:07 visual_prompt]: 	Training 200/553. train loss: 0.7371,	0.8373 s / batch. (data: 5.47e-03). ETA=11:39:26, max mem: 20.9 GB 
[11/29 09:35:50 visual_prompt]: 	Training 300/553. train loss: 0.4131,	0.8555 s / batch. (data: 2.10e-02). ETA=11:53:14, max mem: 20.9 GB 
[11/29 09:37:31 visual_prompt]: 	Training 400/553. train loss: 0.6812,	0.8713 s / batch. (data: 6.38e-02). ETA=12:04:58, max mem: 20.9 GB 
[11/29 09:39:16 visual_prompt]: 	Training 500/553. train loss: 0.5299,	0.9794 s / batch. (data: 1.46e-01). ETA=13:33:17, max mem: 20.9 GB 
[11/29 09:40:11 visual_prompt]: Epoch 10 / 100: avg data time: 2.19e-01, avg batch time: 1.0427, average train loss: 1.1561
[11/29 09:41:11 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.3065, average loss: 0.7289
[11/29 09:41:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.17	
[11/29 09:41:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[11/29 09:43:01 visual_prompt]: 	Training 100/553. train loss: 2.1311,	0.8360 s / batch. (data: 2.96e-04). ETA=11:32:04, max mem: 20.9 GB 
[11/29 09:44:46 visual_prompt]: 	Training 200/553. train loss: 1.6184,	0.8400 s / batch. (data: 7.95e-03). ETA=11:33:57, max mem: 20.9 GB 
[11/29 09:46:29 visual_prompt]: 	Training 300/553. train loss: 0.0302,	1.9298 s / batch. (data: 1.10e+00). ETA=1 day, 2:31:06, max mem: 20.9 GB 
[11/29 09:48:11 visual_prompt]: 	Training 400/553. train loss: 0.6176,	0.8143 s / batch. (data: 4.07e-04). ETA=11:10:03, max mem: 20.9 GB 
[11/29 09:49:54 visual_prompt]: 	Training 500/553. train loss: 0.7791,	0.8360 s / batch. (data: 3.00e-04). ETA=11:26:28, max mem: 20.9 GB 
[11/29 09:50:46 visual_prompt]: Epoch 11 / 100: avg data time: 2.16e-01, avg batch time: 1.0407, average train loss: 0.9843
[11/29 09:51:46 visual_prompt]: Inference (val):avg data time: 1.37e-04, avg batch time: 0.3051, average loss: 0.6849
[11/29 09:51:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.17	
[11/29 09:51:46 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[11/29 09:53:36 visual_prompt]: 	Training 100/553. train loss: 0.7765,	0.8402 s / batch. (data: 2.12e-02). ETA=11:27:47, max mem: 20.9 GB 
[11/29 09:55:21 visual_prompt]: 	Training 200/553. train loss: 0.6089,	1.9791 s / batch. (data: 1.17e+00). ETA=1 day, 2:56:48, max mem: 20.9 GB 
[11/29 09:57:02 visual_prompt]: 	Training 300/553. train loss: 0.7602,	0.8320 s / batch. (data: 7.96e-03). ETA=11:18:18, max mem: 20.9 GB 
[11/29 09:58:46 visual_prompt]: 	Training 400/553. train loss: 0.5513,	0.8207 s / batch. (data: 4.20e-04). ETA=11:07:44, max mem: 20.9 GB 
[11/29 10:00:31 visual_prompt]: 	Training 500/553. train loss: 4.2357,	0.8288 s / batch. (data: 8.05e-03). ETA=11:12:55, max mem: 20.9 GB 
[11/29 10:01:23 visual_prompt]: Epoch 12 / 100: avg data time: 2.20e-01, avg batch time: 1.0432, average train loss: 1.0818
[11/29 10:02:23 visual_prompt]: Inference (val):avg data time: 3.82e-05, avg batch time: 0.3097, average loss: 1.6687
[11/29 10:02:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.49	
[11/29 10:02:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[11/29 10:04:12 visual_prompt]: 	Training 100/553. train loss: 0.5888,	0.8392 s / batch. (data: 5.46e-03). ETA=11:19:16, max mem: 20.9 GB 
[11/29 10:05:53 visual_prompt]: 	Training 200/553. train loss: 0.6360,	0.8139 s / batch. (data: 5.58e-03). ETA=10:57:25, max mem: 20.9 GB 
[11/29 10:07:37 visual_prompt]: 	Training 300/553. train loss: 0.5082,	1.9932 s / batch. (data: 1.16e+00). ETA=1 day, 2:46:39, max mem: 20.9 GB 
[11/29 10:09:19 visual_prompt]: 	Training 400/553. train loss: 4.0660,	0.8253 s / batch. (data: 9.24e-03). ETA=11:03:51, max mem: 20.9 GB 
[11/29 10:11:05 visual_prompt]: 	Training 500/553. train loss: 0.8391,	0.8073 s / batch. (data: 4.40e-04). ETA=10:48:04, max mem: 20.9 GB 
[11/29 10:11:59 visual_prompt]: Epoch 13 / 100: avg data time: 2.17e-01, avg batch time: 1.0414, average train loss: 1.0679
[11/29 10:12:58 visual_prompt]: Inference (val):avg data time: 1.56e-04, avg batch time: 0.3054, average loss: 1.2025
[11/29 10:12:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.58	
[11/29 10:12:58 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[11/29 10:14:47 visual_prompt]: 	Training 100/553. train loss: 0.6127,	0.8440 s / batch. (data: 7.93e-03). ETA=11:15:22, max mem: 20.9 GB 
[11/29 10:16:30 visual_prompt]: 	Training 200/553. train loss: 0.1792,	1.4697 s / batch. (data: 6.46e-01). ETA=19:33:37, max mem: 20.9 GB 
[11/29 10:18:14 visual_prompt]: 	Training 300/553. train loss: 0.7110,	0.8330 s / batch. (data: 3.66e-04). ETA=11:03:44, max mem: 20.9 GB 
[11/29 10:19:57 visual_prompt]: 	Training 400/553. train loss: 0.6591,	0.8200 s / batch. (data: 3.66e-04). ETA=10:52:02, max mem: 20.9 GB 
[11/29 10:21:41 visual_prompt]: 	Training 500/553. train loss: 2.0891,	0.8321 s / batch. (data: 2.87e-04). ETA=11:00:16, max mem: 20.9 GB 
[11/29 10:22:34 visual_prompt]: Epoch 14 / 100: avg data time: 2.18e-01, avg batch time: 1.0417, average train loss: 1.1317
[11/29 10:23:34 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3055, average loss: 0.7286
[11/29 10:23:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.69	
[11/29 10:23:34 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[11/29 10:25:21 visual_prompt]: 	Training 100/553. train loss: 1.1146,	0.9000 s / batch. (data: 7.56e-02). ETA=11:51:50, max mem: 20.9 GB 
[11/29 10:27:04 visual_prompt]: 	Training 200/553. train loss: 6.5137,	0.8166 s / batch. (data: 2.91e-04). ETA=10:44:34, max mem: 20.9 GB 
[11/29 10:28:49 visual_prompt]: 	Training 300/553. train loss: 1.6192,	0.8224 s / batch. (data: 5.55e-03). ETA=10:47:44, max mem: 20.9 GB 
[11/29 10:30:30 visual_prompt]: 	Training 400/553. train loss: 1.1032,	0.8643 s / batch. (data: 2.43e-02). ETA=11:19:20, max mem: 20.9 GB 
[11/29 10:32:15 visual_prompt]: 	Training 500/553. train loss: 0.6259,	0.8280 s / batch. (data: 3.35e-04). ETA=10:49:22, max mem: 20.9 GB 
[11/29 10:33:09 visual_prompt]: Epoch 15 / 100: avg data time: 2.17e-01, avg batch time: 1.0404, average train loss: 1.3634
[11/29 10:34:09 visual_prompt]: Inference (val):avg data time: 3.95e-05, avg batch time: 0.3067, average loss: 2.0656
[11/29 10:34:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.88	
[11/29 10:34:09 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[11/29 10:35:55 visual_prompt]: 	Training 100/553. train loss: 0.8009,	0.8241 s / batch. (data: 2.93e-04). ETA=10:44:12, max mem: 20.9 GB 
[11/29 10:37:40 visual_prompt]: 	Training 200/553. train loss: 1.4988,	0.8532 s / batch. (data: 2.51e-02). ETA=11:05:32, max mem: 20.9 GB 
[11/29 10:39:23 visual_prompt]: 	Training 300/553. train loss: 1.0280,	0.8305 s / batch. (data: 2.24e-02). ETA=10:46:29, max mem: 20.9 GB 
[11/29 10:41:07 visual_prompt]: 	Training 400/553. train loss: 0.6460,	0.8345 s / batch. (data: 7.32e-04). ETA=10:48:11, max mem: 20.9 GB 
[11/29 10:42:50 visual_prompt]: 	Training 500/553. train loss: 0.9618,	1.0890 s / batch. (data: 2.77e-01). ETA=14:04:06, max mem: 20.9 GB 
[11/29 10:43:44 visual_prompt]: Epoch 16 / 100: avg data time: 2.16e-01, avg batch time: 1.0392, average train loss: 0.9734
[11/29 10:44:43 visual_prompt]: Inference (val):avg data time: 4.11e-05, avg batch time: 0.3063, average loss: 0.6828
[11/29 10:44:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.31	
[11/29 10:44:43 visual_prompt]: Best epoch 16: best metric: -0.683
[11/29 10:44:43 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[11/29 10:46:30 visual_prompt]: 	Training 100/553. train loss: 0.8205,	0.8443 s / batch. (data: 8.26e-03). ETA=10:52:15, max mem: 20.9 GB 
[11/29 10:48:15 visual_prompt]: 	Training 200/553. train loss: 1.7765,	0.8280 s / batch. (data: 3.82e-04). ETA=10:38:18, max mem: 20.9 GB 
[11/29 10:49:58 visual_prompt]: 	Training 300/553. train loss: 1.4992,	0.8280 s / batch. (data: 3.06e-04). ETA=10:36:54, max mem: 20.9 GB 
[11/29 10:52:16 visual_prompt]: 	Training 400/553. train loss: 0.9255,	0.8321 s / batch. (data: 3.81e-04). ETA=10:38:41, max mem: 20.9 GB 
[11/29 10:54:09 visual_prompt]: 	Training 500/553. train loss: 0.6938,	1.3193 s / batch. (data: 5.11e-01). ETA=16:50:22, max mem: 20.9 GB 
[11/29 10:55:04 visual_prompt]: Epoch 17 / 100: avg data time: 3.00e-01, avg batch time: 1.1227, average train loss: 0.9912
[11/29 10:56:04 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3082, average loss: 0.7131
[11/29 10:56:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 65.57	
[11/29 10:56:04 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[11/29 10:57:52 visual_prompt]: 	Training 100/553. train loss: 0.7639,	0.8368 s / batch. (data: 2.94e-04). ETA=10:38:43, max mem: 20.9 GB 
[11/29 10:59:39 visual_prompt]: 	Training 200/553. train loss: 1.1218,	0.8451 s / batch. (data: 1.16e-03). ETA=10:43:38, max mem: 20.9 GB 
[11/29 11:01:23 visual_prompt]: 	Training 300/553. train loss: 0.4815,	0.8540 s / batch. (data: 1.60e-02). ETA=10:49:00, max mem: 20.9 GB 
[11/29 11:03:06 visual_prompt]: 	Training 400/553. train loss: 0.9788,	0.8342 s / batch. (data: 1.07e-02). ETA=10:32:34, max mem: 20.9 GB 
[11/29 11:04:49 visual_prompt]: 	Training 500/553. train loss: 1.5766,	0.8345 s / batch. (data: 1.19e-02). ETA=10:31:24, max mem: 20.9 GB 
[11/29 11:05:41 visual_prompt]: Epoch 18 / 100: avg data time: 2.18e-01, avg batch time: 1.0431, average train loss: 1.3131
[11/29 11:06:41 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.3071, average loss: 0.9938
[11/29 11:06:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.91	
[11/29 11:06:41 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[11/29 11:08:28 visual_prompt]: 	Training 100/553. train loss: 0.8804,	0.8484 s / batch. (data: 2.57e-02). ETA=10:39:47, max mem: 20.9 GB 
[11/29 11:10:11 visual_prompt]: 	Training 200/553. train loss: 0.7377,	0.8279 s / batch. (data: 7.91e-03). ETA=10:22:58, max mem: 20.9 GB 
[11/29 11:11:55 visual_prompt]: 	Training 300/553. train loss: 3.0461,	0.8360 s / batch. (data: 3.28e-04). ETA=10:27:37, max mem: 20.9 GB 
[11/29 11:13:40 visual_prompt]: 	Training 400/553. train loss: 0.6691,	0.8456 s / batch. (data: 6.05e-03). ETA=10:33:28, max mem: 20.9 GB 
[11/29 11:15:19 visual_prompt]: 	Training 500/553. train loss: 0.5790,	0.8406 s / batch. (data: 1.06e-02). ETA=10:28:19, max mem: 20.9 GB 
[11/29 11:16:13 visual_prompt]: Epoch 19 / 100: avg data time: 2.11e-01, avg batch time: 1.0354, average train loss: 0.9104
[11/29 11:17:13 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.3065, average loss: 1.7240
[11/29 11:17:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.08	
[11/29 11:17:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[11/29 11:18:59 visual_prompt]: 	Training 100/553. train loss: 0.7645,	0.8256 s / batch. (data: 3.43e-04). ETA=10:14:58, max mem: 20.9 GB 
[11/29 11:20:44 visual_prompt]: 	Training 200/553. train loss: 0.4673,	0.8240 s / batch. (data: 2.92e-04). ETA=10:12:25, max mem: 20.9 GB 
[11/29 11:22:28 visual_prompt]: 	Training 300/553. train loss: 1.8378,	0.8088 s / batch. (data: 3.82e-04). ETA=9:59:47, max mem: 20.9 GB 
[11/29 11:24:11 visual_prompt]: 	Training 400/553. train loss: 0.5997,	0.8396 s / batch. (data: 3.31e-04). ETA=10:21:14, max mem: 20.9 GB 
[11/29 11:25:54 visual_prompt]: 	Training 500/553. train loss: 0.7122,	0.8307 s / batch. (data: 7.96e-03). ETA=10:13:13, max mem: 20.9 GB 
[11/29 11:26:49 visual_prompt]: Epoch 20 / 100: avg data time: 2.18e-01, avg batch time: 1.0417, average train loss: 0.9603
[11/29 11:27:48 visual_prompt]: Inference (val):avg data time: 4.07e-05, avg batch time: 0.3067, average loss: 0.6917
[11/29 11:27:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 64.64	
[11/29 11:27:48 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[11/29 11:29:38 visual_prompt]: 	Training 100/553. train loss: 0.9778,	1.6772 s / batch. (data: 8.55e-01). ETA=20:33:52, max mem: 20.9 GB 
[11/29 11:31:21 visual_prompt]: 	Training 200/553. train loss: 1.2474,	0.8320 s / batch. (data: 7.97e-03). ETA=10:10:40, max mem: 20.9 GB 
[11/29 11:33:03 visual_prompt]: 	Training 300/553. train loss: 2.5566,	0.8360 s / batch. (data: 3.66e-04). ETA=10:12:14, max mem: 20.9 GB 
[11/29 11:34:46 visual_prompt]: 	Training 400/553. train loss: 1.3980,	0.8411 s / batch. (data: 8.98e-03). ETA=10:14:33, max mem: 20.9 GB 
[11/29 11:36:31 visual_prompt]: 	Training 500/553. train loss: 0.8321,	0.8479 s / batch. (data: 5.60e-03). ETA=10:18:07, max mem: 20.9 GB 
[11/29 11:37:24 visual_prompt]: Epoch 21 / 100: avg data time: 2.18e-01, avg batch time: 1.0408, average train loss: 0.9988
[11/29 11:38:24 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.3061, average loss: 0.7338
[11/29 11:38:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 59.84	
[11/29 11:38:24 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[11/29 11:40:11 visual_prompt]: 	Training 100/553. train loss: 1.1736,	0.8325 s / batch. (data: 4.13e-04). ETA=10:04:47, max mem: 20.9 GB 
[11/29 11:41:54 visual_prompt]: 	Training 200/553. train loss: 0.6435,	0.8280 s / batch. (data: 7.94e-03). ETA=10:00:07, max mem: 20.9 GB 
[11/29 11:43:36 visual_prompt]: 	Training 300/553. train loss: 0.2492,	0.8149 s / batch. (data: 2.98e-04). ETA=9:49:18, max mem: 20.9 GB 
[11/29 11:45:20 visual_prompt]: 	Training 400/553. train loss: 0.5551,	0.8160 s / batch. (data: 5.49e-03). ETA=9:48:41, max mem: 20.9 GB 
[11/29 11:47:03 visual_prompt]: 	Training 500/553. train loss: 0.5989,	0.8208 s / batch. (data: 4.72e-04). ETA=9:50:48, max mem: 20.9 GB 
[11/29 11:47:58 visual_prompt]: Epoch 22 / 100: avg data time: 2.15e-01, avg batch time: 1.0385, average train loss: 0.9580
[11/29 11:48:57 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.3080, average loss: 0.8673
[11/29 11:48:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.81	
[11/29 11:48:57 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[11/29 11:50:46 visual_prompt]: 	Training 100/553. train loss: 1.5493,	0.8320 s / batch. (data: 3.28e-04). ETA=9:56:45, max mem: 20.9 GB 
[11/29 11:53:04 visual_prompt]: 	Training 200/553. train loss: 1.3994,	1.0203 s / batch. (data: 2.15e-01). ETA=12:10:04, max mem: 20.9 GB 
[11/29 11:54:54 visual_prompt]: 	Training 300/553. train loss: 0.6830,	0.8224 s / batch. (data: 4.22e-04). ETA=9:47:08, max mem: 20.9 GB 
[11/29 11:56:36 visual_prompt]: 	Training 400/553. train loss: 0.5141,	0.8120 s / batch. (data: 7.96e-04). ETA=9:38:19, max mem: 20.9 GB 
[11/29 11:58:18 visual_prompt]: 	Training 500/553. train loss: 1.0323,	0.8360 s / batch. (data: 5.42e-03). ETA=9:54:01, max mem: 20.9 GB 
[11/29 11:59:12 visual_prompt]: Epoch 23 / 100: avg data time: 2.88e-01, avg batch time: 1.1107, average train loss: 0.9491
[11/29 12:00:11 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.3072, average loss: 0.7012
[11/29 12:00:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 62.77	
[11/29 12:00:11 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[11/29 12:01:56 visual_prompt]: 	Training 100/553. train loss: 1.1732,	0.8393 s / batch. (data: 4.18e-04). ETA=9:54:15, max mem: 20.9 GB 
[11/29 12:03:39 visual_prompt]: 	Training 200/553. train loss: 0.7263,	0.8226 s / batch. (data: 3.03e-04). ETA=9:41:00, max mem: 20.9 GB 
[11/29 12:05:23 visual_prompt]: 	Training 300/553. train loss: 0.6324,	0.9800 s / batch. (data: 1.54e-01). ETA=11:30:35, max mem: 20.9 GB 
[11/29 12:07:07 visual_prompt]: 	Training 400/553. train loss: 0.5983,	0.8315 s / batch. (data: 1.14e-02). ETA=9:44:32, max mem: 20.9 GB 
[11/29 12:08:52 visual_prompt]: 	Training 500/553. train loss: 0.6307,	0.8211 s / batch. (data: 4.29e-04). ETA=9:35:52, max mem: 20.9 GB 
[11/29 12:09:47 visual_prompt]: Epoch 24 / 100: avg data time: 2.18e-01, avg batch time: 1.0414, average train loss: 0.9542
[11/29 12:10:47 visual_prompt]: Inference (val):avg data time: 2.53e-04, avg batch time: 0.3076, average loss: 1.0284
[11/29 12:10:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 66.76	
[11/29 12:10:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[11/29 12:12:38 visual_prompt]: 	Training 100/553. train loss: 0.7718,	0.8231 s / batch. (data: 5.51e-03). ETA=9:35:11, max mem: 20.9 GB 
[11/29 12:14:18 visual_prompt]: 	Training 200/553. train loss: 1.9327,	1.2139 s / batch. (data: 3.76e-01). ETA=14:06:16, max mem: 20.9 GB 
[11/29 12:16:01 visual_prompt]: 	Training 300/553. train loss: 0.6931,	1.2513 s / batch. (data: 4.33e-01). ETA=14:30:15, max mem: 20.9 GB 
[11/29 12:17:45 visual_prompt]: 	Training 400/553. train loss: 0.6703,	1.4653 s / batch. (data: 6.14e-01). ETA=16:56:39, max mem: 20.9 GB 
[11/29 12:19:29 visual_prompt]: 	Training 500/553. train loss: 1.3267,	1.9479 s / batch. (data: 1.13e+00). ETA=22:28:14, max mem: 20.9 GB 
[11/29 12:20:23 visual_prompt]: Epoch 25 / 100: avg data time: 2.16e-01, avg batch time: 1.0411, average train loss: 0.9651
[11/29 12:21:23 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3073, average loss: 1.7048
[11/29 12:21:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 62.19	
[11/29 12:21:23 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[11/29 12:23:10 visual_prompt]: 	Training 100/553. train loss: 0.5803,	0.8358 s / batch. (data: 3.01e-04). ETA=9:36:21, max mem: 20.9 GB 
[11/29 12:24:55 visual_prompt]: 	Training 200/553. train loss: 1.9303,	1.7570 s / batch. (data: 9.29e-01). ETA=20:08:40, max mem: 20.9 GB 
[11/29 12:26:39 visual_prompt]: 	Training 300/553. train loss: 0.4806,	0.8240 s / batch. (data: 3.06e-04). ETA=9:25:28, max mem: 20.9 GB 
[11/29 12:28:23 visual_prompt]: 	Training 400/553. train loss: 1.2638,	0.8077 s / batch. (data: 3.02e-04). ETA=9:12:54, max mem: 20.9 GB 
[11/29 12:30:04 visual_prompt]: 	Training 500/553. train loss: 0.5712,	0.8236 s / batch. (data: 5.57e-03). ETA=9:22:26, max mem: 20.9 GB 
[11/29 12:30:58 visual_prompt]: Epoch 26 / 100: avg data time: 2.16e-01, avg batch time: 1.0397, average train loss: 0.9355
[11/29 12:31:57 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.3053, average loss: 0.6400
[11/29 12:31:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 68.85	
[11/29 12:31:57 visual_prompt]: Best epoch 26: best metric: -0.640
[11/29 12:31:57 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[11/29 12:33:45 visual_prompt]: 	Training 100/553. train loss: 0.5589,	0.8227 s / batch. (data: 1.57e-02). ETA=9:19:44, max mem: 20.9 GB 
[11/29 12:35:28 visual_prompt]: 	Training 200/553. train loss: 0.9818,	1.5600 s / batch. (data: 7.52e-01). ETA=17:38:45, max mem: 20.9 GB 
[11/29 12:37:12 visual_prompt]: 	Training 300/553. train loss: 0.6183,	0.8280 s / batch. (data: 5.46e-03). ETA=9:20:34, max mem: 20.9 GB 
[11/29 12:38:57 visual_prompt]: 	Training 400/553. train loss: 0.5808,	0.8289 s / batch. (data: 1.10e-02). ETA=9:19:50, max mem: 20.9 GB 
[11/29 12:40:40 visual_prompt]: 	Training 500/553. train loss: 0.8439,	0.8333 s / batch. (data: 1.05e-02). ETA=9:21:24, max mem: 20.9 GB 
[11/29 12:41:32 visual_prompt]: Epoch 27 / 100: avg data time: 2.18e-01, avg batch time: 1.0405, average train loss: 0.8934
[11/29 12:42:32 visual_prompt]: Inference (val):avg data time: 1.64e-04, avg batch time: 0.3063, average loss: 1.1167
[11/29 12:42:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 68.13	
[11/29 12:42:32 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[11/29 12:44:18 visual_prompt]: 	Training 100/553. train loss: 0.5879,	1.0449 s / batch. (data: 2.35e-01). ETA=11:41:16, max mem: 20.9 GB 
[11/29 12:46:03 visual_prompt]: 	Training 200/553. train loss: 1.5727,	0.8465 s / batch. (data: 1.05e-02). ETA=9:26:42, max mem: 20.9 GB 
[11/29 12:47:48 visual_prompt]: 	Training 300/553. train loss: 1.3673,	1.6080 s / batch. (data: 7.93e-01). ETA=17:53:49, max mem: 20.9 GB 
[11/29 12:49:30 visual_prompt]: 	Training 400/553. train loss: 0.5791,	0.8470 s / batch. (data: 7.88e-04). ETA=9:24:15, max mem: 20.9 GB 

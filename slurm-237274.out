/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:14:08 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:14:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:14:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:14:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:14:08 visual_prompt]: Training with config:
[09/26 10:14:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:14:08 visual_prompt]: Loading training data...
2023-09-26 10:14:09.111448: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-26 10:14:09.159113: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-26 10:14:15.068884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[09/26 10:14:26 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 10:14:28 visual_prompt]: Number of images: 800
[09/26 10:14:28 visual_prompt]: Number of classes: 9 / 9
[09/26 10:14:28 visual_prompt]: Loading validation data...
[09/26 10:14:28 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 10:14:29 visual_prompt]: Number of images: 200
[09/26 10:14:29 visual_prompt]: Number of classes: 9 / 9
[09/26 10:14:29 visual_prompt]: Constructing models...
[09/26 10:14:31 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 10:14:31 visual_prompt]: tuned percent:0.542
[09/26 10:14:33 visual_prompt]: Device used for model: 0
[09/26 10:14:33 visual_prompt]: Setting up Evaluator...
[09/26 10:14:33 visual_prompt]: Setting up Trainer...
[09/26 10:14:33 visual_prompt]: 	Setting up the optimizer...
[09/26 10:14:33 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:14:44 visual_prompt]: Epoch 1 / 100: avg data time: 1.58e-01, avg batch time: 0.7856, average train loss: 2.8597
[09/26 10:14:45 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1653, average loss: 2.9516
[09/26 10:14:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 10:14:45 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 10:14:45 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 10:14:52 visual_prompt]: Epoch 2 / 100: avg data time: 4.09e-02, avg batch time: 0.4804, average train loss: 20.0977
[09/26 10:14:53 visual_prompt]: Inference (val):avg data time: 1.70e-05, avg batch time: 0.1658, average loss: 27.6087
[09/26 10:14:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.50	
[09/26 10:14:53 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 10:15:00 visual_prompt]: Epoch 3 / 100: avg data time: 5.47e-02, avg batch time: 0.4929, average train loss: 30.8898
[09/26 10:15:01 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1651, average loss: 43.2879
[09/26 10:15:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 10:15:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 10:15:08 visual_prompt]: Epoch 4 / 100: avg data time: 4.45e-02, avg batch time: 0.4860, average train loss: 59.8615
[09/26 10:15:09 visual_prompt]: Inference (val):avg data time: 1.82e-05, avg batch time: 0.1657, average loss: 74.6063
[09/26 10:15:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 10:15:09 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 10:15:16 visual_prompt]: Epoch 5 / 100: avg data time: 4.05e-02, avg batch time: 0.4819, average train loss: 62.1774
[09/26 10:15:17 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1662, average loss: 84.5712
[09/26 10:15:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 56.50	
[09/26 10:15:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 10:15:24 visual_prompt]: Epoch 6 / 100: avg data time: 4.08e-02, avg batch time: 0.4843, average train loss: 99.6166
[09/26 10:15:25 visual_prompt]: Inference (val):avg data time: 1.64e-05, avg batch time: 0.1669, average loss: 63.9287
[09/26 10:15:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/26 10:15:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 10:15:32 visual_prompt]: Epoch 7 / 100: avg data time: 5.84e-02, avg batch time: 0.4998, average train loss: 133.4274
[09/26 10:15:33 visual_prompt]: Inference (val):avg data time: 1.68e-05, avg batch time: 0.1667, average loss: 34.4616
[09/26 10:15:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.00	top5: 51.00	
[09/26 10:15:33 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 10:15:40 visual_prompt]: Epoch 8 / 100: avg data time: 5.22e-02, avg batch time: 0.4964, average train loss: 132.7395
[09/26 10:15:41 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1671, average loss: 112.9924
[09/26 10:15:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 10:15:41 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 10:15:48 visual_prompt]: Epoch 9 / 100: avg data time: 4.38e-02, avg batch time: 0.4896, average train loss: 143.7674
[09/26 10:15:49 visual_prompt]: Inference (val):avg data time: 1.56e-05, avg batch time: 0.1673, average loss: 220.1379
[09/26 10:15:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 48.50	
[09/26 10:15:49 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 10:15:56 visual_prompt]: Epoch 10 / 100: avg data time: 5.33e-02, avg batch time: 0.4963, average train loss: 145.9063
[09/26 10:15:57 visual_prompt]: Inference (val):avg data time: 1.57e-05, avg batch time: 0.1673, average loss: 135.2348
[09/26 10:15:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 10:15:57 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 10:16:04 visual_prompt]: Epoch 11 / 100: avg data time: 4.79e-02, avg batch time: 0.4908, average train loss: 198.8483
[09/26 10:16:05 visual_prompt]: Inference (val):avg data time: 1.91e-05, avg batch time: 0.1672, average loss: 216.3808
[09/26 10:16:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 10:16:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 10:16:12 visual_prompt]: Epoch 12 / 100: avg data time: 4.76e-02, avg batch time: 0.4901, average train loss: 192.7177
[09/26 10:16:13 visual_prompt]: Inference (val):avg data time: 1.75e-05, avg batch time: 0.1674, average loss: 203.6490
[09/26 10:16:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 10:16:13 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 10:16:20 visual_prompt]: Epoch 13 / 100: avg data time: 4.46e-02, avg batch time: 0.4873, average train loss: 200.5579
[09/26 10:16:22 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1675, average loss: 281.4148
[09/26 10:16:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 48.50	
[09/26 10:16:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 10:16:28 visual_prompt]: Epoch 14 / 100: avg data time: 5.01e-02, avg batch time: 0.4924, average train loss: 210.1504
[09/26 10:16:30 visual_prompt]: Inference (val):avg data time: 1.61e-05, avg batch time: 0.1675, average loss: 88.7306
[09/26 10:16:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 10:16:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 10:16:36 visual_prompt]: Epoch 15 / 100: avg data time: 5.04e-02, avg batch time: 0.4946, average train loss: 165.8264
[09/26 10:16:38 visual_prompt]: Inference (val):avg data time: 1.68e-05, avg batch time: 0.1674, average loss: 151.8963
[09/26 10:16:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 49.50	
[09/26 10:16:38 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 10:16:44 visual_prompt]: Epoch 16 / 100: avg data time: 5.20e-02, avg batch time: 0.4949, average train loss: 199.7395
[09/26 10:16:46 visual_prompt]: Inference (val):avg data time: 1.79e-05, avg batch time: 0.1669, average loss: 154.9363
[09/26 10:16:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.50	
[09/26 10:16:46 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 10:16:52 visual_prompt]: Epoch 17 / 100: avg data time: 3.94e-02, avg batch time: 0.4846, average train loss: 171.9182
[09/26 10:16:54 visual_prompt]: Inference (val):avg data time: 1.82e-05, avg batch time: 0.1675, average loss: 246.4661
[09/26 10:16:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 10:16:54 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 10:17:00 visual_prompt]: Epoch 18 / 100: avg data time: 4.77e-02, avg batch time: 0.4927, average train loss: 165.1443
[09/26 10:17:02 visual_prompt]: Inference (val):avg data time: 1.63e-05, avg batch time: 0.1673, average loss: 109.8343
[09/26 10:17:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 60.50	
[09/26 10:17:02 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 10:17:08 visual_prompt]: Epoch 19 / 100: avg data time: 4.41e-02, avg batch time: 0.4871, average train loss: 140.2779
[09/26 10:17:10 visual_prompt]: Inference (val):avg data time: 1.84e-05, avg batch time: 0.1671, average loss: 157.5882
[09/26 10:17:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 10:17:10 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 10:17:16 visual_prompt]: Epoch 20 / 100: avg data time: 4.03e-02, avg batch time: 0.4854, average train loss: 189.8801
[09/26 10:17:18 visual_prompt]: Inference (val):avg data time: 1.61e-05, avg batch time: 0.1673, average loss: 137.2967
[09/26 10:17:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.50	
[09/26 10:17:18 visual_prompt]: Best epoch 20: best metric: 0.145
[09/26 10:17:18 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 10:17:25 visual_prompt]: Epoch 21 / 100: avg data time: 5.34e-02, avg batch time: 0.4961, average train loss: 232.2456
[09/26 10:17:26 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1672, average loss: 149.7676
[09/26 10:17:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 10:17:26 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 10:17:33 visual_prompt]: Epoch 22 / 100: avg data time: 5.28e-02, avg batch time: 0.4951, average train loss: 184.4567
[09/26 10:17:34 visual_prompt]: Inference (val):avg data time: 1.56e-05, avg batch time: 0.1673, average loss: 199.0645
[09/26 10:17:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.00	top5: 54.50	
[09/26 10:17:34 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 10:17:41 visual_prompt]: Epoch 23 / 100: avg data time: 4.09e-02, avg batch time: 0.4855, average train loss: 167.2707
[09/26 10:17:42 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1671, average loss: 185.8130
[09/26 10:17:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 10:17:42 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 10:17:49 visual_prompt]: Epoch 24 / 100: avg data time: 4.01e-02, avg batch time: 0.4848, average train loss: 190.1922
[09/26 10:17:50 visual_prompt]: Inference (val):avg data time: 1.66e-05, avg batch time: 0.1675, average loss: 202.5120
[09/26 10:17:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 10:17:50 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 10:17:57 visual_prompt]: Epoch 25 / 100: avg data time: 4.29e-02, avg batch time: 0.4873, average train loss: 238.6185
[09/26 10:17:58 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.1673, average loss: 216.8349
[09/26 10:17:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 10:17:58 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 10:18:05 visual_prompt]: Epoch 26 / 100: avg data time: 4.95e-02, avg batch time: 0.4938, average train loss: 196.6078
[09/26 10:18:06 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1680, average loss: 188.6909
[09/26 10:18:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 10:18:06 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 10:18:13 visual_prompt]: Epoch 27 / 100: avg data time: 4.17e-02, avg batch time: 0.4844, average train loss: 210.1200
[09/26 10:18:14 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1674, average loss: 330.2200
[09/26 10:18:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 10:18:14 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 10:18:21 visual_prompt]: Epoch 28 / 100: avg data time: 4.26e-02, avg batch time: 0.4864, average train loss: 229.8964
[09/26 10:18:22 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1679, average loss: 157.0743
[09/26 10:18:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 10:18:22 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 10:18:29 visual_prompt]: Epoch 29 / 100: avg data time: 5.92e-02, avg batch time: 0.5032, average train loss: 212.1973
[09/26 10:18:31 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1673, average loss: 188.3836
[09/26 10:18:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 10:18:31 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 10:18:37 visual_prompt]: Epoch 30 / 100: avg data time: 4.51e-02, avg batch time: 0.4893, average train loss: 178.0961
[09/26 10:18:39 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1677, average loss: 147.3969
[09/26 10:18:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/26 10:18:39 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 10:18:45 visual_prompt]: Epoch 31 / 100: avg data time: 4.53e-02, avg batch time: 0.4898, average train loss: 175.3817
[09/26 10:18:47 visual_prompt]: Inference (val):avg data time: 1.67e-05, avg batch time: 0.1672, average loss: 127.8002
[09/26 10:18:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 53.00	
[09/26 10:18:47 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 10:18:53 visual_prompt]: Epoch 32 / 100: avg data time: 4.55e-02, avg batch time: 0.4889, average train loss: 160.6801
[09/26 10:18:55 visual_prompt]: Inference (val):avg data time: 1.62e-05, avg batch time: 0.1674, average loss: 137.7176
[09/26 10:18:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 10:18:55 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 10:19:01 visual_prompt]: Epoch 33 / 100: avg data time: 5.44e-02, avg batch time: 0.4977, average train loss: 245.4345
[09/26 10:19:03 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1674, average loss: 181.4912
[09/26 10:19:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 10:19:03 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 10:19:09 visual_prompt]: Epoch 34 / 100: avg data time: 3.62e-02, avg batch time: 0.4826, average train loss: 201.3501
[09/26 10:19:11 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1674, average loss: 62.1846
[09/26 10:19:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 10:19:11 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 10:19:17 visual_prompt]: Epoch 35 / 100: avg data time: 4.51e-02, avg batch time: 0.4874, average train loss: 181.6744
[09/26 10:19:19 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1671, average loss: 184.5535
[09/26 10:19:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 10:19:19 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 10:19:25 visual_prompt]: Epoch 36 / 100: avg data time: 4.41e-02, avg batch time: 0.4870, average train loss: 193.2972
[09/26 10:19:27 visual_prompt]: Inference (val):avg data time: 1.84e-05, avg batch time: 0.1673, average loss: 174.9900
[09/26 10:19:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 10:19:27 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 10:19:33 visual_prompt]: Epoch 37 / 100: avg data time: 4.33e-02, avg batch time: 0.4882, average train loss: 221.7329
[09/26 10:19:35 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1675, average loss: 229.6303
[09/26 10:19:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 10:19:35 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 10:19:41 visual_prompt]: Epoch 38 / 100: avg data time: 4.55e-02, avg batch time: 0.4879, average train loss: 171.4369
[09/26 10:19:43 visual_prompt]: Inference (val):avg data time: 1.69e-05, avg batch time: 0.1674, average loss: 120.8062
[09/26 10:19:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 10:19:43 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 10:19:49 visual_prompt]: Epoch 39 / 100: avg data time: 4.77e-02, avg batch time: 0.4920, average train loss: 180.5672
[09/26 10:19:51 visual_prompt]: Inference (val):avg data time: 1.54e-05, avg batch time: 0.1672, average loss: 276.1522
[09/26 10:19:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.00	
[09/26 10:19:51 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 10:19:57 visual_prompt]: Epoch 40 / 100: avg data time: 4.47e-02, avg batch time: 0.4875, average train loss: 189.6875
[09/26 10:19:59 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1674, average loss: 123.2502
[09/26 10:19:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/26 10:19:59 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 10:20:05 visual_prompt]: Epoch 41 / 100: avg data time: 4.10e-02, avg batch time: 0.4838, average train loss: 176.4447
[09/26 10:20:07 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1672, average loss: 163.1265
[09/26 10:20:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.50	
[09/26 10:20:07 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 10:20:14 visual_prompt]: Epoch 42 / 100: avg data time: 5.68e-02, avg batch time: 0.5007, average train loss: 153.5778
[09/26 10:20:15 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1674, average loss: 224.0563
[09/26 10:20:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 10:20:15 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 10:20:22 visual_prompt]: Epoch 43 / 100: avg data time: 5.73e-02, avg batch time: 0.5006, average train loss: 152.1945
[09/26 10:20:23 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1678, average loss: 238.8319
[09/26 10:20:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 10:20:23 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 10:20:30 visual_prompt]: Epoch 44 / 100: avg data time: 5.26e-02, avg batch time: 0.4959, average train loss: 163.6919
[09/26 10:20:31 visual_prompt]: Inference (val):avg data time: 1.88e-05, avg batch time: 0.1676, average loss: 104.7263
[09/26 10:20:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 10:20:31 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 10:20:38 visual_prompt]: Epoch 45 / 100: avg data time: 5.64e-02, avg batch time: 0.4995, average train loss: 149.1729
[09/26 10:20:40 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1675, average loss: 107.9982
[09/26 10:20:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.50	
[09/26 10:20:40 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 10:20:46 visual_prompt]: Epoch 46 / 100: avg data time: 4.37e-02, avg batch time: 0.4874, average train loss: 181.6478
[09/26 10:20:48 visual_prompt]: Inference (val):avg data time: 1.75e-05, avg batch time: 0.1676, average loss: 153.2091
[09/26 10:20:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 10:20:48 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 10:20:54 visual_prompt]: Epoch 47 / 100: avg data time: 4.37e-02, avg batch time: 0.4887, average train loss: 187.8141
[09/26 10:20:56 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1676, average loss: 187.8371
[09/26 10:20:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.00	
[09/26 10:20:56 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 10:21:02 visual_prompt]: Epoch 48 / 100: avg data time: 4.87e-02, avg batch time: 0.4933, average train loss: 146.6011
[09/26 10:21:04 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1673, average loss: 174.5580
[09/26 10:21:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 10:21:04 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 10:21:11 visual_prompt]: Epoch 49 / 100: avg data time: 5.49e-02, avg batch time: 0.4986, average train loss: 156.3855
[09/26 10:21:12 visual_prompt]: Inference (val):avg data time: 1.61e-05, avg batch time: 0.1676, average loss: 93.6828
[09/26 10:21:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/26 10:21:12 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 10:21:19 visual_prompt]: Epoch 50 / 100: avg data time: 5.83e-02, avg batch time: 0.5012, average train loss: 121.1331
[09/26 10:21:20 visual_prompt]: Inference (val):avg data time: 1.50e-05, avg batch time: 0.1675, average loss: 79.1498
[09/26 10:21:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 10:21:20 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 10:21:27 visual_prompt]: Epoch 51 / 100: avg data time: 5.55e-02, avg batch time: 0.4988, average train loss: 75.2141
[09/26 10:21:28 visual_prompt]: Inference (val):avg data time: 1.44e-05, avg batch time: 0.1671, average loss: 72.8718
[09/26 10:21:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 10:21:28 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 10:21:35 visual_prompt]: Epoch 52 / 100: avg data time: 4.40e-02, avg batch time: 0.4866, average train loss: 117.4762
[09/26 10:21:36 visual_prompt]: Inference (val):avg data time: 1.65e-05, avg batch time: 0.1672, average loss: 77.6632
[09/26 10:21:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.50	
[09/26 10:21:36 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 10:21:43 visual_prompt]: Epoch 53 / 100: avg data time: 5.49e-02, avg batch time: 0.4980, average train loss: 135.5788
[09/26 10:21:45 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1672, average loss: 130.9465
[09/26 10:21:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 10:21:45 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 10:21:51 visual_prompt]: Epoch 54 / 100: avg data time: 4.48e-02, avg batch time: 0.4893, average train loss: 131.9683
[09/26 10:21:53 visual_prompt]: Inference (val):avg data time: 1.69e-05, avg batch time: 0.1673, average loss: 103.7598
[09/26 10:21:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 10:21:53 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 10:21:59 visual_prompt]: Epoch 55 / 100: avg data time: 4.40e-02, avg batch time: 0.4868, average train loss: 104.8653
[09/26 10:22:00 visual_prompt]: Inference (val):avg data time: 1.59e-05, avg batch time: 0.1673, average loss: 101.8920
[09/26 10:22:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 10:22:00 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 10:22:07 visual_prompt]: Epoch 56 / 100: avg data time: 4.21e-02, avg batch time: 0.4868, average train loss: 87.3385
[09/26 10:22:08 visual_prompt]: Inference (val):avg data time: 1.67e-05, avg batch time: 0.1669, average loss: 121.7581
[09/26 10:22:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.00	
[09/26 10:22:08 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 10:22:15 visual_prompt]: Epoch 57 / 100: avg data time: 4.77e-02, avg batch time: 0.4921, average train loss: 81.4185
[09/26 10:22:17 visual_prompt]: Inference (val):avg data time: 1.78e-05, avg batch time: 0.1672, average loss: 89.1336
[09/26 10:22:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 10:22:17 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 10:22:23 visual_prompt]: Epoch 58 / 100: avg data time: 3.99e-02, avg batch time: 0.4856, average train loss: 98.6684
[09/26 10:22:24 visual_prompt]: Inference (val):avg data time: 1.82e-05, avg batch time: 0.1669, average loss: 93.1379
[09/26 10:22:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 10:22:24 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 10:22:31 visual_prompt]: Epoch 59 / 100: avg data time: 4.61e-02, avg batch time: 0.4898, average train loss: 102.7586
[09/26 10:22:33 visual_prompt]: Inference (val):avg data time: 1.62e-05, avg batch time: 0.1671, average loss: 107.4018
[09/26 10:22:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 10:22:33 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 10:22:39 visual_prompt]: Epoch 60 / 100: avg data time: 5.66e-02, avg batch time: 0.5008, average train loss: 98.6803
[09/26 10:22:41 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1669, average loss: 106.7497
[09/26 10:22:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.50	
[09/26 10:22:41 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 10:22:47 visual_prompt]: Epoch 61 / 100: avg data time: 4.26e-02, avg batch time: 0.4879, average train loss: 99.3021
[09/26 10:22:49 visual_prompt]: Inference (val):avg data time: 1.79e-05, avg batch time: 0.1673, average loss: 60.5397
[09/26 10:22:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 63.00	
[09/26 10:22:49 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 10:22:56 visual_prompt]: Epoch 62 / 100: avg data time: 5.89e-02, avg batch time: 0.5016, average train loss: 129.7270
[09/26 10:22:57 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1673, average loss: 37.3676
[09/26 10:22:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.50	
[09/26 10:22:57 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 10:23:04 visual_prompt]: Epoch 63 / 100: avg data time: 5.24e-02, avg batch time: 0.4957, average train loss: 92.4957
[09/26 10:23:05 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1672, average loss: 49.5933
[09/26 10:23:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 10:23:05 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 10:23:12 visual_prompt]: Epoch 64 / 100: avg data time: 5.51e-02, avg batch time: 0.4982, average train loss: 86.8299
[09/26 10:23:14 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1671, average loss: 89.3151
[09/26 10:23:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.50	top5: 54.00	
[09/26 10:23:14 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 10:23:20 visual_prompt]: Epoch 65 / 100: avg data time: 3.95e-02, avg batch time: 0.4844, average train loss: 77.2927
[09/26 10:23:21 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1675, average loss: 98.3737
[09/26 10:23:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 10:23:21 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 10:23:28 visual_prompt]: Epoch 66 / 100: avg data time: 5.80e-02, avg batch time: 0.5005, average train loss: 87.8807
[09/26 10:23:30 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1671, average loss: 58.0704
[09/26 10:23:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 52.50	
[09/26 10:23:30 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 10:23:36 visual_prompt]: Epoch 67 / 100: avg data time: 4.65e-02, avg batch time: 0.4892, average train loss: 61.9294
[09/26 10:23:38 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1672, average loss: 66.7449
[09/26 10:23:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 10:23:38 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 10:23:44 visual_prompt]: Epoch 68 / 100: avg data time: 5.22e-02, avg batch time: 0.4959, average train loss: 56.7537
[09/26 10:23:46 visual_prompt]: Inference (val):avg data time: 1.73e-05, avg batch time: 0.1671, average loss: 61.4158
[09/26 10:23:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 10:23:46 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 10:23:53 visual_prompt]: Epoch 69 / 100: avg data time: 5.29e-02, avg batch time: 0.4964, average train loss: 62.2247
[09/26 10:23:54 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1674, average loss: 84.8982
[09/26 10:23:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 10:23:54 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 10:24:01 visual_prompt]: Epoch 70 / 100: avg data time: 5.53e-02, avg batch time: 0.4984, average train loss: 61.0485
[09/26 10:24:02 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1674, average loss: 40.8836
[09/26 10:24:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 57.50	
[09/26 10:24:02 visual_prompt]: Best epoch 70: best metric: 0.155
[09/26 10:24:02 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 10:24:09 visual_prompt]: Epoch 71 / 100: avg data time: 5.43e-02, avg batch time: 0.4976, average train loss: 47.1575
[09/26 10:24:10 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1673, average loss: 110.9243
[09/26 10:24:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 10:24:10 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 10:24:17 visual_prompt]: Epoch 72 / 100: avg data time: 5.35e-02, avg batch time: 0.4959, average train loss: 53.3914
[09/26 10:24:18 visual_prompt]: Inference (val):avg data time: 1.42e-05, avg batch time: 0.1673, average loss: 100.7582
[09/26 10:24:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.50	
[09/26 10:24:18 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 10:24:25 visual_prompt]: Epoch 73 / 100: avg data time: 4.73e-02, avg batch time: 0.4929, average train loss: 77.3617
[09/26 10:24:27 visual_prompt]: Inference (val):avg data time: 1.73e-05, avg batch time: 0.1673, average loss: 46.9147
[09/26 10:24:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.50	
[09/26 10:24:27 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 10:24:33 visual_prompt]: Epoch 74 / 100: avg data time: 5.69e-02, avg batch time: 0.5006, average train loss: 48.3828
[09/26 10:24:35 visual_prompt]: Inference (val):avg data time: 1.50e-05, avg batch time: 0.1674, average loss: 36.0750
[09/26 10:24:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 10:24:35 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 10:24:42 visual_prompt]: Epoch 75 / 100: avg data time: 5.77e-02, avg batch time: 0.5006, average train loss: 50.4532
[09/26 10:24:43 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1672, average loss: 53.6348
[09/26 10:24:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 10:24:43 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 10:24:50 visual_prompt]: Epoch 76 / 100: avg data time: 4.58e-02, avg batch time: 0.4892, average train loss: 38.3296
[09/26 10:24:51 visual_prompt]: Inference (val):avg data time: 1.48e-05, avg batch time: 0.1675, average loss: 43.8345
[09/26 10:24:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 10:24:51 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 10:24:58 visual_prompt]: Epoch 77 / 100: avg data time: 6.10e-02, avg batch time: 0.5046, average train loss: 44.6221
[09/26 10:24:59 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1673, average loss: 41.9256
[09/26 10:24:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 10:24:59 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 10:25:06 visual_prompt]: Epoch 78 / 100: avg data time: 3.87e-02, avg batch time: 0.4817, average train loss: 29.7578
[09/26 10:25:07 visual_prompt]: Inference (val):avg data time: 1.30e-05, avg batch time: 0.1672, average loss: 18.3338
[09/26 10:25:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 10:25:07 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 10:25:14 visual_prompt]: Epoch 79 / 100: avg data time: 5.01e-02, avg batch time: 0.4934, average train loss: 46.9693
[09/26 10:25:15 visual_prompt]: Inference (val):avg data time: 1.84e-05, avg batch time: 0.1670, average loss: 50.9592
[09/26 10:25:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 10:25:15 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 10:25:22 visual_prompt]: Epoch 80 / 100: avg data time: 4.23e-02, avg batch time: 0.4848, average train loss: 30.6851
[09/26 10:25:23 visual_prompt]: Inference (val):avg data time: 1.63e-05, avg batch time: 0.1673, average loss: 24.0182
[09/26 10:25:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 10:25:23 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 10:25:30 visual_prompt]: Epoch 81 / 100: avg data time: 4.23e-02, avg batch time: 0.4865, average train loss: 30.9173
[09/26 10:25:31 visual_prompt]: Inference (val):avg data time: 1.70e-05, avg batch time: 0.1673, average loss: 23.2771
[09/26 10:25:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 53.50	
[09/26 10:25:31 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 10:25:38 visual_prompt]: Epoch 82 / 100: avg data time: 4.67e-02, avg batch time: 0.4909, average train loss: 21.9425
[09/26 10:25:39 visual_prompt]: Inference (val):avg data time: 1.74e-05, avg batch time: 0.1671, average loss: 18.1176
[09/26 10:25:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 61.50	
[09/26 10:25:39 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 10:25:46 visual_prompt]: Epoch 83 / 100: avg data time: 4.92e-02, avg batch time: 0.4930, average train loss: 16.2603
[09/26 10:25:47 visual_prompt]: Inference (val):avg data time: 1.72e-05, avg batch time: 0.1673, average loss: 8.0719
[09/26 10:25:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.00	
[09/26 10:25:47 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 10:25:54 visual_prompt]: Epoch 84 / 100: avg data time: 4.28e-02, avg batch time: 0.4889, average train loss: 17.0619
[09/26 10:25:56 visual_prompt]: Inference (val):avg data time: 1.78e-05, avg batch time: 0.1675, average loss: 32.0229
[09/26 10:25:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.50	
[09/26 10:25:56 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 10:26:02 visual_prompt]: Epoch 85 / 100: avg data time: 6.11e-02, avg batch time: 0.5047, average train loss: 21.8546
[09/26 10:26:04 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1673, average loss: 15.0668
[09/26 10:26:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.50	
[09/26 10:26:04 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 10:26:11 visual_prompt]: Epoch 86 / 100: avg data time: 4.68e-02, avg batch time: 0.4914, average train loss: 11.8509
[09/26 10:26:12 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1672, average loss: 13.9438
[09/26 10:26:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 56.00	
[09/26 10:26:12 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 10:26:19 visual_prompt]: Epoch 87 / 100: avg data time: 5.47e-02, avg batch time: 0.4996, average train loss: 12.7840
[09/26 10:26:20 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1674, average loss: 12.3981
[09/26 10:26:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 10:26:20 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 10:26:27 visual_prompt]: Epoch 88 / 100: avg data time: 5.18e-02, avg batch time: 0.4951, average train loss: 10.2327
[09/26 10:26:28 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1674, average loss: 6.5687
[09/26 10:26:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 10:26:28 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 10:26:35 visual_prompt]: Epoch 89 / 100: avg data time: 5.72e-02, avg batch time: 0.5013, average train loss: 7.3116
[09/26 10:26:36 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1674, average loss: 7.5956
[09/26 10:26:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.50	
[09/26 10:26:36 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 10:26:43 visual_prompt]: Epoch 90 / 100: avg data time: 5.50e-02, avg batch time: 0.4994, average train loss: 4.2414
[09/26 10:26:45 visual_prompt]: Inference (val):avg data time: 1.74e-05, avg batch time: 0.1671, average loss: 4.1811
[09/26 10:26:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 10:26:45 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 10:26:51 visual_prompt]: Epoch 91 / 100: avg data time: 5.64e-02, avg batch time: 0.4990, average train loss: 3.1390
[09/26 10:26:53 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1672, average loss: 2.3815
[09/26 10:26:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 56.00	
[09/26 10:26:53 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 10:27:00 visual_prompt]: Epoch 92 / 100: avg data time: 5.45e-02, avg batch time: 0.4976, average train loss: 2.5002
[09/26 10:27:01 visual_prompt]: Inference (val):avg data time: 1.65e-05, avg batch time: 0.1669, average loss: 2.4997
[09/26 10:27:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 10:27:01 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 10:27:08 visual_prompt]: Epoch 93 / 100: avg data time: 4.27e-02, avg batch time: 0.4877, average train loss: 2.3398
[09/26 10:27:09 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1674, average loss: 2.3191
[09/26 10:27:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 10:27:09 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 10:27:16 visual_prompt]: Epoch 94 / 100: avg data time: 4.60e-02, avg batch time: 0.4890, average train loss: 2.2926
[09/26 10:27:17 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.1671, average loss: 2.2563
[09/26 10:27:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 10:27:17 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 10:27:24 visual_prompt]: Epoch 95 / 100: avg data time: 4.72e-02, avg batch time: 0.4908, average train loss: 2.2373
[09/26 10:27:25 visual_prompt]: Inference (val):avg data time: 1.50e-05, avg batch time: 0.1671, average loss: 2.2570
[09/26 10:27:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 10:27:25 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 10:27:32 visual_prompt]: Epoch 96 / 100: avg data time: 4.04e-02, avg batch time: 0.4844, average train loss: 2.2263
[09/26 10:27:33 visual_prompt]: Inference (val):avg data time: 1.77e-05, avg batch time: 0.1673, average loss: 2.2013
[09/26 10:27:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 10:27:33 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 10:27:40 visual_prompt]: Epoch 97 / 100: avg data time: 5.43e-02, avg batch time: 0.4971, average train loss: 2.2191
[09/26 10:27:41 visual_prompt]: Inference (val):avg data time: 1.38e-05, avg batch time: 0.1674, average loss: 2.2222
[09/26 10:27:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 10:27:41 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 10:27:48 visual_prompt]: Epoch 98 / 100: avg data time: 5.64e-02, avg batch time: 0.4999, average train loss: 2.2111
[09/26 10:27:50 visual_prompt]: Inference (val):avg data time: 1.57e-05, avg batch time: 0.1670, average loss: 2.1921
[09/26 10:27:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 10:27:50 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 10:27:56 visual_prompt]: Epoch 99 / 100: avg data time: 5.26e-02, avg batch time: 0.4959, average train loss: 2.2022
[09/26 10:27:58 visual_prompt]: Inference (val):avg data time: 1.78e-05, avg batch time: 0.1673, average loss: 2.1970
[09/26 10:27:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 10:27:58 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 10:28:04 visual_prompt]: Epoch 100 / 100: avg data time: 4.12e-02, avg batch time: 0.4890, average train loss: 2.1920
[09/26 10:28:06 visual_prompt]: Inference (val):avg data time: 1.55e-05, avg batch time: 0.1674, average loss: 2.1982
[09/26 10:28:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 10:28:06 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:28:06 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:28:06 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:28:06 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:28:06 visual_prompt]: Training with config:
[09/26 10:28:06 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:28:06 visual_prompt]: Loading training data...
[09/26 10:28:06 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 10:28:07 visual_prompt]: Number of images: 800
[09/26 10:28:07 visual_prompt]: Number of classes: 9 / 9
[09/26 10:28:07 visual_prompt]: Loading validation data...
[09/26 10:28:07 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 10:28:08 visual_prompt]: Number of images: 200
[09/26 10:28:08 visual_prompt]: Number of classes: 9 / 9
[09/26 10:28:08 visual_prompt]: Constructing models...
[09/26 10:28:10 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 10:28:10 visual_prompt]: tuned percent:0.542
[09/26 10:28:10 visual_prompt]: Device used for model: 0
[09/26 10:28:10 visual_prompt]: Setting up Evaluator...
[09/26 10:28:10 visual_prompt]: Setting up Trainer...
[09/26 10:28:10 visual_prompt]: 	Setting up the optimizer...
[09/26 10:28:10 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:28:17 visual_prompt]: Epoch 1 / 100: avg data time: 5.73e-02, avg batch time: 0.5001, average train loss: 2.8789
[09/26 10:28:19 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1667, average loss: 2.9516
[09/26 10:28:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 10:28:19 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 10:28:19 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 10:28:26 visual_prompt]: Epoch 2 / 100: avg data time: 5.64e-02, avg batch time: 0.4995, average train loss: 51.0011
[09/26 10:28:27 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1665, average loss: 41.6714
[09/26 10:28:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 50.50	
[09/26 10:28:27 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 10:28:34 visual_prompt]: Epoch 3 / 100: avg data time: 5.63e-02, avg batch time: 0.4989, average train loss: 61.3812
[09/26 10:28:35 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1668, average loss: 70.0588
[09/26 10:28:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 10:28:35 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 10:28:42 visual_prompt]: Epoch 4 / 100: avg data time: 6.27e-02, avg batch time: 0.5050, average train loss: 52.6360
[09/26 10:28:44 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1669, average loss: 62.3226
[09/26 10:28:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 10:28:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 10:28:50 visual_prompt]: Epoch 5 / 100: avg data time: 5.38e-02, avg batch time: 0.4965, average train loss: 131.6177
[09/26 10:28:52 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1670, average loss: 161.0246
[09/26 10:28:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 10:28:52 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 10:28:59 visual_prompt]: Epoch 6 / 100: avg data time: 4.58e-02, avg batch time: 0.4895, average train loss: 168.2574
[09/26 10:29:00 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1673, average loss: 107.0784
[09/26 10:29:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 10:29:00 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 10:29:07 visual_prompt]: Epoch 7 / 100: avg data time: 4.34e-02, avg batch time: 0.4870, average train loss: 121.9685
[09/26 10:29:08 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1668, average loss: 200.9197
[09/26 10:29:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 10:29:08 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 10:29:15 visual_prompt]: Epoch 8 / 100: avg data time: 5.54e-02, avg batch time: 0.4981, average train loss: 115.0049
[09/26 10:29:17 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1675, average loss: 186.6229
[09/26 10:29:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 10:29:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 10:29:23 visual_prompt]: Epoch 9 / 100: avg data time: 4.86e-02, avg batch time: 0.4930, average train loss: 157.5765
[09/26 10:29:25 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1671, average loss: 134.5125
[09/26 10:29:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 50.50	
[09/26 10:29:25 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 10:29:32 visual_prompt]: Epoch 10 / 100: avg data time: 5.99e-02, avg batch time: 0.5038, average train loss: 150.1813
[09/26 10:29:33 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1672, average loss: 124.7422
[09/26 10:29:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 10:29:33 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 10:29:40 visual_prompt]: Epoch 11 / 100: avg data time: 5.71e-02, avg batch time: 0.4991, average train loss: 259.8851
[09/26 10:29:41 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1671, average loss: 313.1918
[09/26 10:29:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 10:29:41 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 10:29:48 visual_prompt]: Epoch 12 / 100: avg data time: 5.70e-02, avg batch time: 0.4995, average train loss: 251.7736
[09/26 10:29:50 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1670, average loss: 366.6653
[09/26 10:29:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 10:29:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 10:29:57 visual_prompt]: Epoch 13 / 100: avg data time: 5.38e-02, avg batch time: 0.4965, average train loss: 283.0403
[09/26 10:29:58 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1670, average loss: 343.7860
[09/26 10:29:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.50	
[09/26 10:29:58 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 10:30:05 visual_prompt]: Epoch 14 / 100: avg data time: 5.33e-02, avg batch time: 0.4967, average train loss: 327.4663
[09/26 10:30:06 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1672, average loss: 356.7180
[09/26 10:30:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 10:30:06 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 10:30:13 visual_prompt]: Epoch 15 / 100: avg data time: 4.31e-02, avg batch time: 0.4877, average train loss: 271.9665
[09/26 10:30:15 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1671, average loss: 186.6271
[09/26 10:30:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 10:30:15 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 10:30:21 visual_prompt]: Epoch 16 / 100: avg data time: 4.53e-02, avg batch time: 0.4880, average train loss: 194.7830
[09/26 10:30:23 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1676, average loss: 152.5181
[09/26 10:30:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 10:30:23 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 10:30:29 visual_prompt]: Epoch 17 / 100: avg data time: 6.18e-02, avg batch time: 0.5037, average train loss: 257.4769
[09/26 10:30:31 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1669, average loss: 222.7769
[09/26 10:30:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.50	
[09/26 10:30:31 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 10:30:38 visual_prompt]: Epoch 18 / 100: avg data time: 5.54e-02, avg batch time: 0.4983, average train loss: 215.3551
[09/26 10:30:39 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1674, average loss: 219.3768
[09/26 10:30:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 10:30:39 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 10:30:46 visual_prompt]: Epoch 19 / 100: avg data time: 4.92e-02, avg batch time: 0.4930, average train loss: 280.3189
[09/26 10:30:47 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1672, average loss: 583.3059
[09/26 10:30:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 10:30:47 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 10:30:54 visual_prompt]: Epoch 20 / 100: avg data time: 4.48e-02, avg batch time: 0.4892, average train loss: 246.0052
[09/26 10:30:56 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1673, average loss: 267.5357
[09/26 10:30:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.50	
[09/26 10:30:56 visual_prompt]: Best epoch 20: best metric: 0.145
[09/26 10:30:56 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 10:31:02 visual_prompt]: Epoch 21 / 100: avg data time: 5.57e-02, avg batch time: 0.4991, average train loss: 248.7311
[09/26 10:31:04 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1670, average loss: 343.8255
[09/26 10:31:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 10:31:04 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 10:31:11 visual_prompt]: Epoch 22 / 100: avg data time: 5.51e-02, avg batch time: 0.4981, average train loss: 337.9103
[09/26 10:31:12 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1672, average loss: 427.0341
[09/26 10:31:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 10:31:12 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 10:31:19 visual_prompt]: Epoch 23 / 100: avg data time: 5.53e-02, avg batch time: 0.4979, average train loss: 230.0278
[09/26 10:31:20 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1674, average loss: 181.7232
[09/26 10:31:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.50	
[09/26 10:31:20 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 10:31:27 visual_prompt]: Epoch 24 / 100: avg data time: 4.71e-02, avg batch time: 0.4898, average train loss: 281.1226
[09/26 10:31:29 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1670, average loss: 127.5974
[09/26 10:31:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 10:31:29 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 10:31:35 visual_prompt]: Epoch 25 / 100: avg data time: 5.22e-02, avg batch time: 0.4964, average train loss: 192.6077
[09/26 10:31:37 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1672, average loss: 101.7495
[09/26 10:31:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 10:31:37 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 10:31:43 visual_prompt]: Epoch 26 / 100: avg data time: 4.78e-02, avg batch time: 0.4910, average train loss: 229.9225
[09/26 10:31:45 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1677, average loss: 201.7021
[09/26 10:31:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 62.50	
[09/26 10:31:45 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 10:31:52 visual_prompt]: Epoch 27 / 100: avg data time: 5.61e-02, avg batch time: 0.5002, average train loss: 192.9128
[09/26 10:31:53 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1672, average loss: 147.9659
[09/26 10:31:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 10:31:53 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 10:32:00 visual_prompt]: Epoch 28 / 100: avg data time: 5.56e-02, avg batch time: 0.4982, average train loss: 249.9703
[09/26 10:32:02 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1681, average loss: 161.9550
[09/26 10:32:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 10:32:02 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 10:32:08 visual_prompt]: Epoch 29 / 100: avg data time: 4.63e-02, avg batch time: 0.4913, average train loss: 201.5072
[09/26 10:32:10 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1676, average loss: 162.0770
[09/26 10:32:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 10:32:10 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 10:32:17 visual_prompt]: Epoch 30 / 100: avg data time: 5.61e-02, avg batch time: 0.5003, average train loss: 184.5687
[09/26 10:32:18 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1672, average loss: 145.2944
[09/26 10:32:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 10:32:18 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 10:32:25 visual_prompt]: Epoch 31 / 100: avg data time: 4.59e-02, avg batch time: 0.4888, average train loss: 247.6677
[09/26 10:32:26 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1676, average loss: 492.9930
[09/26 10:32:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 10:32:26 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 10:32:33 visual_prompt]: Epoch 32 / 100: avg data time: 4.90e-02, avg batch time: 0.4943, average train loss: 237.5623
[09/26 10:32:34 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1674, average loss: 117.0197
[09/26 10:32:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 60.50	
[09/26 10:32:34 visual_prompt]: Best epoch 32: best metric: 0.150
[09/26 10:32:34 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 10:32:41 visual_prompt]: Epoch 33 / 100: avg data time: 4.99e-02, avg batch time: 0.4940, average train loss: 171.7646
[09/26 10:32:43 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1671, average loss: 100.3815
[09/26 10:32:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 10:32:43 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 10:32:49 visual_prompt]: Epoch 34 / 100: avg data time: 5.31e-02, avg batch time: 0.4977, average train loss: 204.2795
[09/26 10:32:51 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1677, average loss: 251.5884
[09/26 10:32:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 10:32:51 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 10:32:58 visual_prompt]: Epoch 35 / 100: avg data time: 6.16e-02, avg batch time: 0.5039, average train loss: 212.9528
[09/26 10:32:59 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1676, average loss: 121.2135
[09/26 10:32:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 10:32:59 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 10:33:06 visual_prompt]: Epoch 36 / 100: avg data time: 4.40e-02, avg batch time: 0.4898, average train loss: 198.4440
[09/26 10:33:08 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1673, average loss: 466.8022
[09/26 10:33:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 10:33:08 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 10:33:14 visual_prompt]: Epoch 37 / 100: avg data time: 4.88e-02, avg batch time: 0.4926, average train loss: 261.2295
[09/26 10:33:16 visual_prompt]: Inference (val):avg data time: 4.51e-05, avg batch time: 0.1672, average loss: 264.8460
[09/26 10:33:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 10:33:16 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 10:33:22 visual_prompt]: Epoch 38 / 100: avg data time: 4.58e-02, avg batch time: 0.4893, average train loss: 202.9417
[09/26 10:33:24 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1678, average loss: 160.9467
[09/26 10:33:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 10:33:24 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 10:33:31 visual_prompt]: Epoch 39 / 100: avg data time: 5.43e-02, avg batch time: 0.4983, average train loss: 178.3748
[09/26 10:33:32 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1671, average loss: 171.1920
[09/26 10:33:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 10:33:32 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 10:33:39 visual_prompt]: Epoch 40 / 100: avg data time: 5.36e-02, avg batch time: 0.4957, average train loss: 248.4893
[09/26 10:33:40 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1674, average loss: 282.7074
[09/26 10:33:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 10:33:40 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 10:33:47 visual_prompt]: Epoch 41 / 100: avg data time: 5.87e-02, avg batch time: 0.5020, average train loss: 206.1286
[09/26 10:33:49 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1669, average loss: 174.0101
[09/26 10:33:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 10:33:49 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 10:33:56 visual_prompt]: Epoch 42 / 100: avg data time: 5.55e-02, avg batch time: 0.4988, average train loss: 162.2849
[09/26 10:33:57 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1671, average loss: 113.6132
[09/26 10:33:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 10:33:57 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 10:34:04 visual_prompt]: Epoch 43 / 100: avg data time: 5.82e-02, avg batch time: 0.5007, average train loss: 170.4734
[09/26 10:34:05 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1673, average loss: 139.2175
[09/26 10:34:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 10:34:05 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 10:34:12 visual_prompt]: Epoch 44 / 100: avg data time: 5.62e-02, avg batch time: 0.5002, average train loss: 149.0802
[09/26 10:34:14 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1670, average loss: 254.4572
[09/26 10:34:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 51.00	
[09/26 10:34:14 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 10:34:20 visual_prompt]: Epoch 45 / 100: avg data time: 4.86e-02, avg batch time: 0.4917, average train loss: 197.8737
[09/26 10:34:22 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1673, average loss: 113.2263
[09/26 10:34:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 10:34:22 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 10:34:29 visual_prompt]: Epoch 46 / 100: avg data time: 5.90e-02, avg batch time: 0.5009, average train loss: 137.1012
[09/26 10:34:30 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1671, average loss: 168.4138
[09/26 10:34:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 10:34:30 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 10:34:37 visual_prompt]: Epoch 47 / 100: avg data time: 4.64e-02, avg batch time: 0.4891, average train loss: 189.0080
[09/26 10:34:38 visual_prompt]: Inference (val):avg data time: 3.91e-05, avg batch time: 0.1677, average loss: 83.7093
[09/26 10:34:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 58.50	
[09/26 10:34:38 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 10:34:45 visual_prompt]: Epoch 48 / 100: avg data time: 4.42e-02, avg batch time: 0.4896, average train loss: 164.5842
[09/26 10:34:47 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1674, average loss: 182.3456
[09/26 10:34:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 10:34:47 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 10:34:53 visual_prompt]: Epoch 49 / 100: avg data time: 5.20e-02, avg batch time: 0.4953, average train loss: 153.9110
[09/26 10:34:55 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1677, average loss: 185.3129
[09/26 10:34:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 10:34:55 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 10:35:02 visual_prompt]: Epoch 50 / 100: avg data time: 5.74e-02, avg batch time: 0.5011, average train loss: 204.3722
[09/26 10:35:03 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1672, average loss: 181.1859
[09/26 10:35:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 10:35:03 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 10:35:10 visual_prompt]: Epoch 51 / 100: avg data time: 6.18e-02, avg batch time: 0.5051, average train loss: 132.0190
[09/26 10:35:12 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1676, average loss: 200.8199
[09/26 10:35:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 10:35:12 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 10:35:18 visual_prompt]: Epoch 52 / 100: avg data time: 5.04e-02, avg batch time: 0.4941, average train loss: 165.8775
[09/26 10:35:20 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1674, average loss: 146.8091
[09/26 10:35:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 59.50	
[09/26 10:35:20 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 10:35:27 visual_prompt]: Epoch 53 / 100: avg data time: 5.72e-02, avg batch time: 0.5008, average train loss: 132.7164
[09/26 10:35:28 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1674, average loss: 138.8223
[09/26 10:35:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 10:35:28 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 10:35:35 visual_prompt]: Epoch 54 / 100: avg data time: 4.32e-02, avg batch time: 0.4882, average train loss: 112.6131
[09/26 10:35:36 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1676, average loss: 84.7319
[09/26 10:35:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 10:35:36 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 10:35:43 visual_prompt]: Epoch 55 / 100: avg data time: 5.67e-02, avg batch time: 0.5003, average train loss: 153.0793
[09/26 10:35:45 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1676, average loss: 120.0515
[09/26 10:35:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 10:35:45 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 10:35:51 visual_prompt]: Epoch 56 / 100: avg data time: 5.55e-02, avg batch time: 0.4989, average train loss: 165.9297
[09/26 10:35:53 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1674, average loss: 98.5321
[09/26 10:35:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.00	
[09/26 10:35:53 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 10:36:00 visual_prompt]: Epoch 57 / 100: avg data time: 5.85e-02, avg batch time: 0.5025, average train loss: 172.8911
[09/26 10:36:01 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1676, average loss: 86.1711
[09/26 10:36:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 10:36:01 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 10:36:08 visual_prompt]: Epoch 58 / 100: avg data time: 4.71e-02, avg batch time: 0.4918, average train loss: 81.5667
[09/26 10:36:10 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1679, average loss: 90.4700
[09/26 10:36:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 10:36:10 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 10:36:16 visual_prompt]: Epoch 59 / 100: avg data time: 4.73e-02, avg batch time: 0.4926, average train loss: 114.1749
[09/26 10:36:18 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1674, average loss: 122.3395
[09/26 10:36:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 10:36:18 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 10:36:25 visual_prompt]: Epoch 60 / 100: avg data time: 5.42e-02, avg batch time: 0.4989, average train loss: 113.4372
[09/26 10:36:26 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1677, average loss: 102.6352
[09/26 10:36:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 10:36:26 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 10:36:33 visual_prompt]: Epoch 61 / 100: avg data time: 5.65e-02, avg batch time: 0.4998, average train loss: 128.2872
[09/26 10:36:35 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1675, average loss: 122.6369
[09/26 10:36:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 48.50	
[09/26 10:36:35 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 10:36:41 visual_prompt]: Epoch 62 / 100: avg data time: 5.05e-02, avg batch time: 0.4947, average train loss: 91.1686
[09/26 10:36:43 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1673, average loss: 120.1304
[09/26 10:36:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/26 10:36:43 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 10:36:50 visual_prompt]: Epoch 63 / 100: avg data time: 5.50e-02, avg batch time: 0.4985, average train loss: 101.9239
[09/26 10:36:51 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1676, average loss: 92.0845
[09/26 10:36:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 10:36:51 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 10:36:58 visual_prompt]: Epoch 64 / 100: avg data time: 5.15e-02, avg batch time: 0.4967, average train loss: 90.7841
[09/26 10:36:59 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1672, average loss: 74.2874
[09/26 10:36:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 10:36:59 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 10:37:06 visual_prompt]: Epoch 65 / 100: avg data time: 4.98e-02, avg batch time: 0.4928, average train loss: 87.6686
[09/26 10:37:07 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1677, average loss: 80.1333
[09/26 10:37:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 10:37:07 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 10:37:14 visual_prompt]: Epoch 66 / 100: avg data time: 4.59e-02, avg batch time: 0.4914, average train loss: 74.3600
[09/26 10:37:16 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1676, average loss: 74.3104
[09/26 10:37:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.00	
[09/26 10:37:16 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 10:37:22 visual_prompt]: Epoch 67 / 100: avg data time: 4.41e-02, avg batch time: 0.4876, average train loss: 62.8398
[09/26 10:37:24 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1676, average loss: 102.6236
[09/26 10:37:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 10:37:24 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 10:37:31 visual_prompt]: Epoch 68 / 100: avg data time: 5.54e-02, avg batch time: 0.4992, average train loss: 73.2968
[09/26 10:37:32 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1670, average loss: 52.2561
[09/26 10:37:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 10:37:32 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 10:37:39 visual_prompt]: Epoch 69 / 100: avg data time: 4.30e-02, avg batch time: 0.4868, average train loss: 64.3156
[09/26 10:37:40 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1672, average loss: 49.5189
[09/26 10:37:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 10:37:40 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 10:37:47 visual_prompt]: Epoch 70 / 100: avg data time: 4.52e-02, avg batch time: 0.4883, average train loss: 42.1756
[09/26 10:37:48 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1674, average loss: 39.5008
[09/26 10:37:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 10:37:48 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 10:37:55 visual_prompt]: Epoch 71 / 100: avg data time: 4.22e-02, avg batch time: 0.4880, average train loss: 50.9114
[09/26 10:37:57 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1674, average loss: 55.5177
[09/26 10:37:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 54.00	
[09/26 10:37:57 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 10:38:03 visual_prompt]: Epoch 72 / 100: avg data time: 4.39e-02, avg batch time: 0.4886, average train loss: 58.1747
[09/26 10:38:05 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1672, average loss: 82.9704
[09/26 10:38:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.00	
[09/26 10:38:05 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 10:38:11 visual_prompt]: Epoch 73 / 100: avg data time: 5.51e-02, avg batch time: 0.4981, average train loss: 65.6726
[09/26 10:38:13 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1674, average loss: 37.6822
[09/26 10:38:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 49.50	
[09/26 10:38:13 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 10:38:20 visual_prompt]: Epoch 74 / 100: avg data time: 4.25e-02, avg batch time: 0.4867, average train loss: 52.1704
[09/26 10:38:21 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1676, average loss: 60.6198
[09/26 10:38:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/26 10:38:21 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 10:38:28 visual_prompt]: Epoch 75 / 100: avg data time: 4.46e-02, avg batch time: 0.4886, average train loss: 41.4500
[09/26 10:38:29 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1674, average loss: 56.4600
[09/26 10:38:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.50	
[09/26 10:38:29 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 10:38:36 visual_prompt]: Epoch 76 / 100: avg data time: 5.07e-02, avg batch time: 0.4956, average train loss: 45.8064
[09/26 10:38:37 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1674, average loss: 45.8103
[09/26 10:38:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 52.00	
[09/26 10:38:37 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 10:38:44 visual_prompt]: Epoch 77 / 100: avg data time: 5.31e-02, avg batch time: 0.4975, average train loss: 34.8160
[09/26 10:38:46 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1672, average loss: 43.6212
[09/26 10:38:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 10:38:46 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 10:38:52 visual_prompt]: Epoch 78 / 100: avg data time: 4.41e-02, avg batch time: 0.4874, average train loss: 34.3093
[09/26 10:38:54 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1670, average loss: 25.9280
[09/26 10:38:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 10:38:54 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 10:39:00 visual_prompt]: Epoch 79 / 100: avg data time: 4.47e-02, avg batch time: 0.4889, average train loss: 45.9163
[09/26 10:39:02 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1669, average loss: 51.8802
[09/26 10:39:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 10:39:02 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 10:39:09 visual_prompt]: Epoch 80 / 100: avg data time: 4.69e-02, avg batch time: 0.4909, average train loss: 33.1761
[09/26 10:39:10 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1669, average loss: 28.1422
[09/26 10:39:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/26 10:39:10 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 10:39:17 visual_prompt]: Epoch 81 / 100: avg data time: 4.54e-02, avg batch time: 0.4886, average train loss: 25.2877
[09/26 10:39:18 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1673, average loss: 24.2918
[09/26 10:39:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 10:39:18 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 10:39:25 visual_prompt]: Epoch 82 / 100: avg data time: 4.32e-02, avg batch time: 0.4866, average train loss: 18.8351
[09/26 10:39:26 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1668, average loss: 11.2120
[09/26 10:39:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 10:39:26 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 10:39:33 visual_prompt]: Epoch 83 / 100: avg data time: 5.25e-02, avg batch time: 0.4969, average train loss: 9.8734
[09/26 10:39:35 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1667, average loss: 9.5241
[09/26 10:39:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 10:39:35 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 10:39:41 visual_prompt]: Epoch 84 / 100: avg data time: 4.14e-02, avg batch time: 0.4849, average train loss: 8.6717
[09/26 10:39:43 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1671, average loss: 7.1824
[09/26 10:39:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 10:39:43 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 10:39:49 visual_prompt]: Epoch 85 / 100: avg data time: 4.52e-02, avg batch time: 0.4904, average train loss: 6.9717
[09/26 10:39:51 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1670, average loss: 10.1160
[09/26 10:39:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 10:39:51 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 10:39:58 visual_prompt]: Epoch 86 / 100: avg data time: 5.80e-02, avg batch time: 0.5017, average train loss: 8.6947
[09/26 10:39:59 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1675, average loss: 5.7559
[09/26 10:39:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 10:39:59 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 10:40:06 visual_prompt]: Epoch 87 / 100: avg data time: 5.36e-02, avg batch time: 0.4959, average train loss: 5.7955
[09/26 10:40:07 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1670, average loss: 7.8335
[09/26 10:40:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 10:40:07 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 10:40:14 visual_prompt]: Epoch 88 / 100: avg data time: 4.69e-02, avg batch time: 0.4893, average train loss: 5.4451
[09/26 10:40:15 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1675, average loss: 2.9770
[09/26 10:40:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 10:40:15 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 10:40:22 visual_prompt]: Epoch 89 / 100: avg data time: 5.07e-02, avg batch time: 0.4951, average train loss: 2.8809
[09/26 10:40:24 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1671, average loss: 2.3461
[09/26 10:40:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 10:40:24 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 10:40:30 visual_prompt]: Epoch 90 / 100: avg data time: 4.88e-02, avg batch time: 0.4921, average train loss: 2.4181
[09/26 10:40:32 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1671, average loss: 2.2317
[09/26 10:40:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 10:40:32 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 10:40:39 visual_prompt]: Epoch 91 / 100: avg data time: 5.68e-02, avg batch time: 0.4991, average train loss: 2.2940
[09/26 10:40:40 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1671, average loss: 2.4791
[09/26 10:40:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.50	
[09/26 10:40:40 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 10:40:47 visual_prompt]: Epoch 92 / 100: avg data time: 4.34e-02, avg batch time: 0.4867, average train loss: 2.4056
[09/26 10:40:48 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1669, average loss: 2.3295
[09/26 10:40:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 10:40:48 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 10:40:55 visual_prompt]: Epoch 93 / 100: avg data time: 5.08e-02, avg batch time: 0.4953, average train loss: 2.3045
[09/26 10:40:57 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1670, average loss: 2.3135
[09/26 10:40:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 10:40:57 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 10:41:04 visual_prompt]: Epoch 94 / 100: avg data time: 5.41e-02, avg batch time: 0.4992, average train loss: 2.2532
[09/26 10:41:05 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1672, average loss: 2.2343
[09/26 10:41:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 10:41:05 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 10:41:12 visual_prompt]: Epoch 95 / 100: avg data time: 5.57e-02, avg batch time: 0.4996, average train loss: 2.2285
[09/26 10:41:13 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.1674, average loss: 2.2185
[09/26 10:41:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 10:41:13 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 10:41:20 visual_prompt]: Epoch 96 / 100: avg data time: 5.55e-02, avg batch time: 0.4985, average train loss: 2.2343
[09/26 10:41:22 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1673, average loss: 2.2660
[09/26 10:41:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 10:41:22 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 10:41:28 visual_prompt]: Epoch 97 / 100: avg data time: 5.75e-02, avg batch time: 0.5007, average train loss: 2.2264
[09/26 10:41:30 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1674, average loss: 2.2041
[09/26 10:41:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 10:41:30 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 10:41:37 visual_prompt]: Epoch 98 / 100: avg data time: 4.70e-02, avg batch time: 0.4920, average train loss: 2.2091
[09/26 10:41:38 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1675, average loss: 2.1968
[09/26 10:41:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 56.00	
[09/26 10:41:38 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 10:41:45 visual_prompt]: Epoch 99 / 100: avg data time: 4.90e-02, avg batch time: 0.4940, average train loss: 2.1916
[09/26 10:41:46 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1675, average loss: 2.1929
[09/26 10:41:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 57.00	
[09/26 10:41:46 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 10:41:53 visual_prompt]: Epoch 100 / 100: avg data time: 6.51e-02, avg batch time: 0.5095, average train loss: 2.1891
[09/26 10:41:55 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1674, average loss: 2.1930
[09/26 10:41:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 58.00	
[09/26 10:41:55 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:41:55 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:41:55 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:41:55 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:41:55 visual_prompt]: Training with config:
[09/26 10:41:55 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:41:55 visual_prompt]: Loading training data...
[09/26 10:41:55 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 10:41:56 visual_prompt]: Number of images: 800
[09/26 10:41:56 visual_prompt]: Number of classes: 9 / 9
[09/26 10:41:56 visual_prompt]: Loading validation data...
[09/26 10:41:56 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 10:41:56 visual_prompt]: Number of images: 200
[09/26 10:41:56 visual_prompt]: Number of classes: 9 / 9
[09/26 10:41:56 visual_prompt]: Constructing models...
[09/26 10:41:59 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 10:41:59 visual_prompt]: tuned percent:0.542
[09/26 10:41:59 visual_prompt]: Device used for model: 0
[09/26 10:41:59 visual_prompt]: Setting up Evaluator...
[09/26 10:41:59 visual_prompt]: Setting up Trainer...
[09/26 10:41:59 visual_prompt]: 	Setting up the optimizer...
[09/26 10:41:59 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:42:06 visual_prompt]: Epoch 1 / 100: avg data time: 5.26e-02, avg batch time: 0.4953, average train loss: 2.8782
[09/26 10:42:07 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1670, average loss: 2.9516
[09/26 10:42:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 10:42:07 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 10:42:07 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 10:42:14 visual_prompt]: Epoch 2 / 100: avg data time: 4.48e-02, avg batch time: 0.4880, average train loss: 34.4191
[09/26 10:42:15 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1665, average loss: 45.6354
[09/26 10:42:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.50	top5: 54.50	
[09/26 10:42:15 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 10:42:22 visual_prompt]: Epoch 3 / 100: avg data time: 4.37e-02, avg batch time: 0.4871, average train loss: 54.9126
[09/26 10:42:23 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1668, average loss: 44.2402
[09/26 10:42:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 10:42:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 10:42:30 visual_prompt]: Epoch 4 / 100: avg data time: 4.65e-02, avg batch time: 0.4905, average train loss: 43.8862
[09/26 10:42:32 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1669, average loss: 63.0562
[09/26 10:42:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 10:42:32 visual_prompt]: Best epoch 4: best metric: 0.145
[09/26 10:42:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 10:42:38 visual_prompt]: Epoch 5 / 100: avg data time: 4.54e-02, avg batch time: 0.4894, average train loss: 80.6065
[09/26 10:42:40 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1670, average loss: 34.2980
[09/26 10:42:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.00	
[09/26 10:42:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 10:42:46 visual_prompt]: Epoch 6 / 100: avg data time: 4.26e-02, avg batch time: 0.4859, average train loss: 99.3182
[09/26 10:42:48 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1670, average loss: 106.7360
[09/26 10:42:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 57.00	
[09/26 10:42:48 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 10:42:55 visual_prompt]: Epoch 7 / 100: avg data time: 5.43e-02, avg batch time: 0.4981, average train loss: 113.4415
[09/26 10:42:56 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1669, average loss: 167.6748
[09/26 10:42:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 48.50	
[09/26 10:42:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 10:43:03 visual_prompt]: Epoch 8 / 100: avg data time: 5.13e-02, avg batch time: 0.4958, average train loss: 144.4895
[09/26 10:43:04 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1670, average loss: 95.3284
[09/26 10:43:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 62.50	
[09/26 10:43:04 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 10:43:11 visual_prompt]: Epoch 9 / 100: avg data time: 4.60e-02, avg batch time: 0.4899, average train loss: 148.2049
[09/26 10:43:12 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1671, average loss: 133.0135
[09/26 10:43:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 10:43:12 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 10:43:19 visual_prompt]: Epoch 10 / 100: avg data time: 5.41e-02, avg batch time: 0.4961, average train loss: 179.4400
[09/26 10:43:21 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1670, average loss: 216.1791
[09/26 10:43:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 10:43:21 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 10:43:27 visual_prompt]: Epoch 11 / 100: avg data time: 5.95e-02, avg batch time: 0.5021, average train loss: 201.4489
[09/26 10:43:29 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1672, average loss: 194.2721
[09/26 10:43:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 10:43:29 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 10:43:36 visual_prompt]: Epoch 12 / 100: avg data time: 5.07e-02, avg batch time: 0.4967, average train loss: 206.5544
[09/26 10:43:37 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1674, average loss: 430.9401
[09/26 10:43:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 10:43:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 10:43:44 visual_prompt]: Epoch 13 / 100: avg data time: 5.68e-02, avg batch time: 0.5001, average train loss: 315.3195
[09/26 10:43:45 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1675, average loss: 144.9753
[09/26 10:43:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.00	
[09/26 10:43:46 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 10:43:52 visual_prompt]: Epoch 14 / 100: avg data time: 4.54e-02, avg batch time: 0.4917, average train loss: 190.9831
[09/26 10:43:54 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1674, average loss: 173.0223
[09/26 10:43:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 10:43:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 10:44:00 visual_prompt]: Epoch 15 / 100: avg data time: 5.43e-02, avg batch time: 0.4978, average train loss: 219.2899
[09/26 10:44:02 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1674, average loss: 210.9291
[09/26 10:44:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 10:44:02 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 10:44:09 visual_prompt]: Epoch 16 / 100: avg data time: 5.11e-02, avg batch time: 0.4960, average train loss: 218.2256
[09/26 10:44:10 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1673, average loss: 314.2054
[09/26 10:44:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 10:44:10 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 10:44:17 visual_prompt]: Epoch 17 / 100: avg data time: 5.89e-02, avg batch time: 0.5026, average train loss: 226.1618
[09/26 10:44:19 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1671, average loss: 202.5399
[09/26 10:44:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 10:44:19 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 10:44:26 visual_prompt]: Epoch 18 / 100: avg data time: 6.30e-02, avg batch time: 0.5053, average train loss: 286.8126
[09/26 10:44:27 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1671, average loss: 424.8318
[09/26 10:44:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 10:44:27 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 10:44:34 visual_prompt]: Epoch 19 / 100: avg data time: 4.34e-02, avg batch time: 0.4867, average train loss: 399.7465
[09/26 10:44:35 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1671, average loss: 427.2260
[09/26 10:44:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.00	
[09/26 10:44:35 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 10:44:42 visual_prompt]: Epoch 20 / 100: avg data time: 4.27e-02, avg batch time: 0.4867, average train loss: 257.6055
[09/26 10:44:43 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1674, average loss: 242.1222
[09/26 10:44:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 10:44:43 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 10:44:50 visual_prompt]: Epoch 21 / 100: avg data time: 5.78e-02, avg batch time: 0.5015, average train loss: 202.1287
[09/26 10:44:52 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1674, average loss: 184.8929
[09/26 10:44:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 10:44:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 10:44:59 visual_prompt]: Epoch 22 / 100: avg data time: 5.78e-02, avg batch time: 0.5012, average train loss: 272.8701
[09/26 10:45:00 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1676, average loss: 350.3646
[09/26 10:45:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 53.00	
[09/26 10:45:00 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 10:45:07 visual_prompt]: Epoch 23 / 100: avg data time: 4.87e-02, avg batch time: 0.4912, average train loss: 299.3617
[09/26 10:45:08 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1675, average loss: 444.6812
[09/26 10:45:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.00	
[09/26 10:45:08 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 10:45:15 visual_prompt]: Epoch 24 / 100: avg data time: 4.57e-02, avg batch time: 0.4894, average train loss: 285.1879
[09/26 10:45:16 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1672, average loss: 607.0134
[09/26 10:45:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 58.50	
[09/26 10:45:16 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 10:45:23 visual_prompt]: Epoch 25 / 100: avg data time: 5.67e-02, avg batch time: 0.4996, average train loss: 300.7733
[09/26 10:45:25 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1670, average loss: 248.6489
[09/26 10:45:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 10:45:25 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 10:45:32 visual_prompt]: Epoch 26 / 100: avg data time: 5.36e-02, avg batch time: 0.4964, average train loss: 243.4096
[09/26 10:45:33 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1676, average loss: 160.6441
[09/26 10:45:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.50	
[09/26 10:45:33 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 10:45:40 visual_prompt]: Epoch 27 / 100: avg data time: 4.76e-02, avg batch time: 0.4924, average train loss: 187.4713
[09/26 10:45:41 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1672, average loss: 183.7432
[09/26 10:45:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 10:45:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 10:45:48 visual_prompt]: Epoch 28 / 100: avg data time: 5.30e-02, avg batch time: 0.4971, average train loss: 236.4464
[09/26 10:45:50 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1670, average loss: 305.0564
[09/26 10:45:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 10:45:50 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 10:45:56 visual_prompt]: Epoch 29 / 100: avg data time: 4.50e-02, avg batch time: 0.4896, average train loss: 211.2222
[09/26 10:45:58 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1674, average loss: 228.7807
[09/26 10:45:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.00	
[09/26 10:45:58 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 10:46:04 visual_prompt]: Epoch 30 / 100: avg data time: 5.02e-02, avg batch time: 0.4940, average train loss: 195.2994
[09/26 10:46:06 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1675, average loss: 259.6855
[09/26 10:46:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 10:46:06 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 10:46:13 visual_prompt]: Epoch 31 / 100: avg data time: 5.76e-02, avg batch time: 0.4999, average train loss: 293.2309
[09/26 10:46:14 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1677, average loss: 161.2370
[09/26 10:46:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 10:46:14 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 10:46:21 visual_prompt]: Epoch 32 / 100: avg data time: 4.60e-02, avg batch time: 0.4896, average train loss: 249.5117
[09/26 10:46:22 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1671, average loss: 271.4263
[09/26 10:46:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 10:46:22 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 10:46:29 visual_prompt]: Epoch 33 / 100: avg data time: 6.06e-02, avg batch time: 0.5038, average train loss: 230.8926
[09/26 10:46:31 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1672, average loss: 212.8797
[09/26 10:46:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 10:46:31 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 10:46:37 visual_prompt]: Epoch 34 / 100: avg data time: 4.59e-02, avg batch time: 0.4897, average train loss: 225.0161
[09/26 10:46:39 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1675, average loss: 257.6884
[09/26 10:46:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 54.00	
[09/26 10:46:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 10:46:46 visual_prompt]: Epoch 35 / 100: avg data time: 5.36e-02, avg batch time: 0.4969, average train loss: 241.3377
[09/26 10:46:47 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1670, average loss: 233.4681
[09/26 10:46:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 10:46:47 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 10:46:54 visual_prompt]: Epoch 36 / 100: avg data time: 5.55e-02, avg batch time: 0.4975, average train loss: 274.8919
[09/26 10:46:55 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1670, average loss: 184.5847
[09/26 10:46:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.50	
[09/26 10:46:55 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 10:47:02 visual_prompt]: Epoch 37 / 100: avg data time: 5.41e-02, avg batch time: 0.4968, average train loss: 214.4928
[09/26 10:47:04 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1671, average loss: 225.4764
[09/26 10:47:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.50	
[09/26 10:47:04 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 10:47:10 visual_prompt]: Epoch 38 / 100: avg data time: 4.51e-02, avg batch time: 0.4895, average train loss: 193.5239
[09/26 10:47:12 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1674, average loss: 153.1155
[09/26 10:47:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 10:47:12 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 10:47:18 visual_prompt]: Epoch 39 / 100: avg data time: 4.20e-02, avg batch time: 0.4894, average train loss: 145.2657
[09/26 10:47:20 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1669, average loss: 168.2043
[09/26 10:47:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 10:47:20 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 10:47:27 visual_prompt]: Epoch 40 / 100: avg data time: 4.87e-02, avg batch time: 0.4920, average train loss: 163.4083
[09/26 10:47:28 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1675, average loss: 126.7388
[09/26 10:47:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 10:47:28 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 10:47:35 visual_prompt]: Epoch 41 / 100: avg data time: 4.95e-02, avg batch time: 0.4932, average train loss: 205.5985
[09/26 10:47:36 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1674, average loss: 195.3192
[09/26 10:47:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.00	
[09/26 10:47:36 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 10:47:43 visual_prompt]: Epoch 42 / 100: avg data time: 4.45e-02, avg batch time: 0.4880, average train loss: 136.2141
[09/26 10:47:44 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1672, average loss: 122.3166
[09/26 10:47:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 10:47:44 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 10:47:51 visual_prompt]: Epoch 43 / 100: avg data time: 5.09e-02, avg batch time: 0.4951, average train loss: 154.1773
[09/26 10:47:53 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1672, average loss: 170.3493
[09/26 10:47:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 10:47:53 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 10:47:59 visual_prompt]: Epoch 44 / 100: avg data time: 5.52e-02, avg batch time: 0.4978, average train loss: 119.8836
[09/26 10:48:01 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1669, average loss: 88.2542
[09/26 10:48:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 10:48:01 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 10:48:08 visual_prompt]: Epoch 45 / 100: avg data time: 4.41e-02, avg batch time: 0.4886, average train loss: 108.0396
[09/26 10:48:09 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1671, average loss: 124.0046
[09/26 10:48:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 56.50	
[09/26 10:48:09 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 10:48:16 visual_prompt]: Epoch 46 / 100: avg data time: 4.57e-02, avg batch time: 0.4896, average train loss: 129.7196
[09/26 10:48:17 visual_prompt]: Inference (val):avg data time: 4.40e-05, avg batch time: 0.1673, average loss: 104.2664
[09/26 10:48:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/26 10:48:17 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 10:48:24 visual_prompt]: Epoch 47 / 100: avg data time: 5.71e-02, avg batch time: 0.5002, average train loss: 133.5285
[09/26 10:48:26 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1672, average loss: 128.6314
[09/26 10:48:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 10:48:26 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 10:48:32 visual_prompt]: Epoch 48 / 100: avg data time: 4.72e-02, avg batch time: 0.4913, average train loss: 106.1706
[09/26 10:48:34 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1668, average loss: 67.9456
[09/26 10:48:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 10:48:34 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 10:48:40 visual_prompt]: Epoch 49 / 100: avg data time: 5.02e-02, avg batch time: 0.4932, average train loss: 116.7132
[09/26 10:48:42 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1674, average loss: 158.3649
[09/26 10:48:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 10:48:42 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 10:48:49 visual_prompt]: Epoch 50 / 100: avg data time: 4.81e-02, avg batch time: 0.4918, average train loss: 141.6662
[09/26 10:48:50 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1669, average loss: 94.4518
[09/26 10:48:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 51.50	
[09/26 10:48:50 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 10:48:57 visual_prompt]: Epoch 51 / 100: avg data time: 5.95e-02, avg batch time: 0.5019, average train loss: 116.7747
[09/26 10:48:59 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1670, average loss: 75.3643
[09/26 10:48:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 10:48:59 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 10:49:05 visual_prompt]: Epoch 52 / 100: avg data time: 4.97e-02, avg batch time: 0.4928, average train loss: 98.0065
[09/26 10:49:07 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1672, average loss: 41.1017
[09/26 10:49:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 10:49:07 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 10:49:13 visual_prompt]: Epoch 53 / 100: avg data time: 5.25e-02, avg batch time: 0.4969, average train loss: 90.0224
[09/26 10:49:15 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1671, average loss: 84.4976
[09/26 10:49:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 10:49:15 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 10:49:22 visual_prompt]: Epoch 54 / 100: avg data time: 5.40e-02, avg batch time: 0.4968, average train loss: 97.5529
[09/26 10:49:23 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1669, average loss: 100.8718
[09/26 10:49:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.00	
[09/26 10:49:23 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 10:49:30 visual_prompt]: Epoch 55 / 100: avg data time: 4.83e-02, avg batch time: 0.4918, average train loss: 140.5087
[09/26 10:49:31 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1672, average loss: 108.6182
[09/26 10:49:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 50.00	
[09/26 10:49:31 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 10:49:38 visual_prompt]: Epoch 56 / 100: avg data time: 4.77e-02, avg batch time: 0.4908, average train loss: 101.2561
[09/26 10:49:39 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1672, average loss: 95.4442
[09/26 10:49:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 10:49:39 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 10:49:46 visual_prompt]: Epoch 57 / 100: avg data time: 4.49e-02, avg batch time: 0.4882, average train loss: 94.4890
[09/26 10:49:48 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1672, average loss: 115.0311
[09/26 10:49:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 57.50	
[09/26 10:49:48 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 10:49:54 visual_prompt]: Epoch 58 / 100: avg data time: 5.37e-02, avg batch time: 0.4975, average train loss: 119.0076
[09/26 10:49:56 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1668, average loss: 130.3010
[09/26 10:49:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 10:49:56 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 10:50:03 visual_prompt]: Epoch 59 / 100: avg data time: 4.45e-02, avg batch time: 0.4897, average train loss: 112.0808
[09/26 10:50:04 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1673, average loss: 80.0325
[09/26 10:50:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 51.50	
[09/26 10:50:04 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 10:50:11 visual_prompt]: Epoch 60 / 100: avg data time: 4.72e-02, avg batch time: 0.4913, average train loss: 88.0534
[09/26 10:50:12 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1673, average loss: 92.4003
[09/26 10:50:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 10:50:12 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 10:50:19 visual_prompt]: Epoch 61 / 100: avg data time: 5.36e-02, avg batch time: 0.4971, average train loss: 80.7765
[09/26 10:50:20 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1674, average loss: 82.1740
[09/26 10:50:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.50	top5: 53.50	
[09/26 10:50:20 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 10:50:27 visual_prompt]: Epoch 62 / 100: avg data time: 5.61e-02, avg batch time: 0.4992, average train loss: 67.3999
[09/26 10:50:29 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1674, average loss: 42.7905
[09/26 10:50:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 10:50:29 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 10:50:35 visual_prompt]: Epoch 63 / 100: avg data time: 5.59e-02, avg batch time: 0.4981, average train loss: 72.1682
[09/26 10:50:37 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1672, average loss: 58.5031
[09/26 10:50:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 10:50:37 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 10:50:44 visual_prompt]: Epoch 64 / 100: avg data time: 5.66e-02, avg batch time: 0.4995, average train loss: 52.7597
[09/26 10:50:45 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1673, average loss: 57.4092
[09/26 10:50:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 10:50:45 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 10:50:52 visual_prompt]: Epoch 65 / 100: avg data time: 4.45e-02, avg batch time: 0.4896, average train loss: 54.6692
[09/26 10:50:53 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1672, average loss: 43.1187
[09/26 10:50:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.50	
[09/26 10:50:53 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 10:51:00 visual_prompt]: Epoch 66 / 100: avg data time: 4.37e-02, avg batch time: 0.4870, average train loss: 48.1644
[09/26 10:51:02 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1672, average loss: 50.2764
[09/26 10:51:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 10:51:02 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 10:51:08 visual_prompt]: Epoch 67 / 100: avg data time: 4.29e-02, avg batch time: 0.4877, average train loss: 51.5300
[09/26 10:51:10 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1671, average loss: 65.0492
[09/26 10:51:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 10:51:10 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 10:51:16 visual_prompt]: Epoch 68 / 100: avg data time: 4.65e-02, avg batch time: 0.4903, average train loss: 52.7785
[09/26 10:51:18 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1671, average loss: 25.3997
[09/26 10:51:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.50	
[09/26 10:51:18 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 10:51:25 visual_prompt]: Epoch 69 / 100: avg data time: 4.45e-02, avg batch time: 0.4894, average train loss: 38.1719
[09/26 10:51:26 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1675, average loss: 61.5822
[09/26 10:51:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.00	
[09/26 10:51:26 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 10:51:33 visual_prompt]: Epoch 70 / 100: avg data time: 4.59e-02, avg batch time: 0.4901, average train loss: 35.5633
[09/26 10:51:34 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.1672, average loss: 30.2989
[09/26 10:51:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.50	
[09/26 10:51:34 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 10:51:41 visual_prompt]: Epoch 71 / 100: avg data time: 5.26e-02, avg batch time: 0.4960, average train loss: 21.3065
[09/26 10:51:43 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1670, average loss: 19.7529
[09/26 10:51:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 10:51:43 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 10:51:49 visual_prompt]: Epoch 72 / 100: avg data time: 4.45e-02, avg batch time: 0.4901, average train loss: 29.3067
[09/26 10:51:51 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1674, average loss: 34.8052
[09/26 10:51:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/26 10:51:51 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 10:51:58 visual_prompt]: Epoch 73 / 100: avg data time: 5.68e-02, avg batch time: 0.4988, average train loss: 29.4936
[09/26 10:51:59 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1674, average loss: 21.6612
[09/26 10:51:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 54.50	
[09/26 10:51:59 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 10:52:06 visual_prompt]: Epoch 74 / 100: avg data time: 4.66e-02, avg batch time: 0.4912, average train loss: 22.6919
[09/26 10:52:07 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1669, average loss: 29.1090
[09/26 10:52:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 52.50	
[09/26 10:52:07 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 10:52:14 visual_prompt]: Epoch 75 / 100: avg data time: 6.09e-02, avg batch time: 0.5033, average train loss: 19.8224
[09/26 10:52:16 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1675, average loss: 27.7359
[09/26 10:52:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 10:52:16 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 10:52:22 visual_prompt]: Epoch 76 / 100: avg data time: 4.43e-02, avg batch time: 0.4893, average train loss: 24.2544
[09/26 10:52:24 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1673, average loss: 16.6942
[09/26 10:52:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.50	
[09/26 10:52:24 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 10:52:31 visual_prompt]: Epoch 77 / 100: avg data time: 5.71e-02, avg batch time: 0.5007, average train loss: 18.1587
[09/26 10:52:32 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1672, average loss: 19.2804
[09/26 10:52:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 10:52:32 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 10:52:39 visual_prompt]: Epoch 78 / 100: avg data time: 5.88e-02, avg batch time: 0.5014, average train loss: 16.3288
[09/26 10:52:40 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1673, average loss: 13.5448
[09/26 10:52:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 10:52:40 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 10:52:47 visual_prompt]: Epoch 79 / 100: avg data time: 4.35e-02, avg batch time: 0.4869, average train loss: 10.9402
[09/26 10:52:49 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1672, average loss: 10.3615
[09/26 10:52:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 10:52:49 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 10:52:55 visual_prompt]: Epoch 80 / 100: avg data time: 4.62e-02, avg batch time: 0.4916, average train loss: 9.6282
[09/26 10:52:57 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1673, average loss: 8.6242
[09/26 10:52:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 10:52:57 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 10:53:04 visual_prompt]: Epoch 81 / 100: avg data time: 6.12e-02, avg batch time: 0.5038, average train loss: 8.3774
[09/26 10:53:05 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1670, average loss: 6.8054
[09/26 10:53:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 10:53:05 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 10:53:12 visual_prompt]: Epoch 82 / 100: avg data time: 5.03e-02, avg batch time: 0.4937, average train loss: 5.1536
[09/26 10:53:13 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1673, average loss: 4.2323
[09/26 10:53:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 10:53:13 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 10:53:20 visual_prompt]: Epoch 83 / 100: avg data time: 5.74e-02, avg batch time: 0.5012, average train loss: 3.1881
[09/26 10:53:22 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1673, average loss: 2.7577
[09/26 10:53:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.00	
[09/26 10:53:22 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 10:53:28 visual_prompt]: Epoch 84 / 100: avg data time: 4.84e-02, avg batch time: 0.4923, average train loss: 2.7856
[09/26 10:53:30 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1675, average loss: 2.7938
[09/26 10:53:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 50.50	
[09/26 10:53:30 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 10:53:37 visual_prompt]: Epoch 85 / 100: avg data time: 5.17e-02, avg batch time: 0.4946, average train loss: 2.6024
[09/26 10:53:38 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1674, average loss: 2.5512
[09/26 10:53:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 10:53:38 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 10:53:45 visual_prompt]: Epoch 86 / 100: avg data time: 5.19e-02, avg batch time: 0.4948, average train loss: 2.3320
[09/26 10:53:46 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1673, average loss: 2.5366
[09/26 10:53:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 10:53:46 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 10:53:53 visual_prompt]: Epoch 87 / 100: avg data time: 5.71e-02, avg batch time: 0.5021, average train loss: 2.3551
[09/26 10:53:55 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1676, average loss: 2.3802
[09/26 10:53:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 10:53:55 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 10:54:01 visual_prompt]: Epoch 88 / 100: avg data time: 4.85e-02, avg batch time: 0.4929, average train loss: 2.2598
[09/26 10:54:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1672, average loss: 2.2309
[09/26 10:54:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 66.50	
[09/26 10:54:03 visual_prompt]: Best epoch 88: best metric: 0.170
[09/26 10:54:03 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 10:54:10 visual_prompt]: Epoch 89 / 100: avg data time: 6.02e-02, avg batch time: 0.5031, average train loss: 2.2435
[09/26 10:54:11 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1672, average loss: 2.1948
[09/26 10:54:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 66.00	
[09/26 10:54:11 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 10:54:18 visual_prompt]: Epoch 90 / 100: avg data time: 4.93e-02, avg batch time: 0.4926, average train loss: 2.2135
[09/26 10:54:19 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1673, average loss: 2.2853
[09/26 10:54:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 66.00	
[09/26 10:54:19 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 10:54:26 visual_prompt]: Epoch 91 / 100: avg data time: 4.60e-02, avg batch time: 0.4924, average train loss: 2.1962
[09/26 10:54:28 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1673, average loss: 2.1860
[09/26 10:54:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 62.00	
[09/26 10:54:28 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 10:54:34 visual_prompt]: Epoch 92 / 100: avg data time: 5.62e-02, avg batch time: 0.4992, average train loss: 2.1515
[09/26 10:54:36 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1673, average loss: 2.2056
[09/26 10:54:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 62.00	
[09/26 10:54:36 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 10:54:43 visual_prompt]: Epoch 93 / 100: avg data time: 5.61e-02, avg batch time: 0.4985, average train loss: 2.1370
[09/26 10:54:44 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1676, average loss: 2.1641
[09/26 10:54:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 68.00	
[09/26 10:54:44 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 10:54:51 visual_prompt]: Epoch 94 / 100: avg data time: 4.92e-02, avg batch time: 0.4943, average train loss: 2.1348
[09/26 10:54:52 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1675, average loss: 2.2538
[09/26 10:54:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 10:54:52 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 10:54:59 visual_prompt]: Epoch 95 / 100: avg data time: 4.97e-02, avg batch time: 0.4965, average train loss: 2.1365
[09/26 10:55:01 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1674, average loss: 2.1359
[09/26 10:55:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 66.50	
[09/26 10:55:01 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 10:55:07 visual_prompt]: Epoch 96 / 100: avg data time: 4.64e-02, avg batch time: 0.4925, average train loss: 2.0920
[09/26 10:55:09 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1673, average loss: 2.1162
[09/26 10:55:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 72.50	
[09/26 10:55:09 visual_prompt]: Best epoch 96: best metric: 0.190
[09/26 10:55:09 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 10:55:15 visual_prompt]: Epoch 97 / 100: avg data time: 4.59e-02, avg batch time: 0.4930, average train loss: 2.0553
[09/26 10:55:17 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1675, average loss: 2.1514
[09/26 10:55:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 67.50	
[09/26 10:55:17 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 10:55:24 visual_prompt]: Epoch 98 / 100: avg data time: 5.92e-02, avg batch time: 0.5034, average train loss: 2.0274
[09/26 10:55:25 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1675, average loss: 2.1580
[09/26 10:55:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 68.00	
[09/26 10:55:25 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 10:55:32 visual_prompt]: Epoch 99 / 100: avg data time: 5.15e-02, avg batch time: 0.4958, average train loss: 2.0256
[09/26 10:55:34 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1676, average loss: 2.0979
[09/26 10:55:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 69.00	
[09/26 10:55:34 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 10:55:40 visual_prompt]: Epoch 100 / 100: avg data time: 4.76e-02, avg batch time: 0.4913, average train loss: 2.0121
[09/26 10:55:42 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1673, average loss: 2.1151
[09/26 10:55:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 70.50	
[09/26 10:55:42 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 10:55:42 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 10:55:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 10:55:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 10:55:42 visual_prompt]: Training with config:
[09/26 10:55:42 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr50.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 10:55:42 visual_prompt]: Loading training data...
[09/26 10:55:42 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 10:55:43 visual_prompt]: Number of images: 800
[09/26 10:55:43 visual_prompt]: Number of classes: 9 / 9
[09/26 10:55:43 visual_prompt]: Loading validation data...
[09/26 10:55:43 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 10:55:43 visual_prompt]: Number of images: 200
[09/26 10:55:43 visual_prompt]: Number of classes: 9 / 9
[09/26 10:55:43 visual_prompt]: Constructing models...
[09/26 10:55:46 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 10:55:46 visual_prompt]: tuned percent:0.542
[09/26 10:55:46 visual_prompt]: Device used for model: 0
[09/26 10:55:46 visual_prompt]: Setting up Evaluator...
[09/26 10:55:46 visual_prompt]: Setting up Trainer...
[09/26 10:55:46 visual_prompt]: 	Setting up the optimizer...
[09/26 10:55:46 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 10:55:52 visual_prompt]: Epoch 1 / 100: avg data time: 4.68e-02, avg batch time: 0.4906, average train loss: 2.8723
[09/26 10:55:54 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1670, average loss: 2.9516
[09/26 10:55:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 10:55:54 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 10:55:54 visual_prompt]: Training 2 / 100 epoch, with learning rate 5.0
[09/26 10:56:01 visual_prompt]: Epoch 2 / 100: avg data time: 5.39e-02, avg batch time: 0.4971, average train loss: 32.3068
[09/26 10:56:02 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1671, average loss: 33.4546
[09/26 10:56:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 57.50	
[09/26 10:56:02 visual_prompt]: Best epoch 2: best metric: 0.145
[09/26 10:56:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 10.0
[09/26 10:56:09 visual_prompt]: Epoch 3 / 100: avg data time: 4.30e-02, avg batch time: 0.4887, average train loss: 48.4797
[09/26 10:56:10 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1675, average loss: 60.9323
[09/26 10:56:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 10:56:10 visual_prompt]: Training 4 / 100 epoch, with learning rate 15.0
[09/26 10:56:17 visual_prompt]: Epoch 4 / 100: avg data time: 5.13e-02, avg batch time: 0.4947, average train loss: 74.5570
[09/26 10:56:19 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1673, average loss: 58.0736
[09/26 10:56:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 59.50	
[09/26 10:56:19 visual_prompt]: Training 5 / 100 epoch, with learning rate 20.0
[09/26 10:56:25 visual_prompt]: Epoch 5 / 100: avg data time: 4.66e-02, avg batch time: 0.4929, average train loss: 117.6158
[09/26 10:56:27 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1674, average loss: 130.9755
[09/26 10:56:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 10:56:27 visual_prompt]: Training 6 / 100 epoch, with learning rate 25.0
[09/26 10:56:34 visual_prompt]: Epoch 6 / 100: avg data time: 4.91e-02, avg batch time: 0.4920, average train loss: 106.7712
[09/26 10:56:35 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1674, average loss: 54.7122
[09/26 10:56:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.50	
[09/26 10:56:35 visual_prompt]: Training 7 / 100 epoch, with learning rate 30.0
[09/26 10:56:42 visual_prompt]: Epoch 7 / 100: avg data time: 4.40e-02, avg batch time: 0.4890, average train loss: 97.5432
[09/26 10:56:43 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1674, average loss: 100.8184
[09/26 10:56:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.00	
[09/26 10:56:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 35.0
[09/26 10:56:50 visual_prompt]: Epoch 8 / 100: avg data time: 5.58e-02, avg batch time: 0.5001, average train loss: 120.0375
[09/26 10:56:52 visual_prompt]: Inference (val):avg data time: 3.72e-05, avg batch time: 0.1671, average loss: 185.8885
[09/26 10:56:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 10:56:52 visual_prompt]: Training 9 / 100 epoch, with learning rate 40.0
[09/26 10:56:58 visual_prompt]: Epoch 9 / 100: avg data time: 4.78e-02, avg batch time: 0.4927, average train loss: 155.6323
[09/26 10:57:00 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1669, average loss: 175.7390
[09/26 10:57:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/26 10:57:00 visual_prompt]: Training 10 / 100 epoch, with learning rate 45.0
[09/26 10:57:07 visual_prompt]: Epoch 10 / 100: avg data time: 4.44e-02, avg batch time: 0.4899, average train loss: 167.4960
[09/26 10:57:08 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1670, average loss: 122.9054
[09/26 10:57:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 53.00	
[09/26 10:57:08 visual_prompt]: Training 11 / 100 epoch, with learning rate 50.0
[09/26 10:57:15 visual_prompt]: Epoch 11 / 100: avg data time: 4.56e-02, avg batch time: 0.4919, average train loss: 155.1540
[09/26 10:57:16 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1674, average loss: 218.5854
[09/26 10:57:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 10:57:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 49.9847706754774
[09/26 10:57:23 visual_prompt]: Epoch 12 / 100: avg data time: 4.84e-02, avg batch time: 0.4925, average train loss: 156.4031
[09/26 10:57:24 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1673, average loss: 169.0392
[09/26 10:57:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 51.50	
[09/26 10:57:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 49.939101256495604
[09/26 10:57:31 visual_prompt]: Epoch 13 / 100: avg data time: 4.98e-02, avg batch time: 0.4940, average train loss: 135.8192
[09/26 10:57:33 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1673, average loss: 146.9901
[09/26 10:57:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 10:57:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 49.86304738420683
[09/26 10:57:39 visual_prompt]: Epoch 14 / 100: avg data time: 4.59e-02, avg batch time: 0.4906, average train loss: 123.3829
[09/26 10:57:41 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1671, average loss: 130.2543
[09/26 10:57:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 48.50	
[09/26 10:57:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 49.75670171853926
[09/26 10:57:48 visual_prompt]: Epoch 15 / 100: avg data time: 4.38e-02, avg batch time: 0.4873, average train loss: 109.8966
[09/26 10:57:49 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1670, average loss: 128.3483
[09/26 10:57:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 10:57:49 visual_prompt]: Training 16 / 100 epoch, with learning rate 49.6201938253052
[09/26 10:57:56 visual_prompt]: Epoch 16 / 100: avg data time: 4.62e-02, avg batch time: 0.4929, average train loss: 111.4399
[09/26 10:57:57 visual_prompt]: Inference (val):avg data time: 1.87e-05, avg batch time: 0.1672, average loss: 97.9880
[09/26 10:57:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 10:57:57 visual_prompt]: Training 17 / 100 epoch, with learning rate 49.45369001834514
[09/26 10:58:04 visual_prompt]: Epoch 17 / 100: avg data time: 5.80e-02, avg batch time: 0.5020, average train loss: 123.1712
[09/26 10:58:06 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1675, average loss: 65.7910
[09/26 10:58:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 10:58:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 49.25739315689991
[09/26 10:58:12 visual_prompt]: Epoch 18 / 100: avg data time: 4.49e-02, avg batch time: 0.4891, average train loss: 107.3338
[09/26 10:58:14 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1675, average loss: 137.6690
[09/26 10:58:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.50	
[09/26 10:58:14 visual_prompt]: Training 19 / 100 epoch, with learning rate 49.03154239845797
[09/26 10:58:21 visual_prompt]: Epoch 19 / 100: avg data time: 4.40e-02, avg batch time: 0.4881, average train loss: 130.0515
[09/26 10:58:22 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1675, average loss: 168.7931
[09/26 10:58:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 10:58:22 visual_prompt]: Training 20 / 100 epoch, with learning rate 48.77641290737884
[09/26 10:58:29 visual_prompt]: Epoch 20 / 100: avg data time: 5.56e-02, avg batch time: 0.5008, average train loss: 119.1740
[09/26 10:58:30 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1676, average loss: 61.8533
[09/26 10:58:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.00	top5: 53.00	
[09/26 10:58:30 visual_prompt]: Training 21 / 100 epoch, with learning rate 48.49231551964771
[09/26 10:58:37 visual_prompt]: Epoch 21 / 100: avg data time: 6.63e-02, avg batch time: 0.5088, average train loss: 89.0314
[09/26 10:58:39 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1673, average loss: 108.7107
[09/26 10:58:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/26 10:58:39 visual_prompt]: Training 22 / 100 epoch, with learning rate 48.17959636416968
[09/26 10:58:46 visual_prompt]: Epoch 22 / 100: avg data time: 4.48e-02, avg batch time: 0.4922, average train loss: 93.1501
[09/26 10:58:47 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1671, average loss: 150.7717
[09/26 10:58:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 10:58:47 visual_prompt]: Training 23 / 100 epoch, with learning rate 47.83863644106502
[09/26 10:58:54 visual_prompt]: Epoch 23 / 100: avg data time: 5.11e-02, avg batch time: 0.4936, average train loss: 87.8673
[09/26 10:58:55 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1676, average loss: 90.7211
[09/26 10:58:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 10:58:55 visual_prompt]: Training 24 / 100 epoch, with learning rate 47.46985115747918
[09/26 10:59:02 visual_prompt]: Epoch 24 / 100: avg data time: 4.34e-02, avg batch time: 0.4879, average train loss: 80.4937
[09/26 10:59:04 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1671, average loss: 88.6066
[09/26 10:59:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/26 10:59:04 visual_prompt]: Training 25 / 100 epoch, with learning rate 47.073689821473174
[09/26 10:59:10 visual_prompt]: Epoch 25 / 100: avg data time: 4.95e-02, avg batch time: 0.4950, average train loss: 65.5143
[09/26 10:59:12 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1675, average loss: 69.6998
[09/26 10:59:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 52.00	
[09/26 10:59:12 visual_prompt]: Training 26 / 100 epoch, with learning rate 46.65063509461097
[09/26 10:59:19 visual_prompt]: Epoch 26 / 100: avg data time: 5.94e-02, avg batch time: 0.5032, average train loss: 69.0575
[09/26 10:59:20 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1673, average loss: 85.2967
[09/26 10:59:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 51.50	
[09/26 10:59:20 visual_prompt]: Training 27 / 100 epoch, with learning rate 46.20120240391065
[09/26 10:59:27 visual_prompt]: Epoch 27 / 100: avg data time: 4.42e-02, avg batch time: 0.4888, average train loss: 71.6146
[09/26 10:59:28 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1674, average loss: 81.4923
[09/26 10:59:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 10:59:28 visual_prompt]: Training 28 / 100 epoch, with learning rate 45.72593931387604
[09/26 10:59:35 visual_prompt]: Epoch 28 / 100: avg data time: 4.44e-02, avg batch time: 0.4879, average train loss: 94.3187
[09/26 10:59:37 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1675, average loss: 75.3196
[09/26 10:59:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 61.50	
[09/26 10:59:37 visual_prompt]: Best epoch 28: best metric: 0.150
[09/26 10:59:37 visual_prompt]: Training 29 / 100 epoch, with learning rate 45.22542485937369
[09/26 10:59:43 visual_prompt]: Epoch 29 / 100: avg data time: 5.10e-02, avg batch time: 0.4957, average train loss: 67.5944
[09/26 10:59:45 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1674, average loss: 113.6823
[09/26 10:59:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 50.50	
[09/26 10:59:45 visual_prompt]: Training 30 / 100 epoch, with learning rate 44.70026884016804
[09/26 10:59:52 visual_prompt]: Epoch 30 / 100: avg data time: 4.85e-02, avg batch time: 0.4907, average train loss: 97.9681
[09/26 10:59:53 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1673, average loss: 108.4346
[09/26 10:59:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 10:59:53 visual_prompt]: Training 31 / 100 epoch, with learning rate 44.15111107797445
[09/26 11:00:00 visual_prompt]: Epoch 31 / 100: avg data time: 4.65e-02, avg batch time: 0.4895, average train loss: 90.8885
[09/26 11:00:01 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1675, average loss: 91.1370
[09/26 11:00:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 53.00	
[09/26 11:00:01 visual_prompt]: Training 32 / 100 epoch, with learning rate 43.57862063693486
[09/26 11:00:08 visual_prompt]: Epoch 32 / 100: avg data time: 4.51e-02, avg batch time: 0.4894, average train loss: 87.9668
[09/26 11:00:10 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1672, average loss: 55.3619
[09/26 11:00:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.00	
[09/26 11:00:10 visual_prompt]: Training 33 / 100 epoch, with learning rate 42.98349500846628
[09/26 11:00:16 visual_prompt]: Epoch 33 / 100: avg data time: 4.46e-02, avg batch time: 0.4885, average train loss: 66.6605
[09/26 11:00:18 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1672, average loss: 57.4811
[09/26 11:00:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 55.50	
[09/26 11:00:18 visual_prompt]: Training 34 / 100 epoch, with learning rate 42.36645926147493
[09/26 11:00:25 visual_prompt]: Epoch 34 / 100: avg data time: 5.82e-02, avg batch time: 0.5019, average train loss: 107.0697
[09/26 11:00:26 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1673, average loss: 103.8820
[09/26 11:00:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 53.50	
[09/26 11:00:26 visual_prompt]: Training 35 / 100 epoch, with learning rate 41.72826515897145
[09/26 11:00:33 visual_prompt]: Epoch 35 / 100: avg data time: 5.05e-02, avg batch time: 0.4956, average train loss: 111.3074
[09/26 11:00:34 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1673, average loss: 88.7077
[09/26 11:00:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:00:34 visual_prompt]: Training 36 / 100 epoch, with learning rate 41.06969024216348
[09/26 11:00:41 visual_prompt]: Epoch 36 / 100: avg data time: 6.21e-02, avg batch time: 0.5042, average train loss: 82.2087
[09/26 11:00:43 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1671, average loss: 89.7007
[09/26 11:00:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.50	
[09/26 11:00:43 visual_prompt]: Training 37 / 100 epoch, with learning rate 40.391536883141455
[09/26 11:00:50 visual_prompt]: Epoch 37 / 100: avg data time: 5.91e-02, avg batch time: 0.5045, average train loss: 87.7711
[09/26 11:00:51 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1674, average loss: 68.9138
[09/26 11:00:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 52.00	
[09/26 11:00:51 visual_prompt]: Training 38 / 100 epoch, with learning rate 39.69463130731183
[09/26 11:00:58 visual_prompt]: Epoch 38 / 100: avg data time: 5.98e-02, avg batch time: 0.5026, average train loss: 68.6832
[09/26 11:01:00 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1674, average loss: 62.7609
[09/26 11:01:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 60.50	
[09/26 11:01:00 visual_prompt]: Training 39 / 100 epoch, with learning rate 38.97982258676867
[09/26 11:01:06 visual_prompt]: Epoch 39 / 100: avg data time: 4.54e-02, avg batch time: 0.4884, average train loss: 58.4568
[09/26 11:01:08 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1669, average loss: 56.5737
[09/26 11:01:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 11:01:08 visual_prompt]: Training 40 / 100 epoch, with learning rate 38.24798160583012
[09/26 11:01:15 visual_prompt]: Epoch 40 / 100: avg data time: 5.69e-02, avg batch time: 0.4993, average train loss: 76.7946
[09/26 11:01:16 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1677, average loss: 93.1364
[09/26 11:01:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 11:01:16 visual_prompt]: Training 41 / 100 epoch, with learning rate 37.5
[09/26 11:01:23 visual_prompt]: Epoch 41 / 100: avg data time: 4.42e-02, avg batch time: 0.4875, average train loss: 74.3631
[09/26 11:01:24 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1675, average loss: 69.2257
[09/26 11:01:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 11:01:24 visual_prompt]: Training 42 / 100 epoch, with learning rate 36.736789069647266
[09/26 11:01:31 visual_prompt]: Epoch 42 / 100: avg data time: 6.01e-02, avg batch time: 0.5051, average train loss: 56.2396
[09/26 11:01:33 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1674, average loss: 66.5792
[09/26 11:01:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 11:01:33 visual_prompt]: Training 43 / 100 epoch, with learning rate 35.959278669726935
[09/26 11:01:40 visual_prompt]: Epoch 43 / 100: avg data time: 6.44e-02, avg batch time: 0.5088, average train loss: 66.6765
[09/26 11:01:41 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1671, average loss: 40.1523
[09/26 11:01:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 11:01:41 visual_prompt]: Training 44 / 100 epoch, with learning rate 35.16841607689501
[09/26 11:01:48 visual_prompt]: Epoch 44 / 100: avg data time: 6.33e-02, avg batch time: 0.5081, average train loss: 44.5206
[09/26 11:01:49 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1674, average loss: 28.4202
[09/26 11:01:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.50	
[09/26 11:01:49 visual_prompt]: Training 45 / 100 epoch, with learning rate 34.365164835397806
[09/26 11:01:56 visual_prompt]: Epoch 45 / 100: avg data time: 5.37e-02, avg batch time: 0.4970, average train loss: 33.3611
[09/26 11:01:58 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1672, average loss: 30.0155
[09/26 11:01:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 57.00	
[09/26 11:01:58 visual_prompt]: Training 46 / 100 epoch, with learning rate 33.55050358314172
[09/26 11:02:04 visual_prompt]: Epoch 46 / 100: avg data time: 4.78e-02, avg batch time: 0.4913, average train loss: 39.9444
[09/26 11:02:06 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1677, average loss: 31.0890
[09/26 11:02:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 55.00	
[09/26 11:02:06 visual_prompt]: Training 47 / 100 epoch, with learning rate 32.72542485937369
[09/26 11:02:13 visual_prompt]: Epoch 47 / 100: avg data time: 5.74e-02, avg batch time: 0.5009, average train loss: 50.7257
[09/26 11:02:14 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1675, average loss: 60.6674
[09/26 11:02:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 11:02:14 visual_prompt]: Training 48 / 100 epoch, with learning rate 31.89093389542498
[09/26 11:02:21 visual_prompt]: Epoch 48 / 100: avg data time: 6.78e-02, avg batch time: 0.5105, average train loss: 61.8930
[09/26 11:02:23 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1672, average loss: 34.1861
[09/26 11:02:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:02:23 visual_prompt]: Training 49 / 100 epoch, with learning rate 31.04804738999169
[09/26 11:02:30 visual_prompt]: Epoch 49 / 100: avg data time: 5.76e-02, avg batch time: 0.5020, average train loss: 46.6895
[09/26 11:02:31 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1674, average loss: 23.0784
[09/26 11:02:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 56.00	
[09/26 11:02:31 visual_prompt]: Best epoch 49: best metric: 0.165
[09/26 11:02:31 visual_prompt]: Training 50 / 100 epoch, with learning rate 30.19779227044398
[09/26 11:02:38 visual_prompt]: Epoch 50 / 100: avg data time: 5.13e-02, avg batch time: 0.4959, average train loss: 46.4678
[09/26 11:02:39 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1678, average loss: 58.7443
[09/26 11:02:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 62.50	
[09/26 11:02:39 visual_prompt]: Best epoch 50: best metric: 0.185
[09/26 11:02:39 visual_prompt]: Training 51 / 100 epoch, with learning rate 29.341204441673263
[09/26 11:02:46 visual_prompt]: Epoch 51 / 100: avg data time: 4.52e-02, avg batch time: 0.4880, average train loss: 54.8316
[09/26 11:02:47 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1676, average loss: 59.3838
[09/26 11:02:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 11:02:47 visual_prompt]: Training 52 / 100 epoch, with learning rate 28.479327524001636
[09/26 11:02:54 visual_prompt]: Epoch 52 / 100: avg data time: 4.97e-02, avg batch time: 0.4926, average train loss: 43.6854
[09/26 11:02:56 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1678, average loss: 60.8780
[09/26 11:02:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 11:02:56 visual_prompt]: Training 53 / 100 epoch, with learning rate 27.61321158169134
[09/26 11:03:02 visual_prompt]: Epoch 53 / 100: avg data time: 4.80e-02, avg batch time: 0.4924, average train loss: 48.6853
[09/26 11:03:04 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1673, average loss: 55.9288
[09/26 11:03:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.00	
[09/26 11:03:04 visual_prompt]: Training 54 / 100 epoch, with learning rate 26.74391184360313
[09/26 11:03:11 visual_prompt]: Epoch 54 / 100: avg data time: 5.54e-02, avg batch time: 0.4989, average train loss: 40.9717
[09/26 11:03:12 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1676, average loss: 38.2849
[09/26 11:03:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:03:12 visual_prompt]: Training 55 / 100 epoch, with learning rate 25.872487417562528
[09/26 11:03:19 visual_prompt]: Epoch 55 / 100: avg data time: 4.50e-02, avg batch time: 0.4904, average train loss: 28.9012
[09/26 11:03:20 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1677, average loss: 22.2339
[09/26 11:03:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 59.00	
[09/26 11:03:20 visual_prompt]: Training 56 / 100 epoch, with learning rate 25.0
[09/26 11:03:27 visual_prompt]: Epoch 56 / 100: avg data time: 5.23e-02, avg batch time: 0.4952, average train loss: 24.9867
[09/26 11:03:29 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1676, average loss: 25.8449
[09/26 11:03:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 60.00	
[09/26 11:03:29 visual_prompt]: Training 57 / 100 epoch, with learning rate 24.127512582437483
[09/26 11:03:35 visual_prompt]: Epoch 57 / 100: avg data time: 4.93e-02, avg batch time: 0.4931, average train loss: 19.5163
[09/26 11:03:37 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1676, average loss: 18.1042
[09/26 11:03:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 62.00	
[09/26 11:03:37 visual_prompt]: Training 58 / 100 epoch, with learning rate 23.256088156396867
[09/26 11:03:44 visual_prompt]: Epoch 58 / 100: avg data time: 5.15e-02, avg batch time: 0.4956, average train loss: 15.2404
[09/26 11:03:45 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1676, average loss: 13.0500
[09/26 11:03:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 68.00	
[09/26 11:03:45 visual_prompt]: Training 59 / 100 epoch, with learning rate 22.38678841830867
[09/26 11:03:52 visual_prompt]: Epoch 59 / 100: avg data time: 4.34e-02, avg batch time: 0.4875, average train loss: 16.9031
[09/26 11:03:54 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1678, average loss: 9.3004
[09/26 11:03:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 69.50	
[09/26 11:03:54 visual_prompt]: Training 60 / 100 epoch, with learning rate 21.52067247599837
[09/26 11:04:00 visual_prompt]: Epoch 60 / 100: avg data time: 5.29e-02, avg batch time: 0.4972, average train loss: 12.8281
[09/26 11:04:02 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1675, average loss: 17.0443
[09/26 11:04:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 11:04:02 visual_prompt]: Training 61 / 100 epoch, with learning rate 20.65879555832674
[09/26 11:04:09 visual_prompt]: Epoch 61 / 100: avg data time: 4.59e-02, avg batch time: 0.4917, average train loss: 21.5540
[09/26 11:04:10 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1678, average loss: 24.9692
[09/26 11:04:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 59.00	
[09/26 11:04:10 visual_prompt]: Training 62 / 100 epoch, with learning rate 19.80220772955602
[09/26 11:04:17 visual_prompt]: Epoch 62 / 100: avg data time: 4.81e-02, avg batch time: 0.4926, average train loss: 19.2917
[09/26 11:04:18 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1679, average loss: 22.8903
[09/26 11:04:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 61.00	
[09/26 11:04:18 visual_prompt]: Best epoch 62: best metric: 0.195
[09/26 11:04:18 visual_prompt]: Training 63 / 100 epoch, with learning rate 18.95195261000831
[09/26 11:04:25 visual_prompt]: Epoch 63 / 100: avg data time: 5.92e-02, avg batch time: 0.5032, average train loss: 16.9344
[09/26 11:04:27 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1676, average loss: 17.6271
[09/26 11:04:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 54.00	
[09/26 11:04:27 visual_prompt]: Training 64 / 100 epoch, with learning rate 18.10906610457502
[09/26 11:04:33 visual_prompt]: Epoch 64 / 100: avg data time: 4.65e-02, avg batch time: 0.4917, average train loss: 15.5219
[09/26 11:04:35 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1675, average loss: 9.4368
[09/26 11:04:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 57.50	
[09/26 11:04:35 visual_prompt]: Training 65 / 100 epoch, with learning rate 17.274575140626318
[09/26 11:04:42 visual_prompt]: Epoch 65 / 100: avg data time: 4.95e-02, avg batch time: 0.4937, average train loss: 11.7870
[09/26 11:04:43 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1678, average loss: 10.0688
[09/26 11:04:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 58.50	
[09/26 11:04:43 visual_prompt]: Training 66 / 100 epoch, with learning rate 16.449496416858285
[09/26 11:04:50 visual_prompt]: Epoch 66 / 100: avg data time: 4.30e-02, avg batch time: 0.4892, average train loss: 8.9360
[09/26 11:04:51 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1676, average loss: 13.6367
[09/26 11:04:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 11:04:51 visual_prompt]: Training 67 / 100 epoch, with learning rate 15.634835164602197
[09/26 11:04:58 visual_prompt]: Epoch 67 / 100: avg data time: 5.33e-02, avg batch time: 0.5005, average train loss: 9.1100
[09/26 11:04:59 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1677, average loss: 10.1282
[09/26 11:04:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 65.00	
[09/26 11:04:59 visual_prompt]: Training 68 / 100 epoch, with learning rate 14.831583923104999
[09/26 11:05:06 visual_prompt]: Epoch 68 / 100: avg data time: 4.39e-02, avg batch time: 0.4906, average train loss: 9.8845
[09/26 11:05:08 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1676, average loss: 7.6703
[09/26 11:05:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 74.00	
[09/26 11:05:08 visual_prompt]: Training 69 / 100 epoch, with learning rate 14.040721330273062
[09/26 11:05:14 visual_prompt]: Epoch 69 / 100: avg data time: 4.85e-02, avg batch time: 0.4942, average train loss: 6.2757
[09/26 11:05:16 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1676, average loss: 4.8259
[09/26 11:05:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 73.50	
[09/26 11:05:16 visual_prompt]: Training 70 / 100 epoch, with learning rate 13.263210930352736
[09/26 11:05:23 visual_prompt]: Epoch 70 / 100: avg data time: 4.43e-02, avg batch time: 0.4896, average train loss: 6.8660
[09/26 11:05:24 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1678, average loss: 7.3569
[09/26 11:05:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 64.50	
[09/26 11:05:24 visual_prompt]: Training 71 / 100 epoch, with learning rate 12.500000000000005
[09/26 11:05:31 visual_prompt]: Epoch 71 / 100: avg data time: 4.62e-02, avg batch time: 0.4907, average train loss: 6.8540
[09/26 11:05:32 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1676, average loss: 5.9417
[09/26 11:05:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 76.00	
[09/26 11:05:32 visual_prompt]: Training 72 / 100 epoch, with learning rate 11.75201839416988
[09/26 11:05:39 visual_prompt]: Epoch 72 / 100: avg data time: 4.45e-02, avg batch time: 0.4898, average train loss: 4.7189
[09/26 11:05:40 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1674, average loss: 5.1769
[09/26 11:05:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 70.00	
[09/26 11:05:40 visual_prompt]: Training 73 / 100 epoch, with learning rate 11.020177413231332
[09/26 11:05:47 visual_prompt]: Epoch 73 / 100: avg data time: 5.67e-02, avg batch time: 0.5005, average train loss: 3.8852
[09/26 11:05:49 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1677, average loss: 4.5361
[09/26 11:05:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 71.00	
[09/26 11:05:49 visual_prompt]: Training 74 / 100 epoch, with learning rate 10.305368692688175
[09/26 11:05:55 visual_prompt]: Epoch 74 / 100: avg data time: 4.38e-02, avg batch time: 0.4893, average train loss: 2.8185
[09/26 11:05:57 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1673, average loss: 3.2812
[09/26 11:05:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 77.50	
[09/26 11:05:57 visual_prompt]: Training 75 / 100 epoch, with learning rate 9.608463116858543
[09/26 11:06:04 visual_prompt]: Epoch 75 / 100: avg data time: 4.58e-02, avg batch time: 0.4909, average train loss: 2.4414
[09/26 11:06:05 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1676, average loss: 3.5776
[09/26 11:06:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 75.50	
[09/26 11:06:05 visual_prompt]: Training 76 / 100 epoch, with learning rate 8.930309757836516
[09/26 11:06:12 visual_prompt]: Epoch 76 / 100: avg data time: 4.46e-02, avg batch time: 0.4892, average train loss: 2.5951
[09/26 11:06:13 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1677, average loss: 3.4267
[09/26 11:06:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 74.50	
[09/26 11:06:13 visual_prompt]: Training 77 / 100 epoch, with learning rate 8.271734841028552
[09/26 11:06:20 visual_prompt]: Epoch 77 / 100: avg data time: 5.34e-02, avg batch time: 0.4969, average train loss: 2.8199
[09/26 11:06:22 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1674, average loss: 2.9582
[09/26 11:06:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 75.00	
[09/26 11:06:22 visual_prompt]: Training 78 / 100 epoch, with learning rate 7.633540738525066
[09/26 11:06:28 visual_prompt]: Epoch 78 / 100: avg data time: 4.63e-02, avg batch time: 0.4901, average train loss: 2.4020
[09/26 11:06:30 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1677, average loss: 3.3942
[09/26 11:06:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 74.50	
[09/26 11:06:30 visual_prompt]: Training 79 / 100 epoch, with learning rate 7.016504991533726
[09/26 11:06:37 visual_prompt]: Epoch 79 / 100: avg data time: 4.33e-02, avg batch time: 0.4881, average train loss: 2.4414
[09/26 11:06:38 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1675, average loss: 3.3061
[09/26 11:06:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 76.50	
[09/26 11:06:38 visual_prompt]: Training 80 / 100 epoch, with learning rate 6.4213793630651415
[09/26 11:06:45 visual_prompt]: Epoch 80 / 100: avg data time: 4.36e-02, avg batch time: 0.4873, average train loss: 2.3585
[09/26 11:06:46 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1674, average loss: 3.0334
[09/26 11:06:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 74.50	
[09/26 11:06:46 visual_prompt]: Training 81 / 100 epoch, with learning rate 5.848888922025552
[09/26 11:06:53 visual_prompt]: Epoch 81 / 100: avg data time: 5.47e-02, avg batch time: 0.4988, average train loss: 2.3928
[09/26 11:06:55 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1674, average loss: 2.9633
[09/26 11:06:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 75.00	
[09/26 11:06:55 visual_prompt]: Training 82 / 100 epoch, with learning rate 5.2997311598319525
[09/26 11:07:01 visual_prompt]: Epoch 82 / 100: avg data time: 4.55e-02, avg batch time: 0.4912, average train loss: 2.1162
[09/26 11:07:03 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1676, average loss: 2.8535
[09/26 11:07:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 75.50	
[09/26 11:07:03 visual_prompt]: Training 83 / 100 epoch, with learning rate 4.774575140626316
[09/26 11:07:10 visual_prompt]: Epoch 83 / 100: avg data time: 6.26e-02, avg batch time: 0.5062, average train loss: 2.0296
[09/26 11:07:11 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1677, average loss: 3.2320
[09/26 11:07:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 71.50	
[09/26 11:07:11 visual_prompt]: Training 84 / 100 epoch, with learning rate 4.27406068612396
[09/26 11:07:18 visual_prompt]: Epoch 84 / 100: avg data time: 6.73e-02, avg batch time: 0.5107, average train loss: 1.9871
[09/26 11:07:20 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1674, average loss: 2.7814
[09/26 11:07:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 75.00	
[09/26 11:07:20 visual_prompt]: Training 85 / 100 epoch, with learning rate 3.798797596089351
[09/26 11:07:26 visual_prompt]: Epoch 85 / 100: avg data time: 4.48e-02, avg batch time: 0.4916, average train loss: 2.0441
[09/26 11:07:28 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1673, average loss: 3.3201
[09/26 11:07:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 68.00	
[09/26 11:07:28 visual_prompt]: Training 86 / 100 epoch, with learning rate 3.349364905389032
[09/26 11:07:35 visual_prompt]: Epoch 86 / 100: avg data time: 4.74e-02, avg batch time: 0.4914, average train loss: 2.0479
[09/26 11:07:36 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1674, average loss: 2.9941
[09/26 11:07:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 70.00	
[09/26 11:07:36 visual_prompt]: Training 87 / 100 epoch, with learning rate 2.9263101785268253
[09/26 11:07:43 visual_prompt]: Epoch 87 / 100: avg data time: 5.50e-02, avg batch time: 0.4986, average train loss: 1.9701
[09/26 11:07:44 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1672, average loss: 2.8281
[09/26 11:07:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 75.50	
[09/26 11:07:44 visual_prompt]: Training 88 / 100 epoch, with learning rate 2.5301488425208296
[09/26 11:07:51 visual_prompt]: Epoch 88 / 100: avg data time: 5.22e-02, avg batch time: 0.4958, average train loss: 1.9584
[09/26 11:07:53 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1674, average loss: 2.6591
[09/26 11:07:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 75.00	
[09/26 11:07:53 visual_prompt]: Training 89 / 100 epoch, with learning rate 2.1613635589349753
[09/26 11:07:59 visual_prompt]: Epoch 89 / 100: avg data time: 5.23e-02, avg batch time: 0.4965, average train loss: 1.9240
[09/26 11:08:01 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1672, average loss: 2.8894
[09/26 11:08:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 69.00	
[09/26 11:08:01 visual_prompt]: Training 90 / 100 epoch, with learning rate 1.820403635830317
[09/26 11:08:08 visual_prompt]: Epoch 90 / 100: avg data time: 4.43e-02, avg batch time: 0.4889, average train loss: 1.8862
[09/26 11:08:09 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1672, average loss: 2.6830
[09/26 11:08:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 72.50	
[09/26 11:08:09 visual_prompt]: Training 91 / 100 epoch, with learning rate 1.507684480352292
[09/26 11:08:16 visual_prompt]: Epoch 91 / 100: avg data time: 5.39e-02, avg batch time: 0.4983, average train loss: 1.9025
[09/26 11:08:17 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1673, average loss: 2.6021
[09/26 11:08:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 73.50	
[09/26 11:08:17 visual_prompt]: Training 92 / 100 epoch, with learning rate 1.2235870926211616
[09/26 11:08:24 visual_prompt]: Epoch 92 / 100: avg data time: 4.41e-02, avg batch time: 0.4881, average train loss: 1.8821
[09/26 11:08:26 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1672, average loss: 2.7214
[09/26 11:08:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 75.00	
[09/26 11:08:26 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.9684576015420276
[09/26 11:08:32 visual_prompt]: Epoch 93 / 100: avg data time: 4.43e-02, avg batch time: 0.4878, average train loss: 1.8720
[09/26 11:08:34 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1675, average loss: 2.7406
[09/26 11:08:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 73.50	
[09/26 11:08:34 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.7426068431000882
[09/26 11:08:41 visual_prompt]: Epoch 94 / 100: avg data time: 4.41e-02, avg batch time: 0.4890, average train loss: 1.8309
[09/26 11:08:42 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1671, average loss: 2.6743
[09/26 11:08:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 74.00	
[09/26 11:08:42 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.5463099816548578
[09/26 11:08:49 visual_prompt]: Epoch 95 / 100: avg data time: 5.84e-02, avg batch time: 0.5012, average train loss: 1.8265
[09/26 11:08:50 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1674, average loss: 2.6902
[09/26 11:08:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 74.00	
[09/26 11:08:50 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.3798061746947995
[09/26 11:08:57 visual_prompt]: Epoch 96 / 100: avg data time: 5.87e-02, avg batch time: 0.5029, average train loss: 1.8317
[09/26 11:08:59 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1674, average loss: 2.6765
[09/26 11:08:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 75.00	
[09/26 11:08:59 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.24329828146074095
[09/26 11:09:05 visual_prompt]: Epoch 97 / 100: avg data time: 4.70e-02, avg batch time: 0.4907, average train loss: 1.8193
[09/26 11:09:07 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1677, average loss: 2.7004
[09/26 11:09:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 74.00	
[09/26 11:09:07 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.13695261579316775
[09/26 11:09:14 visual_prompt]: Epoch 98 / 100: avg data time: 5.63e-02, avg batch time: 0.5003, average train loss: 1.8214
[09/26 11:09:15 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1674, average loss: 2.6747
[09/26 11:09:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 74.50	
[09/26 11:09:15 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.06089874350439506
[09/26 11:09:22 visual_prompt]: Epoch 99 / 100: avg data time: 5.07e-02, avg batch time: 0.4969, average train loss: 1.7923
[09/26 11:09:24 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1677, average loss: 2.6700
[09/26 11:09:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 75.00	
[09/26 11:09:24 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.015229324522605947
[09/26 11:09:30 visual_prompt]: Epoch 100 / 100: avg data time: 5.99e-02, avg batch time: 0.5031, average train loss: 1.8273
[09/26 11:09:32 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1672, average loss: 2.6705
[09/26 11:09:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 74.50	
[09/26 11:09:32 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:09:32 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:09:32 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:09:32 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:09:32 visual_prompt]: Training with config:
[09/26 11:09:32 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:09:32 visual_prompt]: Loading training data...
[09/26 11:09:32 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 11:09:33 visual_prompt]: Number of images: 800
[09/26 11:09:33 visual_prompt]: Number of classes: 9 / 9
[09/26 11:09:33 visual_prompt]: Loading validation data...
[09/26 11:09:33 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 11:09:33 visual_prompt]: Number of images: 200
[09/26 11:09:33 visual_prompt]: Number of classes: 9 / 9
[09/26 11:09:33 visual_prompt]: Constructing models...
[09/26 11:09:36 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 11:09:36 visual_prompt]: tuned percent:0.542
[09/26 11:09:36 visual_prompt]: Device used for model: 0
[09/26 11:09:36 visual_prompt]: Setting up Evaluator...
[09/26 11:09:36 visual_prompt]: Setting up Trainer...
[09/26 11:09:36 visual_prompt]: 	Setting up the optimizer...
[09/26 11:09:36 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:09:43 visual_prompt]: Epoch 1 / 100: avg data time: 5.11e-02, avg batch time: 0.4944, average train loss: 2.8636
[09/26 11:09:44 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1673, average loss: 2.9516
[09/26 11:09:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 11:09:44 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 11:09:44 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 11:09:51 visual_prompt]: Epoch 2 / 100: avg data time: 4.54e-02, avg batch time: 0.4903, average train loss: 19.0515
[09/26 11:09:52 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1664, average loss: 12.6370
[09/26 11:09:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 11:09:52 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 11:09:59 visual_prompt]: Epoch 3 / 100: avg data time: 4.84e-02, avg batch time: 0.4913, average train loss: 12.8579
[09/26 11:10:01 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1671, average loss: 42.9231
[09/26 11:10:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 51.00	
[09/26 11:10:01 visual_prompt]: Best epoch 3: best metric: 0.135
[09/26 11:10:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 11:10:07 visual_prompt]: Epoch 4 / 100: avg data time: 5.64e-02, avg batch time: 0.4991, average train loss: 32.1137
[09/26 11:10:09 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1672, average loss: 18.8473
[09/26 11:10:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 11:10:09 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 11:10:15 visual_prompt]: Epoch 5 / 100: avg data time: 4.47e-02, avg batch time: 0.4893, average train loss: 45.9153
[09/26 11:10:17 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1674, average loss: 61.9335
[09/26 11:10:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 6.50	top5: 55.50	
[09/26 11:10:17 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 11:10:24 visual_prompt]: Epoch 6 / 100: avg data time: 4.50e-02, avg batch time: 0.4894, average train loss: 56.2335
[09/26 11:10:25 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1671, average loss: 48.5876
[09/26 11:10:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 11:10:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 11:10:32 visual_prompt]: Epoch 7 / 100: avg data time: 5.12e-02, avg batch time: 0.4951, average train loss: 59.1243
[09/26 11:10:33 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1673, average loss: 40.2757
[09/26 11:10:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 61.00	
[09/26 11:10:33 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 11:10:40 visual_prompt]: Epoch 8 / 100: avg data time: 4.65e-02, avg batch time: 0.4910, average train loss: 66.1924
[09/26 11:10:42 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1669, average loss: 71.5654
[09/26 11:10:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 11:10:42 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 11:10:48 visual_prompt]: Epoch 9 / 100: avg data time: 4.50e-02, avg batch time: 0.4887, average train loss: 74.0500
[09/26 11:10:50 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1669, average loss: 31.9360
[09/26 11:10:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 11:10:50 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 11:10:57 visual_prompt]: Epoch 10 / 100: avg data time: 5.47e-02, avg batch time: 0.4978, average train loss: 82.1960
[09/26 11:10:58 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1674, average loss: 141.0347
[09/26 11:10:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 57.00	
[09/26 11:10:58 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 11:11:05 visual_prompt]: Epoch 11 / 100: avg data time: 5.62e-02, avg batch time: 0.4993, average train loss: 94.1561
[09/26 11:11:06 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1672, average loss: 57.2976
[09/26 11:11:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 11:11:06 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 11:11:13 visual_prompt]: Epoch 12 / 100: avg data time: 4.93e-02, avg batch time: 0.4942, average train loss: 93.9395
[09/26 11:11:15 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1674, average loss: 165.9429
[09/26 11:11:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.00	
[09/26 11:11:15 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 11:11:21 visual_prompt]: Epoch 13 / 100: avg data time: 4.51e-02, avg batch time: 0.4916, average train loss: 95.7781
[09/26 11:11:23 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1675, average loss: 146.4118
[09/26 11:11:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 51.50	
[09/26 11:11:23 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 11:11:30 visual_prompt]: Epoch 14 / 100: avg data time: 4.77e-02, avg batch time: 0.4920, average train loss: 121.2382
[09/26 11:11:31 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1673, average loss: 179.3523
[09/26 11:11:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:11:31 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 11:11:38 visual_prompt]: Epoch 15 / 100: avg data time: 4.99e-02, avg batch time: 0.4937, average train loss: 120.1799
[09/26 11:11:39 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1671, average loss: 102.3225
[09/26 11:11:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 11:11:39 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 11:11:46 visual_prompt]: Epoch 16 / 100: avg data time: 5.10e-02, avg batch time: 0.4940, average train loss: 93.9992
[09/26 11:11:48 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1671, average loss: 71.6265
[09/26 11:11:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 11:11:48 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 11:11:54 visual_prompt]: Epoch 17 / 100: avg data time: 4.63e-02, avg batch time: 0.4901, average train loss: 58.1193
[09/26 11:11:56 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1676, average loss: 59.2091
[09/26 11:11:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 11:11:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 11:12:02 visual_prompt]: Epoch 18 / 100: avg data time: 4.52e-02, avg batch time: 0.4886, average train loss: 75.6207
[09/26 11:12:04 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1673, average loss: 98.5654
[09/26 11:12:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/26 11:12:04 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 11:12:11 visual_prompt]: Epoch 19 / 100: avg data time: 5.32e-02, avg batch time: 0.4978, average train loss: 100.4513
[09/26 11:12:12 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1673, average loss: 87.2190
[09/26 11:12:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 11:12:12 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 11:12:19 visual_prompt]: Epoch 20 / 100: avg data time: 4.18e-02, avg batch time: 0.4863, average train loss: 116.3606
[09/26 11:12:20 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1674, average loss: 122.2599
[09/26 11:12:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 11:12:20 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 11:12:27 visual_prompt]: Epoch 21 / 100: avg data time: 5.48e-02, avg batch time: 0.4985, average train loss: 117.9401
[09/26 11:12:29 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1671, average loss: 127.2489
[09/26 11:12:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 11:12:29 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 11:12:36 visual_prompt]: Epoch 22 / 100: avg data time: 5.23e-02, avg batch time: 0.4952, average train loss: 97.6002
[09/26 11:12:37 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1673, average loss: 160.7533
[09/26 11:12:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/26 11:12:37 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 11:12:44 visual_prompt]: Epoch 23 / 100: avg data time: 4.54e-02, avg batch time: 0.4886, average train loss: 105.3276
[09/26 11:12:45 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1675, average loss: 94.0009
[09/26 11:12:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.00	
[09/26 11:12:45 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 11:12:52 visual_prompt]: Epoch 24 / 100: avg data time: 4.88e-02, avg batch time: 0.4930, average train loss: 104.6534
[09/26 11:12:54 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1675, average loss: 68.1564
[09/26 11:12:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 11:12:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 11:13:00 visual_prompt]: Epoch 25 / 100: avg data time: 4.92e-02, avg batch time: 0.4924, average train loss: 92.0985
[09/26 11:13:02 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1673, average loss: 114.5243
[09/26 11:13:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.50	
[09/26 11:13:02 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 11:13:09 visual_prompt]: Epoch 26 / 100: avg data time: 4.94e-02, avg batch time: 0.4939, average train loss: 101.7255
[09/26 11:13:10 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1675, average loss: 88.0222
[09/26 11:13:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 11:13:10 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 11:13:17 visual_prompt]: Epoch 27 / 100: avg data time: 4.54e-02, avg batch time: 0.4918, average train loss: 78.3874
[09/26 11:13:19 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1669, average loss: 87.3027
[09/26 11:13:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.50	
[09/26 11:13:19 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 11:13:25 visual_prompt]: Epoch 28 / 100: avg data time: 5.73e-02, avg batch time: 0.5004, average train loss: 99.1614
[09/26 11:13:27 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1672, average loss: 101.0234
[09/26 11:13:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 11:13:27 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 11:13:34 visual_prompt]: Epoch 29 / 100: avg data time: 5.05e-02, avg batch time: 0.4942, average train loss: 102.7143
[09/26 11:13:35 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1669, average loss: 120.6198
[09/26 11:13:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.00	
[09/26 11:13:35 visual_prompt]: Best epoch 29: best metric: 0.145
[09/26 11:13:35 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 11:13:42 visual_prompt]: Epoch 30 / 100: avg data time: 4.51e-02, avg batch time: 0.4897, average train loss: 94.7894
[09/26 11:13:43 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1672, average loss: 70.0320
[09/26 11:13:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 50.50	
[09/26 11:13:43 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 11:13:50 visual_prompt]: Epoch 31 / 100: avg data time: 5.60e-02, avg batch time: 0.5001, average train loss: 93.3831
[09/26 11:13:52 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1668, average loss: 94.2706
[09/26 11:13:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.00	
[09/26 11:13:52 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 11:13:58 visual_prompt]: Epoch 32 / 100: avg data time: 4.61e-02, avg batch time: 0.4893, average train loss: 76.4949
[09/26 11:14:00 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1670, average loss: 77.0621
[09/26 11:14:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:14:00 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 11:14:07 visual_prompt]: Epoch 33 / 100: avg data time: 4.57e-02, avg batch time: 0.4904, average train loss: 107.1347
[09/26 11:14:08 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1671, average loss: 70.2513
[09/26 11:14:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/26 11:14:08 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 11:14:15 visual_prompt]: Epoch 34 / 100: avg data time: 4.85e-02, avg batch time: 0.4920, average train loss: 86.4606
[09/26 11:14:16 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1670, average loss: 43.8282
[09/26 11:14:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 56.50	
[09/26 11:14:16 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 11:14:23 visual_prompt]: Epoch 35 / 100: avg data time: 6.54e-02, avg batch time: 0.5082, average train loss: 81.2186
[09/26 11:14:25 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1666, average loss: 96.8195
[09/26 11:14:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 11:14:25 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 11:14:32 visual_prompt]: Epoch 36 / 100: avg data time: 5.52e-02, avg batch time: 0.4987, average train loss: 83.0201
[09/26 11:14:33 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1671, average loss: 82.7651
[09/26 11:14:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 11:14:33 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 11:14:40 visual_prompt]: Epoch 37 / 100: avg data time: 5.37e-02, avg batch time: 0.4962, average train loss: 90.0077
[09/26 11:14:42 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1672, average loss: 63.2598
[09/26 11:14:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 11:14:42 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 11:14:48 visual_prompt]: Epoch 38 / 100: avg data time: 5.69e-02, avg batch time: 0.4996, average train loss: 91.4004
[09/26 11:14:50 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1672, average loss: 69.5533
[09/26 11:14:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 11:14:50 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 11:14:57 visual_prompt]: Epoch 39 / 100: avg data time: 6.08e-02, avg batch time: 0.5028, average train loss: 90.9930
[09/26 11:14:58 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1673, average loss: 70.3067
[09/26 11:14:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 11:14:58 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 11:15:05 visual_prompt]: Epoch 40 / 100: avg data time: 5.70e-02, avg batch time: 0.5001, average train loss: 87.4379
[09/26 11:15:07 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1672, average loss: 57.8410
[09/26 11:15:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.50	
[09/26 11:15:07 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 11:15:13 visual_prompt]: Epoch 41 / 100: avg data time: 4.52e-02, avg batch time: 0.4881, average train loss: 98.3224
[09/26 11:15:15 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1671, average loss: 102.4904
[09/26 11:15:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 11:15:15 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 11:15:22 visual_prompt]: Epoch 42 / 100: avg data time: 4.51e-02, avg batch time: 0.4891, average train loss: 100.6345
[09/26 11:15:23 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1675, average loss: 54.5635
[09/26 11:15:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 11:15:23 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 11:15:30 visual_prompt]: Epoch 43 / 100: avg data time: 4.89e-02, avg batch time: 0.4922, average train loss: 101.6586
[09/26 11:15:31 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1673, average loss: 70.1049
[09/26 11:15:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 11:15:31 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 11:15:38 visual_prompt]: Epoch 44 / 100: avg data time: 4.66e-02, avg batch time: 0.4901, average train loss: 77.4210
[09/26 11:15:39 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1669, average loss: 133.2018
[09/26 11:15:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 60.00	
[09/26 11:15:39 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 11:15:46 visual_prompt]: Epoch 45 / 100: avg data time: 5.84e-02, avg batch time: 0.5022, average train loss: 90.7253
[09/26 11:15:48 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1677, average loss: 75.3802
[09/26 11:15:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.50	
[09/26 11:15:48 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 11:15:55 visual_prompt]: Epoch 46 / 100: avg data time: 5.74e-02, avg batch time: 0.5001, average train loss: 68.1721
[09/26 11:15:56 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1674, average loss: 82.4253
[09/26 11:15:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 52.50	
[09/26 11:15:56 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 11:16:03 visual_prompt]: Epoch 47 / 100: avg data time: 5.70e-02, avg batch time: 0.4995, average train loss: 73.1169
[09/26 11:16:04 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1674, average loss: 122.0723
[09/26 11:16:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 11:16:04 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 11:16:11 visual_prompt]: Epoch 48 / 100: avg data time: 4.50e-02, avg batch time: 0.4887, average train loss: 73.3146
[09/26 11:16:13 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1672, average loss: 58.4786
[09/26 11:16:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 52.00	
[09/26 11:16:13 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 11:16:20 visual_prompt]: Epoch 49 / 100: avg data time: 6.03e-02, avg batch time: 0.5072, average train loss: 67.2117
[09/26 11:16:21 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1672, average loss: 63.5298
[09/26 11:16:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 11:16:21 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 11:16:28 visual_prompt]: Epoch 50 / 100: avg data time: 6.08e-02, avg batch time: 0.5054, average train loss: 63.6784
[09/26 11:16:29 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1670, average loss: 66.5672
[09/26 11:16:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:16:30 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 11:16:36 visual_prompt]: Epoch 51 / 100: avg data time: 5.46e-02, avg batch time: 0.4992, average train loss: 53.1010
[09/26 11:16:38 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1673, average loss: 35.7757
[09/26 11:16:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 51.50	
[09/26 11:16:38 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 11:16:45 visual_prompt]: Epoch 52 / 100: avg data time: 4.52e-02, avg batch time: 0.4891, average train loss: 47.7033
[09/26 11:16:46 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1670, average loss: 39.2847
[09/26 11:16:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 11:16:46 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 11:16:53 visual_prompt]: Epoch 53 / 100: avg data time: 4.24e-02, avg batch time: 0.4872, average train loss: 50.9808
[09/26 11:16:54 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1673, average loss: 113.3156
[09/26 11:16:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:16:54 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 11:17:01 visual_prompt]: Epoch 54 / 100: avg data time: 5.57e-02, avg batch time: 0.4992, average train loss: 106.5983
[09/26 11:17:03 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1676, average loss: 79.4222
[09/26 11:17:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 59.00	
[09/26 11:17:03 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 11:17:09 visual_prompt]: Epoch 55 / 100: avg data time: 4.17e-02, avg batch time: 0.4873, average train loss: 69.8897
[09/26 11:17:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1675, average loss: 76.4812
[09/26 11:17:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 11:17:11 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 11:17:18 visual_prompt]: Epoch 56 / 100: avg data time: 4.80e-02, avg batch time: 0.4918, average train loss: 62.7032
[09/26 11:17:19 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1670, average loss: 55.5925
[09/26 11:17:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.50	
[09/26 11:17:19 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 11:17:26 visual_prompt]: Epoch 57 / 100: avg data time: 4.39e-02, avg batch time: 0.4876, average train loss: 61.8836
[09/26 11:17:27 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1673, average loss: 26.5245
[09/26 11:17:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 11:17:27 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 11:17:34 visual_prompt]: Epoch 58 / 100: avg data time: 5.61e-02, avg batch time: 0.4987, average train loss: 40.0280
[09/26 11:17:36 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1672, average loss: 45.8724
[09/26 11:17:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 11:17:36 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 11:17:42 visual_prompt]: Epoch 59 / 100: avg data time: 5.47e-02, avg batch time: 0.4986, average train loss: 49.5137
[09/26 11:17:44 visual_prompt]: Inference (val):avg data time: 4.43e-05, avg batch time: 0.1673, average loss: 58.7468
[09/26 11:17:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.50	
[09/26 11:17:44 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 11:17:51 visual_prompt]: Epoch 60 / 100: avg data time: 4.28e-02, avg batch time: 0.4864, average train loss: 47.5225
[09/26 11:17:52 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1675, average loss: 46.9510
[09/26 11:17:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 11:17:52 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 11:17:59 visual_prompt]: Epoch 61 / 100: avg data time: 4.67e-02, avg batch time: 0.4923, average train loss: 52.3591
[09/26 11:18:00 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1673, average loss: 40.5119
[09/26 11:18:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 11:18:00 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 11:18:07 visual_prompt]: Epoch 62 / 100: avg data time: 5.79e-02, avg batch time: 0.5013, average train loss: 45.8047
[09/26 11:18:09 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1671, average loss: 57.7490
[09/26 11:18:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 11:18:09 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 11:18:16 visual_prompt]: Epoch 63 / 100: avg data time: 4.51e-02, avg batch time: 0.4909, average train loss: 45.2473
[09/26 11:18:17 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1675, average loss: 35.9888
[09/26 11:18:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 11:18:17 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 11:18:24 visual_prompt]: Epoch 64 / 100: avg data time: 4.46e-02, avg batch time: 0.4884, average train loss: 39.5689
[09/26 11:18:25 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1674, average loss: 34.1527
[09/26 11:18:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 11:18:25 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 11:18:32 visual_prompt]: Epoch 65 / 100: avg data time: 4.60e-02, avg batch time: 0.4902, average train loss: 39.0930
[09/26 11:18:34 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1677, average loss: 46.7583
[09/26 11:18:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.00	
[09/26 11:18:34 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 11:18:40 visual_prompt]: Epoch 66 / 100: avg data time: 5.22e-02, avg batch time: 0.4950, average train loss: 46.1368
[09/26 11:18:42 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1670, average loss: 39.8249
[09/26 11:18:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/26 11:18:42 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 11:18:48 visual_prompt]: Epoch 67 / 100: avg data time: 4.34e-02, avg batch time: 0.4871, average train loss: 41.7447
[09/26 11:18:50 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1674, average loss: 77.6690
[09/26 11:18:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 11:18:50 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 11:18:57 visual_prompt]: Epoch 68 / 100: avg data time: 5.31e-02, avg batch time: 0.4961, average train loss: 42.1051
[09/26 11:18:58 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1673, average loss: 35.1973
[09/26 11:18:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 58.50	
[09/26 11:18:58 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 11:19:05 visual_prompt]: Epoch 69 / 100: avg data time: 5.88e-02, avg batch time: 0.5012, average train loss: 35.4640
[09/26 11:19:07 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1673, average loss: 37.5934
[09/26 11:19:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 11:19:07 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 11:19:13 visual_prompt]: Epoch 70 / 100: avg data time: 6.29e-02, avg batch time: 0.5060, average train loss: 33.5005
[09/26 11:19:15 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1670, average loss: 23.4703
[09/26 11:19:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 11:19:15 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 11:19:22 visual_prompt]: Epoch 71 / 100: avg data time: 4.52e-02, avg batch time: 0.4885, average train loss: 29.5635
[09/26 11:19:23 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1674, average loss: 30.5687
[09/26 11:19:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.50	
[09/26 11:19:23 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 11:19:30 visual_prompt]: Epoch 72 / 100: avg data time: 5.70e-02, avg batch time: 0.5027, average train loss: 28.8418
[09/26 11:19:32 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1676, average loss: 21.2232
[09/26 11:19:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 11:19:32 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 11:19:38 visual_prompt]: Epoch 73 / 100: avg data time: 4.87e-02, avg batch time: 0.4927, average train loss: 28.7351
[09/26 11:19:40 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1672, average loss: 26.8220
[09/26 11:19:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 11:19:40 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 11:19:46 visual_prompt]: Epoch 74 / 100: avg data time: 4.63e-02, avg batch time: 0.4891, average train loss: 29.4776
[09/26 11:19:48 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1671, average loss: 25.5182
[09/26 11:19:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 11:19:48 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 11:19:55 visual_prompt]: Epoch 75 / 100: avg data time: 5.74e-02, avg batch time: 0.5023, average train loss: 22.2871
[09/26 11:19:56 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1673, average loss: 18.4709
[09/26 11:19:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 11:19:56 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 11:20:03 visual_prompt]: Epoch 76 / 100: avg data time: 6.00e-02, avg batch time: 0.5023, average train loss: 16.5785
[09/26 11:20:05 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1673, average loss: 11.6761
[09/26 11:20:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 11:20:05 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 11:20:12 visual_prompt]: Epoch 77 / 100: avg data time: 5.99e-02, avg batch time: 0.5032, average train loss: 23.8286
[09/26 11:20:13 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1670, average loss: 21.4885
[09/26 11:20:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 11:20:13 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 11:20:20 visual_prompt]: Epoch 78 / 100: avg data time: 5.27e-02, avg batch time: 0.4963, average train loss: 15.9538
[09/26 11:20:21 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1676, average loss: 14.6920
[09/26 11:20:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 11:20:21 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 11:20:28 visual_prompt]: Epoch 79 / 100: avg data time: 4.56e-02, avg batch time: 0.4906, average train loss: 12.4756
[09/26 11:20:30 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1671, average loss: 26.6003
[09/26 11:20:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.50	
[09/26 11:20:30 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 11:20:36 visual_prompt]: Epoch 80 / 100: avg data time: 5.37e-02, avg batch time: 0.4973, average train loss: 17.9886
[09/26 11:20:38 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1671, average loss: 15.5699
[09/26 11:20:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 11:20:38 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 11:20:45 visual_prompt]: Epoch 81 / 100: avg data time: 6.85e-02, avg batch time: 0.5112, average train loss: 18.4361
[09/26 11:20:47 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1673, average loss: 12.0688
[09/26 11:20:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 11:20:47 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 11:20:53 visual_prompt]: Epoch 82 / 100: avg data time: 6.02e-02, avg batch time: 0.5037, average train loss: 11.2726
[09/26 11:20:55 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1670, average loss: 8.8293
[09/26 11:20:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.00	
[09/26 11:20:55 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 11:21:02 visual_prompt]: Epoch 83 / 100: avg data time: 4.85e-02, avg batch time: 0.4928, average train loss: 7.4278
[09/26 11:21:03 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1670, average loss: 5.2034
[09/26 11:21:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 11:21:03 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 11:21:10 visual_prompt]: Epoch 84 / 100: avg data time: 4.99e-02, avg batch time: 0.4956, average train loss: 3.8636
[09/26 11:21:11 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1673, average loss: 3.1266
[09/26 11:21:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 11:21:11 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 11:21:18 visual_prompt]: Epoch 85 / 100: avg data time: 4.58e-02, avg batch time: 0.4924, average train loss: 3.1619
[09/26 11:21:20 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1672, average loss: 3.0992
[09/26 11:21:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 48.50	
[09/26 11:21:20 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 11:21:26 visual_prompt]: Epoch 86 / 100: avg data time: 4.32e-02, avg batch time: 0.4871, average train loss: 2.7897
[09/26 11:21:28 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1675, average loss: 3.1366
[09/26 11:21:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 11:21:28 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 11:21:35 visual_prompt]: Epoch 87 / 100: avg data time: 5.28e-02, avg batch time: 0.4958, average train loss: 2.6732
[09/26 11:21:36 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1674, average loss: 2.3311
[09/26 11:21:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 49.00	
[09/26 11:21:36 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 11:21:43 visual_prompt]: Epoch 88 / 100: avg data time: 4.40e-02, avg batch time: 0.4870, average train loss: 2.3285
[09/26 11:21:44 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1676, average loss: 2.3197
[09/26 11:21:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:21:44 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 11:21:51 visual_prompt]: Epoch 89 / 100: avg data time: 5.71e-02, avg batch time: 0.5030, average train loss: 2.3177
[09/26 11:21:53 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1673, average loss: 2.3592
[09/26 11:21:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 53.50	
[09/26 11:21:53 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 11:21:59 visual_prompt]: Epoch 90 / 100: avg data time: 4.51e-02, avg batch time: 0.4893, average train loss: 2.3306
[09/26 11:22:01 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1674, average loss: 2.3083
[09/26 11:22:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 11:22:01 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 11:22:08 visual_prompt]: Epoch 91 / 100: avg data time: 4.37e-02, avg batch time: 0.4882, average train loss: 2.2649
[09/26 11:22:09 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1676, average loss: 2.2207
[09/26 11:22:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 52.00	
[09/26 11:22:09 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 11:22:16 visual_prompt]: Epoch 92 / 100: avg data time: 4.68e-02, avg batch time: 0.4918, average train loss: 2.2492
[09/26 11:22:17 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1673, average loss: 2.2426
[09/26 11:22:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 11:22:17 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 11:22:24 visual_prompt]: Epoch 93 / 100: avg data time: 5.22e-02, avg batch time: 0.4975, average train loss: 2.2261
[09/26 11:22:25 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1673, average loss: 2.2242
[09/26 11:22:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 11:22:25 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 11:22:32 visual_prompt]: Epoch 94 / 100: avg data time: 5.83e-02, avg batch time: 0.5007, average train loss: 2.2341
[09/26 11:22:34 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1674, average loss: 2.1928
[09/26 11:22:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 11:22:34 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 11:22:40 visual_prompt]: Epoch 95 / 100: avg data time: 5.43e-02, avg batch time: 0.4964, average train loss: 2.2247
[09/26 11:22:42 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1674, average loss: 2.2651
[09/26 11:22:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 11:22:42 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 11:22:49 visual_prompt]: Epoch 96 / 100: avg data time: 4.84e-02, avg batch time: 0.4912, average train loss: 2.2147
[09/26 11:22:50 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1671, average loss: 2.2026
[09/26 11:22:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 11:22:50 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 11:22:57 visual_prompt]: Epoch 97 / 100: avg data time: 4.65e-02, avg batch time: 0.4902, average train loss: 2.2057
[09/26 11:22:58 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1670, average loss: 2.2047
[09/26 11:22:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:22:58 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 11:23:05 visual_prompt]: Epoch 98 / 100: avg data time: 4.84e-02, avg batch time: 0.4932, average train loss: 2.1940
[09/26 11:23:06 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1674, average loss: 2.2006
[09/26 11:23:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 11:23:06 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 11:23:13 visual_prompt]: Epoch 99 / 100: avg data time: 4.62e-02, avg batch time: 0.4908, average train loss: 2.1925
[09/26 11:23:15 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1675, average loss: 2.1948
[09/26 11:23:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:23:15 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 11:23:21 visual_prompt]: Epoch 100 / 100: avg data time: 4.43e-02, avg batch time: 0.4876, average train loss: 2.1913
[09/26 11:23:23 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1672, average loss: 2.1931
[09/26 11:23:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:23:23 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:23:23 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:23:23 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:23:23 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:23:23 visual_prompt]: Training with config:
[09/26 11:23:23 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:23:23 visual_prompt]: Loading training data...
[09/26 11:23:23 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 11:23:24 visual_prompt]: Number of images: 800
[09/26 11:23:24 visual_prompt]: Number of classes: 9 / 9
[09/26 11:23:24 visual_prompt]: Loading validation data...
[09/26 11:23:24 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 11:23:24 visual_prompt]: Number of images: 200
[09/26 11:23:24 visual_prompt]: Number of classes: 9 / 9
[09/26 11:23:24 visual_prompt]: Constructing models...
[09/26 11:23:27 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 11:23:27 visual_prompt]: tuned percent:0.542
[09/26 11:23:27 visual_prompt]: Device used for model: 0
[09/26 11:23:27 visual_prompt]: Setting up Evaluator...
[09/26 11:23:27 visual_prompt]: Setting up Trainer...
[09/26 11:23:27 visual_prompt]: 	Setting up the optimizer...
[09/26 11:23:27 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:23:34 visual_prompt]: Epoch 1 / 100: avg data time: 5.66e-02, avg batch time: 0.4997, average train loss: 2.8688
[09/26 11:23:35 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1670, average loss: 2.9516
[09/26 11:23:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 11:23:35 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 11:23:35 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 11:23:42 visual_prompt]: Epoch 2 / 100: avg data time: 5.21e-02, avg batch time: 0.4953, average train loss: 12.1367
[09/26 11:23:44 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.1670, average loss: 10.3773
[09/26 11:23:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.00	
[09/26 11:23:44 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 11:23:50 visual_prompt]: Epoch 3 / 100: avg data time: 6.44e-02, avg batch time: 0.5065, average train loss: 38.1115
[09/26 11:23:52 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1670, average loss: 54.0987
[09/26 11:23:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 11:23:52 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 11:23:59 visual_prompt]: Epoch 4 / 100: avg data time: 4.99e-02, avg batch time: 0.4932, average train loss: 46.9912
[09/26 11:24:00 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1669, average loss: 40.6070
[09/26 11:24:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 60.50	
[09/26 11:24:00 visual_prompt]: Best epoch 4: best metric: 0.135
[09/26 11:24:00 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 11:24:07 visual_prompt]: Epoch 5 / 100: avg data time: 5.44e-02, avg batch time: 0.4992, average train loss: 41.6987
[09/26 11:24:09 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1672, average loss: 59.4324
[09/26 11:24:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 11:24:09 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 11:24:15 visual_prompt]: Epoch 6 / 100: avg data time: 6.09e-02, avg batch time: 0.5036, average train loss: 49.8198
[09/26 11:24:17 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1670, average loss: 84.7576
[09/26 11:24:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.00	
[09/26 11:24:17 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 11:24:24 visual_prompt]: Epoch 7 / 100: avg data time: 5.70e-02, avg batch time: 0.5011, average train loss: 66.6884
[09/26 11:24:25 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1669, average loss: 95.2114
[09/26 11:24:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 11:24:25 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 11:24:32 visual_prompt]: Epoch 8 / 100: avg data time: 5.63e-02, avg batch time: 0.4990, average train loss: 82.0156
[09/26 11:24:34 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1674, average loss: 56.8340
[09/26 11:24:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:24:34 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 11:24:41 visual_prompt]: Epoch 9 / 100: avg data time: 6.39e-02, avg batch time: 0.5074, average train loss: 101.4029
[09/26 11:24:42 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1676, average loss: 81.7413
[09/26 11:24:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 11:24:42 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 11:24:49 visual_prompt]: Epoch 10 / 100: avg data time: 6.10e-02, avg batch time: 0.5042, average train loss: 90.9460
[09/26 11:24:50 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1676, average loss: 78.9610
[09/26 11:24:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 11:24:50 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 11:24:57 visual_prompt]: Epoch 11 / 100: avg data time: 5.87e-02, avg batch time: 0.5017, average train loss: 100.9690
[09/26 11:24:59 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1671, average loss: 138.0811
[09/26 11:24:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 51.50	
[09/26 11:24:59 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 11:25:06 visual_prompt]: Epoch 12 / 100: avg data time: 4.41e-02, avg batch time: 0.4882, average train loss: 108.0958
[09/26 11:25:07 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.1671, average loss: 84.3450
[09/26 11:25:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 11:25:07 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 11:25:14 visual_prompt]: Epoch 13 / 100: avg data time: 5.66e-02, avg batch time: 0.4991, average train loss: 122.6215
[09/26 11:25:15 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1674, average loss: 96.9162
[09/26 11:25:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 11:25:15 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 11:25:22 visual_prompt]: Epoch 14 / 100: avg data time: 4.45e-02, avg batch time: 0.4904, average train loss: 136.8290
[09/26 11:25:24 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1675, average loss: 127.4415
[09/26 11:25:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 11:25:24 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 11:25:30 visual_prompt]: Epoch 15 / 100: avg data time: 5.67e-02, avg batch time: 0.4996, average train loss: 124.9386
[09/26 11:25:32 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1672, average loss: 284.5487
[09/26 11:25:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.50	
[09/26 11:25:32 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 11:25:39 visual_prompt]: Epoch 16 / 100: avg data time: 4.59e-02, avg batch time: 0.4888, average train loss: 143.4445
[09/26 11:25:40 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1675, average loss: 74.9292
[09/26 11:25:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.00	
[09/26 11:25:40 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 11:25:47 visual_prompt]: Epoch 17 / 100: avg data time: 5.87e-02, avg batch time: 0.5016, average train loss: 152.1911
[09/26 11:25:49 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1674, average loss: 100.5341
[09/26 11:25:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 11:25:49 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 11:25:55 visual_prompt]: Epoch 18 / 100: avg data time: 5.28e-02, avg batch time: 0.4982, average train loss: 98.6563
[09/26 11:25:57 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1677, average loss: 130.4086
[09/26 11:25:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 11:25:57 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 11:26:04 visual_prompt]: Epoch 19 / 100: avg data time: 5.75e-02, avg batch time: 0.5011, average train loss: 114.8833
[09/26 11:26:05 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1673, average loss: 121.2091
[09/26 11:26:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 11:26:05 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 11:26:12 visual_prompt]: Epoch 20 / 100: avg data time: 6.00e-02, avg batch time: 0.5033, average train loss: 151.7551
[09/26 11:26:14 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1672, average loss: 112.6087
[09/26 11:26:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 11:26:14 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 11:26:21 visual_prompt]: Epoch 21 / 100: avg data time: 5.19e-02, avg batch time: 0.4962, average train loss: 85.2110
[09/26 11:26:22 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1674, average loss: 101.5848
[09/26 11:26:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 11:26:22 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 11:26:29 visual_prompt]: Epoch 22 / 100: avg data time: 5.19e-02, avg batch time: 0.4967, average train loss: 84.3798
[09/26 11:26:31 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1674, average loss: 72.0763
[09/26 11:26:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 60.50	
[09/26 11:26:31 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 11:26:37 visual_prompt]: Epoch 23 / 100: avg data time: 6.09e-02, avg batch time: 0.5051, average train loss: 127.1807
[09/26 11:26:39 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1676, average loss: 172.9760
[09/26 11:26:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.00	
[09/26 11:26:39 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 11:26:46 visual_prompt]: Epoch 24 / 100: avg data time: 6.45e-02, avg batch time: 0.5077, average train loss: 152.3577
[09/26 11:26:47 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1673, average loss: 99.6007
[09/26 11:26:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.50	
[09/26 11:26:47 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 11:26:54 visual_prompt]: Epoch 25 / 100: avg data time: 6.48e-02, avg batch time: 0.5068, average train loss: 146.8494
[09/26 11:26:56 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1675, average loss: 96.6202
[09/26 11:26:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 11:26:56 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 11:27:03 visual_prompt]: Epoch 26 / 100: avg data time: 6.26e-02, avg batch time: 0.5065, average train loss: 119.8470
[09/26 11:27:04 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1674, average loss: 72.2034
[09/26 11:27:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 54.50	
[09/26 11:27:04 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 11:27:11 visual_prompt]: Epoch 27 / 100: avg data time: 5.22e-02, avg batch time: 0.4963, average train loss: 113.1319
[09/26 11:27:12 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1676, average loss: 134.6502
[09/26 11:27:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 11:27:13 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 11:27:19 visual_prompt]: Epoch 28 / 100: avg data time: 5.32e-02, avg batch time: 0.4969, average train loss: 99.7887
[09/26 11:27:21 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1678, average loss: 84.3333
[09/26 11:27:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 53.00	
[09/26 11:27:21 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 11:27:28 visual_prompt]: Epoch 29 / 100: avg data time: 5.51e-02, avg batch time: 0.4989, average train loss: 88.6521
[09/26 11:27:29 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1672, average loss: 112.3913
[09/26 11:27:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 11:27:29 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 11:27:36 visual_prompt]: Epoch 30 / 100: avg data time: 4.79e-02, avg batch time: 0.4924, average train loss: 109.4580
[09/26 11:27:37 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1675, average loss: 119.1841
[09/26 11:27:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 11:27:37 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 11:27:44 visual_prompt]: Epoch 31 / 100: avg data time: 5.59e-02, avg batch time: 0.4994, average train loss: 149.3663
[09/26 11:27:46 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1666, average loss: 152.1306
[09/26 11:27:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 11:27:46 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 11:27:53 visual_prompt]: Epoch 32 / 100: avg data time: 5.70e-02, avg batch time: 0.5001, average train loss: 107.4182
[09/26 11:27:54 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1674, average loss: 112.0166
[09/26 11:27:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 11:27:54 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 11:28:01 visual_prompt]: Epoch 33 / 100: avg data time: 5.76e-02, avg batch time: 0.5004, average train loss: 106.8420
[09/26 11:28:03 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1671, average loss: 166.4577
[09/26 11:28:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 11:28:03 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 11:28:09 visual_prompt]: Epoch 34 / 100: avg data time: 5.61e-02, avg batch time: 0.4994, average train loss: 151.1234
[09/26 11:28:11 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1670, average loss: 170.1158
[09/26 11:28:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 11:28:11 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 11:28:18 visual_prompt]: Epoch 35 / 100: avg data time: 4.37e-02, avg batch time: 0.4882, average train loss: 109.2239
[09/26 11:28:19 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1671, average loss: 81.6763
[09/26 11:28:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:28:19 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 11:28:26 visual_prompt]: Epoch 36 / 100: avg data time: 4.79e-02, avg batch time: 0.4902, average train loss: 101.2588
[09/26 11:28:27 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1672, average loss: 99.0458
[09/26 11:28:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 11:28:27 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 11:28:34 visual_prompt]: Epoch 37 / 100: avg data time: 5.16e-02, avg batch time: 0.4947, average train loss: 113.1662
[09/26 11:28:36 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1669, average loss: 92.4956
[09/26 11:28:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 11:28:36 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 11:28:42 visual_prompt]: Epoch 38 / 100: avg data time: 4.44e-02, avg batch time: 0.4886, average train loss: 97.9085
[09/26 11:28:44 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1670, average loss: 92.4658
[09/26 11:28:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 11:28:44 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 11:28:51 visual_prompt]: Epoch 39 / 100: avg data time: 4.48e-02, avg batch time: 0.4889, average train loss: 94.8954
[09/26 11:28:52 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1671, average loss: 76.0159
[09/26 11:28:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/26 11:28:52 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 11:28:59 visual_prompt]: Epoch 40 / 100: avg data time: 5.33e-02, avg batch time: 0.4969, average train loss: 62.9436
[09/26 11:29:00 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1670, average loss: 96.1700
[09/26 11:29:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 11:29:00 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 11:29:07 visual_prompt]: Epoch 41 / 100: avg data time: 5.57e-02, avg batch time: 0.4990, average train loss: 95.1569
[09/26 11:29:09 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1672, average loss: 109.5778
[09/26 11:29:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:29:09 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 11:29:16 visual_prompt]: Epoch 42 / 100: avg data time: 6.04e-02, avg batch time: 0.5034, average train loss: 88.8543
[09/26 11:29:17 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1672, average loss: 96.8327
[09/26 11:29:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 11:29:17 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 11:29:24 visual_prompt]: Epoch 43 / 100: avg data time: 4.51e-02, avg batch time: 0.4905, average train loss: 75.4772
[09/26 11:29:25 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1674, average loss: 96.7023
[09/26 11:29:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 11:29:25 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 11:29:32 visual_prompt]: Epoch 44 / 100: avg data time: 4.45e-02, avg batch time: 0.4897, average train loss: 58.4534
[09/26 11:29:33 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1671, average loss: 62.0125
[09/26 11:29:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 11:29:33 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 11:29:40 visual_prompt]: Epoch 45 / 100: avg data time: 5.50e-02, avg batch time: 0.4984, average train loss: 86.2614
[09/26 11:29:42 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1672, average loss: 62.6759
[09/26 11:29:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 11:29:42 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 11:29:49 visual_prompt]: Epoch 46 / 100: avg data time: 6.07e-02, avg batch time: 0.5031, average train loss: 70.9368
[09/26 11:29:50 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1672, average loss: 62.7255
[09/26 11:29:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 53.50	
[09/26 11:29:50 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 11:29:57 visual_prompt]: Epoch 47 / 100: avg data time: 4.79e-02, avg batch time: 0.4927, average train loss: 78.8960
[09/26 11:29:58 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1672, average loss: 52.2179
[09/26 11:29:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 11:29:58 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 11:30:05 visual_prompt]: Epoch 48 / 100: avg data time: 5.11e-02, avg batch time: 0.4948, average train loss: 110.4630
[09/26 11:30:07 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1675, average loss: 107.3750
[09/26 11:30:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.50	
[09/26 11:30:07 visual_prompt]: Best epoch 48: best metric: 0.145
[09/26 11:30:07 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 11:30:14 visual_prompt]: Epoch 49 / 100: avg data time: 6.32e-02, avg batch time: 0.5061, average train loss: 110.9583
[09/26 11:30:15 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1673, average loss: 69.4085
[09/26 11:30:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/26 11:30:15 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 11:30:22 visual_prompt]: Epoch 50 / 100: avg data time: 6.00e-02, avg batch time: 0.5032, average train loss: 81.2065
[09/26 11:30:24 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1677, average loss: 97.0030
[09/26 11:30:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 11:30:24 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 11:30:30 visual_prompt]: Epoch 51 / 100: avg data time: 4.47e-02, avg batch time: 0.4886, average train loss: 65.5906
[09/26 11:30:32 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1670, average loss: 59.9551
[09/26 11:30:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.00	
[09/26 11:30:32 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 11:30:39 visual_prompt]: Epoch 52 / 100: avg data time: 6.13e-02, avg batch time: 0.5039, average train loss: 78.1898
[09/26 11:30:40 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1670, average loss: 83.0827
[09/26 11:30:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 11:30:40 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 11:30:47 visual_prompt]: Epoch 53 / 100: avg data time: 5.31e-02, avg batch time: 0.4970, average train loss: 73.3322
[09/26 11:30:48 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1673, average loss: 50.8301
[09/26 11:30:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 11:30:48 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 11:30:55 visual_prompt]: Epoch 54 / 100: avg data time: 5.33e-02, avg batch time: 0.4983, average train loss: 69.7062
[09/26 11:30:57 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1670, average loss: 110.1147
[09/26 11:30:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:30:57 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 11:31:04 visual_prompt]: Epoch 55 / 100: avg data time: 4.83e-02, avg batch time: 0.4928, average train loss: 63.5550
[09/26 11:31:05 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1676, average loss: 51.4179
[09/26 11:31:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 11:31:05 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 11:31:12 visual_prompt]: Epoch 56 / 100: avg data time: 5.52e-02, avg batch time: 0.4977, average train loss: 43.7526
[09/26 11:31:13 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1672, average loss: 47.2186
[09/26 11:31:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 11:31:13 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 11:31:20 visual_prompt]: Epoch 57 / 100: avg data time: 5.55e-02, avg batch time: 0.4986, average train loss: 43.4316
[09/26 11:31:22 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1671, average loss: 35.9422
[09/26 11:31:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 11:31:22 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 11:31:29 visual_prompt]: Epoch 58 / 100: avg data time: 5.58e-02, avg batch time: 0.4976, average train loss: 63.1300
[09/26 11:31:30 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1668, average loss: 50.6702
[09/26 11:31:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:31:30 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 11:31:37 visual_prompt]: Epoch 59 / 100: avg data time: 5.39e-02, avg batch time: 0.4982, average train loss: 50.0712
[09/26 11:31:39 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1672, average loss: 38.5868
[09/26 11:31:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:31:39 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 11:31:45 visual_prompt]: Epoch 60 / 100: avg data time: 5.12e-02, avg batch time: 0.4946, average train loss: 41.6835
[09/26 11:31:47 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1676, average loss: 90.4207
[09/26 11:31:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.50	
[09/26 11:31:47 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 11:31:54 visual_prompt]: Epoch 61 / 100: avg data time: 5.28e-02, avg batch time: 0.4955, average train loss: 58.4450
[09/26 11:31:55 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1668, average loss: 56.5633
[09/26 11:31:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/26 11:31:55 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 11:32:02 visual_prompt]: Epoch 62 / 100: avg data time: 5.90e-02, avg batch time: 0.5016, average train loss: 57.8767
[09/26 11:32:04 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1677, average loss: 69.2160
[09/26 11:32:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 11:32:04 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 11:32:10 visual_prompt]: Epoch 63 / 100: avg data time: 4.53e-02, avg batch time: 0.4898, average train loss: 88.8697
[09/26 11:32:12 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1673, average loss: 68.2920
[09/26 11:32:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 50.50	
[09/26 11:32:12 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 11:32:19 visual_prompt]: Epoch 64 / 100: avg data time: 5.22e-02, avg batch time: 0.4962, average train loss: 66.0586
[09/26 11:32:20 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1670, average loss: 71.1443
[09/26 11:32:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 11:32:20 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 11:32:27 visual_prompt]: Epoch 65 / 100: avg data time: 5.05e-02, avg batch time: 0.4947, average train loss: 50.6190
[09/26 11:32:29 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1673, average loss: 33.3640
[09/26 11:32:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 11:32:29 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 11:32:35 visual_prompt]: Epoch 66 / 100: avg data time: 5.03e-02, avg batch time: 0.4962, average train loss: 35.1577
[09/26 11:32:37 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1674, average loss: 36.5063
[09/26 11:32:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 11:32:37 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 11:32:44 visual_prompt]: Epoch 67 / 100: avg data time: 5.40e-02, avg batch time: 0.4972, average train loss: 31.3460
[09/26 11:32:45 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1673, average loss: 26.7975
[09/26 11:32:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 11:32:45 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 11:32:52 visual_prompt]: Epoch 68 / 100: avg data time: 5.85e-02, avg batch time: 0.5020, average train loss: 36.9697
[09/26 11:32:54 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1675, average loss: 29.1600
[09/26 11:32:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 11:32:54 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 11:33:01 visual_prompt]: Epoch 69 / 100: avg data time: 6.24e-02, avg batch time: 0.5055, average train loss: 23.7536
[09/26 11:33:02 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1673, average loss: 17.3538
[09/26 11:33:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 48.50	
[09/26 11:33:02 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 11:33:09 visual_prompt]: Epoch 70 / 100: avg data time: 6.69e-02, avg batch time: 0.5095, average train loss: 27.1631
[09/26 11:33:11 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1670, average loss: 38.5826
[09/26 11:33:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 11:33:11 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 11:33:17 visual_prompt]: Epoch 71 / 100: avg data time: 4.59e-02, avg batch time: 0.4895, average train loss: 39.1941
[09/26 11:33:19 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1678, average loss: 23.9748
[09/26 11:33:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 11:33:19 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 11:33:26 visual_prompt]: Epoch 72 / 100: avg data time: 6.15e-02, avg batch time: 0.5061, average train loss: 24.7977
[09/26 11:33:27 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1671, average loss: 19.0673
[09/26 11:33:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 57.00	
[09/26 11:33:27 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 11:33:34 visual_prompt]: Epoch 73 / 100: avg data time: 5.71e-02, avg batch time: 0.4996, average train loss: 19.8166
[09/26 11:33:36 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1677, average loss: 13.4018
[09/26 11:33:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 11:33:36 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 11:33:42 visual_prompt]: Epoch 74 / 100: avg data time: 5.68e-02, avg batch time: 0.5014, average train loss: 16.4921
[09/26 11:33:44 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1670, average loss: 20.0773
[09/26 11:33:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 11:33:44 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 11:33:51 visual_prompt]: Epoch 75 / 100: avg data time: 6.19e-02, avg batch time: 0.5055, average train loss: 15.6784
[09/26 11:33:52 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1675, average loss: 24.8022
[09/26 11:33:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 11:33:52 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 11:33:59 visual_prompt]: Epoch 76 / 100: avg data time: 4.58e-02, avg batch time: 0.4900, average train loss: 18.4212
[09/26 11:34:01 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1674, average loss: 21.4525
[09/26 11:34:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 11:34:01 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 11:34:08 visual_prompt]: Epoch 77 / 100: avg data time: 6.02e-02, avg batch time: 0.5032, average train loss: 13.9626
[09/26 11:34:09 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1673, average loss: 18.1023
[09/26 11:34:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 11:34:09 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 11:34:16 visual_prompt]: Epoch 78 / 100: avg data time: 5.08e-02, avg batch time: 0.4956, average train loss: 14.8267
[09/26 11:34:18 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1675, average loss: 8.9126
[09/26 11:34:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.00	
[09/26 11:34:18 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 11:34:24 visual_prompt]: Epoch 79 / 100: avg data time: 4.81e-02, avg batch time: 0.4925, average train loss: 7.3897
[09/26 11:34:26 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1677, average loss: 5.4034
[09/26 11:34:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/26 11:34:26 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 11:34:33 visual_prompt]: Epoch 80 / 100: avg data time: 6.35e-02, avg batch time: 0.5067, average train loss: 4.7827
[09/26 11:34:34 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1678, average loss: 7.2119
[09/26 11:34:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 11:34:34 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 11:34:41 visual_prompt]: Epoch 81 / 100: avg data time: 5.63e-02, avg batch time: 0.4993, average train loss: 4.6228
[09/26 11:34:43 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1674, average loss: 3.3771
[09/26 11:34:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 11:34:43 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 11:34:49 visual_prompt]: Epoch 82 / 100: avg data time: 5.96e-02, avg batch time: 0.5035, average train loss: 3.3806
[09/26 11:34:51 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1675, average loss: 2.8757
[09/26 11:34:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 11:34:51 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 11:34:58 visual_prompt]: Epoch 83 / 100: avg data time: 4.45e-02, avg batch time: 0.4882, average train loss: 2.7425
[09/26 11:34:59 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1673, average loss: 2.8802
[09/26 11:34:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 48.50	
[09/26 11:34:59 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 11:35:06 visual_prompt]: Epoch 84 / 100: avg data time: 5.40e-02, avg batch time: 0.4998, average train loss: 2.5953
[09/26 11:35:08 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1674, average loss: 2.3285
[09/26 11:35:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 11:35:08 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 11:35:14 visual_prompt]: Epoch 85 / 100: avg data time: 5.63e-02, avg batch time: 0.4986, average train loss: 2.3473
[09/26 11:35:16 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1671, average loss: 2.4055
[09/26 11:35:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.00	
[09/26 11:35:16 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 11:35:23 visual_prompt]: Epoch 86 / 100: avg data time: 5.56e-02, avg batch time: 0.4985, average train loss: 2.3949
[09/26 11:35:24 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1670, average loss: 2.4942
[09/26 11:35:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 48.50	
[09/26 11:35:24 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 11:35:31 visual_prompt]: Epoch 87 / 100: avg data time: 5.42e-02, avg batch time: 0.4976, average train loss: 2.2840
[09/26 11:35:33 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1673, average loss: 2.2358
[09/26 11:35:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 11:35:33 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 11:35:40 visual_prompt]: Epoch 88 / 100: avg data time: 6.41e-02, avg batch time: 0.5060, average train loss: 2.2329
[09/26 11:35:41 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1671, average loss: 2.2735
[09/26 11:35:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 58.00	
[09/26 11:35:41 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 11:35:48 visual_prompt]: Epoch 89 / 100: avg data time: 6.42e-02, avg batch time: 0.5081, average train loss: 2.2483
[09/26 11:35:50 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1672, average loss: 2.2013
[09/26 11:35:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.00	
[09/26 11:35:50 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 11:35:57 visual_prompt]: Epoch 90 / 100: avg data time: 5.81e-02, avg batch time: 0.5010, average train loss: 2.1788
[09/26 11:35:58 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1672, average loss: 2.1301
[09/26 11:35:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 72.50	
[09/26 11:35:58 visual_prompt]: Best epoch 90: best metric: 0.150
[09/26 11:35:58 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 11:36:05 visual_prompt]: Epoch 91 / 100: avg data time: 6.02e-02, avg batch time: 0.5029, average train loss: 2.1864
[09/26 11:36:06 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1670, average loss: 2.1866
[09/26 11:36:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 68.50	
[09/26 11:36:06 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 11:36:13 visual_prompt]: Epoch 92 / 100: avg data time: 5.29e-02, avg batch time: 0.4980, average train loss: 2.1676
[09/26 11:36:15 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1674, average loss: 2.4107
[09/26 11:36:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 53.50	
[09/26 11:36:15 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 11:36:22 visual_prompt]: Epoch 93 / 100: avg data time: 5.69e-02, avg batch time: 0.5004, average train loss: 2.1352
[09/26 11:36:23 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1671, average loss: 2.0891
[09/26 11:36:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 77.00	
[09/26 11:36:23 visual_prompt]: Best epoch 93: best metric: 0.180
[09/26 11:36:23 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 11:36:30 visual_prompt]: Epoch 94 / 100: avg data time: 4.76e-02, avg batch time: 0.4914, average train loss: 2.0712
[09/26 11:36:31 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1673, average loss: 2.0097
[09/26 11:36:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 80.00	
[09/26 11:36:31 visual_prompt]: Best epoch 94: best metric: 0.220
[09/26 11:36:31 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 11:36:38 visual_prompt]: Epoch 95 / 100: avg data time: 5.60e-02, avg batch time: 0.5003, average train loss: 1.9592
[09/26 11:36:40 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1673, average loss: 2.2195
[09/26 11:36:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 64.00	
[09/26 11:36:40 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 11:36:47 visual_prompt]: Epoch 96 / 100: avg data time: 6.43e-02, avg batch time: 0.5083, average train loss: 2.0069
[09/26 11:36:48 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1675, average loss: 1.9907
[09/26 11:36:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 83.50	
[09/26 11:36:48 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 11:36:55 visual_prompt]: Epoch 97 / 100: avg data time: 5.39e-02, avg batch time: 0.4991, average train loss: 1.8623
[09/26 11:36:57 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1674, average loss: 1.8240
[09/26 11:36:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 87.00	
[09/26 11:36:57 visual_prompt]: Best epoch 97: best metric: 0.245
[09/26 11:36:57 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 11:37:03 visual_prompt]: Epoch 98 / 100: avg data time: 5.87e-02, avg batch time: 0.5034, average train loss: 1.7929
[09/26 11:37:05 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1673, average loss: 1.8702
[09/26 11:37:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 85.50	
[09/26 11:37:05 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 11:37:12 visual_prompt]: Epoch 99 / 100: avg data time: 5.15e-02, avg batch time: 0.4955, average train loss: 1.7500
[09/26 11:37:13 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1675, average loss: 1.8224
[09/26 11:37:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 11:37:13 visual_prompt]: Best epoch 99: best metric: 0.260
[09/26 11:37:13 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 11:37:20 visual_prompt]: Epoch 100 / 100: avg data time: 4.68e-02, avg batch time: 0.4913, average train loss: 1.7176
[09/26 11:37:21 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1674, average loss: 1.7925
[09/26 11:37:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 87.50	
[09/26 11:37:21 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:37:21 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:37:21 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:37:21 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:37:21 visual_prompt]: Training with config:
[09/26 11:37:21 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:37:21 visual_prompt]: Loading training data...
[09/26 11:37:21 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 11:37:23 visual_prompt]: Number of images: 800
[09/26 11:37:23 visual_prompt]: Number of classes: 9 / 9
[09/26 11:37:23 visual_prompt]: Loading validation data...
[09/26 11:37:23 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 11:37:23 visual_prompt]: Number of images: 200
[09/26 11:37:23 visual_prompt]: Number of classes: 9 / 9
[09/26 11:37:23 visual_prompt]: Constructing models...
[09/26 11:37:25 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 11:37:25 visual_prompt]: tuned percent:0.542
[09/26 11:37:25 visual_prompt]: Device used for model: 0
[09/26 11:37:25 visual_prompt]: Setting up Evaluator...
[09/26 11:37:25 visual_prompt]: Setting up Trainer...
[09/26 11:37:25 visual_prompt]: 	Setting up the optimizer...
[09/26 11:37:25 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:37:32 visual_prompt]: Epoch 1 / 100: avg data time: 4.48e-02, avg batch time: 0.4903, average train loss: 2.8665
[09/26 11:37:34 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1672, average loss: 2.9516
[09/26 11:37:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 11:37:34 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 11:37:34 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 11:37:40 visual_prompt]: Epoch 2 / 100: avg data time: 4.56e-02, avg batch time: 0.4893, average train loss: 14.6967
[09/26 11:37:42 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1667, average loss: 8.9600
[09/26 11:37:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 11:37:42 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 11:37:49 visual_prompt]: Epoch 3 / 100: avg data time: 5.73e-02, avg batch time: 0.4992, average train loss: 42.1715
[09/26 11:37:50 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1673, average loss: 19.0288
[09/26 11:37:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 11:37:50 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 11:37:57 visual_prompt]: Epoch 4 / 100: avg data time: 4.32e-02, avg batch time: 0.4859, average train loss: 41.4726
[09/26 11:37:58 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1667, average loss: 34.3546
[09/26 11:37:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 11:37:58 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 11:38:05 visual_prompt]: Epoch 5 / 100: avg data time: 4.40e-02, avg batch time: 0.4878, average train loss: 52.6607
[09/26 11:38:07 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1668, average loss: 47.9754
[09/26 11:38:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 11:38:07 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 11:38:14 visual_prompt]: Epoch 6 / 100: avg data time: 5.38e-02, avg batch time: 0.4980, average train loss: 77.1503
[09/26 11:38:15 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1671, average loss: 73.0246
[09/26 11:38:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 11:38:15 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 11:38:22 visual_prompt]: Epoch 7 / 100: avg data time: 4.77e-02, avg batch time: 0.4911, average train loss: 78.6878
[09/26 11:38:23 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1672, average loss: 106.2769
[09/26 11:38:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 11:38:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 11:38:30 visual_prompt]: Epoch 8 / 100: avg data time: 5.37e-02, avg batch time: 0.4982, average train loss: 91.1500
[09/26 11:38:32 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1672, average loss: 65.4986
[09/26 11:38:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 11:38:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 11:38:38 visual_prompt]: Epoch 9 / 100: avg data time: 4.82e-02, avg batch time: 0.4926, average train loss: 115.6046
[09/26 11:38:40 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1670, average loss: 113.8399
[09/26 11:38:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 11:38:40 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 11:38:47 visual_prompt]: Epoch 10 / 100: avg data time: 4.38e-02, avg batch time: 0.4898, average train loss: 113.7893
[09/26 11:38:48 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1672, average loss: 109.2633
[09/26 11:38:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 11:38:48 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 11:38:55 visual_prompt]: Epoch 11 / 100: avg data time: 4.37e-02, avg batch time: 0.4876, average train loss: 129.1751
[09/26 11:38:57 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1672, average loss: 137.1621
[09/26 11:38:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 11:38:57 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 11:39:03 visual_prompt]: Epoch 12 / 100: avg data time: 4.56e-02, avg batch time: 0.4909, average train loss: 138.1920
[09/26 11:39:05 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1673, average loss: 104.6893
[09/26 11:39:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 11:39:05 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 11:39:12 visual_prompt]: Epoch 13 / 100: avg data time: 5.22e-02, avg batch time: 0.4954, average train loss: 120.7385
[09/26 11:39:13 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1674, average loss: 89.1239
[09/26 11:39:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 11:39:13 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 11:39:20 visual_prompt]: Epoch 14 / 100: avg data time: 5.33e-02, avg batch time: 0.4978, average train loss: 108.7906
[09/26 11:39:22 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1676, average loss: 159.9348
[09/26 11:39:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/26 11:39:22 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 11:39:28 visual_prompt]: Epoch 15 / 100: avg data time: 4.25e-02, avg batch time: 0.4867, average train loss: 177.7120
[09/26 11:39:30 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1679, average loss: 128.6822
[09/26 11:39:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 52.50	
[09/26 11:39:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 11:39:36 visual_prompt]: Epoch 16 / 100: avg data time: 4.62e-02, avg batch time: 0.4896, average train loss: 173.5851
[09/26 11:39:38 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1675, average loss: 198.9146
[09/26 11:39:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.50	top5: 53.50	
[09/26 11:39:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 11:39:45 visual_prompt]: Epoch 17 / 100: avg data time: 4.25e-02, avg batch time: 0.4862, average train loss: 150.1246
[09/26 11:39:46 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1673, average loss: 78.4028
[09/26 11:39:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 11:39:46 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 11:39:53 visual_prompt]: Epoch 18 / 100: avg data time: 4.49e-02, avg batch time: 0.4877, average train loss: 134.3605
[09/26 11:39:54 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1675, average loss: 104.9362
[09/26 11:39:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/26 11:39:54 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 11:40:01 visual_prompt]: Epoch 19 / 100: avg data time: 4.50e-02, avg batch time: 0.4900, average train loss: 115.6022
[09/26 11:40:02 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1675, average loss: 100.4549
[09/26 11:40:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.00	
[09/26 11:40:02 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 11:40:09 visual_prompt]: Epoch 20 / 100: avg data time: 5.87e-02, avg batch time: 0.5028, average train loss: 99.6138
[09/26 11:40:11 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1676, average loss: 120.3722
[09/26 11:40:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 11:40:11 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 11:40:18 visual_prompt]: Epoch 21 / 100: avg data time: 6.15e-02, avg batch time: 0.5059, average train loss: 160.0235
[09/26 11:40:19 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1674, average loss: 182.0035
[09/26 11:40:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 11:40:19 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 11:40:26 visual_prompt]: Epoch 22 / 100: avg data time: 5.13e-02, avg batch time: 0.4970, average train loss: 113.9599
[09/26 11:40:28 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1677, average loss: 69.1834
[09/26 11:40:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.50	
[09/26 11:40:28 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 11:40:34 visual_prompt]: Epoch 23 / 100: avg data time: 4.71e-02, avg batch time: 0.4940, average train loss: 85.7196
[09/26 11:40:36 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1675, average loss: 70.9057
[09/26 11:40:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.50	
[09/26 11:40:36 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 11:40:43 visual_prompt]: Epoch 24 / 100: avg data time: 4.42e-02, avg batch time: 0.4903, average train loss: 79.9123
[09/26 11:40:44 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1676, average loss: 88.8228
[09/26 11:40:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 11:40:44 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 11:40:51 visual_prompt]: Epoch 25 / 100: avg data time: 6.00e-02, avg batch time: 0.5024, average train loss: 70.7493
[09/26 11:40:53 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.1675, average loss: 61.4854
[09/26 11:40:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 11:40:53 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 11:40:59 visual_prompt]: Epoch 26 / 100: avg data time: 5.60e-02, avg batch time: 0.4999, average train loss: 95.0511
[09/26 11:41:01 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1677, average loss: 294.7263
[09/26 11:41:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 11:41:01 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 11:41:08 visual_prompt]: Epoch 27 / 100: avg data time: 5.84e-02, avg batch time: 0.5013, average train loss: 143.8575
[09/26 11:41:09 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1676, average loss: 102.5838
[09/26 11:41:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.50	
[09/26 11:41:09 visual_prompt]: Best epoch 27: best metric: 0.145
[09/26 11:41:09 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 11:41:16 visual_prompt]: Epoch 28 / 100: avg data time: 5.62e-02, avg batch time: 0.4996, average train loss: 97.1134
[09/26 11:41:18 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1669, average loss: 203.7502
[09/26 11:41:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 11:41:18 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 11:41:24 visual_prompt]: Epoch 29 / 100: avg data time: 4.58e-02, avg batch time: 0.4889, average train loss: 118.0069
[09/26 11:41:26 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1675, average loss: 187.3002
[09/26 11:41:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 11:41:26 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 11:41:33 visual_prompt]: Epoch 30 / 100: avg data time: 5.65e-02, avg batch time: 0.5004, average train loss: 105.8959
[09/26 11:41:34 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1676, average loss: 157.4961
[09/26 11:41:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.50	
[09/26 11:41:34 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 11:41:41 visual_prompt]: Epoch 31 / 100: avg data time: 5.54e-02, avg batch time: 0.4994, average train loss: 97.5544
[09/26 11:41:43 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1680, average loss: 68.0838
[09/26 11:41:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 11:41:43 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 11:41:49 visual_prompt]: Epoch 32 / 100: avg data time: 5.08e-02, avg batch time: 0.4960, average train loss: 105.6854
[09/26 11:41:51 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1672, average loss: 107.0629
[09/26 11:41:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 11:41:51 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 11:41:58 visual_prompt]: Epoch 33 / 100: avg data time: 5.76e-02, avg batch time: 0.5000, average train loss: 85.8092
[09/26 11:41:59 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1673, average loss: 84.1395
[09/26 11:41:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 48.50	
[09/26 11:41:59 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 11:42:06 visual_prompt]: Epoch 34 / 100: avg data time: 4.81e-02, avg batch time: 0.4928, average train loss: 60.7080
[09/26 11:42:08 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1672, average loss: 59.5590
[09/26 11:42:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 11:42:08 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 11:42:14 visual_prompt]: Epoch 35 / 100: avg data time: 6.23e-02, avg batch time: 0.5052, average train loss: 71.2130
[09/26 11:42:16 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1673, average loss: 80.0514
[09/26 11:42:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 11:42:16 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 11:42:23 visual_prompt]: Epoch 36 / 100: avg data time: 5.77e-02, avg batch time: 0.5013, average train loss: 53.6381
[09/26 11:42:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1675, average loss: 65.7043
[09/26 11:42:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 11:42:24 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 11:42:31 visual_prompt]: Epoch 37 / 100: avg data time: 5.30e-02, avg batch time: 0.4991, average train loss: 38.0333
[09/26 11:42:33 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1673, average loss: 46.0079
[09/26 11:42:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:42:33 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 11:42:40 visual_prompt]: Epoch 38 / 100: avg data time: 6.26e-02, avg batch time: 0.5049, average train loss: 47.0773
[09/26 11:42:41 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1673, average loss: 46.0754
[09/26 11:42:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:42:41 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 11:42:48 visual_prompt]: Epoch 39 / 100: avg data time: 5.86e-02, avg batch time: 0.5025, average train loss: 45.3497
[09/26 11:42:50 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1675, average loss: 30.9187
[09/26 11:42:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 48.50	
[09/26 11:42:50 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 11:42:56 visual_prompt]: Epoch 40 / 100: avg data time: 5.64e-02, avg batch time: 0.4997, average train loss: 34.5801
[09/26 11:42:58 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1672, average loss: 24.3352
[09/26 11:42:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 11:42:58 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 11:43:05 visual_prompt]: Epoch 41 / 100: avg data time: 6.00e-02, avg batch time: 0.5033, average train loss: 30.6994
[09/26 11:43:06 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1671, average loss: 39.3493
[09/26 11:43:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 11:43:06 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 11:43:13 visual_prompt]: Epoch 42 / 100: avg data time: 4.75e-02, avg batch time: 0.4899, average train loss: 34.0840
[09/26 11:43:15 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1672, average loss: 44.7491
[09/26 11:43:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:43:15 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 11:43:21 visual_prompt]: Epoch 43 / 100: avg data time: 5.50e-02, avg batch time: 0.4994, average train loss: 34.8314
[09/26 11:43:23 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1675, average loss: 62.0116
[09/26 11:43:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/26 11:43:23 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 11:43:30 visual_prompt]: Epoch 44 / 100: avg data time: 5.96e-02, avg batch time: 0.5035, average train loss: 50.5879
[09/26 11:43:31 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1671, average loss: 29.7108
[09/26 11:43:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.00	
[09/26 11:43:31 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 11:43:38 visual_prompt]: Epoch 45 / 100: avg data time: 6.21e-02, avg batch time: 0.5049, average train loss: 59.5640
[09/26 11:43:40 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1671, average loss: 60.7973
[09/26 11:43:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 60.00	
[09/26 11:43:40 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 11:43:47 visual_prompt]: Epoch 46 / 100: avg data time: 5.52e-02, avg batch time: 0.4978, average train loss: 57.0578
[09/26 11:43:48 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1671, average loss: 66.5410
[09/26 11:43:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.50	
[09/26 11:43:48 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 11:43:55 visual_prompt]: Epoch 47 / 100: avg data time: 5.26e-02, avg batch time: 0.4953, average train loss: 43.7358
[09/26 11:43:57 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1670, average loss: 39.3264
[09/26 11:43:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.50	
[09/26 11:43:57 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 11:44:03 visual_prompt]: Epoch 48 / 100: avg data time: 6.01e-02, avg batch time: 0.5027, average train loss: 42.3716
[09/26 11:44:05 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1672, average loss: 41.9789
[09/26 11:44:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 48.50	
[09/26 11:44:05 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 11:44:12 visual_prompt]: Epoch 49 / 100: avg data time: 6.91e-02, avg batch time: 0.5117, average train loss: 36.5892
[09/26 11:44:14 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1669, average loss: 44.6791
[09/26 11:44:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 11:44:14 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 11:44:20 visual_prompt]: Epoch 50 / 100: avg data time: 6.05e-02, avg batch time: 0.5030, average train loss: 30.5121
[09/26 11:44:22 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1673, average loss: 25.8552
[09/26 11:44:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 11:44:22 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 11:44:29 visual_prompt]: Epoch 51 / 100: avg data time: 6.46e-02, avg batch time: 0.5070, average train loss: 33.6036
[09/26 11:44:30 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1673, average loss: 23.9787
[09/26 11:44:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 11:44:30 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 11:44:37 visual_prompt]: Epoch 52 / 100: avg data time: 6.33e-02, avg batch time: 0.5054, average train loss: 33.5046
[09/26 11:44:39 visual_prompt]: Inference (val):avg data time: 4.70e-04, avg batch time: 0.1677, average loss: 30.9262
[09/26 11:44:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 11:44:39 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 11:44:46 visual_prompt]: Epoch 53 / 100: avg data time: 5.18e-02, avg batch time: 0.4963, average train loss: 38.0739
[09/26 11:44:47 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1671, average loss: 44.1965
[09/26 11:44:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 11:44:47 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 11:44:54 visual_prompt]: Epoch 54 / 100: avg data time: 5.07e-02, avg batch time: 0.4942, average train loss: 40.2771
[09/26 11:44:55 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1670, average loss: 31.9174
[09/26 11:44:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 11:44:55 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 11:45:02 visual_prompt]: Epoch 55 / 100: avg data time: 4.75e-02, avg batch time: 0.4913, average train loss: 29.5707
[09/26 11:45:04 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1672, average loss: 16.6471
[09/26 11:45:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 11:45:04 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 11:45:10 visual_prompt]: Epoch 56 / 100: avg data time: 4.76e-02, avg batch time: 0.4918, average train loss: 37.1646
[09/26 11:45:12 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1673, average loss: 34.2745
[09/26 11:45:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 11:45:12 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 11:45:19 visual_prompt]: Epoch 57 / 100: avg data time: 4.55e-02, avg batch time: 0.4887, average train loss: 31.1611
[09/26 11:45:20 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1669, average loss: 21.5608
[09/26 11:45:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 11:45:20 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 11:45:27 visual_prompt]: Epoch 58 / 100: avg data time: 4.33e-02, avg batch time: 0.4896, average train loss: 30.7811
[09/26 11:45:28 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1671, average loss: 16.3231
[09/26 11:45:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 11:45:28 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 11:45:35 visual_prompt]: Epoch 59 / 100: avg data time: 4.12e-02, avg batch time: 0.4864, average train loss: 20.5760
[09/26 11:45:36 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1672, average loss: 19.0432
[09/26 11:45:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 11:45:36 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 11:45:43 visual_prompt]: Epoch 60 / 100: avg data time: 4.54e-02, avg batch time: 0.4888, average train loss: 22.0501
[09/26 11:45:45 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1670, average loss: 13.2284
[09/26 11:45:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 54.00	
[09/26 11:45:45 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 11:45:51 visual_prompt]: Epoch 61 / 100: avg data time: 4.65e-02, avg batch time: 0.4898, average train loss: 13.4741
[09/26 11:45:53 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1671, average loss: 8.7673
[09/26 11:45:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 59.00	
[09/26 11:45:53 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 11:46:00 visual_prompt]: Epoch 62 / 100: avg data time: 5.82e-02, avg batch time: 0.5005, average train loss: 15.1116
[09/26 11:46:01 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1670, average loss: 16.1756
[09/26 11:46:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 11:46:01 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 11:46:08 visual_prompt]: Epoch 63 / 100: avg data time: 6.46e-02, avg batch time: 0.5076, average train loss: 18.6919
[09/26 11:46:10 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1672, average loss: 24.2214
[09/26 11:46:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 11:46:10 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 11:46:16 visual_prompt]: Epoch 64 / 100: avg data time: 6.06e-02, avg batch time: 0.5033, average train loss: 13.8881
[09/26 11:46:18 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1670, average loss: 13.0633
[09/26 11:46:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:46:18 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 11:46:25 visual_prompt]: Epoch 65 / 100: avg data time: 4.49e-02, avg batch time: 0.4879, average train loss: 16.9419
[09/26 11:46:26 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1671, average loss: 15.3159
[09/26 11:46:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:46:26 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 11:46:33 visual_prompt]: Epoch 66 / 100: avg data time: 5.25e-02, avg batch time: 0.4956, average train loss: 13.7994
[09/26 11:46:34 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1670, average loss: 14.0044
[09/26 11:46:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 11:46:34 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 11:46:41 visual_prompt]: Epoch 67 / 100: avg data time: 4.60e-02, avg batch time: 0.4888, average train loss: 13.5281
[09/26 11:46:43 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1669, average loss: 14.5147
[09/26 11:46:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 11:46:43 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 11:46:49 visual_prompt]: Epoch 68 / 100: avg data time: 4.31e-02, avg batch time: 0.4869, average train loss: 12.0092
[09/26 11:46:51 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1674, average loss: 9.3879
[09/26 11:46:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 11:46:51 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 11:46:58 visual_prompt]: Epoch 69 / 100: avg data time: 5.67e-02, avg batch time: 0.4993, average train loss: 10.8959
[09/26 11:46:59 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1671, average loss: 9.9146
[09/26 11:46:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.00	
[09/26 11:46:59 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 11:47:06 visual_prompt]: Epoch 70 / 100: avg data time: 4.72e-02, avg batch time: 0.4922, average train loss: 9.1520
[09/26 11:47:08 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.1673, average loss: 11.4623
[09/26 11:47:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 61.00	
[09/26 11:47:08 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 11:47:14 visual_prompt]: Epoch 71 / 100: avg data time: 5.28e-02, avg batch time: 0.4972, average train loss: 7.4383
[09/26 11:47:16 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1672, average loss: 5.7224
[09/26 11:47:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 11:47:16 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 11:47:23 visual_prompt]: Epoch 72 / 100: avg data time: 4.34e-02, avg batch time: 0.4865, average train loss: 4.3083
[09/26 11:47:24 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1671, average loss: 2.7844
[09/26 11:47:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 54.00	
[09/26 11:47:24 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 11:47:31 visual_prompt]: Epoch 73 / 100: avg data time: 4.88e-02, avg batch time: 0.4934, average train loss: 2.7374
[09/26 11:47:32 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1673, average loss: 2.4587
[09/26 11:47:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 11:47:32 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 11:47:39 visual_prompt]: Epoch 74 / 100: avg data time: 4.43e-02, avg batch time: 0.4886, average train loss: 2.7351
[09/26 11:47:41 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1669, average loss: 2.7682
[09/26 11:47:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 11:47:41 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 11:47:47 visual_prompt]: Epoch 75 / 100: avg data time: 5.36e-02, avg batch time: 0.4965, average train loss: 2.7252
[09/26 11:47:49 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1666, average loss: 2.8702
[09/26 11:47:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.50	
[09/26 11:47:49 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 11:47:56 visual_prompt]: Epoch 76 / 100: avg data time: 6.15e-02, avg batch time: 0.5052, average train loss: 2.6022
[09/26 11:47:58 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1670, average loss: 2.7479
[09/26 11:47:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.50	
[09/26 11:47:58 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 11:48:04 visual_prompt]: Epoch 77 / 100: avg data time: 4.72e-02, avg batch time: 0.4913, average train loss: 2.7580
[09/26 11:48:06 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1671, average loss: 2.5152
[09/26 11:48:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 11:48:06 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 11:48:13 visual_prompt]: Epoch 78 / 100: avg data time: 6.09e-02, avg batch time: 0.5037, average train loss: 2.4139
[09/26 11:48:14 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1669, average loss: 2.4152
[09/26 11:48:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 11:48:14 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 11:48:21 visual_prompt]: Epoch 79 / 100: avg data time: 4.45e-02, avg batch time: 0.4893, average train loss: 2.4451
[09/26 11:48:23 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1669, average loss: 2.5514
[09/26 11:48:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.00	
[09/26 11:48:23 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 11:48:29 visual_prompt]: Epoch 80 / 100: avg data time: 5.17e-02, avg batch time: 0.4944, average train loss: 2.5050
[09/26 11:48:31 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1672, average loss: 2.4242
[09/26 11:48:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 11:48:31 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 11:48:38 visual_prompt]: Epoch 81 / 100: avg data time: 6.53e-02, avg batch time: 0.5079, average train loss: 2.4113
[09/26 11:48:39 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1669, average loss: 2.4584
[09/26 11:48:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 11:48:39 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 11:48:46 visual_prompt]: Epoch 82 / 100: avg data time: 5.09e-02, avg batch time: 0.4952, average train loss: 2.4333
[09/26 11:48:48 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1672, average loss: 2.2688
[09/26 11:48:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 61.50	
[09/26 11:48:48 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 11:48:54 visual_prompt]: Epoch 83 / 100: avg data time: 4.67e-02, avg batch time: 0.4897, average train loss: 2.3870
[09/26 11:48:56 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1670, average loss: 2.3452
[09/26 11:48:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:48:56 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 11:49:03 visual_prompt]: Epoch 84 / 100: avg data time: 6.21e-02, avg batch time: 0.5042, average train loss: 2.3521
[09/26 11:49:04 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1670, average loss: 2.3137
[09/26 11:49:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 62.00	
[09/26 11:49:04 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 11:49:11 visual_prompt]: Epoch 85 / 100: avg data time: 6.50e-02, avg batch time: 0.5064, average train loss: 2.3479
[09/26 11:49:13 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1672, average loss: 2.3861
[09/26 11:49:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 54.50	
[09/26 11:49:13 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 11:49:19 visual_prompt]: Epoch 86 / 100: avg data time: 4.38e-02, avg batch time: 0.4885, average train loss: 2.2477
[09/26 11:49:21 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1672, average loss: 2.2649
[09/26 11:49:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.50	
[09/26 11:49:21 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 11:49:28 visual_prompt]: Epoch 87 / 100: avg data time: 6.19e-02, avg batch time: 0.5045, average train loss: 2.2401
[09/26 11:49:29 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1670, average loss: 2.3603
[09/26 11:49:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:49:29 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 11:49:36 visual_prompt]: Epoch 88 / 100: avg data time: 5.14e-02, avg batch time: 0.4955, average train loss: 2.2736
[09/26 11:49:38 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1675, average loss: 2.2819
[09/26 11:49:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 61.00	
[09/26 11:49:38 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 11:49:44 visual_prompt]: Epoch 89 / 100: avg data time: 6.36e-02, avg batch time: 0.5056, average train loss: 2.3064
[09/26 11:49:46 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1674, average loss: 2.3393
[09/26 11:49:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 56.50	
[09/26 11:49:46 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 11:49:53 visual_prompt]: Epoch 90 / 100: avg data time: 6.41e-02, avg batch time: 0.5059, average train loss: 2.2550
[09/26 11:49:54 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1674, average loss: 2.2358
[09/26 11:49:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 60.00	
[09/26 11:49:54 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 11:50:01 visual_prompt]: Epoch 91 / 100: avg data time: 5.51e-02, avg batch time: 0.4987, average train loss: 2.2051
[09/26 11:50:03 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1672, average loss: 2.1970
[09/26 11:50:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 60.00	
[09/26 11:50:03 visual_prompt]: Best epoch 91: best metric: 0.195
[09/26 11:50:03 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 11:50:10 visual_prompt]: Epoch 92 / 100: avg data time: 5.84e-02, avg batch time: 0.5011, average train loss: 2.1729
[09/26 11:50:11 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1669, average loss: 2.1970
[09/26 11:50:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 11:50:11 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 11:50:18 visual_prompt]: Epoch 93 / 100: avg data time: 5.62e-02, avg batch time: 0.4992, average train loss: 2.1410
[09/26 11:50:20 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1668, average loss: 2.1331
[09/26 11:50:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 64.50	
[09/26 11:50:20 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 11:50:26 visual_prompt]: Epoch 94 / 100: avg data time: 5.35e-02, avg batch time: 0.4959, average train loss: 2.1341
[09/26 11:50:28 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1671, average loss: 2.1364
[09/26 11:50:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 68.00	
[09/26 11:50:28 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 11:50:35 visual_prompt]: Epoch 95 / 100: avg data time: 6.13e-02, avg batch time: 0.5051, average train loss: 2.1209
[09/26 11:50:36 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1674, average loss: 2.1443
[09/26 11:50:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 65.50	
[09/26 11:50:36 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 11:50:43 visual_prompt]: Epoch 96 / 100: avg data time: 5.53e-02, avg batch time: 0.4976, average train loss: 2.1054
[09/26 11:50:45 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1672, average loss: 2.1467
[09/26 11:50:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 66.50	
[09/26 11:50:45 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 11:50:52 visual_prompt]: Epoch 97 / 100: avg data time: 5.54e-02, avg batch time: 0.4987, average train loss: 2.0883
[09/26 11:50:53 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1673, average loss: 2.1097
[09/26 11:50:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 71.00	
[09/26 11:50:53 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 11:51:00 visual_prompt]: Epoch 98 / 100: avg data time: 5.79e-02, avg batch time: 0.4998, average train loss: 2.0747
[09/26 11:51:01 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1673, average loss: 2.1051
[09/26 11:51:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 72.00	
[09/26 11:51:01 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 11:51:08 visual_prompt]: Epoch 99 / 100: avg data time: 5.35e-02, avg batch time: 0.4970, average train loss: 2.0711
[09/26 11:51:10 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1672, average loss: 2.1034
[09/26 11:51:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 72.50	
[09/26 11:51:10 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 11:51:17 visual_prompt]: Epoch 100 / 100: avg data time: 6.56e-02, avg batch time: 0.5091, average train loss: 2.0658
[09/26 11:51:18 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1670, average loss: 2.1028
[09/26 11:51:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 73.00	
[09/26 11:51:18 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 11:51:18 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 11:51:18 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 11:51:18 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 11:51:18 visual_prompt]: Training with config:
[09/26 11:51:18 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr25.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 11:51:18 visual_prompt]: Loading training data...
[09/26 11:51:18 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 11:51:19 visual_prompt]: Number of images: 800
[09/26 11:51:19 visual_prompt]: Number of classes: 9 / 9
[09/26 11:51:19 visual_prompt]: Loading validation data...
[09/26 11:51:19 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 11:51:20 visual_prompt]: Number of images: 200
[09/26 11:51:20 visual_prompt]: Number of classes: 9 / 9
[09/26 11:51:20 visual_prompt]: Constructing models...
[09/26 11:51:23 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 11:51:23 visual_prompt]: tuned percent:0.542
[09/26 11:51:23 visual_prompt]: Device used for model: 0
[09/26 11:51:23 visual_prompt]: Setting up Evaluator...
[09/26 11:51:23 visual_prompt]: Setting up Trainer...
[09/26 11:51:23 visual_prompt]: 	Setting up the optimizer...
[09/26 11:51:23 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 11:51:29 visual_prompt]: Epoch 1 / 100: avg data time: 5.41e-02, avg batch time: 0.4973, average train loss: 2.8776
[09/26 11:51:31 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1669, average loss: 2.9516
[09/26 11:51:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 11:51:31 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 11:51:31 visual_prompt]: Training 2 / 100 epoch, with learning rate 2.5
[09/26 11:51:38 visual_prompt]: Epoch 2 / 100: avg data time: 4.31e-02, avg batch time: 0.4889, average train loss: 12.8490
[09/26 11:51:39 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1670, average loss: 12.7696
[09/26 11:51:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 11:51:39 visual_prompt]: Training 3 / 100 epoch, with learning rate 5.0
[09/26 11:51:46 visual_prompt]: Epoch 3 / 100: avg data time: 4.24e-02, avg batch time: 0.4863, average train loss: 25.3288
[09/26 11:51:47 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1669, average loss: 27.8618
[09/26 11:51:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/26 11:51:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 7.5
[09/26 11:51:54 visual_prompt]: Epoch 4 / 100: avg data time: 4.94e-02, avg batch time: 0.4937, average train loss: 33.0370
[09/26 11:51:56 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1670, average loss: 42.5596
[09/26 11:51:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 11:51:56 visual_prompt]: Training 5 / 100 epoch, with learning rate 10.0
[09/26 11:52:02 visual_prompt]: Epoch 5 / 100: avg data time: 4.79e-02, avg batch time: 0.4927, average train loss: 30.8729
[09/26 11:52:04 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1673, average loss: 33.0711
[09/26 11:52:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.00	
[09/26 11:52:04 visual_prompt]: Training 6 / 100 epoch, with learning rate 12.5
[09/26 11:52:10 visual_prompt]: Epoch 6 / 100: avg data time: 4.68e-02, avg batch time: 0.4893, average train loss: 32.9122
[09/26 11:52:12 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1669, average loss: 24.3302
[09/26 11:52:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 11:52:12 visual_prompt]: Training 7 / 100 epoch, with learning rate 15.0
[09/26 11:52:19 visual_prompt]: Epoch 7 / 100: avg data time: 4.55e-02, avg batch time: 0.4911, average train loss: 38.0707
[09/26 11:52:20 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1671, average loss: 30.9931
[09/26 11:52:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.50	
[09/26 11:52:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 17.5
[09/26 11:52:27 visual_prompt]: Epoch 8 / 100: avg data time: 4.73e-02, avg batch time: 0.4929, average train loss: 42.3784
[09/26 11:52:29 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1674, average loss: 50.5700
[09/26 11:52:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 50.50	
[09/26 11:52:29 visual_prompt]: Training 9 / 100 epoch, with learning rate 20.0
[09/26 11:52:35 visual_prompt]: Epoch 9 / 100: avg data time: 4.91e-02, avg batch time: 0.4935, average train loss: 68.7608
[09/26 11:52:37 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1677, average loss: 62.7647
[09/26 11:52:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.50	
[09/26 11:52:37 visual_prompt]: Training 10 / 100 epoch, with learning rate 22.5
[09/26 11:52:43 visual_prompt]: Epoch 10 / 100: avg data time: 4.56e-02, avg batch time: 0.4904, average train loss: 43.9642
[09/26 11:52:45 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1672, average loss: 55.2181
[09/26 11:52:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 11:52:45 visual_prompt]: Training 11 / 100 epoch, with learning rate 25.0
[09/26 11:52:52 visual_prompt]: Epoch 11 / 100: avg data time: 4.93e-02, avg batch time: 0.4932, average train loss: 73.0356
[09/26 11:52:53 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1673, average loss: 74.4282
[09/26 11:52:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.50	
[09/26 11:52:53 visual_prompt]: Training 12 / 100 epoch, with learning rate 24.9923853377387
[09/26 11:53:00 visual_prompt]: Epoch 12 / 100: avg data time: 4.94e-02, avg batch time: 0.4936, average train loss: 76.8705
[09/26 11:53:01 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1676, average loss: 72.4903
[09/26 11:53:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 11:53:01 visual_prompt]: Training 13 / 100 epoch, with learning rate 24.969550628247802
[09/26 11:53:08 visual_prompt]: Epoch 13 / 100: avg data time: 4.44e-02, avg batch time: 0.4890, average train loss: 61.5841
[09/26 11:53:10 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1673, average loss: 62.7925
[09/26 11:53:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 48.50	
[09/26 11:53:10 visual_prompt]: Training 14 / 100 epoch, with learning rate 24.931523692103415
[09/26 11:53:16 visual_prompt]: Epoch 14 / 100: avg data time: 4.43e-02, avg batch time: 0.4890, average train loss: 73.9652
[09/26 11:53:18 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1677, average loss: 73.7222
[09/26 11:53:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 11:53:18 visual_prompt]: Training 15 / 100 epoch, with learning rate 24.87835085926963
[09/26 11:53:24 visual_prompt]: Epoch 15 / 100: avg data time: 4.59e-02, avg batch time: 0.4882, average train loss: 64.0774
[09/26 11:53:26 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1672, average loss: 67.7708
[09/26 11:53:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/26 11:53:26 visual_prompt]: Training 16 / 100 epoch, with learning rate 24.8100969126526
[09/26 11:53:33 visual_prompt]: Epoch 16 / 100: avg data time: 5.73e-02, avg batch time: 0.5001, average train loss: 65.0312
[09/26 11:53:34 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1675, average loss: 62.0727
[09/26 11:53:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 11:53:34 visual_prompt]: Training 17 / 100 epoch, with learning rate 24.72684500917257
[09/26 11:53:41 visual_prompt]: Epoch 17 / 100: avg data time: 4.06e-02, avg batch time: 0.4850, average train loss: 69.1830
[09/26 11:53:43 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1680, average loss: 68.3803
[09/26 11:53:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 11:53:43 visual_prompt]: Training 18 / 100 epoch, with learning rate 24.628696578449954
[09/26 11:53:49 visual_prompt]: Epoch 18 / 100: avg data time: 4.46e-02, avg batch time: 0.4877, average train loss: 86.5332
[09/26 11:53:51 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1680, average loss: 83.1063
[09/26 11:53:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 11:53:51 visual_prompt]: Training 19 / 100 epoch, with learning rate 24.515771199228986
[09/26 11:53:57 visual_prompt]: Epoch 19 / 100: avg data time: 4.39e-02, avg batch time: 0.4914, average train loss: 60.3880
[09/26 11:53:59 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1674, average loss: 37.4074
[09/26 11:53:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 11:53:59 visual_prompt]: Training 20 / 100 epoch, with learning rate 24.38820645368942
[09/26 11:54:06 visual_prompt]: Epoch 20 / 100: avg data time: 5.34e-02, avg batch time: 0.4973, average train loss: 32.1414
[09/26 11:54:07 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1673, average loss: 32.3914
[09/26 11:54:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 11:54:07 visual_prompt]: Training 21 / 100 epoch, with learning rate 24.246157759823856
[09/26 11:54:14 visual_prompt]: Epoch 21 / 100: avg data time: 5.37e-02, avg batch time: 0.4961, average train loss: 47.4472
[09/26 11:54:16 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1673, average loss: 66.5456
[09/26 11:54:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.00	
[09/26 11:54:16 visual_prompt]: Training 22 / 100 epoch, with learning rate 24.08979818208484
[09/26 11:54:22 visual_prompt]: Epoch 22 / 100: avg data time: 4.88e-02, avg batch time: 0.4920, average train loss: 60.1616
[09/26 11:54:24 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1672, average loss: 30.7451
[09/26 11:54:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 11:54:24 visual_prompt]: Training 23 / 100 epoch, with learning rate 23.91931822053251
[09/26 11:54:31 visual_prompt]: Epoch 23 / 100: avg data time: 5.57e-02, avg batch time: 0.4986, average train loss: 42.2432
[09/26 11:54:32 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1673, average loss: 28.9983
[09/26 11:54:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/26 11:54:32 visual_prompt]: Training 24 / 100 epoch, with learning rate 23.73492557873959
[09/26 11:54:39 visual_prompt]: Epoch 24 / 100: avg data time: 5.41e-02, avg batch time: 0.4968, average train loss: 49.6275
[09/26 11:54:41 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1675, average loss: 42.2327
[09/26 11:54:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 57.00	
[09/26 11:54:41 visual_prompt]: Best epoch 24: best metric: 0.145
[09/26 11:54:41 visual_prompt]: Training 25 / 100 epoch, with learning rate 23.536844910736587
[09/26 11:54:47 visual_prompt]: Epoch 25 / 100: avg data time: 4.40e-02, avg batch time: 0.4897, average train loss: 49.3446
[09/26 11:54:49 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1672, average loss: 52.8394
[09/26 11:54:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.00	
[09/26 11:54:49 visual_prompt]: Training 26 / 100 epoch, with learning rate 23.325317547305485
[09/26 11:54:56 visual_prompt]: Epoch 26 / 100: avg data time: 4.75e-02, avg batch time: 0.4912, average train loss: 48.8298
[09/26 11:54:57 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1674, average loss: 32.5120
[09/26 11:54:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 48.50	
[09/26 11:54:57 visual_prompt]: Training 27 / 100 epoch, with learning rate 23.100601201955325
[09/26 11:55:04 visual_prompt]: Epoch 27 / 100: avg data time: 4.97e-02, avg batch time: 0.4930, average train loss: 48.0424
[09/26 11:55:05 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1675, average loss: 55.7288
[09/26 11:55:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 11:55:05 visual_prompt]: Training 28 / 100 epoch, with learning rate 22.86296965693802
[09/26 11:55:12 visual_prompt]: Epoch 28 / 100: avg data time: 6.06e-02, avg batch time: 0.5037, average train loss: 48.9194
[09/26 11:55:14 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1675, average loss: 35.6888
[09/26 11:55:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 11:55:14 visual_prompt]: Training 29 / 100 epoch, with learning rate 22.612712429686844
[09/26 11:55:21 visual_prompt]: Epoch 29 / 100: avg data time: 5.78e-02, avg batch time: 0.5013, average train loss: 42.5949
[09/26 11:55:22 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1674, average loss: 47.5118
[09/26 11:55:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 53.00	
[09/26 11:55:22 visual_prompt]: Training 30 / 100 epoch, with learning rate 22.35013442008402
[09/26 11:55:29 visual_prompt]: Epoch 30 / 100: avg data time: 5.73e-02, avg batch time: 0.5003, average train loss: 52.4532
[09/26 11:55:30 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1674, average loss: 65.7624
[09/26 11:55:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 11:55:30 visual_prompt]: Training 31 / 100 epoch, with learning rate 22.075555538987224
[09/26 11:55:37 visual_prompt]: Epoch 31 / 100: avg data time: 5.26e-02, avg batch time: 0.4949, average train loss: 52.9992
[09/26 11:55:39 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1670, average loss: 39.3106
[09/26 11:55:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 11:55:39 visual_prompt]: Training 32 / 100 epoch, with learning rate 21.78931031846743
[09/26 11:55:46 visual_prompt]: Epoch 32 / 100: avg data time: 5.59e-02, avg batch time: 0.4997, average train loss: 36.5142
[09/26 11:55:47 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1672, average loss: 37.1421
[09/26 11:55:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.50	
[09/26 11:55:47 visual_prompt]: Training 33 / 100 epoch, with learning rate 21.49174750423314
[09/26 11:55:54 visual_prompt]: Epoch 33 / 100: avg data time: 6.17e-02, avg batch time: 0.5054, average train loss: 34.5116
[09/26 11:55:56 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1671, average loss: 30.7012
[09/26 11:55:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 11:55:56 visual_prompt]: Training 34 / 100 epoch, with learning rate 21.183229630737465
[09/26 11:56:02 visual_prompt]: Epoch 34 / 100: avg data time: 6.24e-02, avg batch time: 0.5075, average train loss: 41.6676
[09/26 11:56:04 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1676, average loss: 40.8471
[09/26 11:56:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 11:56:04 visual_prompt]: Training 35 / 100 epoch, with learning rate 20.864132579485727
[09/26 11:56:11 visual_prompt]: Epoch 35 / 100: avg data time: 4.78e-02, avg batch time: 0.4940, average train loss: 45.6705
[09/26 11:56:12 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1673, average loss: 60.1167
[09/26 11:56:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 11:56:12 visual_prompt]: Training 36 / 100 epoch, with learning rate 20.53484512108174
[09/26 11:56:19 visual_prompt]: Epoch 36 / 100: avg data time: 5.77e-02, avg batch time: 0.5027, average train loss: 50.3580
[09/26 11:56:21 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1674, average loss: 28.0013
[09/26 11:56:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 11:56:21 visual_prompt]: Training 37 / 100 epoch, with learning rate 20.195768441570728
[09/26 11:56:27 visual_prompt]: Epoch 37 / 100: avg data time: 4.66e-02, avg batch time: 0.4930, average train loss: 47.3159
[09/26 11:56:29 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1674, average loss: 30.0087
[09/26 11:56:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 11:56:29 visual_prompt]: Training 38 / 100 epoch, with learning rate 19.847315653655915
[09/26 11:56:36 visual_prompt]: Epoch 38 / 100: avg data time: 5.74e-02, avg batch time: 0.5016, average train loss: 36.1524
[09/26 11:56:37 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1674, average loss: 33.3338
[09/26 11:56:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.50	
[09/26 11:56:37 visual_prompt]: Training 39 / 100 epoch, with learning rate 19.489911293384335
[09/26 11:56:44 visual_prompt]: Epoch 39 / 100: avg data time: 4.89e-02, avg batch time: 0.4932, average train loss: 37.8333
[09/26 11:56:46 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1674, average loss: 36.3534
[09/26 11:56:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 11:56:46 visual_prompt]: Training 40 / 100 epoch, with learning rate 19.12399080291506
[09/26 11:56:52 visual_prompt]: Epoch 40 / 100: avg data time: 5.67e-02, avg batch time: 0.5011, average train loss: 30.4104
[09/26 11:56:54 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1673, average loss: 33.4982
[09/26 11:56:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.00	
[09/26 11:56:54 visual_prompt]: Training 41 / 100 epoch, with learning rate 18.75
[09/26 11:57:01 visual_prompt]: Epoch 41 / 100: avg data time: 6.22e-02, avg batch time: 0.5055, average train loss: 28.5358
[09/26 11:57:02 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1671, average loss: 39.1511
[09/26 11:57:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 50.50	
[09/26 11:57:02 visual_prompt]: Training 42 / 100 epoch, with learning rate 18.368394534823633
[09/26 11:57:09 visual_prompt]: Epoch 42 / 100: avg data time: 4.46e-02, avg batch time: 0.4888, average train loss: 32.9750
[09/26 11:57:11 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1675, average loss: 27.1446
[09/26 11:57:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 11:57:11 visual_prompt]: Training 43 / 100 epoch, with learning rate 17.979639334863467
[09/26 11:57:17 visual_prompt]: Epoch 43 / 100: avg data time: 4.57e-02, avg batch time: 0.4894, average train loss: 33.3861
[09/26 11:57:19 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1675, average loss: 30.5909
[09/26 11:57:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 11:57:19 visual_prompt]: Training 44 / 100 epoch, with learning rate 17.584208038447503
[09/26 11:57:25 visual_prompt]: Epoch 44 / 100: avg data time: 4.12e-02, avg batch time: 0.4840, average train loss: 28.0855
[09/26 11:57:27 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1675, average loss: 19.3472
[09/26 11:57:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 11:57:27 visual_prompt]: Training 45 / 100 epoch, with learning rate 17.182582417698903
[09/26 11:57:34 visual_prompt]: Epoch 45 / 100: avg data time: 5.40e-02, avg batch time: 0.4989, average train loss: 21.5006
[09/26 11:57:35 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1673, average loss: 26.4453
[09/26 11:57:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 11:57:35 visual_prompt]: Training 46 / 100 epoch, with learning rate 16.77525179157086
[09/26 11:57:42 visual_prompt]: Epoch 46 / 100: avg data time: 4.45e-02, avg batch time: 0.4889, average train loss: 24.2075
[09/26 11:57:43 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1674, average loss: 19.7937
[09/26 11:57:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 11:57:43 visual_prompt]: Training 47 / 100 epoch, with learning rate 16.362712429686844
[09/26 11:57:50 visual_prompt]: Epoch 47 / 100: avg data time: 5.87e-02, avg batch time: 0.5023, average train loss: 25.0767
[09/26 11:57:52 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1672, average loss: 27.0495
[09/26 11:57:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 11:57:52 visual_prompt]: Training 48 / 100 epoch, with learning rate 15.94546694771249
[09/26 11:57:59 visual_prompt]: Epoch 48 / 100: avg data time: 5.44e-02, avg batch time: 0.5002, average train loss: 32.4227
[09/26 11:58:00 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1673, average loss: 41.9802
[09/26 11:58:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 11:58:00 visual_prompt]: Training 49 / 100 epoch, with learning rate 15.524023694995845
[09/26 11:58:07 visual_prompt]: Epoch 49 / 100: avg data time: 4.56e-02, avg batch time: 0.4909, average train loss: 38.9706
[09/26 11:58:08 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1671, average loss: 38.1072
[09/26 11:58:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.00	
[09/26 11:58:08 visual_prompt]: Training 50 / 100 epoch, with learning rate 15.09889613522199
[09/26 11:58:15 visual_prompt]: Epoch 50 / 100: avg data time: 6.08e-02, avg batch time: 0.5051, average train loss: 39.8563
[09/26 11:58:17 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1674, average loss: 25.0081
[09/26 11:58:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 48.50	
[09/26 11:58:17 visual_prompt]: Training 51 / 100 epoch, with learning rate 14.670602220836631
[09/26 11:58:24 visual_prompt]: Epoch 51 / 100: avg data time: 5.74e-02, avg batch time: 0.5013, average train loss: 22.2218
[09/26 11:58:25 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1674, average loss: 17.5453
[09/26 11:58:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 11:58:25 visual_prompt]: Training 52 / 100 epoch, with learning rate 14.239663762000818
[09/26 11:58:32 visual_prompt]: Epoch 52 / 100: avg data time: 6.04e-02, avg batch time: 0.5045, average train loss: 21.0024
[09/26 11:58:34 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1672, average loss: 22.0399
[09/26 11:58:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 48.50	
[09/26 11:58:34 visual_prompt]: Training 53 / 100 epoch, with learning rate 13.80660579084567
[09/26 11:58:40 visual_prompt]: Epoch 53 / 100: avg data time: 4.73e-02, avg batch time: 0.4927, average train loss: 21.0221
[09/26 11:58:42 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1672, average loss: 18.7708
[09/26 11:58:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.50	top5: 50.50	
[09/26 11:58:42 visual_prompt]: Training 54 / 100 epoch, with learning rate 13.371955921801565
[09/26 11:58:49 visual_prompt]: Epoch 54 / 100: avg data time: 4.68e-02, avg batch time: 0.4920, average train loss: 18.2105
[09/26 11:58:50 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1674, average loss: 22.2683
[09/26 11:58:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 11:58:50 visual_prompt]: Training 55 / 100 epoch, with learning rate 12.936243708781264
[09/26 11:58:57 visual_prompt]: Epoch 55 / 100: avg data time: 4.45e-02, avg batch time: 0.4903, average train loss: 13.7243
[09/26 11:58:58 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1674, average loss: 7.6626
[09/26 11:58:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 59.50	
[09/26 11:58:58 visual_prompt]: Training 56 / 100 epoch, with learning rate 12.5
[09/26 11:59:05 visual_prompt]: Epoch 56 / 100: avg data time: 4.67e-02, avg batch time: 0.4906, average train loss: 12.0230
[09/26 11:59:07 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1676, average loss: 14.9421
[09/26 11:59:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 51.50	
[09/26 11:59:07 visual_prompt]: Training 57 / 100 epoch, with learning rate 12.063756291218741
[09/26 11:59:13 visual_prompt]: Epoch 57 / 100: avg data time: 4.53e-02, avg batch time: 0.4930, average train loss: 10.7839
[09/26 11:59:15 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1673, average loss: 10.4489
[09/26 11:59:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 11:59:15 visual_prompt]: Training 58 / 100 epoch, with learning rate 11.628044078198434
[09/26 11:59:22 visual_prompt]: Epoch 58 / 100: avg data time: 5.38e-02, avg batch time: 0.4981, average train loss: 9.2876
[09/26 11:59:23 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1678, average loss: 9.6241
[09/26 11:59:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.00	
[09/26 11:59:23 visual_prompt]: Training 59 / 100 epoch, with learning rate 11.193394209154334
[09/26 11:59:30 visual_prompt]: Epoch 59 / 100: avg data time: 5.58e-02, avg batch time: 0.4994, average train loss: 8.7988
[09/26 11:59:32 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1685, average loss: 8.7671
[09/26 11:59:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 63.00	
[09/26 11:59:32 visual_prompt]: Training 60 / 100 epoch, with learning rate 10.760336237999185
[09/26 11:59:39 visual_prompt]: Epoch 60 / 100: avg data time: 6.27e-02, avg batch time: 0.5075, average train loss: 9.1242
[09/26 11:59:40 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1676, average loss: 4.2204
[09/26 11:59:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 69.50	
[09/26 11:59:40 visual_prompt]: Training 61 / 100 epoch, with learning rate 10.32939777916337
[09/26 11:59:47 visual_prompt]: Epoch 61 / 100: avg data time: 6.35e-02, avg batch time: 0.5070, average train loss: 7.7598
[09/26 11:59:49 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1676, average loss: 10.2146
[09/26 11:59:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 11:59:49 visual_prompt]: Training 62 / 100 epoch, with learning rate 9.90110386477801
[09/26 11:59:55 visual_prompt]: Epoch 62 / 100: avg data time: 5.84e-02, avg batch time: 0.5019, average train loss: 6.0009
[09/26 11:59:57 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1677, average loss: 5.0404
[09/26 11:59:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 56.00	
[09/26 11:59:57 visual_prompt]: Training 63 / 100 epoch, with learning rate 9.475976305004155
[09/26 12:00:04 visual_prompt]: Epoch 63 / 100: avg data time: 4.84e-02, avg batch time: 0.4928, average train loss: 3.9142
[09/26 12:00:05 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1676, average loss: 4.3242
[09/26 12:00:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 59.00	
[09/26 12:00:05 visual_prompt]: Best epoch 63: best metric: 0.150
[09/26 12:00:05 visual_prompt]: Training 64 / 100 epoch, with learning rate 9.05453305228751
[09/26 12:00:12 visual_prompt]: Epoch 64 / 100: avg data time: 4.58e-02, avg batch time: 0.4908, average train loss: 3.4361
[09/26 12:00:14 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1675, average loss: 2.9131
[09/26 12:00:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 63.00	
[09/26 12:00:14 visual_prompt]: Best epoch 64: best metric: 0.215
[09/26 12:00:14 visual_prompt]: Training 65 / 100 epoch, with learning rate 8.637287570313159
[09/26 12:00:20 visual_prompt]: Epoch 65 / 100: avg data time: 5.66e-02, avg batch time: 0.5000, average train loss: 3.3899
[09/26 12:00:22 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1676, average loss: 4.1693
[09/26 12:00:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 60.00	
[09/26 12:00:22 visual_prompt]: Training 66 / 100 epoch, with learning rate 8.224748208429142
[09/26 12:00:29 visual_prompt]: Epoch 66 / 100: avg data time: 5.73e-02, avg batch time: 0.5008, average train loss: 3.2626
[09/26 12:00:30 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1676, average loss: 3.8341
[09/26 12:00:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 54.50	
[09/26 12:00:30 visual_prompt]: Training 67 / 100 epoch, with learning rate 7.817417582301099
[09/26 12:00:37 visual_prompt]: Epoch 67 / 100: avg data time: 4.83e-02, avg batch time: 0.4920, average train loss: 2.7041
[09/26 12:00:39 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1677, average loss: 2.9388
[09/26 12:00:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 67.50	
[09/26 12:00:39 visual_prompt]: Training 68 / 100 epoch, with learning rate 7.415791961552499
[09/26 12:00:45 visual_prompt]: Epoch 68 / 100: avg data time: 4.74e-02, avg batch time: 0.4924, average train loss: 2.5644
[09/26 12:00:47 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1676, average loss: 3.0472
[09/26 12:00:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 65.00	
[09/26 12:00:47 visual_prompt]: Training 69 / 100 epoch, with learning rate 7.020360665136531
[09/26 12:00:54 visual_prompt]: Epoch 69 / 100: avg data time: 6.03e-02, avg batch time: 0.5049, average train loss: 2.5714
[09/26 12:00:55 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1675, average loss: 2.8926
[09/26 12:00:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 72.50	
[09/26 12:00:55 visual_prompt]: Training 70 / 100 epoch, with learning rate 6.631605465176368
[09/26 12:01:02 visual_prompt]: Epoch 70 / 100: avg data time: 5.21e-02, avg batch time: 0.4954, average train loss: 2.3449
[09/26 12:01:04 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1676, average loss: 2.2829
[09/26 12:01:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 69.00	
[09/26 12:01:04 visual_prompt]: Training 71 / 100 epoch, with learning rate 6.250000000000003
[09/26 12:01:11 visual_prompt]: Epoch 71 / 100: avg data time: 6.01e-02, avg batch time: 0.5036, average train loss: 2.3559
[09/26 12:01:12 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1674, average loss: 2.1512
[09/26 12:01:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 75.00	
[09/26 12:01:12 visual_prompt]: Training 72 / 100 epoch, with learning rate 5.87600919708494
[09/26 12:01:19 visual_prompt]: Epoch 72 / 100: avg data time: 6.07e-02, avg batch time: 0.5045, average train loss: 2.2071
[09/26 12:01:20 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1674, average loss: 2.2459
[09/26 12:01:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 69.00	
[09/26 12:01:20 visual_prompt]: Best epoch 72: best metric: 0.225
[09/26 12:01:20 visual_prompt]: Training 73 / 100 epoch, with learning rate 5.510088706615666
[09/26 12:01:27 visual_prompt]: Epoch 73 / 100: avg data time: 5.83e-02, avg batch time: 0.5016, average train loss: 2.0616
[09/26 12:01:29 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1675, average loss: 2.2388
[09/26 12:01:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 78.50	
[09/26 12:01:29 visual_prompt]: Training 74 / 100 epoch, with learning rate 5.1526843463440875
[09/26 12:01:36 visual_prompt]: Epoch 74 / 100: avg data time: 6.09e-02, avg batch time: 0.5057, average train loss: 2.1086
[09/26 12:01:37 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1675, average loss: 2.2754
[09/26 12:01:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 72.50	
[09/26 12:01:37 visual_prompt]: Training 75 / 100 epoch, with learning rate 4.804231558429271
[09/26 12:01:44 visual_prompt]: Epoch 75 / 100: avg data time: 6.28e-02, avg batch time: 0.5070, average train loss: 2.2368
[09/26 12:01:46 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1677, average loss: 2.4783
[09/26 12:01:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 65.50	
[09/26 12:01:46 visual_prompt]: Training 76 / 100 epoch, with learning rate 4.465154878918258
[09/26 12:01:53 visual_prompt]: Epoch 76 / 100: avg data time: 6.30e-02, avg batch time: 0.5066, average train loss: 2.1339
[09/26 12:01:54 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1675, average loss: 2.3889
[09/26 12:01:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 72.00	
[09/26 12:01:54 visual_prompt]: Training 77 / 100 epoch, with learning rate 4.135867420514276
[09/26 12:02:01 visual_prompt]: Epoch 77 / 100: avg data time: 5.00e-02, avg batch time: 0.4945, average train loss: 2.0915
[09/26 12:02:02 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1675, average loss: 2.1903
[09/26 12:02:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 76.00	
[09/26 12:02:02 visual_prompt]: Training 78 / 100 epoch, with learning rate 3.816770369262533
[09/26 12:02:09 visual_prompt]: Epoch 78 / 100: avg data time: 6.07e-02, avg batch time: 0.5040, average train loss: 2.1252
[09/26 12:02:11 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1674, average loss: 2.0923
[09/26 12:02:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 80.50	
[09/26 12:02:11 visual_prompt]: Training 79 / 100 epoch, with learning rate 3.508252495766863
[09/26 12:02:18 visual_prompt]: Epoch 79 / 100: avg data time: 5.83e-02, avg batch time: 0.5017, average train loss: 2.2278
[09/26 12:02:19 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1674, average loss: 2.3527
[09/26 12:02:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 73.00	
[09/26 12:02:19 visual_prompt]: Training 80 / 100 epoch, with learning rate 3.2106896815325707
[09/26 12:02:26 visual_prompt]: Epoch 80 / 100: avg data time: 5.81e-02, avg batch time: 0.5005, average train loss: 2.0477
[09/26 12:02:28 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1676, average loss: 2.2415
[09/26 12:02:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 74.50	
[09/26 12:02:28 visual_prompt]: Training 81 / 100 epoch, with learning rate 2.924444461012776
[09/26 12:02:34 visual_prompt]: Epoch 81 / 100: avg data time: 5.74e-02, avg batch time: 0.5003, average train loss: 2.0289
[09/26 12:02:36 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1675, average loss: 2.2175
[09/26 12:02:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 72.50	
[09/26 12:02:36 visual_prompt]: Training 82 / 100 epoch, with learning rate 2.6498655799159763
[09/26 12:02:43 visual_prompt]: Epoch 82 / 100: avg data time: 6.13e-02, avg batch time: 0.5050, average train loss: 2.0044
[09/26 12:02:45 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1672, average loss: 2.2476
[09/26 12:02:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 72.00	
[09/26 12:02:45 visual_prompt]: Training 83 / 100 epoch, with learning rate 2.387287570313158
[09/26 12:02:51 visual_prompt]: Epoch 83 / 100: avg data time: 5.68e-02, avg batch time: 0.4997, average train loss: 2.0297
[09/26 12:02:53 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1675, average loss: 2.1479
[09/26 12:02:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 76.00	
[09/26 12:02:53 visual_prompt]: Training 84 / 100 epoch, with learning rate 2.13703034306198
[09/26 12:03:00 visual_prompt]: Epoch 84 / 100: avg data time: 5.86e-02, avg batch time: 0.5026, average train loss: 1.9843
[09/26 12:03:01 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1674, average loss: 2.1009
[09/26 12:03:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 78.00	
[09/26 12:03:01 visual_prompt]: Training 85 / 100 epoch, with learning rate 1.8993987980446756
[09/26 12:03:08 visual_prompt]: Epoch 85 / 100: avg data time: 5.52e-02, avg batch time: 0.4986, average train loss: 1.9127
[09/26 12:03:10 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1675, average loss: 2.0403
[09/26 12:03:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 74.50	
[09/26 12:03:10 visual_prompt]: Training 86 / 100 epoch, with learning rate 1.674682452694516
[09/26 12:03:16 visual_prompt]: Epoch 86 / 100: avg data time: 6.12e-02, avg batch time: 0.5050, average train loss: 1.8976
[09/26 12:03:18 visual_prompt]: Inference (val):avg data time: 1.78e-05, avg batch time: 0.1673, average loss: 2.0808
[09/26 12:03:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 72.00	
[09/26 12:03:18 visual_prompt]: Best epoch 86: best metric: 0.230
[09/26 12:03:18 visual_prompt]: Training 87 / 100 epoch, with learning rate 1.4631550892634126
[09/26 12:03:25 visual_prompt]: Epoch 87 / 100: avg data time: 5.72e-02, avg batch time: 0.5013, average train loss: 1.8639
[09/26 12:03:26 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1674, average loss: 2.0312
[09/26 12:03:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 75.00	
[09/26 12:03:26 visual_prompt]: Training 88 / 100 epoch, with learning rate 1.2650744212604148
[09/26 12:03:33 visual_prompt]: Epoch 88 / 100: avg data time: 5.74e-02, avg batch time: 0.5022, average train loss: 1.8679
[09/26 12:03:35 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1678, average loss: 2.0182
[09/26 12:03:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 74.00	
[09/26 12:03:35 visual_prompt]: Training 89 / 100 epoch, with learning rate 1.0806817794674877
[09/26 12:03:42 visual_prompt]: Epoch 89 / 100: avg data time: 5.74e-02, avg batch time: 0.5012, average train loss: 1.8755
[09/26 12:03:43 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1675, average loss: 2.2177
[09/26 12:03:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 72.00	
[09/26 12:03:43 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.9102018179151585
[09/26 12:03:50 visual_prompt]: Epoch 90 / 100: avg data time: 5.71e-02, avg batch time: 0.5009, average train loss: 1.8577
[09/26 12:03:52 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1676, average loss: 2.0450
[09/26 12:03:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 76.50	
[09/26 12:03:52 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.753842240176146
[09/26 12:03:59 visual_prompt]: Epoch 91 / 100: avg data time: 6.09e-02, avg batch time: 0.5030, average train loss: 1.8297
[09/26 12:04:00 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1674, average loss: 2.0028
[09/26 12:04:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 79.00	
[09/26 12:04:00 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.6117935463105808
[09/26 12:04:07 visual_prompt]: Epoch 92 / 100: avg data time: 4.73e-02, avg batch time: 0.4909, average train loss: 1.8321
[09/26 12:04:08 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1673, average loss: 2.1116
[09/26 12:04:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 75.50	
[09/26 12:04:08 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.4842288007710138
[09/26 12:04:15 visual_prompt]: Epoch 93 / 100: avg data time: 6.02e-02, avg batch time: 0.5041, average train loss: 1.7916
[09/26 12:04:17 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1674, average loss: 2.0048
[09/26 12:04:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 79.50	
[09/26 12:04:17 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.3713034215500441
[09/26 12:04:23 visual_prompt]: Epoch 94 / 100: avg data time: 4.66e-02, avg batch time: 0.4907, average train loss: 1.8047
[09/26 12:04:25 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1674, average loss: 2.0572
[09/26 12:04:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 78.00	
[09/26 12:04:25 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.2731549908274289
[09/26 12:04:32 visual_prompt]: Epoch 95 / 100: avg data time: 4.64e-02, avg batch time: 0.4898, average train loss: 1.8020
[09/26 12:04:33 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1673, average loss: 2.0598
[09/26 12:04:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 74.00	
[09/26 12:04:33 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.18990308734739975
[09/26 12:04:40 visual_prompt]: Epoch 96 / 100: avg data time: 4.82e-02, avg batch time: 0.4946, average train loss: 1.7909
[09/26 12:04:41 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1675, average loss: 2.0116
[09/26 12:04:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 78.50	
[09/26 12:04:41 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.12164914073037048
[09/26 12:04:48 visual_prompt]: Epoch 97 / 100: avg data time: 4.43e-02, avg batch time: 0.4877, average train loss: 1.8042
[09/26 12:04:49 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1672, average loss: 2.0377
[09/26 12:04:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 78.50	
[09/26 12:04:49 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.06847630789658388
[09/26 12:04:56 visual_prompt]: Epoch 98 / 100: avg data time: 4.56e-02, avg batch time: 0.4906, average train loss: 1.7806
[09/26 12:04:58 visual_prompt]: Inference (val):avg data time: 1.80e-05, avg batch time: 0.1670, average loss: 2.0382
[09/26 12:04:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 78.50	
[09/26 12:04:58 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.03044937175219753
[09/26 12:05:05 visual_prompt]: Epoch 99 / 100: avg data time: 5.90e-02, avg batch time: 0.5020, average train loss: 1.8088
[09/26 12:05:06 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1672, average loss: 2.0402
[09/26 12:05:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 78.50	
[09/26 12:05:06 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0076146622613029735
[09/26 12:05:13 visual_prompt]: Epoch 100 / 100: avg data time: 6.08e-02, avg batch time: 0.5043, average train loss: 1.7940
[09/26 12:05:15 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1671, average loss: 2.0401
[09/26 12:05:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 78.50	
[09/26 12:05:15 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:05:15 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:05:15 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:05:15 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:05:15 visual_prompt]: Training with config:
[09/26 12:05:15 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:05:15 visual_prompt]: Loading training data...
[09/26 12:05:15 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 12:05:16 visual_prompt]: Number of images: 800
[09/26 12:05:16 visual_prompt]: Number of classes: 9 / 9
[09/26 12:05:16 visual_prompt]: Loading validation data...
[09/26 12:05:16 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 12:05:16 visual_prompt]: Number of images: 200
[09/26 12:05:16 visual_prompt]: Number of classes: 9 / 9
[09/26 12:05:16 visual_prompt]: Constructing models...
[09/26 12:05:18 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 12:05:18 visual_prompt]: tuned percent:0.542
[09/26 12:05:19 visual_prompt]: Device used for model: 0
[09/26 12:05:19 visual_prompt]: Setting up Evaluator...
[09/26 12:05:19 visual_prompt]: Setting up Trainer...
[09/26 12:05:19 visual_prompt]: 	Setting up the optimizer...
[09/26 12:05:19 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:05:25 visual_prompt]: Epoch 1 / 100: avg data time: 5.96e-02, avg batch time: 0.5028, average train loss: 2.8586
[09/26 12:05:27 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1667, average loss: 2.9516
[09/26 12:05:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 12:05:27 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 12:05:27 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 12:05:34 visual_prompt]: Epoch 2 / 100: avg data time: 4.40e-02, avg batch time: 0.4889, average train loss: 6.6528
[09/26 12:05:35 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1667, average loss: 3.3203
[09/26 12:05:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 12:05:35 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 12:05:42 visual_prompt]: Epoch 3 / 100: avg data time: 4.47e-02, avg batch time: 0.4868, average train loss: 4.7321
[09/26 12:05:43 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1665, average loss: 4.0485
[09/26 12:05:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 12:05:43 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 12:05:50 visual_prompt]: Epoch 4 / 100: avg data time: 5.47e-02, avg batch time: 0.4980, average train loss: 5.6099
[09/26 12:05:52 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1671, average loss: 8.1226
[09/26 12:05:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/26 12:05:52 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 12:05:59 visual_prompt]: Epoch 5 / 100: avg data time: 6.32e-02, avg batch time: 0.5074, average train loss: 14.0542
[09/26 12:06:00 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1672, average loss: 14.1320
[09/26 12:06:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 12:06:00 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 12:06:07 visual_prompt]: Epoch 6 / 100: avg data time: 5.42e-02, avg batch time: 0.4972, average train loss: 18.1232
[09/26 12:06:09 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1671, average loss: 14.3564
[09/26 12:06:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 61.00	
[09/26 12:06:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 12:06:15 visual_prompt]: Epoch 7 / 100: avg data time: 4.39e-02, avg batch time: 0.4881, average train loss: 26.9806
[09/26 12:06:17 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1668, average loss: 25.9770
[09/26 12:06:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 53.50	
[09/26 12:06:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 12:06:24 visual_prompt]: Epoch 8 / 100: avg data time: 5.16e-02, avg batch time: 0.4949, average train loss: 29.8849
[09/26 12:06:25 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1675, average loss: 30.8123
[09/26 12:06:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 12:06:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 12:06:32 visual_prompt]: Epoch 9 / 100: avg data time: 4.63e-02, avg batch time: 0.4903, average train loss: 33.7911
[09/26 12:06:34 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1676, average loss: 36.9370
[09/26 12:06:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 12:06:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 12:06:40 visual_prompt]: Epoch 10 / 100: avg data time: 5.94e-02, avg batch time: 0.5023, average train loss: 43.7811
[09/26 12:06:42 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1669, average loss: 27.8792
[09/26 12:06:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 12:06:42 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 12:06:49 visual_prompt]: Epoch 11 / 100: avg data time: 6.15e-02, avg batch time: 0.5043, average train loss: 41.2928
[09/26 12:06:51 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1669, average loss: 49.8987
[09/26 12:06:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 12:06:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 12:06:57 visual_prompt]: Epoch 12 / 100: avg data time: 5.71e-02, avg batch time: 0.4995, average train loss: 41.4459
[09/26 12:06:59 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1674, average loss: 46.5499
[09/26 12:06:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 12:06:59 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 12:07:06 visual_prompt]: Epoch 13 / 100: avg data time: 5.97e-02, avg batch time: 0.5037, average train loss: 50.1553
[09/26 12:07:07 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1675, average loss: 19.4912
[09/26 12:07:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 12:07:07 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 12:07:14 visual_prompt]: Epoch 14 / 100: avg data time: 5.27e-02, avg batch time: 0.4966, average train loss: 41.3424
[09/26 12:07:16 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1671, average loss: 49.9624
[09/26 12:07:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 12:07:16 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 12:07:22 visual_prompt]: Epoch 15 / 100: avg data time: 5.69e-02, avg batch time: 0.5015, average train loss: 38.9130
[09/26 12:07:24 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1673, average loss: 50.5996
[09/26 12:07:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 12:07:24 visual_prompt]: Best epoch 15: best metric: 0.145
[09/26 12:07:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 12:07:31 visual_prompt]: Epoch 16 / 100: avg data time: 5.76e-02, avg batch time: 0.5007, average train loss: 48.8685
[09/26 12:07:32 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1674, average loss: 32.2852
[09/26 12:07:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 12:07:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 12:07:39 visual_prompt]: Epoch 17 / 100: avg data time: 5.55e-02, avg batch time: 0.4982, average train loss: 50.4453
[09/26 12:07:41 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1672, average loss: 25.2797
[09/26 12:07:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 12:07:41 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 12:07:48 visual_prompt]: Epoch 18 / 100: avg data time: 5.80e-02, avg batch time: 0.5002, average train loss: 34.4699
[09/26 12:07:49 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1674, average loss: 34.4925
[09/26 12:07:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 12:07:49 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 12:07:56 visual_prompt]: Epoch 19 / 100: avg data time: 5.77e-02, avg batch time: 0.5012, average train loss: 52.6803
[09/26 12:07:58 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1673, average loss: 38.9547
[09/26 12:07:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 56.00	
[09/26 12:07:58 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 12:08:04 visual_prompt]: Epoch 20 / 100: avg data time: 4.54e-02, avg batch time: 0.4894, average train loss: 48.6223
[09/26 12:08:06 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1678, average loss: 58.4164
[09/26 12:08:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 12:08:06 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 12:08:13 visual_prompt]: Epoch 21 / 100: avg data time: 6.00e-02, avg batch time: 0.5037, average train loss: 42.7973
[09/26 12:08:14 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1672, average loss: 43.6746
[09/26 12:08:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 12:08:14 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 12:08:21 visual_prompt]: Epoch 22 / 100: avg data time: 5.65e-02, avg batch time: 0.5006, average train loss: 39.9490
[09/26 12:08:23 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1673, average loss: 34.5279
[09/26 12:08:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 12:08:23 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 12:08:29 visual_prompt]: Epoch 23 / 100: avg data time: 5.63e-02, avg batch time: 0.4988, average train loss: 32.1925
[09/26 12:08:31 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1671, average loss: 55.3799
[09/26 12:08:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 12:08:31 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 12:08:38 visual_prompt]: Epoch 24 / 100: avg data time: 5.58e-02, avg batch time: 0.5011, average train loss: 49.5329
[09/26 12:08:39 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1674, average loss: 45.4534
[09/26 12:08:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.50	
[09/26 12:08:39 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 12:08:46 visual_prompt]: Epoch 25 / 100: avg data time: 6.13e-02, avg batch time: 0.5047, average train loss: 35.3057
[09/26 12:08:48 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1670, average loss: 36.3871
[09/26 12:08:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 12:08:48 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 12:08:55 visual_prompt]: Epoch 26 / 100: avg data time: 5.63e-02, avg batch time: 0.4986, average train loss: 40.8548
[09/26 12:08:56 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1672, average loss: 30.4631
[09/26 12:08:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 58.50	
[09/26 12:08:56 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 12:09:03 visual_prompt]: Epoch 27 / 100: avg data time: 5.72e-02, avg batch time: 0.5007, average train loss: 34.5781
[09/26 12:09:05 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1672, average loss: 33.6604
[09/26 12:09:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.00	
[09/26 12:09:05 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 12:09:11 visual_prompt]: Epoch 28 / 100: avg data time: 5.65e-02, avg batch time: 0.5004, average train loss: 41.2403
[09/26 12:09:13 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1671, average loss: 44.5978
[09/26 12:09:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 54.50	
[09/26 12:09:13 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 12:09:20 visual_prompt]: Epoch 29 / 100: avg data time: 5.90e-02, avg batch time: 0.5012, average train loss: 60.8718
[09/26 12:09:21 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1672, average loss: 44.1455
[09/26 12:09:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 12:09:21 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 12:09:28 visual_prompt]: Epoch 30 / 100: avg data time: 6.28e-02, avg batch time: 0.5048, average train loss: 49.2258
[09/26 12:09:30 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1670, average loss: 43.2371
[09/26 12:09:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 12:09:30 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 12:09:37 visual_prompt]: Epoch 31 / 100: avg data time: 5.89e-02, avg batch time: 0.5028, average train loss: 43.5380
[09/26 12:09:38 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1673, average loss: 22.1292
[09/26 12:09:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 56.00	
[09/26 12:09:38 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 12:09:45 visual_prompt]: Epoch 32 / 100: avg data time: 5.03e-02, avg batch time: 0.4942, average train loss: 34.2557
[09/26 12:09:47 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1670, average loss: 55.3973
[09/26 12:09:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 12:09:47 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 12:09:53 visual_prompt]: Epoch 33 / 100: avg data time: 4.70e-02, avg batch time: 0.4930, average train loss: 31.5005
[09/26 12:09:55 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1674, average loss: 34.5691
[09/26 12:09:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.00	
[09/26 12:09:55 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 12:10:02 visual_prompt]: Epoch 34 / 100: avg data time: 6.14e-02, avg batch time: 0.5032, average train loss: 52.1389
[09/26 12:10:03 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1675, average loss: 46.2268
[09/26 12:10:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 61.00	
[09/26 12:10:03 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 12:10:10 visual_prompt]: Epoch 35 / 100: avg data time: 5.16e-02, avg batch time: 0.4958, average train loss: 39.2010
[09/26 12:10:12 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1669, average loss: 28.9957
[09/26 12:10:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 12:10:12 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 12:10:19 visual_prompt]: Epoch 36 / 100: avg data time: 6.19e-02, avg batch time: 0.5059, average train loss: 35.4200
[09/26 12:10:20 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1671, average loss: 21.1667
[09/26 12:10:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 57.00	
[09/26 12:10:20 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 12:10:27 visual_prompt]: Epoch 37 / 100: avg data time: 4.81e-02, avg batch time: 0.4938, average train loss: 44.7123
[09/26 12:10:28 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1673, average loss: 59.9501
[09/26 12:10:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 12:10:28 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 12:10:35 visual_prompt]: Epoch 38 / 100: avg data time: 4.87e-02, avg batch time: 0.4919, average train loss: 40.4730
[09/26 12:10:37 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1674, average loss: 31.4013
[09/26 12:10:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 12:10:37 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 12:10:43 visual_prompt]: Epoch 39 / 100: avg data time: 6.39e-02, avg batch time: 0.5073, average train loss: 32.5551
[09/26 12:10:45 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1673, average loss: 27.6851
[09/26 12:10:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 49.50	
[09/26 12:10:45 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 12:10:52 visual_prompt]: Epoch 40 / 100: avg data time: 4.85e-02, avg batch time: 0.4932, average train loss: 28.3721
[09/26 12:10:53 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1675, average loss: 24.4529
[09/26 12:10:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 12:10:53 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 12:11:00 visual_prompt]: Epoch 41 / 100: avg data time: 4.89e-02, avg batch time: 0.4960, average train loss: 29.1981
[09/26 12:11:02 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1671, average loss: 29.8921
[09/26 12:11:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 12:11:02 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 12:11:08 visual_prompt]: Epoch 42 / 100: avg data time: 5.68e-02, avg batch time: 0.5001, average train loss: 36.2906
[09/26 12:11:10 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1670, average loss: 21.1530
[09/26 12:11:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 12:11:10 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 12:11:17 visual_prompt]: Epoch 43 / 100: avg data time: 6.03e-02, avg batch time: 0.5027, average train loss: 31.1897
[09/26 12:11:18 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1675, average loss: 30.1641
[09/26 12:11:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.00	
[09/26 12:11:18 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 12:11:25 visual_prompt]: Epoch 44 / 100: avg data time: 5.55e-02, avg batch time: 0.4985, average train loss: 25.6019
[09/26 12:11:27 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1670, average loss: 20.1780
[09/26 12:11:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 12:11:27 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 12:11:33 visual_prompt]: Epoch 45 / 100: avg data time: 6.29e-02, avg batch time: 0.5052, average train loss: 24.6519
[09/26 12:11:35 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1670, average loss: 17.4229
[09/26 12:11:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 12:11:35 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 12:11:42 visual_prompt]: Epoch 46 / 100: avg data time: 6.03e-02, avg batch time: 0.5025, average train loss: 29.6866
[09/26 12:11:43 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1672, average loss: 56.0088
[09/26 12:11:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.00	
[09/26 12:11:43 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 12:11:50 visual_prompt]: Epoch 47 / 100: avg data time: 6.17e-02, avg batch time: 0.5042, average train loss: 28.4223
[09/26 12:11:52 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1668, average loss: 19.8626
[09/26 12:11:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 53.50	
[09/26 12:11:52 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 12:11:59 visual_prompt]: Epoch 48 / 100: avg data time: 5.16e-02, avg batch time: 0.4965, average train loss: 23.4797
[09/26 12:12:00 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1669, average loss: 19.3445
[09/26 12:12:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 59.00	
[09/26 12:12:00 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 12:12:07 visual_prompt]: Epoch 49 / 100: avg data time: 5.07e-02, avg batch time: 0.4935, average train loss: 25.4546
[09/26 12:12:09 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1669, average loss: 24.9054
[09/26 12:12:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 12:12:09 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 12:12:15 visual_prompt]: Epoch 50 / 100: avg data time: 5.95e-02, avg batch time: 0.5025, average train loss: 29.9714
[09/26 12:12:17 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1670, average loss: 28.5991
[09/26 12:12:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 12:12:17 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 12:12:24 visual_prompt]: Epoch 51 / 100: avg data time: 5.65e-02, avg batch time: 0.4987, average train loss: 33.2719
[09/26 12:12:25 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1671, average loss: 22.9968
[09/26 12:12:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.50	
[09/26 12:12:25 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 12:12:32 visual_prompt]: Epoch 52 / 100: avg data time: 6.01e-02, avg batch time: 0.5029, average train loss: 25.4482
[09/26 12:12:34 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1670, average loss: 22.9400
[09/26 12:12:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 12:12:34 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 12:12:41 visual_prompt]: Epoch 53 / 100: avg data time: 5.55e-02, avg batch time: 0.4987, average train loss: 29.3355
[09/26 12:12:42 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1669, average loss: 25.2739
[09/26 12:12:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.00	
[09/26 12:12:42 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 12:12:49 visual_prompt]: Epoch 54 / 100: avg data time: 5.55e-02, avg batch time: 0.4985, average train loss: 20.9992
[09/26 12:12:51 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1668, average loss: 58.6947
[09/26 12:12:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 12:12:51 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 12:12:57 visual_prompt]: Epoch 55 / 100: avg data time: 6.08e-02, avg batch time: 0.5034, average train loss: 35.5917
[09/26 12:12:59 visual_prompt]: Inference (val):avg data time: 4.75e-05, avg batch time: 0.1671, average loss: 25.9189
[09/26 12:12:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 12:12:59 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 12:13:06 visual_prompt]: Epoch 56 / 100: avg data time: 4.78e-02, avg batch time: 0.4922, average train loss: 23.6748
[09/26 12:13:07 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1672, average loss: 11.8007
[09/26 12:13:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.00	
[09/26 12:13:07 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 12:13:14 visual_prompt]: Epoch 57 / 100: avg data time: 5.82e-02, avg batch time: 0.5013, average train loss: 19.8800
[09/26 12:13:16 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1672, average loss: 17.1903
[09/26 12:13:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 12:13:16 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 12:13:22 visual_prompt]: Epoch 58 / 100: avg data time: 4.35e-02, avg batch time: 0.4887, average train loss: 15.8555
[09/26 12:13:24 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1671, average loss: 17.4399
[09/26 12:13:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.50	
[09/26 12:13:24 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 12:13:31 visual_prompt]: Epoch 59 / 100: avg data time: 7.00e-02, avg batch time: 0.5125, average train loss: 14.8488
[09/26 12:13:32 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1673, average loss: 13.5451
[09/26 12:13:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 12:13:32 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 12:13:39 visual_prompt]: Epoch 60 / 100: avg data time: 4.41e-02, avg batch time: 0.4914, average train loss: 15.0839
[09/26 12:13:41 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1672, average loss: 26.3254
[09/26 12:13:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 12:13:41 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 12:13:48 visual_prompt]: Epoch 61 / 100: avg data time: 6.09e-02, avg batch time: 0.5062, average train loss: 16.9186
[09/26 12:13:49 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1671, average loss: 13.8459
[09/26 12:13:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.00	
[09/26 12:13:49 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 12:13:56 visual_prompt]: Epoch 62 / 100: avg data time: 4.48e-02, avg batch time: 0.4880, average train loss: 14.5506
[09/26 12:13:57 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1678, average loss: 13.7520
[09/26 12:13:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 48.50	
[09/26 12:13:57 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 12:14:04 visual_prompt]: Epoch 63 / 100: avg data time: 5.29e-02, avg batch time: 0.4990, average train loss: 13.8636
[09/26 12:14:06 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1673, average loss: 20.2838
[09/26 12:14:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 60.50	
[09/26 12:14:06 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 12:14:13 visual_prompt]: Epoch 64 / 100: avg data time: 6.02e-02, avg batch time: 0.5043, average train loss: 18.1516
[09/26 12:14:14 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1676, average loss: 12.4545
[09/26 12:14:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 12:14:14 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 12:14:21 visual_prompt]: Epoch 65 / 100: avg data time: 5.31e-02, avg batch time: 0.4966, average train loss: 18.1763
[09/26 12:14:23 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1674, average loss: 11.4235
[09/26 12:14:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 12:14:23 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 12:14:29 visual_prompt]: Epoch 66 / 100: avg data time: 4.64e-02, avg batch time: 0.4928, average train loss: 9.9533
[09/26 12:14:31 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1670, average loss: 7.6169
[09/26 12:14:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.00	
[09/26 12:14:31 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 12:14:38 visual_prompt]: Epoch 67 / 100: avg data time: 4.93e-02, avg batch time: 0.4927, average train loss: 11.9820
[09/26 12:14:39 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1675, average loss: 12.0078
[09/26 12:14:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 12:14:39 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 12:14:46 visual_prompt]: Epoch 68 / 100: avg data time: 5.90e-02, avg batch time: 0.5027, average train loss: 9.5165
[09/26 12:14:48 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1672, average loss: 11.2517
[09/26 12:14:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 12:14:48 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 12:14:54 visual_prompt]: Epoch 69 / 100: avg data time: 6.08e-02, avg batch time: 0.5039, average train loss: 10.7330
[09/26 12:14:56 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1670, average loss: 6.3279
[09/26 12:14:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 12:14:56 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 12:15:03 visual_prompt]: Epoch 70 / 100: avg data time: 5.73e-02, avg batch time: 0.5004, average train loss: 9.5635
[09/26 12:15:04 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1671, average loss: 7.7763
[09/26 12:15:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 12:15:04 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 12:15:11 visual_prompt]: Epoch 71 / 100: avg data time: 6.06e-02, avg batch time: 0.5028, average train loss: 9.9149
[09/26 12:15:13 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1670, average loss: 7.5474
[09/26 12:15:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 12:15:13 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 12:15:20 visual_prompt]: Epoch 72 / 100: avg data time: 5.09e-02, avg batch time: 0.4951, average train loss: 8.3793
[09/26 12:15:21 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.1674, average loss: 5.9976
[09/26 12:15:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 12:15:21 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 12:15:28 visual_prompt]: Epoch 73 / 100: avg data time: 4.45e-02, avg batch time: 0.4896, average train loss: 5.7753
[09/26 12:15:29 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1672, average loss: 5.0991
[09/26 12:15:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 12:15:29 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 12:15:36 visual_prompt]: Epoch 74 / 100: avg data time: 6.13e-02, avg batch time: 0.5036, average train loss: 4.0196
[09/26 12:15:38 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1671, average loss: 3.0839
[09/26 12:15:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 12:15:38 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 12:15:45 visual_prompt]: Epoch 75 / 100: avg data time: 4.61e-02, avg batch time: 0.4909, average train loss: 3.3517
[09/26 12:15:46 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1670, average loss: 2.8441
[09/26 12:15:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 12:15:46 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 12:15:53 visual_prompt]: Epoch 76 / 100: avg data time: 5.11e-02, avg batch time: 0.4960, average train loss: 2.5780
[09/26 12:15:54 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1671, average loss: 2.6653
[09/26 12:15:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 62.50	
[09/26 12:15:54 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 12:16:01 visual_prompt]: Epoch 77 / 100: avg data time: 4.68e-02, avg batch time: 0.4916, average train loss: 2.5110
[09/26 12:16:03 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1672, average loss: 2.5695
[09/26 12:16:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 12:16:03 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 12:16:10 visual_prompt]: Epoch 78 / 100: avg data time: 5.88e-02, avg batch time: 0.5008, average train loss: 2.3539
[09/26 12:16:11 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1674, average loss: 2.4576
[09/26 12:16:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 12:16:11 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 12:16:18 visual_prompt]: Epoch 79 / 100: avg data time: 4.69e-02, avg batch time: 0.4925, average train loss: 2.5725
[09/26 12:16:19 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1674, average loss: 2.3157
[09/26 12:16:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 12:16:19 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 12:16:26 visual_prompt]: Epoch 80 / 100: avg data time: 5.99e-02, avg batch time: 0.5034, average train loss: 2.3456
[09/26 12:16:28 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1670, average loss: 2.2798
[09/26 12:16:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 12:16:28 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 12:16:35 visual_prompt]: Epoch 81 / 100: avg data time: 5.77e-02, avg batch time: 0.5007, average train loss: 2.2552
[09/26 12:16:36 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1675, average loss: 2.3376
[09/26 12:16:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.50	top5: 55.50	
[09/26 12:16:36 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 12:16:43 visual_prompt]: Epoch 82 / 100: avg data time: 5.18e-02, avg batch time: 0.4961, average train loss: 2.2896
[09/26 12:16:45 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1674, average loss: 2.2621
[09/26 12:16:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 12:16:45 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 12:16:51 visual_prompt]: Epoch 83 / 100: avg data time: 4.77e-02, avg batch time: 0.4921, average train loss: 2.2959
[09/26 12:16:53 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1675, average loss: 2.2795
[09/26 12:16:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 12:16:53 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 12:17:00 visual_prompt]: Epoch 84 / 100: avg data time: 6.23e-02, avg batch time: 0.5056, average train loss: 2.2790
[09/26 12:17:01 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1673, average loss: 2.2867
[09/26 12:17:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 12:17:01 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 12:17:08 visual_prompt]: Epoch 85 / 100: avg data time: 4.64e-02, avg batch time: 0.4900, average train loss: 2.2609
[09/26 12:17:10 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1672, average loss: 2.2591
[09/26 12:17:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 12:17:10 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 12:17:17 visual_prompt]: Epoch 86 / 100: avg data time: 5.39e-02, avg batch time: 0.4987, average train loss: 2.2365
[09/26 12:17:18 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1674, average loss: 2.2296
[09/26 12:17:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 12:17:18 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 12:17:25 visual_prompt]: Epoch 87 / 100: avg data time: 6.13e-02, avg batch time: 0.5043, average train loss: 2.2383
[09/26 12:17:27 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1670, average loss: 2.2643
[09/26 12:17:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 12:17:27 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 12:17:33 visual_prompt]: Epoch 88 / 100: avg data time: 5.96e-02, avg batch time: 0.5026, average train loss: 2.2166
[09/26 12:17:35 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1672, average loss: 2.2628
[09/26 12:17:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 12:17:35 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 12:17:42 visual_prompt]: Epoch 89 / 100: avg data time: 6.18e-02, avg batch time: 0.5042, average train loss: 2.2541
[09/26 12:17:43 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1667, average loss: 2.3120
[09/26 12:17:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 12:17:43 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 12:17:50 visual_prompt]: Epoch 90 / 100: avg data time: 5.82e-02, avg batch time: 0.5005, average train loss: 2.2443
[09/26 12:17:52 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1669, average loss: 2.2518
[09/26 12:17:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.50	
[09/26 12:17:52 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 12:17:59 visual_prompt]: Epoch 91 / 100: avg data time: 4.53e-02, avg batch time: 0.4898, average train loss: 2.2408
[09/26 12:18:00 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1676, average loss: 2.2232
[09/26 12:18:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 12:18:00 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 12:18:07 visual_prompt]: Epoch 92 / 100: avg data time: 6.27e-02, avg batch time: 0.5058, average train loss: 2.2213
[09/26 12:18:09 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1672, average loss: 2.2081
[09/26 12:18:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 12:18:09 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 12:18:15 visual_prompt]: Epoch 93 / 100: avg data time: 5.56e-02, avg batch time: 0.4987, average train loss: 2.2139
[09/26 12:18:17 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1668, average loss: 2.1955
[09/26 12:18:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 12:18:17 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 12:18:24 visual_prompt]: Epoch 94 / 100: avg data time: 5.02e-02, avg batch time: 0.4936, average train loss: 2.2090
[09/26 12:18:25 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1670, average loss: 2.2044
[09/26 12:18:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 12:18:25 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 12:18:32 visual_prompt]: Epoch 95 / 100: avg data time: 5.94e-02, avg batch time: 0.5013, average train loss: 2.2042
[09/26 12:18:34 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1669, average loss: 2.2036
[09/26 12:18:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 12:18:34 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 12:18:41 visual_prompt]: Epoch 96 / 100: avg data time: 5.95e-02, avg batch time: 0.5036, average train loss: 2.1961
[09/26 12:18:42 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1669, average loss: 2.1903
[09/26 12:18:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 12:18:42 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 12:18:49 visual_prompt]: Epoch 97 / 100: avg data time: 6.07e-02, avg batch time: 0.5040, average train loss: 2.1929
[09/26 12:18:51 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1673, average loss: 2.1998
[09/26 12:18:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 12:18:51 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 12:18:57 visual_prompt]: Epoch 98 / 100: avg data time: 6.41e-02, avg batch time: 0.5058, average train loss: 2.1924
[09/26 12:18:59 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1672, average loss: 2.1988
[09/26 12:18:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 12:18:59 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 12:19:06 visual_prompt]: Epoch 99 / 100: avg data time: 6.00e-02, avg batch time: 0.5024, average train loss: 2.1910
[09/26 12:19:07 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1674, average loss: 2.1967
[09/26 12:19:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 12:19:07 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 12:19:14 visual_prompt]: Epoch 100 / 100: avg data time: 6.25e-02, avg batch time: 0.5057, average train loss: 2.1903
[09/26 12:19:16 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1669, average loss: 2.1967
[09/26 12:19:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 12:19:16 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:19:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:19:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:19:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:19:16 visual_prompt]: Training with config:
[09/26 12:19:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:19:16 visual_prompt]: Loading training data...
[09/26 12:19:16 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 12:19:17 visual_prompt]: Number of images: 800
[09/26 12:19:17 visual_prompt]: Number of classes: 9 / 9
[09/26 12:19:17 visual_prompt]: Loading validation data...
[09/26 12:19:17 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 12:19:17 visual_prompt]: Number of images: 200
[09/26 12:19:17 visual_prompt]: Number of classes: 9 / 9
[09/26 12:19:17 visual_prompt]: Constructing models...
[09/26 12:19:20 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 12:19:20 visual_prompt]: tuned percent:0.542
[09/26 12:19:20 visual_prompt]: Device used for model: 0
[09/26 12:19:20 visual_prompt]: Setting up Evaluator...
[09/26 12:19:20 visual_prompt]: Setting up Trainer...
[09/26 12:19:20 visual_prompt]: 	Setting up the optimizer...
[09/26 12:19:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:19:27 visual_prompt]: Epoch 1 / 100: avg data time: 5.43e-02, avg batch time: 0.4979, average train loss: 2.8598
[09/26 12:19:28 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1667, average loss: 2.9516
[09/26 12:19:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 12:19:28 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 12:19:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 12:19:35 visual_prompt]: Epoch 2 / 100: avg data time: 4.76e-02, avg batch time: 0.4923, average train loss: 7.4143
[09/26 12:19:37 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1663, average loss: 6.3349
[09/26 12:19:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.50	
[09/26 12:19:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 12:19:43 visual_prompt]: Epoch 3 / 100: avg data time: 4.66e-02, avg batch time: 0.4889, average train loss: 6.3867
[09/26 12:19:45 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1668, average loss: 5.8336
[09/26 12:19:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 12:19:45 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 12:19:51 visual_prompt]: Epoch 4 / 100: avg data time: 5.57e-02, avg batch time: 0.4979, average train loss: 6.8153
[09/26 12:19:53 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1665, average loss: 6.1324
[09/26 12:19:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 12:19:53 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 12:20:00 visual_prompt]: Epoch 5 / 100: avg data time: 5.68e-02, avg batch time: 0.4987, average train loss: 4.5393
[09/26 12:20:01 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1667, average loss: 4.2630
[09/26 12:20:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.50	
[09/26 12:20:01 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 12:20:08 visual_prompt]: Epoch 6 / 100: avg data time: 4.48e-02, avg batch time: 0.4902, average train loss: 7.1310
[09/26 12:20:10 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1664, average loss: 6.7975
[09/26 12:20:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 12:20:10 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 12:20:16 visual_prompt]: Epoch 7 / 100: avg data time: 4.67e-02, avg batch time: 0.4896, average train loss: 21.9761
[09/26 12:20:18 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1677, average loss: 22.4922
[09/26 12:20:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 54.50	
[09/26 12:20:18 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 12:20:25 visual_prompt]: Epoch 8 / 100: avg data time: 5.68e-02, avg batch time: 0.5002, average train loss: 31.2266
[09/26 12:20:26 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1670, average loss: 35.6461
[09/26 12:20:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 12:20:26 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 12:20:33 visual_prompt]: Epoch 9 / 100: avg data time: 4.28e-02, avg batch time: 0.4869, average train loss: 31.5186
[09/26 12:20:34 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1665, average loss: 55.0345
[09/26 12:20:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 12:20:34 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 12:20:41 visual_prompt]: Epoch 10 / 100: avg data time: 5.84e-02, avg batch time: 0.5001, average train loss: 44.4389
[09/26 12:20:43 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1663, average loss: 41.9924
[09/26 12:20:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 12:20:43 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 12:20:49 visual_prompt]: Epoch 11 / 100: avg data time: 4.40e-02, avg batch time: 0.4875, average train loss: 53.3973
[09/26 12:20:51 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1670, average loss: 34.9654
[09/26 12:20:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/26 12:20:51 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 12:20:58 visual_prompt]: Epoch 12 / 100: avg data time: 5.58e-02, avg batch time: 0.4995, average train loss: 43.0617
[09/26 12:20:59 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1669, average loss: 70.6004
[09/26 12:20:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 12:20:59 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 12:21:06 visual_prompt]: Epoch 13 / 100: avg data time: 4.78e-02, avg batch time: 0.4906, average train loss: 53.4991
[09/26 12:21:07 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1666, average loss: 38.8797
[09/26 12:21:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.50	
[09/26 12:21:07 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 12:21:14 visual_prompt]: Epoch 14 / 100: avg data time: 5.32e-02, avg batch time: 0.4984, average train loss: 52.2104
[09/26 12:21:16 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1670, average loss: 28.6636
[09/26 12:21:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 12:21:16 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 12:21:22 visual_prompt]: Epoch 15 / 100: avg data time: 4.35e-02, avg batch time: 0.4876, average train loss: 33.8079
[09/26 12:21:24 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1666, average loss: 54.6017
[09/26 12:21:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 12:21:24 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 12:21:31 visual_prompt]: Epoch 16 / 100: avg data time: 4.47e-02, avg batch time: 0.4887, average train loss: 41.6579
[09/26 12:21:32 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1667, average loss: 73.9698
[09/26 12:21:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.50	
[09/26 12:21:32 visual_prompt]: Best epoch 16: best metric: 0.145
[09/26 12:21:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 12:21:39 visual_prompt]: Epoch 17 / 100: avg data time: 5.38e-02, avg batch time: 0.4984, average train loss: 73.9161
[09/26 12:21:40 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1667, average loss: 47.9983
[09/26 12:21:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/26 12:21:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 12:21:47 visual_prompt]: Epoch 18 / 100: avg data time: 5.37e-02, avg batch time: 0.4964, average train loss: 43.9605
[09/26 12:21:49 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1674, average loss: 41.1664
[09/26 12:21:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 12:21:49 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 12:21:56 visual_prompt]: Epoch 19 / 100: avg data time: 5.37e-02, avg batch time: 0.4970, average train loss: 67.0075
[09/26 12:21:57 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1672, average loss: 47.7229
[09/26 12:21:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.50	
[09/26 12:21:57 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 12:22:04 visual_prompt]: Epoch 20 / 100: avg data time: 4.72e-02, avg batch time: 0.4912, average train loss: 54.8774
[09/26 12:22:05 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1666, average loss: 53.7964
[09/26 12:22:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 12:22:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 12:22:12 visual_prompt]: Epoch 21 / 100: avg data time: 6.07e-02, avg batch time: 0.5033, average train loss: 54.3749
[09/26 12:22:14 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1672, average loss: 52.3067
[09/26 12:22:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 12:22:14 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 12:22:21 visual_prompt]: Epoch 22 / 100: avg data time: 5.07e-02, avg batch time: 0.4951, average train loss: 55.6323
[09/26 12:22:22 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1676, average loss: 33.7530
[09/26 12:22:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 12:22:22 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 12:22:29 visual_prompt]: Epoch 23 / 100: avg data time: 5.69e-02, avg batch time: 0.4991, average train loss: 57.2563
[09/26 12:22:31 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1675, average loss: 67.5389
[09/26 12:22:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 12:22:31 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 12:22:37 visual_prompt]: Epoch 24 / 100: avg data time: 4.61e-02, avg batch time: 0.4888, average train loss: 45.3013
[09/26 12:22:39 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1673, average loss: 65.2828
[09/26 12:22:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 48.50	
[09/26 12:22:39 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 12:22:45 visual_prompt]: Epoch 25 / 100: avg data time: 4.35e-02, avg batch time: 0.4855, average train loss: 39.7124
[09/26 12:22:47 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1669, average loss: 82.8934
[09/26 12:22:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.00	
[09/26 12:22:47 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 12:22:54 visual_prompt]: Epoch 26 / 100: avg data time: 5.66e-02, avg batch time: 0.4990, average train loss: 44.4500
[09/26 12:22:55 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1674, average loss: 42.8517
[09/26 12:22:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 12:22:55 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 12:23:02 visual_prompt]: Epoch 27 / 100: avg data time: 5.41e-02, avg batch time: 0.4980, average train loss: 44.9036
[09/26 12:23:04 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1666, average loss: 72.9965
[09/26 12:23:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 54.00	
[09/26 12:23:04 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 12:23:11 visual_prompt]: Epoch 28 / 100: avg data time: 5.23e-02, avg batch time: 0.4955, average train loss: 36.1866
[09/26 12:23:12 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1667, average loss: 29.6909
[09/26 12:23:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 12:23:12 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 12:23:19 visual_prompt]: Epoch 29 / 100: avg data time: 5.52e-02, avg batch time: 0.4987, average train loss: 32.4876
[09/26 12:23:21 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1670, average loss: 25.7862
[09/26 12:23:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.50	
[09/26 12:23:21 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 12:23:27 visual_prompt]: Epoch 30 / 100: avg data time: 4.75e-02, avg batch time: 0.4916, average train loss: 26.3909
[09/26 12:23:29 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1670, average loss: 25.8737
[09/26 12:23:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 12:23:29 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 12:23:36 visual_prompt]: Epoch 31 / 100: avg data time: 4.74e-02, avg batch time: 0.4905, average train loss: 23.5951
[09/26 12:23:37 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1669, average loss: 21.3701
[09/26 12:23:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.00	
[09/26 12:23:37 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 12:23:44 visual_prompt]: Epoch 32 / 100: avg data time: 4.77e-02, avg batch time: 0.4933, average train loss: 30.5936
[09/26 12:23:45 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1668, average loss: 32.4785
[09/26 12:23:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 12:23:45 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 12:23:52 visual_prompt]: Epoch 33 / 100: avg data time: 5.31e-02, avg batch time: 0.4954, average train loss: 32.2987
[09/26 12:23:54 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1672, average loss: 33.8060
[09/26 12:23:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 52.00	
[09/26 12:23:54 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 12:24:01 visual_prompt]: Epoch 34 / 100: avg data time: 5.92e-02, avg batch time: 0.5019, average train loss: 24.1048
[09/26 12:24:02 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1673, average loss: 43.6457
[09/26 12:24:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 62.50	
[09/26 12:24:02 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 12:24:09 visual_prompt]: Epoch 35 / 100: avg data time: 5.50e-02, avg batch time: 0.4973, average train loss: 26.6156
[09/26 12:24:11 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1675, average loss: 30.2406
[09/26 12:24:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 12:24:11 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 12:24:17 visual_prompt]: Epoch 36 / 100: avg data time: 4.79e-02, avg batch time: 0.4916, average train loss: 27.6758
[09/26 12:24:19 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1674, average loss: 25.1393
[09/26 12:24:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 12:24:19 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 12:24:26 visual_prompt]: Epoch 37 / 100: avg data time: 4.60e-02, avg batch time: 0.4904, average train loss: 20.8854
[09/26 12:24:27 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1671, average loss: 28.3086
[09/26 12:24:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 12:24:27 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 12:24:34 visual_prompt]: Epoch 38 / 100: avg data time: 4.67e-02, avg batch time: 0.4915, average train loss: 47.0033
[09/26 12:24:35 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1673, average loss: 52.6081
[09/26 12:24:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.00	
[09/26 12:24:35 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 12:24:42 visual_prompt]: Epoch 39 / 100: avg data time: 6.34e-02, avg batch time: 0.5050, average train loss: 49.3378
[09/26 12:24:44 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1675, average loss: 31.2451
[09/26 12:24:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 12:24:44 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 12:24:51 visual_prompt]: Epoch 40 / 100: avg data time: 5.83e-02, avg batch time: 0.5028, average train loss: 26.8359
[09/26 12:24:52 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1674, average loss: 43.1581
[09/26 12:24:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 12:24:52 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 12:24:59 visual_prompt]: Epoch 41 / 100: avg data time: 5.09e-02, avg batch time: 0.4948, average train loss: 30.8875
[09/26 12:25:01 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1671, average loss: 30.8533
[09/26 12:25:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 12:25:01 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 12:25:08 visual_prompt]: Epoch 42 / 100: avg data time: 6.11e-02, avg batch time: 0.5050, average train loss: 27.9973
[09/26 12:25:09 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1677, average loss: 13.2771
[09/26 12:25:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 12:25:09 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 12:25:16 visual_prompt]: Epoch 43 / 100: avg data time: 6.01e-02, avg batch time: 0.5028, average train loss: 31.8075
[09/26 12:25:18 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1674, average loss: 24.7520
[09/26 12:25:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 12:25:18 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 12:25:24 visual_prompt]: Epoch 44 / 100: avg data time: 4.53e-02, avg batch time: 0.4916, average train loss: 28.2066
[09/26 12:25:26 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1676, average loss: 28.6223
[09/26 12:25:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 12:25:26 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 12:25:33 visual_prompt]: Epoch 45 / 100: avg data time: 6.06e-02, avg batch time: 0.5031, average train loss: 29.2818
[09/26 12:25:34 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1677, average loss: 25.9696
[09/26 12:25:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 12:25:34 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 12:25:41 visual_prompt]: Epoch 46 / 100: avg data time: 5.35e-02, avg batch time: 0.4968, average train loss: 21.0613
[09/26 12:25:43 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1672, average loss: 16.8551
[09/26 12:25:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 60.50	
[09/26 12:25:43 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 12:25:49 visual_prompt]: Epoch 47 / 100: avg data time: 5.89e-02, avg batch time: 0.5020, average train loss: 25.9420
[09/26 12:25:51 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1676, average loss: 36.1940
[09/26 12:25:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.00	
[09/26 12:25:51 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 12:25:58 visual_prompt]: Epoch 48 / 100: avg data time: 4.41e-02, avg batch time: 0.4885, average train loss: 30.7971
[09/26 12:25:59 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1674, average loss: 30.3332
[09/26 12:25:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 12:25:59 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 12:26:06 visual_prompt]: Epoch 49 / 100: avg data time: 6.15e-02, avg batch time: 0.5036, average train loss: 25.6703
[09/26 12:26:08 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1674, average loss: 21.7796
[09/26 12:26:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 12:26:08 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 12:26:14 visual_prompt]: Epoch 50 / 100: avg data time: 5.68e-02, avg batch time: 0.4991, average train loss: 22.8698
[09/26 12:26:16 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1679, average loss: 21.7322
[09/26 12:26:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.50	
[09/26 12:26:16 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 12:26:23 visual_prompt]: Epoch 51 / 100: avg data time: 5.82e-02, avg batch time: 0.5015, average train loss: 22.5208
[09/26 12:26:24 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1679, average loss: 22.3797
[09/26 12:26:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 12:26:24 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 12:26:31 visual_prompt]: Epoch 52 / 100: avg data time: 6.33e-02, avg batch time: 0.5061, average train loss: 23.1063
[09/26 12:26:33 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1672, average loss: 21.2840
[09/26 12:26:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 12:26:33 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 12:26:40 visual_prompt]: Epoch 53 / 100: avg data time: 6.39e-02, avg batch time: 0.5065, average train loss: 27.3937
[09/26 12:26:41 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1669, average loss: 32.9134
[09/26 12:26:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 12:26:41 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 12:26:48 visual_prompt]: Epoch 54 / 100: avg data time: 6.06e-02, avg batch time: 0.5033, average train loss: 38.6266
[09/26 12:26:50 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1673, average loss: 57.7915
[09/26 12:26:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 12:26:50 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 12:26:57 visual_prompt]: Epoch 55 / 100: avg data time: 5.87e-02, avg batch time: 0.5013, average train loss: 34.2574
[09/26 12:26:58 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1670, average loss: 21.7035
[09/26 12:26:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.50	
[09/26 12:26:58 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 12:27:05 visual_prompt]: Epoch 56 / 100: avg data time: 4.93e-02, avg batch time: 0.4922, average train loss: 28.6777
[09/26 12:27:06 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1671, average loss: 25.0165
[09/26 12:27:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 12:27:06 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 12:27:13 visual_prompt]: Epoch 57 / 100: avg data time: 6.60e-02, avg batch time: 0.5077, average train loss: 21.4908
[09/26 12:27:15 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1674, average loss: 11.1360
[09/26 12:27:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.50	
[09/26 12:27:15 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 12:27:22 visual_prompt]: Epoch 58 / 100: avg data time: 4.74e-02, avg batch time: 0.4931, average train loss: 22.1661
[09/26 12:27:23 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1667, average loss: 20.3935
[09/26 12:27:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 12:27:23 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 12:27:30 visual_prompt]: Epoch 59 / 100: avg data time: 5.53e-02, avg batch time: 0.4987, average train loss: 15.2130
[09/26 12:27:31 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1672, average loss: 21.6621
[09/26 12:27:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 57.50	
[09/26 12:27:31 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 12:27:38 visual_prompt]: Epoch 60 / 100: avg data time: 4.88e-02, avg batch time: 0.4943, average train loss: 12.3388
[09/26 12:27:40 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1674, average loss: 6.8093
[09/26 12:27:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 12:27:40 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 12:27:46 visual_prompt]: Epoch 61 / 100: avg data time: 4.87e-02, avg batch time: 0.4907, average train loss: 16.5975
[09/26 12:27:48 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1672, average loss: 11.6641
[09/26 12:27:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 12:27:48 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 12:27:55 visual_prompt]: Epoch 62 / 100: avg data time: 5.05e-02, avg batch time: 0.4951, average train loss: 15.1046
[09/26 12:27:56 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.1668, average loss: 8.9891
[09/26 12:27:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 12:27:56 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 12:28:03 visual_prompt]: Epoch 63 / 100: avg data time: 5.17e-02, avg batch time: 0.4956, average train loss: 9.8996
[09/26 12:28:05 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1669, average loss: 9.2044
[09/26 12:28:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.50	
[09/26 12:28:05 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 12:28:11 visual_prompt]: Epoch 64 / 100: avg data time: 5.49e-02, avg batch time: 0.4986, average train loss: 7.7924
[09/26 12:28:13 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1675, average loss: 8.3817
[09/26 12:28:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 12:28:13 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 12:28:20 visual_prompt]: Epoch 65 / 100: avg data time: 5.96e-02, avg batch time: 0.5028, average train loss: 11.5129
[09/26 12:28:21 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1675, average loss: 40.6927
[09/26 12:28:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 12:28:21 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 12:28:28 visual_prompt]: Epoch 66 / 100: avg data time: 5.25e-02, avg batch time: 0.4946, average train loss: 17.4304
[09/26 12:28:30 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1673, average loss: 13.0806
[09/26 12:28:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.00	top5: 56.00	
[09/26 12:28:30 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 12:28:36 visual_prompt]: Epoch 67 / 100: avg data time: 4.86e-02, avg batch time: 0.4931, average train loss: 10.5577
[09/26 12:28:38 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1674, average loss: 8.7682
[09/26 12:28:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 12:28:38 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 12:28:45 visual_prompt]: Epoch 68 / 100: avg data time: 5.70e-02, avg batch time: 0.4989, average train loss: 10.1105
[09/26 12:28:46 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1670, average loss: 10.9893
[09/26 12:28:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 12:28:46 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 12:28:53 visual_prompt]: Epoch 69 / 100: avg data time: 4.55e-02, avg batch time: 0.4876, average train loss: 7.4596
[09/26 12:28:54 visual_prompt]: Inference (val):avg data time: 3.94e-05, avg batch time: 0.1675, average loss: 5.0741
[09/26 12:28:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 12:28:54 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 12:29:01 visual_prompt]: Epoch 70 / 100: avg data time: 5.05e-02, avg batch time: 0.4942, average train loss: 4.1221
[09/26 12:29:03 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1673, average loss: 3.4823
[09/26 12:29:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 12:29:03 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 12:29:09 visual_prompt]: Epoch 71 / 100: avg data time: 4.29e-02, avg batch time: 0.4882, average train loss: 3.2358
[09/26 12:29:11 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1673, average loss: 2.6687
[09/26 12:29:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 12:29:11 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 12:29:18 visual_prompt]: Epoch 72 / 100: avg data time: 4.60e-02, avg batch time: 0.4900, average train loss: 2.6507
[09/26 12:29:19 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1670, average loss: 2.4905
[09/26 12:29:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.50	
[09/26 12:29:19 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 12:29:26 visual_prompt]: Epoch 73 / 100: avg data time: 4.46e-02, avg batch time: 0.4892, average train loss: 2.4266
[09/26 12:29:27 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1672, average loss: 2.6422
[09/26 12:29:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 12:29:27 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 12:29:34 visual_prompt]: Epoch 74 / 100: avg data time: 4.64e-02, avg batch time: 0.4910, average train loss: 2.3791
[09/26 12:29:36 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1668, average loss: 2.3785
[09/26 12:29:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 12:29:36 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 12:29:43 visual_prompt]: Epoch 75 / 100: avg data time: 6.10e-02, avg batch time: 0.5045, average train loss: 2.4215
[09/26 12:29:44 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1674, average loss: 2.7742
[09/26 12:29:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 12:29:44 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 12:29:51 visual_prompt]: Epoch 76 / 100: avg data time: 5.77e-02, avg batch time: 0.5015, average train loss: 2.6394
[09/26 12:29:52 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1673, average loss: 2.7379
[09/26 12:29:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 12:29:52 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 12:29:59 visual_prompt]: Epoch 77 / 100: avg data time: 5.87e-02, avg batch time: 0.5011, average train loss: 2.5396
[09/26 12:30:01 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1674, average loss: 2.4085
[09/26 12:30:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.00	
[09/26 12:30:01 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 12:30:07 visual_prompt]: Epoch 78 / 100: avg data time: 4.46e-02, avg batch time: 0.4871, average train loss: 2.4500
[09/26 12:30:09 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1672, average loss: 2.5168
[09/26 12:30:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 12:30:09 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 12:30:16 visual_prompt]: Epoch 79 / 100: avg data time: 4.70e-02, avg batch time: 0.4909, average train loss: 2.4190
[09/26 12:30:17 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1673, average loss: 2.3889
[09/26 12:30:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/26 12:30:17 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 12:30:24 visual_prompt]: Epoch 80 / 100: avg data time: 5.04e-02, avg batch time: 0.4943, average train loss: 2.2867
[09/26 12:30:26 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1673, average loss: 2.2427
[09/26 12:30:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.00	
[09/26 12:30:26 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 12:30:32 visual_prompt]: Epoch 81 / 100: avg data time: 5.83e-02, avg batch time: 0.5011, average train loss: 2.2807
[09/26 12:30:34 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1678, average loss: 2.2429
[09/26 12:30:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 54.50	
[09/26 12:30:34 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 12:30:41 visual_prompt]: Epoch 82 / 100: avg data time: 5.69e-02, avg batch time: 0.5007, average train loss: 2.2644
[09/26 12:30:42 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1679, average loss: 2.2319
[09/26 12:30:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 12:30:42 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 12:30:49 visual_prompt]: Epoch 83 / 100: avg data time: 4.43e-02, avg batch time: 0.4867, average train loss: 2.2570
[09/26 12:30:51 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1675, average loss: 2.1480
[09/26 12:30:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 67.00	
[09/26 12:30:51 visual_prompt]: Best epoch 83: best metric: 0.180
[09/26 12:30:51 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 12:30:57 visual_prompt]: Epoch 84 / 100: avg data time: 5.90e-02, avg batch time: 0.5021, average train loss: 2.2036
[09/26 12:30:59 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1674, average loss: 2.3155
[09/26 12:30:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 12:30:59 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 12:31:06 visual_prompt]: Epoch 85 / 100: avg data time: 5.03e-02, avg batch time: 0.4946, average train loss: 2.1700
[09/26 12:31:07 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1668, average loss: 2.3767
[09/26 12:31:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 62.50	
[09/26 12:31:07 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 12:31:14 visual_prompt]: Epoch 86 / 100: avg data time: 5.32e-02, avg batch time: 0.4954, average train loss: 2.1347
[09/26 12:31:16 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1672, average loss: 2.3530
[09/26 12:31:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 65.00	
[09/26 12:31:16 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 12:31:22 visual_prompt]: Epoch 87 / 100: avg data time: 4.74e-02, avg batch time: 0.4917, average train loss: 2.1440
[09/26 12:31:24 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1670, average loss: 2.1805
[09/26 12:31:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 71.00	
[09/26 12:31:24 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 12:31:31 visual_prompt]: Epoch 88 / 100: avg data time: 5.31e-02, avg batch time: 0.4970, average train loss: 2.1736
[09/26 12:31:32 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1671, average loss: 2.0338
[09/26 12:31:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 77.50	
[09/26 12:31:32 visual_prompt]: Best epoch 88: best metric: 0.200
[09/26 12:31:32 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 12:31:39 visual_prompt]: Epoch 89 / 100: avg data time: 4.61e-02, avg batch time: 0.4895, average train loss: 2.0745
[09/26 12:31:40 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1673, average loss: 2.1795
[09/26 12:31:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 70.00	
[09/26 12:31:40 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 12:31:47 visual_prompt]: Epoch 90 / 100: avg data time: 4.32e-02, avg batch time: 0.4863, average train loss: 2.0017
[09/26 12:31:49 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1671, average loss: 1.9568
[09/26 12:31:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 80.50	
[09/26 12:31:49 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 12:31:55 visual_prompt]: Epoch 91 / 100: avg data time: 5.06e-02, avg batch time: 0.4955, average train loss: 1.9635
[09/26 12:31:57 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1673, average loss: 2.1388
[09/26 12:31:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 71.50	
[09/26 12:31:57 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 12:32:04 visual_prompt]: Epoch 92 / 100: avg data time: 4.60e-02, avg batch time: 0.4911, average train loss: 1.9519
[09/26 12:32:05 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1679, average loss: 1.9017
[09/26 12:32:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 86.00	
[09/26 12:32:05 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 12:32:12 visual_prompt]: Epoch 93 / 100: avg data time: 5.92e-02, avg batch time: 0.5036, average train loss: 1.8780
[09/26 12:32:14 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1677, average loss: 1.9401
[09/26 12:32:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 82.50	
[09/26 12:32:14 visual_prompt]: Best epoch 93: best metric: 0.205
[09/26 12:32:14 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 12:32:21 visual_prompt]: Epoch 94 / 100: avg data time: 6.80e-02, avg batch time: 0.5131, average train loss: 1.8116
[09/26 12:32:22 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1684, average loss: 1.8372
[09/26 12:32:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 87.50	
[09/26 12:32:22 visual_prompt]: Best epoch 94: best metric: 0.225
[09/26 12:32:22 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 12:32:29 visual_prompt]: Epoch 95 / 100: avg data time: 5.09e-02, avg batch time: 0.4979, average train loss: 1.7731
[09/26 12:32:31 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1684, average loss: 1.9195
[09/26 12:32:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 82.50	
[09/26 12:32:31 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 12:32:38 visual_prompt]: Epoch 96 / 100: avg data time: 5.62e-02, avg batch time: 0.5032, average train loss: 1.7916
[09/26 12:32:39 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1685, average loss: 1.9072
[09/26 12:32:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 84.50	
[09/26 12:32:39 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 12:32:46 visual_prompt]: Epoch 97 / 100: avg data time: 5.67e-02, avg batch time: 0.5042, average train loss: 1.6842
[09/26 12:32:47 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1688, average loss: 1.8471
[09/26 12:32:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 88.50	
[09/26 12:32:47 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 12:32:54 visual_prompt]: Epoch 98 / 100: avg data time: 6.09e-02, avg batch time: 0.5068, average train loss: 1.6208
[09/26 12:32:56 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1682, average loss: 1.8830
[09/26 12:32:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 87.50	
[09/26 12:32:56 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 12:33:03 visual_prompt]: Epoch 99 / 100: avg data time: 5.81e-02, avg batch time: 0.5043, average train loss: 1.5938
[09/26 12:33:04 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1683, average loss: 1.8702
[09/26 12:33:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 86.50	
[09/26 12:33:04 visual_prompt]: Best epoch 99: best metric: 0.230
[09/26 12:33:04 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 12:33:11 visual_prompt]: Epoch 100 / 100: avg data time: 6.30e-02, avg batch time: 0.5103, average train loss: 1.5744
[09/26 12:33:13 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1684, average loss: 1.8832
[09/26 12:33:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 86.00	
[09/26 12:33:13 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:33:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:33:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:33:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:33:13 visual_prompt]: Training with config:
[09/26 12:33:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:33:13 visual_prompt]: Loading training data...
[09/26 12:33:13 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 12:33:14 visual_prompt]: Number of images: 800
[09/26 12:33:14 visual_prompt]: Number of classes: 9 / 9
[09/26 12:33:14 visual_prompt]: Loading validation data...
[09/26 12:33:14 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 12:33:14 visual_prompt]: Number of images: 200
[09/26 12:33:14 visual_prompt]: Number of classes: 9 / 9
[09/26 12:33:14 visual_prompt]: Constructing models...
[09/26 12:33:17 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 12:33:17 visual_prompt]: tuned percent:0.542
[09/26 12:33:17 visual_prompt]: Device used for model: 0
[09/26 12:33:17 visual_prompt]: Setting up Evaluator...
[09/26 12:33:17 visual_prompt]: Setting up Trainer...
[09/26 12:33:17 visual_prompt]: 	Setting up the optimizer...
[09/26 12:33:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:33:24 visual_prompt]: Epoch 1 / 100: avg data time: 4.97e-02, avg batch time: 0.4990, average train loss: 2.8731
[09/26 12:33:25 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1679, average loss: 2.9516
[09/26 12:33:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 12:33:25 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 12:33:25 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 12:33:32 visual_prompt]: Epoch 2 / 100: avg data time: 4.87e-02, avg batch time: 0.4964, average train loss: 7.4830
[09/26 12:33:34 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1680, average loss: 6.4266
[09/26 12:33:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 12:33:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 12:33:40 visual_prompt]: Epoch 3 / 100: avg data time: 4.04e-02, avg batch time: 0.4881, average train loss: 5.0538
[09/26 12:33:42 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1682, average loss: 5.5484
[09/26 12:33:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 12:33:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 12:33:48 visual_prompt]: Epoch 4 / 100: avg data time: 4.78e-02, avg batch time: 0.4944, average train loss: 5.4588
[09/26 12:33:50 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1680, average loss: 6.6394
[09/26 12:33:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 51.50	
[09/26 12:33:50 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 12:33:57 visual_prompt]: Epoch 5 / 100: avg data time: 4.49e-02, avg batch time: 0.4915, average train loss: 8.1508
[09/26 12:33:58 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1685, average loss: 11.5570
[09/26 12:33:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 12:33:58 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 12:34:05 visual_prompt]: Epoch 6 / 100: avg data time: 4.47e-02, avg batch time: 0.4912, average train loss: 11.6097
[09/26 12:34:06 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1679, average loss: 10.0297
[09/26 12:34:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 12:34:06 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 12:34:13 visual_prompt]: Epoch 7 / 100: avg data time: 4.48e-02, avg batch time: 0.4921, average train loss: 16.3063
[09/26 12:34:15 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1680, average loss: 18.7253
[09/26 12:34:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/26 12:34:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 12:34:21 visual_prompt]: Epoch 8 / 100: avg data time: 5.01e-02, avg batch time: 0.4965, average train loss: 19.5650
[09/26 12:34:23 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1679, average loss: 26.5380
[09/26 12:34:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 12:34:23 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 12:34:30 visual_prompt]: Epoch 9 / 100: avg data time: 5.08e-02, avg batch time: 0.4998, average train loss: 23.7336
[09/26 12:34:31 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1679, average loss: 17.2157
[09/26 12:34:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/26 12:34:31 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 12:34:38 visual_prompt]: Epoch 10 / 100: avg data time: 4.33e-02, avg batch time: 0.4896, average train loss: 18.7888
[09/26 12:34:39 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1679, average loss: 36.0451
[09/26 12:34:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 12:34:39 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 12:34:46 visual_prompt]: Epoch 11 / 100: avg data time: 4.50e-02, avg batch time: 0.4915, average train loss: 36.4181
[09/26 12:34:48 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1679, average loss: 28.3575
[09/26 12:34:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 12:34:48 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 12:34:54 visual_prompt]: Epoch 12 / 100: avg data time: 4.94e-02, avg batch time: 0.4978, average train loss: 36.3702
[09/26 12:34:56 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1686, average loss: 35.6224
[09/26 12:34:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 12:34:56 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 12:35:03 visual_prompt]: Epoch 13 / 100: avg data time: 4.62e-02, avg batch time: 0.4930, average train loss: 36.4952
[09/26 12:35:04 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1683, average loss: 30.4610
[09/26 12:35:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/26 12:35:04 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 12:35:11 visual_prompt]: Epoch 14 / 100: avg data time: 5.60e-02, avg batch time: 0.5016, average train loss: 41.9624
[09/26 12:35:13 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1685, average loss: 49.0542
[09/26 12:35:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.50	
[09/26 12:35:13 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 12:35:19 visual_prompt]: Epoch 15 / 100: avg data time: 5.27e-02, avg batch time: 0.4981, average train loss: 41.1740
[09/26 12:35:21 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1679, average loss: 43.2716
[09/26 12:35:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 12:35:21 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 12:35:28 visual_prompt]: Epoch 16 / 100: avg data time: 4.39e-02, avg batch time: 0.4909, average train loss: 35.1497
[09/26 12:35:29 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1680, average loss: 21.9437
[09/26 12:35:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 12:35:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 12:35:36 visual_prompt]: Epoch 17 / 100: avg data time: 5.21e-02, avg batch time: 0.4981, average train loss: 29.6190
[09/26 12:35:38 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1680, average loss: 18.2611
[09/26 12:35:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 12:35:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 12:35:44 visual_prompt]: Epoch 18 / 100: avg data time: 5.97e-02, avg batch time: 0.5065, average train loss: 25.8908
[09/26 12:35:46 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1684, average loss: 16.3307
[09/26 12:35:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 52.00	
[09/26 12:35:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 12:35:53 visual_prompt]: Epoch 19 / 100: avg data time: 4.71e-02, avg batch time: 0.4945, average train loss: 19.1313
[09/26 12:35:54 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1683, average loss: 23.4466
[09/26 12:35:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 12:35:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 12:36:01 visual_prompt]: Epoch 20 / 100: avg data time: 6.01e-02, avg batch time: 0.5052, average train loss: 17.2170
[09/26 12:36:03 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1677, average loss: 17.2222
[09/26 12:36:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/26 12:36:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 12:36:10 visual_prompt]: Epoch 21 / 100: avg data time: 4.81e-02, avg batch time: 0.4948, average train loss: 17.6985
[09/26 12:36:11 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1673, average loss: 18.4511
[09/26 12:36:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 12:36:11 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 12:36:18 visual_prompt]: Epoch 22 / 100: avg data time: 5.36e-02, avg batch time: 0.5009, average train loss: 19.2417
[09/26 12:36:20 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1676, average loss: 24.2940
[09/26 12:36:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 12:36:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 12:36:26 visual_prompt]: Epoch 23 / 100: avg data time: 4.71e-02, avg batch time: 0.4935, average train loss: 18.8029
[09/26 12:36:28 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1681, average loss: 25.3857
[09/26 12:36:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.50	
[09/26 12:36:28 visual_prompt]: Best epoch 23: best metric: 0.145
[09/26 12:36:28 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 12:36:35 visual_prompt]: Epoch 24 / 100: avg data time: 6.07e-02, avg batch time: 0.5056, average train loss: 25.1863
[09/26 12:36:36 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1677, average loss: 27.9362
[09/26 12:36:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 12:36:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 12:36:43 visual_prompt]: Epoch 25 / 100: avg data time: 5.22e-02, avg batch time: 0.4966, average train loss: 25.1401
[09/26 12:36:45 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1678, average loss: 19.5628
[09/26 12:36:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 12:36:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 12:36:52 visual_prompt]: Epoch 26 / 100: avg data time: 6.21e-02, avg batch time: 0.5070, average train loss: 22.5571
[09/26 12:36:53 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1676, average loss: 20.0007
[09/26 12:36:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.50	
[09/26 12:36:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 12:37:00 visual_prompt]: Epoch 27 / 100: avg data time: 5.73e-02, avg batch time: 0.5024, average train loss: 21.5785
[09/26 12:37:02 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1680, average loss: 26.0235
[09/26 12:37:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 12:37:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 12:37:08 visual_prompt]: Epoch 28 / 100: avg data time: 4.47e-02, avg batch time: 0.4901, average train loss: 23.4376
[09/26 12:37:10 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1677, average loss: 27.9147
[09/26 12:37:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/26 12:37:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 12:37:17 visual_prompt]: Epoch 29 / 100: avg data time: 5.52e-02, avg batch time: 0.5004, average train loss: 24.5765
[09/26 12:37:18 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1677, average loss: 17.8310
[09/26 12:37:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 12:37:18 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 12:37:25 visual_prompt]: Epoch 30 / 100: avg data time: 5.55e-02, avg batch time: 0.5013, average train loss: 25.3666
[09/26 12:37:27 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1676, average loss: 43.8511
[09/26 12:37:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 12:37:27 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 12:37:33 visual_prompt]: Epoch 31 / 100: avg data time: 5.48e-02, avg batch time: 0.5005, average train loss: 33.3566
[09/26 12:37:35 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1674, average loss: 36.4755
[09/26 12:37:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 12:37:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 12:37:42 visual_prompt]: Epoch 32 / 100: avg data time: 5.71e-02, avg batch time: 0.5011, average train loss: 26.4239
[09/26 12:37:44 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1674, average loss: 26.5606
[09/26 12:37:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.00	top5: 55.50	
[09/26 12:37:44 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 12:37:50 visual_prompt]: Epoch 33 / 100: avg data time: 4.71e-02, avg batch time: 0.4920, average train loss: 24.1055
[09/26 12:37:52 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1676, average loss: 17.0160
[09/26 12:37:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 50.50	
[09/26 12:37:52 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 12:37:59 visual_prompt]: Epoch 34 / 100: avg data time: 6.02e-02, avg batch time: 0.5033, average train loss: 14.4340
[09/26 12:38:00 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1673, average loss: 15.1171
[09/26 12:38:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 12:38:00 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 12:38:07 visual_prompt]: Epoch 35 / 100: avg data time: 5.57e-02, avg batch time: 0.4998, average train loss: 15.5059
[09/26 12:38:09 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1675, average loss: 17.6449
[09/26 12:38:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 12:38:09 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 12:38:15 visual_prompt]: Epoch 36 / 100: avg data time: 5.00e-02, avg batch time: 0.4943, average train loss: 15.9444
[09/26 12:38:17 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1673, average loss: 12.4633
[09/26 12:38:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 12:38:17 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 12:38:24 visual_prompt]: Epoch 37 / 100: avg data time: 6.11e-02, avg batch time: 0.5035, average train loss: 13.8576
[09/26 12:38:25 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.1674, average loss: 13.7853
[09/26 12:38:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 53.00	
[09/26 12:38:25 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 12:38:32 visual_prompt]: Epoch 38 / 100: avg data time: 7.00e-02, avg batch time: 0.5122, average train loss: 11.3162
[09/26 12:38:34 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1672, average loss: 11.2863
[09/26 12:38:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 50.50	
[09/26 12:38:34 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 12:38:41 visual_prompt]: Epoch 39 / 100: avg data time: 5.85e-02, avg batch time: 0.5020, average train loss: 12.5685
[09/26 12:38:42 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1673, average loss: 13.3662
[09/26 12:38:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.50	
[09/26 12:38:42 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 12:38:49 visual_prompt]: Epoch 40 / 100: avg data time: 5.82e-02, avg batch time: 0.5003, average train loss: 12.5784
[09/26 12:38:51 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1672, average loss: 21.3779
[09/26 12:38:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 12:38:51 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 12:38:57 visual_prompt]: Epoch 41 / 100: avg data time: 4.96e-02, avg batch time: 0.4941, average train loss: 20.5581
[09/26 12:38:59 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1673, average loss: 24.9908
[09/26 12:38:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 56.50	
[09/26 12:38:59 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 12:39:06 visual_prompt]: Epoch 42 / 100: avg data time: 5.93e-02, avg batch time: 0.5025, average train loss: 15.8431
[09/26 12:39:07 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1677, average loss: 13.6895
[09/26 12:39:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.50	
[09/26 12:39:07 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 12:39:14 visual_prompt]: Epoch 43 / 100: avg data time: 4.99e-02, avg batch time: 0.4943, average train loss: 21.6236
[09/26 12:39:16 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1669, average loss: 14.8305
[09/26 12:39:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 12:39:16 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 12:39:23 visual_prompt]: Epoch 44 / 100: avg data time: 5.55e-02, avg batch time: 0.4986, average train loss: 29.4370
[09/26 12:39:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1674, average loss: 40.6357
[09/26 12:39:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 6.50	top5: 48.50	
[09/26 12:39:24 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 12:39:31 visual_prompt]: Epoch 45 / 100: avg data time: 5.55e-02, avg batch time: 0.4990, average train loss: 29.3336
[09/26 12:39:32 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1671, average loss: 26.6143
[09/26 12:39:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 12:39:32 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 12:39:39 visual_prompt]: Epoch 46 / 100: avg data time: 4.69e-02, avg batch time: 0.4926, average train loss: 25.0359
[09/26 12:39:41 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1676, average loss: 31.7610
[09/26 12:39:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 12:39:41 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 12:39:48 visual_prompt]: Epoch 47 / 100: avg data time: 6.03e-02, avg batch time: 0.5045, average train loss: 22.4029
[09/26 12:39:49 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1674, average loss: 11.0242
[09/26 12:39:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 12:39:49 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 12:39:56 visual_prompt]: Epoch 48 / 100: avg data time: 6.16e-02, avg batch time: 0.5050, average train loss: 14.9373
[09/26 12:39:58 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1673, average loss: 14.7430
[09/26 12:39:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.00	
[09/26 12:39:58 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 12:40:04 visual_prompt]: Epoch 49 / 100: avg data time: 5.57e-02, avg batch time: 0.4994, average train loss: 10.8092
[09/26 12:40:06 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1675, average loss: 9.6803
[09/26 12:40:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 12:40:06 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 12:40:13 visual_prompt]: Epoch 50 / 100: avg data time: 5.88e-02, avg batch time: 0.5020, average train loss: 9.5551
[09/26 12:40:14 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1672, average loss: 4.8799
[09/26 12:40:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 12:40:14 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 12:40:21 visual_prompt]: Epoch 51 / 100: avg data time: 5.37e-02, avg batch time: 0.4983, average train loss: 5.7099
[09/26 12:40:23 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1672, average loss: 8.4291
[09/26 12:40:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 53.50	
[09/26 12:40:23 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 12:40:29 visual_prompt]: Epoch 52 / 100: avg data time: 6.28e-02, avg batch time: 0.5068, average train loss: 7.2452
[09/26 12:40:31 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1674, average loss: 6.4653
[09/26 12:40:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 56.50	
[09/26 12:40:31 visual_prompt]: Best epoch 52: best metric: 0.150
[09/26 12:40:31 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 12:40:38 visual_prompt]: Epoch 53 / 100: avg data time: 4.56e-02, avg batch time: 0.4904, average train loss: 6.1913
[09/26 12:40:39 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1671, average loss: 5.0022
[09/26 12:40:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 12:40:39 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 12:40:46 visual_prompt]: Epoch 54 / 100: avg data time: 6.25e-02, avg batch time: 0.5059, average train loss: 4.3706
[09/26 12:40:48 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1672, average loss: 2.4984
[09/26 12:40:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 12:40:48 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 12:40:54 visual_prompt]: Epoch 55 / 100: avg data time: 5.57e-02, avg batch time: 0.4991, average train loss: 3.1301
[09/26 12:40:56 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1674, average loss: 3.1142
[09/26 12:40:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 12:40:56 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 12:41:03 visual_prompt]: Epoch 56 / 100: avg data time: 5.91e-02, avg batch time: 0.5027, average train loss: 2.7656
[09/26 12:41:04 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1677, average loss: 2.4346
[09/26 12:41:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 12:41:04 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 12:41:11 visual_prompt]: Epoch 57 / 100: avg data time: 5.65e-02, avg batch time: 0.5013, average train loss: 2.5369
[09/26 12:41:13 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1673, average loss: 2.2613
[09/26 12:41:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 60.00	
[09/26 12:41:13 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 12:41:20 visual_prompt]: Epoch 58 / 100: avg data time: 4.58e-02, avg batch time: 0.4925, average train loss: 2.6242
[09/26 12:41:21 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1675, average loss: 2.6533
[09/26 12:41:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 60.50	
[09/26 12:41:21 visual_prompt]: Best epoch 58: best metric: 0.155
[09/26 12:41:21 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 12:41:28 visual_prompt]: Epoch 59 / 100: avg data time: 5.06e-02, avg batch time: 0.4960, average train loss: 2.4607
[09/26 12:41:29 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1673, average loss: 2.4217
[09/26 12:41:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 61.50	
[09/26 12:41:29 visual_prompt]: Best epoch 59: best metric: 0.180
[09/26 12:41:29 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 12:41:36 visual_prompt]: Epoch 60 / 100: avg data time: 5.02e-02, avg batch time: 0.4940, average train loss: 2.4439
[09/26 12:41:38 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1675, average loss: 3.0338
[09/26 12:41:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 54.00	
[09/26 12:41:38 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 12:41:45 visual_prompt]: Epoch 61 / 100: avg data time: 5.05e-02, avg batch time: 0.4941, average train loss: 2.5432
[09/26 12:41:46 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1672, average loss: 2.3872
[09/26 12:41:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 62.00	
[09/26 12:41:46 visual_prompt]: Best epoch 61: best metric: 0.185
[09/26 12:41:46 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 12:41:53 visual_prompt]: Epoch 62 / 100: avg data time: 5.89e-02, avg batch time: 0.5014, average train loss: 2.2848
[09/26 12:41:54 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1670, average loss: 2.2118
[09/26 12:41:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 71.00	
[09/26 12:41:54 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 12:42:01 visual_prompt]: Epoch 63 / 100: avg data time: 5.51e-02, avg batch time: 0.5001, average train loss: 2.2243
[09/26 12:42:03 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1679, average loss: 2.1122
[09/26 12:42:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 73.00	
[09/26 12:42:03 visual_prompt]: Best epoch 63: best metric: 0.195
[09/26 12:42:03 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 12:42:10 visual_prompt]: Epoch 64 / 100: avg data time: 5.14e-02, avg batch time: 0.4945, average train loss: 2.1762
[09/26 12:42:11 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1678, average loss: 2.1974
[09/26 12:42:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 70.50	
[09/26 12:42:11 visual_prompt]: Best epoch 64: best metric: 0.205
[09/26 12:42:11 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 12:42:18 visual_prompt]: Epoch 65 / 100: avg data time: 5.60e-02, avg batch time: 0.4985, average train loss: 2.2649
[09/26 12:42:20 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.1674, average loss: 2.1098
[09/26 12:42:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 77.50	
[09/26 12:42:20 visual_prompt]: Best epoch 65: best metric: 0.235
[09/26 12:42:20 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 12:42:26 visual_prompt]: Epoch 66 / 100: avg data time: 4.60e-02, avg batch time: 0.4898, average train loss: 2.1898
[09/26 12:42:28 visual_prompt]: Inference (val):avg data time: 3.69e-05, avg batch time: 0.1670, average loss: 1.9169
[09/26 12:42:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 80.50	
[09/26 12:42:28 visual_prompt]: Best epoch 66: best metric: 0.245
[09/26 12:42:28 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 12:42:35 visual_prompt]: Epoch 67 / 100: avg data time: 5.85e-02, avg batch time: 0.5014, average train loss: 2.0315
[09/26 12:42:36 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1674, average loss: 2.1694
[09/26 12:42:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 75.00	
[09/26 12:42:36 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 12:42:43 visual_prompt]: Epoch 68 / 100: avg data time: 5.92e-02, avg batch time: 0.5036, average train loss: 2.0933
[09/26 12:42:45 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1672, average loss: 2.3525
[09/26 12:42:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 73.00	
[09/26 12:42:45 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 12:42:52 visual_prompt]: Epoch 69 / 100: avg data time: 5.29e-02, avg batch time: 0.4983, average train loss: 1.9926
[09/26 12:42:53 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1675, average loss: 1.9018
[09/26 12:42:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 86.00	
[09/26 12:42:53 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 12:43:00 visual_prompt]: Epoch 70 / 100: avg data time: 4.91e-02, avg batch time: 0.4933, average train loss: 1.8844
[09/26 12:43:01 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1679, average loss: 2.0861
[09/26 12:43:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 81.00	
[09/26 12:43:01 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 12:43:08 visual_prompt]: Epoch 71 / 100: avg data time: 5.21e-02, avg batch time: 0.4958, average train loss: 2.0012
[09/26 12:43:10 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1670, average loss: 1.9644
[09/26 12:43:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 86.00	
[09/26 12:43:10 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 12:43:17 visual_prompt]: Epoch 72 / 100: avg data time: 5.77e-02, avg batch time: 0.5018, average train loss: 1.8304
[09/26 12:43:18 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1674, average loss: 1.7870
[09/26 12:43:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.00	
[09/26 12:43:18 visual_prompt]: Best epoch 72: best metric: 0.260
[09/26 12:43:18 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 12:43:25 visual_prompt]: Epoch 73 / 100: avg data time: 5.83e-02, avg batch time: 0.5030, average train loss: 1.7405
[09/26 12:43:27 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1675, average loss: 2.1079
[09/26 12:43:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 80.50	
[09/26 12:43:27 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 12:43:33 visual_prompt]: Epoch 74 / 100: avg data time: 6.01e-02, avg batch time: 0.5051, average train loss: 1.7099
[09/26 12:43:35 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1677, average loss: 1.9174
[09/26 12:43:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 85.50	
[09/26 12:43:35 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 12:43:42 visual_prompt]: Epoch 75 / 100: avg data time: 5.33e-02, avg batch time: 0.4982, average train loss: 1.7957
[09/26 12:43:43 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1672, average loss: 2.1490
[09/26 12:43:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 78.00	
[09/26 12:43:43 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 12:43:50 visual_prompt]: Epoch 76 / 100: avg data time: 6.89e-02, avg batch time: 0.5121, average train loss: 1.8373
[09/26 12:43:52 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1676, average loss: 1.9836
[09/26 12:43:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 83.50	
[09/26 12:43:52 visual_prompt]: Best epoch 76: best metric: 0.290
[09/26 12:43:52 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 12:43:59 visual_prompt]: Epoch 77 / 100: avg data time: 4.80e-02, avg batch time: 0.4921, average train loss: 1.8790
[09/26 12:44:00 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1674, average loss: 1.9467
[09/26 12:44:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 85.50	
[09/26 12:44:00 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 12:44:07 visual_prompt]: Epoch 78 / 100: avg data time: 6.06e-02, avg batch time: 0.5035, average train loss: 1.7489
[09/26 12:44:09 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1671, average loss: 1.9028
[09/26 12:44:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 84.50	
[09/26 12:44:09 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 12:44:15 visual_prompt]: Epoch 79 / 100: avg data time: 4.62e-02, avg batch time: 0.4906, average train loss: 1.6845
[09/26 12:44:17 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1677, average loss: 2.1175
[09/26 12:44:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 84.50	
[09/26 12:44:17 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 12:44:24 visual_prompt]: Epoch 80 / 100: avg data time: 5.43e-02, avg batch time: 0.4971, average train loss: 1.6095
[09/26 12:44:25 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1674, average loss: 1.9057
[09/26 12:44:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 91.00	
[09/26 12:44:25 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 12:44:32 visual_prompt]: Epoch 81 / 100: avg data time: 6.07e-02, avg batch time: 0.5042, average train loss: 1.5214
[09/26 12:44:34 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1670, average loss: 1.9750
[09/26 12:44:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 85.50	
[09/26 12:44:34 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 12:44:41 visual_prompt]: Epoch 82 / 100: avg data time: 5.62e-02, avg batch time: 0.4995, average train loss: 1.5121
[09/26 12:44:42 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1676, average loss: 1.8767
[09/26 12:44:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 89.00	
[09/26 12:44:42 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 12:44:49 visual_prompt]: Epoch 83 / 100: avg data time: 4.99e-02, avg batch time: 0.4952, average train loss: 1.4669
[09/26 12:44:51 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1673, average loss: 1.9010
[09/26 12:44:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 89.50	
[09/26 12:44:51 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 12:44:57 visual_prompt]: Epoch 84 / 100: avg data time: 6.20e-02, avg batch time: 0.5061, average train loss: 1.5047
[09/26 12:44:59 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1672, average loss: 1.8249
[09/26 12:44:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 91.00	
[09/26 12:44:59 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 12:45:06 visual_prompt]: Epoch 85 / 100: avg data time: 4.99e-02, avg batch time: 0.4933, average train loss: 1.4812
[09/26 12:45:07 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1677, average loss: 1.9109
[09/26 12:45:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 88.00	
[09/26 12:45:07 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 12:45:14 visual_prompt]: Epoch 86 / 100: avg data time: 5.45e-02, avg batch time: 0.4978, average train loss: 1.3480
[09/26 12:45:16 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1671, average loss: 1.7963
[09/26 12:45:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 92.00	
[09/26 12:45:16 visual_prompt]: Best epoch 86: best metric: 0.330
[09/26 12:45:16 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 12:45:22 visual_prompt]: Epoch 87 / 100: avg data time: 5.11e-02, avg batch time: 0.4960, average train loss: 1.3101
[09/26 12:45:24 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1675, average loss: 2.1449
[09/26 12:45:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 89.00	
[09/26 12:45:24 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 12:45:31 visual_prompt]: Epoch 88 / 100: avg data time: 5.35e-02, avg batch time: 0.4973, average train loss: 1.4826
[09/26 12:45:32 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1672, average loss: 1.9379
[09/26 12:45:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 90.00	
[09/26 12:45:32 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 12:45:39 visual_prompt]: Epoch 89 / 100: avg data time: 5.77e-02, avg batch time: 0.5003, average train loss: 1.3603
[09/26 12:45:41 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1673, average loss: 1.9023
[09/26 12:45:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 90.00	
[09/26 12:45:41 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 12:45:48 visual_prompt]: Epoch 90 / 100: avg data time: 5.55e-02, avg batch time: 0.4977, average train loss: 1.2091
[09/26 12:45:49 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1678, average loss: 1.8456
[09/26 12:45:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 89.50	
[09/26 12:45:49 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 12:45:56 visual_prompt]: Epoch 91 / 100: avg data time: 5.84e-02, avg batch time: 0.5015, average train loss: 1.1221
[09/26 12:45:58 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1673, average loss: 2.0392
[09/26 12:45:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 90.50	
[09/26 12:45:58 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 12:46:05 visual_prompt]: Epoch 92 / 100: avg data time: 6.19e-02, avg batch time: 0.5057, average train loss: 1.1885
[09/26 12:46:06 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1675, average loss: 1.9715
[09/26 12:46:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 12:46:06 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 12:46:13 visual_prompt]: Epoch 93 / 100: avg data time: 6.26e-02, avg batch time: 0.5065, average train loss: 1.0963
[09/26 12:46:15 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1673, average loss: 2.1114
[09/26 12:46:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 87.00	
[09/26 12:46:15 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 12:46:21 visual_prompt]: Epoch 94 / 100: avg data time: 6.42e-02, avg batch time: 0.5091, average train loss: 1.0806
[09/26 12:46:23 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1675, average loss: 2.1103
[09/26 12:46:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 88.50	
[09/26 12:46:23 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 12:46:30 visual_prompt]: Epoch 95 / 100: avg data time: 6.02e-02, avg batch time: 0.5030, average train loss: 1.0207
[09/26 12:46:32 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1673, average loss: 2.1120
[09/26 12:46:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 12:46:32 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 12:46:38 visual_prompt]: Epoch 96 / 100: avg data time: 6.28e-02, avg batch time: 0.5064, average train loss: 0.9897
[09/26 12:46:40 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1674, average loss: 2.0611
[09/26 12:46:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 90.50	
[09/26 12:46:40 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 12:46:47 visual_prompt]: Epoch 97 / 100: avg data time: 6.06e-02, avg batch time: 0.5039, average train loss: 0.9845
[09/26 12:46:48 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1674, average loss: 2.0749
[09/26 12:46:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 91.00	
[09/26 12:46:48 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 12:46:55 visual_prompt]: Epoch 98 / 100: avg data time: 5.36e-02, avg batch time: 0.4971, average train loss: 0.9448
[09/26 12:46:57 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1674, average loss: 2.0845
[09/26 12:46:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 90.00	
[09/26 12:46:57 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 12:47:03 visual_prompt]: Epoch 99 / 100: avg data time: 4.74e-02, avg batch time: 0.4938, average train loss: 0.9548
[09/26 12:47:05 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1675, average loss: 2.0970
[09/26 12:47:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 90.00	
[09/26 12:47:05 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 12:47:12 visual_prompt]: Epoch 100 / 100: avg data time: 4.80e-02, avg batch time: 0.4973, average train loss: 0.9569
[09/26 12:47:13 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1672, average loss: 2.0945
[09/26 12:47:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 90.00	
[09/26 12:47:13 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 12:47:13 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 12:47:13 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 12:47:13 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 12:47:13 visual_prompt]: Training with config:
[09/26 12:47:13 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr10.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 10.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 12:47:13 visual_prompt]: Loading training data...
[09/26 12:47:13 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 12:47:14 visual_prompt]: Number of images: 800
[09/26 12:47:14 visual_prompt]: Number of classes: 9 / 9
[09/26 12:47:14 visual_prompt]: Loading validation data...
[09/26 12:47:14 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 12:47:15 visual_prompt]: Number of images: 200
[09/26 12:47:15 visual_prompt]: Number of classes: 9 / 9
[09/26 12:47:15 visual_prompt]: Constructing models...
[09/26 12:47:17 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 12:47:17 visual_prompt]: tuned percent:0.542
[09/26 12:47:17 visual_prompt]: Device used for model: 0
[09/26 12:47:17 visual_prompt]: Setting up Evaluator...
[09/26 12:47:17 visual_prompt]: Setting up Trainer...
[09/26 12:47:17 visual_prompt]: 	Setting up the optimizer...
[09/26 12:47:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 12:47:24 visual_prompt]: Epoch 1 / 100: avg data time: 5.06e-02, avg batch time: 0.4952, average train loss: 2.8589
[09/26 12:47:26 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1669, average loss: 2.9516
[09/26 12:47:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 12:47:26 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 12:47:26 visual_prompt]: Training 2 / 100 epoch, with learning rate 1.0
[09/26 12:47:32 visual_prompt]: Epoch 2 / 100: avg data time: 5.51e-02, avg batch time: 0.4976, average train loss: 7.5950
[09/26 12:47:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1666, average loss: 6.8399
[09/26 12:47:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 12:47:34 visual_prompt]: Training 3 / 100 epoch, with learning rate 2.0
[09/26 12:47:41 visual_prompt]: Epoch 3 / 100: avg data time: 5.55e-02, avg batch time: 0.4975, average train loss: 11.0740
[09/26 12:47:42 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1665, average loss: 7.9609
[09/26 12:47:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.50	
[09/26 12:47:42 visual_prompt]: Training 4 / 100 epoch, with learning rate 3.0
[09/26 12:47:49 visual_prompt]: Epoch 4 / 100: avg data time: 5.17e-02, avg batch time: 0.4942, average train loss: 7.1310
[09/26 12:47:50 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1668, average loss: 8.3388
[09/26 12:47:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 12:47:50 visual_prompt]: Training 5 / 100 epoch, with learning rate 4.0
[09/26 12:47:57 visual_prompt]: Epoch 5 / 100: avg data time: 4.65e-02, avg batch time: 0.4895, average train loss: 16.7526
[09/26 12:47:59 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1670, average loss: 20.9911
[09/26 12:47:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 12:47:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 5.0
[09/26 12:48:06 visual_prompt]: Epoch 6 / 100: avg data time: 5.73e-02, avg batch time: 0.5000, average train loss: 27.8588
[09/26 12:48:07 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1669, average loss: 22.0278
[09/26 12:48:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 12:48:07 visual_prompt]: Training 7 / 100 epoch, with learning rate 6.0
[09/26 12:48:14 visual_prompt]: Epoch 7 / 100: avg data time: 4.50e-02, avg batch time: 0.4902, average train loss: 29.3092
[09/26 12:48:15 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1668, average loss: 25.1128
[09/26 12:48:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 12:48:15 visual_prompt]: Training 8 / 100 epoch, with learning rate 7.0
[09/26 12:48:22 visual_prompt]: Epoch 8 / 100: avg data time: 6.21e-02, avg batch time: 0.5045, average train loss: 23.3145
[09/26 12:48:24 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1673, average loss: 18.9792
[09/26 12:48:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/26 12:48:24 visual_prompt]: Training 9 / 100 epoch, with learning rate 8.0
[09/26 12:48:31 visual_prompt]: Epoch 9 / 100: avg data time: 4.97e-02, avg batch time: 0.4946, average train loss: 21.9775
[09/26 12:48:32 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1669, average loss: 27.4555
[09/26 12:48:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.00	
[09/26 12:48:32 visual_prompt]: Best epoch 9: best metric: 0.145
[09/26 12:48:32 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.0
[09/26 12:48:39 visual_prompt]: Epoch 10 / 100: avg data time: 5.85e-02, avg batch time: 0.5023, average train loss: 19.1611
[09/26 12:48:41 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1673, average loss: 18.0062
[09/26 12:48:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 50.50	
[09/26 12:48:41 visual_prompt]: Training 11 / 100 epoch, with learning rate 10.0
[09/26 12:48:47 visual_prompt]: Epoch 11 / 100: avg data time: 5.51e-02, avg batch time: 0.4973, average train loss: 19.2647
[09/26 12:48:49 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1669, average loss: 21.7743
[09/26 12:48:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 12:48:49 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.996954135095478
[09/26 12:48:55 visual_prompt]: Epoch 12 / 100: avg data time: 4.53e-02, avg batch time: 0.4891, average train loss: 24.2884
[09/26 12:48:57 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1673, average loss: 30.5687
[09/26 12:48:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.50	
[09/26 12:48:57 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.987820251299121
[09/26 12:49:04 visual_prompt]: Epoch 13 / 100: avg data time: 4.67e-02, avg batch time: 0.4911, average train loss: 27.8948
[09/26 12:49:05 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1669, average loss: 18.4119
[09/26 12:49:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 12:49:05 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.972609476841367
[09/26 12:49:12 visual_prompt]: Epoch 14 / 100: avg data time: 5.16e-02, avg batch time: 0.4958, average train loss: 22.2971
[09/26 12:49:14 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1669, average loss: 21.9735
[09/26 12:49:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/26 12:49:14 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.951340343707852
[09/26 12:49:20 visual_prompt]: Epoch 15 / 100: avg data time: 4.85e-02, avg batch time: 0.4911, average train loss: 26.6026
[09/26 12:49:22 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1672, average loss: 19.9696
[09/26 12:49:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 12:49:22 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.924038765061042
[09/26 12:49:29 visual_prompt]: Epoch 16 / 100: avg data time: 4.80e-02, avg batch time: 0.4932, average train loss: 17.4745
[09/26 12:49:30 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1674, average loss: 12.4201
[09/26 12:49:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.50	
[09/26 12:49:30 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.890738003669028
[09/26 12:49:37 visual_prompt]: Epoch 17 / 100: avg data time: 4.82e-02, avg batch time: 0.4910, average train loss: 15.2165
[09/26 12:49:38 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1669, average loss: 18.7448
[09/26 12:49:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 12:49:38 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.851478631379981
[09/26 12:49:45 visual_prompt]: Epoch 18 / 100: avg data time: 5.47e-02, avg batch time: 0.4971, average train loss: 15.5898
[09/26 12:49:47 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1673, average loss: 14.5294
[09/26 12:49:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 12:49:47 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.806308479691594
[09/26 12:49:54 visual_prompt]: Epoch 19 / 100: avg data time: 5.57e-02, avg batch time: 0.4977, average train loss: 14.5588
[09/26 12:49:55 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1670, average loss: 12.4336
[09/26 12:49:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 53.50	
[09/26 12:49:55 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.755282581475768
[09/26 12:50:02 visual_prompt]: Epoch 20 / 100: avg data time: 5.65e-02, avg batch time: 0.5000, average train loss: 15.8323
[09/26 12:50:03 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1667, average loss: 17.2781
[09/26 12:50:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 63.00	
[09/26 12:50:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.698463103929543
[09/26 12:50:10 visual_prompt]: Epoch 21 / 100: avg data time: 5.40e-02, avg batch time: 0.4984, average train loss: 17.3085
[09/26 12:50:12 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1669, average loss: 20.5144
[09/26 12:50:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 12:50:12 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.635919272833938
[09/26 12:50:18 visual_prompt]: Epoch 22 / 100: avg data time: 5.37e-02, avg batch time: 0.4966, average train loss: 17.0277
[09/26 12:50:20 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1671, average loss: 16.9684
[09/26 12:50:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.00	
[09/26 12:50:20 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.567727288213003
[09/26 12:50:27 visual_prompt]: Epoch 23 / 100: avg data time: 5.47e-02, avg batch time: 0.4970, average train loss: 14.4752
[09/26 12:50:28 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1671, average loss: 8.3635
[09/26 12:50:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 63.00	
[09/26 12:50:28 visual_prompt]: Best epoch 23: best metric: 0.155
[09/26 12:50:28 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.493970231495835
[09/26 12:50:35 visual_prompt]: Epoch 24 / 100: avg data time: 4.42e-02, avg batch time: 0.4864, average train loss: 9.2941
[09/26 12:50:37 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1671, average loss: 8.6937
[09/26 12:50:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 12:50:37 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.414737964294634
[09/26 12:50:43 visual_prompt]: Epoch 25 / 100: avg data time: 5.29e-02, avg batch time: 0.4985, average train loss: 8.5643
[09/26 12:50:45 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1674, average loss: 9.3249
[09/26 12:50:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 63.50	
[09/26 12:50:45 visual_prompt]: Training 26 / 100 epoch, with learning rate 9.330127018922195
[09/26 12:50:52 visual_prompt]: Epoch 26 / 100: avg data time: 5.34e-02, avg batch time: 0.4977, average train loss: 10.8600
[09/26 12:50:53 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1673, average loss: 6.2107
[09/26 12:50:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 61.00	
[09/26 12:50:53 visual_prompt]: Best epoch 26: best metric: 0.190
[09/26 12:50:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 9.240240480782129
[09/26 12:51:00 visual_prompt]: Epoch 27 / 100: avg data time: 5.13e-02, avg batch time: 0.4957, average train loss: 9.2016
[09/26 12:51:02 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1670, average loss: 9.6486
[09/26 12:51:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 55.00	
[09/26 12:51:02 visual_prompt]: Training 28 / 100 epoch, with learning rate 9.145187862775208
[09/26 12:51:08 visual_prompt]: Epoch 28 / 100: avg data time: 4.70e-02, avg batch time: 0.4905, average train loss: 7.4365
[09/26 12:51:10 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1671, average loss: 6.2747
[09/26 12:51:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 71.00	
[09/26 12:51:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 9.045084971874736
[09/26 12:51:16 visual_prompt]: Epoch 29 / 100: avg data time: 4.54e-02, avg batch time: 0.4886, average train loss: 6.3683
[09/26 12:51:18 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1673, average loss: 5.5548
[09/26 12:51:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 58.00	
[09/26 12:51:18 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.940053768033609
[09/26 12:51:25 visual_prompt]: Epoch 30 / 100: avg data time: 4.52e-02, avg batch time: 0.4886, average train loss: 7.7027
[09/26 12:51:26 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.1672, average loss: 7.2038
[09/26 12:51:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 52.00	
[09/26 12:51:26 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.83022221559489
[09/26 12:51:33 visual_prompt]: Epoch 31 / 100: avg data time: 4.72e-02, avg batch time: 0.4907, average train loss: 8.0064
[09/26 12:51:35 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1671, average loss: 6.7510
[09/26 12:51:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 61.00	
[09/26 12:51:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.715724127386972
[09/26 12:51:41 visual_prompt]: Epoch 32 / 100: avg data time: 5.36e-02, avg batch time: 0.4970, average train loss: 7.3736
[09/26 12:51:43 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1672, average loss: 7.4137
[09/26 12:51:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 61.50	
[09/26 12:51:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.596699001693256
[09/26 12:51:50 visual_prompt]: Epoch 33 / 100: avg data time: 5.70e-02, avg batch time: 0.5012, average train loss: 8.4016
[09/26 12:51:51 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1672, average loss: 7.5147
[09/26 12:51:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 62.50	
[09/26 12:51:51 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.473291852294986
[09/26 12:51:58 visual_prompt]: Epoch 34 / 100: avg data time: 6.20e-02, avg batch time: 0.5048, average train loss: 5.5064
[09/26 12:52:00 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1672, average loss: 5.1955
[09/26 12:52:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 64.00	
[09/26 12:52:00 visual_prompt]: Training 35 / 100 epoch, with learning rate 8.345653031794292
[09/26 12:52:07 visual_prompt]: Epoch 35 / 100: avg data time: 4.51e-02, avg batch time: 0.4914, average train loss: 4.3437
[09/26 12:52:08 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1671, average loss: 3.9278
[09/26 12:52:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 70.50	
[09/26 12:52:08 visual_prompt]: Training 36 / 100 epoch, with learning rate 8.213938048432697
[09/26 12:52:15 visual_prompt]: Epoch 36 / 100: avg data time: 4.48e-02, avg batch time: 0.4897, average train loss: 3.1643
[09/26 12:52:16 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1671, average loss: 3.9029
[09/26 12:52:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 64.50	
[09/26 12:52:16 visual_prompt]: Training 37 / 100 epoch, with learning rate 8.078307376628292
[09/26 12:52:23 visual_prompt]: Epoch 37 / 100: avg data time: 4.23e-02, avg batch time: 0.4881, average train loss: 2.7584
[09/26 12:52:25 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1676, average loss: 3.0125
[09/26 12:52:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 68.00	
[09/26 12:52:25 visual_prompt]: Best epoch 37: best metric: 0.200
[09/26 12:52:25 visual_prompt]: Training 38 / 100 epoch, with learning rate 7.938926261462366
[09/26 12:52:31 visual_prompt]: Epoch 38 / 100: avg data time: 5.05e-02, avg batch time: 0.4958, average train loss: 2.5938
[09/26 12:52:33 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1670, average loss: 2.4200
[09/26 12:52:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 80.00	
[09/26 12:52:33 visual_prompt]: Best epoch 38: best metric: 0.225
[09/26 12:52:33 visual_prompt]: Training 39 / 100 epoch, with learning rate 7.795964517353734
[09/26 12:52:40 visual_prompt]: Epoch 39 / 100: avg data time: 5.48e-02, avg batch time: 0.4978, average train loss: 2.0670
[09/26 12:52:41 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1671, average loss: 2.7284
[09/26 12:52:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 72.00	
[09/26 12:52:41 visual_prompt]: Training 40 / 100 epoch, with learning rate 7.649596321166024
[09/26 12:52:48 visual_prompt]: Epoch 40 / 100: avg data time: 4.72e-02, avg batch time: 0.4912, average train loss: 1.9819
[09/26 12:52:50 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1673, average loss: 2.1931
[09/26 12:52:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 84.00	
[09/26 12:52:50 visual_prompt]: Training 41 / 100 epoch, with learning rate 7.5
[09/26 12:52:56 visual_prompt]: Epoch 41 / 100: avg data time: 5.08e-02, avg batch time: 0.4947, average train loss: 1.9651
[09/26 12:52:58 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1672, average loss: 2.2160
[09/26 12:52:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 82.50	
[09/26 12:52:58 visual_prompt]: Training 42 / 100 epoch, with learning rate 7.347357813929454
[09/26 12:53:05 visual_prompt]: Epoch 42 / 100: avg data time: 4.41e-02, avg batch time: 0.4874, average train loss: 2.1859
[09/26 12:53:06 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1671, average loss: 2.4963
[09/26 12:53:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 81.50	
[09/26 12:53:06 visual_prompt]: Training 43 / 100 epoch, with learning rate 7.191855733945387
[09/26 12:53:13 visual_prompt]: Epoch 43 / 100: avg data time: 4.78e-02, avg batch time: 0.4920, average train loss: 1.9683
[09/26 12:53:15 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1671, average loss: 2.9272
[09/26 12:53:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 74.00	
[09/26 12:53:15 visual_prompt]: Training 44 / 100 epoch, with learning rate 7.033683215379002
[09/26 12:53:21 visual_prompt]: Epoch 44 / 100: avg data time: 4.96e-02, avg batch time: 0.4947, average train loss: 2.3635
[09/26 12:53:23 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1674, average loss: 2.2592
[09/26 12:53:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 83.00	
[09/26 12:53:23 visual_prompt]: Best epoch 44: best metric: 0.240
[09/26 12:53:23 visual_prompt]: Training 45 / 100 epoch, with learning rate 6.873032967079561
[09/26 12:53:30 visual_prompt]: Epoch 45 / 100: avg data time: 5.13e-02, avg batch time: 0.4961, average train loss: 2.2272
[09/26 12:53:31 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1673, average loss: 3.1332
[09/26 12:53:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 79.50	
[09/26 12:53:31 visual_prompt]: Training 46 / 100 epoch, with learning rate 6.710100716628345
[09/26 12:53:38 visual_prompt]: Epoch 46 / 100: avg data time: 4.53e-02, avg batch time: 0.4903, average train loss: 1.9885
[09/26 12:53:39 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1673, average loss: 2.4530
[09/26 12:53:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 82.00	
[09/26 12:53:40 visual_prompt]: Training 47 / 100 epoch, with learning rate 6.545084971874737
[09/26 12:53:46 visual_prompt]: Epoch 47 / 100: avg data time: 5.59e-02, avg batch time: 0.4994, average train loss: 1.8714
[09/26 12:53:48 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1673, average loss: 2.2761
[09/26 12:53:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 87.00	
[09/26 12:53:48 visual_prompt]: Training 48 / 100 epoch, with learning rate 6.378186779084995
[09/26 12:53:55 visual_prompt]: Epoch 48 / 100: avg data time: 5.21e-02, avg batch time: 0.4975, average train loss: 1.8400
[09/26 12:53:56 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1677, average loss: 2.5430
[09/26 12:53:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 83.50	
[09/26 12:53:56 visual_prompt]: Training 49 / 100 epoch, with learning rate 6.209609477998338
[09/26 12:54:03 visual_prompt]: Epoch 49 / 100: avg data time: 5.95e-02, avg batch time: 0.5028, average train loss: 1.7594
[09/26 12:54:05 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1671, average loss: 2.2448
[09/26 12:54:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 83.50	
[09/26 12:54:05 visual_prompt]: Training 50 / 100 epoch, with learning rate 6.039558454088796
[09/26 12:54:12 visual_prompt]: Epoch 50 / 100: avg data time: 5.81e-02, avg batch time: 0.5007, average train loss: 1.7948
[09/26 12:54:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1673, average loss: 1.9840
[09/26 12:54:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 89.00	
[09/26 12:54:13 visual_prompt]: Training 51 / 100 epoch, with learning rate 5.868240888334652
[09/26 12:54:20 visual_prompt]: Epoch 51 / 100: avg data time: 5.18e-02, avg batch time: 0.4951, average train loss: 1.6031
[09/26 12:54:22 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1666, average loss: 2.0825
[09/26 12:54:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 87.00	
[09/26 12:54:22 visual_prompt]: Training 52 / 100 epoch, with learning rate 5.695865504800327
[09/26 12:54:28 visual_prompt]: Epoch 52 / 100: avg data time: 5.86e-02, avg batch time: 0.5012, average train loss: 1.6399
[09/26 12:54:30 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1670, average loss: 2.0112
[09/26 12:54:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 89.00	
[09/26 12:54:30 visual_prompt]: Training 53 / 100 epoch, with learning rate 5.522642316338268
[09/26 12:54:37 visual_prompt]: Epoch 53 / 100: avg data time: 5.75e-02, avg batch time: 0.5017, average train loss: 1.6550
[09/26 12:54:38 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1674, average loss: 2.0333
[09/26 12:54:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 90.00	
[09/26 12:54:38 visual_prompt]: Best epoch 53: best metric: 0.255
[09/26 12:54:38 visual_prompt]: Training 54 / 100 epoch, with learning rate 5.3487823687206255
[09/26 12:54:45 visual_prompt]: Epoch 54 / 100: avg data time: 5.09e-02, avg batch time: 0.4952, average train loss: 1.6496
[09/26 12:54:47 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1671, average loss: 1.9997
[09/26 12:54:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 91.00	
[09/26 12:54:47 visual_prompt]: Training 55 / 100 epoch, with learning rate 5.174497483512505
[09/26 12:54:54 visual_prompt]: Epoch 55 / 100: avg data time: 5.77e-02, avg batch time: 0.5006, average train loss: 1.5937
[09/26 12:54:55 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1675, average loss: 2.4432
[09/26 12:54:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 84.50	
[09/26 12:54:55 visual_prompt]: Training 56 / 100 epoch, with learning rate 5.0
[09/26 12:55:02 visual_prompt]: Epoch 56 / 100: avg data time: 5.79e-02, avg batch time: 0.5017, average train loss: 1.8052
[09/26 12:55:04 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1677, average loss: 2.0642
[09/26 12:55:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 89.50	
[09/26 12:55:04 visual_prompt]: Training 57 / 100 epoch, with learning rate 4.8255025164874965
[09/26 12:55:10 visual_prompt]: Epoch 57 / 100: avg data time: 4.58e-02, avg batch time: 0.4912, average train loss: 1.8415
[09/26 12:55:12 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1677, average loss: 2.1057
[09/26 12:55:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 88.50	
[09/26 12:55:12 visual_prompt]: Training 58 / 100 epoch, with learning rate 4.651217631279374
[09/26 12:55:19 visual_prompt]: Epoch 58 / 100: avg data time: 5.49e-02, avg batch time: 0.4981, average train loss: 1.6260
[09/26 12:55:20 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1674, average loss: 2.1426
[09/26 12:55:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 90.50	
[09/26 12:55:20 visual_prompt]: Best epoch 58: best metric: 0.290
[09/26 12:55:20 visual_prompt]: Training 59 / 100 epoch, with learning rate 4.477357683661733
[09/26 12:55:27 visual_prompt]: Epoch 59 / 100: avg data time: 5.50e-02, avg batch time: 0.4983, average train loss: 1.5811
[09/26 12:55:28 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1674, average loss: 2.2900
[09/26 12:55:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 88.00	
[09/26 12:55:28 visual_prompt]: Training 60 / 100 epoch, with learning rate 4.3041344951996745
[09/26 12:55:35 visual_prompt]: Epoch 60 / 100: avg data time: 4.74e-02, avg batch time: 0.4938, average train loss: 1.7274
[09/26 12:55:37 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1674, average loss: 2.2438
[09/26 12:55:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 86.00	
[09/26 12:55:37 visual_prompt]: Training 61 / 100 epoch, with learning rate 4.131759111665349
[09/26 12:55:44 visual_prompt]: Epoch 61 / 100: avg data time: 6.21e-02, avg batch time: 0.5057, average train loss: 1.7122
[09/26 12:55:45 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1672, average loss: 2.1335
[09/26 12:55:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 90.50	
[09/26 12:55:45 visual_prompt]: Training 62 / 100 epoch, with learning rate 3.960441545911204
[09/26 12:55:52 visual_prompt]: Epoch 62 / 100: avg data time: 4.68e-02, avg batch time: 0.4920, average train loss: 1.5137
[09/26 12:55:53 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1669, average loss: 2.1897
[09/26 12:55:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 87.50	
[09/26 12:55:53 visual_prompt]: Training 63 / 100 epoch, with learning rate 3.790390522001662
[09/26 12:56:00 visual_prompt]: Epoch 63 / 100: avg data time: 5.36e-02, avg batch time: 0.4973, average train loss: 1.6083
[09/26 12:56:02 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1671, average loss: 2.2663
[09/26 12:56:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 88.50	
[09/26 12:56:02 visual_prompt]: Training 64 / 100 epoch, with learning rate 3.6218132209150045
[09/26 12:56:09 visual_prompt]: Epoch 64 / 100: avg data time: 6.44e-02, avg batch time: 0.5080, average train loss: 1.5769
[09/26 12:56:10 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1673, average loss: 1.9896
[09/26 12:56:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 92.00	
[09/26 12:56:10 visual_prompt]: Training 65 / 100 epoch, with learning rate 3.454915028125263
[09/26 12:56:17 visual_prompt]: Epoch 65 / 100: avg data time: 4.58e-02, avg batch time: 0.4897, average train loss: 1.4404
[09/26 12:56:18 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1669, average loss: 2.1915
[09/26 12:56:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 90.00	
[09/26 12:56:18 visual_prompt]: Training 66 / 100 epoch, with learning rate 3.289899283371657
[09/26 12:56:25 visual_prompt]: Epoch 66 / 100: avg data time: 6.04e-02, avg batch time: 0.5031, average train loss: 1.3953
[09/26 12:56:27 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1676, average loss: 2.1989
[09/26 12:56:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 91.50	
[09/26 12:56:27 visual_prompt]: Training 67 / 100 epoch, with learning rate 3.1269670329204398
[09/26 12:56:34 visual_prompt]: Epoch 67 / 100: avg data time: 4.54e-02, avg batch time: 0.4922, average train loss: 1.3994
[09/26 12:56:35 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1672, average loss: 2.0629
[09/26 12:56:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 91.50	
[09/26 12:56:35 visual_prompt]: Training 68 / 100 epoch, with learning rate 2.9663167846209997
[09/26 12:56:42 visual_prompt]: Epoch 68 / 100: avg data time: 4.74e-02, avg batch time: 0.4932, average train loss: 1.3771
[09/26 12:56:43 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1674, average loss: 1.9281
[09/26 12:56:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 92.00	
[09/26 12:56:43 visual_prompt]: Training 69 / 100 epoch, with learning rate 2.8081442660546125
[09/26 12:56:50 visual_prompt]: Epoch 69 / 100: avg data time: 4.55e-02, avg batch time: 0.4895, average train loss: 1.4213
[09/26 12:56:52 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1675, average loss: 2.1059
[09/26 12:56:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 92.00	
[09/26 12:56:52 visual_prompt]: Training 70 / 100 epoch, with learning rate 2.6526421860705476
[09/26 12:56:58 visual_prompt]: Epoch 70 / 100: avg data time: 4.72e-02, avg batch time: 0.4926, average train loss: 1.2814
[09/26 12:57:00 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1675, average loss: 2.1307
[09/26 12:57:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 91.00	
[09/26 12:57:00 visual_prompt]: Training 71 / 100 epoch, with learning rate 2.500000000000001
[09/26 12:57:07 visual_prompt]: Epoch 71 / 100: avg data time: 4.65e-02, avg batch time: 0.4920, average train loss: 1.2197
[09/26 12:57:08 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1676, average loss: 2.0184
[09/26 12:57:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 92.50	
[09/26 12:57:08 visual_prompt]: Training 72 / 100 epoch, with learning rate 2.350403678833976
[09/26 12:57:15 visual_prompt]: Epoch 72 / 100: avg data time: 5.03e-02, avg batch time: 0.4961, average train loss: 1.2702
[09/26 12:57:16 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1674, average loss: 2.0062
[09/26 12:57:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 92.50	
[09/26 12:57:16 visual_prompt]: Best epoch 72: best metric: 0.295
[09/26 12:57:16 visual_prompt]: Training 73 / 100 epoch, with learning rate 2.2040354826462667
[09/26 12:57:23 visual_prompt]: Epoch 73 / 100: avg data time: 4.50e-02, avg batch time: 0.4889, average train loss: 1.2664
[09/26 12:57:25 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1673, average loss: 2.0202
[09/26 12:57:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 12:57:25 visual_prompt]: Best epoch 73: best metric: 0.305
[09/26 12:57:25 visual_prompt]: Training 74 / 100 epoch, with learning rate 2.061073738537635
[09/26 12:57:31 visual_prompt]: Epoch 74 / 100: avg data time: 4.54e-02, avg batch time: 0.4892, average train loss: 1.1758
[09/26 12:57:33 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1673, average loss: 2.0544
[09/26 12:57:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 92.00	
[09/26 12:57:33 visual_prompt]: Training 75 / 100 epoch, with learning rate 1.9216926233717087
[09/26 12:57:40 visual_prompt]: Epoch 75 / 100: avg data time: 5.77e-02, avg batch time: 0.5007, average train loss: 1.1677
[09/26 12:57:41 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1676, average loss: 2.1037
[09/26 12:57:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 91.50	
[09/26 12:57:41 visual_prompt]: Training 76 / 100 epoch, with learning rate 1.7860619515673033
[09/26 12:57:48 visual_prompt]: Epoch 76 / 100: avg data time: 5.67e-02, avg batch time: 0.5013, average train loss: 1.1382
[09/26 12:57:50 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1675, average loss: 2.1324
[09/26 12:57:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 91.50	
[09/26 12:57:50 visual_prompt]: Training 77 / 100 epoch, with learning rate 1.6543469682057106
[09/26 12:57:56 visual_prompt]: Epoch 77 / 100: avg data time: 4.83e-02, avg batch time: 0.4933, average train loss: 1.2461
[09/26 12:57:58 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1677, average loss: 2.0803
[09/26 12:57:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 94.00	
[09/26 12:57:58 visual_prompt]: Best epoch 77: best metric: 0.320
[09/26 12:57:58 visual_prompt]: Training 78 / 100 epoch, with learning rate 1.5267081477050133
[09/26 12:58:05 visual_prompt]: Epoch 78 / 100: avg data time: 5.05e-02, avg batch time: 0.4957, average train loss: 1.1595
[09/26 12:58:06 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1674, average loss: 2.0300
[09/26 12:58:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 91.00	
[09/26 12:58:06 visual_prompt]: Training 79 / 100 epoch, with learning rate 1.403300998306745
[09/26 12:58:13 visual_prompt]: Epoch 79 / 100: avg data time: 5.01e-02, avg batch time: 0.4966, average train loss: 1.1646
[09/26 12:58:15 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1675, average loss: 2.0421
[09/26 12:58:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 91.50	
[09/26 12:58:15 visual_prompt]: Training 80 / 100 epoch, with learning rate 1.2842758726130281
[09/26 12:58:22 visual_prompt]: Epoch 80 / 100: avg data time: 5.77e-02, avg batch time: 0.5019, average train loss: 1.0810
[09/26 12:58:23 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1675, average loss: 1.9996
[09/26 12:58:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.50	
[09/26 12:58:23 visual_prompt]: Training 81 / 100 epoch, with learning rate 1.1697777844051105
[09/26 12:58:30 visual_prompt]: Epoch 81 / 100: avg data time: 5.97e-02, avg batch time: 0.5039, average train loss: 1.0291
[09/26 12:58:32 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1675, average loss: 2.0271
[09/26 12:58:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.50	
[09/26 12:58:32 visual_prompt]: Training 82 / 100 epoch, with learning rate 1.0599462319663906
[09/26 12:58:38 visual_prompt]: Epoch 82 / 100: avg data time: 4.45e-02, avg batch time: 0.4876, average train loss: 1.0192
[09/26 12:58:40 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1675, average loss: 2.0443
[09/26 12:58:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 93.00	
[09/26 12:58:40 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.9549150281252633
[09/26 12:58:46 visual_prompt]: Epoch 83 / 100: avg data time: 4.63e-02, avg batch time: 0.4903, average train loss: 0.9964
[09/26 12:58:48 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1675, average loss: 2.0713
[09/26 12:58:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 92.50	
[09/26 12:58:48 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.8548121372247919
[09/26 12:58:55 visual_prompt]: Epoch 84 / 100: avg data time: 4.80e-02, avg batch time: 0.4929, average train loss: 0.9560
[09/26 12:58:56 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1675, average loss: 2.0717
[09/26 12:58:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 92.00	
[09/26 12:58:56 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.7597595192178702
[09/26 12:59:03 visual_prompt]: Epoch 85 / 100: avg data time: 4.70e-02, avg batch time: 0.4908, average train loss: 0.9613
[09/26 12:59:05 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1678, average loss: 2.0923
[09/26 12:59:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 91.50	
[09/26 12:59:05 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.6698729810778065
[09/26 12:59:11 visual_prompt]: Epoch 86 / 100: avg data time: 5.40e-02, avg batch time: 0.4981, average train loss: 0.9637
[09/26 12:59:13 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1676, average loss: 2.0868
[09/26 12:59:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.50	
[09/26 12:59:13 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.5852620357053651
[09/26 12:59:20 visual_prompt]: Epoch 87 / 100: avg data time: 5.54e-02, avg batch time: 0.5000, average train loss: 0.9476
[09/26 12:59:22 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1672, average loss: 2.0831
[09/26 12:59:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 93.50	
[09/26 12:59:22 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.5060297685041659
[09/26 12:59:28 visual_prompt]: Epoch 88 / 100: avg data time: 4.49e-02, avg batch time: 0.4893, average train loss: 0.9560
[09/26 12:59:30 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1674, average loss: 2.0711
[09/26 12:59:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 91.50	
[09/26 12:59:30 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.4322727117869951
[09/26 12:59:37 visual_prompt]: Epoch 89 / 100: avg data time: 5.72e-02, avg batch time: 0.5015, average train loss: 0.9076
[09/26 12:59:38 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1674, average loss: 2.0593
[09/26 12:59:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 93.00	
[09/26 12:59:38 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.36408072716606343
[09/26 12:59:45 visual_prompt]: Epoch 90 / 100: avg data time: 5.58e-02, avg batch time: 0.5000, average train loss: 0.8973
[09/26 12:59:47 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1672, average loss: 2.0743
[09/26 12:59:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 92.00	
[09/26 12:59:47 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.3015368960704584
[09/26 12:59:53 visual_prompt]: Epoch 91 / 100: avg data time: 6.05e-02, avg batch time: 0.5035, average train loss: 0.9140
[09/26 12:59:55 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1674, average loss: 2.0631
[09/26 12:59:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.50	
[09/26 12:59:55 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.24471741852423234
[09/26 13:00:02 visual_prompt]: Epoch 92 / 100: avg data time: 5.46e-02, avg batch time: 0.4981, average train loss: 0.9113
[09/26 13:00:03 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1676, average loss: 2.0532
[09/26 13:00:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 93.50	
[09/26 13:00:03 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.19369152030840553
[09/26 13:00:10 visual_prompt]: Epoch 93 / 100: avg data time: 5.00e-02, avg batch time: 0.4927, average train loss: 0.8972
[09/26 13:00:12 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1677, average loss: 2.0609
[09/26 13:00:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.00	
[09/26 13:00:12 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.14852136862001764
[09/26 13:00:18 visual_prompt]: Epoch 94 / 100: avg data time: 4.65e-02, avg batch time: 0.4917, average train loss: 0.8896
[09/26 13:00:20 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.1667, average loss: 2.0591
[09/26 13:00:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.50	
[09/26 13:00:20 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.10926199633097156
[09/26 13:00:27 visual_prompt]: Epoch 95 / 100: avg data time: 5.66e-02, avg batch time: 0.5027, average train loss: 0.8763
[09/26 13:00:29 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1669, average loss: 2.0827
[09/26 13:00:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 13:00:29 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0759612349389599
[09/26 13:00:35 visual_prompt]: Epoch 96 / 100: avg data time: 5.42e-02, avg batch time: 0.4992, average train loss: 0.8767
[09/26 13:00:37 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1673, average loss: 2.0703
[09/26 13:00:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.00	
[09/26 13:00:37 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.04865965629214819
[09/26 13:00:44 visual_prompt]: Epoch 97 / 100: avg data time: 5.93e-02, avg batch time: 0.5018, average train loss: 0.8784
[09/26 13:00:45 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1676, average loss: 2.0746
[09/26 13:00:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 13:00:45 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.02739052315863355
[09/26 13:00:52 visual_prompt]: Epoch 98 / 100: avg data time: 6.00e-02, avg batch time: 0.5033, average train loss: 0.8968
[09/26 13:00:54 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.1671, average loss: 2.0778
[09/26 13:00:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 92.00	
[09/26 13:00:54 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.012179748700879012
[09/26 13:01:01 visual_prompt]: Epoch 99 / 100: avg data time: 5.44e-02, avg batch time: 0.4974, average train loss: 0.8983
[09/26 13:01:02 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1672, average loss: 2.0765
[09/26 13:01:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.00	
[09/26 13:01:02 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0030458649045211894
[09/26 13:01:09 visual_prompt]: Epoch 100 / 100: avg data time: 5.38e-02, avg batch time: 0.4983, average train loss: 0.8552
[09/26 13:01:11 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1668, average loss: 2.0765
[09/26 13:01:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.00	
[09/26 13:01:11 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:01:11 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:01:11 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:01:11 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:01:11 visual_prompt]: Training with config:
[09/26 13:01:11 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:01:11 visual_prompt]: Loading training data...
[09/26 13:01:11 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 13:01:12 visual_prompt]: Number of images: 800
[09/26 13:01:12 visual_prompt]: Number of classes: 9 / 9
[09/26 13:01:12 visual_prompt]: Loading validation data...
[09/26 13:01:12 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 13:01:12 visual_prompt]: Number of images: 200
[09/26 13:01:12 visual_prompt]: Number of classes: 9 / 9
[09/26 13:01:12 visual_prompt]: Constructing models...
[09/26 13:01:14 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 13:01:14 visual_prompt]: tuned percent:0.542
[09/26 13:01:15 visual_prompt]: Device used for model: 0
[09/26 13:01:15 visual_prompt]: Setting up Evaluator...
[09/26 13:01:15 visual_prompt]: Setting up Trainer...
[09/26 13:01:15 visual_prompt]: 	Setting up the optimizer...
[09/26 13:01:15 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:01:21 visual_prompt]: Epoch 1 / 100: avg data time: 4.59e-02, avg batch time: 0.4908, average train loss: 2.8657
[09/26 13:01:23 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1669, average loss: 2.9516
[09/26 13:01:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 13:01:23 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 13:01:23 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 13:01:30 visual_prompt]: Epoch 2 / 100: avg data time: 5.43e-02, avg batch time: 0.4968, average train loss: 4.8836
[09/26 13:01:31 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1667, average loss: 4.4083
[09/26 13:01:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 13:01:31 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 13:01:38 visual_prompt]: Epoch 3 / 100: avg data time: 5.01e-02, avg batch time: 0.4926, average train loss: 3.8255
[09/26 13:01:39 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1669, average loss: 2.7505
[09/26 13:01:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.50	
[09/26 13:01:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 13:01:46 visual_prompt]: Epoch 4 / 100: avg data time: 4.82e-02, avg batch time: 0.4915, average train loss: 2.7914
[09/26 13:01:48 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1671, average loss: 2.9795
[09/26 13:01:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.50	
[09/26 13:01:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 13:01:54 visual_prompt]: Epoch 5 / 100: avg data time: 4.39e-02, avg batch time: 0.4877, average train loss: 2.6410
[09/26 13:01:56 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1666, average loss: 2.3136
[09/26 13:01:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 61.50	
[09/26 13:01:56 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 13:02:02 visual_prompt]: Epoch 6 / 100: avg data time: 4.61e-02, avg batch time: 0.4891, average train loss: 2.6503
[09/26 13:02:04 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1671, average loss: 3.2432
[09/26 13:02:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.00	
[09/26 13:02:04 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 13:02:11 visual_prompt]: Epoch 7 / 100: avg data time: 4.76e-02, avg batch time: 0.4905, average train loss: 3.0045
[09/26 13:02:12 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1672, average loss: 2.7282
[09/26 13:02:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 13:02:12 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 13:02:19 visual_prompt]: Epoch 8 / 100: avg data time: 4.76e-02, avg batch time: 0.4907, average train loss: 4.3942
[09/26 13:02:20 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1673, average loss: 4.6412
[09/26 13:02:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 13:02:20 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 13:02:27 visual_prompt]: Epoch 9 / 100: avg data time: 5.70e-02, avg batch time: 0.5003, average train loss: 7.9918
[09/26 13:02:29 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1670, average loss: 10.2375
[09/26 13:02:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.50	
[09/26 13:02:29 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 13:02:36 visual_prompt]: Epoch 10 / 100: avg data time: 5.67e-02, avg batch time: 0.5001, average train loss: 17.7832
[09/26 13:02:37 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1673, average loss: 9.8843
[09/26 13:02:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/26 13:02:37 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 13:02:44 visual_prompt]: Epoch 11 / 100: avg data time: 5.42e-02, avg batch time: 0.4976, average train loss: 25.5814
[09/26 13:02:45 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1668, average loss: 23.3813
[09/26 13:02:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 13:02:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 13:02:52 visual_prompt]: Epoch 12 / 100: avg data time: 5.64e-02, avg batch time: 0.5002, average train loss: 25.7127
[09/26 13:02:54 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1670, average loss: 29.2987
[09/26 13:02:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.50	
[09/26 13:02:54 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 13:03:00 visual_prompt]: Epoch 13 / 100: avg data time: 4.57e-02, avg batch time: 0.4892, average train loss: 21.0295
[09/26 13:03:02 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1669, average loss: 24.1524
[09/26 13:03:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 13:03:02 visual_prompt]: Best epoch 13: best metric: 0.145
[09/26 13:03:02 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 13:03:09 visual_prompt]: Epoch 14 / 100: avg data time: 5.46e-02, avg batch time: 0.4978, average train loss: 17.8769
[09/26 13:03:10 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1665, average loss: 23.6349
[09/26 13:03:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 13:03:10 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 13:03:17 visual_prompt]: Epoch 15 / 100: avg data time: 5.17e-02, avg batch time: 0.4937, average train loss: 20.0590
[09/26 13:03:19 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1669, average loss: 17.3176
[09/26 13:03:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 13:03:19 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 13:03:25 visual_prompt]: Epoch 16 / 100: avg data time: 4.35e-02, avg batch time: 0.4875, average train loss: 22.9722
[09/26 13:03:27 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1671, average loss: 36.8228
[09/26 13:03:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 50.50	
[09/26 13:03:27 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 13:03:34 visual_prompt]: Epoch 17 / 100: avg data time: 5.35e-02, avg batch time: 0.4960, average train loss: 17.7804
[09/26 13:03:35 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1670, average loss: 22.5191
[09/26 13:03:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 13:03:35 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 13:03:42 visual_prompt]: Epoch 18 / 100: avg data time: 4.52e-02, avg batch time: 0.4891, average train loss: 16.4971
[09/26 13:03:43 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1668, average loss: 20.3569
[09/26 13:03:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 53.00	
[09/26 13:03:43 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 13:03:50 visual_prompt]: Epoch 19 / 100: avg data time: 4.41e-02, avg batch time: 0.4880, average train loss: 19.5762
[09/26 13:03:52 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1671, average loss: 23.5179
[09/26 13:03:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.50	
[09/26 13:03:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 13:03:58 visual_prompt]: Epoch 20 / 100: avg data time: 5.53e-02, avg batch time: 0.4984, average train loss: 18.5435
[09/26 13:04:00 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1672, average loss: 20.7743
[09/26 13:04:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 57.50	
[09/26 13:04:00 visual_prompt]: Best epoch 20: best metric: 0.160
[09/26 13:04:00 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 13:04:07 visual_prompt]: Epoch 21 / 100: avg data time: 5.58e-02, avg batch time: 0.4980, average train loss: 25.1579
[09/26 13:04:08 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1675, average loss: 20.4998
[09/26 13:04:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 13:04:08 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 13:04:15 visual_prompt]: Epoch 22 / 100: avg data time: 5.07e-02, avg batch time: 0.4943, average train loss: 23.5878
[09/26 13:04:17 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1675, average loss: 30.2911
[09/26 13:04:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 13:04:17 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 13:04:23 visual_prompt]: Epoch 23 / 100: avg data time: 4.90e-02, avg batch time: 0.4925, average train loss: 19.5318
[09/26 13:04:25 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1673, average loss: 24.2659
[09/26 13:04:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.00	
[09/26 13:04:25 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 13:04:32 visual_prompt]: Epoch 24 / 100: avg data time: 4.33e-02, avg batch time: 0.4866, average train loss: 25.5054
[09/26 13:04:33 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1671, average loss: 11.4653
[09/26 13:04:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 13:04:33 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 13:04:40 visual_prompt]: Epoch 25 / 100: avg data time: 4.78e-02, avg batch time: 0.4922, average train loss: 24.1690
[09/26 13:04:42 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.1672, average loss: 36.0868
[09/26 13:04:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 13:04:42 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 13:04:48 visual_prompt]: Epoch 26 / 100: avg data time: 4.52e-02, avg batch time: 0.4896, average train loss: 29.3009
[09/26 13:04:50 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1672, average loss: 26.1553
[09/26 13:04:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 13:04:50 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 13:04:57 visual_prompt]: Epoch 27 / 100: avg data time: 4.75e-02, avg batch time: 0.4917, average train loss: 21.6694
[09/26 13:04:58 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1672, average loss: 12.3662
[09/26 13:04:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 13:04:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 13:05:05 visual_prompt]: Epoch 28 / 100: avg data time: 5.73e-02, avg batch time: 0.5014, average train loss: 13.0744
[09/26 13:05:07 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1672, average loss: 18.3406
[09/26 13:05:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 13:05:07 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 13:05:13 visual_prompt]: Epoch 29 / 100: avg data time: 4.65e-02, avg batch time: 0.4898, average train loss: 14.1389
[09/26 13:05:15 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1673, average loss: 15.0333
[09/26 13:05:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 13:05:15 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 13:05:22 visual_prompt]: Epoch 30 / 100: avg data time: 5.40e-02, avg batch time: 0.4977, average train loss: 12.4061
[09/26 13:05:23 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1671, average loss: 13.8193
[09/26 13:05:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 13:05:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 13:05:30 visual_prompt]: Epoch 31 / 100: avg data time: 4.62e-02, avg batch time: 0.4901, average train loss: 21.7295
[09/26 13:05:31 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1671, average loss: 13.8881
[09/26 13:05:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 13:05:31 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 13:05:38 visual_prompt]: Epoch 32 / 100: avg data time: 5.32e-02, avg batch time: 0.4962, average train loss: 15.0993
[09/26 13:05:40 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1669, average loss: 19.1569
[09/26 13:05:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 13:05:40 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 13:05:47 visual_prompt]: Epoch 33 / 100: avg data time: 6.71e-02, avg batch time: 0.5099, average train loss: 12.8193
[09/26 13:05:48 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1666, average loss: 12.1822
[09/26 13:05:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 13:05:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 13:05:55 visual_prompt]: Epoch 34 / 100: avg data time: 4.37e-02, avg batch time: 0.4858, average train loss: 14.1124
[09/26 13:05:57 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1666, average loss: 8.6080
[09/26 13:05:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/26 13:05:57 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 13:06:04 visual_prompt]: Epoch 35 / 100: avg data time: 5.34e-02, avg batch time: 0.4968, average train loss: 11.6042
[09/26 13:06:05 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.1672, average loss: 11.8281
[09/26 13:06:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 13:06:05 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 13:06:12 visual_prompt]: Epoch 36 / 100: avg data time: 4.87e-02, avg batch time: 0.4940, average train loss: 15.6366
[09/26 13:06:13 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1668, average loss: 12.0942
[09/26 13:06:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.50	
[09/26 13:06:13 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 13:06:20 visual_prompt]: Epoch 37 / 100: avg data time: 4.85e-02, avg batch time: 0.4916, average train loss: 18.2179
[09/26 13:06:22 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1671, average loss: 15.2267
[09/26 13:06:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/26 13:06:22 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 13:06:29 visual_prompt]: Epoch 38 / 100: avg data time: 6.47e-02, avg batch time: 0.5077, average train loss: 12.1342
[09/26 13:06:30 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1669, average loss: 15.2956
[09/26 13:06:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 13:06:30 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 13:06:37 visual_prompt]: Epoch 39 / 100: avg data time: 5.84e-02, avg batch time: 0.5011, average train loss: 13.0741
[09/26 13:06:39 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1672, average loss: 8.8783
[09/26 13:06:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.00	
[09/26 13:06:39 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 13:06:46 visual_prompt]: Epoch 40 / 100: avg data time: 5.00e-02, avg batch time: 0.4951, average train loss: 10.5547
[09/26 13:06:47 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1671, average loss: 8.0680
[09/26 13:06:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 13:06:47 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 13:06:54 visual_prompt]: Epoch 41 / 100: avg data time: 6.79e-02, avg batch time: 0.5117, average train loss: 11.9090
[09/26 13:06:56 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1673, average loss: 19.4245
[09/26 13:06:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 13:06:56 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 13:07:03 visual_prompt]: Epoch 42 / 100: avg data time: 5.65e-02, avg batch time: 0.5008, average train loss: 16.7426
[09/26 13:07:04 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1673, average loss: 22.8744
[09/26 13:07:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 13:07:04 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 13:07:11 visual_prompt]: Epoch 43 / 100: avg data time: 5.68e-02, avg batch time: 0.5006, average train loss: 21.1002
[09/26 13:07:13 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1671, average loss: 13.7981
[09/26 13:07:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 13:07:13 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 13:07:19 visual_prompt]: Epoch 44 / 100: avg data time: 4.47e-02, avg batch time: 0.4885, average train loss: 12.2115
[09/26 13:07:21 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1667, average loss: 12.5297
[09/26 13:07:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 13:07:21 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 13:07:28 visual_prompt]: Epoch 45 / 100: avg data time: 5.43e-02, avg batch time: 0.4971, average train loss: 12.4134
[09/26 13:07:29 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1676, average loss: 20.7249
[09/26 13:07:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.00	
[09/26 13:07:29 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 13:07:36 visual_prompt]: Epoch 46 / 100: avg data time: 5.51e-02, avg batch time: 0.4998, average train loss: 11.5797
[09/26 13:07:38 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1673, average loss: 6.9306
[09/26 13:07:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 13:07:38 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 13:07:44 visual_prompt]: Epoch 47 / 100: avg data time: 5.83e-02, avg batch time: 0.5006, average train loss: 7.9309
[09/26 13:07:46 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1674, average loss: 6.8913
[09/26 13:07:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 13:07:46 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 13:07:53 visual_prompt]: Epoch 48 / 100: avg data time: 5.92e-02, avg batch time: 0.5035, average train loss: 10.5901
[09/26 13:07:55 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1675, average loss: 11.3888
[09/26 13:07:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 56.50	
[09/26 13:07:55 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 13:08:01 visual_prompt]: Epoch 49 / 100: avg data time: 5.90e-02, avg batch time: 0.5019, average train loss: 11.4907
[09/26 13:08:03 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1675, average loss: 35.1112
[09/26 13:08:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 13:08:03 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 13:08:10 visual_prompt]: Epoch 50 / 100: avg data time: 6.07e-02, avg batch time: 0.5034, average train loss: 13.3809
[09/26 13:08:12 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1669, average loss: 15.1223
[09/26 13:08:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/26 13:08:12 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 13:08:18 visual_prompt]: Epoch 51 / 100: avg data time: 5.47e-02, avg batch time: 0.4977, average train loss: 9.9835
[09/26 13:08:20 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1664, average loss: 6.6551
[09/26 13:08:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 13:08:20 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 13:08:27 visual_prompt]: Epoch 52 / 100: avg data time: 5.32e-02, avg batch time: 0.4968, average train loss: 7.1173
[09/26 13:08:28 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1674, average loss: 5.1841
[09/26 13:08:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.00	
[09/26 13:08:28 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 13:08:35 visual_prompt]: Epoch 53 / 100: avg data time: 5.76e-02, avg batch time: 0.5009, average train loss: 7.5154
[09/26 13:08:37 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1675, average loss: 8.6915
[09/26 13:08:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 13:08:37 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 13:08:44 visual_prompt]: Epoch 54 / 100: avg data time: 6.02e-02, avg batch time: 0.5035, average train loss: 6.4483
[09/26 13:08:45 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1669, average loss: 5.0326
[09/26 13:08:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 61.50	
[09/26 13:08:45 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 13:08:52 visual_prompt]: Epoch 55 / 100: avg data time: 5.26e-02, avg batch time: 0.4950, average train loss: 6.3227
[09/26 13:08:54 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1668, average loss: 4.2817
[09/26 13:08:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 13:08:54 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 13:09:00 visual_prompt]: Epoch 56 / 100: avg data time: 4.66e-02, avg batch time: 0.4891, average train loss: 5.0574
[09/26 13:09:02 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1674, average loss: 7.0418
[09/26 13:09:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 13:09:02 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 13:09:09 visual_prompt]: Epoch 57 / 100: avg data time: 6.39e-02, avg batch time: 0.5074, average train loss: 5.8853
[09/26 13:09:10 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1673, average loss: 4.5370
[09/26 13:09:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.00	
[09/26 13:09:10 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 13:09:17 visual_prompt]: Epoch 58 / 100: avg data time: 5.00e-02, avg batch time: 0.4945, average train loss: 5.6821
[09/26 13:09:19 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1676, average loss: 8.7679
[09/26 13:09:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 13:09:19 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 13:09:26 visual_prompt]: Epoch 59 / 100: avg data time: 5.71e-02, avg batch time: 0.4999, average train loss: 6.5997
[09/26 13:09:27 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1671, average loss: 6.3684
[09/26 13:09:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 54.50	
[09/26 13:09:27 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 13:09:34 visual_prompt]: Epoch 60 / 100: avg data time: 5.55e-02, avg batch time: 0.4985, average train loss: 5.8710
[09/26 13:09:35 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1678, average loss: 10.7257
[09/26 13:09:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/26 13:09:35 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 13:09:42 visual_prompt]: Epoch 61 / 100: avg data time: 4.56e-02, avg batch time: 0.4886, average train loss: 7.3053
[09/26 13:09:44 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1672, average loss: 6.2577
[09/26 13:09:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 13:09:44 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 13:09:50 visual_prompt]: Epoch 62 / 100: avg data time: 4.92e-02, avg batch time: 0.4910, average train loss: 6.4259
[09/26 13:09:52 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1676, average loss: 11.3862
[09/26 13:09:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 13:09:52 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 13:09:59 visual_prompt]: Epoch 63 / 100: avg data time: 4.81e-02, avg batch time: 0.4905, average train loss: 7.5624
[09/26 13:10:00 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1674, average loss: 7.0098
[09/26 13:10:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.50	
[09/26 13:10:00 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 13:10:07 visual_prompt]: Epoch 64 / 100: avg data time: 4.88e-02, avg batch time: 0.4933, average train loss: 5.2431
[09/26 13:10:08 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1674, average loss: 3.6062
[09/26 13:10:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 13:10:08 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 13:10:15 visual_prompt]: Epoch 65 / 100: avg data time: 4.63e-02, avg batch time: 0.4884, average train loss: 4.0079
[09/26 13:10:17 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1675, average loss: 3.0207
[09/26 13:10:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 13:10:17 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 13:10:23 visual_prompt]: Epoch 66 / 100: avg data time: 4.61e-02, avg batch time: 0.4898, average train loss: 2.7415
[09/26 13:10:25 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1672, average loss: 2.4066
[09/26 13:10:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 13:10:25 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 13:10:32 visual_prompt]: Epoch 67 / 100: avg data time: 4.90e-02, avg batch time: 0.4914, average train loss: 2.6512
[09/26 13:10:33 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1674, average loss: 3.2039
[09/26 13:10:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.00	
[09/26 13:10:33 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 13:10:40 visual_prompt]: Epoch 68 / 100: avg data time: 4.95e-02, avg batch time: 0.4936, average train loss: 2.9678
[09/26 13:10:41 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1672, average loss: 2.5122
[09/26 13:10:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 13:10:41 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 13:10:48 visual_prompt]: Epoch 69 / 100: avg data time: 4.72e-02, avg batch time: 0.4908, average train loss: 2.8417
[09/26 13:10:50 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1675, average loss: 3.4735
[09/26 13:10:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 13:10:50 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 13:10:56 visual_prompt]: Epoch 70 / 100: avg data time: 5.57e-02, avg batch time: 0.4994, average train loss: 3.3086
[09/26 13:10:58 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1678, average loss: 2.3812
[09/26 13:10:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 13:10:58 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 13:11:05 visual_prompt]: Epoch 71 / 100: avg data time: 5.14e-02, avg batch time: 0.4940, average train loss: 2.3431
[09/26 13:11:06 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1676, average loss: 2.4720
[09/26 13:11:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 57.00	
[09/26 13:11:06 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 13:11:13 visual_prompt]: Epoch 72 / 100: avg data time: 4.62e-02, avg batch time: 0.4906, average train loss: 2.3999
[09/26 13:11:15 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1669, average loss: 2.4970
[09/26 13:11:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.00	
[09/26 13:11:15 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 13:11:21 visual_prompt]: Epoch 73 / 100: avg data time: 4.64e-02, avg batch time: 0.4901, average train loss: 2.3521
[09/26 13:11:23 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1670, average loss: 2.2702
[09/26 13:11:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 13:11:23 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 13:11:29 visual_prompt]: Epoch 74 / 100: avg data time: 4.96e-02, avg batch time: 0.4923, average train loss: 2.2698
[09/26 13:11:31 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1671, average loss: 2.2878
[09/26 13:11:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 13:11:31 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 13:11:38 visual_prompt]: Epoch 75 / 100: avg data time: 4.91e-02, avg batch time: 0.4943, average train loss: 2.2693
[09/26 13:11:39 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1674, average loss: 2.2874
[09/26 13:11:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 55.00	
[09/26 13:11:39 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 13:11:46 visual_prompt]: Epoch 76 / 100: avg data time: 5.57e-02, avg batch time: 0.4991, average train loss: 2.2994
[09/26 13:11:48 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1671, average loss: 2.3330
[09/26 13:11:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 13:11:48 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 13:11:54 visual_prompt]: Epoch 77 / 100: avg data time: 4.62e-02, avg batch time: 0.4895, average train loss: 2.3107
[09/26 13:11:56 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1671, average loss: 2.2765
[09/26 13:11:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 13:11:56 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 13:12:03 visual_prompt]: Epoch 78 / 100: avg data time: 5.12e-02, avg batch time: 0.4935, average train loss: 2.2723
[09/26 13:12:04 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1669, average loss: 2.2443
[09/26 13:12:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 13:12:04 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 13:12:11 visual_prompt]: Epoch 79 / 100: avg data time: 4.42e-02, avg batch time: 0.4875, average train loss: 2.2375
[09/26 13:12:13 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1669, average loss: 2.2025
[09/26 13:12:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 13:12:13 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 13:12:19 visual_prompt]: Epoch 80 / 100: avg data time: 4.54e-02, avg batch time: 0.4896, average train loss: 2.2383
[09/26 13:12:21 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1671, average loss: 2.1971
[09/26 13:12:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 13:12:21 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 13:12:28 visual_prompt]: Epoch 81 / 100: avg data time: 4.79e-02, avg batch time: 0.4912, average train loss: 2.2383
[09/26 13:12:29 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1667, average loss: 2.3078
[09/26 13:12:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 13:12:29 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 13:12:36 visual_prompt]: Epoch 82 / 100: avg data time: 5.46e-02, avg batch time: 0.4976, average train loss: 2.2414
[09/26 13:12:38 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1666, average loss: 2.2455
[09/26 13:12:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 13:12:38 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 13:12:44 visual_prompt]: Epoch 83 / 100: avg data time: 4.79e-02, avg batch time: 0.4910, average train loss: 2.2204
[09/26 13:12:46 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1673, average loss: 2.2217
[09/26 13:12:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 13:12:46 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 13:12:53 visual_prompt]: Epoch 84 / 100: avg data time: 5.31e-02, avg batch time: 0.4963, average train loss: 2.2396
[09/26 13:12:54 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1678, average loss: 2.2654
[09/26 13:12:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 13:12:54 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 13:13:01 visual_prompt]: Epoch 85 / 100: avg data time: 5.16e-02, avg batch time: 0.4957, average train loss: 2.2141
[09/26 13:13:03 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1671, average loss: 2.2289
[09/26 13:13:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 13:13:03 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 13:13:09 visual_prompt]: Epoch 86 / 100: avg data time: 5.40e-02, avg batch time: 0.4983, average train loss: 2.2082
[09/26 13:13:11 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1673, average loss: 2.2157
[09/26 13:13:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 13:13:11 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 13:13:18 visual_prompt]: Epoch 87 / 100: avg data time: 4.64e-02, avg batch time: 0.4895, average train loss: 2.2212
[09/26 13:13:19 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1668, average loss: 2.2048
[09/26 13:13:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 13:13:19 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 13:13:26 visual_prompt]: Epoch 88 / 100: avg data time: 4.74e-02, avg batch time: 0.4901, average train loss: 2.2244
[09/26 13:13:28 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1669, average loss: 2.2429
[09/26 13:13:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 13:13:28 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 13:13:34 visual_prompt]: Epoch 89 / 100: avg data time: 5.46e-02, avg batch time: 0.4990, average train loss: 2.2292
[09/26 13:13:36 visual_prompt]: Inference (val):avg data time: 4.90e-05, avg batch time: 0.1670, average loss: 2.2053
[09/26 13:13:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 13:13:36 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 13:13:43 visual_prompt]: Epoch 90 / 100: avg data time: 4.76e-02, avg batch time: 0.4912, average train loss: 2.2134
[09/26 13:13:44 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1676, average loss: 2.2110
[09/26 13:13:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 13:13:44 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 13:13:51 visual_prompt]: Epoch 91 / 100: avg data time: 4.45e-02, avg batch time: 0.4887, average train loss: 2.2117
[09/26 13:13:53 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1675, average loss: 2.1997
[09/26 13:13:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 13:13:53 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 13:13:59 visual_prompt]: Epoch 92 / 100: avg data time: 4.74e-02, avg batch time: 0.4904, average train loss: 2.2004
[09/26 13:14:01 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.1671, average loss: 2.1998
[09/26 13:14:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 13:14:01 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 13:14:08 visual_prompt]: Epoch 93 / 100: avg data time: 4.60e-02, avg batch time: 0.4906, average train loss: 2.2086
[09/26 13:14:09 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1673, average loss: 2.1907
[09/26 13:14:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 13:14:09 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 13:14:16 visual_prompt]: Epoch 94 / 100: avg data time: 4.54e-02, avg batch time: 0.4919, average train loss: 2.1970
[09/26 13:14:18 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1672, average loss: 2.2054
[09/26 13:14:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.00	
[09/26 13:14:18 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 13:14:24 visual_prompt]: Epoch 95 / 100: avg data time: 5.85e-02, avg batch time: 0.5023, average train loss: 2.1921
[09/26 13:14:26 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1672, average loss: 2.1968
[09/26 13:14:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 13:14:26 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 13:14:33 visual_prompt]: Epoch 96 / 100: avg data time: 5.46e-02, avg batch time: 0.4973, average train loss: 2.1920
[09/26 13:14:34 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1668, average loss: 2.1953
[09/26 13:14:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 13:14:34 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 13:14:41 visual_prompt]: Epoch 97 / 100: avg data time: 5.12e-02, avg batch time: 0.4958, average train loss: 2.1920
[09/26 13:14:43 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1669, average loss: 2.1932
[09/26 13:14:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 13:14:43 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 13:14:50 visual_prompt]: Epoch 98 / 100: avg data time: 4.78e-02, avg batch time: 0.4936, average train loss: 2.1910
[09/26 13:14:51 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1672, average loss: 2.1926
[09/26 13:14:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 13:14:51 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 13:14:58 visual_prompt]: Epoch 99 / 100: avg data time: 6.18e-02, avg batch time: 0.5062, average train loss: 2.1904
[09/26 13:15:00 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1668, average loss: 2.1936
[09/26 13:15:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 13:15:00 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 13:15:07 visual_prompt]: Epoch 100 / 100: avg data time: 5.99e-02, avg batch time: 0.5035, average train loss: 2.1900
[09/26 13:15:08 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1672, average loss: 2.1939
[09/26 13:15:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 13:15:08 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:15:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:15:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:15:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:15:08 visual_prompt]: Training with config:
[09/26 13:15:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:15:08 visual_prompt]: Loading training data...
[09/26 13:15:08 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 13:15:09 visual_prompt]: Number of images: 800
[09/26 13:15:09 visual_prompt]: Number of classes: 9 / 9
[09/26 13:15:09 visual_prompt]: Loading validation data...
[09/26 13:15:09 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 13:15:10 visual_prompt]: Number of images: 200
[09/26 13:15:10 visual_prompt]: Number of classes: 9 / 9
[09/26 13:15:10 visual_prompt]: Constructing models...
[09/26 13:15:12 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 13:15:12 visual_prompt]: tuned percent:0.542
[09/26 13:15:12 visual_prompt]: Device used for model: 0
[09/26 13:15:12 visual_prompt]: Setting up Evaluator...
[09/26 13:15:12 visual_prompt]: Setting up Trainer...
[09/26 13:15:12 visual_prompt]: 	Setting up the optimizer...
[09/26 13:15:12 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:15:19 visual_prompt]: Epoch 1 / 100: avg data time: 5.88e-02, avg batch time: 0.5019, average train loss: 2.8674
[09/26 13:15:20 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1668, average loss: 2.9516
[09/26 13:15:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 13:15:20 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 13:15:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 13:15:27 visual_prompt]: Epoch 2 / 100: avg data time: 4.67e-02, avg batch time: 0.4908, average train loss: 5.0933
[09/26 13:15:29 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1670, average loss: 5.0168
[09/26 13:15:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 13:15:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 13:15:36 visual_prompt]: Epoch 3 / 100: avg data time: 5.33e-02, avg batch time: 0.4962, average train loss: 3.3586
[09/26 13:15:37 visual_prompt]: Inference (val):avg data time: 1.93e-05, avg batch time: 0.1666, average loss: 2.5510
[09/26 13:15:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 13:15:37 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 13:15:44 visual_prompt]: Epoch 4 / 100: avg data time: 6.01e-02, avg batch time: 0.5031, average train loss: 2.7098
[09/26 13:15:45 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1673, average loss: 2.6776
[09/26 13:15:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 13:15:45 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 13:15:52 visual_prompt]: Epoch 5 / 100: avg data time: 4.68e-02, avg batch time: 0.4901, average train loss: 2.5366
[09/26 13:15:54 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1669, average loss: 2.4504
[09/26 13:15:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 62.50	
[09/26 13:15:54 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 13:16:01 visual_prompt]: Epoch 6 / 100: avg data time: 5.70e-02, avg batch time: 0.5010, average train loss: 2.5638
[09/26 13:16:02 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1673, average loss: 2.6576
[09/26 13:16:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.50	
[09/26 13:16:02 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 13:16:09 visual_prompt]: Epoch 7 / 100: avg data time: 5.17e-02, avg batch time: 0.4953, average train loss: 2.9927
[09/26 13:16:10 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1671, average loss: 2.9313
[09/26 13:16:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 13:16:10 visual_prompt]: Best epoch 7: best metric: 0.145
[09/26 13:16:10 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 13:16:17 visual_prompt]: Epoch 8 / 100: avg data time: 5.59e-02, avg batch time: 0.5000, average train loss: 3.2033
[09/26 13:16:19 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1669, average loss: 3.1941
[09/26 13:16:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 13:16:19 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 13:16:25 visual_prompt]: Epoch 9 / 100: avg data time: 4.78e-02, avg batch time: 0.4909, average train loss: 3.9720
[09/26 13:16:27 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1674, average loss: 6.1898
[09/26 13:16:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 13:16:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 13:16:34 visual_prompt]: Epoch 10 / 100: avg data time: 5.70e-02, avg batch time: 0.5004, average train loss: 7.3159
[09/26 13:16:35 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1670, average loss: 11.9701
[09/26 13:16:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.00	
[09/26 13:16:35 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 13:16:42 visual_prompt]: Epoch 11 / 100: avg data time: 5.42e-02, avg batch time: 0.4982, average train loss: 11.0051
[09/26 13:16:44 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1670, average loss: 8.5166
[09/26 13:16:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 13:16:44 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 13:16:51 visual_prompt]: Epoch 12 / 100: avg data time: 5.08e-02, avg batch time: 0.4936, average train loss: 13.2073
[09/26 13:16:52 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1672, average loss: 21.6217
[09/26 13:16:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 53.00	
[09/26 13:16:52 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 13:16:59 visual_prompt]: Epoch 13 / 100: avg data time: 5.20e-02, avg batch time: 0.4961, average train loss: 28.1193
[09/26 13:17:00 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1674, average loss: 32.7059
[09/26 13:17:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 13:17:00 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 13:17:07 visual_prompt]: Epoch 14 / 100: avg data time: 4.72e-02, avg batch time: 0.4918, average train loss: 32.8222
[09/26 13:17:09 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1672, average loss: 13.2630
[09/26 13:17:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 13:17:09 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 13:17:16 visual_prompt]: Epoch 15 / 100: avg data time: 5.73e-02, avg batch time: 0.5008, average train loss: 18.1109
[09/26 13:17:17 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1672, average loss: 10.1599
[09/26 13:17:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.50	
[09/26 13:17:17 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 13:17:24 visual_prompt]: Epoch 16 / 100: avg data time: 4.93e-02, avg batch time: 0.4936, average train loss: 13.8987
[09/26 13:17:25 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1672, average loss: 12.5575
[09/26 13:17:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 13:17:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 13:17:32 visual_prompt]: Epoch 17 / 100: avg data time: 4.61e-02, avg batch time: 0.4919, average train loss: 14.9449
[09/26 13:17:34 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1672, average loss: 14.7542
[09/26 13:17:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 51.50	
[09/26 13:17:34 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 13:17:40 visual_prompt]: Epoch 18 / 100: avg data time: 5.20e-02, avg batch time: 0.4961, average train loss: 17.2633
[09/26 13:17:42 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1669, average loss: 11.4401
[09/26 13:17:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 13:17:42 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 13:17:49 visual_prompt]: Epoch 19 / 100: avg data time: 4.72e-02, avg batch time: 0.4903, average train loss: 15.3309
[09/26 13:17:50 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1671, average loss: 18.5592
[09/26 13:17:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 51.50	
[09/26 13:17:50 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 13:17:57 visual_prompt]: Epoch 20 / 100: avg data time: 4.45e-02, avg batch time: 0.4904, average train loss: 30.6028
[09/26 13:17:58 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1677, average loss: 47.2190
[09/26 13:17:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 49.50	
[09/26 13:17:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 13:18:05 visual_prompt]: Epoch 21 / 100: avg data time: 5.28e-02, avg batch time: 0.4966, average train loss: 35.2280
[09/26 13:18:07 visual_prompt]: Inference (val):avg data time: 1.91e-05, avg batch time: 0.1673, average loss: 16.7270
[09/26 13:18:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.50	
[09/26 13:18:07 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 13:18:14 visual_prompt]: Epoch 22 / 100: avg data time: 5.25e-02, avg batch time: 0.4978, average train loss: 21.2246
[09/26 13:18:15 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1672, average loss: 28.2463
[09/26 13:18:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 13:18:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 13:18:22 visual_prompt]: Epoch 23 / 100: avg data time: 5.49e-02, avg batch time: 0.4989, average train loss: 21.4920
[09/26 13:18:24 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1675, average loss: 19.2574
[09/26 13:18:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 52.00	
[09/26 13:18:24 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 13:18:30 visual_prompt]: Epoch 24 / 100: avg data time: 4.58e-02, avg batch time: 0.4914, average train loss: 15.4943
[09/26 13:18:32 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1674, average loss: 17.0935
[09/26 13:18:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 13:18:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 13:18:38 visual_prompt]: Epoch 25 / 100: avg data time: 4.56e-02, avg batch time: 0.4903, average train loss: 17.3981
[09/26 13:18:40 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1671, average loss: 18.9212
[09/26 13:18:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 13:18:40 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 13:18:47 visual_prompt]: Epoch 26 / 100: avg data time: 5.54e-02, avg batch time: 0.4979, average train loss: 21.5202
[09/26 13:18:48 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1671, average loss: 17.2004
[09/26 13:18:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 13:18:48 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 13:18:55 visual_prompt]: Epoch 27 / 100: avg data time: 4.31e-02, avg batch time: 0.4876, average train loss: 19.3489
[09/26 13:18:56 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1671, average loss: 17.1278
[09/26 13:18:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.00	
[09/26 13:18:56 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 13:19:03 visual_prompt]: Epoch 28 / 100: avg data time: 5.39e-02, avg batch time: 0.4976, average train loss: 11.8821
[09/26 13:19:05 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1671, average loss: 9.9154
[09/26 13:19:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 13:19:05 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 13:19:12 visual_prompt]: Epoch 29 / 100: avg data time: 5.34e-02, avg batch time: 0.4963, average train loss: 18.8066
[09/26 13:19:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1672, average loss: 15.7098
[09/26 13:19:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.50	
[09/26 13:19:13 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 13:19:20 visual_prompt]: Epoch 30 / 100: avg data time: 4.70e-02, avg batch time: 0.4916, average train loss: 25.5511
[09/26 13:19:22 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1674, average loss: 25.8846
[09/26 13:19:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 13:19:22 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 13:19:28 visual_prompt]: Epoch 31 / 100: avg data time: 5.46e-02, avg batch time: 0.4983, average train loss: 19.1325
[09/26 13:19:30 visual_prompt]: Inference (val):avg data time: 3.70e-05, avg batch time: 0.1673, average loss: 13.4706
[09/26 13:19:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 13:19:30 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 13:19:37 visual_prompt]: Epoch 32 / 100: avg data time: 4.38e-02, avg batch time: 0.4894, average train loss: 19.6379
[09/26 13:19:38 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1673, average loss: 18.6609
[09/26 13:19:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 50.50	
[09/26 13:19:38 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 13:19:45 visual_prompt]: Epoch 33 / 100: avg data time: 5.55e-02, avg batch time: 0.4978, average train loss: 14.7420
[09/26 13:19:47 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1675, average loss: 14.0660
[09/26 13:19:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 13:19:47 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 13:19:53 visual_prompt]: Epoch 34 / 100: avg data time: 4.53e-02, avg batch time: 0.4899, average train loss: 11.9466
[09/26 13:19:55 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1672, average loss: 9.3445
[09/26 13:19:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 13:19:55 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 13:20:02 visual_prompt]: Epoch 35 / 100: avg data time: 5.47e-02, avg batch time: 0.4973, average train loss: 16.8039
[09/26 13:20:03 visual_prompt]: Inference (val):avg data time: 4.05e-05, avg batch time: 0.1668, average loss: 13.8858
[09/26 13:20:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 54.00	
[09/26 13:20:03 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 13:20:10 visual_prompt]: Epoch 36 / 100: avg data time: 4.21e-02, avg batch time: 0.4854, average train loss: 17.1406
[09/26 13:20:12 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1670, average loss: 21.0510
[09/26 13:20:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 13:20:12 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 13:20:18 visual_prompt]: Epoch 37 / 100: avg data time: 4.81e-02, avg batch time: 0.4924, average train loss: 13.2312
[09/26 13:20:20 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1667, average loss: 8.0590
[09/26 13:20:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 13:20:20 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 13:20:27 visual_prompt]: Epoch 38 / 100: avg data time: 5.65e-02, avg batch time: 0.4988, average train loss: 8.3470
[09/26 13:20:28 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1668, average loss: 4.7032
[09/26 13:20:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.00	top5: 48.50	
[09/26 13:20:28 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 13:20:35 visual_prompt]: Epoch 39 / 100: avg data time: 5.43e-02, avg batch time: 0.4971, average train loss: 6.4134
[09/26 13:20:37 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1669, average loss: 7.5503
[09/26 13:20:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 13:20:37 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 13:20:43 visual_prompt]: Epoch 40 / 100: avg data time: 4.75e-02, avg batch time: 0.4922, average train loss: 6.3470
[09/26 13:20:45 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1672, average loss: 7.6041
[09/26 13:20:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.00	
[09/26 13:20:45 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 13:20:52 visual_prompt]: Epoch 41 / 100: avg data time: 4.77e-02, avg batch time: 0.4924, average train loss: 9.6306
[09/26 13:20:53 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.1673, average loss: 11.3102
[09/26 13:20:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 62.50	
[09/26 13:20:53 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 13:21:00 visual_prompt]: Epoch 42 / 100: avg data time: 6.33e-02, avg batch time: 0.5056, average train loss: 12.5190
[09/26 13:21:02 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1670, average loss: 11.9027
[09/26 13:21:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.50	top5: 52.00	
[09/26 13:21:02 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 13:21:09 visual_prompt]: Epoch 43 / 100: avg data time: 4.80e-02, avg batch time: 0.4935, average train loss: 13.5781
[09/26 13:21:10 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1673, average loss: 32.6157
[09/26 13:21:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 49.00	
[09/26 13:21:10 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 13:21:17 visual_prompt]: Epoch 44 / 100: avg data time: 5.10e-02, avg batch time: 0.4954, average train loss: 9.8637
[09/26 13:21:18 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1673, average loss: 7.4289
[09/26 13:21:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 13:21:18 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 13:21:25 visual_prompt]: Epoch 45 / 100: avg data time: 5.21e-02, avg batch time: 0.4957, average train loss: 5.1072
[09/26 13:21:27 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1667, average loss: 5.0341
[09/26 13:21:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 13:21:27 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 13:21:33 visual_prompt]: Epoch 46 / 100: avg data time: 4.97e-02, avg batch time: 0.4936, average train loss: 4.0207
[09/26 13:21:35 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1675, average loss: 3.8443
[09/26 13:21:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 52.00	
[09/26 13:21:35 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 13:21:42 visual_prompt]: Epoch 47 / 100: avg data time: 4.54e-02, avg batch time: 0.4878, average train loss: 2.9767
[09/26 13:21:43 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1667, average loss: 2.6183
[09/26 13:21:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 13:21:43 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 13:21:50 visual_prompt]: Epoch 48 / 100: avg data time: 5.72e-02, avg batch time: 0.5018, average train loss: 2.6336
[09/26 13:21:52 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1667, average loss: 2.5863
[09/26 13:21:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 50.00	
[09/26 13:21:52 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 13:21:59 visual_prompt]: Epoch 49 / 100: avg data time: 6.55e-02, avg batch time: 0.5078, average train loss: 2.5279
[09/26 13:22:00 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1669, average loss: 2.4166
[09/26 13:22:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.50	
[09/26 13:22:00 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 13:22:07 visual_prompt]: Epoch 50 / 100: avg data time: 4.90e-02, avg batch time: 0.4930, average train loss: 2.6632
[09/26 13:22:09 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1672, average loss: 2.3599
[09/26 13:22:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 13:22:09 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 13:22:15 visual_prompt]: Epoch 51 / 100: avg data time: 5.91e-02, avg batch time: 0.5021, average train loss: 3.0137
[09/26 13:22:17 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1670, average loss: 3.3074
[09/26 13:22:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.50	
[09/26 13:22:17 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 13:22:24 visual_prompt]: Epoch 52 / 100: avg data time: 5.39e-02, avg batch time: 0.4966, average train loss: 4.0844
[09/26 13:22:25 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1671, average loss: 5.7162
[09/26 13:22:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 13:22:25 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 13:22:32 visual_prompt]: Epoch 53 / 100: avg data time: 4.48e-02, avg batch time: 0.4894, average train loss: 6.1951
[09/26 13:22:34 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1668, average loss: 3.9029
[09/26 13:22:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 13:22:34 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 13:22:40 visual_prompt]: Epoch 54 / 100: avg data time: 5.55e-02, avg batch time: 0.4976, average train loss: 3.6984
[09/26 13:22:42 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1670, average loss: 3.0898
[09/26 13:22:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 13:22:42 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 13:22:49 visual_prompt]: Epoch 55 / 100: avg data time: 6.33e-02, avg batch time: 0.5054, average train loss: 2.6496
[09/26 13:22:50 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1669, average loss: 2.3647
[09/26 13:22:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 56.00	
[09/26 13:22:50 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 13:22:57 visual_prompt]: Epoch 56 / 100: avg data time: 5.83e-02, avg batch time: 0.5003, average train loss: 2.3848
[09/26 13:22:59 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1670, average loss: 2.3837
[09/26 13:22:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 13:22:59 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 13:23:06 visual_prompt]: Epoch 57 / 100: avg data time: 6.05e-02, avg batch time: 0.5030, average train loss: 2.3958
[09/26 13:23:07 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1672, average loss: 2.4552
[09/26 13:23:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/26 13:23:07 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 13:23:14 visual_prompt]: Epoch 58 / 100: avg data time: 5.60e-02, avg batch time: 0.4988, average train loss: 2.4488
[09/26 13:23:16 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1670, average loss: 2.3221
[09/26 13:23:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 13:23:16 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 13:23:22 visual_prompt]: Epoch 59 / 100: avg data time: 4.63e-02, avg batch time: 0.4921, average train loss: 2.3588
[09/26 13:23:24 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1670, average loss: 2.4991
[09/26 13:23:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 13:23:24 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 13:23:31 visual_prompt]: Epoch 60 / 100: avg data time: 5.55e-02, avg batch time: 0.4972, average train loss: 2.3434
[09/26 13:23:32 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1670, average loss: 2.4847
[09/26 13:23:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 13:23:32 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 13:23:39 visual_prompt]: Epoch 61 / 100: avg data time: 5.56e-02, avg batch time: 0.4986, average train loss: 2.3227
[09/26 13:23:41 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1669, average loss: 2.3031
[09/26 13:23:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 13:23:41 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 13:23:47 visual_prompt]: Epoch 62 / 100: avg data time: 5.95e-02, avg batch time: 0.5020, average train loss: 2.3216
[09/26 13:23:49 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1670, average loss: 2.2918
[09/26 13:23:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 13:23:49 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 13:23:56 visual_prompt]: Epoch 63 / 100: avg data time: 5.66e-02, avg batch time: 0.4990, average train loss: 2.3189
[09/26 13:23:57 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1669, average loss: 2.2390
[09/26 13:23:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 13:23:57 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 13:24:04 visual_prompt]: Epoch 64 / 100: avg data time: 5.33e-02, avg batch time: 0.4961, average train loss: 2.2980
[09/26 13:24:06 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1666, average loss: 2.2321
[09/26 13:24:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 60.00	
[09/26 13:24:06 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 13:24:12 visual_prompt]: Epoch 65 / 100: avg data time: 4.71e-02, avg batch time: 0.4890, average train loss: 2.2942
[09/26 13:24:14 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1670, average loss: 2.3558
[09/26 13:24:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 13:24:14 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 13:24:21 visual_prompt]: Epoch 66 / 100: avg data time: 5.64e-02, avg batch time: 0.5006, average train loss: 2.3531
[09/26 13:24:22 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1669, average loss: 2.2745
[09/26 13:24:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.00	
[09/26 13:24:22 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 13:24:29 visual_prompt]: Epoch 67 / 100: avg data time: 4.60e-02, avg batch time: 0.4885, average train loss: 2.3237
[09/26 13:24:30 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1666, average loss: 2.3768
[09/26 13:24:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 13:24:30 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 13:24:37 visual_prompt]: Epoch 68 / 100: avg data time: 5.59e-02, avg batch time: 0.5003, average train loss: 2.2760
[09/26 13:24:39 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1667, average loss: 2.2315
[09/26 13:24:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 13:24:39 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 13:24:46 visual_prompt]: Epoch 69 / 100: avg data time: 5.43e-02, avg batch time: 0.4973, average train loss: 2.3026
[09/26 13:24:47 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1671, average loss: 2.2233
[09/26 13:24:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 13:24:47 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 13:24:54 visual_prompt]: Epoch 70 / 100: avg data time: 4.40e-02, avg batch time: 0.4880, average train loss: 2.3150
[09/26 13:24:55 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1665, average loss: 2.4533
[09/26 13:24:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 13:24:55 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 13:25:02 visual_prompt]: Epoch 71 / 100: avg data time: 4.31e-02, avg batch time: 0.4883, average train loss: 2.2825
[09/26 13:25:04 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1669, average loss: 2.3046
[09/26 13:25:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 13:25:04 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 13:25:10 visual_prompt]: Epoch 72 / 100: avg data time: 5.83e-02, avg batch time: 0.5019, average train loss: 2.3489
[09/26 13:25:12 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.1668, average loss: 2.2529
[09/26 13:25:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 54.50	
[09/26 13:25:12 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 13:25:19 visual_prompt]: Epoch 73 / 100: avg data time: 4.74e-02, avg batch time: 0.4906, average train loss: 2.2592
[09/26 13:25:20 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1668, average loss: 2.2692
[09/26 13:25:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 13:25:20 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 13:25:27 visual_prompt]: Epoch 74 / 100: avg data time: 4.72e-02, avg batch time: 0.4918, average train loss: 2.2533
[09/26 13:25:29 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1668, average loss: 2.1981
[09/26 13:25:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 64.50	
[09/26 13:25:29 visual_prompt]: Best epoch 74: best metric: 0.150
[09/26 13:25:29 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 13:25:36 visual_prompt]: Epoch 75 / 100: avg data time: 6.24e-02, avg batch time: 0.5046, average train loss: 2.2605
[09/26 13:25:37 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1670, average loss: 2.2255
[09/26 13:25:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 13:25:37 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 13:25:44 visual_prompt]: Epoch 76 / 100: avg data time: 5.21e-02, avg batch time: 0.4947, average train loss: 2.2197
[09/26 13:25:46 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1668, average loss: 2.2216
[09/26 13:25:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 56.50	
[09/26 13:25:46 visual_prompt]: Best epoch 76: best metric: 0.180
[09/26 13:25:46 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 13:25:52 visual_prompt]: Epoch 77 / 100: avg data time: 5.46e-02, avg batch time: 0.4975, average train loss: 2.2130
[09/26 13:25:54 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1671, average loss: 2.2142
[09/26 13:25:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 13:25:54 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 13:26:01 visual_prompt]: Epoch 78 / 100: avg data time: 4.93e-02, avg batch time: 0.4931, average train loss: 2.2211
[09/26 13:26:02 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1678, average loss: 2.2070
[09/26 13:26:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 13:26:02 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 13:26:09 visual_prompt]: Epoch 79 / 100: avg data time: 4.54e-02, avg batch time: 0.4887, average train loss: 2.2032
[09/26 13:26:10 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1668, average loss: 2.2275
[09/26 13:26:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 13:26:10 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 13:26:17 visual_prompt]: Epoch 80 / 100: avg data time: 5.57e-02, avg batch time: 0.4994, average train loss: 2.1780
[09/26 13:26:19 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1672, average loss: 2.2636
[09/26 13:26:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 64.00	
[09/26 13:26:19 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 13:26:26 visual_prompt]: Epoch 81 / 100: avg data time: 5.05e-02, avg batch time: 0.4962, average train loss: 2.2867
[09/26 13:26:27 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1670, average loss: 2.2227
[09/26 13:26:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 58.00	
[09/26 13:26:27 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 13:26:34 visual_prompt]: Epoch 82 / 100: avg data time: 4.65e-02, avg batch time: 0.4897, average train loss: 2.2214
[09/26 13:26:36 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1669, average loss: 2.2073
[09/26 13:26:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 13:26:36 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 13:26:42 visual_prompt]: Epoch 83 / 100: avg data time: 5.24e-02, avg batch time: 0.4969, average train loss: 2.2216
[09/26 13:26:44 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1672, average loss: 2.2164
[09/26 13:26:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 13:26:44 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 13:26:51 visual_prompt]: Epoch 84 / 100: avg data time: 5.87e-02, avg batch time: 0.5009, average train loss: 2.2400
[09/26 13:26:52 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1672, average loss: 2.2228
[09/26 13:26:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 13:26:52 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 13:26:59 visual_prompt]: Epoch 85 / 100: avg data time: 4.55e-02, avg batch time: 0.4898, average train loss: 2.2286
[09/26 13:27:01 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1673, average loss: 2.2083
[09/26 13:27:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.00	
[09/26 13:27:01 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 13:27:07 visual_prompt]: Epoch 86 / 100: avg data time: 4.78e-02, avg batch time: 0.4914, average train loss: 2.2094
[09/26 13:27:09 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1673, average loss: 2.2037
[09/26 13:27:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.50	
[09/26 13:27:09 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 13:27:16 visual_prompt]: Epoch 87 / 100: avg data time: 5.84e-02, avg batch time: 0.5010, average train loss: 2.2091
[09/26 13:27:17 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1674, average loss: 2.1866
[09/26 13:27:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 63.00	
[09/26 13:27:17 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 13:27:24 visual_prompt]: Epoch 88 / 100: avg data time: 6.32e-02, avg batch time: 0.5065, average train loss: 2.2114
[09/26 13:27:26 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1673, average loss: 2.1967
[09/26 13:27:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 13:27:26 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 13:27:32 visual_prompt]: Epoch 89 / 100: avg data time: 5.99e-02, avg batch time: 0.5032, average train loss: 2.1957
[09/26 13:27:34 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1670, average loss: 2.1718
[09/26 13:27:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 64.50	
[09/26 13:27:34 visual_prompt]: Best epoch 89: best metric: 0.190
[09/26 13:27:34 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 13:27:41 visual_prompt]: Epoch 90 / 100: avg data time: 6.08e-02, avg batch time: 0.5049, average train loss: 2.1786
[09/26 13:27:43 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1675, average loss: 2.1499
[09/26 13:27:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 69.50	
[09/26 13:27:43 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 13:27:49 visual_prompt]: Epoch 91 / 100: avg data time: 5.82e-02, avg batch time: 0.5023, average train loss: 2.1528
[09/26 13:27:51 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1678, average loss: 2.2164
[09/26 13:27:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 59.00	
[09/26 13:27:51 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 13:27:58 visual_prompt]: Epoch 92 / 100: avg data time: 5.21e-02, avg batch time: 0.4980, average train loss: 2.1297
[09/26 13:27:59 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1674, average loss: 2.2324
[09/26 13:27:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 57.00	
[09/26 13:27:59 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 13:28:06 visual_prompt]: Epoch 93 / 100: avg data time: 5.74e-02, avg batch time: 0.5028, average train loss: 2.0957
[09/26 13:28:08 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1674, average loss: 2.1933
[09/26 13:28:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 64.50	
[09/26 13:28:08 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 13:28:15 visual_prompt]: Epoch 94 / 100: avg data time: 6.13e-02, avg batch time: 0.5054, average train loss: 2.0698
[09/26 13:28:16 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1673, average loss: 2.0963
[09/26 13:28:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 71.00	
[09/26 13:28:16 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 13:28:23 visual_prompt]: Epoch 95 / 100: avg data time: 5.66e-02, avg batch time: 0.5000, average train loss: 2.0288
[09/26 13:28:25 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1673, average loss: 2.0259
[09/26 13:28:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 77.50	
[09/26 13:28:25 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 13:28:32 visual_prompt]: Epoch 96 / 100: avg data time: 5.66e-02, avg batch time: 0.5003, average train loss: 1.9978
[09/26 13:28:33 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1671, average loss: 2.0179
[09/26 13:28:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 78.50	
[09/26 13:28:33 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 13:28:40 visual_prompt]: Epoch 97 / 100: avg data time: 6.39e-02, avg batch time: 0.5073, average train loss: 1.9743
[09/26 13:28:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1673, average loss: 1.9752
[09/26 13:28:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 80.50	
[09/26 13:28:42 visual_prompt]: Best epoch 97: best metric: 0.205
[09/26 13:28:42 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 13:28:48 visual_prompt]: Epoch 98 / 100: avg data time: 5.77e-02, avg batch time: 0.5015, average train loss: 1.9496
[09/26 13:28:50 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1669, average loss: 1.9782
[09/26 13:28:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 80.00	
[09/26 13:28:50 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 13:28:57 visual_prompt]: Epoch 99 / 100: avg data time: 6.10e-02, avg batch time: 0.5035, average train loss: 1.9298
[09/26 13:28:59 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1673, average loss: 1.9459
[09/26 13:28:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 81.50	
[09/26 13:28:59 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 13:29:05 visual_prompt]: Epoch 100 / 100: avg data time: 5.52e-02, avg batch time: 0.4995, average train loss: 1.9165
[09/26 13:29:07 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1674, average loss: 1.9446
[09/26 13:29:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 82.50	
[09/26 13:29:07 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:29:07 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:29:07 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:29:07 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:29:07 visual_prompt]: Training with config:
[09/26 13:29:07 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:29:07 visual_prompt]: Loading training data...
[09/26 13:29:07 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 13:29:08 visual_prompt]: Number of images: 800
[09/26 13:29:08 visual_prompt]: Number of classes: 9 / 9
[09/26 13:29:08 visual_prompt]: Loading validation data...
[09/26 13:29:08 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 13:29:08 visual_prompt]: Number of images: 200
[09/26 13:29:08 visual_prompt]: Number of classes: 9 / 9
[09/26 13:29:08 visual_prompt]: Constructing models...
[09/26 13:29:11 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 13:29:11 visual_prompt]: tuned percent:0.542
[09/26 13:29:11 visual_prompt]: Device used for model: 0
[09/26 13:29:11 visual_prompt]: Setting up Evaluator...
[09/26 13:29:11 visual_prompt]: Setting up Trainer...
[09/26 13:29:11 visual_prompt]: 	Setting up the optimizer...
[09/26 13:29:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:29:18 visual_prompt]: Epoch 1 / 100: avg data time: 6.30e-02, avg batch time: 0.5052, average train loss: 2.8752
[09/26 13:29:19 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1666, average loss: 2.9516
[09/26 13:29:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 13:29:19 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 13:29:19 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 13:29:26 visual_prompt]: Epoch 2 / 100: avg data time: 5.29e-02, avg batch time: 0.4959, average train loss: 5.0915
[09/26 13:29:28 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1667, average loss: 5.0926
[09/26 13:29:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 13:29:28 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 13:29:34 visual_prompt]: Epoch 3 / 100: avg data time: 4.41e-02, avg batch time: 0.4885, average train loss: 3.8733
[09/26 13:29:36 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1676, average loss: 2.2981
[09/26 13:29:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 13:29:36 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 13:29:43 visual_prompt]: Epoch 4 / 100: avg data time: 4.64e-02, avg batch time: 0.4894, average train loss: 2.4864
[09/26 13:29:44 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1672, average loss: 2.5304
[09/26 13:29:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 13:29:44 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 13:29:51 visual_prompt]: Epoch 5 / 100: avg data time: 5.28e-02, avg batch time: 0.4962, average train loss: 2.4328
[09/26 13:29:52 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1672, average loss: 3.3398
[09/26 13:29:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.50	
[09/26 13:29:52 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 13:29:59 visual_prompt]: Epoch 6 / 100: avg data time: 4.61e-02, avg batch time: 0.4916, average train loss: 3.0613
[09/26 13:30:01 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1673, average loss: 2.9512
[09/26 13:30:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 60.00	
[09/26 13:30:01 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 13:30:07 visual_prompt]: Epoch 7 / 100: avg data time: 5.08e-02, avg batch time: 0.4943, average train loss: 3.2164
[09/26 13:30:09 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1668, average loss: 3.0369
[09/26 13:30:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 13:30:09 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 13:30:16 visual_prompt]: Epoch 8 / 100: avg data time: 4.84e-02, avg batch time: 0.4938, average train loss: 3.3105
[09/26 13:30:17 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1671, average loss: 3.6144
[09/26 13:30:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 57.50	
[09/26 13:30:17 visual_prompt]: Best epoch 8: best metric: 0.145
[09/26 13:30:17 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 13:30:24 visual_prompt]: Epoch 9 / 100: avg data time: 5.42e-02, avg batch time: 0.4991, average train loss: 5.0063
[09/26 13:30:26 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1675, average loss: 6.2970
[09/26 13:30:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.50	
[09/26 13:30:26 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 13:30:32 visual_prompt]: Epoch 10 / 100: avg data time: 5.06e-02, avg batch time: 0.4942, average train loss: 7.4570
[09/26 13:30:34 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1672, average loss: 9.7628
[09/26 13:30:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 13:30:34 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 13:30:41 visual_prompt]: Epoch 11 / 100: avg data time: 4.58e-02, avg batch time: 0.4896, average train loss: 13.2527
[09/26 13:30:42 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1670, average loss: 11.8750
[09/26 13:30:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 13:30:42 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 13:30:49 visual_prompt]: Epoch 12 / 100: avg data time: 4.88e-02, avg batch time: 0.4927, average train loss: 18.9166
[09/26 13:30:51 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1680, average loss: 10.3174
[09/26 13:30:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/26 13:30:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 13:30:57 visual_prompt]: Epoch 13 / 100: avg data time: 5.38e-02, avg batch time: 0.4975, average train loss: 9.1634
[09/26 13:30:59 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.1672, average loss: 9.9585
[09/26 13:30:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 13:30:59 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 13:31:06 visual_prompt]: Epoch 14 / 100: avg data time: 4.86e-02, avg batch time: 0.4935, average train loss: 11.9114
[09/26 13:31:07 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1670, average loss: 18.9742
[09/26 13:31:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 13:31:07 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 13:31:14 visual_prompt]: Epoch 15 / 100: avg data time: 4.90e-02, avg batch time: 0.4927, average train loss: 11.0235
[09/26 13:31:16 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1673, average loss: 11.6104
[09/26 13:31:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/26 13:31:16 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 13:31:22 visual_prompt]: Epoch 16 / 100: avg data time: 5.32e-02, avg batch time: 0.4962, average train loss: 9.0437
[09/26 13:31:24 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1672, average loss: 11.6374
[09/26 13:31:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.50	
[09/26 13:31:24 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 13:31:31 visual_prompt]: Epoch 17 / 100: avg data time: 5.84e-02, avg batch time: 0.5023, average train loss: 8.9508
[09/26 13:31:32 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1671, average loss: 7.6388
[09/26 13:31:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 13:31:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 13:31:39 visual_prompt]: Epoch 18 / 100: avg data time: 5.41e-02, avg batch time: 0.4994, average train loss: 6.6166
[09/26 13:31:41 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1673, average loss: 7.2073
[09/26 13:31:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 13:31:41 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 13:31:48 visual_prompt]: Epoch 19 / 100: avg data time: 5.62e-02, avg batch time: 0.4995, average train loss: 6.4947
[09/26 13:31:49 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1672, average loss: 6.6866
[09/26 13:31:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.00	
[09/26 13:31:49 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 13:31:56 visual_prompt]: Epoch 20 / 100: avg data time: 4.69e-02, avg batch time: 0.4901, average train loss: 6.8706
[09/26 13:31:58 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1675, average loss: 5.8750
[09/26 13:31:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 13:31:58 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 13:32:04 visual_prompt]: Epoch 21 / 100: avg data time: 4.44e-02, avg batch time: 0.4880, average train loss: 7.1347
[09/26 13:32:06 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1674, average loss: 5.2999
[09/26 13:32:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.50	
[09/26 13:32:06 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 13:32:13 visual_prompt]: Epoch 22 / 100: avg data time: 4.62e-02, avg batch time: 0.4914, average train loss: 3.8378
[09/26 13:32:14 visual_prompt]: Inference (val):avg data time: 4.94e-05, avg batch time: 0.1675, average loss: 2.9601
[09/26 13:32:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 55.50	
[09/26 13:32:14 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 13:32:21 visual_prompt]: Epoch 23 / 100: avg data time: 5.21e-02, avg batch time: 0.4953, average train loss: 3.2289
[09/26 13:32:23 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1676, average loss: 2.9157
[09/26 13:32:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 61.50	
[09/26 13:32:23 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 13:32:29 visual_prompt]: Epoch 24 / 100: avg data time: 5.25e-02, avg batch time: 0.4967, average train loss: 4.1781
[09/26 13:32:31 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1672, average loss: 5.0842
[09/26 13:32:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.00	
[09/26 13:32:31 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 13:32:38 visual_prompt]: Epoch 25 / 100: avg data time: 5.42e-02, avg batch time: 0.4979, average train loss: 3.5963
[09/26 13:32:39 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1670, average loss: 3.7841
[09/26 13:32:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 13:32:39 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 13:32:46 visual_prompt]: Epoch 26 / 100: avg data time: 5.13e-02, avg batch time: 0.4952, average train loss: 3.5808
[09/26 13:32:48 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1670, average loss: 2.9772
[09/26 13:32:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 58.50	
[09/26 13:32:48 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 13:32:55 visual_prompt]: Epoch 27 / 100: avg data time: 6.38e-02, avg batch time: 0.5077, average train loss: 3.4328
[09/26 13:32:56 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1675, average loss: 3.2134
[09/26 13:32:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 13:32:56 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 13:33:03 visual_prompt]: Epoch 28 / 100: avg data time: 5.43e-02, avg batch time: 0.4979, average train loss: 2.9697
[09/26 13:33:05 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1676, average loss: 2.8276
[09/26 13:33:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.00	
[09/26 13:33:05 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 13:33:12 visual_prompt]: Epoch 29 / 100: avg data time: 4.81e-02, avg batch time: 0.4911, average train loss: 2.5812
[09/26 13:33:13 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1672, average loss: 2.5133
[09/26 13:33:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 64.50	
[09/26 13:33:13 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 13:33:20 visual_prompt]: Epoch 30 / 100: avg data time: 5.09e-02, avg batch time: 0.4943, average train loss: 2.3656
[09/26 13:33:21 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1674, average loss: 2.8013
[09/26 13:33:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 65.50	
[09/26 13:33:21 visual_prompt]: Best epoch 30: best metric: 0.220
[09/26 13:33:21 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 13:33:28 visual_prompt]: Epoch 31 / 100: avg data time: 6.06e-02, avg batch time: 0.5037, average train loss: 2.7032
[09/26 13:33:30 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1672, average loss: 2.9486
[09/26 13:33:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 68.00	
[09/26 13:33:30 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 13:33:37 visual_prompt]: Epoch 32 / 100: avg data time: 5.84e-02, avg batch time: 0.5012, average train loss: 2.6634
[09/26 13:33:38 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1671, average loss: 2.2592
[09/26 13:33:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 61.00	
[09/26 13:33:38 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 13:33:45 visual_prompt]: Epoch 33 / 100: avg data time: 5.48e-02, avg batch time: 0.4978, average train loss: 2.2003
[09/26 13:33:47 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1669, average loss: 2.4299
[09/26 13:33:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 74.00	
[09/26 13:33:47 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 13:33:54 visual_prompt]: Epoch 34 / 100: avg data time: 6.42e-02, avg batch time: 0.5097, average train loss: 2.2625
[09/26 13:33:55 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1671, average loss: 2.1568
[09/26 13:33:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 74.00	
[09/26 13:33:55 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 13:34:02 visual_prompt]: Epoch 35 / 100: avg data time: 6.67e-02, avg batch time: 0.5089, average train loss: 2.2054
[09/26 13:34:04 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1675, average loss: 2.4028
[09/26 13:34:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 78.50	
[09/26 13:34:04 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 13:34:10 visual_prompt]: Epoch 36 / 100: avg data time: 5.58e-02, avg batch time: 0.4995, average train loss: 2.1621
[09/26 13:34:12 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1670, average loss: 2.7337
[09/26 13:34:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 58.50	
[09/26 13:34:12 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 13:34:19 visual_prompt]: Epoch 37 / 100: avg data time: 6.60e-02, avg batch time: 0.5096, average train loss: 2.4101
[09/26 13:34:21 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1672, average loss: 2.2956
[09/26 13:34:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 76.50	
[09/26 13:34:21 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 13:34:27 visual_prompt]: Epoch 38 / 100: avg data time: 4.57e-02, avg batch time: 0.4903, average train loss: 2.0756
[09/26 13:34:29 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1670, average loss: 2.1297
[09/26 13:34:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 81.50	
[09/26 13:34:29 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 13:34:36 visual_prompt]: Epoch 39 / 100: avg data time: 6.32e-02, avg batch time: 0.5068, average train loss: 2.1572
[09/26 13:34:38 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1672, average loss: 2.3989
[09/26 13:34:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 77.00	
[09/26 13:34:38 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 13:34:44 visual_prompt]: Epoch 40 / 100: avg data time: 5.56e-02, avg batch time: 0.4994, average train loss: 2.0318
[09/26 13:34:46 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1671, average loss: 3.0312
[09/26 13:34:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 67.50	
[09/26 13:34:46 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 13:34:53 visual_prompt]: Epoch 41 / 100: avg data time: 5.93e-02, avg batch time: 0.5038, average train loss: 2.7668
[09/26 13:34:54 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1675, average loss: 2.3984
[09/26 13:34:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 74.50	
[09/26 13:34:54 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 13:35:01 visual_prompt]: Epoch 42 / 100: avg data time: 4.34e-02, avg batch time: 0.4887, average train loss: 2.3153
[09/26 13:35:03 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1673, average loss: 3.2808
[09/26 13:35:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 59.00	
[09/26 13:35:03 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 13:35:10 visual_prompt]: Epoch 43 / 100: avg data time: 6.21e-02, avg batch time: 0.5043, average train loss: 2.3123
[09/26 13:35:11 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1674, average loss: 2.4057
[09/26 13:35:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 75.50	
[09/26 13:35:11 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 13:35:18 visual_prompt]: Epoch 44 / 100: avg data time: 6.40e-02, avg batch time: 0.5072, average train loss: 2.1004
[09/26 13:35:20 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1670, average loss: 1.9858
[09/26 13:35:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 82.00	
[09/26 13:35:20 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 13:35:27 visual_prompt]: Epoch 45 / 100: avg data time: 5.86e-02, avg batch time: 0.5026, average train loss: 1.9555
[09/26 13:35:28 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1672, average loss: 2.1787
[09/26 13:35:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 77.00	
[09/26 13:35:28 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 13:35:35 visual_prompt]: Epoch 46 / 100: avg data time: 6.06e-02, avg batch time: 0.5043, average train loss: 2.0194
[09/26 13:35:37 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1673, average loss: 2.3552
[09/26 13:35:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 76.00	
[09/26 13:35:37 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 13:35:43 visual_prompt]: Epoch 47 / 100: avg data time: 5.71e-02, avg batch time: 0.4992, average train loss: 2.0615
[09/26 13:35:45 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1674, average loss: 2.3534
[09/26 13:35:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 75.00	
[09/26 13:35:45 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 13:35:52 visual_prompt]: Epoch 48 / 100: avg data time: 6.69e-02, avg batch time: 0.5100, average train loss: 1.9299
[09/26 13:35:54 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1674, average loss: 2.1436
[09/26 13:35:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 81.00	
[09/26 13:35:54 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 13:36:01 visual_prompt]: Epoch 49 / 100: avg data time: 6.58e-02, avg batch time: 0.5083, average train loss: 1.7905
[09/26 13:36:02 visual_prompt]: Inference (val):avg data time: 3.81e-05, avg batch time: 0.1672, average loss: 1.9060
[09/26 13:36:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 87.00	
[09/26 13:36:02 visual_prompt]: Best epoch 49: best metric: 0.245
[09/26 13:36:02 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 13:36:09 visual_prompt]: Epoch 50 / 100: avg data time: 5.71e-02, avg batch time: 0.5012, average train loss: 1.7815
[09/26 13:36:11 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1672, average loss: 2.0465
[09/26 13:36:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 87.00	
[09/26 13:36:11 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 13:36:18 visual_prompt]: Epoch 51 / 100: avg data time: 6.72e-02, avg batch time: 0.5107, average train loss: 1.8427
[09/26 13:36:19 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1673, average loss: 2.0264
[09/26 13:36:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 85.00	
[09/26 13:36:19 visual_prompt]: Best epoch 51: best metric: 0.270
[09/26 13:36:19 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 13:36:26 visual_prompt]: Epoch 52 / 100: avg data time: 6.63e-02, avg batch time: 0.5090, average train loss: 1.6996
[09/26 13:36:28 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1674, average loss: 2.3053
[09/26 13:36:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 83.00	
[09/26 13:36:28 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 13:36:35 visual_prompt]: Epoch 53 / 100: avg data time: 5.57e-02, avg batch time: 0.4985, average train loss: 1.6488
[09/26 13:36:36 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1671, average loss: 2.0285
[09/26 13:36:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 84.50	
[09/26 13:36:36 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 13:36:43 visual_prompt]: Epoch 54 / 100: avg data time: 6.38e-02, avg batch time: 0.5076, average train loss: 1.5508
[09/26 13:36:45 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1672, average loss: 2.0945
[09/26 13:36:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 84.00	
[09/26 13:36:45 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 13:36:52 visual_prompt]: Epoch 55 / 100: avg data time: 6.24e-02, avg batch time: 0.5055, average train loss: 1.4920
[09/26 13:36:53 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1674, average loss: 2.2581
[09/26 13:36:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 85.00	
[09/26 13:36:53 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 13:37:00 visual_prompt]: Epoch 56 / 100: avg data time: 5.68e-02, avg batch time: 0.4997, average train loss: 1.4931
[09/26 13:37:02 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1671, average loss: 2.0468
[09/26 13:37:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 88.50	
[09/26 13:37:02 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 13:37:08 visual_prompt]: Epoch 57 / 100: avg data time: 5.42e-02, avg batch time: 0.5003, average train loss: 1.4578
[09/26 13:37:10 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1675, average loss: 2.5004
[09/26 13:37:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 84.50	
[09/26 13:37:10 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 13:37:17 visual_prompt]: Epoch 58 / 100: avg data time: 5.47e-02, avg batch time: 0.4984, average train loss: 1.6180
[09/26 13:37:19 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1676, average loss: 2.0954
[09/26 13:37:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 89.00	
[09/26 13:37:19 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 13:37:25 visual_prompt]: Epoch 59 / 100: avg data time: 6.57e-02, avg batch time: 0.5084, average train loss: 1.5650
[09/26 13:37:27 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1672, average loss: 2.0548
[09/26 13:37:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 86.00	
[09/26 13:37:27 visual_prompt]: Best epoch 59: best metric: 0.290
[09/26 13:37:27 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 13:37:34 visual_prompt]: Epoch 60 / 100: avg data time: 5.32e-02, avg batch time: 0.4956, average train loss: 1.3797
[09/26 13:37:35 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1670, average loss: 2.1290
[09/26 13:37:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 91.00	
[09/26 13:37:35 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 13:37:42 visual_prompt]: Epoch 61 / 100: avg data time: 5.79e-02, avg batch time: 0.5002, average train loss: 1.3921
[09/26 13:37:44 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1670, average loss: 2.2935
[09/26 13:37:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 89.50	
[09/26 13:37:44 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 13:37:51 visual_prompt]: Epoch 62 / 100: avg data time: 5.82e-02, avg batch time: 0.5020, average train loss: 1.5119
[09/26 13:37:52 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1673, average loss: 2.0004
[09/26 13:37:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 88.50	
[09/26 13:37:52 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 13:37:59 visual_prompt]: Epoch 63 / 100: avg data time: 4.91e-02, avg batch time: 0.4953, average train loss: 1.3200
[09/26 13:38:00 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1671, average loss: 2.2561
[09/26 13:38:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 91.00	
[09/26 13:38:00 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 13:38:07 visual_prompt]: Epoch 64 / 100: avg data time: 5.61e-02, avg batch time: 0.4995, average train loss: 1.3712
[09/26 13:38:09 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1671, average loss: 2.0944
[09/26 13:38:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 87.00	
[09/26 13:38:09 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 13:38:16 visual_prompt]: Epoch 65 / 100: avg data time: 5.81e-02, avg batch time: 0.5021, average train loss: 1.2599
[09/26 13:38:17 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1673, average loss: 2.2692
[09/26 13:38:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 89.50	
[09/26 13:38:17 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 13:38:24 visual_prompt]: Epoch 66 / 100: avg data time: 5.67e-02, avg batch time: 0.5000, average train loss: 1.1504
[09/26 13:38:26 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1668, average loss: 2.1783
[09/26 13:38:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 89.50	
[09/26 13:38:26 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 13:38:33 visual_prompt]: Epoch 67 / 100: avg data time: 6.09e-02, avg batch time: 0.5037, average train loss: 1.2123
[09/26 13:38:34 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1669, average loss: 2.3743
[09/26 13:38:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 83.50	
[09/26 13:38:34 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 13:38:41 visual_prompt]: Epoch 68 / 100: avg data time: 5.91e-02, avg batch time: 0.5031, average train loss: 1.2433
[09/26 13:38:42 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1676, average loss: 2.2014
[09/26 13:38:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 87.50	
[09/26 13:38:43 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 13:38:49 visual_prompt]: Epoch 69 / 100: avg data time: 5.38e-02, avg batch time: 0.4984, average train loss: 1.1379
[09/26 13:38:51 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1670, average loss: 2.4572
[09/26 13:38:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 93.00	
[09/26 13:38:51 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 13:38:58 visual_prompt]: Epoch 70 / 100: avg data time: 5.19e-02, avg batch time: 0.4959, average train loss: 1.1810
[09/26 13:38:59 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1673, average loss: 2.5069
[09/26 13:38:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 89.00	
[09/26 13:38:59 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 13:39:06 visual_prompt]: Epoch 71 / 100: avg data time: 5.35e-02, avg batch time: 0.4960, average train loss: 1.0638
[09/26 13:39:08 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1674, average loss: 2.8002
[09/26 13:39:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 13:39:08 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 13:39:14 visual_prompt]: Epoch 72 / 100: avg data time: 5.85e-02, avg batch time: 0.5021, average train loss: 1.0672
[09/26 13:39:16 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1673, average loss: 2.6221
[09/26 13:39:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 89.50	
[09/26 13:39:16 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 13:39:23 visual_prompt]: Epoch 73 / 100: avg data time: 5.13e-02, avg batch time: 0.4956, average train loss: 1.1482
[09/26 13:39:24 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1671, average loss: 2.2131
[09/26 13:39:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 93.00	
[09/26 13:39:24 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 13:39:31 visual_prompt]: Epoch 74 / 100: avg data time: 6.44e-02, avg batch time: 0.5069, average train loss: 0.9236
[09/26 13:39:33 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1669, average loss: 2.7264
[09/26 13:39:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 89.00	
[09/26 13:39:33 visual_prompt]: Best epoch 74: best metric: 0.295
[09/26 13:39:33 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 13:39:40 visual_prompt]: Epoch 75 / 100: avg data time: 6.03e-02, avg batch time: 0.5036, average train loss: 0.8357
[09/26 13:39:41 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1671, average loss: 3.0751
[09/26 13:39:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 88.50	
[09/26 13:39:41 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 13:39:48 visual_prompt]: Epoch 76 / 100: avg data time: 4.95e-02, avg batch time: 0.4949, average train loss: 0.6997
[09/26 13:39:50 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1670, average loss: 3.4410
[09/26 13:39:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 90.00	
[09/26 13:39:50 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 13:39:56 visual_prompt]: Epoch 77 / 100: avg data time: 4.99e-02, avg batch time: 0.4948, average train loss: 0.8394
[09/26 13:39:58 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1674, average loss: 3.1245
[09/26 13:39:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 90.00	
[09/26 13:39:58 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 13:40:05 visual_prompt]: Epoch 78 / 100: avg data time: 5.51e-02, avg batch time: 0.5002, average train loss: 0.7049
[09/26 13:40:06 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1674, average loss: 2.9473
[09/26 13:40:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 91.00	
[09/26 13:40:06 visual_prompt]: Best epoch 78: best metric: 0.305
[09/26 13:40:06 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 13:40:13 visual_prompt]: Epoch 79 / 100: avg data time: 4.67e-02, avg batch time: 0.4921, average train loss: 0.6240
[09/26 13:40:15 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1671, average loss: 3.9223
[09/26 13:40:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 90.50	
[09/26 13:40:15 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 13:40:21 visual_prompt]: Epoch 80 / 100: avg data time: 5.17e-02, avg batch time: 0.4963, average train loss: 0.7354
[09/26 13:40:23 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1673, average loss: 3.6621
[09/26 13:40:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 88.50	
[09/26 13:40:23 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 13:40:30 visual_prompt]: Epoch 81 / 100: avg data time: 4.53e-02, avg batch time: 0.4911, average train loss: 0.6876
[09/26 13:40:31 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1676, average loss: 3.1407
[09/26 13:40:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 89.50	
[09/26 13:40:31 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 13:40:38 visual_prompt]: Epoch 82 / 100: avg data time: 4.91e-02, avg batch time: 0.4941, average train loss: 0.4697
[09/26 13:40:39 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1673, average loss: 4.1124
[09/26 13:40:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 91.50	
[09/26 13:40:39 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 13:40:46 visual_prompt]: Epoch 83 / 100: avg data time: 4.91e-02, avg batch time: 0.4922, average train loss: 0.4128
[09/26 13:40:48 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1673, average loss: 4.8875
[09/26 13:40:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 87.00	
[09/26 13:40:48 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 13:40:54 visual_prompt]: Epoch 84 / 100: avg data time: 4.77e-02, avg batch time: 0.4925, average train loss: 0.4548
[09/26 13:40:56 visual_prompt]: Inference (val):avg data time: 4.57e-05, avg batch time: 0.1674, average loss: 4.2262
[09/26 13:40:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 88.50	
[09/26 13:40:56 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 13:41:03 visual_prompt]: Epoch 85 / 100: avg data time: 5.76e-02, avg batch time: 0.5005, average train loss: 0.3625
[09/26 13:41:04 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1674, average loss: 4.4978
[09/26 13:41:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 92.00	
[09/26 13:41:04 visual_prompt]: Best epoch 85: best metric: 0.315
[09/26 13:41:04 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 13:41:11 visual_prompt]: Epoch 86 / 100: avg data time: 5.44e-02, avg batch time: 0.4983, average train loss: 0.2604
[09/26 13:41:13 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1673, average loss: 5.8915
[09/26 13:41:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 88.50	
[09/26 13:41:13 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 13:41:20 visual_prompt]: Epoch 87 / 100: avg data time: 5.51e-02, avg batch time: 0.5000, average train loss: 0.2539
[09/26 13:41:21 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1671, average loss: 6.5558
[09/26 13:41:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 87.50	
[09/26 13:41:21 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 13:41:28 visual_prompt]: Epoch 88 / 100: avg data time: 5.63e-02, avg batch time: 0.4993, average train loss: 0.2348
[09/26 13:41:30 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1674, average loss: 6.3122
[09/26 13:41:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 89.00	
[09/26 13:41:30 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 13:41:36 visual_prompt]: Epoch 89 / 100: avg data time: 5.19e-02, avg batch time: 0.4971, average train loss: 0.2024
[09/26 13:41:38 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1672, average loss: 6.4660
[09/26 13:41:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 91.50	
[09/26 13:41:38 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 13:41:45 visual_prompt]: Epoch 90 / 100: avg data time: 5.39e-02, avg batch time: 0.4979, average train loss: 0.1511
[09/26 13:41:46 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.1673, average loss: 6.8038
[09/26 13:41:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 90.50	
[09/26 13:41:46 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 13:41:53 visual_prompt]: Epoch 91 / 100: avg data time: 4.51e-02, avg batch time: 0.4883, average train loss: 0.1055
[09/26 13:41:55 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1673, average loss: 7.4490
[09/26 13:41:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 89.50	
[09/26 13:41:55 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 13:42:01 visual_prompt]: Epoch 92 / 100: avg data time: 4.85e-02, avg batch time: 0.4918, average train loss: 0.0748
[09/26 13:42:03 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1673, average loss: 7.6751
[09/26 13:42:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 88.00	
[09/26 13:42:03 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 13:42:10 visual_prompt]: Epoch 93 / 100: avg data time: 5.02e-02, avg batch time: 0.4942, average train loss: 0.0749
[09/26 13:42:11 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1673, average loss: 7.8733
[09/26 13:42:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 88.50	
[09/26 13:42:11 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 13:42:18 visual_prompt]: Epoch 94 / 100: avg data time: 5.43e-02, avg batch time: 0.4985, average train loss: 0.0705
[09/26 13:42:20 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1676, average loss: 8.2072
[09/26 13:42:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 87.50	
[09/26 13:42:20 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 13:42:27 visual_prompt]: Epoch 95 / 100: avg data time: 5.39e-02, avg batch time: 0.4983, average train loss: 0.0675
[09/26 13:42:28 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1675, average loss: 8.2471
[09/26 13:42:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 90.00	
[09/26 13:42:28 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 13:42:35 visual_prompt]: Epoch 96 / 100: avg data time: 4.37e-02, avg batch time: 0.4896, average train loss: 0.0549
[09/26 13:42:37 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1673, average loss: 8.0588
[09/26 13:42:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 88.50	
[09/26 13:42:37 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 13:42:43 visual_prompt]: Epoch 97 / 100: avg data time: 4.48e-02, avg batch time: 0.4889, average train loss: 0.0487
[09/26 13:42:45 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1673, average loss: 7.8999
[09/26 13:42:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 89.00	
[09/26 13:42:45 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 13:42:52 visual_prompt]: Epoch 98 / 100: avg data time: 5.40e-02, avg batch time: 0.4974, average train loss: 0.0466
[09/26 13:42:53 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1677, average loss: 8.1058
[09/26 13:42:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 88.50	
[09/26 13:42:53 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 13:43:00 visual_prompt]: Epoch 99 / 100: avg data time: 4.65e-02, avg batch time: 0.4919, average train loss: 0.0479
[09/26 13:43:02 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1674, average loss: 8.1690
[09/26 13:43:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.50	
[09/26 13:43:02 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 13:43:08 visual_prompt]: Epoch 100 / 100: avg data time: 4.48e-02, avg batch time: 0.4889, average train loss: 0.0412
[09/26 13:43:10 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1675, average loss: 8.1782
[09/26 13:43:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.50	
[09/26 13:43:10 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:43:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:43:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:43:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:43:10 visual_prompt]: Training with config:
[09/26 13:43:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr5.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:43:10 visual_prompt]: Loading training data...
[09/26 13:43:10 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 13:43:11 visual_prompt]: Number of images: 800
[09/26 13:43:11 visual_prompt]: Number of classes: 9 / 9
[09/26 13:43:11 visual_prompt]: Loading validation data...
[09/26 13:43:11 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 13:43:11 visual_prompt]: Number of images: 200
[09/26 13:43:11 visual_prompt]: Number of classes: 9 / 9
[09/26 13:43:11 visual_prompt]: Constructing models...
[09/26 13:43:14 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 13:43:14 visual_prompt]: tuned percent:0.542
[09/26 13:43:14 visual_prompt]: Device used for model: 0
[09/26 13:43:14 visual_prompt]: Setting up Evaluator...
[09/26 13:43:14 visual_prompt]: Setting up Trainer...
[09/26 13:43:14 visual_prompt]: 	Setting up the optimizer...
[09/26 13:43:14 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:43:21 visual_prompt]: Epoch 1 / 100: avg data time: 5.63e-02, avg batch time: 0.4988, average train loss: 2.8584
[09/26 13:43:22 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1668, average loss: 2.9516
[09/26 13:43:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 13:43:22 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 13:43:22 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/26 13:43:29 visual_prompt]: Epoch 2 / 100: avg data time: 5.35e-02, avg batch time: 0.4960, average train loss: 5.1805
[09/26 13:43:30 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1669, average loss: 5.2805
[09/26 13:43:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.00	
[09/26 13:43:30 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/26 13:43:37 visual_prompt]: Epoch 3 / 100: avg data time: 5.55e-02, avg batch time: 0.4993, average train loss: 3.1511
[09/26 13:43:39 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1668, average loss: 2.6597
[09/26 13:43:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 54.00	
[09/26 13:43:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/26 13:43:46 visual_prompt]: Epoch 4 / 100: avg data time: 5.14e-02, avg batch time: 0.4942, average train loss: 2.6653
[09/26 13:43:47 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1668, average loss: 2.6649
[09/26 13:43:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.00	
[09/26 13:43:47 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/26 13:43:54 visual_prompt]: Epoch 5 / 100: avg data time: 5.84e-02, avg batch time: 0.5019, average train loss: 2.8404
[09/26 13:43:56 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1667, average loss: 2.9514
[09/26 13:43:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 13:43:56 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/26 13:44:03 visual_prompt]: Epoch 6 / 100: avg data time: 6.28e-02, avg batch time: 0.5059, average train loss: 3.5229
[09/26 13:44:04 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1669, average loss: 3.9314
[09/26 13:44:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 13:44:04 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/26 13:44:11 visual_prompt]: Epoch 7 / 100: avg data time: 5.12e-02, avg batch time: 0.4948, average train loss: 4.5703
[09/26 13:44:13 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1668, average loss: 4.8445
[09/26 13:44:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 13:44:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/26 13:44:19 visual_prompt]: Epoch 8 / 100: avg data time: 6.46e-02, avg batch time: 0.5071, average train loss: 5.4264
[09/26 13:44:21 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1671, average loss: 4.7518
[09/26 13:44:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.50	
[09/26 13:44:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/26 13:44:28 visual_prompt]: Epoch 9 / 100: avg data time: 6.35e-02, avg batch time: 0.5061, average train loss: 7.4066
[09/26 13:44:30 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1672, average loss: 9.2275
[09/26 13:44:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 13:44:30 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/26 13:44:36 visual_prompt]: Epoch 10 / 100: avg data time: 6.55e-02, avg batch time: 0.5074, average train loss: 9.9446
[09/26 13:44:38 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1670, average loss: 8.8714
[09/26 13:44:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 13:44:38 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/26 13:44:45 visual_prompt]: Epoch 11 / 100: avg data time: 6.12e-02, avg batch time: 0.5031, average train loss: 9.4211
[09/26 13:44:46 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1672, average loss: 14.3252
[09/26 13:44:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 13:44:46 visual_prompt]: Best epoch 11: best metric: 0.145
[09/26 13:44:46 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/26 13:44:53 visual_prompt]: Epoch 12 / 100: avg data time: 5.11e-02, avg batch time: 0.4955, average train loss: 12.0240
[09/26 13:44:55 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1671, average loss: 12.5178
[09/26 13:44:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.50	
[09/26 13:44:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/26 13:45:02 visual_prompt]: Epoch 13 / 100: avg data time: 5.95e-02, avg batch time: 0.5035, average train loss: 9.5808
[09/26 13:45:03 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1672, average loss: 10.2482
[09/26 13:45:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 54.00	
[09/26 13:45:03 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/26 13:45:10 visual_prompt]: Epoch 14 / 100: avg data time: 6.78e-02, avg batch time: 0.5109, average train loss: 12.4592
[09/26 13:45:12 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1672, average loss: 8.3365
[09/26 13:45:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 13:45:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/26 13:45:19 visual_prompt]: Epoch 15 / 100: avg data time: 5.88e-02, avg batch time: 0.5007, average train loss: 12.5530
[09/26 13:45:20 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1671, average loss: 6.2305
[09/26 13:45:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 13:45:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/26 13:45:27 visual_prompt]: Epoch 16 / 100: avg data time: 5.82e-02, avg batch time: 0.5009, average train loss: 8.9065
[09/26 13:45:29 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1672, average loss: 6.8731
[09/26 13:45:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 56.50	
[09/26 13:45:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/26 13:45:36 visual_prompt]: Epoch 17 / 100: avg data time: 5.84e-02, avg batch time: 0.5024, average train loss: 6.3168
[09/26 13:45:37 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1672, average loss: 6.1474
[09/26 13:45:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 13:45:37 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/26 13:45:44 visual_prompt]: Epoch 18 / 100: avg data time: 5.50e-02, avg batch time: 0.4978, average train loss: 8.3605
[09/26 13:45:46 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1672, average loss: 7.9910
[09/26 13:45:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 51.50	
[09/26 13:45:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/26 13:45:52 visual_prompt]: Epoch 19 / 100: avg data time: 6.07e-02, avg batch time: 0.5055, average train loss: 6.2807
[09/26 13:45:54 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1670, average loss: 5.9424
[09/26 13:45:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 13:45:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/26 13:46:01 visual_prompt]: Epoch 20 / 100: avg data time: 6.86e-02, avg batch time: 0.5117, average train loss: 4.5218
[09/26 13:46:03 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1673, average loss: 4.6853
[09/26 13:46:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 13:46:03 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/26 13:46:09 visual_prompt]: Epoch 21 / 100: avg data time: 5.05e-02, avg batch time: 0.4934, average train loss: 3.9939
[09/26 13:46:11 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1672, average loss: 3.1229
[09/26 13:46:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 55.50	
[09/26 13:46:11 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/26 13:46:18 visual_prompt]: Epoch 22 / 100: avg data time: 5.66e-02, avg batch time: 0.5005, average train loss: 2.9200
[09/26 13:46:19 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1669, average loss: 2.4905
[09/26 13:46:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 63.00	
[09/26 13:46:19 visual_prompt]: Best epoch 22: best metric: 0.160
[09/26 13:46:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/26 13:46:26 visual_prompt]: Epoch 23 / 100: avg data time: 5.14e-02, avg batch time: 0.4959, average train loss: 2.4589
[09/26 13:46:28 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1674, average loss: 2.6183
[09/26 13:46:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 68.00	
[09/26 13:46:28 visual_prompt]: Best epoch 23: best metric: 0.175
[09/26 13:46:28 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/26 13:46:34 visual_prompt]: Epoch 24 / 100: avg data time: 4.99e-02, avg batch time: 0.4939, average train loss: 2.5492
[09/26 13:46:36 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1670, average loss: 2.4827
[09/26 13:46:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 66.00	
[09/26 13:46:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/26 13:46:43 visual_prompt]: Epoch 25 / 100: avg data time: 5.26e-02, avg batch time: 0.4974, average train loss: 2.3021
[09/26 13:46:44 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1677, average loss: 2.5075
[09/26 13:46:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 69.50	
[09/26 13:46:44 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/26 13:46:51 visual_prompt]: Epoch 26 / 100: avg data time: 6.39e-02, avg batch time: 0.5078, average train loss: 2.5117
[09/26 13:46:53 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1675, average loss: 3.4691
[09/26 13:46:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 58.00	
[09/26 13:46:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/26 13:47:00 visual_prompt]: Epoch 27 / 100: avg data time: 6.11e-02, avg batch time: 0.5035, average train loss: 2.7537
[09/26 13:47:01 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1673, average loss: 2.7175
[09/26 13:47:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 62.50	
[09/26 13:47:01 visual_prompt]: Best epoch 27: best metric: 0.190
[09/26 13:47:01 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/26 13:47:08 visual_prompt]: Epoch 28 / 100: avg data time: 5.47e-02, avg batch time: 0.4988, average train loss: 2.5959
[09/26 13:47:10 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1676, average loss: 2.5759
[09/26 13:47:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 70.50	
[09/26 13:47:10 visual_prompt]: Best epoch 28: best metric: 0.195
[09/26 13:47:10 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/26 13:47:16 visual_prompt]: Epoch 29 / 100: avg data time: 5.00e-02, avg batch time: 0.4936, average train loss: 2.5329
[09/26 13:47:18 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1671, average loss: 2.6999
[09/26 13:47:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 61.00	
[09/26 13:47:18 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/26 13:47:25 visual_prompt]: Epoch 30 / 100: avg data time: 5.25e-02, avg batch time: 0.4964, average train loss: 2.3405
[09/26 13:47:26 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1672, average loss: 2.0477
[09/26 13:47:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 85.00	
[09/26 13:47:26 visual_prompt]: Best epoch 30: best metric: 0.215
[09/26 13:47:26 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/26 13:47:33 visual_prompt]: Epoch 31 / 100: avg data time: 5.98e-02, avg batch time: 0.5031, average train loss: 2.1707
[09/26 13:47:35 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1676, average loss: 2.5336
[09/26 13:47:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 78.00	
[09/26 13:47:35 visual_prompt]: Best epoch 31: best metric: 0.250
[09/26 13:47:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/26 13:47:42 visual_prompt]: Epoch 32 / 100: avg data time: 6.24e-02, avg batch time: 0.5052, average train loss: 2.2379
[09/26 13:47:43 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1672, average loss: 2.4656
[09/26 13:47:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 84.50	
[09/26 13:47:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/26 13:47:50 visual_prompt]: Epoch 33 / 100: avg data time: 4.94e-02, avg batch time: 0.4930, average train loss: 2.0495
[09/26 13:47:52 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1668, average loss: 2.1348
[09/26 13:47:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 81.50	
[09/26 13:47:52 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/26 13:47:58 visual_prompt]: Epoch 34 / 100: avg data time: 5.42e-02, avg batch time: 0.4987, average train loss: 2.0006
[09/26 13:48:00 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1670, average loss: 2.4481
[09/26 13:48:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 81.00	
[09/26 13:48:00 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/26 13:48:07 visual_prompt]: Epoch 35 / 100: avg data time: 4.78e-02, avg batch time: 0.4924, average train loss: 1.9593
[09/26 13:48:08 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1674, average loss: 2.1752
[09/26 13:48:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 84.50	
[09/26 13:48:08 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/26 13:48:15 visual_prompt]: Epoch 36 / 100: avg data time: 5.03e-02, avg batch time: 0.4951, average train loss: 1.8881
[09/26 13:48:17 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1673, average loss: 2.8652
[09/26 13:48:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 76.00	
[09/26 13:48:17 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/26 13:48:23 visual_prompt]: Epoch 37 / 100: avg data time: 6.05e-02, avg batch time: 0.5046, average train loss: 1.8524
[09/26 13:48:25 visual_prompt]: Inference (val):avg data time: 1.90e-05, avg batch time: 0.1671, average loss: 2.2588
[09/26 13:48:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 83.00	
[09/26 13:48:25 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/26 13:48:32 visual_prompt]: Epoch 38 / 100: avg data time: 5.09e-02, avg batch time: 0.4945, average train loss: 1.7193
[09/26 13:48:33 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1675, average loss: 2.3364
[09/26 13:48:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 88.00	
[09/26 13:48:33 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/26 13:48:40 visual_prompt]: Epoch 39 / 100: avg data time: 5.19e-02, avg batch time: 0.4961, average train loss: 1.7322
[09/26 13:48:42 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1671, average loss: 2.0035
[09/26 13:48:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 88.50	
[09/26 13:48:42 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/26 13:48:48 visual_prompt]: Epoch 40 / 100: avg data time: 5.07e-02, avg batch time: 0.4942, average train loss: 1.5988
[09/26 13:48:50 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1675, average loss: 2.1556
[09/26 13:48:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 91.50	
[09/26 13:48:50 visual_prompt]: Best epoch 40: best metric: 0.255
[09/26 13:48:50 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/26 13:48:57 visual_prompt]: Epoch 41 / 100: avg data time: 4.97e-02, avg batch time: 0.4940, average train loss: 1.8008
[09/26 13:48:58 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1671, average loss: 2.0908
[09/26 13:48:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 87.50	
[09/26 13:48:58 visual_prompt]: Best epoch 41: best metric: 0.270
[09/26 13:48:58 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/26 13:49:05 visual_prompt]: Epoch 42 / 100: avg data time: 4.87e-02, avg batch time: 0.4933, average train loss: 1.6671
[09/26 13:49:07 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1674, average loss: 2.1805
[09/26 13:49:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.00	
[09/26 13:49:07 visual_prompt]: Best epoch 42: best metric: 0.275
[09/26 13:49:07 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/26 13:49:13 visual_prompt]: Epoch 43 / 100: avg data time: 4.99e-02, avg batch time: 0.4940, average train loss: 1.4356
[09/26 13:49:15 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1674, average loss: 2.6951
[09/26 13:49:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 90.50	
[09/26 13:49:15 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/26 13:49:22 visual_prompt]: Epoch 44 / 100: avg data time: 5.70e-02, avg batch time: 0.5008, average train loss: 1.5056
[09/26 13:49:23 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1670, average loss: 2.3228
[09/26 13:49:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 87.50	
[09/26 13:49:23 visual_prompt]: Best epoch 44: best metric: 0.325
[09/26 13:49:23 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/26 13:49:30 visual_prompt]: Epoch 45 / 100: avg data time: 5.06e-02, avg batch time: 0.4940, average train loss: 1.5950
[09/26 13:49:32 visual_prompt]: Inference (val):avg data time: 3.57e-05, avg batch time: 0.1670, average loss: 2.1335
[09/26 13:49:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 87.00	
[09/26 13:49:32 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/26 13:49:39 visual_prompt]: Epoch 46 / 100: avg data time: 5.18e-02, avg batch time: 0.4961, average train loss: 1.5228
[09/26 13:49:40 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1674, average loss: 2.4877
[09/26 13:49:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 84.00	
[09/26 13:49:40 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/26 13:49:47 visual_prompt]: Epoch 47 / 100: avg data time: 5.13e-02, avg batch time: 0.4954, average train loss: 1.5778
[09/26 13:49:48 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1675, average loss: 2.3867
[09/26 13:49:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 90.50	
[09/26 13:49:48 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/26 13:49:55 visual_prompt]: Epoch 48 / 100: avg data time: 6.20e-02, avg batch time: 0.5057, average train loss: 1.5142
[09/26 13:49:57 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1676, average loss: 2.3477
[09/26 13:49:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 93.00	
[09/26 13:49:57 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/26 13:50:04 visual_prompt]: Epoch 49 / 100: avg data time: 5.47e-02, avg batch time: 0.4981, average train loss: 1.2073
[09/26 13:50:05 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1669, average loss: 2.1733
[09/26 13:50:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 90.00	
[09/26 13:50:05 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/26 13:50:12 visual_prompt]: Epoch 50 / 100: avg data time: 4.71e-02, avg batch time: 0.4914, average train loss: 1.3086
[09/26 13:50:14 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1674, average loss: 2.3726
[09/26 13:50:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.50	
[09/26 13:50:14 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/26 13:50:20 visual_prompt]: Epoch 51 / 100: avg data time: 5.17e-02, avg batch time: 0.4947, average train loss: 1.1665
[09/26 13:50:22 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.1671, average loss: 2.4298
[09/26 13:50:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 90.00	
[09/26 13:50:22 visual_prompt]: Best epoch 51: best metric: 0.335
[09/26 13:50:22 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/26 13:50:29 visual_prompt]: Epoch 52 / 100: avg data time: 5.34e-02, avg batch time: 0.4967, average train loss: 1.0371
[09/26 13:50:30 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1675, average loss: 2.3483
[09/26 13:50:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 89.50	
[09/26 13:50:30 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/26 13:50:37 visual_prompt]: Epoch 53 / 100: avg data time: 5.92e-02, avg batch time: 0.5038, average train loss: 1.0628
[09/26 13:50:39 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1671, average loss: 2.1728
[09/26 13:50:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 93.50	
[09/26 13:50:39 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/26 13:50:46 visual_prompt]: Epoch 54 / 100: avg data time: 6.52e-02, avg batch time: 0.5078, average train loss: 1.0664
[09/26 13:50:47 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1675, average loss: 2.2905
[09/26 13:50:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 92.00	
[09/26 13:50:47 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/26 13:50:54 visual_prompt]: Epoch 55 / 100: avg data time: 4.77e-02, avg batch time: 0.4921, average train loss: 1.0517
[09/26 13:50:56 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1672, average loss: 2.3747
[09/26 13:50:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 90.50	
[09/26 13:50:56 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/26 13:51:03 visual_prompt]: Epoch 56 / 100: avg data time: 5.73e-02, avg batch time: 0.5023, average train loss: 0.9771
[09/26 13:51:04 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1672, average loss: 2.8920
[09/26 13:51:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 89.50	
[09/26 13:51:04 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/26 13:51:11 visual_prompt]: Epoch 57 / 100: avg data time: 5.92e-02, avg batch time: 0.5032, average train loss: 0.9409
[09/26 13:51:13 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1671, average loss: 3.2137
[09/26 13:51:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 88.00	
[09/26 13:51:13 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/26 13:51:20 visual_prompt]: Epoch 58 / 100: avg data time: 6.61e-02, avg batch time: 0.5087, average train loss: 0.9741
[09/26 13:51:21 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1672, average loss: 2.4546
[09/26 13:51:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 94.50	
[09/26 13:51:21 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/26 13:51:28 visual_prompt]: Epoch 59 / 100: avg data time: 5.91e-02, avg batch time: 0.5029, average train loss: 0.8894
[09/26 13:51:30 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1672, average loss: 2.7624
[09/26 13:51:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 89.50	
[09/26 13:51:30 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/26 13:51:37 visual_prompt]: Epoch 60 / 100: avg data time: 6.02e-02, avg batch time: 0.5024, average train loss: 0.6966
[09/26 13:51:38 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1670, average loss: 2.7906
[09/26 13:51:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 91.50	
[09/26 13:51:38 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/26 13:51:45 visual_prompt]: Epoch 61 / 100: avg data time: 5.38e-02, avg batch time: 0.4997, average train loss: 0.6352
[09/26 13:51:47 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1672, average loss: 3.4652
[09/26 13:51:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 90.50	
[09/26 13:51:47 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/26 13:51:54 visual_prompt]: Epoch 62 / 100: avg data time: 6.32e-02, avg batch time: 0.5069, average train loss: 0.9064
[09/26 13:51:55 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1670, average loss: 2.9640
[09/26 13:51:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 13:51:55 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/26 13:52:02 visual_prompt]: Epoch 63 / 100: avg data time: 5.98e-02, avg batch time: 0.5037, average train loss: 0.9433
[09/26 13:52:04 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1671, average loss: 2.6276
[09/26 13:52:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 89.50	
[09/26 13:52:04 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/26 13:52:11 visual_prompt]: Epoch 64 / 100: avg data time: 5.04e-02, avg batch time: 0.4938, average train loss: 0.8310
[09/26 13:52:12 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1669, average loss: 2.7235
[09/26 13:52:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 90.00	
[09/26 13:52:12 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/26 13:52:19 visual_prompt]: Epoch 65 / 100: avg data time: 6.38e-02, avg batch time: 0.5073, average train loss: 0.7300
[09/26 13:52:21 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1669, average loss: 2.8213
[09/26 13:52:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 91.00	
[09/26 13:52:21 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/26 13:52:28 visual_prompt]: Epoch 66 / 100: avg data time: 6.42e-02, avg batch time: 0.5090, average train loss: 0.5496
[09/26 13:52:30 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1669, average loss: 3.1293
[09/26 13:52:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 90.00	
[09/26 13:52:30 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/26 13:52:37 visual_prompt]: Epoch 67 / 100: avg data time: 6.60e-02, avg batch time: 0.5084, average train loss: 0.5035
[09/26 13:52:38 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1669, average loss: 2.8925
[09/26 13:52:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 91.00	
[09/26 13:52:38 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/26 13:52:45 visual_prompt]: Epoch 68 / 100: avg data time: 5.94e-02, avg batch time: 0.5029, average train loss: 0.3984
[09/26 13:52:47 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1667, average loss: 3.6050
[09/26 13:52:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 91.00	
[09/26 13:52:47 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/26 13:52:54 visual_prompt]: Epoch 69 / 100: avg data time: 6.20e-02, avg batch time: 0.5042, average train loss: 0.3789
[09/26 13:52:55 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1671, average loss: 3.6504
[09/26 13:52:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 91.50	
[09/26 13:52:55 visual_prompt]: Best epoch 69: best metric: 0.355
[09/26 13:52:55 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/26 13:53:02 visual_prompt]: Epoch 70 / 100: avg data time: 6.54e-02, avg batch time: 0.5077, average train loss: 0.4238
[09/26 13:53:04 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1669, average loss: 3.5972
[09/26 13:53:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.50	
[09/26 13:53:04 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/26 13:53:11 visual_prompt]: Epoch 71 / 100: avg data time: 6.96e-02, avg batch time: 0.5123, average train loss: 0.4360
[09/26 13:53:12 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1668, average loss: 3.4540
[09/26 13:53:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 94.00	
[09/26 13:53:12 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/26 13:53:19 visual_prompt]: Epoch 72 / 100: avg data time: 5.01e-02, avg batch time: 0.4957, average train loss: 0.3693
[09/26 13:53:21 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1668, average loss: 4.1941
[09/26 13:53:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.50	
[09/26 13:53:21 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/26 13:53:28 visual_prompt]: Epoch 73 / 100: avg data time: 6.15e-02, avg batch time: 0.5049, average train loss: 0.3250
[09/26 13:53:29 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1674, average loss: 4.2831
[09/26 13:53:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 92.50	
[09/26 13:53:29 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/26 13:53:36 visual_prompt]: Epoch 74 / 100: avg data time: 6.46e-02, avg batch time: 0.5082, average train loss: 0.3078
[09/26 13:53:38 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1671, average loss: 4.4498
[09/26 13:53:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 13:53:38 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/26 13:53:45 visual_prompt]: Epoch 75 / 100: avg data time: 6.51e-02, avg batch time: 0.5081, average train loss: 0.2955
[09/26 13:53:46 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1676, average loss: 4.6191
[09/26 13:53:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 92.00	
[09/26 13:53:46 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/26 13:53:53 visual_prompt]: Epoch 76 / 100: avg data time: 6.50e-02, avg batch time: 0.5085, average train loss: 0.2762
[09/26 13:53:55 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1674, average loss: 4.7831
[09/26 13:53:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.50	
[09/26 13:53:55 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/26 13:54:02 visual_prompt]: Epoch 77 / 100: avg data time: 5.78e-02, avg batch time: 0.5001, average train loss: 0.2464
[09/26 13:54:04 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1679, average loss: 4.5546
[09/26 13:54:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 92.50	
[09/26 13:54:04 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/26 13:54:10 visual_prompt]: Epoch 78 / 100: avg data time: 6.71e-02, avg batch time: 0.5099, average train loss: 0.2058
[09/26 13:54:12 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1674, average loss: 5.2854
[09/26 13:54:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 92.00	
[09/26 13:54:12 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/26 13:54:19 visual_prompt]: Epoch 79 / 100: avg data time: 6.65e-02, avg batch time: 0.5092, average train loss: 0.1905
[09/26 13:54:21 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1677, average loss: 5.0595
[09/26 13:54:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 91.50	
[09/26 13:54:21 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/26 13:54:28 visual_prompt]: Epoch 80 / 100: avg data time: 6.41e-02, avg batch time: 0.5078, average train loss: 0.1852
[09/26 13:54:29 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1676, average loss: 4.9790
[09/26 13:54:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 93.00	
[09/26 13:54:29 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/26 13:54:36 visual_prompt]: Epoch 81 / 100: avg data time: 6.77e-02, avg batch time: 0.5112, average train loss: 0.1668
[09/26 13:54:38 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1679, average loss: 5.4005
[09/26 13:54:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 93.00	
[09/26 13:54:38 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/26 13:54:45 visual_prompt]: Epoch 82 / 100: avg data time: 6.56e-02, avg batch time: 0.5109, average train loss: 0.1654
[09/26 13:54:47 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1676, average loss: 5.4234
[09/26 13:54:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 93.00	
[09/26 13:54:47 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/26 13:54:54 visual_prompt]: Epoch 83 / 100: avg data time: 6.78e-02, avg batch time: 0.5100, average train loss: 0.1325
[09/26 13:54:55 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1671, average loss: 5.6241
[09/26 13:54:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 92.50	
[09/26 13:54:55 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/26 13:55:02 visual_prompt]: Epoch 84 / 100: avg data time: 6.56e-02, avg batch time: 0.5100, average train loss: 0.1221
[09/26 13:55:04 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1670, average loss: 5.4865
[09/26 13:55:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 92.00	
[09/26 13:55:04 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/26 13:55:11 visual_prompt]: Epoch 85 / 100: avg data time: 5.45e-02, avg batch time: 0.4973, average train loss: 0.1178
[09/26 13:55:12 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1669, average loss: 5.9842
[09/26 13:55:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.50	
[09/26 13:55:12 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/26 13:55:19 visual_prompt]: Epoch 86 / 100: avg data time: 5.57e-02, avg batch time: 0.4985, average train loss: 0.1198
[09/26 13:55:21 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1670, average loss: 5.8769
[09/26 13:55:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 13:55:21 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/26 13:55:28 visual_prompt]: Epoch 87 / 100: avg data time: 5.60e-02, avg batch time: 0.4997, average train loss: 0.1174
[09/26 13:55:29 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1678, average loss: 5.6533
[09/26 13:55:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 92.50	
[09/26 13:55:29 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/26 13:55:36 visual_prompt]: Epoch 88 / 100: avg data time: 4.85e-02, avg batch time: 0.4906, average train loss: 0.0950
[09/26 13:55:38 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1670, average loss: 6.2494
[09/26 13:55:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 93.00	
[09/26 13:55:38 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/26 13:55:45 visual_prompt]: Epoch 89 / 100: avg data time: 4.58e-02, avg batch time: 0.4885, average train loss: 0.0943
[09/26 13:55:46 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1670, average loss: 5.8959
[09/26 13:55:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 93.50	
[09/26 13:55:46 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/26 13:55:53 visual_prompt]: Epoch 90 / 100: avg data time: 5.86e-02, avg batch time: 0.5016, average train loss: 0.0848
[09/26 13:55:55 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1669, average loss: 5.8409
[09/26 13:55:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 93.00	
[09/26 13:55:55 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/26 13:56:02 visual_prompt]: Epoch 91 / 100: avg data time: 5.19e-02, avg batch time: 0.4964, average train loss: 0.0668
[09/26 13:56:03 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1670, average loss: 6.0807
[09/26 13:56:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.00	
[09/26 13:56:03 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/26 13:56:10 visual_prompt]: Epoch 92 / 100: avg data time: 5.25e-02, avg batch time: 0.4975, average train loss: 0.0670
[09/26 13:56:12 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1675, average loss: 6.1047
[09/26 13:56:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 92.50	
[09/26 13:56:12 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/26 13:56:19 visual_prompt]: Epoch 93 / 100: avg data time: 4.64e-02, avg batch time: 0.4905, average train loss: 0.0774
[09/26 13:56:20 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1674, average loss: 6.3421
[09/26 13:56:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 93.00	
[09/26 13:56:20 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/26 13:56:27 visual_prompt]: Epoch 94 / 100: avg data time: 4.41e-02, avg batch time: 0.4883, average train loss: 0.0675
[09/26 13:56:29 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1674, average loss: 6.0900
[09/26 13:56:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 13:56:29 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/26 13:56:35 visual_prompt]: Epoch 95 / 100: avg data time: 5.28e-02, avg batch time: 0.4965, average train loss: 0.0735
[09/26 13:56:37 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1671, average loss: 6.3049
[09/26 13:56:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 93.00	
[09/26 13:56:37 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/26 13:56:44 visual_prompt]: Epoch 96 / 100: avg data time: 6.14e-02, avg batch time: 0.5043, average train loss: 0.0741
[09/26 13:56:46 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1676, average loss: 6.2438
[09/26 13:56:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 92.50	
[09/26 13:56:46 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/26 13:56:52 visual_prompt]: Epoch 97 / 100: avg data time: 5.46e-02, avg batch time: 0.4997, average train loss: 0.0687
[09/26 13:56:54 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1672, average loss: 6.2666
[09/26 13:56:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 92.00	
[09/26 13:56:54 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/26 13:57:01 visual_prompt]: Epoch 98 / 100: avg data time: 5.35e-02, avg batch time: 0.4975, average train loss: 0.0629
[09/26 13:57:03 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1674, average loss: 6.2675
[09/26 13:57:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 92.00	
[09/26 13:57:03 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/26 13:57:09 visual_prompt]: Epoch 99 / 100: avg data time: 4.87e-02, avg batch time: 0.4919, average train loss: 0.0708
[09/26 13:57:11 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1677, average loss: 6.2609
[09/26 13:57:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.00	
[09/26 13:57:11 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/26 13:57:18 visual_prompt]: Epoch 100 / 100: avg data time: 6.01e-02, avg batch time: 0.5027, average train loss: 0.0591
[09/26 13:57:20 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1677, average loss: 6.2602
[09/26 13:57:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.00	
[09/26 13:57:20 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 13:57:20 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 13:57:20 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 13:57:20 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 13:57:20 visual_prompt]: Training with config:
[09/26 13:57:20 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 13:57:20 visual_prompt]: Loading training data...
[09/26 13:57:20 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 13:57:21 visual_prompt]: Number of images: 800
[09/26 13:57:21 visual_prompt]: Number of classes: 9 / 9
[09/26 13:57:21 visual_prompt]: Loading validation data...
[09/26 13:57:21 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 13:57:21 visual_prompt]: Number of images: 200
[09/26 13:57:21 visual_prompt]: Number of classes: 9 / 9
[09/26 13:57:21 visual_prompt]: Constructing models...
[09/26 13:57:24 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 13:57:24 visual_prompt]: tuned percent:0.542
[09/26 13:57:24 visual_prompt]: Device used for model: 0
[09/26 13:57:24 visual_prompt]: Setting up Evaluator...
[09/26 13:57:24 visual_prompt]: Setting up Trainer...
[09/26 13:57:24 visual_prompt]: 	Setting up the optimizer...
[09/26 13:57:24 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 13:57:30 visual_prompt]: Epoch 1 / 100: avg data time: 5.19e-02, avg batch time: 0.4960, average train loss: 2.8624
[09/26 13:57:32 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1675, average loss: 2.9516
[09/26 13:57:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 13:57:32 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 13:57:32 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 13:57:39 visual_prompt]: Epoch 2 / 100: avg data time: 5.61e-02, avg batch time: 0.5001, average train loss: 3.6488
[09/26 13:57:40 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1664, average loss: 3.5028
[09/26 13:57:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 48.50	
[09/26 13:57:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 13:57:47 visual_prompt]: Epoch 3 / 100: avg data time: 6.21e-02, avg batch time: 0.5051, average train loss: 2.6214
[09/26 13:57:49 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1674, average loss: 2.3177
[09/26 13:57:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 58.50	
[09/26 13:57:49 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 13:57:56 visual_prompt]: Epoch 4 / 100: avg data time: 5.41e-02, avg batch time: 0.4980, average train loss: 2.2910
[09/26 13:57:57 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1674, average loss: 2.2731
[09/26 13:57:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 13:57:57 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 13:58:04 visual_prompt]: Epoch 5 / 100: avg data time: 6.03e-02, avg batch time: 0.5031, average train loss: 2.3033
[09/26 13:58:06 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1675, average loss: 2.3312
[09/26 13:58:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.50	
[09/26 13:58:06 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 13:58:13 visual_prompt]: Epoch 6 / 100: avg data time: 5.59e-02, avg batch time: 0.4984, average train loss: 2.5266
[09/26 13:58:14 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1674, average loss: 2.5404
[09/26 13:58:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 13:58:14 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 13:58:21 visual_prompt]: Epoch 7 / 100: avg data time: 4.70e-02, avg batch time: 0.4920, average train loss: 2.7986
[09/26 13:58:22 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1668, average loss: 2.3124
[09/26 13:58:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.00	
[09/26 13:58:22 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 13:58:29 visual_prompt]: Epoch 8 / 100: avg data time: 5.85e-02, avg batch time: 0.5020, average train loss: 2.5604
[09/26 13:58:31 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1676, average loss: 2.5075
[09/26 13:58:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 13:58:31 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 13:58:38 visual_prompt]: Epoch 9 / 100: avg data time: 5.38e-02, avg batch time: 0.4972, average train loss: 2.5530
[09/26 13:58:39 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1668, average loss: 2.6022
[09/26 13:58:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 13:58:39 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 13:58:46 visual_prompt]: Epoch 10 / 100: avg data time: 6.21e-02, avg batch time: 0.5057, average train loss: 2.6561
[09/26 13:58:48 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1672, average loss: 2.6071
[09/26 13:58:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 13:58:48 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 13:58:55 visual_prompt]: Epoch 11 / 100: avg data time: 5.37e-02, avg batch time: 0.4991, average train loss: 2.8034
[09/26 13:58:56 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1674, average loss: 2.7593
[09/26 13:58:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 13:58:56 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 13:59:03 visual_prompt]: Epoch 12 / 100: avg data time: 5.16e-02, avg batch time: 0.4960, average train loss: 2.4841
[09/26 13:59:05 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1673, average loss: 2.7625
[09/26 13:59:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 13:59:05 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 13:59:11 visual_prompt]: Epoch 13 / 100: avg data time: 4.62e-02, avg batch time: 0.4898, average train loss: 2.5838
[09/26 13:59:13 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1670, average loss: 2.7920
[09/26 13:59:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 13:59:13 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 13:59:20 visual_prompt]: Epoch 14 / 100: avg data time: 5.77e-02, avg batch time: 0.5017, average train loss: 2.6199
[09/26 13:59:21 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1672, average loss: 2.5262
[09/26 13:59:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 13:59:21 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 13:59:28 visual_prompt]: Epoch 15 / 100: avg data time: 5.77e-02, avg batch time: 0.4998, average train loss: 4.5116
[09/26 13:59:30 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1667, average loss: 6.9014
[09/26 13:59:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.50	top5: 55.00	
[09/26 13:59:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 13:59:36 visual_prompt]: Epoch 16 / 100: avg data time: 4.91e-02, avg batch time: 0.4940, average train loss: 6.7916
[09/26 13:59:38 visual_prompt]: Inference (val):avg data time: 1.99e-05, avg batch time: 0.1670, average loss: 9.3975
[09/26 13:59:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/26 13:59:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 13:59:45 visual_prompt]: Epoch 17 / 100: avg data time: 5.82e-02, avg batch time: 0.5021, average train loss: 6.6909
[09/26 13:59:47 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1672, average loss: 3.7447
[09/26 13:59:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 56.50	
[09/26 13:59:47 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 13:59:53 visual_prompt]: Epoch 18 / 100: avg data time: 6.04e-02, avg batch time: 0.5032, average train loss: 5.4202
[09/26 13:59:55 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1675, average loss: 4.0208
[09/26 13:59:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 13:59:55 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 14:00:02 visual_prompt]: Epoch 19 / 100: avg data time: 6.22e-02, avg batch time: 0.5059, average train loss: 3.6021
[09/26 14:00:03 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1668, average loss: 3.5771
[09/26 14:00:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 14:00:03 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 14:00:10 visual_prompt]: Epoch 20 / 100: avg data time: 5.73e-02, avg batch time: 0.5002, average train loss: 6.6254
[09/26 14:00:12 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1669, average loss: 5.4977
[09/26 14:00:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 14:00:12 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 14:00:19 visual_prompt]: Epoch 21 / 100: avg data time: 5.45e-02, avg batch time: 0.4973, average train loss: 4.6846
[09/26 14:00:20 visual_prompt]: Inference (val):avg data time: 1.92e-05, avg batch time: 0.1670, average loss: 4.9524
[09/26 14:00:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 14:00:20 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 14:00:27 visual_prompt]: Epoch 22 / 100: avg data time: 5.25e-02, avg batch time: 0.4959, average train loss: 5.6087
[09/26 14:00:29 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1675, average loss: 8.8673
[09/26 14:00:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 48.50	
[09/26 14:00:29 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 14:00:35 visual_prompt]: Epoch 23 / 100: avg data time: 5.37e-02, avg batch time: 0.4981, average train loss: 5.5874
[09/26 14:00:37 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1673, average loss: 3.7164
[09/26 14:00:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 14:00:37 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 14:00:44 visual_prompt]: Epoch 24 / 100: avg data time: 5.94e-02, avg batch time: 0.5027, average train loss: 4.9883
[09/26 14:00:46 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1671, average loss: 6.8679
[09/26 14:00:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 14:00:46 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 14:00:52 visual_prompt]: Epoch 25 / 100: avg data time: 4.77e-02, avg batch time: 0.4948, average train loss: 6.5213
[09/26 14:00:54 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1673, average loss: 9.7170
[09/26 14:00:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 14:00:54 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 14:01:01 visual_prompt]: Epoch 26 / 100: avg data time: 5.00e-02, avg batch time: 0.4944, average train loss: 8.3646
[09/26 14:01:02 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1672, average loss: 8.6581
[09/26 14:01:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 53.00	
[09/26 14:01:02 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 14:01:09 visual_prompt]: Epoch 27 / 100: avg data time: 5.41e-02, avg batch time: 0.4972, average train loss: 5.1987
[09/26 14:01:11 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1670, average loss: 3.6828
[09/26 14:01:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 61.50	
[09/26 14:01:11 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 14:01:17 visual_prompt]: Epoch 28 / 100: avg data time: 6.17e-02, avg batch time: 0.5054, average train loss: 3.7508
[09/26 14:01:19 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1673, average loss: 4.8795
[09/26 14:01:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 14:01:19 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 14:01:26 visual_prompt]: Epoch 29 / 100: avg data time: 5.37e-02, avg batch time: 0.4964, average train loss: 4.4771
[09/26 14:01:27 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1669, average loss: 3.6585
[09/26 14:01:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 14:01:27 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 14:01:34 visual_prompt]: Epoch 30 / 100: avg data time: 6.58e-02, avg batch time: 0.5087, average train loss: 4.6682
[09/26 14:01:36 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1673, average loss: 5.8633
[09/26 14:01:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.50	
[09/26 14:01:36 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 14:01:43 visual_prompt]: Epoch 31 / 100: avg data time: 5.77e-02, avg batch time: 0.5012, average train loss: 5.3026
[09/26 14:01:44 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1673, average loss: 7.8245
[09/26 14:01:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 49.50	
[09/26 14:01:44 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 14:01:51 visual_prompt]: Epoch 32 / 100: avg data time: 5.98e-02, avg batch time: 0.5039, average train loss: 6.7610
[09/26 14:01:53 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1672, average loss: 5.8007
[09/26 14:01:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 14:01:53 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 14:02:00 visual_prompt]: Epoch 33 / 100: avg data time: 4.94e-02, avg batch time: 0.4939, average train loss: 4.5840
[09/26 14:02:01 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1671, average loss: 3.8837
[09/26 14:02:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 52.00	
[09/26 14:02:01 visual_prompt]: Best epoch 33: best metric: 0.145
[09/26 14:02:01 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 14:02:08 visual_prompt]: Epoch 34 / 100: avg data time: 5.15e-02, avg batch time: 0.4949, average train loss: 2.9858
[09/26 14:02:10 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1671, average loss: 3.6257
[09/26 14:02:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 14:02:10 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 14:02:16 visual_prompt]: Epoch 35 / 100: avg data time: 5.01e-02, avg batch time: 0.4949, average train loss: 3.1662
[09/26 14:02:18 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1672, average loss: 2.8972
[09/26 14:02:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 14:02:18 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 14:02:25 visual_prompt]: Epoch 36 / 100: avg data time: 5.40e-02, avg batch time: 0.4974, average train loss: 2.8730
[09/26 14:02:26 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1673, average loss: 2.7119
[09/26 14:02:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 14:02:26 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 14:02:33 visual_prompt]: Epoch 37 / 100: avg data time: 5.24e-02, avg batch time: 0.4945, average train loss: 2.5376
[09/26 14:02:35 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1675, average loss: 2.5481
[09/26 14:02:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 14:02:35 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 14:02:42 visual_prompt]: Epoch 38 / 100: avg data time: 4.83e-02, avg batch time: 0.4940, average train loss: 2.9002
[09/26 14:02:43 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1673, average loss: 5.6968
[09/26 14:02:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 48.50	
[09/26 14:02:43 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 14:02:50 visual_prompt]: Epoch 39 / 100: avg data time: 6.56e-02, avg batch time: 0.5077, average train loss: 5.0133
[09/26 14:02:52 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1671, average loss: 4.6231
[09/26 14:02:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 14:02:52 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 14:02:58 visual_prompt]: Epoch 40 / 100: avg data time: 5.33e-02, avg batch time: 0.4963, average train loss: 3.4846
[09/26 14:03:00 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1671, average loss: 2.8529
[09/26 14:03:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 14:03:00 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 14:03:07 visual_prompt]: Epoch 41 / 100: avg data time: 5.50e-02, avg batch time: 0.4988, average train loss: 3.8199
[09/26 14:03:08 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1670, average loss: 2.4861
[09/26 14:03:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 14:03:08 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 14:03:15 visual_prompt]: Epoch 42 / 100: avg data time: 5.15e-02, avg batch time: 0.4948, average train loss: 2.6354
[09/26 14:03:17 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1671, average loss: 2.9607
[09/26 14:03:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:03:17 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 14:03:24 visual_prompt]: Epoch 43 / 100: avg data time: 5.20e-02, avg batch time: 0.4956, average train loss: 2.7192
[09/26 14:03:25 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1674, average loss: 2.2967
[09/26 14:03:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 57.50	
[09/26 14:03:25 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 14:03:32 visual_prompt]: Epoch 44 / 100: avg data time: 4.81e-02, avg batch time: 0.4927, average train loss: 2.8553
[09/26 14:03:34 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1670, average loss: 3.5308
[09/26 14:03:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 14:03:34 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 14:03:40 visual_prompt]: Epoch 45 / 100: avg data time: 5.44e-02, avg batch time: 0.4982, average train loss: 3.8201
[09/26 14:03:42 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1670, average loss: 4.7264
[09/26 14:03:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 14:03:42 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 14:03:49 visual_prompt]: Epoch 46 / 100: avg data time: 5.89e-02, avg batch time: 0.5026, average train loss: 5.5886
[09/26 14:03:50 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1673, average loss: 5.5814
[09/26 14:03:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.00	
[09/26 14:03:50 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 14:03:57 visual_prompt]: Epoch 47 / 100: avg data time: 6.36e-02, avg batch time: 0.5060, average train loss: 4.0907
[09/26 14:03:59 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1669, average loss: 3.4291
[09/26 14:03:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/26 14:03:59 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 14:04:06 visual_prompt]: Epoch 48 / 100: avg data time: 5.27e-02, avg batch time: 0.4963, average train loss: 3.7722
[09/26 14:04:07 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1671, average loss: 3.6612
[09/26 14:04:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 62.50	
[09/26 14:04:07 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 14:04:14 visual_prompt]: Epoch 49 / 100: avg data time: 4.99e-02, avg batch time: 0.4949, average train loss: 3.0528
[09/26 14:04:16 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1670, average loss: 2.5619
[09/26 14:04:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 14:04:16 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 14:04:22 visual_prompt]: Epoch 50 / 100: avg data time: 5.31e-02, avg batch time: 0.4977, average train loss: 2.4487
[09/26 14:04:24 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1672, average loss: 2.3493
[09/26 14:04:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 14:04:24 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 14:04:31 visual_prompt]: Epoch 51 / 100: avg data time: 6.16e-02, avg batch time: 0.5053, average train loss: 2.3235
[09/26 14:04:32 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1670, average loss: 2.2921
[09/26 14:04:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 61.50	
[09/26 14:04:32 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 14:04:39 visual_prompt]: Epoch 52 / 100: avg data time: 5.54e-02, avg batch time: 0.4990, average train loss: 2.3091
[09/26 14:04:41 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1670, average loss: 2.3161
[09/26 14:04:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 14:04:41 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 14:04:48 visual_prompt]: Epoch 53 / 100: avg data time: 4.90e-02, avg batch time: 0.4936, average train loss: 2.2820
[09/26 14:04:49 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1672, average loss: 2.4765
[09/26 14:04:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 14:04:49 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 14:04:56 visual_prompt]: Epoch 54 / 100: avg data time: 4.73e-02, avg batch time: 0.4898, average train loss: 2.3551
[09/26 14:04:57 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1671, average loss: 2.5690
[09/26 14:04:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 14:04:57 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 14:05:04 visual_prompt]: Epoch 55 / 100: avg data time: 5.91e-02, avg batch time: 0.5013, average train loss: 2.5944
[09/26 14:05:06 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1671, average loss: 2.5594
[09/26 14:05:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 55.50	
[09/26 14:05:06 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 14:05:13 visual_prompt]: Epoch 56 / 100: avg data time: 4.90e-02, avg batch time: 0.4935, average train loss: 2.7756
[09/26 14:05:14 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1670, average loss: 2.6398
[09/26 14:05:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 14:05:14 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 14:05:21 visual_prompt]: Epoch 57 / 100: avg data time: 4.85e-02, avg batch time: 0.4929, average train loss: 2.8121
[09/26 14:05:23 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1670, average loss: 2.5265
[09/26 14:05:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 14:05:23 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 14:05:29 visual_prompt]: Epoch 58 / 100: avg data time: 6.05e-02, avg batch time: 0.5022, average train loss: 2.3481
[09/26 14:05:31 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1669, average loss: 2.3682
[09/26 14:05:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 14:05:31 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 14:05:38 visual_prompt]: Epoch 59 / 100: avg data time: 5.19e-02, avg batch time: 0.4964, average train loss: 2.3563
[09/26 14:05:39 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1668, average loss: 2.2907
[09/26 14:05:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 14:05:39 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 14:05:46 visual_prompt]: Epoch 60 / 100: avg data time: 5.20e-02, avg batch time: 0.4945, average train loss: 2.3133
[09/26 14:05:48 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1668, average loss: 2.3310
[09/26 14:05:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 14:05:48 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 14:05:55 visual_prompt]: Epoch 61 / 100: avg data time: 5.30e-02, avg batch time: 0.4948, average train loss: 2.3928
[09/26 14:05:56 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1670, average loss: 2.2975
[09/26 14:05:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 14:05:56 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 14:06:03 visual_prompt]: Epoch 62 / 100: avg data time: 5.32e-02, avg batch time: 0.4985, average train loss: 2.2656
[09/26 14:06:05 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1670, average loss: 2.2543
[09/26 14:06:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.50	
[09/26 14:06:05 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 14:06:12 visual_prompt]: Epoch 63 / 100: avg data time: 6.38e-02, avg batch time: 0.5083, average train loss: 2.2637
[09/26 14:06:13 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1673, average loss: 2.2862
[09/26 14:06:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.00	
[09/26 14:06:13 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 14:06:20 visual_prompt]: Epoch 64 / 100: avg data time: 5.78e-02, avg batch time: 0.5024, average train loss: 2.2800
[09/26 14:06:22 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1671, average loss: 2.3102
[09/26 14:06:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 14:06:22 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 14:06:29 visual_prompt]: Epoch 65 / 100: avg data time: 6.17e-02, avg batch time: 0.5052, average train loss: 2.2902
[09/26 14:06:30 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1672, average loss: 2.2473
[09/26 14:06:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 14:06:30 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 14:06:37 visual_prompt]: Epoch 66 / 100: avg data time: 5.97e-02, avg batch time: 0.5038, average train loss: 2.2605
[09/26 14:06:39 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1674, average loss: 2.2183
[09/26 14:06:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 14:06:39 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 14:06:46 visual_prompt]: Epoch 67 / 100: avg data time: 6.61e-02, avg batch time: 0.5093, average train loss: 2.2458
[09/26 14:06:47 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1673, average loss: 2.1980
[09/26 14:06:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 14:06:47 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 14:06:54 visual_prompt]: Epoch 68 / 100: avg data time: 6.46e-02, avg batch time: 0.5083, average train loss: 2.2402
[09/26 14:06:56 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1672, average loss: 2.2843
[09/26 14:06:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 14:06:56 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 14:07:03 visual_prompt]: Epoch 69 / 100: avg data time: 6.39e-02, avg batch time: 0.5063, average train loss: 2.2719
[09/26 14:07:04 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1672, average loss: 2.2633
[09/26 14:07:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 14:07:04 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 14:07:11 visual_prompt]: Epoch 70 / 100: avg data time: 6.07e-02, avg batch time: 0.5030, average train loss: 2.2762
[09/26 14:07:13 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1671, average loss: 2.3108
[09/26 14:07:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 14:07:13 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 14:07:20 visual_prompt]: Epoch 71 / 100: avg data time: 5.98e-02, avg batch time: 0.5024, average train loss: 2.2822
[09/26 14:07:21 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1670, average loss: 2.2786
[09/26 14:07:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 14:07:21 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 14:07:28 visual_prompt]: Epoch 72 / 100: avg data time: 6.51e-02, avg batch time: 0.5085, average train loss: 2.2747
[09/26 14:07:30 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1672, average loss: 2.2905
[09/26 14:07:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 14:07:30 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 14:07:37 visual_prompt]: Epoch 73 / 100: avg data time: 5.97e-02, avg batch time: 0.5028, average train loss: 2.2627
[09/26 14:07:38 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1672, average loss: 2.2071
[09/26 14:07:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 14:07:38 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 14:07:45 visual_prompt]: Epoch 74 / 100: avg data time: 6.57e-02, avg batch time: 0.5080, average train loss: 2.2214
[09/26 14:07:47 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1671, average loss: 2.1985
[09/26 14:07:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 14:07:47 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 14:07:54 visual_prompt]: Epoch 75 / 100: avg data time: 5.19e-02, avg batch time: 0.4951, average train loss: 2.2190
[09/26 14:07:55 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1674, average loss: 2.1967
[09/26 14:07:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 14:07:55 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 14:08:02 visual_prompt]: Epoch 76 / 100: avg data time: 6.39e-02, avg batch time: 0.5068, average train loss: 2.2295
[09/26 14:08:04 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1671, average loss: 2.2069
[09/26 14:08:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 14:08:04 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 14:08:11 visual_prompt]: Epoch 77 / 100: avg data time: 6.18e-02, avg batch time: 0.5059, average train loss: 2.2205
[09/26 14:08:12 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1671, average loss: 2.1953
[09/26 14:08:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 14:08:12 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 14:08:19 visual_prompt]: Epoch 78 / 100: avg data time: 5.77e-02, avg batch time: 0.5001, average train loss: 2.2148
[09/26 14:08:21 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1673, average loss: 2.2138
[09/26 14:08:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 14:08:21 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 14:08:28 visual_prompt]: Epoch 79 / 100: avg data time: 7.24e-02, avg batch time: 0.5149, average train loss: 2.2116
[09/26 14:08:29 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1669, average loss: 2.2122
[09/26 14:08:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 14:08:29 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 14:08:36 visual_prompt]: Epoch 80 / 100: avg data time: 6.90e-02, avg batch time: 0.5109, average train loss: 2.2393
[09/26 14:08:38 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1667, average loss: 2.2501
[09/26 14:08:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 14:08:38 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 14:08:45 visual_prompt]: Epoch 81 / 100: avg data time: 5.96e-02, avg batch time: 0.5033, average train loss: 2.2350
[09/26 14:08:47 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1666, average loss: 2.2499
[09/26 14:08:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 14:08:47 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 14:08:54 visual_prompt]: Epoch 82 / 100: avg data time: 6.97e-02, avg batch time: 0.5117, average train loss: 2.2187
[09/26 14:08:55 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1667, average loss: 2.1997
[09/26 14:08:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 14:08:55 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 14:09:02 visual_prompt]: Epoch 83 / 100: avg data time: 6.27e-02, avg batch time: 0.5062, average train loss: 2.2274
[09/26 14:09:04 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1667, average loss: 2.2304
[09/26 14:09:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 14:09:04 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 14:09:11 visual_prompt]: Epoch 84 / 100: avg data time: 6.41e-02, avg batch time: 0.5072, average train loss: 2.2098
[09/26 14:09:12 visual_prompt]: Inference (val):avg data time: 4.55e-05, avg batch time: 0.1672, average loss: 2.2099
[09/26 14:09:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 14:09:12 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 14:09:19 visual_prompt]: Epoch 85 / 100: avg data time: 5.43e-02, avg batch time: 0.4965, average train loss: 2.2086
[09/26 14:09:21 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1666, average loss: 2.2038
[09/26 14:09:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:09:21 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 14:09:28 visual_prompt]: Epoch 86 / 100: avg data time: 6.81e-02, avg batch time: 0.5102, average train loss: 2.2089
[09/26 14:09:29 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1667, average loss: 2.1986
[09/26 14:09:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 14:09:29 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 14:09:36 visual_prompt]: Epoch 87 / 100: avg data time: 6.63e-02, avg batch time: 0.5079, average train loss: 2.2041
[09/26 14:09:38 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1666, average loss: 2.1883
[09/26 14:09:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 14:09:38 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 14:09:45 visual_prompt]: Epoch 88 / 100: avg data time: 4.97e-02, avg batch time: 0.4928, average train loss: 2.1946
[09/26 14:09:46 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1671, average loss: 2.1997
[09/26 14:09:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:09:46 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 14:09:53 visual_prompt]: Epoch 89 / 100: avg data time: 4.55e-02, avg batch time: 0.4892, average train loss: 2.1997
[09/26 14:09:55 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1668, average loss: 2.1977
[09/26 14:09:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 14:09:55 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 14:10:02 visual_prompt]: Epoch 90 / 100: avg data time: 6.58e-02, avg batch time: 0.5088, average train loss: 2.1946
[09/26 14:10:04 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1667, average loss: 2.2011
[09/26 14:10:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:10:04 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 14:10:10 visual_prompt]: Epoch 91 / 100: avg data time: 5.94e-02, avg batch time: 0.5022, average train loss: 2.1941
[09/26 14:10:12 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1668, average loss: 2.1969
[09/26 14:10:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:10:12 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 14:10:19 visual_prompt]: Epoch 92 / 100: avg data time: 6.65e-02, avg batch time: 0.5081, average train loss: 2.1919
[09/26 14:10:21 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1671, average loss: 2.1961
[09/26 14:10:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:10:21 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 14:10:27 visual_prompt]: Epoch 93 / 100: avg data time: 6.03e-02, avg batch time: 0.5027, average train loss: 2.1945
[09/26 14:10:29 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1672, average loss: 2.1944
[09/26 14:10:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:10:29 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 14:10:36 visual_prompt]: Epoch 94 / 100: avg data time: 6.05e-02, avg batch time: 0.5030, average train loss: 2.1917
[09/26 14:10:38 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1670, average loss: 2.1967
[09/26 14:10:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 14:10:38 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 14:10:44 visual_prompt]: Epoch 95 / 100: avg data time: 4.88e-02, avg batch time: 0.4930, average train loss: 2.1913
[09/26 14:10:46 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1669, average loss: 2.1965
[09/26 14:10:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:10:46 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 14:10:53 visual_prompt]: Epoch 96 / 100: avg data time: 5.84e-02, avg batch time: 0.5003, average train loss: 2.1909
[09/26 14:10:54 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1671, average loss: 2.1959
[09/26 14:10:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:10:54 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 14:11:01 visual_prompt]: Epoch 97 / 100: avg data time: 5.65e-02, avg batch time: 0.4989, average train loss: 2.1907
[09/26 14:11:03 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1668, average loss: 2.1962
[09/26 14:11:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:11:03 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 14:11:10 visual_prompt]: Epoch 98 / 100: avg data time: 5.22e-02, avg batch time: 0.4956, average train loss: 2.1904
[09/26 14:11:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1670, average loss: 2.1962
[09/26 14:11:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:11:11 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 14:11:18 visual_prompt]: Epoch 99 / 100: avg data time: 5.50e-02, avg batch time: 0.4977, average train loss: 2.1903
[09/26 14:11:20 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1671, average loss: 2.1962
[09/26 14:11:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:11:20 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 14:11:27 visual_prompt]: Epoch 100 / 100: avg data time: 6.13e-02, avg batch time: 0.5046, average train loss: 2.1903
[09/26 14:11:28 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1672, average loss: 2.1962
[09/26 14:11:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:11:28 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:11:28 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:11:28 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:11:28 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:11:28 visual_prompt]: Training with config:
[09/26 14:11:28 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:11:28 visual_prompt]: Loading training data...
[09/26 14:11:28 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 14:11:29 visual_prompt]: Number of images: 800
[09/26 14:11:29 visual_prompt]: Number of classes: 9 / 9
[09/26 14:11:29 visual_prompt]: Loading validation data...
[09/26 14:11:29 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 14:11:30 visual_prompt]: Number of images: 200
[09/26 14:11:30 visual_prompt]: Number of classes: 9 / 9
[09/26 14:11:30 visual_prompt]: Constructing models...
[09/26 14:11:32 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 14:11:32 visual_prompt]: tuned percent:0.542
[09/26 14:11:32 visual_prompt]: Device used for model: 0
[09/26 14:11:32 visual_prompt]: Setting up Evaluator...
[09/26 14:11:32 visual_prompt]: Setting up Trainer...
[09/26 14:11:32 visual_prompt]: 	Setting up the optimizer...
[09/26 14:11:32 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:11:39 visual_prompt]: Epoch 1 / 100: avg data time: 5.86e-02, avg batch time: 0.5019, average train loss: 2.8711
[09/26 14:11:41 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1667, average loss: 2.9516
[09/26 14:11:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 14:11:41 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 14:11:41 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 14:11:47 visual_prompt]: Epoch 2 / 100: avg data time: 5.55e-02, avg batch time: 0.4987, average train loss: 3.6272
[09/26 14:11:49 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1666, average loss: 3.5457
[09/26 14:11:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.00	
[09/26 14:11:49 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 14:11:56 visual_prompt]: Epoch 3 / 100: avg data time: 4.92e-02, avg batch time: 0.4915, average train loss: 2.7562
[09/26 14:11:57 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1671, average loss: 2.2827
[09/26 14:11:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 14:11:57 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 14:12:04 visual_prompt]: Epoch 4 / 100: avg data time: 4.71e-02, avg batch time: 0.4916, average train loss: 2.3544
[09/26 14:12:06 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1668, average loss: 2.2473
[09/26 14:12:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 60.00	
[09/26 14:12:06 visual_prompt]: Best epoch 4: best metric: 0.135
[09/26 14:12:06 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 14:12:12 visual_prompt]: Epoch 5 / 100: avg data time: 4.92e-02, avg batch time: 0.4946, average train loss: 2.3311
[09/26 14:12:14 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1667, average loss: 2.2481
[09/26 14:12:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 14:12:14 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 14:12:21 visual_prompt]: Epoch 6 / 100: avg data time: 6.30e-02, avg batch time: 0.5059, average train loss: 2.2730
[09/26 14:12:22 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1674, average loss: 2.4781
[09/26 14:12:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 54.00	
[09/26 14:12:23 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 14:12:29 visual_prompt]: Epoch 7 / 100: avg data time: 6.00e-02, avg batch time: 0.5036, average train loss: 2.3674
[09/26 14:12:31 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1671, average loss: 2.7653
[09/26 14:12:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 14:12:31 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 14:12:38 visual_prompt]: Epoch 8 / 100: avg data time: 5.89e-02, avg batch time: 0.5027, average train loss: 2.4270
[09/26 14:12:39 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1668, average loss: 3.2578
[09/26 14:12:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 14:12:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 14:12:46 visual_prompt]: Epoch 9 / 100: avg data time: 4.57e-02, avg batch time: 0.4910, average train loss: 2.7462
[09/26 14:12:48 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1669, average loss: 2.4417
[09/26 14:12:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 50.00	
[09/26 14:12:48 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 14:12:55 visual_prompt]: Epoch 10 / 100: avg data time: 6.36e-02, avg batch time: 0.5067, average train loss: 2.3922
[09/26 14:12:56 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1669, average loss: 2.3945
[09/26 14:12:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 14:12:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 14:13:03 visual_prompt]: Epoch 11 / 100: avg data time: 5.37e-02, avg batch time: 0.4966, average train loss: 2.4954
[09/26 14:13:05 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1669, average loss: 2.6794
[09/26 14:13:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 14:13:05 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 14:13:11 visual_prompt]: Epoch 12 / 100: avg data time: 4.81e-02, avg batch time: 0.4921, average train loss: 2.9094
[09/26 14:13:13 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1669, average loss: 3.5649
[09/26 14:13:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 14:13:13 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 14:13:20 visual_prompt]: Epoch 13 / 100: avg data time: 4.94e-02, avg batch time: 0.4938, average train loss: 3.8991
[09/26 14:13:21 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1670, average loss: 5.3480
[09/26 14:13:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 14:13:21 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 14:13:28 visual_prompt]: Epoch 14 / 100: avg data time: 6.00e-02, avg batch time: 0.5034, average train loss: 3.7660
[09/26 14:13:30 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1670, average loss: 2.8055
[09/26 14:13:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 14:13:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 14:13:37 visual_prompt]: Epoch 15 / 100: avg data time: 4.90e-02, avg batch time: 0.4923, average train loss: 2.5574
[09/26 14:13:38 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1673, average loss: 2.4031
[09/26 14:13:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 14:13:38 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 14:13:45 visual_prompt]: Epoch 16 / 100: avg data time: 4.70e-02, avg batch time: 0.4926, average train loss: 2.4016
[09/26 14:13:46 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1668, average loss: 2.3113
[09/26 14:13:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 14:13:46 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 14:13:53 visual_prompt]: Epoch 17 / 100: avg data time: 5.45e-02, avg batch time: 0.4975, average train loss: 2.4680
[09/26 14:13:55 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1669, average loss: 2.5806
[09/26 14:13:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 14:13:55 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 14:14:02 visual_prompt]: Epoch 18 / 100: avg data time: 5.09e-02, avg batch time: 0.4946, average train loss: 2.4730
[09/26 14:14:03 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1668, average loss: 2.3144
[09/26 14:14:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 14:14:03 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 14:14:10 visual_prompt]: Epoch 19 / 100: avg data time: 5.78e-02, avg batch time: 0.5009, average train loss: 2.3485
[09/26 14:14:12 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1665, average loss: 2.3910
[09/26 14:14:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 14:14:12 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 14:14:18 visual_prompt]: Epoch 20 / 100: avg data time: 5.04e-02, avg batch time: 0.4939, average train loss: 2.4663
[09/26 14:14:20 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1669, average loss: 2.5621
[09/26 14:14:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 14:14:20 visual_prompt]: Best epoch 20: best metric: 0.145
[09/26 14:14:20 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 14:14:27 visual_prompt]: Epoch 21 / 100: avg data time: 5.85e-02, avg batch time: 0.5009, average train loss: 2.4932
[09/26 14:14:28 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1668, average loss: 2.4407
[09/26 14:14:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 14:14:28 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 14:14:35 visual_prompt]: Epoch 22 / 100: avg data time: 4.82e-02, avg batch time: 0.4905, average train loss: 2.4825
[09/26 14:14:37 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1666, average loss: 2.5484
[09/26 14:14:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 14:14:37 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 14:14:44 visual_prompt]: Epoch 23 / 100: avg data time: 5.71e-02, avg batch time: 0.4999, average train loss: 2.3498
[09/26 14:14:45 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1665, average loss: 2.3790
[09/26 14:14:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 50.00	
[09/26 14:14:45 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 14:14:52 visual_prompt]: Epoch 24 / 100: avg data time: 5.67e-02, avg batch time: 0.5000, average train loss: 2.3195
[09/26 14:14:54 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1666, average loss: 2.3891
[09/26 14:14:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.50	
[09/26 14:14:54 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 14:15:00 visual_prompt]: Epoch 25 / 100: avg data time: 4.98e-02, avg batch time: 0.4928, average train loss: 2.3325
[09/26 14:15:02 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1670, average loss: 2.3197
[09/26 14:15:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 49.50	
[09/26 14:15:02 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 14:15:09 visual_prompt]: Epoch 26 / 100: avg data time: 5.61e-02, avg batch time: 0.4998, average train loss: 2.2920
[09/26 14:15:10 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1670, average loss: 2.3215
[09/26 14:15:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 14:15:10 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 14:15:17 visual_prompt]: Epoch 27 / 100: avg data time: 5.08e-02, avg batch time: 0.4948, average train loss: 2.3012
[09/26 14:15:19 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1667, average loss: 2.3759
[09/26 14:15:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.00	
[09/26 14:15:19 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 14:15:25 visual_prompt]: Epoch 28 / 100: avg data time: 4.96e-02, avg batch time: 0.4939, average train loss: 2.3552
[09/26 14:15:27 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1666, average loss: 2.2780
[09/26 14:15:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 14:15:27 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 14:15:34 visual_prompt]: Epoch 29 / 100: avg data time: 5.05e-02, avg batch time: 0.4928, average train loss: 2.2834
[09/26 14:15:35 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1669, average loss: 2.3218
[09/26 14:15:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 14:15:35 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 14:15:42 visual_prompt]: Epoch 30 / 100: avg data time: 5.84e-02, avg batch time: 0.5012, average train loss: 2.2645
[09/26 14:15:44 visual_prompt]: Inference (val):avg data time: 4.55e-05, avg batch time: 0.1667, average loss: 2.2579
[09/26 14:15:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 14:15:44 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 14:15:51 visual_prompt]: Epoch 31 / 100: avg data time: 5.44e-02, avg batch time: 0.4982, average train loss: 2.3003
[09/26 14:15:52 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1668, average loss: 2.3318
[09/26 14:15:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.50	
[09/26 14:15:52 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 14:15:59 visual_prompt]: Epoch 32 / 100: avg data time: 4.59e-02, avg batch time: 0.4877, average train loss: 2.3432
[09/26 14:16:00 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1672, average loss: 2.4143
[09/26 14:16:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 14:16:00 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 14:16:07 visual_prompt]: Epoch 33 / 100: avg data time: 5.06e-02, avg batch time: 0.4951, average train loss: 2.3710
[09/26 14:16:09 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1670, average loss: 2.4437
[09/26 14:16:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 14:16:09 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 14:16:16 visual_prompt]: Epoch 34 / 100: avg data time: 5.96e-02, avg batch time: 0.5024, average train loss: 2.4120
[09/26 14:16:17 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1671, average loss: 2.4144
[09/26 14:16:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 14:16:17 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 14:16:24 visual_prompt]: Epoch 35 / 100: avg data time: 5.04e-02, avg batch time: 0.4944, average train loss: 2.2783
[09/26 14:16:26 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1669, average loss: 2.2739
[09/26 14:16:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 14:16:26 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 14:16:32 visual_prompt]: Epoch 36 / 100: avg data time: 5.81e-02, avg batch time: 0.5010, average train loss: 2.2879
[09/26 14:16:34 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1666, average loss: 2.3478
[09/26 14:16:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:16:34 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 14:16:41 visual_prompt]: Epoch 37 / 100: avg data time: 4.86e-02, avg batch time: 0.4918, average train loss: 2.4167
[09/26 14:16:42 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1667, average loss: 2.4749
[09/26 14:16:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 53.50	
[09/26 14:16:42 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 14:16:49 visual_prompt]: Epoch 38 / 100: avg data time: 4.72e-02, avg batch time: 0.4922, average train loss: 2.4195
[09/26 14:16:51 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1667, average loss: 2.5866
[09/26 14:16:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 14:16:51 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 14:16:57 visual_prompt]: Epoch 39 / 100: avg data time: 4.95e-02, avg batch time: 0.4929, average train loss: 2.6944
[09/26 14:16:59 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1669, average loss: 2.5123
[09/26 14:16:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.00	
[09/26 14:16:59 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 14:17:06 visual_prompt]: Epoch 40 / 100: avg data time: 4.89e-02, avg batch time: 0.4925, average train loss: 2.8401
[09/26 14:17:07 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1670, average loss: 2.8892
[09/26 14:17:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 14:17:07 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 14:17:14 visual_prompt]: Epoch 41 / 100: avg data time: 4.88e-02, avg batch time: 0.4932, average train loss: 3.3956
[09/26 14:17:16 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1665, average loss: 3.3073
[09/26 14:17:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 14:17:16 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 14:17:22 visual_prompt]: Epoch 42 / 100: avg data time: 5.85e-02, avg batch time: 0.5025, average train loss: 2.9093
[09/26 14:17:24 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1670, average loss: 2.7067
[09/26 14:17:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.00	
[09/26 14:17:24 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 14:17:31 visual_prompt]: Epoch 43 / 100: avg data time: 6.02e-02, avg batch time: 0.5038, average train loss: 2.5443
[09/26 14:17:33 visual_prompt]: Inference (val):avg data time: 1.85e-05, avg batch time: 0.1670, average loss: 2.6468
[09/26 14:17:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 14:17:33 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 14:17:39 visual_prompt]: Epoch 44 / 100: avg data time: 5.15e-02, avg batch time: 0.4947, average train loss: 2.5058
[09/26 14:17:41 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1669, average loss: 2.4573
[09/26 14:17:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 55.00	
[09/26 14:17:41 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 14:17:48 visual_prompt]: Epoch 45 / 100: avg data time: 4.87e-02, avg batch time: 0.4932, average train loss: 2.3385
[09/26 14:17:49 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1672, average loss: 2.3000
[09/26 14:17:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.50	
[09/26 14:17:49 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 14:17:56 visual_prompt]: Epoch 46 / 100: avg data time: 4.89e-02, avg batch time: 0.4917, average train loss: 2.3643
[09/26 14:17:57 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1665, average loss: 2.2913
[09/26 14:17:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 14:17:57 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 14:18:04 visual_prompt]: Epoch 47 / 100: avg data time: 6.18e-02, avg batch time: 0.5059, average train loss: 2.3278
[09/26 14:18:06 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1667, average loss: 2.4141
[09/26 14:18:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 14:18:06 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 14:18:13 visual_prompt]: Epoch 48 / 100: avg data time: 5.48e-02, avg batch time: 0.4981, average train loss: 2.3598
[09/26 14:18:14 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1667, average loss: 2.3853
[09/26 14:18:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/26 14:18:14 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 14:18:21 visual_prompt]: Epoch 49 / 100: avg data time: 5.74e-02, avg batch time: 0.5005, average train loss: 2.3114
[09/26 14:18:23 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1665, average loss: 2.3273
[09/26 14:18:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/26 14:18:23 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 14:18:29 visual_prompt]: Epoch 50 / 100: avg data time: 5.02e-02, avg batch time: 0.4951, average train loss: 2.3020
[09/26 14:18:31 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1669, average loss: 2.3807
[09/26 14:18:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 14:18:31 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 14:18:38 visual_prompt]: Epoch 51 / 100: avg data time: 5.70e-02, avg batch time: 0.4993, average train loss: 2.2787
[09/26 14:18:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1669, average loss: 2.2794
[09/26 14:18:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 14:18:39 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 14:18:46 visual_prompt]: Epoch 52 / 100: avg data time: 5.15e-02, avg batch time: 0.4949, average train loss: 2.2556
[09/26 14:18:48 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1666, average loss: 2.2451
[09/26 14:18:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 14:18:48 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 14:18:55 visual_prompt]: Epoch 53 / 100: avg data time: 4.74e-02, avg batch time: 0.4940, average train loss: 2.3132
[09/26 14:18:56 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1665, average loss: 2.2906
[09/26 14:18:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 14:18:56 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 14:19:03 visual_prompt]: Epoch 54 / 100: avg data time: 5.67e-02, avg batch time: 0.4987, average train loss: 2.2921
[09/26 14:19:05 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1663, average loss: 2.3231
[09/26 14:19:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 14:19:05 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 14:19:11 visual_prompt]: Epoch 55 / 100: avg data time: 5.74e-02, avg batch time: 0.5004, average train loss: 2.2654
[09/26 14:19:13 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1669, average loss: 2.2510
[09/26 14:19:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 14:19:13 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 14:19:20 visual_prompt]: Epoch 56 / 100: avg data time: 4.46e-02, avg batch time: 0.4892, average train loss: 2.2858
[09/26 14:19:21 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1669, average loss: 2.4256
[09/26 14:19:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 14:19:21 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 14:19:28 visual_prompt]: Epoch 57 / 100: avg data time: 4.35e-02, avg batch time: 0.4871, average train loss: 2.3102
[09/26 14:19:29 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1669, average loss: 2.2807
[09/26 14:19:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 14:19:29 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 14:19:36 visual_prompt]: Epoch 58 / 100: avg data time: 4.86e-02, avg batch time: 0.4924, average train loss: 2.2771
[09/26 14:19:38 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1668, average loss: 2.3673
[09/26 14:19:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 14:19:38 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 14:19:45 visual_prompt]: Epoch 59 / 100: avg data time: 5.12e-02, avg batch time: 0.4963, average train loss: 2.3473
[09/26 14:19:46 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1666, average loss: 2.2907
[09/26 14:19:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 14:19:46 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 14:19:53 visual_prompt]: Epoch 60 / 100: avg data time: 5.41e-02, avg batch time: 0.4970, average train loss: 2.2794
[09/26 14:19:55 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1667, average loss: 2.3443
[09/26 14:19:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 14:19:55 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 14:20:01 visual_prompt]: Epoch 61 / 100: avg data time: 4.95e-02, avg batch time: 0.4937, average train loss: 2.2878
[09/26 14:20:03 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1665, average loss: 2.2890
[09/26 14:20:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 14:20:03 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 14:20:10 visual_prompt]: Epoch 62 / 100: avg data time: 4.49e-02, avg batch time: 0.4892, average train loss: 2.2942
[09/26 14:20:11 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1665, average loss: 2.2588
[09/26 14:20:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 54.50	
[09/26 14:20:11 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 14:20:18 visual_prompt]: Epoch 63 / 100: avg data time: 6.36e-02, avg batch time: 0.5059, average train loss: 2.3173
[09/26 14:20:20 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1661, average loss: 2.3338
[09/26 14:20:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 14:20:20 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 14:20:27 visual_prompt]: Epoch 64 / 100: avg data time: 4.90e-02, avg batch time: 0.4923, average train loss: 2.2857
[09/26 14:20:29 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1664, average loss: 2.2211
[09/26 14:20:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.00	
[09/26 14:20:29 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 14:20:35 visual_prompt]: Epoch 65 / 100: avg data time: 5.48e-02, avg batch time: 0.4980, average train loss: 2.2362
[09/26 14:20:37 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1665, average loss: 2.2441
[09/26 14:20:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 14:20:37 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 14:20:44 visual_prompt]: Epoch 66 / 100: avg data time: 6.50e-02, avg batch time: 0.5084, average train loss: 2.2319
[09/26 14:20:46 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1665, average loss: 2.2105
[09/26 14:20:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 59.00	
[09/26 14:20:46 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 14:20:53 visual_prompt]: Epoch 67 / 100: avg data time: 6.15e-02, avg batch time: 0.5055, average train loss: 2.2426
[09/26 14:20:54 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1669, average loss: 2.2368
[09/26 14:20:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 51.50	
[09/26 14:20:54 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 14:21:01 visual_prompt]: Epoch 68 / 100: avg data time: 6.55e-02, avg batch time: 0.5070, average train loss: 2.2169
[09/26 14:21:03 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1666, average loss: 2.2508
[09/26 14:21:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 55.50	
[09/26 14:21:03 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 14:21:10 visual_prompt]: Epoch 69 / 100: avg data time: 6.04e-02, avg batch time: 0.5031, average train loss: 2.2065
[09/26 14:21:11 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1667, average loss: 2.1991
[09/26 14:21:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 56.50	
[09/26 14:21:11 visual_prompt]: Best epoch 69: best metric: 0.180
[09/26 14:21:11 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 14:21:18 visual_prompt]: Epoch 70 / 100: avg data time: 7.02e-02, avg batch time: 0.5130, average train loss: 2.1797
[09/26 14:21:20 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1663, average loss: 2.1697
[09/26 14:21:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 64.50	
[09/26 14:21:20 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 14:21:27 visual_prompt]: Epoch 71 / 100: avg data time: 6.97e-02, avg batch time: 0.5124, average train loss: 2.2314
[09/26 14:21:29 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1664, average loss: 2.1883
[09/26 14:21:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 59.50	
[09/26 14:21:29 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 14:21:36 visual_prompt]: Epoch 72 / 100: avg data time: 6.06e-02, avg batch time: 0.5036, average train loss: 2.1597
[09/26 14:21:37 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1666, average loss: 2.2099
[09/26 14:21:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 65.50	
[09/26 14:21:37 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 14:21:44 visual_prompt]: Epoch 73 / 100: avg data time: 5.90e-02, avg batch time: 0.5042, average train loss: 2.1267
[09/26 14:21:46 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1664, average loss: 2.2387
[09/26 14:21:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 65.00	
[09/26 14:21:46 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 14:21:53 visual_prompt]: Epoch 74 / 100: avg data time: 6.33e-02, avg batch time: 0.5064, average train loss: 2.2184
[09/26 14:21:54 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1662, average loss: 2.2174
[09/26 14:21:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 14:21:54 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 14:22:01 visual_prompt]: Epoch 75 / 100: avg data time: 4.76e-02, avg batch time: 0.4899, average train loss: 2.1583
[09/26 14:22:03 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1665, average loss: 2.1562
[09/26 14:22:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 69.00	
[09/26 14:22:03 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 14:22:10 visual_prompt]: Epoch 76 / 100: avg data time: 5.71e-02, avg batch time: 0.4992, average train loss: 2.0513
[09/26 14:22:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1660, average loss: 2.0534
[09/26 14:22:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 76.00	
[09/26 14:22:11 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 14:22:18 visual_prompt]: Epoch 77 / 100: avg data time: 5.49e-02, avg batch time: 0.4970, average train loss: 2.1919
[09/26 14:22:20 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1669, average loss: 2.1531
[09/26 14:22:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 68.50	
[09/26 14:22:20 visual_prompt]: Best epoch 77: best metric: 0.190
[09/26 14:22:20 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 14:22:27 visual_prompt]: Epoch 78 / 100: avg data time: 5.38e-02, avg batch time: 0.4957, average train loss: 2.0774
[09/26 14:22:28 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1669, average loss: 1.9708
[09/26 14:22:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 83.50	
[09/26 14:22:28 visual_prompt]: Best epoch 78: best metric: 0.225
[09/26 14:22:28 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 14:22:35 visual_prompt]: Epoch 79 / 100: avg data time: 5.41e-02, avg batch time: 0.4976, average train loss: 2.0003
[09/26 14:22:37 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1671, average loss: 2.0483
[09/26 14:22:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 76.00	
[09/26 14:22:37 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 14:22:44 visual_prompt]: Epoch 80 / 100: avg data time: 7.83e-02, avg batch time: 0.5203, average train loss: 1.9820
[09/26 14:22:46 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1668, average loss: 2.5001
[09/26 14:22:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 14:22:46 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 14:22:53 visual_prompt]: Epoch 81 / 100: avg data time: 5.66e-02, avg batch time: 0.4982, average train loss: 2.0531
[09/26 14:22:54 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1669, average loss: 1.9459
[09/26 14:22:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 80.50	
[09/26 14:22:54 visual_prompt]: Best epoch 81: best metric: 0.235
[09/26 14:22:54 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 14:23:01 visual_prompt]: Epoch 82 / 100: avg data time: 6.36e-02, avg batch time: 0.5052, average train loss: 2.0351
[09/26 14:23:03 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1667, average loss: 2.0366
[09/26 14:23:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 77.50	
[09/26 14:23:03 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 14:23:10 visual_prompt]: Epoch 83 / 100: avg data time: 5.36e-02, avg batch time: 0.4963, average train loss: 2.0092
[09/26 14:23:11 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1670, average loss: 1.9316
[09/26 14:23:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 83.50	
[09/26 14:23:11 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 14:23:18 visual_prompt]: Epoch 84 / 100: avg data time: 5.57e-02, avg batch time: 0.4988, average train loss: 1.8957
[09/26 14:23:20 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1671, average loss: 2.1197
[09/26 14:23:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 72.50	
[09/26 14:23:20 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 14:23:27 visual_prompt]: Epoch 85 / 100: avg data time: 5.94e-02, avg batch time: 0.5020, average train loss: 1.9266
[09/26 14:23:28 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1665, average loss: 1.9363
[09/26 14:23:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 80.50	
[09/26 14:23:28 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 14:23:35 visual_prompt]: Epoch 86 / 100: avg data time: 5.76e-02, avg batch time: 0.5005, average train loss: 1.8324
[09/26 14:23:37 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1672, average loss: 1.9036
[09/26 14:23:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 84.50	
[09/26 14:23:37 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 14:23:44 visual_prompt]: Epoch 87 / 100: avg data time: 4.59e-02, avg batch time: 0.4902, average train loss: 1.7933
[09/26 14:23:46 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1671, average loss: 1.9787
[09/26 14:23:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 83.00	
[09/26 14:23:46 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 14:23:52 visual_prompt]: Epoch 88 / 100: avg data time: 5.80e-02, avg batch time: 0.5016, average train loss: 1.7921
[09/26 14:23:54 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1673, average loss: 2.0222
[09/26 14:23:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 80.00	
[09/26 14:23:54 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 14:24:01 visual_prompt]: Epoch 89 / 100: avg data time: 6.85e-02, avg batch time: 0.5111, average train loss: 1.7979
[09/26 14:24:03 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1676, average loss: 1.8704
[09/26 14:24:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 85.50	
[09/26 14:24:03 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 14:24:10 visual_prompt]: Epoch 90 / 100: avg data time: 6.22e-02, avg batch time: 0.5053, average train loss: 1.7750
[09/26 14:24:11 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1672, average loss: 1.9189
[09/26 14:24:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 84.00	
[09/26 14:24:11 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 14:24:18 visual_prompt]: Epoch 91 / 100: avg data time: 5.63e-02, avg batch time: 0.4996, average train loss: 1.7480
[09/26 14:24:20 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1670, average loss: 1.9084
[09/26 14:24:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 85.50	
[09/26 14:24:20 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 14:24:27 visual_prompt]: Epoch 92 / 100: avg data time: 5.96e-02, avg batch time: 0.5022, average train loss: 1.6527
[09/26 14:24:29 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1666, average loss: 1.9153
[09/26 14:24:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 84.00	
[09/26 14:24:29 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 14:24:35 visual_prompt]: Epoch 93 / 100: avg data time: 5.50e-02, avg batch time: 0.4974, average train loss: 1.6280
[09/26 14:24:37 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1669, average loss: 1.9586
[09/26 14:24:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 83.50	
[09/26 14:24:37 visual_prompt]: Best epoch 93: best metric: 0.240
[09/26 14:24:37 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 14:24:44 visual_prompt]: Epoch 94 / 100: avg data time: 6.65e-02, avg batch time: 0.5089, average train loss: 1.5996
[09/26 14:24:46 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1674, average loss: 1.8666
[09/26 14:24:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 88.00	
[09/26 14:24:46 visual_prompt]: Best epoch 94: best metric: 0.270
[09/26 14:24:46 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 14:24:52 visual_prompt]: Epoch 95 / 100: avg data time: 6.03e-02, avg batch time: 0.5029, average train loss: 1.5450
[09/26 14:24:54 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1666, average loss: 1.9239
[09/26 14:24:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 85.50	
[09/26 14:24:54 visual_prompt]: Best epoch 95: best metric: 0.280
[09/26 14:24:54 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 14:25:01 visual_prompt]: Epoch 96 / 100: avg data time: 5.89e-02, avg batch time: 0.5011, average train loss: 1.5337
[09/26 14:25:03 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1671, average loss: 1.9740
[09/26 14:25:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 87.00	
[09/26 14:25:03 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 14:25:09 visual_prompt]: Epoch 97 / 100: avg data time: 5.04e-02, avg batch time: 0.4948, average train loss: 1.5025
[09/26 14:25:11 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1672, average loss: 1.9208
[09/26 14:25:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 84.00	
[09/26 14:25:11 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 14:25:18 visual_prompt]: Epoch 98 / 100: avg data time: 5.02e-02, avg batch time: 0.4950, average train loss: 1.4674
[09/26 14:25:20 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1672, average loss: 1.9513
[09/26 14:25:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 86.50	
[09/26 14:25:20 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 14:25:26 visual_prompt]: Epoch 99 / 100: avg data time: 6.13e-02, avg batch time: 0.5046, average train loss: 1.4575
[09/26 14:25:28 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1667, average loss: 1.9695
[09/26 14:25:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 87.00	
[09/26 14:25:28 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 14:25:35 visual_prompt]: Epoch 100 / 100: avg data time: 5.56e-02, avg batch time: 0.4981, average train loss: 1.4510
[09/26 14:25:37 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1670, average loss: 1.9568
[09/26 14:25:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 87.00	
[09/26 14:25:37 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:25:37 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:25:37 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:25:37 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:25:37 visual_prompt]: Training with config:
[09/26 14:25:37 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:25:37 visual_prompt]: Loading training data...
[09/26 14:25:37 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 14:25:38 visual_prompt]: Number of images: 800
[09/26 14:25:38 visual_prompt]: Number of classes: 9 / 9
[09/26 14:25:38 visual_prompt]: Loading validation data...
[09/26 14:25:38 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 14:25:38 visual_prompt]: Number of images: 200
[09/26 14:25:38 visual_prompt]: Number of classes: 9 / 9
[09/26 14:25:38 visual_prompt]: Constructing models...
[09/26 14:25:41 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 14:25:41 visual_prompt]: tuned percent:0.542
[09/26 14:25:41 visual_prompt]: Device used for model: 0
[09/26 14:25:41 visual_prompt]: Setting up Evaluator...
[09/26 14:25:41 visual_prompt]: Setting up Trainer...
[09/26 14:25:41 visual_prompt]: 	Setting up the optimizer...
[09/26 14:25:41 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:25:48 visual_prompt]: Epoch 1 / 100: avg data time: 6.75e-02, avg batch time: 0.5102, average train loss: 2.8715
[09/26 14:25:49 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1670, average loss: 2.9516
[09/26 14:25:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 14:25:49 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 14:25:49 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 14:25:56 visual_prompt]: Epoch 2 / 100: avg data time: 4.93e-02, avg batch time: 0.4924, average train loss: 3.6817
[09/26 14:25:58 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1673, average loss: 3.5684
[09/26 14:25:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 53.00	
[09/26 14:25:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 14:26:05 visual_prompt]: Epoch 3 / 100: avg data time: 6.36e-02, avg batch time: 0.5072, average train loss: 2.7070
[09/26 14:26:06 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1671, average loss: 2.3847
[09/26 14:26:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 14:26:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 14:26:13 visual_prompt]: Epoch 4 / 100: avg data time: 4.88e-02, avg batch time: 0.4913, average train loss: 2.3044
[09/26 14:26:14 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1667, average loss: 2.3672
[09/26 14:26:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 14:26:14 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 14:26:21 visual_prompt]: Epoch 5 / 100: avg data time: 4.93e-02, avg batch time: 0.4927, average train loss: 2.3166
[09/26 14:26:23 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1670, average loss: 2.2931
[09/26 14:26:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 14:26:23 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 14:26:29 visual_prompt]: Epoch 6 / 100: avg data time: 4.57e-02, avg batch time: 0.4895, average train loss: 2.3566
[09/26 14:26:31 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.1674, average loss: 2.4032
[09/26 14:26:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.00	
[09/26 14:26:31 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 14:26:38 visual_prompt]: Epoch 7 / 100: avg data time: 5.60e-02, avg batch time: 0.4995, average train loss: 2.4062
[09/26 14:26:39 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1673, average loss: 2.5847
[09/26 14:26:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 14:26:39 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 14:26:46 visual_prompt]: Epoch 8 / 100: avg data time: 5.20e-02, avg batch time: 0.4952, average train loss: 2.4457
[09/26 14:26:48 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1670, average loss: 2.3703
[09/26 14:26:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 59.50	
[09/26 14:26:48 visual_prompt]: Best epoch 8: best metric: 0.155
[09/26 14:26:48 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 14:26:55 visual_prompt]: Epoch 9 / 100: avg data time: 5.09e-02, avg batch time: 0.4962, average train loss: 2.3110
[09/26 14:26:56 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1670, average loss: 2.4874
[09/26 14:26:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 14:26:56 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 14:27:03 visual_prompt]: Epoch 10 / 100: avg data time: 4.68e-02, avg batch time: 0.4923, average train loss: 2.4155
[09/26 14:27:05 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1670, average loss: 2.6636
[09/26 14:27:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 14:27:05 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 14:27:11 visual_prompt]: Epoch 11 / 100: avg data time: 4.75e-02, avg batch time: 0.4922, average train loss: 2.7514
[09/26 14:27:13 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1671, average loss: 2.7870
[09/26 14:27:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 60.50	
[09/26 14:27:13 visual_prompt]: Best epoch 11: best metric: 0.175
[09/26 14:27:13 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 14:27:20 visual_prompt]: Epoch 12 / 100: avg data time: 5.70e-02, avg batch time: 0.5020, average train loss: 2.4921
[09/26 14:27:21 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1671, average loss: 3.0210
[09/26 14:27:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 64.50	
[09/26 14:27:21 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 14:27:28 visual_prompt]: Epoch 13 / 100: avg data time: 4.81e-02, avg batch time: 0.4924, average train loss: 2.6150
[09/26 14:27:30 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1672, average loss: 2.3397
[09/26 14:27:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 71.00	
[09/26 14:27:30 visual_prompt]: Best epoch 13: best metric: 0.180
[09/26 14:27:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 14:27:36 visual_prompt]: Epoch 14 / 100: avg data time: 4.79e-02, avg batch time: 0.4936, average train loss: 2.2347
[09/26 14:27:38 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1671, average loss: 2.1664
[09/26 14:27:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 75.00	
[09/26 14:27:38 visual_prompt]: Best epoch 14: best metric: 0.190
[09/26 14:27:38 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 14:27:45 visual_prompt]: Epoch 15 / 100: avg data time: 5.87e-02, avg batch time: 0.5022, average train loss: 2.2442
[09/26 14:27:46 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1675, average loss: 2.1935
[09/26 14:27:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 81.50	
[09/26 14:27:46 visual_prompt]: Best epoch 15: best metric: 0.205
[09/26 14:27:46 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 14:27:53 visual_prompt]: Epoch 16 / 100: avg data time: 4.84e-02, avg batch time: 0.4942, average train loss: 2.2181
[09/26 14:27:55 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1673, average loss: 2.4892
[09/26 14:27:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 75.50	
[09/26 14:27:55 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 14:28:01 visual_prompt]: Epoch 17 / 100: avg data time: 4.76e-02, avg batch time: 0.4925, average train loss: 2.0344
[09/26 14:28:03 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1672, average loss: 1.9454
[09/26 14:28:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 82.00	
[09/26 14:28:03 visual_prompt]: Best epoch 17: best metric: 0.210
[09/26 14:28:03 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 14:28:10 visual_prompt]: Epoch 18 / 100: avg data time: 5.39e-02, avg batch time: 0.4983, average train loss: 1.8965
[09/26 14:28:11 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1675, average loss: 2.2987
[09/26 14:28:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 80.50	
[09/26 14:28:11 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 14:28:18 visual_prompt]: Epoch 19 / 100: avg data time: 5.04e-02, avg batch time: 0.4967, average train loss: 1.9093
[09/26 14:28:20 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1677, average loss: 2.6765
[09/26 14:28:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 75.50	
[09/26 14:28:20 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 14:28:27 visual_prompt]: Epoch 20 / 100: avg data time: 5.41e-02, avg batch time: 0.4990, average train loss: 2.1459
[09/26 14:28:28 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1674, average loss: 2.3146
[09/26 14:28:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 82.50	
[09/26 14:28:28 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 14:28:35 visual_prompt]: Epoch 21 / 100: avg data time: 5.88e-02, avg batch time: 0.5037, average train loss: 1.9025
[09/26 14:28:37 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1678, average loss: 2.0849
[09/26 14:28:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 85.50	
[09/26 14:28:37 visual_prompt]: Best epoch 21: best metric: 0.225
[09/26 14:28:37 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 14:28:43 visual_prompt]: Epoch 22 / 100: avg data time: 4.69e-02, avg batch time: 0.4925, average train loss: 2.0558
[09/26 14:28:45 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1674, average loss: 2.2752
[09/26 14:28:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 78.00	
[09/26 14:28:45 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 14:28:52 visual_prompt]: Epoch 23 / 100: avg data time: 4.91e-02, avg batch time: 0.4937, average train loss: 2.0460
[09/26 14:28:53 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1673, average loss: 1.7660
[09/26 14:28:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 91.50	
[09/26 14:28:53 visual_prompt]: Best epoch 23: best metric: 0.265
[09/26 14:28:53 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 14:29:00 visual_prompt]: Epoch 24 / 100: avg data time: 5.09e-02, avg batch time: 0.4968, average train loss: 1.6456
[09/26 14:29:02 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1674, average loss: 2.1251
[09/26 14:29:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 88.00	
[09/26 14:29:02 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 14:29:09 visual_prompt]: Epoch 25 / 100: avg data time: 4.83e-02, avg batch time: 0.4938, average train loss: 1.6310
[09/26 14:29:10 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1673, average loss: 2.0396
[09/26 14:29:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 88.50	
[09/26 14:29:10 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 14:29:17 visual_prompt]: Epoch 26 / 100: avg data time: 5.16e-02, avg batch time: 0.4971, average train loss: 1.7778
[09/26 14:29:19 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1670, average loss: 1.9294
[09/26 14:29:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 93.00	
[09/26 14:29:19 visual_prompt]: Best epoch 26: best metric: 0.270
[09/26 14:29:19 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 14:29:25 visual_prompt]: Epoch 27 / 100: avg data time: 5.28e-02, avg batch time: 0.4965, average train loss: 1.5773
[09/26 14:29:27 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1673, average loss: 2.6245
[09/26 14:29:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 84.50	
[09/26 14:29:27 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 14:29:34 visual_prompt]: Epoch 28 / 100: avg data time: 4.92e-02, avg batch time: 0.4948, average train loss: 1.5386
[09/26 14:29:35 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1675, average loss: 1.5344
[09/26 14:29:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 14:29:35 visual_prompt]: Best epoch 28: best metric: 0.340
[09/26 14:29:35 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 14:29:42 visual_prompt]: Epoch 29 / 100: avg data time: 4.79e-02, avg batch time: 0.4959, average train loss: 1.4841
[09/26 14:29:44 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1673, average loss: 1.7491
[09/26 14:29:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 94.50	
[09/26 14:29:44 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 14:29:50 visual_prompt]: Epoch 30 / 100: avg data time: 4.76e-02, avg batch time: 0.4947, average train loss: 1.3782
[09/26 14:29:52 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1674, average loss: 2.1740
[09/26 14:29:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 88.50	
[09/26 14:29:52 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 14:29:59 visual_prompt]: Epoch 31 / 100: avg data time: 4.80e-02, avg batch time: 0.4941, average train loss: 1.7883
[09/26 14:30:00 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1672, average loss: 1.8302
[09/26 14:30:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 93.50	
[09/26 14:30:00 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 14:30:07 visual_prompt]: Epoch 32 / 100: avg data time: 4.85e-02, avg batch time: 0.4934, average train loss: 1.2847
[09/26 14:30:09 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1677, average loss: 1.9666
[09/26 14:30:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 97.00	
[09/26 14:30:09 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 14:30:15 visual_prompt]: Epoch 33 / 100: avg data time: 5.55e-02, avg batch time: 0.4998, average train loss: 1.3475
[09/26 14:30:17 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1675, average loss: 1.9396
[09/26 14:30:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 95.50	
[09/26 14:30:17 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 14:30:24 visual_prompt]: Epoch 34 / 100: avg data time: 5.00e-02, avg batch time: 0.4945, average train loss: 1.4580
[09/26 14:30:26 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1670, average loss: 2.1927
[09/26 14:30:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 93.00	
[09/26 14:30:26 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 14:30:32 visual_prompt]: Epoch 35 / 100: avg data time: 4.90e-02, avg batch time: 0.4931, average train loss: 1.5443
[09/26 14:30:34 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1673, average loss: 1.9708
[09/26 14:30:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 96.50	
[09/26 14:30:34 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 14:30:41 visual_prompt]: Epoch 36 / 100: avg data time: 5.52e-02, avg batch time: 0.5005, average train loss: 1.2479
[09/26 14:30:42 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1674, average loss: 2.0903
[09/26 14:30:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 96.00	
[09/26 14:30:42 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 14:30:49 visual_prompt]: Epoch 37 / 100: avg data time: 5.33e-02, avg batch time: 0.4978, average train loss: 1.4190
[09/26 14:30:51 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1675, average loss: 1.9215
[09/26 14:30:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 96.50	
[09/26 14:30:51 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 14:30:58 visual_prompt]: Epoch 38 / 100: avg data time: 4.87e-02, avg batch time: 0.4928, average train loss: 1.4056
[09/26 14:30:59 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1673, average loss: 2.1278
[09/26 14:30:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 92.00	
[09/26 14:30:59 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 14:31:06 visual_prompt]: Epoch 39 / 100: avg data time: 5.10e-02, avg batch time: 0.4975, average train loss: 1.2810
[09/26 14:31:08 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1674, average loss: 1.9044
[09/26 14:31:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.00	
[09/26 14:31:08 visual_prompt]: Best epoch 39: best metric: 0.355
[09/26 14:31:08 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 14:31:14 visual_prompt]: Epoch 40 / 100: avg data time: 5.10e-02, avg batch time: 0.4962, average train loss: 1.1086
[09/26 14:31:16 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1677, average loss: 3.0106
[09/26 14:31:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 96.00	
[09/26 14:31:16 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 14:31:23 visual_prompt]: Epoch 41 / 100: avg data time: 4.65e-02, avg batch time: 0.4914, average train loss: 1.2564
[09/26 14:31:24 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1671, average loss: 2.3162
[09/26 14:31:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 93.50	
[09/26 14:31:24 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 14:31:31 visual_prompt]: Epoch 42 / 100: avg data time: 5.83e-02, avg batch time: 0.5027, average train loss: 1.0757
[09/26 14:31:33 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1672, average loss: 1.9066
[09/26 14:31:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 97.50	
[09/26 14:31:33 visual_prompt]: Best epoch 42: best metric: 0.390
[09/26 14:31:33 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 14:31:40 visual_prompt]: Epoch 43 / 100: avg data time: 5.40e-02, avg batch time: 0.4980, average train loss: 1.0759
[09/26 14:31:41 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1673, average loss: 1.8373
[09/26 14:31:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 97.50	
[09/26 14:31:41 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 14:31:48 visual_prompt]: Epoch 44 / 100: avg data time: 4.98e-02, avg batch time: 0.4951, average train loss: 0.9662
[09/26 14:31:49 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1678, average loss: 2.2122
[09/26 14:31:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 97.50	
[09/26 14:31:49 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 14:31:56 visual_prompt]: Epoch 45 / 100: avg data time: 4.86e-02, avg batch time: 0.4952, average train loss: 0.9759
[09/26 14:31:58 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1673, average loss: 2.0178
[09/26 14:31:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 97.00	
[09/26 14:31:58 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 14:32:05 visual_prompt]: Epoch 46 / 100: avg data time: 5.31e-02, avg batch time: 0.4977, average train loss: 0.9819
[09/26 14:32:06 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1675, average loss: 2.4763
[09/26 14:32:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 92.50	
[09/26 14:32:06 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 14:32:13 visual_prompt]: Epoch 47 / 100: avg data time: 5.30e-02, avg batch time: 0.4969, average train loss: 0.9417
[09/26 14:32:15 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1672, average loss: 3.3652
[09/26 14:32:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 91.00	
[09/26 14:32:15 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 14:32:21 visual_prompt]: Epoch 48 / 100: avg data time: 5.65e-02, avg batch time: 0.5006, average train loss: 1.0141
[09/26 14:32:23 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1670, average loss: 2.7941
[09/26 14:32:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 85.00	
[09/26 14:32:23 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 14:32:30 visual_prompt]: Epoch 49 / 100: avg data time: 5.88e-02, avg batch time: 0.5037, average train loss: 1.2427
[09/26 14:32:31 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1672, average loss: 1.7069
[09/26 14:32:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 96.50	
[09/26 14:32:31 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 14:32:38 visual_prompt]: Epoch 50 / 100: avg data time: 5.78e-02, avg batch time: 0.5018, average train loss: 0.8583
[09/26 14:32:40 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1675, average loss: 2.7285
[09/26 14:32:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.00	
[09/26 14:32:40 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 14:32:47 visual_prompt]: Epoch 51 / 100: avg data time: 4.96e-02, avg batch time: 0.4939, average train loss: 1.1090
[09/26 14:32:48 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1678, average loss: 2.3550
[09/26 14:32:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 87.00	
[09/26 14:32:48 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 14:32:55 visual_prompt]: Epoch 52 / 100: avg data time: 4.63e-02, avg batch time: 0.4911, average train loss: 0.8339
[09/26 14:32:57 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1676, average loss: 2.0764
[09/26 14:32:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.50	
[09/26 14:32:57 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 14:33:03 visual_prompt]: Epoch 53 / 100: avg data time: 5.90e-02, avg batch time: 0.5030, average train loss: 0.7181
[09/26 14:33:05 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1670, average loss: 2.2134
[09/26 14:33:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 97.50	
[09/26 14:33:05 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 14:33:12 visual_prompt]: Epoch 54 / 100: avg data time: 4.82e-02, avg batch time: 0.4928, average train loss: 1.0894
[09/26 14:33:14 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1675, average loss: 2.3801
[09/26 14:33:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 93.50	
[09/26 14:33:14 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 14:33:20 visual_prompt]: Epoch 55 / 100: avg data time: 4.99e-02, avg batch time: 0.4946, average train loss: 0.7256
[09/26 14:33:22 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1673, average loss: 2.5561
[09/26 14:33:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 95.50	
[09/26 14:33:22 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 14:33:29 visual_prompt]: Epoch 56 / 100: avg data time: 5.85e-02, avg batch time: 0.5020, average train loss: 0.5807
[09/26 14:33:30 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1674, average loss: 2.4699
[09/26 14:33:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 96.50	
[09/26 14:33:30 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 14:33:37 visual_prompt]: Epoch 57 / 100: avg data time: 5.77e-02, avg batch time: 0.5029, average train loss: 0.5982
[09/26 14:33:39 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1673, average loss: 3.3201
[09/26 14:33:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 94.00	
[09/26 14:33:39 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 14:33:46 visual_prompt]: Epoch 58 / 100: avg data time: 5.71e-02, avg batch time: 0.5013, average train loss: 0.6583
[09/26 14:33:47 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1677, average loss: 2.5419
[09/26 14:33:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 96.50	
[09/26 14:33:47 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 14:33:54 visual_prompt]: Epoch 59 / 100: avg data time: 5.90e-02, avg batch time: 0.5029, average train loss: 0.5630
[09/26 14:33:56 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1671, average loss: 2.5471
[09/26 14:33:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 97.50	
[09/26 14:33:56 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 14:34:02 visual_prompt]: Epoch 60 / 100: avg data time: 5.13e-02, avg batch time: 0.4969, average train loss: 0.5697
[09/26 14:34:04 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1670, average loss: 3.5103
[09/26 14:34:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 93.00	
[09/26 14:34:04 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 14:34:11 visual_prompt]: Epoch 61 / 100: avg data time: 5.11e-02, avg batch time: 0.4948, average train loss: 0.4986
[09/26 14:34:13 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1673, average loss: 2.8137
[09/26 14:34:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 98.50	
[09/26 14:34:13 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 14:34:19 visual_prompt]: Epoch 62 / 100: avg data time: 6.43e-02, avg batch time: 0.5075, average train loss: 0.4534
[09/26 14:34:21 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1670, average loss: 3.1163
[09/26 14:34:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 96.50	
[09/26 14:34:21 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 14:34:28 visual_prompt]: Epoch 63 / 100: avg data time: 5.62e-02, avg batch time: 0.4997, average train loss: 0.3303
[09/26 14:34:29 visual_prompt]: Inference (val):avg data time: 2.01e-05, avg batch time: 0.1673, average loss: 4.1393
[09/26 14:34:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 96.50	
[09/26 14:34:29 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 14:34:36 visual_prompt]: Epoch 64 / 100: avg data time: 5.60e-02, avg batch time: 0.4996, average train loss: 0.4561
[09/26 14:34:38 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1672, average loss: 3.3350
[09/26 14:34:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 97.00	
[09/26 14:34:38 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 14:34:45 visual_prompt]: Epoch 65 / 100: avg data time: 6.11e-02, avg batch time: 0.5052, average train loss: 0.4258
[09/26 14:34:46 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1672, average loss: 2.8782
[09/26 14:34:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 97.50	
[09/26 14:34:46 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 14:34:53 visual_prompt]: Epoch 66 / 100: avg data time: 5.27e-02, avg batch time: 0.4969, average train loss: 0.3085
[09/26 14:34:55 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1669, average loss: 4.2391
[09/26 14:34:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 98.00	
[09/26 14:34:55 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 14:35:02 visual_prompt]: Epoch 67 / 100: avg data time: 5.90e-02, avg batch time: 0.5018, average train loss: 0.2943
[09/26 14:35:03 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1670, average loss: 4.0948
[09/26 14:35:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 96.50	
[09/26 14:35:03 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 14:35:10 visual_prompt]: Epoch 68 / 100: avg data time: 4.74e-02, avg batch time: 0.4922, average train loss: 0.5113
[09/26 14:35:11 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1669, average loss: 3.4977
[09/26 14:35:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 96.50	
[09/26 14:35:11 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 14:35:18 visual_prompt]: Epoch 69 / 100: avg data time: 5.14e-02, avg batch time: 0.4951, average train loss: 0.3925
[09/26 14:35:20 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1673, average loss: 3.4188
[09/26 14:35:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 95.00	
[09/26 14:35:20 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 14:35:26 visual_prompt]: Epoch 70 / 100: avg data time: 4.68e-02, avg batch time: 0.4913, average train loss: 0.3579
[09/26 14:35:28 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1672, average loss: 2.7928
[09/26 14:35:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 99.00	
[09/26 14:35:28 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 14:35:35 visual_prompt]: Epoch 71 / 100: avg data time: 5.02e-02, avg batch time: 0.4946, average train loss: 0.3046
[09/26 14:35:36 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1670, average loss: 3.0546
[09/26 14:35:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 97.00	
[09/26 14:35:36 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 14:35:43 visual_prompt]: Epoch 72 / 100: avg data time: 6.08e-02, avg batch time: 0.5036, average train loss: 0.2047
[09/26 14:35:45 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1671, average loss: 4.3368
[09/26 14:35:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 94.00	
[09/26 14:35:45 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 14:35:52 visual_prompt]: Epoch 73 / 100: avg data time: 5.75e-02, avg batch time: 0.5025, average train loss: 0.1911
[09/26 14:35:53 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1669, average loss: 4.2017
[09/26 14:35:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 96.50	
[09/26 14:35:53 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 14:36:00 visual_prompt]: Epoch 74 / 100: avg data time: 5.11e-02, avg batch time: 0.4957, average train loss: 0.1950
[09/26 14:36:02 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1671, average loss: 4.1636
[09/26 14:36:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 96.50	
[09/26 14:36:02 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 14:36:08 visual_prompt]: Epoch 75 / 100: avg data time: 5.04e-02, avg batch time: 0.4959, average train loss: 0.1884
[09/26 14:36:10 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1669, average loss: 3.6053
[09/26 14:36:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.50	top5: 98.50	
[09/26 14:36:10 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 14:36:17 visual_prompt]: Epoch 76 / 100: avg data time: 5.46e-02, avg batch time: 0.4971, average train loss: 0.1786
[09/26 14:36:18 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1670, average loss: 4.5519
[09/26 14:36:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 94.00	
[09/26 14:36:18 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 14:36:25 visual_prompt]: Epoch 77 / 100: avg data time: 6.36e-02, avg batch time: 0.5074, average train loss: 0.1317
[09/26 14:36:27 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1667, average loss: 4.9640
[09/26 14:36:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 96.50	
[09/26 14:36:27 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 14:36:34 visual_prompt]: Epoch 78 / 100: avg data time: 6.12e-02, avg batch time: 0.5032, average train loss: 0.0803
[09/26 14:36:35 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1672, average loss: 4.7067
[09/26 14:36:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 97.00	
[09/26 14:36:35 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 14:36:42 visual_prompt]: Epoch 79 / 100: avg data time: 4.61e-02, avg batch time: 0.4910, average train loss: 0.0636
[09/26 14:36:44 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1669, average loss: 5.0437
[09/26 14:36:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.50	top5: 97.00	
[09/26 14:36:44 visual_prompt]: Best epoch 79: best metric: 0.395
[09/26 14:36:44 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 14:36:51 visual_prompt]: Epoch 80 / 100: avg data time: 5.62e-02, avg batch time: 0.5014, average train loss: 0.0783
[09/26 14:36:52 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1669, average loss: 5.0532
[09/26 14:36:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 96.50	
[09/26 14:36:52 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 14:36:59 visual_prompt]: Epoch 81 / 100: avg data time: 6.10e-02, avg batch time: 0.5043, average train loss: 0.0490
[09/26 14:37:01 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1670, average loss: 5.1792
[09/26 14:37:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 96.50	
[09/26 14:37:01 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 14:37:07 visual_prompt]: Epoch 82 / 100: avg data time: 5.46e-02, avg batch time: 0.4967, average train loss: 0.0601
[09/26 14:37:09 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1668, average loss: 5.3304
[09/26 14:37:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 96.50	
[09/26 14:37:09 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 14:37:16 visual_prompt]: Epoch 83 / 100: avg data time: 5.71e-02, avg batch time: 0.5002, average train loss: 0.0569
[09/26 14:37:17 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1671, average loss: 5.6194
[09/26 14:37:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 97.00	
[09/26 14:37:17 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 14:37:24 visual_prompt]: Epoch 84 / 100: avg data time: 5.08e-02, avg batch time: 0.4952, average train loss: 0.0344
[09/26 14:37:26 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1672, average loss: 5.5926
[09/26 14:37:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 97.50	
[09/26 14:37:26 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 14:37:32 visual_prompt]: Epoch 85 / 100: avg data time: 4.77e-02, avg batch time: 0.4911, average train loss: 0.0230
[09/26 14:37:34 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1668, average loss: 5.8625
[09/26 14:37:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 97.00	
[09/26 14:37:34 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 14:37:41 visual_prompt]: Epoch 86 / 100: avg data time: 5.31e-02, avg batch time: 0.4964, average train loss: 0.0193
[09/26 14:37:42 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1673, average loss: 6.0196
[09/26 14:37:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 97.50	
[09/26 14:37:42 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 14:37:49 visual_prompt]: Epoch 87 / 100: avg data time: 5.12e-02, avg batch time: 0.4945, average train loss: 0.0184
[09/26 14:37:51 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1668, average loss: 6.5204
[09/26 14:37:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 96.00	
[09/26 14:37:51 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 14:37:57 visual_prompt]: Epoch 88 / 100: avg data time: 5.05e-02, avg batch time: 0.4934, average train loss: 0.0130
[09/26 14:37:59 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1673, average loss: 5.9061
[09/26 14:37:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 97.50	
[09/26 14:37:59 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 14:38:06 visual_prompt]: Epoch 89 / 100: avg data time: 4.57e-02, avg batch time: 0.4902, average train loss: 0.0149
[09/26 14:38:07 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1671, average loss: 6.1319
[09/26 14:38:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 97.50	
[09/26 14:38:07 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 14:38:14 visual_prompt]: Epoch 90 / 100: avg data time: 5.79e-02, avg batch time: 0.5016, average train loss: 0.0124
[09/26 14:38:16 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1673, average loss: 6.2161
[09/26 14:38:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 96.50	
[09/26 14:38:16 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 14:38:23 visual_prompt]: Epoch 91 / 100: avg data time: 7.02e-02, avg batch time: 0.5136, average train loss: 0.0051
[09/26 14:38:24 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1673, average loss: 6.2449
[09/26 14:38:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 96.50	
[09/26 14:38:24 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 14:38:31 visual_prompt]: Epoch 92 / 100: avg data time: 4.97e-02, avg batch time: 0.4930, average train loss: 0.0047
[09/26 14:38:33 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1670, average loss: 6.1572
[09/26 14:38:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 97.50	
[09/26 14:38:33 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 14:38:39 visual_prompt]: Epoch 93 / 100: avg data time: 4.68e-02, avg batch time: 0.4930, average train loss: 0.0086
[09/26 14:38:41 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1672, average loss: 6.1058
[09/26 14:38:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 97.50	
[09/26 14:38:41 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 14:38:48 visual_prompt]: Epoch 94 / 100: avg data time: 6.72e-02, avg batch time: 0.5102, average train loss: 0.0074
[09/26 14:38:50 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1672, average loss: 5.9864
[09/26 14:38:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 97.50	
[09/26 14:38:50 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 14:38:56 visual_prompt]: Epoch 95 / 100: avg data time: 4.86e-02, avg batch time: 0.4931, average train loss: 0.0058
[09/26 14:38:58 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1672, average loss: 6.0327
[09/26 14:38:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 97.50	
[09/26 14:38:58 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 14:39:05 visual_prompt]: Epoch 96 / 100: avg data time: 5.19e-02, avg batch time: 0.4956, average train loss: 0.0068
[09/26 14:39:06 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1670, average loss: 6.1175
[09/26 14:39:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 97.00	
[09/26 14:39:06 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 14:39:13 visual_prompt]: Epoch 97 / 100: avg data time: 5.37e-02, avg batch time: 0.4986, average train loss: 0.0092
[09/26 14:39:15 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1672, average loss: 6.1527
[09/26 14:39:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 97.00	
[09/26 14:39:15 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 14:39:22 visual_prompt]: Epoch 98 / 100: avg data time: 5.61e-02, avg batch time: 0.5023, average train loss: 0.0033
[09/26 14:39:23 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1670, average loss: 6.1563
[09/26 14:39:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 97.00	
[09/26 14:39:23 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 14:39:30 visual_prompt]: Epoch 99 / 100: avg data time: 4.97e-02, avg batch time: 0.4935, average train loss: 0.0078
[09/26 14:39:32 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1672, average loss: 6.1616
[09/26 14:39:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 97.00	
[09/26 14:39:32 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 14:39:38 visual_prompt]: Epoch 100 / 100: avg data time: 5.85e-02, avg batch time: 0.5016, average train loss: 0.0079
[09/26 14:39:40 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1674, average loss: 6.1648
[09/26 14:39:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 97.00	
[09/26 14:39:40 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:39:40 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:39:40 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:39:40 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:39:40 visual_prompt]: Training with config:
[09/26 14:39:40 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr2.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:39:40 visual_prompt]: Loading training data...
[09/26 14:39:40 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 14:39:41 visual_prompt]: Number of images: 800
[09/26 14:39:41 visual_prompt]: Number of classes: 9 / 9
[09/26 14:39:41 visual_prompt]: Loading validation data...
[09/26 14:39:41 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 14:39:41 visual_prompt]: Number of images: 200
[09/26 14:39:41 visual_prompt]: Number of classes: 9 / 9
[09/26 14:39:41 visual_prompt]: Constructing models...
[09/26 14:39:44 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 14:39:44 visual_prompt]: tuned percent:0.542
[09/26 14:39:44 visual_prompt]: Device used for model: 0
[09/26 14:39:44 visual_prompt]: Setting up Evaluator...
[09/26 14:39:44 visual_prompt]: Setting up Trainer...
[09/26 14:39:44 visual_prompt]: 	Setting up the optimizer...
[09/26 14:39:44 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:39:51 visual_prompt]: Epoch 1 / 100: avg data time: 5.58e-02, avg batch time: 0.5014, average train loss: 2.8545
[09/26 14:39:52 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1668, average loss: 2.9516
[09/26 14:39:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 14:39:52 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 14:39:52 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.25
[09/26 14:39:59 visual_prompt]: Epoch 2 / 100: avg data time: 4.64e-02, avg batch time: 0.4895, average train loss: 3.7322
[09/26 14:40:01 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1666, average loss: 3.7102
[09/26 14:40:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 52.00	
[09/26 14:40:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.5
[09/26 14:40:08 visual_prompt]: Epoch 3 / 100: avg data time: 5.68e-02, avg batch time: 0.5002, average train loss: 2.6866
[09/26 14:40:09 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1668, average loss: 2.2551
[09/26 14:40:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 51.00	
[09/26 14:40:09 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.75
[09/26 14:40:16 visual_prompt]: Epoch 4 / 100: avg data time: 6.36e-02, avg batch time: 0.5052, average train loss: 2.2886
[09/26 14:40:18 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1668, average loss: 2.3603
[09/26 14:40:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 14:40:18 visual_prompt]: Training 5 / 100 epoch, with learning rate 1.0
[09/26 14:40:25 visual_prompt]: Epoch 5 / 100: avg data time: 5.57e-02, avg batch time: 0.5002, average train loss: 2.3026
[09/26 14:40:26 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1671, average loss: 2.2654
[09/26 14:40:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.50	
[09/26 14:40:26 visual_prompt]: Best epoch 5: best metric: 0.145
[09/26 14:40:26 visual_prompt]: Training 6 / 100 epoch, with learning rate 1.25
[09/26 14:40:33 visual_prompt]: Epoch 6 / 100: avg data time: 4.75e-02, avg batch time: 0.4903, average train loss: 2.3231
[09/26 14:40:34 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1672, average loss: 2.2297
[09/26 14:40:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 14:40:34 visual_prompt]: Training 7 / 100 epoch, with learning rate 1.5
[09/26 14:40:41 visual_prompt]: Epoch 7 / 100: avg data time: 5.13e-02, avg batch time: 0.4936, average train loss: 2.2393
[09/26 14:40:43 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1671, average loss: 2.4796
[09/26 14:40:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 14:40:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 1.75
[09/26 14:40:49 visual_prompt]: Epoch 8 / 100: avg data time: 6.23e-02, avg batch time: 0.5048, average train loss: 2.3839
[09/26 14:40:51 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1677, average loss: 2.7487
[09/26 14:40:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 52.50	
[09/26 14:40:51 visual_prompt]: Training 9 / 100 epoch, with learning rate 2.0
[09/26 14:40:58 visual_prompt]: Epoch 9 / 100: avg data time: 4.68e-02, avg batch time: 0.4890, average train loss: 2.4910
[09/26 14:40:59 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1669, average loss: 2.6563
[09/26 14:40:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 55.00	
[09/26 14:40:59 visual_prompt]: Training 10 / 100 epoch, with learning rate 2.25
[09/26 14:41:06 visual_prompt]: Epoch 10 / 100: avg data time: 5.43e-02, avg batch time: 0.4972, average train loss: 2.5226
[09/26 14:41:08 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1673, average loss: 2.4210
[09/26 14:41:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 62.00	
[09/26 14:41:08 visual_prompt]: Training 11 / 100 epoch, with learning rate 2.5
[09/26 14:41:14 visual_prompt]: Epoch 11 / 100: avg data time: 4.51e-02, avg batch time: 0.4899, average train loss: 2.3394
[09/26 14:41:16 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1670, average loss: 2.5748
[09/26 14:41:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 58.50	
[09/26 14:41:16 visual_prompt]: Best epoch 11: best metric: 0.155
[09/26 14:41:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[09/26 14:41:23 visual_prompt]: Epoch 12 / 100: avg data time: 5.67e-02, avg batch time: 0.5000, average train loss: 2.5951
[09/26 14:41:24 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1667, average loss: 2.5441
[09/26 14:41:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 66.50	
[09/26 14:41:24 visual_prompt]: Best epoch 12: best metric: 0.195
[09/26 14:41:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[09/26 14:41:31 visual_prompt]: Epoch 13 / 100: avg data time: 6.02e-02, avg batch time: 0.5027, average train loss: 2.6499
[09/26 14:41:33 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1670, average loss: 3.7260
[09/26 14:41:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 55.00	
[09/26 14:41:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[09/26 14:41:40 visual_prompt]: Epoch 14 / 100: avg data time: 5.18e-02, avg batch time: 0.4979, average train loss: 2.8991
[09/26 14:41:41 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1669, average loss: 2.9516
[09/26 14:41:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 60.50	
[09/26 14:41:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 2.487835085926963
[09/26 14:41:48 visual_prompt]: Epoch 15 / 100: avg data time: 6.13e-02, avg batch time: 0.5053, average train loss: 2.7774
[09/26 14:41:50 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1672, average loss: 3.0717
[09/26 14:41:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 58.00	
[09/26 14:41:50 visual_prompt]: Best epoch 15: best metric: 0.205
[09/26 14:41:50 visual_prompt]: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[09/26 14:41:56 visual_prompt]: Epoch 16 / 100: avg data time: 6.21e-02, avg batch time: 0.5053, average train loss: 2.5807
[09/26 14:41:58 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1671, average loss: 2.0801
[09/26 14:41:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 81.00	
[09/26 14:41:58 visual_prompt]: Training 17 / 100 epoch, with learning rate 2.472684500917257
[09/26 14:42:05 visual_prompt]: Epoch 17 / 100: avg data time: 5.90e-02, avg batch time: 0.5020, average train loss: 2.2778
[09/26 14:42:06 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1674, average loss: 2.2181
[09/26 14:42:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 80.50	
[09/26 14:42:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[09/26 14:42:13 visual_prompt]: Epoch 18 / 100: avg data time: 6.48e-02, avg batch time: 0.5067, average train loss: 3.5016
[09/26 14:42:15 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1667, average loss: 4.7273
[09/26 14:42:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 14:42:15 visual_prompt]: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[09/26 14:42:22 visual_prompt]: Epoch 19 / 100: avg data time: 6.02e-02, avg batch time: 0.5025, average train loss: 3.9532
[09/26 14:42:23 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1669, average loss: 3.1033
[09/26 14:42:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 14:42:23 visual_prompt]: Training 20 / 100 epoch, with learning rate 2.438820645368942
[09/26 14:42:30 visual_prompt]: Epoch 20 / 100: avg data time: 6.01e-02, avg batch time: 0.5028, average train loss: 2.6930
[09/26 14:42:32 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1668, average loss: 2.7319
[09/26 14:42:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 48.50	
[09/26 14:42:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[09/26 14:42:39 visual_prompt]: Epoch 21 / 100: avg data time: 6.49e-02, avg batch time: 0.5071, average train loss: 2.4649
[09/26 14:42:40 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1669, average loss: 2.5429
[09/26 14:42:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/26 14:42:40 visual_prompt]: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[09/26 14:42:47 visual_prompt]: Epoch 22 / 100: avg data time: 4.80e-02, avg batch time: 0.4908, average train loss: 2.4167
[09/26 14:42:48 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1670, average loss: 2.4392
[09/26 14:42:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 55.50	
[09/26 14:42:48 visual_prompt]: Training 23 / 100 epoch, with learning rate 2.391931822053251
[09/26 14:42:55 visual_prompt]: Epoch 23 / 100: avg data time: 5.23e-02, avg batch time: 0.4962, average train loss: 2.3879
[09/26 14:42:57 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1669, average loss: 2.6973
[09/26 14:42:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.50	
[09/26 14:42:57 visual_prompt]: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[09/26 14:43:04 visual_prompt]: Epoch 24 / 100: avg data time: 4.67e-02, avg batch time: 0.4913, average train loss: 2.3987
[09/26 14:43:05 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1669, average loss: 2.2774
[09/26 14:43:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 64.50	
[09/26 14:43:05 visual_prompt]: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[09/26 14:43:12 visual_prompt]: Epoch 25 / 100: avg data time: 6.31e-02, avg batch time: 0.5059, average train loss: 2.2627
[09/26 14:43:14 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1673, average loss: 2.1384
[09/26 14:43:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 70.00	
[09/26 14:43:14 visual_prompt]: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[09/26 14:43:20 visual_prompt]: Epoch 26 / 100: avg data time: 4.62e-02, avg batch time: 0.4910, average train loss: 2.2074
[09/26 14:43:22 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1667, average loss: 2.3423
[09/26 14:43:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 68.00	
[09/26 14:43:22 visual_prompt]: Training 27 / 100 epoch, with learning rate 2.310060120195532
[09/26 14:43:29 visual_prompt]: Epoch 27 / 100: avg data time: 5.43e-02, avg batch time: 0.4992, average train loss: 2.1696
[09/26 14:43:30 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1670, average loss: 2.2593
[09/26 14:43:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 76.00	
[09/26 14:43:30 visual_prompt]: Best epoch 27: best metric: 0.215
[09/26 14:43:30 visual_prompt]: Training 28 / 100 epoch, with learning rate 2.286296965693802
[09/26 14:43:37 visual_prompt]: Epoch 28 / 100: avg data time: 5.54e-02, avg batch time: 0.4981, average train loss: 2.1132
[09/26 14:43:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1671, average loss: 1.9617
[09/26 14:43:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 87.50	
[09/26 14:43:39 visual_prompt]: Training 29 / 100 epoch, with learning rate 2.261271242968684
[09/26 14:43:46 visual_prompt]: Epoch 29 / 100: avg data time: 5.75e-02, avg batch time: 0.5024, average train loss: 1.9614
[09/26 14:43:47 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1674, average loss: 2.8343
[09/26 14:43:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 14:43:47 visual_prompt]: Training 30 / 100 epoch, with learning rate 2.2350134420084022
[09/26 14:43:54 visual_prompt]: Epoch 30 / 100: avg data time: 6.15e-02, avg batch time: 0.5040, average train loss: 2.0809
[09/26 14:43:56 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1668, average loss: 1.8825
[09/26 14:43:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 86.50	
[09/26 14:43:56 visual_prompt]: Best epoch 30: best metric: 0.250
[09/26 14:43:56 visual_prompt]: Training 31 / 100 epoch, with learning rate 2.2075555538987226
[09/26 14:44:02 visual_prompt]: Epoch 31 / 100: avg data time: 6.14e-02, avg batch time: 0.5048, average train loss: 1.9695
[09/26 14:44:04 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1671, average loss: 2.6665
[09/26 14:44:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 71.50	
[09/26 14:44:04 visual_prompt]: Training 32 / 100 epoch, with learning rate 2.178931031846743
[09/26 14:44:11 visual_prompt]: Epoch 32 / 100: avg data time: 6.55e-02, avg batch time: 0.5083, average train loss: 2.2004
[09/26 14:44:13 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1673, average loss: 2.3270
[09/26 14:44:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 86.00	
[09/26 14:44:13 visual_prompt]: Training 33 / 100 epoch, with learning rate 2.149174750423314
[09/26 14:44:19 visual_prompt]: Epoch 33 / 100: avg data time: 5.05e-02, avg batch time: 0.4948, average train loss: 2.0655
[09/26 14:44:21 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1669, average loss: 2.1934
[09/26 14:44:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 82.00	
[09/26 14:44:21 visual_prompt]: Training 34 / 100 epoch, with learning rate 2.1183229630737466
[09/26 14:44:28 visual_prompt]: Epoch 34 / 100: avg data time: 4.93e-02, avg batch time: 0.4928, average train loss: 1.9477
[09/26 14:44:29 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1671, average loss: 1.9928
[09/26 14:44:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 85.50	
[09/26 14:44:29 visual_prompt]: Training 35 / 100 epoch, with learning rate 2.086413257948573
[09/26 14:44:36 visual_prompt]: Epoch 35 / 100: avg data time: 4.88e-02, avg batch time: 0.4924, average train loss: 1.9493
[09/26 14:44:38 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1671, average loss: 2.0487
[09/26 14:44:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 90.00	
[09/26 14:44:38 visual_prompt]: Training 36 / 100 epoch, with learning rate 2.053484512108174
[09/26 14:44:44 visual_prompt]: Epoch 36 / 100: avg data time: 6.08e-02, avg batch time: 0.5058, average train loss: 1.7807
[09/26 14:44:46 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1671, average loss: 1.8067
[09/26 14:44:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.50	
[09/26 14:44:46 visual_prompt]: Best epoch 36: best metric: 0.275
[09/26 14:44:46 visual_prompt]: Training 37 / 100 epoch, with learning rate 2.019576844157073
[09/26 14:44:53 visual_prompt]: Epoch 37 / 100: avg data time: 6.81e-02, avg batch time: 0.5111, average train loss: 1.6681
[09/26 14:44:55 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1673, average loss: 1.9104
[09/26 14:44:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 93.50	
[09/26 14:44:55 visual_prompt]: Training 38 / 100 epoch, with learning rate 1.9847315653655915
[09/26 14:45:01 visual_prompt]: Epoch 38 / 100: avg data time: 4.88e-02, avg batch time: 0.4929, average train loss: 1.8152
[09/26 14:45:03 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1672, average loss: 1.9302
[09/26 14:45:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 89.50	
[09/26 14:45:03 visual_prompt]: Training 39 / 100 epoch, with learning rate 1.9489911293384334
[09/26 14:45:10 visual_prompt]: Epoch 39 / 100: avg data time: 5.78e-02, avg batch time: 0.5025, average train loss: 1.7771
[09/26 14:45:11 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1673, average loss: 2.0327
[09/26 14:45:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 91.00	
[09/26 14:45:11 visual_prompt]: Training 40 / 100 epoch, with learning rate 1.912399080291506
[09/26 14:45:18 visual_prompt]: Epoch 40 / 100: avg data time: 4.70e-02, avg batch time: 0.4905, average train loss: 1.7033
[09/26 14:45:20 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1669, average loss: 1.8389
[09/26 14:45:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 94.50	
[09/26 14:45:20 visual_prompt]: Training 41 / 100 epoch, with learning rate 1.875
[09/26 14:45:26 visual_prompt]: Epoch 41 / 100: avg data time: 4.91e-02, avg batch time: 0.4962, average train loss: 1.5635
[09/26 14:45:28 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1672, average loss: 1.8660
[09/26 14:45:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 92.00	
[09/26 14:45:28 visual_prompt]: Best epoch 41: best metric: 0.290
[09/26 14:45:28 visual_prompt]: Training 42 / 100 epoch, with learning rate 1.8368394534823635
[09/26 14:45:35 visual_prompt]: Epoch 42 / 100: avg data time: 5.92e-02, avg batch time: 0.5016, average train loss: 1.4873
[09/26 14:45:36 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1674, average loss: 2.2787
[09/26 14:45:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 94.50	
[09/26 14:45:36 visual_prompt]: Best epoch 42: best metric: 0.305
[09/26 14:45:36 visual_prompt]: Training 43 / 100 epoch, with learning rate 1.7979639334863466
[09/26 14:45:43 visual_prompt]: Epoch 43 / 100: avg data time: 4.52e-02, avg batch time: 0.4886, average train loss: 1.6197
[09/26 14:45:45 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1673, average loss: 2.2475
[09/26 14:45:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 88.00	
[09/26 14:45:45 visual_prompt]: Training 44 / 100 epoch, with learning rate 1.7584208038447504
[09/26 14:45:51 visual_prompt]: Epoch 44 / 100: avg data time: 4.67e-02, avg batch time: 0.4928, average train loss: 1.6524
[09/26 14:45:53 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.1671, average loss: 1.9547
[09/26 14:45:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 94.00	
[09/26 14:45:53 visual_prompt]: Best epoch 44: best metric: 0.320
[09/26 14:45:53 visual_prompt]: Training 45 / 100 epoch, with learning rate 1.7182582417698902
[09/26 14:46:00 visual_prompt]: Epoch 45 / 100: avg data time: 5.35e-02, avg batch time: 0.4966, average train loss: 1.3705
[09/26 14:46:01 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1673, average loss: 2.0579
[09/26 14:46:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 89.50	
[09/26 14:46:01 visual_prompt]: Training 46 / 100 epoch, with learning rate 1.6775251791570862
[09/26 14:46:08 visual_prompt]: Epoch 46 / 100: avg data time: 4.55e-02, avg batch time: 0.4915, average train loss: 1.3813
[09/26 14:46:10 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1672, average loss: 2.1018
[09/26 14:46:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 90.50	
[09/26 14:46:10 visual_prompt]: Training 47 / 100 epoch, with learning rate 1.6362712429686843
[09/26 14:46:17 visual_prompt]: Epoch 47 / 100: avg data time: 6.06e-02, avg batch time: 0.5046, average train loss: 1.4081
[09/26 14:46:18 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1677, average loss: 2.0762
[09/26 14:46:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 93.50	
[09/26 14:46:18 visual_prompt]: Training 48 / 100 epoch, with learning rate 1.5945466947712488
[09/26 14:46:25 visual_prompt]: Epoch 48 / 100: avg data time: 5.75e-02, avg batch time: 0.5017, average train loss: 1.4697
[09/26 14:46:27 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1671, average loss: 1.8548
[09/26 14:46:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 94.50	
[09/26 14:46:27 visual_prompt]: Training 49 / 100 epoch, with learning rate 1.5524023694995845
[09/26 14:46:33 visual_prompt]: Epoch 49 / 100: avg data time: 5.47e-02, avg batch time: 0.4976, average train loss: 1.1762
[09/26 14:46:35 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1669, average loss: 1.8580
[09/26 14:46:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 95.50	
[09/26 14:46:35 visual_prompt]: Training 50 / 100 epoch, with learning rate 1.509889613522199
[09/26 14:46:42 visual_prompt]: Epoch 50 / 100: avg data time: 6.13e-02, avg batch time: 0.5051, average train loss: 1.1013
[09/26 14:46:43 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1671, average loss: 3.2015
[09/26 14:46:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 91.50	
[09/26 14:46:43 visual_prompt]: Training 51 / 100 epoch, with learning rate 1.467060222083663
[09/26 14:46:50 visual_prompt]: Epoch 51 / 100: avg data time: 5.64e-02, avg batch time: 0.5004, average train loss: 1.5499
[09/26 14:46:52 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1671, average loss: 2.7216
[09/26 14:46:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 80.00	
[09/26 14:46:52 visual_prompt]: Training 52 / 100 epoch, with learning rate 1.4239663762000818
[09/26 14:46:59 visual_prompt]: Epoch 52 / 100: avg data time: 4.73e-02, avg batch time: 0.4929, average train loss: 1.3604
[09/26 14:47:00 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1674, average loss: 2.1370
[09/26 14:47:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 95.00	
[09/26 14:47:00 visual_prompt]: Training 53 / 100 epoch, with learning rate 1.380660579084567
[09/26 14:47:07 visual_prompt]: Epoch 53 / 100: avg data time: 6.04e-02, avg batch time: 0.5044, average train loss: 1.1213
[09/26 14:47:09 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1676, average loss: 1.8728
[09/26 14:47:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 96.00	
[09/26 14:47:09 visual_prompt]: Training 54 / 100 epoch, with learning rate 1.3371955921801564
[09/26 14:47:15 visual_prompt]: Epoch 54 / 100: avg data time: 5.04e-02, avg batch time: 0.4938, average train loss: 0.9759
[09/26 14:47:17 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1674, average loss: 2.1561
[09/26 14:47:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 96.00	
[09/26 14:47:17 visual_prompt]: Training 55 / 100 epoch, with learning rate 1.2936243708781263
[09/26 14:47:24 visual_prompt]: Epoch 55 / 100: avg data time: 5.60e-02, avg batch time: 0.5003, average train loss: 0.9704
[09/26 14:47:25 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1672, average loss: 2.4651
[09/26 14:47:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 95.50	
[09/26 14:47:25 visual_prompt]: Training 56 / 100 epoch, with learning rate 1.25
[09/26 14:47:32 visual_prompt]: Epoch 56 / 100: avg data time: 4.50e-02, avg batch time: 0.4910, average train loss: 0.9511
[09/26 14:47:34 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1675, average loss: 2.2085
[09/26 14:47:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 95.50	
[09/26 14:47:34 visual_prompt]: Training 57 / 100 epoch, with learning rate 1.2063756291218741
[09/26 14:47:40 visual_prompt]: Epoch 57 / 100: avg data time: 5.07e-02, avg batch time: 0.4959, average train loss: 0.9653
[09/26 14:47:42 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1677, average loss: 2.1381
[09/26 14:47:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 96.00	
[09/26 14:47:42 visual_prompt]: Training 58 / 100 epoch, with learning rate 1.1628044078198434
[09/26 14:47:49 visual_prompt]: Epoch 58 / 100: avg data time: 5.36e-02, avg batch time: 0.4976, average train loss: 0.9514
[09/26 14:47:50 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1670, average loss: 2.3671
[09/26 14:47:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 96.00	
[09/26 14:47:50 visual_prompt]: Training 59 / 100 epoch, with learning rate 1.1193394209154333
[09/26 14:47:57 visual_prompt]: Epoch 59 / 100: avg data time: 4.63e-02, avg batch time: 0.4917, average train loss: 0.8763
[09/26 14:47:59 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1672, average loss: 2.4210
[09/26 14:47:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 14:47:59 visual_prompt]: Best epoch 59: best metric: 0.340
[09/26 14:47:59 visual_prompt]: Training 60 / 100 epoch, with learning rate 1.0760336237999186
[09/26 14:48:06 visual_prompt]: Epoch 60 / 100: avg data time: 5.10e-02, avg batch time: 0.4944, average train loss: 0.8206
[09/26 14:48:07 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1676, average loss: 2.7172
[09/26 14:48:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 93.50	
[09/26 14:48:07 visual_prompt]: Training 61 / 100 epoch, with learning rate 1.0329397779163372
[09/26 14:48:14 visual_prompt]: Epoch 61 / 100: avg data time: 5.47e-02, avg batch time: 0.5004, average train loss: 0.7938
[09/26 14:48:16 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1673, average loss: 2.4943
[09/26 14:48:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 94.00	
[09/26 14:48:16 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.990110386477801
[09/26 14:48:22 visual_prompt]: Epoch 62 / 100: avg data time: 4.90e-02, avg batch time: 0.4930, average train loss: 0.6719
[09/26 14:48:24 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1672, average loss: 2.4243
[09/26 14:48:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 96.00	
[09/26 14:48:24 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.9475976305004155
[09/26 14:48:31 visual_prompt]: Epoch 63 / 100: avg data time: 5.03e-02, avg batch time: 0.4943, average train loss: 0.7676
[09/26 14:48:32 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1673, average loss: 2.4470
[09/26 14:48:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 96.00	
[09/26 14:48:32 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.9054533052287511
[09/26 14:48:39 visual_prompt]: Epoch 64 / 100: avg data time: 7.15e-02, avg batch time: 0.5144, average train loss: 0.6590
[09/26 14:48:41 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1671, average loss: 2.5116
[09/26 14:48:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.50	
[09/26 14:48:41 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.8637287570313158
[09/26 14:48:48 visual_prompt]: Epoch 65 / 100: avg data time: 6.08e-02, avg batch time: 0.5039, average train loss: 0.5939
[09/26 14:48:49 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1674, average loss: 3.5727
[09/26 14:48:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 90.00	
[09/26 14:48:49 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.8224748208429142
[09/26 14:48:56 visual_prompt]: Epoch 66 / 100: avg data time: 6.26e-02, avg batch time: 0.5053, average train loss: 0.7837
[09/26 14:48:58 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1675, average loss: 2.7561
[09/26 14:48:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 97.00	
[09/26 14:48:58 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.7817417582301099
[09/26 14:49:05 visual_prompt]: Epoch 67 / 100: avg data time: 6.61e-02, avg batch time: 0.5093, average train loss: 0.6006
[09/26 14:49:07 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1677, average loss: 3.2358
[09/26 14:49:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 93.50	
[09/26 14:49:07 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.7415791961552499
[09/26 14:49:13 visual_prompt]: Epoch 68 / 100: avg data time: 6.86e-02, avg batch time: 0.5115, average train loss: 0.5428
[09/26 14:49:15 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1676, average loss: 3.2560
[09/26 14:49:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 94.50	
[09/26 14:49:15 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.7020360665136531
[09/26 14:49:22 visual_prompt]: Epoch 69 / 100: avg data time: 5.79e-02, avg batch time: 0.5014, average train loss: 0.4747
[09/26 14:49:24 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1675, average loss: 3.1753
[09/26 14:49:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 96.00	
[09/26 14:49:24 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.6631605465176369
[09/26 14:49:30 visual_prompt]: Epoch 70 / 100: avg data time: 6.16e-02, avg batch time: 0.5059, average train loss: 0.4325
[09/26 14:49:32 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1673, average loss: 3.2518
[09/26 14:49:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.00	
[09/26 14:49:32 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.6250000000000002
[09/26 14:49:39 visual_prompt]: Epoch 71 / 100: avg data time: 5.94e-02, avg batch time: 0.5036, average train loss: 0.4451
[09/26 14:49:40 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1677, average loss: 3.4070
[09/26 14:49:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 96.50	
[09/26 14:49:41 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.587600919708494
[09/26 14:49:47 visual_prompt]: Epoch 72 / 100: avg data time: 5.85e-02, avg batch time: 0.5041, average train loss: 0.5002
[09/26 14:49:49 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1675, average loss: 3.5390
[09/26 14:49:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 95.00	
[09/26 14:49:49 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.5510088706615667
[09/26 14:49:56 visual_prompt]: Epoch 73 / 100: avg data time: 6.05e-02, avg batch time: 0.5041, average train loss: 0.3750
[09/26 14:49:57 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1675, average loss: 3.5458
[09/26 14:49:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 94.50	
[09/26 14:49:57 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.5152684346344087
[09/26 14:50:04 visual_prompt]: Epoch 74 / 100: avg data time: 5.46e-02, avg batch time: 0.4997, average train loss: 0.3287
[09/26 14:50:06 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1674, average loss: 3.8062
[09/26 14:50:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 96.00	
[09/26 14:50:06 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.48042315584292716
[09/26 14:50:13 visual_prompt]: Epoch 75 / 100: avg data time: 4.82e-02, avg batch time: 0.4932, average train loss: 0.3051
[09/26 14:50:14 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1675, average loss: 3.7138
[09/26 14:50:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 96.50	
[09/26 14:50:14 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.4465154878918258
[09/26 14:50:21 visual_prompt]: Epoch 76 / 100: avg data time: 5.26e-02, avg batch time: 0.4981, average train loss: 0.2436
[09/26 14:50:23 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1677, average loss: 3.8343
[09/26 14:50:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 95.50	
[09/26 14:50:23 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.41358674205142765
[09/26 14:50:29 visual_prompt]: Epoch 77 / 100: avg data time: 6.18e-02, avg batch time: 0.5059, average train loss: 0.2617
[09/26 14:50:31 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1676, average loss: 3.9330
[09/26 14:50:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 96.50	
[09/26 14:50:31 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.3816770369262533
[09/26 14:50:38 visual_prompt]: Epoch 78 / 100: avg data time: 6.80e-02, avg batch time: 0.5118, average train loss: 0.2635
[09/26 14:50:40 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1676, average loss: 3.9748
[09/26 14:50:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.00	
[09/26 14:50:40 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.35082524957668626
[09/26 14:50:46 visual_prompt]: Epoch 79 / 100: avg data time: 6.04e-02, avg batch time: 0.5054, average train loss: 0.2115
[09/26 14:50:48 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1677, average loss: 4.0444
[09/26 14:50:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 96.00	
[09/26 14:50:48 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.32106896815325703
[09/26 14:50:55 visual_prompt]: Epoch 80 / 100: avg data time: 6.32e-02, avg batch time: 0.5069, average train loss: 0.1755
[09/26 14:50:56 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1673, average loss: 4.1620
[09/26 14:50:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 96.50	
[09/26 14:50:56 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.2924444461012776
[09/26 14:51:03 visual_prompt]: Epoch 81 / 100: avg data time: 6.34e-02, avg batch time: 0.5069, average train loss: 0.1928
[09/26 14:51:05 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1675, average loss: 4.1226
[09/26 14:51:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.50	
[09/26 14:51:05 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.26498655799159765
[09/26 14:51:12 visual_prompt]: Epoch 82 / 100: avg data time: 6.11e-02, avg batch time: 0.5055, average train loss: 0.1585
[09/26 14:51:13 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1674, average loss: 4.5733
[09/26 14:51:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.00	
[09/26 14:51:13 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.23872875703131582
[09/26 14:51:20 visual_prompt]: Epoch 83 / 100: avg data time: 6.13e-02, avg batch time: 0.5070, average train loss: 0.1615
[09/26 14:51:22 visual_prompt]: Inference (val):avg data time: 2.05e-05, avg batch time: 0.1672, average loss: 4.4461
[09/26 14:51:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 95.50	
[09/26 14:51:22 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.21370303430619797
[09/26 14:51:29 visual_prompt]: Epoch 84 / 100: avg data time: 6.05e-02, avg batch time: 0.5047, average train loss: 0.1420
[09/26 14:51:30 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1673, average loss: 4.6297
[09/26 14:51:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 95.00	
[09/26 14:51:30 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.18993987980446755
[09/26 14:51:37 visual_prompt]: Epoch 85 / 100: avg data time: 6.48e-02, avg batch time: 0.5100, average train loss: 0.1266
[09/26 14:51:39 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1676, average loss: 4.5459
[09/26 14:51:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 95.50	
[09/26 14:51:39 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.16746824526945162
[09/26 14:51:46 visual_prompt]: Epoch 86 / 100: avg data time: 6.03e-02, avg batch time: 0.5038, average train loss: 0.1422
[09/26 14:51:47 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1681, average loss: 4.5602
[09/26 14:51:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 96.00	
[09/26 14:51:47 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.14631550892634126
[09/26 14:51:54 visual_prompt]: Epoch 87 / 100: avg data time: 6.00e-02, avg batch time: 0.5050, average train loss: 0.1163
[09/26 14:51:56 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1674, average loss: 4.5745
[09/26 14:51:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 96.00	
[09/26 14:51:56 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.12650744212604148
[09/26 14:52:03 visual_prompt]: Epoch 88 / 100: avg data time: 5.06e-02, avg batch time: 0.4942, average train loss: 0.1154
[09/26 14:52:04 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1674, average loss: 4.6649
[09/26 14:52:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 95.50	
[09/26 14:52:04 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.10806817794674878
[09/26 14:52:11 visual_prompt]: Epoch 89 / 100: avg data time: 5.85e-02, avg batch time: 0.5031, average train loss: 0.0990
[09/26 14:52:13 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1675, average loss: 4.7809
[09/26 14:52:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 96.00	
[09/26 14:52:13 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.09102018179151586
[09/26 14:52:20 visual_prompt]: Epoch 90 / 100: avg data time: 5.80e-02, avg batch time: 0.5028, average train loss: 0.0949
[09/26 14:52:21 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1677, average loss: 4.7124
[09/26 14:52:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 96.50	
[09/26 14:52:21 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0753842240176146
[09/26 14:52:28 visual_prompt]: Epoch 91 / 100: avg data time: 4.91e-02, avg batch time: 0.4940, average train loss: 0.0841
[09/26 14:52:29 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1675, average loss: 4.7376
[09/26 14:52:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.50	
[09/26 14:52:29 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.061179354631058086
[09/26 14:52:36 visual_prompt]: Epoch 92 / 100: avg data time: 5.22e-02, avg batch time: 0.4973, average train loss: 0.0792
[09/26 14:52:38 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1675, average loss: 4.7969
[09/26 14:52:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 96.50	
[09/26 14:52:38 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.04842288007710138
[09/26 14:52:45 visual_prompt]: Epoch 93 / 100: avg data time: 4.80e-02, avg batch time: 0.4933, average train loss: 0.0707
[09/26 14:52:46 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1670, average loss: 4.8270
[09/26 14:52:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 96.50	
[09/26 14:52:46 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.03713034215500441
[09/26 14:52:53 visual_prompt]: Epoch 94 / 100: avg data time: 5.04e-02, avg batch time: 0.4957, average train loss: 0.0800
[09/26 14:52:54 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1675, average loss: 4.8723
[09/26 14:52:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 96.50	
[09/26 14:52:54 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.02731549908274289
[09/26 14:53:01 visual_prompt]: Epoch 95 / 100: avg data time: 5.36e-02, avg batch time: 0.4978, average train loss: 0.0814
[09/26 14:53:03 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1671, average loss: 4.8616
[09/26 14:53:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.00	
[09/26 14:53:03 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.018990308734739975
[09/26 14:53:10 visual_prompt]: Epoch 96 / 100: avg data time: 5.28e-02, avg batch time: 0.4967, average train loss: 0.0606
[09/26 14:53:11 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1674, average loss: 4.8915
[09/26 14:53:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.00	
[09/26 14:53:11 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.012164914073037048
[09/26 14:53:18 visual_prompt]: Epoch 97 / 100: avg data time: 5.07e-02, avg batch time: 0.4956, average train loss: 0.0766
[09/26 14:53:19 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1670, average loss: 4.9092
[09/26 14:53:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.50	
[09/26 14:53:20 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.006847630789658388
[09/26 14:53:26 visual_prompt]: Epoch 98 / 100: avg data time: 4.89e-02, avg batch time: 0.4918, average train loss: 0.0746
[09/26 14:53:28 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1672, average loss: 4.9112
[09/26 14:53:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 96.00	
[09/26 14:53:28 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.003044937175219753
[09/26 14:53:35 visual_prompt]: Epoch 99 / 100: avg data time: 5.25e-02, avg batch time: 0.4975, average train loss: 0.0699
[09/26 14:53:36 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1669, average loss: 4.9247
[09/26 14:53:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.50	
[09/26 14:53:36 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0007614662261302974
[09/26 14:53:43 visual_prompt]: Epoch 100 / 100: avg data time: 4.94e-02, avg batch time: 0.4929, average train loss: 0.0733
[09/26 14:53:44 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1671, average loss: 4.9269
[09/26 14:53:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.50	
[09/26 14:53:44 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 14:53:44 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 14:53:44 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 14:53:44 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 14:53:44 visual_prompt]: Training with config:
[09/26 14:53:44 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 14:53:44 visual_prompt]: Loading training data...
[09/26 14:53:44 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 14:53:46 visual_prompt]: Number of images: 800
[09/26 14:53:46 visual_prompt]: Number of classes: 9 / 9
[09/26 14:53:46 visual_prompt]: Loading validation data...
[09/26 14:53:46 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 14:53:46 visual_prompt]: Number of images: 200
[09/26 14:53:46 visual_prompt]: Number of classes: 9 / 9
[09/26 14:53:46 visual_prompt]: Constructing models...
[09/26 14:53:48 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 14:53:48 visual_prompt]: tuned percent:0.542
[09/26 14:53:49 visual_prompt]: Device used for model: 0
[09/26 14:53:49 visual_prompt]: Setting up Evaluator...
[09/26 14:53:49 visual_prompt]: Setting up Trainer...
[09/26 14:53:49 visual_prompt]: 	Setting up the optimizer...
[09/26 14:53:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 14:53:55 visual_prompt]: Epoch 1 / 100: avg data time: 4.89e-02, avg batch time: 0.4955, average train loss: 2.8701
[09/26 14:53:57 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1668, average loss: 2.9516
[09/26 14:53:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 14:53:57 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 14:53:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 14:54:04 visual_prompt]: Epoch 2 / 100: avg data time: 6.10e-02, avg batch time: 0.5047, average train loss: 3.0143
[09/26 14:54:05 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1669, average loss: 2.5884
[09/26 14:54:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 14:54:05 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 14:54:12 visual_prompt]: Epoch 3 / 100: avg data time: 5.35e-02, avg batch time: 0.4969, average train loss: 2.3227
[09/26 14:54:14 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1668, average loss: 2.2845
[09/26 14:54:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 14:54:14 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 14:54:21 visual_prompt]: Epoch 4 / 100: avg data time: 6.18e-02, avg batch time: 0.5041, average train loss: 2.2275
[09/26 14:54:22 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1668, average loss: 2.2498
[09/26 14:54:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 14:54:22 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 14:54:29 visual_prompt]: Epoch 5 / 100: avg data time: 5.87e-02, avg batch time: 0.5022, average train loss: 2.2291
[09/26 14:54:31 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1666, average loss: 2.2026
[09/26 14:54:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.50	
[09/26 14:54:31 visual_prompt]: Best epoch 5: best metric: 0.145
[09/26 14:54:31 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 14:54:38 visual_prompt]: Epoch 6 / 100: avg data time: 6.27e-02, avg batch time: 0.5060, average train loss: 2.2360
[09/26 14:54:39 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1668, average loss: 2.2350
[09/26 14:54:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 14:54:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 14:54:46 visual_prompt]: Epoch 7 / 100: avg data time: 5.75e-02, avg batch time: 0.5004, average train loss: 2.2372
[09/26 14:54:48 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1669, average loss: 2.2744
[09/26 14:54:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 14:54:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 14:54:55 visual_prompt]: Epoch 8 / 100: avg data time: 6.29e-02, avg batch time: 0.5052, average train loss: 2.2425
[09/26 14:54:56 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1667, average loss: 2.2627
[09/26 14:54:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.50	
[09/26 14:54:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 14:55:03 visual_prompt]: Epoch 9 / 100: avg data time: 6.29e-02, avg batch time: 0.5049, average train loss: 2.2581
[09/26 14:55:05 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1669, average loss: 2.2948
[09/26 14:55:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 14:55:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 14:55:12 visual_prompt]: Epoch 10 / 100: avg data time: 6.41e-02, avg batch time: 0.5090, average train loss: 2.2869
[09/26 14:55:13 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1669, average loss: 2.3160
[09/26 14:55:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 14:55:13 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 14:55:20 visual_prompt]: Epoch 11 / 100: avg data time: 5.90e-02, avg batch time: 0.5026, average train loss: 2.2670
[09/26 14:55:22 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1671, average loss: 2.3318
[09/26 14:55:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 14:55:22 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 14:55:29 visual_prompt]: Epoch 12 / 100: avg data time: 6.34e-02, avg batch time: 0.5057, average train loss: 2.3025
[09/26 14:55:30 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1670, average loss: 2.2740
[09/26 14:55:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 14:55:30 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 14:55:37 visual_prompt]: Epoch 13 / 100: avg data time: 4.93e-02, avg batch time: 0.4925, average train loss: 2.3323
[09/26 14:55:39 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.1670, average loss: 2.2244
[09/26 14:55:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 14:55:39 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 14:55:45 visual_prompt]: Epoch 14 / 100: avg data time: 4.80e-02, avg batch time: 0.4923, average train loss: 2.2611
[09/26 14:55:47 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1674, average loss: 2.2648
[09/26 14:55:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 14:55:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 14:55:54 visual_prompt]: Epoch 15 / 100: avg data time: 5.71e-02, avg batch time: 0.5012, average train loss: 2.2498
[09/26 14:55:55 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1670, average loss: 2.2478
[09/26 14:55:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 14:55:55 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 14:56:02 visual_prompt]: Epoch 16 / 100: avg data time: 4.70e-02, avg batch time: 0.4927, average train loss: 2.4602
[09/26 14:56:04 visual_prompt]: Inference (val):avg data time: 3.85e-05, avg batch time: 0.1671, average loss: 2.7330
[09/26 14:56:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 14:56:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 14:56:10 visual_prompt]: Epoch 17 / 100: avg data time: 4.65e-02, avg batch time: 0.4916, average train loss: 2.3739
[09/26 14:56:12 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1672, average loss: 2.6552
[09/26 14:56:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 14:56:12 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 14:56:19 visual_prompt]: Epoch 18 / 100: avg data time: 6.12e-02, avg batch time: 0.5051, average train loss: 2.3182
[09/26 14:56:21 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1672, average loss: 2.4407
[09/26 14:56:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 14:56:21 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 14:56:27 visual_prompt]: Epoch 19 / 100: avg data time: 4.65e-02, avg batch time: 0.4888, average train loss: 2.2891
[09/26 14:56:29 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1671, average loss: 2.3425
[09/26 14:56:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/26 14:56:29 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 14:56:36 visual_prompt]: Epoch 20 / 100: avg data time: 6.68e-02, avg batch time: 0.5106, average train loss: 2.3064
[09/26 14:56:37 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1672, average loss: 2.2266
[09/26 14:56:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 14:56:37 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 14:56:44 visual_prompt]: Epoch 21 / 100: avg data time: 5.38e-02, avg batch time: 0.4981, average train loss: 2.3360
[09/26 14:56:46 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1673, average loss: 2.2577
[09/26 14:56:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 14:56:46 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 14:56:53 visual_prompt]: Epoch 22 / 100: avg data time: 4.91e-02, avg batch time: 0.4943, average train loss: 2.2476
[09/26 14:56:54 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1672, average loss: 2.5514
[09/26 14:56:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 14:56:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 14:57:01 visual_prompt]: Epoch 23 / 100: avg data time: 6.14e-02, avg batch time: 0.5047, average train loss: 2.2507
[09/26 14:57:03 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1671, average loss: 2.3029
[09/26 14:57:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 14:57:03 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 14:57:10 visual_prompt]: Epoch 24 / 100: avg data time: 5.72e-02, avg batch time: 0.5006, average train loss: 2.2945
[09/26 14:57:11 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1674, average loss: 2.2705
[09/26 14:57:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 14:57:11 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 14:57:18 visual_prompt]: Epoch 25 / 100: avg data time: 5.78e-02, avg batch time: 0.5002, average train loss: 2.3271
[09/26 14:57:20 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1670, average loss: 2.2850
[09/26 14:57:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 14:57:20 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 14:57:26 visual_prompt]: Epoch 26 / 100: avg data time: 6.45e-02, avg batch time: 0.5068, average train loss: 2.3387
[09/26 14:57:28 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1670, average loss: 2.4917
[09/26 14:57:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 14:57:28 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 14:57:35 visual_prompt]: Epoch 27 / 100: avg data time: 4.86e-02, avg batch time: 0.4914, average train loss: 2.3313
[09/26 14:57:36 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1667, average loss: 2.3004
[09/26 14:57:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/26 14:57:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 14:57:43 visual_prompt]: Epoch 28 / 100: avg data time: 6.65e-02, avg batch time: 0.5087, average train loss: 2.2458
[09/26 14:57:45 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1671, average loss: 2.2589
[09/26 14:57:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 14:57:45 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 14:57:52 visual_prompt]: Epoch 29 / 100: avg data time: 5.77e-02, avg batch time: 0.5002, average train loss: 2.2702
[09/26 14:57:53 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1669, average loss: 2.2357
[09/26 14:57:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.00	
[09/26 14:57:53 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 14:58:00 visual_prompt]: Epoch 30 / 100: avg data time: 5.85e-02, avg batch time: 0.5018, average train loss: 2.2507
[09/26 14:58:02 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1671, average loss: 2.3108
[09/26 14:58:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.50	
[09/26 14:58:02 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 14:58:09 visual_prompt]: Epoch 31 / 100: avg data time: 5.61e-02, avg batch time: 0.5000, average train loss: 2.3140
[09/26 14:58:10 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1669, average loss: 2.2890
[09/26 14:58:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 14:58:10 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 14:58:17 visual_prompt]: Epoch 32 / 100: avg data time: 5.70e-02, avg batch time: 0.5000, average train loss: 2.2720
[09/26 14:58:19 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1671, average loss: 2.2955
[09/26 14:58:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 14:58:19 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 14:58:25 visual_prompt]: Epoch 33 / 100: avg data time: 5.26e-02, avg batch time: 0.4963, average train loss: 2.2861
[09/26 14:58:27 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1667, average loss: 2.2916
[09/26 14:58:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 14:58:27 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 14:58:34 visual_prompt]: Epoch 34 / 100: avg data time: 6.33e-02, avg batch time: 0.5071, average train loss: 2.2498
[09/26 14:58:36 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1667, average loss: 2.2854
[09/26 14:58:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 14:58:36 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 14:58:42 visual_prompt]: Epoch 35 / 100: avg data time: 5.95e-02, avg batch time: 0.5021, average train loss: 2.2704
[09/26 14:58:44 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1672, average loss: 2.2185
[09/26 14:58:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 14:58:44 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 14:58:51 visual_prompt]: Epoch 36 / 100: avg data time: 5.70e-02, avg batch time: 0.4999, average train loss: 2.2542
[09/26 14:58:53 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1668, average loss: 2.3158
[09/26 14:58:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 14:58:53 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 14:58:59 visual_prompt]: Epoch 37 / 100: avg data time: 6.26e-02, avg batch time: 0.5056, average train loss: 2.2771
[09/26 14:59:01 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1671, average loss: 2.2765
[09/26 14:59:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 14:59:01 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 14:59:08 visual_prompt]: Epoch 38 / 100: avg data time: 5.98e-02, avg batch time: 0.5031, average train loss: 2.2516
[09/26 14:59:10 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1671, average loss: 2.3369
[09/26 14:59:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/26 14:59:10 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 14:59:16 visual_prompt]: Epoch 39 / 100: avg data time: 6.61e-02, avg batch time: 0.5098, average train loss: 2.2573
[09/26 14:59:18 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1671, average loss: 2.2045
[09/26 14:59:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 14:59:18 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 14:59:25 visual_prompt]: Epoch 40 / 100: avg data time: 6.32e-02, avg batch time: 0.5049, average train loss: 2.2727
[09/26 14:59:27 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1673, average loss: 2.2448
[09/26 14:59:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 14:59:27 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 14:59:34 visual_prompt]: Epoch 41 / 100: avg data time: 6.53e-02, avg batch time: 0.5077, average train loss: 2.2432
[09/26 14:59:35 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1667, average loss: 2.3062
[09/26 14:59:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.50	
[09/26 14:59:35 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 14:59:42 visual_prompt]: Epoch 42 / 100: avg data time: 5.49e-02, avg batch time: 0.4986, average train loss: 2.2840
[09/26 14:59:43 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1669, average loss: 2.3017
[09/26 14:59:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.00	
[09/26 14:59:43 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 14:59:50 visual_prompt]: Epoch 43 / 100: avg data time: 4.75e-02, avg batch time: 0.4901, average train loss: 2.2680
[09/26 14:59:52 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1672, average loss: 2.2894
[09/26 14:59:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 14:59:52 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 14:59:59 visual_prompt]: Epoch 44 / 100: avg data time: 5.23e-02, avg batch time: 0.4951, average train loss: 2.2634
[09/26 15:00:00 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1670, average loss: 2.2600
[09/26 15:00:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.50	
[09/26 15:00:00 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 15:00:07 visual_prompt]: Epoch 45 / 100: avg data time: 6.08e-02, avg batch time: 0.5036, average train loss: 2.2606
[09/26 15:00:09 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1668, average loss: 2.2830
[09/26 15:00:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 15:00:09 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 15:00:15 visual_prompt]: Epoch 46 / 100: avg data time: 4.68e-02, avg batch time: 0.4911, average train loss: 2.3223
[09/26 15:00:17 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1670, average loss: 2.2892
[09/26 15:00:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 15:00:17 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 15:00:24 visual_prompt]: Epoch 47 / 100: avg data time: 5.93e-02, avg batch time: 0.5034, average train loss: 2.2457
[09/26 15:00:25 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1670, average loss: 2.2654
[09/26 15:00:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.00	top5: 48.50	
[09/26 15:00:25 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 15:00:32 visual_prompt]: Epoch 48 / 100: avg data time: 6.05e-02, avg batch time: 0.5047, average train loss: 2.2549
[09/26 15:00:34 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1670, average loss: 2.2800
[09/26 15:00:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 15:00:34 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 15:00:41 visual_prompt]: Epoch 49 / 100: avg data time: 5.61e-02, avg batch time: 0.4986, average train loss: 2.2790
[09/26 15:00:42 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1671, average loss: 2.2255
[09/26 15:00:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 15:00:42 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 15:00:49 visual_prompt]: Epoch 50 / 100: avg data time: 6.19e-02, avg batch time: 0.5047, average train loss: 2.2660
[09/26 15:00:51 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1669, average loss: 2.2493
[09/26 15:00:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 15:00:51 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 15:00:57 visual_prompt]: Epoch 51 / 100: avg data time: 5.55e-02, avg batch time: 0.5012, average train loss: 2.2548
[09/26 15:00:59 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1670, average loss: 2.2767
[09/26 15:00:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.00	
[09/26 15:00:59 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 15:01:06 visual_prompt]: Epoch 52 / 100: avg data time: 5.67e-02, avg batch time: 0.4993, average train loss: 2.3650
[09/26 15:01:07 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1672, average loss: 2.2284
[09/26 15:01:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 54.00	
[09/26 15:01:07 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 15:01:14 visual_prompt]: Epoch 53 / 100: avg data time: 4.75e-02, avg batch time: 0.4920, average train loss: 2.2739
[09/26 15:01:16 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1670, average loss: 2.2416
[09/26 15:01:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 15:01:16 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 15:01:23 visual_prompt]: Epoch 54 / 100: avg data time: 5.01e-02, avg batch time: 0.4947, average train loss: 2.2498
[09/26 15:01:24 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1672, average loss: 2.2199
[09/26 15:01:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 61.00	
[09/26 15:01:24 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 15:01:31 visual_prompt]: Epoch 55 / 100: avg data time: 5.04e-02, avg batch time: 0.4949, average train loss: 2.2707
[09/26 15:01:32 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1668, average loss: 2.2172
[09/26 15:01:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 15:01:32 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 15:01:39 visual_prompt]: Epoch 56 / 100: avg data time: 5.38e-02, avg batch time: 0.4973, average train loss: 2.2501
[09/26 15:01:41 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1672, average loss: 2.2150
[09/26 15:01:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 15:01:41 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 15:01:48 visual_prompt]: Epoch 57 / 100: avg data time: 5.33e-02, avg batch time: 0.4975, average train loss: 2.2380
[09/26 15:01:49 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1670, average loss: 2.2174
[09/26 15:01:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 52.00	
[09/26 15:01:49 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 15:01:56 visual_prompt]: Epoch 58 / 100: avg data time: 5.89e-02, avg batch time: 0.5014, average train loss: 2.2236
[09/26 15:01:57 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1669, average loss: 2.2605
[09/26 15:01:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 52.00	
[09/26 15:01:57 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 15:02:04 visual_prompt]: Epoch 59 / 100: avg data time: 4.84e-02, avg batch time: 0.4923, average train loss: 2.2502
[09/26 15:02:06 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1669, average loss: 2.2638
[09/26 15:02:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 15:02:06 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 15:02:13 visual_prompt]: Epoch 60 / 100: avg data time: 5.10e-02, avg batch time: 0.4952, average train loss: 2.2259
[09/26 15:02:14 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1672, average loss: 2.2173
[09/26 15:02:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 15:02:14 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 15:02:21 visual_prompt]: Epoch 61 / 100: avg data time: 5.04e-02, avg batch time: 0.4950, average train loss: 2.2156
[09/26 15:02:22 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1671, average loss: 2.2815
[09/26 15:02:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 15:02:22 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 15:02:29 visual_prompt]: Epoch 62 / 100: avg data time: 5.72e-02, avg batch time: 0.5007, average train loss: 2.2586
[09/26 15:02:31 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1667, average loss: 2.1851
[09/26 15:02:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.50	
[09/26 15:02:31 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 15:02:38 visual_prompt]: Epoch 63 / 100: avg data time: 4.56e-02, avg batch time: 0.4903, average train loss: 2.2489
[09/26 15:02:39 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1669, average loss: 2.1968
[09/26 15:02:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 15:02:39 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 15:02:46 visual_prompt]: Epoch 64 / 100: avg data time: 6.40e-02, avg batch time: 0.5068, average train loss: 2.2228
[09/26 15:02:48 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1670, average loss: 2.2250
[09/26 15:02:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/26 15:02:48 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 15:02:55 visual_prompt]: Epoch 65 / 100: avg data time: 5.87e-02, avg batch time: 0.5040, average train loss: 2.2289
[09/26 15:02:56 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1669, average loss: 2.2419
[09/26 15:02:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 50.50	
[09/26 15:02:56 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 15:03:03 visual_prompt]: Epoch 66 / 100: avg data time: 6.17e-02, avg batch time: 0.5047, average train loss: 2.2269
[09/26 15:03:05 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1671, average loss: 2.1990
[09/26 15:03:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 15:03:05 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 15:03:11 visual_prompt]: Epoch 67 / 100: avg data time: 6.01e-02, avg batch time: 0.5029, average train loss: 2.2271
[09/26 15:03:13 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1670, average loss: 2.2082
[09/26 15:03:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 15:03:13 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 15:03:20 visual_prompt]: Epoch 68 / 100: avg data time: 5.87e-02, avg batch time: 0.5035, average train loss: 2.2195
[09/26 15:03:22 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1666, average loss: 2.2630
[09/26 15:03:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 15:03:22 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 15:03:28 visual_prompt]: Epoch 69 / 100: avg data time: 6.19e-02, avg batch time: 0.5051, average train loss: 2.2143
[09/26 15:03:30 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1668, average loss: 2.1961
[09/26 15:03:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 15:03:30 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 15:03:37 visual_prompt]: Epoch 70 / 100: avg data time: 6.38e-02, avg batch time: 0.5059, average train loss: 2.2088
[09/26 15:03:39 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1670, average loss: 2.2083
[09/26 15:03:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 15:03:39 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 15:03:46 visual_prompt]: Epoch 71 / 100: avg data time: 6.36e-02, avg batch time: 0.5077, average train loss: 2.2239
[09/26 15:03:47 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1672, average loss: 2.2139
[09/26 15:03:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 15:03:47 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 15:03:54 visual_prompt]: Epoch 72 / 100: avg data time: 4.84e-02, avg batch time: 0.4940, average train loss: 2.2008
[09/26 15:03:56 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1668, average loss: 2.1955
[09/26 15:03:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 15:03:56 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 15:04:02 visual_prompt]: Epoch 73 / 100: avg data time: 4.96e-02, avg batch time: 0.4923, average train loss: 2.2040
[09/26 15:04:04 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1671, average loss: 2.2282
[09/26 15:04:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 15:04:04 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 15:04:11 visual_prompt]: Epoch 74 / 100: avg data time: 4.68e-02, avg batch time: 0.4920, average train loss: 2.2168
[09/26 15:04:12 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1669, average loss: 2.2010
[09/26 15:04:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 15:04:12 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 15:04:19 visual_prompt]: Epoch 75 / 100: avg data time: 6.09e-02, avg batch time: 0.5053, average train loss: 2.2025
[09/26 15:04:21 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1671, average loss: 2.1905
[09/26 15:04:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 15:04:21 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 15:04:27 visual_prompt]: Epoch 76 / 100: avg data time: 4.94e-02, avg batch time: 0.4939, average train loss: 2.2011
[09/26 15:04:29 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1670, average loss: 2.2085
[09/26 15:04:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 15:04:29 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 15:04:36 visual_prompt]: Epoch 77 / 100: avg data time: 5.88e-02, avg batch time: 0.5022, average train loss: 2.2018
[09/26 15:04:37 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1668, average loss: 2.2010
[09/26 15:04:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 15:04:37 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 15:04:44 visual_prompt]: Epoch 78 / 100: avg data time: 5.62e-02, avg batch time: 0.4998, average train loss: 2.1970
[09/26 15:04:46 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1670, average loss: 2.1977
[09/26 15:04:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 15:04:46 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 15:04:53 visual_prompt]: Epoch 79 / 100: avg data time: 5.51e-02, avg batch time: 0.4981, average train loss: 2.1974
[09/26 15:04:54 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1673, average loss: 2.1904
[09/26 15:04:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 58.00	
[09/26 15:04:54 visual_prompt]: Best epoch 79: best metric: 0.150
[09/26 15:04:54 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 15:05:01 visual_prompt]: Epoch 80 / 100: avg data time: 4.88e-02, avg batch time: 0.4923, average train loss: 2.1868
[09/26 15:05:03 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1675, average loss: 2.1816
[09/26 15:05:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 60.50	
[09/26 15:05:03 visual_prompt]: Best epoch 80: best metric: 0.155
[09/26 15:05:03 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 15:05:09 visual_prompt]: Epoch 81 / 100: avg data time: 4.90e-02, avg batch time: 0.4939, average train loss: 2.2005
[09/26 15:05:11 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1669, average loss: 2.2085
[09/26 15:05:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.50	top5: 59.50	
[09/26 15:05:11 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 15:05:18 visual_prompt]: Epoch 82 / 100: avg data time: 4.79e-02, avg batch time: 0.4921, average train loss: 2.1970
[09/26 15:05:19 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1668, average loss: 2.2017
[09/26 15:05:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 15:05:19 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 15:05:26 visual_prompt]: Epoch 83 / 100: avg data time: 5.93e-02, avg batch time: 0.5044, average train loss: 2.2010
[09/26 15:05:28 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1672, average loss: 2.2161
[09/26 15:05:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 15:05:28 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 15:05:35 visual_prompt]: Epoch 84 / 100: avg data time: 5.17e-02, avg batch time: 0.4942, average train loss: 2.2059
[09/26 15:05:36 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1671, average loss: 2.1940
[09/26 15:05:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 15:05:36 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 15:05:43 visual_prompt]: Epoch 85 / 100: avg data time: 5.93e-02, avg batch time: 0.5019, average train loss: 2.1983
[09/26 15:05:45 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1671, average loss: 2.1908
[09/26 15:05:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 15:05:45 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 15:05:52 visual_prompt]: Epoch 86 / 100: avg data time: 5.18e-02, avg batch time: 0.4958, average train loss: 2.1971
[09/26 15:05:53 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1669, average loss: 2.1998
[09/26 15:05:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.00	
[09/26 15:05:53 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 15:06:00 visual_prompt]: Epoch 87 / 100: avg data time: 5.99e-02, avg batch time: 0.5020, average train loss: 2.1931
[09/26 15:06:02 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1670, average loss: 2.1992
[09/26 15:06:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 15:06:02 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 15:06:08 visual_prompt]: Epoch 88 / 100: avg data time: 5.40e-02, avg batch time: 0.4960, average train loss: 2.1899
[09/26 15:06:10 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1673, average loss: 2.2100
[09/26 15:06:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 15:06:10 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 15:06:17 visual_prompt]: Epoch 89 / 100: avg data time: 4.61e-02, avg batch time: 0.4909, average train loss: 2.1991
[09/26 15:06:18 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1672, average loss: 2.2082
[09/26 15:06:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 15:06:18 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 15:06:25 visual_prompt]: Epoch 90 / 100: avg data time: 5.02e-02, avg batch time: 0.4930, average train loss: 2.1933
[09/26 15:06:27 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1670, average loss: 2.1922
[09/26 15:06:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 15:06:27 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 15:06:34 visual_prompt]: Epoch 91 / 100: avg data time: 5.71e-02, avg batch time: 0.5010, average train loss: 2.1800
[09/26 15:06:35 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.1671, average loss: 2.1448
[09/26 15:06:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 71.50	
[09/26 15:06:35 visual_prompt]: Best epoch 91: best metric: 0.165
[09/26 15:06:35 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 15:06:42 visual_prompt]: Epoch 92 / 100: avg data time: 6.22e-02, avg batch time: 0.5049, average train loss: 2.1701
[09/26 15:06:44 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1671, average loss: 2.2452
[09/26 15:06:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 15:06:44 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 15:06:51 visual_prompt]: Epoch 93 / 100: avg data time: 4.93e-02, avg batch time: 0.4940, average train loss: 2.1922
[09/26 15:06:52 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1669, average loss: 2.1661
[09/26 15:06:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 63.00	
[09/26 15:06:52 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 15:06:59 visual_prompt]: Epoch 94 / 100: avg data time: 5.43e-02, avg batch time: 0.4974, average train loss: 2.1531
[09/26 15:07:01 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1669, average loss: 2.0758
[09/26 15:07:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 76.00	
[09/26 15:07:01 visual_prompt]: Best epoch 94: best metric: 0.210
[09/26 15:07:01 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 15:07:07 visual_prompt]: Epoch 95 / 100: avg data time: 5.15e-02, avg batch time: 0.4947, average train loss: 2.0664
[09/26 15:07:09 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1671, average loss: 2.0187
[09/26 15:07:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 77.00	
[09/26 15:07:09 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 15:07:16 visual_prompt]: Epoch 96 / 100: avg data time: 6.64e-02, avg batch time: 0.5092, average train loss: 2.0398
[09/26 15:07:17 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1672, average loss: 1.9940
[09/26 15:07:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 81.00	
[09/26 15:07:17 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 15:07:24 visual_prompt]: Epoch 97 / 100: avg data time: 5.00e-02, avg batch time: 0.4962, average train loss: 1.9753
[09/26 15:07:26 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1670, average loss: 1.9564
[09/26 15:07:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 80.50	
[09/26 15:07:26 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 15:07:33 visual_prompt]: Epoch 98 / 100: avg data time: 5.13e-02, avg batch time: 0.4959, average train loss: 1.9258
[09/26 15:07:34 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1672, average loss: 1.9913
[09/26 15:07:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 81.00	
[09/26 15:07:34 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 15:07:41 visual_prompt]: Epoch 99 / 100: avg data time: 5.05e-02, avg batch time: 0.4944, average train loss: 1.8936
[09/26 15:07:43 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1671, average loss: 1.9328
[09/26 15:07:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 83.00	
[09/26 15:07:43 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 15:07:49 visual_prompt]: Epoch 100 / 100: avg data time: 5.03e-02, avg batch time: 0.4946, average train loss: 1.8638
[09/26 15:07:51 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1672, average loss: 1.8927
[09/26 15:07:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 84.50	
[09/26 15:07:51 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 15:07:51 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 15:07:51 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 15:07:51 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 15:07:51 visual_prompt]: Training with config:
[09/26 15:07:51 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 15:07:51 visual_prompt]: Loading training data...
[09/26 15:07:51 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 15:07:52 visual_prompt]: Number of images: 800
[09/26 15:07:52 visual_prompt]: Number of classes: 9 / 9
[09/26 15:07:52 visual_prompt]: Loading validation data...
[09/26 15:07:52 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 15:07:52 visual_prompt]: Number of images: 200
[09/26 15:07:52 visual_prompt]: Number of classes: 9 / 9
[09/26 15:07:52 visual_prompt]: Constructing models...
[09/26 15:07:55 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 15:07:55 visual_prompt]: tuned percent:0.542
[09/26 15:07:55 visual_prompt]: Device used for model: 0
[09/26 15:07:55 visual_prompt]: Setting up Evaluator...
[09/26 15:07:55 visual_prompt]: Setting up Trainer...
[09/26 15:07:55 visual_prompt]: 	Setting up the optimizer...
[09/26 15:07:55 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 15:08:02 visual_prompt]: Epoch 1 / 100: avg data time: 4.64e-02, avg batch time: 0.4924, average train loss: 2.8671
[09/26 15:08:03 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1669, average loss: 2.9516
[09/26 15:08:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 15:08:03 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 15:08:03 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 15:08:10 visual_prompt]: Epoch 2 / 100: avg data time: 4.61e-02, avg batch time: 0.4888, average train loss: 3.0114
[09/26 15:08:11 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1670, average loss: 2.6505
[09/26 15:08:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 49.50	
[09/26 15:08:11 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 15:08:18 visual_prompt]: Epoch 3 / 100: avg data time: 4.85e-02, avg batch time: 0.4936, average train loss: 2.3025
[09/26 15:08:20 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1671, average loss: 2.2273
[09/26 15:08:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 57.50	
[09/26 15:08:20 visual_prompt]: Best epoch 3: best metric: 0.145
[09/26 15:08:20 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 15:08:27 visual_prompt]: Epoch 4 / 100: avg data time: 5.62e-02, avg batch time: 0.4982, average train loss: 2.2477
[09/26 15:08:28 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1668, average loss: 2.2580
[09/26 15:08:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.50	
[09/26 15:08:28 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 15:08:35 visual_prompt]: Epoch 5 / 100: avg data time: 4.46e-02, avg batch time: 0.4894, average train loss: 2.2540
[09/26 15:08:36 visual_prompt]: Inference (val):avg data time: 3.83e-05, avg batch time: 0.1668, average loss: 2.2378
[09/26 15:08:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.50	top5: 55.50	
[09/26 15:08:36 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 15:08:43 visual_prompt]: Epoch 6 / 100: avg data time: 6.33e-02, avg batch time: 0.5051, average train loss: 2.2254
[09/26 15:08:45 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1671, average loss: 2.2290
[09/26 15:08:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 15:08:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 15:08:52 visual_prompt]: Epoch 7 / 100: avg data time: 4.58e-02, avg batch time: 0.4912, average train loss: 2.2358
[09/26 15:08:53 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1669, average loss: 2.3339
[09/26 15:08:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 58.00	
[09/26 15:08:53 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 15:09:00 visual_prompt]: Epoch 8 / 100: avg data time: 4.95e-02, avg batch time: 0.4940, average train loss: 2.2761
[09/26 15:09:02 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1666, average loss: 2.2014
[09/26 15:09:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 64.00	
[09/26 15:09:02 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 15:09:08 visual_prompt]: Epoch 9 / 100: avg data time: 4.49e-02, avg batch time: 0.4890, average train loss: 2.2711
[09/26 15:09:10 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1668, average loss: 2.1598
[09/26 15:09:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 67.50	
[09/26 15:09:10 visual_prompt]: Best epoch 9: best metric: 0.155
[09/26 15:09:10 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 15:09:16 visual_prompt]: Epoch 10 / 100: avg data time: 5.11e-02, avg batch time: 0.4971, average train loss: 2.1812
[09/26 15:09:18 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1673, average loss: 2.0629
[09/26 15:09:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 74.00	
[09/26 15:09:18 visual_prompt]: Best epoch 10: best metric: 0.195
[09/26 15:09:18 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 15:09:25 visual_prompt]: Epoch 11 / 100: avg data time: 5.42e-02, avg batch time: 0.4973, average train loss: 2.3163
[09/26 15:09:26 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1672, average loss: 2.2610
[09/26 15:09:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 15:09:26 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 15:09:33 visual_prompt]: Epoch 12 / 100: avg data time: 5.68e-02, avg batch time: 0.5006, average train loss: 2.3563
[09/26 15:09:35 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1673, average loss: 2.2835
[09/26 15:09:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 54.50	
[09/26 15:09:35 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 15:09:42 visual_prompt]: Epoch 13 / 100: avg data time: 4.58e-02, avg batch time: 0.4899, average train loss: 2.2209
[09/26 15:09:43 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1670, average loss: 2.2122
[09/26 15:09:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 67.50	
[09/26 15:09:43 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 15:09:50 visual_prompt]: Epoch 14 / 100: avg data time: 5.15e-02, avg batch time: 0.4962, average train loss: 2.4403
[09/26 15:09:51 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1672, average loss: 2.3428
[09/26 15:09:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 57.50	
[09/26 15:09:51 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 15:09:58 visual_prompt]: Epoch 15 / 100: avg data time: 5.47e-02, avg batch time: 0.4984, average train loss: 2.4207
[09/26 15:10:00 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1671, average loss: 2.6280
[09/26 15:10:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.00	
[09/26 15:10:00 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 15:10:07 visual_prompt]: Epoch 16 / 100: avg data time: 5.26e-02, avg batch time: 0.4955, average train loss: 2.7224
[09/26 15:10:08 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1670, average loss: 2.4506
[09/26 15:10:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 52.00	
[09/26 15:10:08 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 15:10:15 visual_prompt]: Epoch 17 / 100: avg data time: 4.59e-02, avg batch time: 0.4915, average train loss: 2.3632
[09/26 15:10:16 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1674, average loss: 2.3598
[09/26 15:10:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 60.50	
[09/26 15:10:16 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 15:10:23 visual_prompt]: Epoch 18 / 100: avg data time: 5.06e-02, avg batch time: 0.4963, average train loss: 2.3633
[09/26 15:10:25 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1671, average loss: 2.4003
[09/26 15:10:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 15:10:25 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 15:10:32 visual_prompt]: Epoch 19 / 100: avg data time: 5.62e-02, avg batch time: 0.5003, average train loss: 2.3786
[09/26 15:10:33 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1672, average loss: 2.3614
[09/26 15:10:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 53.50	
[09/26 15:10:33 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 15:10:40 visual_prompt]: Epoch 20 / 100: avg data time: 4.52e-02, avg batch time: 0.4907, average train loss: 2.3432
[09/26 15:10:42 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1670, average loss: 2.4334
[09/26 15:10:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 15:10:42 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 15:10:48 visual_prompt]: Epoch 21 / 100: avg data time: 5.81e-02, avg batch time: 0.5019, average train loss: 2.3734
[09/26 15:10:50 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1668, average loss: 2.2758
[09/26 15:10:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 15:10:50 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 15:10:57 visual_prompt]: Epoch 22 / 100: avg data time: 5.31e-02, avg batch time: 0.4965, average train loss: 2.2834
[09/26 15:10:58 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1669, average loss: 2.3204
[09/26 15:10:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 60.50	
[09/26 15:10:58 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 15:11:05 visual_prompt]: Epoch 23 / 100: avg data time: 4.75e-02, avg batch time: 0.4901, average train loss: 2.2901
[09/26 15:11:07 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1667, average loss: 2.3161
[09/26 15:11:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 15:11:07 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 15:11:14 visual_prompt]: Epoch 24 / 100: avg data time: 4.99e-02, avg batch time: 0.4931, average train loss: 2.2623
[09/26 15:11:15 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1668, average loss: 2.3132
[09/26 15:11:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 64.00	
[09/26 15:11:15 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 15:11:22 visual_prompt]: Epoch 25 / 100: avg data time: 5.60e-02, avg batch time: 0.4991, average train loss: 2.2409
[09/26 15:11:24 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1670, average loss: 2.2950
[09/26 15:11:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 61.00	
[09/26 15:11:24 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 15:11:31 visual_prompt]: Epoch 26 / 100: avg data time: 6.28e-02, avg batch time: 0.5062, average train loss: 2.2736
[09/26 15:11:32 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1670, average loss: 2.2453
[09/26 15:11:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 15:11:32 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 15:11:39 visual_prompt]: Epoch 27 / 100: avg data time: 5.06e-02, avg batch time: 0.4945, average train loss: 2.3372
[09/26 15:11:41 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1670, average loss: 2.3194
[09/26 15:11:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/26 15:11:41 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 15:11:47 visual_prompt]: Epoch 28 / 100: avg data time: 4.70e-02, avg batch time: 0.4909, average train loss: 2.2880
[09/26 15:11:49 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1674, average loss: 2.3123
[09/26 15:11:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 15:11:49 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 15:11:56 visual_prompt]: Epoch 29 / 100: avg data time: 6.33e-02, avg batch time: 0.5061, average train loss: 2.2503
[09/26 15:11:57 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1668, average loss: 2.1403
[09/26 15:11:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 76.00	
[09/26 15:11:57 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 15:12:04 visual_prompt]: Epoch 30 / 100: avg data time: 4.80e-02, avg batch time: 0.4906, average train loss: 2.1916
[09/26 15:12:06 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1668, average loss: 2.1427
[09/26 15:12:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 67.50	
[09/26 15:12:06 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 15:12:13 visual_prompt]: Epoch 31 / 100: avg data time: 5.39e-02, avg batch time: 0.4983, average train loss: 2.1469
[09/26 15:12:14 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1668, average loss: 2.6083
[09/26 15:12:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.50	
[09/26 15:12:14 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 15:12:21 visual_prompt]: Epoch 32 / 100: avg data time: 5.18e-02, avg batch time: 0.4949, average train loss: 2.2766
[09/26 15:12:23 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1670, average loss: 2.4736
[09/26 15:12:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 50.50	
[09/26 15:12:23 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 15:12:29 visual_prompt]: Epoch 33 / 100: avg data time: 5.03e-02, avg batch time: 0.4937, average train loss: 2.3060
[09/26 15:12:31 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1673, average loss: 2.3391
[09/26 15:12:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 15:12:31 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 15:12:38 visual_prompt]: Epoch 34 / 100: avg data time: 4.90e-02, avg batch time: 0.4918, average train loss: 2.3007
[09/26 15:12:39 visual_prompt]: Inference (val):avg data time: 4.45e-05, avg batch time: 0.1670, average loss: 2.4931
[09/26 15:12:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 15:12:39 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 15:12:46 visual_prompt]: Epoch 35 / 100: avg data time: 4.74e-02, avg batch time: 0.4927, average train loss: 2.3251
[09/26 15:12:48 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1671, average loss: 2.2311
[09/26 15:12:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 55.50	
[09/26 15:12:48 visual_prompt]: Best epoch 35: best metric: 0.215
[09/26 15:12:48 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 15:12:54 visual_prompt]: Epoch 36 / 100: avg data time: 4.88e-02, avg batch time: 0.4923, average train loss: 2.2089
[09/26 15:12:56 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1673, average loss: 2.1492
[09/26 15:12:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 64.50	
[09/26 15:12:56 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 15:13:03 visual_prompt]: Epoch 37 / 100: avg data time: 5.41e-02, avg batch time: 0.4966, average train loss: 2.2297
[09/26 15:13:04 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1670, average loss: 2.4014
[09/26 15:13:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 56.50	
[09/26 15:13:04 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 15:13:11 visual_prompt]: Epoch 38 / 100: avg data time: 5.56e-02, avg batch time: 0.4981, average train loss: 2.2549
[09/26 15:13:13 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1672, average loss: 2.1480
[09/26 15:13:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 68.00	
[09/26 15:13:13 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 15:13:19 visual_prompt]: Epoch 39 / 100: avg data time: 5.04e-02, avg batch time: 0.4938, average train loss: 2.1110
[09/26 15:13:21 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1667, average loss: 2.0461
[09/26 15:13:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 76.50	
[09/26 15:13:21 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 15:13:28 visual_prompt]: Epoch 40 / 100: avg data time: 4.92e-02, avg batch time: 0.4938, average train loss: 2.0473
[09/26 15:13:29 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1669, average loss: 2.0653
[09/26 15:13:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 73.50	
[09/26 15:13:29 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 15:13:36 visual_prompt]: Epoch 41 / 100: avg data time: 5.71e-02, avg batch time: 0.4999, average train loss: 2.2829
[09/26 15:13:38 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1670, average loss: 2.4256
[09/26 15:13:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 15:13:38 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 15:13:45 visual_prompt]: Epoch 42 / 100: avg data time: 5.04e-02, avg batch time: 0.4938, average train loss: 2.3516
[09/26 15:13:46 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1669, average loss: 2.3329
[09/26 15:13:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 60.50	
[09/26 15:13:46 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 15:13:53 visual_prompt]: Epoch 43 / 100: avg data time: 4.74e-02, avg batch time: 0.4918, average train loss: 2.1920
[09/26 15:13:55 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1673, average loss: 2.1960
[09/26 15:13:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 67.00	
[09/26 15:13:55 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 15:14:02 visual_prompt]: Epoch 44 / 100: avg data time: 6.11e-02, avg batch time: 0.5041, average train loss: 2.1428
[09/26 15:14:03 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1669, average loss: 2.0936
[09/26 15:14:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 73.00	
[09/26 15:14:03 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 15:14:10 visual_prompt]: Epoch 45 / 100: avg data time: 5.28e-02, avg batch time: 0.4954, average train loss: 2.0581
[09/26 15:14:12 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1670, average loss: 2.0852
[09/26 15:14:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 78.50	
[09/26 15:14:12 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 15:14:18 visual_prompt]: Epoch 46 / 100: avg data time: 5.51e-02, avg batch time: 0.4997, average train loss: 2.2156
[09/26 15:14:20 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1666, average loss: 2.2970
[09/26 15:14:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.50	
[09/26 15:14:20 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 15:14:27 visual_prompt]: Epoch 47 / 100: avg data time: 4.75e-02, avg batch time: 0.4924, average train loss: 2.1409
[09/26 15:14:28 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1674, average loss: 2.1102
[09/26 15:14:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 68.50	
[09/26 15:14:28 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 15:14:35 visual_prompt]: Epoch 48 / 100: avg data time: 4.89e-02, avg batch time: 0.4921, average train loss: 2.0868
[09/26 15:14:37 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1672, average loss: 1.9727
[09/26 15:14:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 83.00	
[09/26 15:14:37 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 15:14:44 visual_prompt]: Epoch 49 / 100: avg data time: 5.27e-02, avg batch time: 0.4964, average train loss: 2.1078
[09/26 15:14:45 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1672, average loss: 2.1282
[09/26 15:14:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 71.50	
[09/26 15:14:45 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 15:14:52 visual_prompt]: Epoch 50 / 100: avg data time: 5.32e-02, avg batch time: 0.4974, average train loss: 1.9921
[09/26 15:14:54 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1671, average loss: 2.0710
[09/26 15:14:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 74.50	
[09/26 15:14:54 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 15:15:00 visual_prompt]: Epoch 51 / 100: avg data time: 5.64e-02, avg batch time: 0.4990, average train loss: 2.2309
[09/26 15:15:02 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1668, average loss: 2.1212
[09/26 15:15:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 72.50	
[09/26 15:15:02 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 15:15:09 visual_prompt]: Epoch 52 / 100: avg data time: 5.79e-02, avg batch time: 0.5006, average train loss: 2.1053
[09/26 15:15:10 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1672, average loss: 2.1292
[09/26 15:15:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 72.00	
[09/26 15:15:10 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 15:15:17 visual_prompt]: Epoch 53 / 100: avg data time: 4.73e-02, avg batch time: 0.4918, average train loss: 2.0741
[09/26 15:15:19 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1670, average loss: 2.1015
[09/26 15:15:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 75.00	
[09/26 15:15:19 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 15:15:26 visual_prompt]: Epoch 54 / 100: avg data time: 4.95e-02, avg batch time: 0.4938, average train loss: 1.9764
[09/26 15:15:27 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1670, average loss: 2.0678
[09/26 15:15:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 79.00	
[09/26 15:15:27 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 15:15:34 visual_prompt]: Epoch 55 / 100: avg data time: 5.05e-02, avg batch time: 0.4949, average train loss: 2.0176
[09/26 15:15:36 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1670, average loss: 2.3238
[09/26 15:15:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 60.50	
[09/26 15:15:36 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 15:15:42 visual_prompt]: Epoch 56 / 100: avg data time: 6.04e-02, avg batch time: 0.5038, average train loss: 2.0347
[09/26 15:15:44 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1668, average loss: 1.9735
[09/26 15:15:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 81.00	
[09/26 15:15:44 visual_prompt]: Best epoch 56: best metric: 0.230
[09/26 15:15:44 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 15:15:51 visual_prompt]: Epoch 57 / 100: avg data time: 5.35e-02, avg batch time: 0.4956, average train loss: 1.9012
[09/26 15:15:52 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1673, average loss: 2.4923
[09/26 15:15:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 64.00	
[09/26 15:15:52 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 15:15:59 visual_prompt]: Epoch 58 / 100: avg data time: 5.68e-02, avg batch time: 0.5010, average train loss: 1.9787
[09/26 15:16:01 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1671, average loss: 2.0398
[09/26 15:16:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 76.50	
[09/26 15:16:01 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 15:16:08 visual_prompt]: Epoch 59 / 100: avg data time: 5.15e-02, avg batch time: 0.4949, average train loss: 1.9678
[09/26 15:16:09 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1670, average loss: 2.2884
[09/26 15:16:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 65.50	
[09/26 15:16:09 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 15:16:16 visual_prompt]: Epoch 60 / 100: avg data time: 6.52e-02, avg batch time: 0.5084, average train loss: 2.0553
[09/26 15:16:18 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1671, average loss: 2.1548
[09/26 15:16:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 68.00	
[09/26 15:16:18 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 15:16:25 visual_prompt]: Epoch 61 / 100: avg data time: 6.48e-02, avg batch time: 0.5071, average train loss: 1.9424
[09/26 15:16:26 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1671, average loss: 1.9866
[09/26 15:16:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 83.00	
[09/26 15:16:26 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 15:16:33 visual_prompt]: Epoch 62 / 100: avg data time: 4.50e-02, avg batch time: 0.4881, average train loss: 1.8725
[09/26 15:16:35 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1669, average loss: 2.0349
[09/26 15:16:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 82.00	
[09/26 15:16:35 visual_prompt]: Best epoch 62: best metric: 0.245
[09/26 15:16:35 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 15:16:41 visual_prompt]: Epoch 63 / 100: avg data time: 5.95e-02, avg batch time: 0.5015, average train loss: 1.8648
[09/26 15:16:43 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1670, average loss: 1.9864
[09/26 15:16:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 76.50	
[09/26 15:16:43 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 15:16:50 visual_prompt]: Epoch 64 / 100: avg data time: 4.97e-02, avg batch time: 0.4929, average train loss: 1.8744
[09/26 15:16:51 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1669, average loss: 1.9615
[09/26 15:16:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 82.50	
[09/26 15:16:51 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 15:16:58 visual_prompt]: Epoch 65 / 100: avg data time: 4.83e-02, avg batch time: 0.4920, average train loss: 1.8192
[09/26 15:17:00 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1670, average loss: 1.8487
[09/26 15:17:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 83.50	
[09/26 15:17:00 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 15:17:07 visual_prompt]: Epoch 66 / 100: avg data time: 6.11e-02, avg batch time: 0.5046, average train loss: 1.7773
[09/26 15:17:08 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1667, average loss: 2.0068
[09/26 15:17:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 80.00	
[09/26 15:17:08 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 15:17:15 visual_prompt]: Epoch 67 / 100: avg data time: 4.86e-02, avg batch time: 0.4919, average train loss: 1.7697
[09/26 15:17:17 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1672, average loss: 2.0331
[09/26 15:17:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 76.50	
[09/26 15:17:17 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 15:17:23 visual_prompt]: Epoch 68 / 100: avg data time: 4.75e-02, avg batch time: 0.4907, average train loss: 1.7860
[09/26 15:17:25 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1674, average loss: 2.0208
[09/26 15:17:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 81.50	
[09/26 15:17:25 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 15:17:32 visual_prompt]: Epoch 69 / 100: avg data time: 5.34e-02, avg batch time: 0.4974, average train loss: 1.7312
[09/26 15:17:33 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1669, average loss: 2.0337
[09/26 15:17:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 80.50	
[09/26 15:17:33 visual_prompt]: Best epoch 69: best metric: 0.270
[09/26 15:17:33 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 15:17:40 visual_prompt]: Epoch 70 / 100: avg data time: 5.62e-02, avg batch time: 0.5007, average train loss: 1.7816
[09/26 15:17:42 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1670, average loss: 2.0326
[09/26 15:17:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 82.50	
[09/26 15:17:42 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 15:17:48 visual_prompt]: Epoch 71 / 100: avg data time: 4.67e-02, avg batch time: 0.4912, average train loss: 1.6943
[09/26 15:17:50 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1671, average loss: 1.9009
[09/26 15:17:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 86.50	
[09/26 15:17:50 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 15:17:57 visual_prompt]: Epoch 72 / 100: avg data time: 6.07e-02, avg batch time: 0.5043, average train loss: 1.6915
[09/26 15:17:58 visual_prompt]: Inference (val):avg data time: 3.93e-05, avg batch time: 0.1672, average loss: 1.9202
[09/26 15:17:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 84.00	
[09/26 15:17:58 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 15:18:05 visual_prompt]: Epoch 73 / 100: avg data time: 5.02e-02, avg batch time: 0.4935, average train loss: 1.6523
[09/26 15:18:07 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1671, average loss: 1.9422
[09/26 15:18:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 82.00	
[09/26 15:18:07 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 15:18:13 visual_prompt]: Epoch 74 / 100: avg data time: 5.79e-02, avg batch time: 0.5008, average train loss: 1.7703
[09/26 15:18:15 visual_prompt]: Inference (val):avg data time: 3.97e-05, avg batch time: 0.1672, average loss: 2.0671
[09/26 15:18:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 78.00	
[09/26 15:18:15 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 15:18:22 visual_prompt]: Epoch 75 / 100: avg data time: 5.34e-02, avg batch time: 0.4966, average train loss: 1.7557
[09/26 15:18:24 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1670, average loss: 1.9998
[09/26 15:18:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 83.50	
[09/26 15:18:24 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 15:18:30 visual_prompt]: Epoch 76 / 100: avg data time: 5.66e-02, avg batch time: 0.4995, average train loss: 1.6408
[09/26 15:18:32 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1670, average loss: 2.1201
[09/26 15:18:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 82.00	
[09/26 15:18:32 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 15:18:39 visual_prompt]: Epoch 77 / 100: avg data time: 4.83e-02, avg batch time: 0.4943, average train loss: 1.5861
[09/26 15:18:40 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1669, average loss: 2.0361
[09/26 15:18:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 84.00	
[09/26 15:18:40 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 15:18:47 visual_prompt]: Epoch 78 / 100: avg data time: 5.08e-02, avg batch time: 0.4966, average train loss: 1.6438
[09/26 15:18:49 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1671, average loss: 1.8788
[09/26 15:18:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 83.50	
[09/26 15:18:49 visual_prompt]: Best epoch 78: best metric: 0.275
[09/26 15:18:49 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 15:18:56 visual_prompt]: Epoch 79 / 100: avg data time: 5.00e-02, avg batch time: 0.4928, average train loss: 1.5495
[09/26 15:18:57 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1672, average loss: 2.1953
[09/26 15:18:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 83.00	
[09/26 15:18:57 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 15:19:04 visual_prompt]: Epoch 80 / 100: avg data time: 6.32e-02, avg batch time: 0.5072, average train loss: 1.5994
[09/26 15:19:06 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1671, average loss: 2.1575
[09/26 15:19:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 78.00	
[09/26 15:19:06 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 15:19:12 visual_prompt]: Epoch 81 / 100: avg data time: 4.64e-02, avg batch time: 0.4908, average train loss: 1.4435
[09/26 15:19:14 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1671, average loss: 2.0410
[09/26 15:19:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 83.50	
[09/26 15:19:14 visual_prompt]: Best epoch 81: best metric: 0.300
[09/26 15:19:14 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 15:19:21 visual_prompt]: Epoch 82 / 100: avg data time: 5.65e-02, avg batch time: 0.5000, average train loss: 1.4276
[09/26 15:19:22 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1671, average loss: 2.2495
[09/26 15:19:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 83.00	
[09/26 15:19:22 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 15:19:29 visual_prompt]: Epoch 83 / 100: avg data time: 4.74e-02, avg batch time: 0.4913, average train loss: 1.3484
[09/26 15:19:31 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1671, average loss: 2.3107
[09/26 15:19:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 84.00	
[09/26 15:19:31 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 15:19:37 visual_prompt]: Epoch 84 / 100: avg data time: 5.11e-02, avg batch time: 0.4938, average train loss: 1.3409
[09/26 15:19:39 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1674, average loss: 2.2880
[09/26 15:19:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 84.00	
[09/26 15:19:39 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 15:19:46 visual_prompt]: Epoch 85 / 100: avg data time: 6.37e-02, avg batch time: 0.5074, average train loss: 1.2828
[09/26 15:19:48 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1674, average loss: 2.3285
[09/26 15:19:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 84.50	
[09/26 15:19:48 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 15:19:54 visual_prompt]: Epoch 86 / 100: avg data time: 4.78e-02, avg batch time: 0.4910, average train loss: 1.2440
[09/26 15:19:56 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1673, average loss: 2.3589
[09/26 15:19:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 83.00	
[09/26 15:19:56 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 15:20:03 visual_prompt]: Epoch 87 / 100: avg data time: 4.89e-02, avg batch time: 0.4946, average train loss: 1.1785
[09/26 15:20:04 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1676, average loss: 2.5732
[09/26 15:20:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 83.00	
[09/26 15:20:04 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 15:20:11 visual_prompt]: Epoch 88 / 100: avg data time: 4.77e-02, avg batch time: 0.4930, average train loss: 1.1358
[09/26 15:20:13 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1672, average loss: 2.5539
[09/26 15:20:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 85.00	
[09/26 15:20:13 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 15:20:19 visual_prompt]: Epoch 89 / 100: avg data time: 6.35e-02, avg batch time: 0.5060, average train loss: 1.0927
[09/26 15:20:21 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1671, average loss: 2.6581
[09/26 15:20:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 84.50	
[09/26 15:20:21 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 15:20:28 visual_prompt]: Epoch 90 / 100: avg data time: 6.12e-02, avg batch time: 0.5042, average train loss: 1.0055
[09/26 15:20:29 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1674, average loss: 2.8123
[09/26 15:20:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 83.00	
[09/26 15:20:29 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 15:20:36 visual_prompt]: Epoch 91 / 100: avg data time: 4.75e-02, avg batch time: 0.4959, average train loss: 0.9438
[09/26 15:20:38 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1673, average loss: 2.3863
[09/26 15:20:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 84.00	
[09/26 15:20:38 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 15:20:45 visual_prompt]: Epoch 92 / 100: avg data time: 5.91e-02, avg batch time: 0.5019, average train loss: 0.9277
[09/26 15:20:46 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1670, average loss: 3.1376
[09/26 15:20:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 84.00	
[09/26 15:20:46 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 15:20:53 visual_prompt]: Epoch 93 / 100: avg data time: 5.73e-02, avg batch time: 0.4999, average train loss: 0.9355
[09/26 15:20:55 visual_prompt]: Inference (val):avg data time: 2.06e-05, avg batch time: 0.1669, average loss: 3.1291
[09/26 15:20:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 84.00	
[09/26 15:20:55 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 15:21:01 visual_prompt]: Epoch 94 / 100: avg data time: 4.76e-02, avg batch time: 0.4901, average train loss: 0.8278
[09/26 15:21:03 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1669, average loss: 3.1466
[09/26 15:21:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 83.50	
[09/26 15:21:03 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 15:21:10 visual_prompt]: Epoch 95 / 100: avg data time: 5.44e-02, avg batch time: 0.4982, average train loss: 0.7801
[09/26 15:21:11 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1670, average loss: 3.1673
[09/26 15:21:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 84.50	
[09/26 15:21:11 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 15:21:18 visual_prompt]: Epoch 96 / 100: avg data time: 5.56e-02, avg batch time: 0.5008, average train loss: 0.7090
[09/26 15:21:20 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1669, average loss: 3.1067
[09/26 15:21:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 84.00	
[09/26 15:21:20 visual_prompt]: Best epoch 96: best metric: 0.305
[09/26 15:21:20 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 15:21:27 visual_prompt]: Epoch 97 / 100: avg data time: 5.02e-02, avg batch time: 0.4940, average train loss: 0.6738
[09/26 15:21:28 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1670, average loss: 3.2512
[09/26 15:21:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 84.00	
[09/26 15:21:28 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 15:21:35 visual_prompt]: Epoch 98 / 100: avg data time: 4.71e-02, avg batch time: 0.4916, average train loss: 0.6487
[09/26 15:21:37 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1674, average loss: 3.2705
[09/26 15:21:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 83.50	
[09/26 15:21:37 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 15:21:43 visual_prompt]: Epoch 99 / 100: avg data time: 4.71e-02, avg batch time: 0.4947, average train loss: 0.6420
[09/26 15:21:45 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1671, average loss: 3.2898
[09/26 15:21:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 83.50	
[09/26 15:21:45 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 15:21:52 visual_prompt]: Epoch 100 / 100: avg data time: 4.73e-02, avg batch time: 0.4902, average train loss: 0.6269
[09/26 15:21:53 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1674, average loss: 3.2855
[09/26 15:21:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 83.50	
[09/26 15:21:53 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 15:21:53 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 15:21:53 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 15:21:53 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 15:21:53 visual_prompt]: Training with config:
[09/26 15:21:53 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 15:21:53 visual_prompt]: Loading training data...
[09/26 15:21:53 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 15:21:54 visual_prompt]: Number of images: 800
[09/26 15:21:54 visual_prompt]: Number of classes: 9 / 9
[09/26 15:21:54 visual_prompt]: Loading validation data...
[09/26 15:21:54 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 15:21:55 visual_prompt]: Number of images: 200
[09/26 15:21:55 visual_prompt]: Number of classes: 9 / 9
[09/26 15:21:55 visual_prompt]: Constructing models...
[09/26 15:21:58 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 15:21:58 visual_prompt]: tuned percent:0.542
[09/26 15:21:58 visual_prompt]: Device used for model: 0
[09/26 15:21:58 visual_prompt]: Setting up Evaluator...
[09/26 15:21:58 visual_prompt]: Setting up Trainer...
[09/26 15:21:58 visual_prompt]: 	Setting up the optimizer...
[09/26 15:21:58 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 15:22:04 visual_prompt]: Epoch 1 / 100: avg data time: 4.83e-02, avg batch time: 0.4942, average train loss: 2.8767
[09/26 15:22:06 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.1670, average loss: 2.9516
[09/26 15:22:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 15:22:06 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 15:22:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 15:22:13 visual_prompt]: Epoch 2 / 100: avg data time: 5.05e-02, avg batch time: 0.4949, average train loss: 2.9708
[09/26 15:22:14 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1670, average loss: 2.6087
[09/26 15:22:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 15:22:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 15:22:21 visual_prompt]: Epoch 3 / 100: avg data time: 5.07e-02, avg batch time: 0.4958, average train loss: 2.3468
[09/26 15:22:23 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1671, average loss: 2.2714
[09/26 15:22:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.00	
[09/26 15:22:23 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 15:22:30 visual_prompt]: Epoch 4 / 100: avg data time: 4.96e-02, avg batch time: 0.4946, average train loss: 2.2497
[09/26 15:22:31 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1670, average loss: 2.2405
[09/26 15:22:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 15:22:31 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 15:22:38 visual_prompt]: Epoch 5 / 100: avg data time: 6.05e-02, avg batch time: 0.5042, average train loss: 2.2172
[09/26 15:22:40 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1668, average loss: 2.2528
[09/26 15:22:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 15:22:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 15:22:46 visual_prompt]: Epoch 6 / 100: avg data time: 5.11e-02, avg batch time: 0.4968, average train loss: 2.2368
[09/26 15:22:48 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1669, average loss: 2.4167
[09/26 15:22:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.50	
[09/26 15:22:48 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 15:22:55 visual_prompt]: Epoch 7 / 100: avg data time: 4.87e-02, avg batch time: 0.4923, average train loss: 2.2285
[09/26 15:22:56 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1673, average loss: 2.1331
[09/26 15:22:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 74.50	
[09/26 15:22:56 visual_prompt]: Best epoch 7: best metric: 0.165
[09/26 15:22:56 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 15:23:03 visual_prompt]: Epoch 8 / 100: avg data time: 6.39e-02, avg batch time: 0.5069, average train loss: 2.3098
[09/26 15:23:05 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1674, average loss: 2.3265
[09/26 15:23:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 60.50	
[09/26 15:23:05 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 15:23:12 visual_prompt]: Epoch 9 / 100: avg data time: 6.07e-02, avg batch time: 0.5029, average train loss: 2.2379
[09/26 15:23:13 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1678, average loss: 2.2082
[09/26 15:23:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 60.00	
[09/26 15:23:13 visual_prompt]: Best epoch 9: best metric: 0.180
[09/26 15:23:13 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 15:23:20 visual_prompt]: Epoch 10 / 100: avg data time: 6.45e-02, avg batch time: 0.5069, average train loss: 2.1513
[09/26 15:23:22 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1676, average loss: 2.2072
[09/26 15:23:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 75.50	
[09/26 15:23:22 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 15:23:29 visual_prompt]: Epoch 11 / 100: avg data time: 5.14e-02, avg batch time: 0.4950, average train loss: 2.1547
[09/26 15:23:30 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1675, average loss: 2.1732
[09/26 15:23:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 77.50	
[09/26 15:23:30 visual_prompt]: Best epoch 11: best metric: 0.195
[09/26 15:23:30 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 15:23:37 visual_prompt]: Epoch 12 / 100: avg data time: 6.06e-02, avg batch time: 0.5040, average train loss: 2.0459
[09/26 15:23:39 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1674, average loss: 2.3344
[09/26 15:23:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 64.50	
[09/26 15:23:39 visual_prompt]: Best epoch 12: best metric: 0.200
[09/26 15:23:39 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 15:23:46 visual_prompt]: Epoch 13 / 100: avg data time: 6.49e-02, avg batch time: 0.5086, average train loss: 2.1079
[09/26 15:23:47 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1679, average loss: 2.0016
[09/26 15:23:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 83.50	
[09/26 15:23:47 visual_prompt]: Best epoch 13: best metric: 0.215
[09/26 15:23:47 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 15:23:54 visual_prompt]: Epoch 14 / 100: avg data time: 6.59e-02, avg batch time: 0.5085, average train loss: 1.8543
[09/26 15:23:56 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1678, average loss: 1.9308
[09/26 15:23:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 79.50	
[09/26 15:23:56 visual_prompt]: Best epoch 14: best metric: 0.245
[09/26 15:23:56 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 15:24:02 visual_prompt]: Epoch 15 / 100: avg data time: 5.60e-02, avg batch time: 0.4993, average train loss: 1.8002
[09/26 15:24:04 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1674, average loss: 1.9318
[09/26 15:24:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 84.00	
[09/26 15:24:04 visual_prompt]: Best epoch 15: best metric: 0.275
[09/26 15:24:04 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 15:24:11 visual_prompt]: Epoch 16 / 100: avg data time: 6.30e-02, avg batch time: 0.5072, average train loss: 1.6951
[09/26 15:24:13 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1677, average loss: 1.7531
[09/26 15:24:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 92.50	
[09/26 15:24:13 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 15:24:19 visual_prompt]: Epoch 17 / 100: avg data time: 5.31e-02, avg batch time: 0.4996, average train loss: 1.8089
[09/26 15:24:21 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1671, average loss: 1.9430
[09/26 15:24:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 82.50	
[09/26 15:24:21 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 15:24:28 visual_prompt]: Epoch 18 / 100: avg data time: 5.88e-02, avg batch time: 0.5039, average train loss: 1.7105
[09/26 15:24:29 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1674, average loss: 2.1919
[09/26 15:24:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 83.00	
[09/26 15:24:29 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 15:24:36 visual_prompt]: Epoch 19 / 100: avg data time: 5.10e-02, avg batch time: 0.4954, average train loss: 1.5449
[09/26 15:24:38 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1673, average loss: 1.8197
[09/26 15:24:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 93.50	
[09/26 15:24:38 visual_prompt]: Best epoch 19: best metric: 0.280
[09/26 15:24:38 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 15:24:45 visual_prompt]: Epoch 20 / 100: avg data time: 5.16e-02, avg batch time: 0.4969, average train loss: 1.5287
[09/26 15:24:46 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1673, average loss: 1.8916
[09/26 15:24:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.00	
[09/26 15:24:46 visual_prompt]: Best epoch 20: best metric: 0.295
[09/26 15:24:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 15:24:53 visual_prompt]: Epoch 21 / 100: avg data time: 5.19e-02, avg batch time: 0.4972, average train loss: 1.5130
[09/26 15:24:54 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1673, average loss: 2.2504
[09/26 15:24:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 87.50	
[09/26 15:24:54 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 15:25:01 visual_prompt]: Epoch 22 / 100: avg data time: 5.73e-02, avg batch time: 0.5027, average train loss: 1.5637
[09/26 15:25:03 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1674, average loss: 1.7093
[09/26 15:25:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 96.00	
[09/26 15:25:03 visual_prompt]: Best epoch 22: best metric: 0.300
[09/26 15:25:03 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 15:25:10 visual_prompt]: Epoch 23 / 100: avg data time: 5.81e-02, avg batch time: 0.5024, average train loss: 1.3879
[09/26 15:25:11 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1673, average loss: 1.8332
[09/26 15:25:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 97.00	
[09/26 15:25:11 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 15:25:18 visual_prompt]: Epoch 24 / 100: avg data time: 5.13e-02, avg batch time: 0.4977, average train loss: 1.4380
[09/26 15:25:20 visual_prompt]: Inference (val):avg data time: 1.87e-05, avg batch time: 0.1677, average loss: 1.9555
[09/26 15:25:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 92.50	
[09/26 15:25:20 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 15:25:27 visual_prompt]: Epoch 25 / 100: avg data time: 5.31e-02, avg batch time: 0.4992, average train loss: 1.5194
[09/26 15:25:28 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1676, average loss: 2.0781
[09/26 15:25:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 92.00	
[09/26 15:25:28 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 15:25:35 visual_prompt]: Epoch 26 / 100: avg data time: 5.42e-02, avg batch time: 0.4985, average train loss: 1.3949
[09/26 15:25:36 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1675, average loss: 2.6484
[09/26 15:25:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 82.00	
[09/26 15:25:36 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 15:25:43 visual_prompt]: Epoch 27 / 100: avg data time: 5.44e-02, avg batch time: 0.5002, average train loss: 1.4391
[09/26 15:25:45 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1673, average loss: 1.8493
[09/26 15:25:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 93.00	
[09/26 15:25:45 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 15:25:52 visual_prompt]: Epoch 28 / 100: avg data time: 5.38e-02, avg batch time: 0.4998, average train loss: 1.3355
[09/26 15:25:53 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1674, average loss: 1.7902
[09/26 15:25:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 92.50	
[09/26 15:25:53 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 15:26:00 visual_prompt]: Epoch 29 / 100: avg data time: 5.02e-02, avg batch time: 0.4950, average train loss: 1.1877
[09/26 15:26:02 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1673, average loss: 1.8615
[09/26 15:26:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 94.00	
[09/26 15:26:02 visual_prompt]: Best epoch 29: best metric: 0.320
[09/26 15:26:02 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 15:26:08 visual_prompt]: Epoch 30 / 100: avg data time: 4.96e-02, avg batch time: 0.4956, average train loss: 1.2238
[09/26 15:26:10 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1675, average loss: 1.8972
[09/26 15:26:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 95.00	
[09/26 15:26:10 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 15:26:17 visual_prompt]: Epoch 31 / 100: avg data time: 4.72e-02, avg batch time: 0.4918, average train loss: 1.2033
[09/26 15:26:18 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1675, average loss: 2.0015
[09/26 15:26:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 93.50	
[09/26 15:26:18 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 15:26:25 visual_prompt]: Epoch 32 / 100: avg data time: 5.19e-02, avg batch time: 0.4959, average train loss: 1.1862
[09/26 15:26:27 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1674, average loss: 1.8386
[09/26 15:26:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 93.50	
[09/26 15:26:27 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 15:26:33 visual_prompt]: Epoch 33 / 100: avg data time: 5.78e-02, avg batch time: 0.5019, average train loss: 0.9884
[09/26 15:26:35 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1677, average loss: 1.8252
[09/26 15:26:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.00	
[09/26 15:26:35 visual_prompt]: Best epoch 33: best metric: 0.355
[09/26 15:26:35 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 15:26:42 visual_prompt]: Epoch 34 / 100: avg data time: 5.52e-02, avg batch time: 0.5013, average train loss: 0.9909
[09/26 15:26:43 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1675, average loss: 2.2545
[09/26 15:26:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 92.50	
[09/26 15:26:43 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 15:26:50 visual_prompt]: Epoch 35 / 100: avg data time: 4.70e-02, avg batch time: 0.4927, average train loss: 1.1186
[09/26 15:26:52 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1678, average loss: 1.9366
[09/26 15:26:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 96.50	
[09/26 15:26:52 visual_prompt]: Best epoch 35: best metric: 0.360
[09/26 15:26:52 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 15:26:59 visual_prompt]: Epoch 36 / 100: avg data time: 7.19e-02, avg batch time: 0.5152, average train loss: 0.9838
[09/26 15:27:00 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1672, average loss: 1.8840
[09/26 15:27:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 95.00	
[09/26 15:27:00 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 15:27:07 visual_prompt]: Epoch 37 / 100: avg data time: 5.36e-02, avg batch time: 0.4990, average train loss: 0.9556
[09/26 15:27:09 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1674, average loss: 2.0331
[09/26 15:27:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 15:27:09 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 15:27:16 visual_prompt]: Epoch 38 / 100: avg data time: 5.10e-02, avg batch time: 0.4957, average train loss: 1.0847
[09/26 15:27:17 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1675, average loss: 1.8997
[09/26 15:27:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 93.50	
[09/26 15:27:17 visual_prompt]: Best epoch 38: best metric: 0.390
[09/26 15:27:17 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 15:27:24 visual_prompt]: Epoch 39 / 100: avg data time: 4.66e-02, avg batch time: 0.4913, average train loss: 0.9134
[09/26 15:27:26 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1674, average loss: 1.9093
[09/26 15:27:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 96.00	
[09/26 15:27:26 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 15:27:32 visual_prompt]: Epoch 40 / 100: avg data time: 5.49e-02, avg batch time: 0.4992, average train loss: 0.8579
[09/26 15:27:34 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1676, average loss: 2.1742
[09/26 15:27:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 95.00	
[09/26 15:27:34 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 15:27:41 visual_prompt]: Epoch 41 / 100: avg data time: 5.39e-02, avg batch time: 0.4984, average train loss: 0.8632
[09/26 15:27:42 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1671, average loss: 2.2727
[09/26 15:27:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 91.50	
[09/26 15:27:42 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 15:27:49 visual_prompt]: Epoch 42 / 100: avg data time: 5.08e-02, avg batch time: 0.4938, average train loss: 0.8488
[09/26 15:27:51 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1674, average loss: 2.7150
[09/26 15:27:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 92.00	
[09/26 15:27:51 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 15:27:58 visual_prompt]: Epoch 43 / 100: avg data time: 5.72e-02, avg batch time: 0.5024, average train loss: 0.8372
[09/26 15:27:59 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1673, average loss: 2.6125
[09/26 15:27:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 93.00	
[09/26 15:27:59 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 15:28:06 visual_prompt]: Epoch 44 / 100: avg data time: 5.48e-02, avg batch time: 0.4989, average train loss: 0.9580
[09/26 15:28:08 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1671, average loss: 2.9045
[09/26 15:28:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 96.00	
[09/26 15:28:08 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 15:28:14 visual_prompt]: Epoch 45 / 100: avg data time: 5.16e-02, avg batch time: 0.4969, average train loss: 1.2754
[09/26 15:28:16 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1672, average loss: 1.9605
[09/26 15:28:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 15:28:16 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 15:28:23 visual_prompt]: Epoch 46 / 100: avg data time: 5.66e-02, avg batch time: 0.4992, average train loss: 0.9151
[09/26 15:28:24 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1672, average loss: 2.0414
[09/26 15:28:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 94.50	
[09/26 15:28:24 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 15:28:31 visual_prompt]: Epoch 47 / 100: avg data time: 4.88e-02, avg batch time: 0.4928, average train loss: 0.7660
[09/26 15:28:33 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1677, average loss: 2.1219
[09/26 15:28:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.50	
[09/26 15:28:33 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 15:28:40 visual_prompt]: Epoch 48 / 100: avg data time: 5.71e-02, avg batch time: 0.5020, average train loss: 0.5374
[09/26 15:28:41 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1672, average loss: 2.5312
[09/26 15:28:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 92.50	
[09/26 15:28:41 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 15:28:48 visual_prompt]: Epoch 49 / 100: avg data time: 4.47e-02, avg batch time: 0.4879, average train loss: 0.4980
[09/26 15:28:49 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1673, average loss: 2.8835
[09/26 15:28:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.50	
[09/26 15:28:49 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 15:28:56 visual_prompt]: Epoch 50 / 100: avg data time: 4.51e-02, avg batch time: 0.4901, average train loss: 0.6247
[09/26 15:28:58 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1676, average loss: 2.5903
[09/26 15:28:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 93.00	
[09/26 15:28:58 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 15:29:05 visual_prompt]: Epoch 51 / 100: avg data time: 5.35e-02, avg batch time: 0.4978, average train loss: 0.4941
[09/26 15:29:06 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1672, average loss: 2.4561
[09/26 15:29:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 95.00	
[09/26 15:29:06 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 15:29:13 visual_prompt]: Epoch 52 / 100: avg data time: 5.92e-02, avg batch time: 0.5028, average train loss: 0.7257
[09/26 15:29:15 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1672, average loss: 2.9322
[09/26 15:29:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 89.50	
[09/26 15:29:15 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 15:29:22 visual_prompt]: Epoch 53 / 100: avg data time: 6.45e-02, avg batch time: 0.5095, average train loss: 0.6907
[09/26 15:29:23 visual_prompt]: Inference (val):avg data time: 1.99e-05, avg batch time: 0.1675, average loss: 2.4347
[09/26 15:29:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 93.50	
[09/26 15:29:23 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 15:29:30 visual_prompt]: Epoch 54 / 100: avg data time: 5.70e-02, avg batch time: 0.5028, average train loss: 0.5049
[09/26 15:29:32 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1674, average loss: 2.4508
[09/26 15:29:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 94.00	
[09/26 15:29:32 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 15:29:38 visual_prompt]: Epoch 55 / 100: avg data time: 5.01e-02, avg batch time: 0.4948, average train loss: 0.3521
[09/26 15:29:40 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1677, average loss: 2.8183
[09/26 15:29:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 97.00	
[09/26 15:29:40 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 15:29:47 visual_prompt]: Epoch 56 / 100: avg data time: 5.38e-02, avg batch time: 0.4973, average train loss: 0.2975
[09/26 15:29:48 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1680, average loss: 3.2687
[09/26 15:29:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 94.50	
[09/26 15:29:48 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 15:29:55 visual_prompt]: Epoch 57 / 100: avg data time: 5.06e-02, avg batch time: 0.4952, average train loss: 0.2524
[09/26 15:29:57 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1676, average loss: 3.4755
[09/26 15:29:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 95.50	
[09/26 15:29:57 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 15:30:04 visual_prompt]: Epoch 58 / 100: avg data time: 5.49e-02, avg batch time: 0.5012, average train loss: 0.2930
[09/26 15:30:05 visual_prompt]: Inference (val):avg data time: 1.91e-05, avg batch time: 0.1672, average loss: 4.0424
[09/26 15:30:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 94.00	
[09/26 15:30:05 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 15:30:12 visual_prompt]: Epoch 59 / 100: avg data time: 5.84e-02, avg batch time: 0.5014, average train loss: 0.5019
[09/26 15:30:14 visual_prompt]: Inference (val):avg data time: 2.03e-05, avg batch time: 0.1675, average loss: 3.4299
[09/26 15:30:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 94.50	
[09/26 15:30:14 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 15:30:20 visual_prompt]: Epoch 60 / 100: avg data time: 5.25e-02, avg batch time: 0.4964, average train loss: 0.4336
[09/26 15:30:22 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1677, average loss: 3.3605
[09/26 15:30:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 92.50	
[09/26 15:30:22 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 15:30:29 visual_prompt]: Epoch 61 / 100: avg data time: 4.89e-02, avg batch time: 0.4949, average train loss: 0.5082
[09/26 15:30:30 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1673, average loss: 3.0329
[09/26 15:30:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 93.00	
[09/26 15:30:30 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 15:30:37 visual_prompt]: Epoch 62 / 100: avg data time: 5.89e-02, avg batch time: 0.5035, average train loss: 0.3685
[09/26 15:30:39 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1675, average loss: 3.1598
[09/26 15:30:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 93.00	
[09/26 15:30:39 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 15:30:46 visual_prompt]: Epoch 63 / 100: avg data time: 4.80e-02, avg batch time: 0.4927, average train loss: 0.2277
[09/26 15:30:47 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1672, average loss: 3.4534
[09/26 15:30:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 93.50	
[09/26 15:30:47 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 15:30:54 visual_prompt]: Epoch 64 / 100: avg data time: 5.33e-02, avg batch time: 0.4973, average train loss: 0.1867
[09/26 15:30:56 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1673, average loss: 3.4385
[09/26 15:30:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 95.50	
[09/26 15:30:56 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 15:31:03 visual_prompt]: Epoch 65 / 100: avg data time: 6.09e-02, avg batch time: 0.5058, average train loss: 0.1647
[09/26 15:31:04 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1677, average loss: 3.7676
[09/26 15:31:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 96.00	
[09/26 15:31:04 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 15:31:11 visual_prompt]: Epoch 66 / 100: avg data time: 5.31e-02, avg batch time: 0.4973, average train loss: 0.1344
[09/26 15:31:13 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1675, average loss: 4.1215
[09/26 15:31:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 95.50	
[09/26 15:31:13 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 15:31:20 visual_prompt]: Epoch 67 / 100: avg data time: 6.26e-02, avg batch time: 0.5076, average train loss: 0.1367
[09/26 15:31:21 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1675, average loss: 4.1905
[09/26 15:31:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.50	
[09/26 15:31:21 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 15:31:28 visual_prompt]: Epoch 68 / 100: avg data time: 5.49e-02, avg batch time: 0.4992, average train loss: 0.2220
[09/26 15:31:30 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1676, average loss: 4.0672
[09/26 15:31:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 95.50	
[09/26 15:31:30 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 15:31:36 visual_prompt]: Epoch 69 / 100: avg data time: 4.72e-02, avg batch time: 0.4906, average train loss: 0.1763
[09/26 15:31:38 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1674, average loss: 3.9372
[09/26 15:31:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 95.50	
[09/26 15:31:38 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 15:31:45 visual_prompt]: Epoch 70 / 100: avg data time: 6.08e-02, avg batch time: 0.5043, average train loss: 0.1327
[09/26 15:31:47 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1674, average loss: 4.3745
[09/26 15:31:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 93.50	
[09/26 15:31:47 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 15:31:53 visual_prompt]: Epoch 71 / 100: avg data time: 6.17e-02, avg batch time: 0.5049, average train loss: 0.1018
[09/26 15:31:55 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1672, average loss: 4.3124
[09/26 15:31:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 95.50	
[09/26 15:31:55 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 15:32:02 visual_prompt]: Epoch 72 / 100: avg data time: 5.97e-02, avg batch time: 0.5037, average train loss: 0.1053
[09/26 15:32:04 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1677, average loss: 4.5250
[09/26 15:32:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 96.00	
[09/26 15:32:04 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 15:32:10 visual_prompt]: Epoch 73 / 100: avg data time: 6.14e-02, avg batch time: 0.5047, average train loss: 0.1070
[09/26 15:32:12 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1678, average loss: 4.1263
[09/26 15:32:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 41.00	top5: 95.50	
[09/26 15:32:12 visual_prompt]: Best epoch 73: best metric: 0.410
[09/26 15:32:12 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 15:32:19 visual_prompt]: Epoch 74 / 100: avg data time: 5.68e-02, avg batch time: 0.5011, average train loss: 0.0725
[09/26 15:32:21 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1677, average loss: 4.8467
[09/26 15:32:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 94.00	
[09/26 15:32:21 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 15:32:27 visual_prompt]: Epoch 75 / 100: avg data time: 4.82e-02, avg batch time: 0.4931, average train loss: 0.0774
[09/26 15:32:29 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1678, average loss: 4.6341
[09/26 15:32:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.50	
[09/26 15:32:29 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 15:32:36 visual_prompt]: Epoch 76 / 100: avg data time: 5.97e-02, avg batch time: 0.5040, average train loss: 0.0499
[09/26 15:32:37 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1674, average loss: 4.7555
[09/26 15:32:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 95.50	
[09/26 15:32:37 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 15:32:44 visual_prompt]: Epoch 77 / 100: avg data time: 6.08e-02, avg batch time: 0.5036, average train loss: 0.0463
[09/26 15:32:46 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1678, average loss: 4.8746
[09/26 15:32:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 95.00	
[09/26 15:32:46 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 15:32:53 visual_prompt]: Epoch 78 / 100: avg data time: 4.97e-02, avg batch time: 0.4941, average train loss: 0.0327
[09/26 15:32:54 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1676, average loss: 4.8256
[09/26 15:32:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 96.50	
[09/26 15:32:54 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 15:33:01 visual_prompt]: Epoch 79 / 100: avg data time: 4.62e-02, avg batch time: 0.4910, average train loss: 0.0256
[09/26 15:33:03 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1678, average loss: 5.0759
[09/26 15:33:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 96.50	
[09/26 15:33:03 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 15:33:10 visual_prompt]: Epoch 80 / 100: avg data time: 5.93e-02, avg batch time: 0.5025, average train loss: 0.0196
[09/26 15:33:11 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.1676, average loss: 4.9120
[09/26 15:33:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.50	top5: 96.50	
[09/26 15:33:11 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 15:33:18 visual_prompt]: Epoch 81 / 100: avg data time: 5.09e-02, avg batch time: 0.5005, average train loss: 0.0183
[09/26 15:33:20 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1676, average loss: 5.1736
[09/26 15:33:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 95.50	
[09/26 15:33:20 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 15:33:26 visual_prompt]: Epoch 82 / 100: avg data time: 4.86e-02, avg batch time: 0.4932, average train loss: 0.0193
[09/26 15:33:28 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1675, average loss: 5.1879
[09/26 15:33:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.50	top5: 96.50	
[09/26 15:33:28 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 15:33:35 visual_prompt]: Epoch 83 / 100: avg data time: 5.36e-02, avg batch time: 0.4980, average train loss: 0.0318
[09/26 15:33:36 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1675, average loss: 5.0439
[09/26 15:33:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.50	top5: 96.50	
[09/26 15:33:36 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 15:33:43 visual_prompt]: Epoch 84 / 100: avg data time: 5.26e-02, avg batch time: 0.4980, average train loss: 0.0261
[09/26 15:33:45 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1672, average loss: 5.2865
[09/26 15:33:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 96.00	
[09/26 15:33:45 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 15:33:52 visual_prompt]: Epoch 85 / 100: avg data time: 5.32e-02, avg batch time: 0.4971, average train loss: 0.0134
[09/26 15:33:53 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1677, average loss: 5.3398
[09/26 15:33:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 95.50	
[09/26 15:33:53 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 15:34:00 visual_prompt]: Epoch 86 / 100: avg data time: 5.72e-02, avg batch time: 0.5024, average train loss: 0.0238
[09/26 15:34:02 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1676, average loss: 5.3509
[09/26 15:34:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.50	top5: 96.00	
[09/26 15:34:02 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 15:34:09 visual_prompt]: Epoch 87 / 100: avg data time: 5.80e-02, avg batch time: 0.5007, average train loss: 0.0093
[09/26 15:34:10 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1674, average loss: 5.3457
[09/26 15:34:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.00	top5: 96.00	
[09/26 15:34:10 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 15:34:17 visual_prompt]: Epoch 88 / 100: avg data time: 5.38e-02, avg batch time: 0.4982, average train loss: 0.0093
[09/26 15:34:19 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1673, average loss: 5.3095
[09/26 15:34:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.50	top5: 96.00	
[09/26 15:34:19 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 15:34:25 visual_prompt]: Epoch 89 / 100: avg data time: 4.89e-02, avg batch time: 0.4931, average train loss: 0.0059
[09/26 15:34:27 visual_prompt]: Inference (val):avg data time: 3.89e-05, avg batch time: 0.1675, average loss: 5.3272
[09/26 15:34:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.50	top5: 96.00	
[09/26 15:34:27 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 15:34:34 visual_prompt]: Epoch 90 / 100: avg data time: 5.44e-02, avg batch time: 0.4991, average train loss: 0.0067
[09/26 15:34:35 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1678, average loss: 5.3252
[09/26 15:34:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.50	top5: 96.00	
[09/26 15:34:36 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 15:34:42 visual_prompt]: Epoch 91 / 100: avg data time: 4.76e-02, avg batch time: 0.4923, average train loss: 0.0075
[09/26 15:34:44 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1682, average loss: 5.3178
[09/26 15:34:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.50	top5: 96.00	
[09/26 15:34:44 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 15:34:51 visual_prompt]: Epoch 92 / 100: avg data time: 5.89e-02, avg batch time: 0.5022, average train loss: 0.0082
[09/26 15:34:52 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1674, average loss: 5.3386
[09/26 15:34:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 96.00	
[09/26 15:34:52 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 15:34:59 visual_prompt]: Epoch 93 / 100: avg data time: 6.11e-02, avg batch time: 0.5068, average train loss: 0.0116
[09/26 15:35:01 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1676, average loss: 5.3564
[09/26 15:35:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.00	top5: 96.00	
[09/26 15:35:01 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 15:35:08 visual_prompt]: Epoch 94 / 100: avg data time: 5.10e-02, avg batch time: 0.4955, average train loss: 0.0061
[09/26 15:35:09 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1676, average loss: 5.3530
[09/26 15:35:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.00	top5: 96.00	
[09/26 15:35:09 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 15:35:16 visual_prompt]: Epoch 95 / 100: avg data time: 4.73e-02, avg batch time: 0.4941, average train loss: 0.0066
[09/26 15:35:18 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1674, average loss: 5.3594
[09/26 15:35:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.50	top5: 96.00	
[09/26 15:35:18 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 15:35:24 visual_prompt]: Epoch 96 / 100: avg data time: 4.97e-02, avg batch time: 0.4963, average train loss: 0.0072
[09/26 15:35:26 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1677, average loss: 5.3630
[09/26 15:35:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.50	top5: 96.00	
[09/26 15:35:26 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 15:35:33 visual_prompt]: Epoch 97 / 100: avg data time: 6.30e-02, avg batch time: 0.5063, average train loss: 0.0080
[09/26 15:35:34 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1674, average loss: 5.3670
[09/26 15:35:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.50	top5: 96.00	
[09/26 15:35:34 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 15:35:41 visual_prompt]: Epoch 98 / 100: avg data time: 5.42e-02, avg batch time: 0.4981, average train loss: 0.0055
[09/26 15:35:43 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1677, average loss: 5.3690
[09/26 15:35:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.00	top5: 96.00	
[09/26 15:35:43 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 15:35:50 visual_prompt]: Epoch 99 / 100: avg data time: 6.77e-02, avg batch time: 0.5119, average train loss: 0.0101
[09/26 15:35:51 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1672, average loss: 5.3699
[09/26 15:35:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.00	top5: 96.00	
[09/26 15:35:51 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 15:35:58 visual_prompt]: Epoch 100 / 100: avg data time: 6.20e-02, avg batch time: 0.5055, average train loss: 0.0067
[09/26 15:36:00 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1672, average loss: 5.3704
[09/26 15:36:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.00	top5: 96.00	
[09/26 15:36:00 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 15:36:00 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 15:36:00 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 15:36:00 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 15:36:00 visual_prompt]: Training with config:
[09/26 15:36:00 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr1.0_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 1.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 15:36:00 visual_prompt]: Loading training data...
[09/26 15:36:00 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 15:36:01 visual_prompt]: Number of images: 800
[09/26 15:36:01 visual_prompt]: Number of classes: 9 / 9
[09/26 15:36:01 visual_prompt]: Loading validation data...
[09/26 15:36:01 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 15:36:01 visual_prompt]: Number of images: 200
[09/26 15:36:01 visual_prompt]: Number of classes: 9 / 9
[09/26 15:36:01 visual_prompt]: Constructing models...
[09/26 15:36:04 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 15:36:04 visual_prompt]: tuned percent:0.542
[09/26 15:36:04 visual_prompt]: Device used for model: 0
[09/26 15:36:04 visual_prompt]: Setting up Evaluator...
[09/26 15:36:04 visual_prompt]: Setting up Trainer...
[09/26 15:36:04 visual_prompt]: 	Setting up the optimizer...
[09/26 15:36:04 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 15:36:11 visual_prompt]: Epoch 1 / 100: avg data time: 5.04e-02, avg batch time: 0.4933, average train loss: 2.8621
[09/26 15:36:12 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1667, average loss: 2.9516
[09/26 15:36:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 15:36:12 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 15:36:12 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.1
[09/26 15:36:19 visual_prompt]: Epoch 2 / 100: avg data time: 5.38e-02, avg batch time: 0.4968, average train loss: 2.9754
[09/26 15:36:21 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1669, average loss: 2.6543
[09/26 15:36:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 45.00	
[09/26 15:36:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.2
[09/26 15:36:27 visual_prompt]: Epoch 3 / 100: avg data time: 5.17e-02, avg batch time: 0.4951, average train loss: 2.3734
[09/26 15:36:29 visual_prompt]: Inference (val):avg data time: 2.04e-05, avg batch time: 0.1670, average loss: 2.2689
[09/26 15:36:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 15:36:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.3
[09/26 15:36:36 visual_prompt]: Epoch 4 / 100: avg data time: 5.55e-02, avg batch time: 0.5010, average train loss: 2.2796
[09/26 15:36:38 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1670, average loss: 2.2720
[09/26 15:36:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 15:36:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.4
[09/26 15:36:44 visual_prompt]: Epoch 5 / 100: avg data time: 6.15e-02, avg batch time: 0.5052, average train loss: 2.3079
[09/26 15:36:46 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1670, average loss: 2.4445
[09/26 15:36:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 15:36:46 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.5
[09/26 15:36:53 visual_prompt]: Epoch 6 / 100: avg data time: 5.09e-02, avg batch time: 0.4943, average train loss: 2.2879
[09/26 15:36:54 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1671, average loss: 2.1921
[09/26 15:36:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 61.50	
[09/26 15:36:54 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.6
[09/26 15:37:01 visual_prompt]: Epoch 7 / 100: avg data time: 4.75e-02, avg batch time: 0.4917, average train loss: 2.2564
[09/26 15:37:03 visual_prompt]: Inference (val):avg data time: 2.09e-05, avg batch time: 0.1672, average loss: 2.2202
[09/26 15:37:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 15:37:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.7
[09/26 15:37:10 visual_prompt]: Epoch 8 / 100: avg data time: 5.09e-02, avg batch time: 0.4957, average train loss: 2.2545
[09/26 15:37:11 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1672, average loss: 2.3203
[09/26 15:37:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 64.50	
[09/26 15:37:11 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.8
[09/26 15:37:18 visual_prompt]: Epoch 9 / 100: avg data time: 5.34e-02, avg batch time: 0.4970, average train loss: 2.2696
[09/26 15:37:19 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1672, average loss: 2.1947
[09/26 15:37:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 67.00	
[09/26 15:37:19 visual_prompt]: Best epoch 9: best metric: 0.175
[09/26 15:37:19 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.9
[09/26 15:37:26 visual_prompt]: Epoch 10 / 100: avg data time: 4.80e-02, avg batch time: 0.4924, average train loss: 2.2010
[09/26 15:37:28 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1675, average loss: 2.2754
[09/26 15:37:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 62.00	
[09/26 15:37:28 visual_prompt]: Training 11 / 100 epoch, with learning rate 1.0
[09/26 15:37:35 visual_prompt]: Epoch 11 / 100: avg data time: 5.41e-02, avg batch time: 0.4982, average train loss: 2.2026
[09/26 15:37:36 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1669, average loss: 2.1821
[09/26 15:37:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 73.50	
[09/26 15:37:36 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.9996954135095479
[09/26 15:37:43 visual_prompt]: Epoch 12 / 100: avg data time: 5.36e-02, avg batch time: 0.4971, average train loss: 2.0381
[09/26 15:37:45 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1675, average loss: 2.3198
[09/26 15:37:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 72.00	
[09/26 15:37:45 visual_prompt]: Best epoch 12: best metric: 0.190
[09/26 15:37:45 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.9987820251299121
[09/26 15:37:51 visual_prompt]: Epoch 13 / 100: avg data time: 5.19e-02, avg batch time: 0.4952, average train loss: 2.2056
[09/26 15:37:53 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1674, average loss: 2.2375
[09/26 15:37:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 79.00	
[09/26 15:37:53 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.9972609476841366
[09/26 15:38:00 visual_prompt]: Epoch 14 / 100: avg data time: 5.71e-02, avg batch time: 0.5013, average train loss: 2.0945
[09/26 15:38:01 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1674, average loss: 2.3110
[09/26 15:38:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 77.50	
[09/26 15:38:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.9951340343707852
[09/26 15:38:08 visual_prompt]: Epoch 15 / 100: avg data time: 4.58e-02, avg batch time: 0.4907, average train loss: 1.9746
[09/26 15:38:10 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1671, average loss: 2.1356
[09/26 15:38:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 80.50	
[09/26 15:38:10 visual_prompt]: Best epoch 15: best metric: 0.200
[09/26 15:38:10 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.9924038765061041
[09/26 15:38:16 visual_prompt]: Epoch 16 / 100: avg data time: 4.82e-02, avg batch time: 0.4919, average train loss: 1.9444
[09/26 15:38:18 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1675, average loss: 2.2947
[09/26 15:38:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 73.00	
[09/26 15:38:18 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.9890738003669028
[09/26 15:38:25 visual_prompt]: Epoch 17 / 100: avg data time: 5.00e-02, avg batch time: 0.4951, average train loss: 1.9171
[09/26 15:38:26 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1673, average loss: 2.5576
[09/26 15:38:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 79.00	
[09/26 15:38:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.9851478631379982
[09/26 15:38:33 visual_prompt]: Epoch 18 / 100: avg data time: 4.85e-02, avg batch time: 0.4934, average train loss: 1.9417
[09/26 15:38:34 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1674, average loss: 1.9575
[09/26 15:38:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 87.50	
[09/26 15:38:34 visual_prompt]: Best epoch 18: best metric: 0.225
[09/26 15:38:34 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.9806308479691594
[09/26 15:38:41 visual_prompt]: Epoch 19 / 100: avg data time: 6.06e-02, avg batch time: 0.5040, average train loss: 1.8119
[09/26 15:38:43 visual_prompt]: Inference (val):avg data time: 3.73e-05, avg batch time: 0.1673, average loss: 1.8275
[09/26 15:38:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 90.00	
[09/26 15:38:43 visual_prompt]: Best epoch 19: best metric: 0.275
[09/26 15:38:43 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.9755282581475768
[09/26 15:38:50 visual_prompt]: Epoch 20 / 100: avg data time: 5.00e-02, avg batch time: 0.4945, average train loss: 1.6020
[09/26 15:38:51 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1675, average loss: 1.9027
[09/26 15:38:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 15:38:51 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.9698463103929542
[09/26 15:38:58 visual_prompt]: Epoch 21 / 100: avg data time: 5.81e-02, avg batch time: 0.5028, average train loss: 1.5598
[09/26 15:39:00 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1674, average loss: 1.7685
[09/26 15:39:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 93.00	
[09/26 15:39:00 visual_prompt]: Best epoch 21: best metric: 0.285
[09/26 15:39:00 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.9635919272833937
[09/26 15:39:07 visual_prompt]: Epoch 22 / 100: avg data time: 5.27e-02, avg batch time: 0.4984, average train loss: 1.5366
[09/26 15:39:08 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1679, average loss: 1.8525
[09/26 15:39:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 93.00	
[09/26 15:39:08 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.9567727288213004
[09/26 15:39:15 visual_prompt]: Epoch 23 / 100: avg data time: 5.85e-02, avg batch time: 0.5037, average train loss: 1.4873
[09/26 15:39:17 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1675, average loss: 1.9854
[09/26 15:39:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 91.50	
[09/26 15:39:17 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.9493970231495835
[09/26 15:39:23 visual_prompt]: Epoch 24 / 100: avg data time: 5.00e-02, avg batch time: 0.4964, average train loss: 1.4718
[09/26 15:39:25 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1674, average loss: 1.8885
[09/26 15:39:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 94.50	
[09/26 15:39:25 visual_prompt]: Best epoch 24: best metric: 0.315
[09/26 15:39:25 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.9414737964294635
[09/26 15:39:32 visual_prompt]: Epoch 25 / 100: avg data time: 5.13e-02, avg batch time: 0.4956, average train loss: 1.3365
[09/26 15:39:33 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1673, average loss: 1.7508
[09/26 15:39:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 94.00	
[09/26 15:39:33 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.9330127018922194
[09/26 15:39:40 visual_prompt]: Epoch 26 / 100: avg data time: 5.46e-02, avg batch time: 0.4997, average train loss: 1.3174
[09/26 15:39:42 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1676, average loss: 1.9684
[09/26 15:39:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.50	
[09/26 15:39:42 visual_prompt]: Best epoch 26: best metric: 0.320
[09/26 15:39:42 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.9240240480782129
[09/26 15:39:49 visual_prompt]: Epoch 27 / 100: avg data time: 5.71e-02, avg batch time: 0.5025, average train loss: 1.3428
[09/26 15:39:50 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1679, average loss: 2.1010
[09/26 15:39:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.00	
[09/26 15:39:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.9145187862775208
[09/26 15:39:57 visual_prompt]: Epoch 28 / 100: avg data time: 6.31e-02, avg batch time: 0.5072, average train loss: 1.6206
[09/26 15:39:59 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1676, average loss: 1.8205
[09/26 15:39:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 91.00	
[09/26 15:39:59 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.9045084971874737
[09/26 15:40:05 visual_prompt]: Epoch 29 / 100: avg data time: 4.76e-02, avg batch time: 0.4923, average train loss: 1.3198
[09/26 15:40:07 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1675, average loss: 1.6446
[09/26 15:40:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 94.50	
[09/26 15:40:07 visual_prompt]: Best epoch 29: best metric: 0.340
[09/26 15:40:07 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.8940053768033609
[09/26 15:40:14 visual_prompt]: Epoch 30 / 100: avg data time: 4.80e-02, avg batch time: 0.4943, average train loss: 1.1320
[09/26 15:40:15 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1677, average loss: 1.6776
[09/26 15:40:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 95.00	
[09/26 15:40:15 visual_prompt]: Best epoch 30: best metric: 0.345
[09/26 15:40:15 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.883022221559489
[09/26 15:40:22 visual_prompt]: Epoch 31 / 100: avg data time: 5.08e-02, avg batch time: 0.4954, average train loss: 1.0305
[09/26 15:40:23 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1678, average loss: 1.7478
[09/26 15:40:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 96.00	
[09/26 15:40:23 visual_prompt]: Best epoch 31: best metric: 0.375
[09/26 15:40:23 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.8715724127386971
[09/26 15:40:30 visual_prompt]: Epoch 32 / 100: avg data time: 5.21e-02, avg batch time: 0.4990, average train loss: 1.0408
[09/26 15:40:32 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1677, average loss: 2.0106
[09/26 15:40:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 94.00	
[09/26 15:40:32 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.8596699001693255
[09/26 15:40:39 visual_prompt]: Epoch 33 / 100: avg data time: 4.79e-02, avg batch time: 0.4924, average train loss: 0.9318
[09/26 15:40:40 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1675, average loss: 2.0609
[09/26 15:40:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.00	
[09/26 15:40:40 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.8473291852294986
[09/26 15:40:47 visual_prompt]: Epoch 34 / 100: avg data time: 5.41e-02, avg batch time: 0.4993, average train loss: 1.0247
[09/26 15:40:48 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1675, average loss: 2.5348
[09/26 15:40:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 93.00	
[09/26 15:40:48 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.8345653031794291
[09/26 15:40:55 visual_prompt]: Epoch 35 / 100: avg data time: 5.61e-02, avg batch time: 0.5020, average train loss: 1.1882
[09/26 15:40:57 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1677, average loss: 1.9451
[09/26 15:40:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.00	
[09/26 15:40:57 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.8213938048432696
[09/26 15:41:04 visual_prompt]: Epoch 36 / 100: avg data time: 4.99e-02, avg batch time: 0.4953, average train loss: 0.9936
[09/26 15:41:05 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1674, average loss: 1.8594
[09/26 15:41:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 97.00	
[09/26 15:41:05 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.8078307376628291
[09/26 15:41:12 visual_prompt]: Epoch 37 / 100: avg data time: 4.75e-02, avg batch time: 0.4935, average train loss: 0.8061
[09/26 15:41:14 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1674, average loss: 1.9384
[09/26 15:41:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 96.50	
[09/26 15:41:14 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.7938926261462366
[09/26 15:41:20 visual_prompt]: Epoch 38 / 100: avg data time: 4.87e-02, avg batch time: 0.4948, average train loss: 0.7622
[09/26 15:41:22 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1676, average loss: 1.9897
[09/26 15:41:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 98.00	
[09/26 15:41:22 visual_prompt]: Best epoch 38: best metric: 0.380
[09/26 15:41:22 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.7795964517353734
[09/26 15:41:29 visual_prompt]: Epoch 39 / 100: avg data time: 4.86e-02, avg batch time: 0.4961, average train loss: 0.7890
[09/26 15:41:30 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1678, average loss: 2.3574
[09/26 15:41:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.00	top5: 93.50	
[09/26 15:41:30 visual_prompt]: Best epoch 39: best metric: 0.400
[09/26 15:41:30 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.7649596321166025
[09/26 15:41:37 visual_prompt]: Epoch 40 / 100: avg data time: 4.67e-02, avg batch time: 0.4937, average train loss: 0.6681
[09/26 15:41:39 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1678, average loss: 2.3439
[09/26 15:41:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 96.00	
[09/26 15:41:39 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.75
[09/26 15:41:45 visual_prompt]: Epoch 41 / 100: avg data time: 4.51e-02, avg batch time: 0.4897, average train loss: 0.6611
[09/26 15:41:47 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1679, average loss: 2.4601
[09/26 15:41:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 96.00	
[09/26 15:41:47 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.7347357813929454
[09/26 15:41:54 visual_prompt]: Epoch 42 / 100: avg data time: 5.26e-02, avg batch time: 0.4989, average train loss: 0.6094
[09/26 15:41:55 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1674, average loss: 2.2768
[09/26 15:41:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 95.50	
[09/26 15:41:55 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.7191855733945387
[09/26 15:42:02 visual_prompt]: Epoch 43 / 100: avg data time: 5.56e-02, avg batch time: 0.5014, average train loss: 0.5403
[09/26 15:42:04 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1675, average loss: 2.8469
[09/26 15:42:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 15:42:04 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.7033683215379002
[09/26 15:42:11 visual_prompt]: Epoch 44 / 100: avg data time: 4.86e-02, avg batch time: 0.4927, average train loss: 0.6716
[09/26 15:42:12 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1676, average loss: 2.8330
[09/26 15:42:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 93.50	
[09/26 15:42:12 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.6873032967079561
[09/26 15:42:19 visual_prompt]: Epoch 45 / 100: avg data time: 4.99e-02, avg batch time: 0.4973, average train loss: 0.5535
[09/26 15:42:21 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1678, average loss: 2.5320
[09/26 15:42:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 96.00	
[09/26 15:42:21 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.6710100716628344
[09/26 15:42:27 visual_prompt]: Epoch 46 / 100: avg data time: 5.00e-02, avg batch time: 0.4960, average train loss: 0.4654
[09/26 15:42:29 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1679, average loss: 2.7814
[09/26 15:42:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 95.00	
[09/26 15:42:29 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.6545084971874737
[09/26 15:42:36 visual_prompt]: Epoch 47 / 100: avg data time: 4.30e-02, avg batch time: 0.4881, average train loss: 0.3746
[09/26 15:42:37 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1678, average loss: 3.2146
[09/26 15:42:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 96.00	
[09/26 15:42:37 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.6378186779084996
[09/26 15:42:44 visual_prompt]: Epoch 48 / 100: avg data time: 5.27e-02, avg batch time: 0.4974, average train loss: 0.4397
[09/26 15:42:46 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1678, average loss: 2.9637
[09/26 15:42:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 95.50	
[09/26 15:42:46 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.6209609477998338
[09/26 15:42:53 visual_prompt]: Epoch 49 / 100: avg data time: 6.01e-02, avg batch time: 0.5049, average train loss: 0.4809
[09/26 15:42:54 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1677, average loss: 3.0916
[09/26 15:42:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 95.50	
[09/26 15:42:54 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.6039558454088796
[09/26 15:43:01 visual_prompt]: Epoch 50 / 100: avg data time: 4.78e-02, avg batch time: 0.4930, average train loss: 0.4968
[09/26 15:43:02 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1675, average loss: 2.8146
[09/26 15:43:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 96.00	
[09/26 15:43:02 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.5868240888334653
[09/26 15:43:09 visual_prompt]: Epoch 51 / 100: avg data time: 5.69e-02, avg batch time: 0.5025, average train loss: 0.4824
[09/26 15:43:11 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1676, average loss: 2.7737
[09/26 15:43:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 97.00	
[09/26 15:43:11 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.5695865504800327
[09/26 15:43:18 visual_prompt]: Epoch 52 / 100: avg data time: 5.88e-02, avg batch time: 0.5030, average train loss: 0.4136
[09/26 15:43:19 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1675, average loss: 3.1447
[09/26 15:43:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.00	
[09/26 15:43:19 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.5522642316338268
[09/26 15:43:26 visual_prompt]: Epoch 53 / 100: avg data time: 4.64e-02, avg batch time: 0.4923, average train loss: 0.3284
[09/26 15:43:28 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1677, average loss: 3.1284
[09/26 15:43:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 96.50	
[09/26 15:43:28 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.5348782368720626
[09/26 15:43:34 visual_prompt]: Epoch 54 / 100: avg data time: 5.84e-02, avg batch time: 0.5023, average train loss: 0.2501
[09/26 15:43:36 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1676, average loss: 3.3966
[09/26 15:43:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 97.00	
[09/26 15:43:36 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.5174497483512506
[09/26 15:43:43 visual_prompt]: Epoch 55 / 100: avg data time: 4.74e-02, avg batch time: 0.4938, average train loss: 0.2355
[09/26 15:43:44 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1677, average loss: 4.0035
[09/26 15:43:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 96.00	
[09/26 15:43:44 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.5
[09/26 15:43:51 visual_prompt]: Epoch 56 / 100: avg data time: 5.03e-02, avg batch time: 0.4949, average train loss: 0.2997
[09/26 15:43:53 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1678, average loss: 4.0110
[09/26 15:43:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 15:43:53 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.48255025164874965
[09/26 15:44:00 visual_prompt]: Epoch 57 / 100: avg data time: 5.14e-02, avg batch time: 0.4960, average train loss: 0.5217
[09/26 15:44:01 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1677, average loss: 3.6104
[09/26 15:44:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 96.00	
[09/26 15:44:01 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.46512176312793735
[09/26 15:44:08 visual_prompt]: Epoch 58 / 100: avg data time: 4.49e-02, avg batch time: 0.4903, average train loss: 0.5152
[09/26 15:44:09 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1675, average loss: 3.3453
[09/26 15:44:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 95.50	
[09/26 15:44:09 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.44773576836617335
[09/26 15:44:16 visual_prompt]: Epoch 59 / 100: avg data time: 5.11e-02, avg batch time: 0.4962, average train loss: 0.4049
[09/26 15:44:18 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1672, average loss: 3.2190
[09/26 15:44:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 96.00	
[09/26 15:44:18 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.4304134495199674
[09/26 15:44:25 visual_prompt]: Epoch 60 / 100: avg data time: 5.07e-02, avg batch time: 0.4973, average train loss: 0.3236
[09/26 15:44:26 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1677, average loss: 3.4971
[09/26 15:44:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 95.00	
[09/26 15:44:26 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.41317591116653485
[09/26 15:44:33 visual_prompt]: Epoch 61 / 100: avg data time: 5.67e-02, avg batch time: 0.5017, average train loss: 0.2208
[09/26 15:44:35 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1674, average loss: 3.4324
[09/26 15:44:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 15:44:35 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.3960441545911204
[09/26 15:44:41 visual_prompt]: Epoch 62 / 100: avg data time: 4.64e-02, avg batch time: 0.4922, average train loss: 0.1464
[09/26 15:44:43 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1675, average loss: 3.7190
[09/26 15:44:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 95.50	
[09/26 15:44:43 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.3790390522001662
[09/26 15:44:50 visual_prompt]: Epoch 63 / 100: avg data time: 5.04e-02, avg batch time: 0.4942, average train loss: 0.1147
[09/26 15:44:51 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1675, average loss: 3.8923
[09/26 15:44:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 96.00	
[09/26 15:44:51 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.36218132209150045
[09/26 15:44:58 visual_prompt]: Epoch 64 / 100: avg data time: 5.57e-02, avg batch time: 0.5002, average train loss: 0.0958
[09/26 15:45:00 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1678, average loss: 4.1863
[09/26 15:45:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 96.50	
[09/26 15:45:00 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.34549150281252633
[09/26 15:45:07 visual_prompt]: Epoch 65 / 100: avg data time: 4.85e-02, avg batch time: 0.4932, average train loss: 0.0836
[09/26 15:45:08 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1675, average loss: 4.2802
[09/26 15:45:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 95.50	
[09/26 15:45:08 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.32898992833716567
[09/26 15:45:15 visual_prompt]: Epoch 66 / 100: avg data time: 5.36e-02, avg batch time: 0.4977, average train loss: 0.0833
[09/26 15:45:17 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1675, average loss: 4.5059
[09/26 15:45:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.50	top5: 96.50	
[09/26 15:45:17 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.31269670329204396
[09/26 15:45:24 visual_prompt]: Epoch 67 / 100: avg data time: 5.21e-02, avg batch time: 0.4971, average train loss: 0.0641
[09/26 15:45:25 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1677, average loss: 4.6396
[09/26 15:45:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 95.50	
[09/26 15:45:25 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.2966316784621
[09/26 15:45:32 visual_prompt]: Epoch 68 / 100: avg data time: 6.21e-02, avg batch time: 0.5091, average train loss: 0.0570
[09/26 15:45:34 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1675, average loss: 4.9262
[09/26 15:45:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 95.50	
[09/26 15:45:34 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.28081442660546124
[09/26 15:45:41 visual_prompt]: Epoch 69 / 100: avg data time: 5.96e-02, avg batch time: 0.5027, average train loss: 0.0279
[09/26 15:45:42 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1671, average loss: 5.1605
[09/26 15:45:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 95.00	
[09/26 15:45:42 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.26526421860705474
[09/26 15:45:49 visual_prompt]: Epoch 70 / 100: avg data time: 6.31e-02, avg batch time: 0.5078, average train loss: 0.0260
[09/26 15:45:51 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1672, average loss: 5.1100
[09/26 15:45:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 96.50	
[09/26 15:45:51 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.2500000000000001
[09/26 15:45:57 visual_prompt]: Epoch 71 / 100: avg data time: 5.27e-02, avg batch time: 0.4994, average train loss: 0.0284
[09/26 15:45:59 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1677, average loss: 5.0881
[09/26 15:45:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 96.50	
[09/26 15:45:59 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.2350403678833976
[09/26 15:46:06 visual_prompt]: Epoch 72 / 100: avg data time: 6.52e-02, avg batch time: 0.5094, average train loss: 0.0284
[09/26 15:46:08 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1676, average loss: 5.3025
[09/26 15:46:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 96.50	
[09/26 15:46:08 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.22040354826462666
[09/26 15:46:14 visual_prompt]: Epoch 73 / 100: avg data time: 5.71e-02, avg batch time: 0.5019, average train loss: 0.0232
[09/26 15:46:16 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1669, average loss: 5.5043
[09/26 15:46:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 96.00	
[09/26 15:46:16 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.2061073738537635
[09/26 15:46:23 visual_prompt]: Epoch 74 / 100: avg data time: 5.83e-02, avg batch time: 0.5015, average train loss: 0.0246
[09/26 15:46:25 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1676, average loss: 5.5862
[09/26 15:46:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 96.00	
[09/26 15:46:25 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.19216926233717085
[09/26 15:46:31 visual_prompt]: Epoch 75 / 100: avg data time: 5.24e-02, avg batch time: 0.4977, average train loss: 0.0228
[09/26 15:46:33 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1677, average loss: 5.3732
[09/26 15:46:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 95.50	
[09/26 15:46:33 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.17860619515673032
[09/26 15:46:40 visual_prompt]: Epoch 76 / 100: avg data time: 5.09e-02, avg batch time: 0.4957, average train loss: 0.0201
[09/26 15:46:41 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1675, average loss: 5.7519
[09/26 15:46:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 94.50	
[09/26 15:46:41 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.16543469682057105
[09/26 15:46:48 visual_prompt]: Epoch 77 / 100: avg data time: 5.74e-02, avg batch time: 0.5036, average train loss: 0.0258
[09/26 15:46:50 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1675, average loss: 5.4948
[09/26 15:46:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.50	
[09/26 15:46:50 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.15267081477050132
[09/26 15:46:57 visual_prompt]: Epoch 78 / 100: avg data time: 5.86e-02, avg batch time: 0.5036, average train loss: 0.0161
[09/26 15:46:58 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1673, average loss: 5.8277
[09/26 15:46:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 94.50	
[09/26 15:46:58 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.14033009983067452
[09/26 15:47:05 visual_prompt]: Epoch 79 / 100: avg data time: 6.26e-02, avg batch time: 0.5073, average train loss: 0.0145
[09/26 15:47:07 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1675, average loss: 5.7006
[09/26 15:47:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.50	
[09/26 15:47:07 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.12842758726130282
[09/26 15:47:14 visual_prompt]: Epoch 80 / 100: avg data time: 5.24e-02, avg batch time: 0.4955, average train loss: 0.0188
[09/26 15:47:15 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1671, average loss: 5.9950
[09/26 15:47:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 94.50	
[09/26 15:47:15 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.11697777844051105
[09/26 15:47:22 visual_prompt]: Epoch 81 / 100: avg data time: 4.97e-02, avg batch time: 0.4952, average train loss: 0.0139
[09/26 15:47:24 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1672, average loss: 5.7160
[09/26 15:47:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 95.50	
[09/26 15:47:24 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.10599462319663905
[09/26 15:47:31 visual_prompt]: Epoch 82 / 100: avg data time: 5.74e-02, avg batch time: 0.5019, average train loss: 0.0119
[09/26 15:47:32 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1676, average loss: 5.8127
[09/26 15:47:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 95.50	
[09/26 15:47:32 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.09549150281252633
[09/26 15:47:39 visual_prompt]: Epoch 83 / 100: avg data time: 5.96e-02, avg batch time: 0.5027, average train loss: 0.0079
[09/26 15:47:40 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1675, average loss: 5.9215
[09/26 15:47:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 94.50	
[09/26 15:47:40 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.08548121372247919
[09/26 15:47:47 visual_prompt]: Epoch 84 / 100: avg data time: 5.43e-02, avg batch time: 0.4994, average train loss: 0.0095
[09/26 15:47:49 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1671, average loss: 5.8408
[09/26 15:47:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.50	
[09/26 15:47:49 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.07597595192178702
[09/26 15:47:56 visual_prompt]: Epoch 85 / 100: avg data time: 6.13e-02, avg batch time: 0.5046, average train loss: 0.0078
[09/26 15:47:57 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1673, average loss: 5.8836
[09/26 15:47:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.50	
[09/26 15:47:57 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.06698729810778065
[09/26 15:48:04 visual_prompt]: Epoch 86 / 100: avg data time: 6.24e-02, avg batch time: 0.5063, average train loss: 0.0091
[09/26 15:48:06 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1671, average loss: 5.9134
[09/26 15:48:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 95.50	
[09/26 15:48:06 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.058526203570536506
[09/26 15:48:13 visual_prompt]: Epoch 87 / 100: avg data time: 5.64e-02, avg batch time: 0.4996, average train loss: 0.0107
[09/26 15:48:14 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1668, average loss: 5.9053
[09/26 15:48:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.50	
[09/26 15:48:14 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.05060297685041659
[09/26 15:48:21 visual_prompt]: Epoch 88 / 100: avg data time: 5.32e-02, avg batch time: 0.4987, average train loss: 0.0078
[09/26 15:48:23 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1673, average loss: 5.8859
[09/26 15:48:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.50	
[09/26 15:48:23 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.04322727117869951
[09/26 15:48:29 visual_prompt]: Epoch 89 / 100: avg data time: 5.31e-02, avg batch time: 0.4971, average train loss: 0.0073
[09/26 15:48:31 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1674, average loss: 5.8989
[09/26 15:48:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.50	
[09/26 15:48:31 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.03640807271660634
[09/26 15:48:38 visual_prompt]: Epoch 90 / 100: avg data time: 6.35e-02, avg batch time: 0.5066, average train loss: 0.0060
[09/26 15:48:40 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1673, average loss: 5.9569
[09/26 15:48:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 95.00	
[09/26 15:48:40 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.03015368960704584
[09/26 15:48:46 visual_prompt]: Epoch 91 / 100: avg data time: 5.04e-02, avg batch time: 0.4942, average train loss: 0.0062
[09/26 15:48:48 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1672, average loss: 5.9579
[09/26 15:48:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 15:48:48 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.024471741852423234
[09/26 15:48:55 visual_prompt]: Epoch 92 / 100: avg data time: 5.77e-02, avg batch time: 0.5026, average train loss: 0.0060
[09/26 15:48:56 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1675, average loss: 5.9711
[09/26 15:48:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 15:48:56 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.019369152030840553
[09/26 15:49:03 visual_prompt]: Epoch 93 / 100: avg data time: 5.15e-02, avg batch time: 0.4953, average train loss: 0.0063
[09/26 15:49:05 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1673, average loss: 5.9694
[09/26 15:49:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.00	
[09/26 15:49:05 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.014852136862001764
[09/26 15:49:11 visual_prompt]: Epoch 94 / 100: avg data time: 5.48e-02, avg batch time: 0.4986, average train loss: 0.0068
[09/26 15:49:13 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1673, average loss: 5.9671
[09/26 15:49:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 15:49:13 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.010926199633097156
[09/26 15:49:20 visual_prompt]: Epoch 95 / 100: avg data time: 5.92e-02, avg batch time: 0.5035, average train loss: 0.0052
[09/26 15:49:22 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1676, average loss: 5.9719
[09/26 15:49:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 15:49:22 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00759612349389599
[09/26 15:49:28 visual_prompt]: Epoch 96 / 100: avg data time: 5.01e-02, avg batch time: 0.4934, average train loss: 0.0077
[09/26 15:49:30 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1672, average loss: 5.9737
[09/26 15:49:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 15:49:30 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.004865965629214819
[09/26 15:49:37 visual_prompt]: Epoch 97 / 100: avg data time: 5.02e-02, avg batch time: 0.4958, average train loss: 0.0066
[09/26 15:49:38 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1674, average loss: 5.9780
[09/26 15:49:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 15:49:38 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.002739052315863355
[09/26 15:49:45 visual_prompt]: Epoch 98 / 100: avg data time: 4.95e-02, avg batch time: 0.4956, average train loss: 0.0064
[09/26 15:49:47 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1675, average loss: 5.9805
[09/26 15:49:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 15:49:47 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0012179748700879012
[09/26 15:49:53 visual_prompt]: Epoch 99 / 100: avg data time: 4.52e-02, avg batch time: 0.4902, average train loss: 0.0052
[09/26 15:49:55 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1676, average loss: 5.9805
[09/26 15:49:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 15:49:55 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00030458649045211894
[09/26 15:50:02 visual_prompt]: Epoch 100 / 100: avg data time: 4.92e-02, avg batch time: 0.4926, average train loss: 0.0092
[09/26 15:50:03 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1671, average loss: 5.9808
[09/26 15:50:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 15:50:03 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 15:50:03 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 15:50:03 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 15:50:03 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 15:50:03 visual_prompt]: Training with config:
[09/26 15:50:03 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 15:50:03 visual_prompt]: Loading training data...
[09/26 15:50:03 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 15:50:04 visual_prompt]: Number of images: 800
[09/26 15:50:04 visual_prompt]: Number of classes: 9 / 9
[09/26 15:50:04 visual_prompt]: Loading validation data...
[09/26 15:50:04 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 15:50:05 visual_prompt]: Number of images: 200
[09/26 15:50:05 visual_prompt]: Number of classes: 9 / 9
[09/26 15:50:05 visual_prompt]: Constructing models...
[09/26 15:50:07 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 15:50:07 visual_prompt]: tuned percent:0.542
[09/26 15:50:07 visual_prompt]: Device used for model: 0
[09/26 15:50:07 visual_prompt]: Setting up Evaluator...
[09/26 15:50:07 visual_prompt]: Setting up Trainer...
[09/26 15:50:07 visual_prompt]: 	Setting up the optimizer...
[09/26 15:50:07 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 15:50:14 visual_prompt]: Epoch 1 / 100: avg data time: 5.61e-02, avg batch time: 0.5007, average train loss: 2.8767
[09/26 15:50:16 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1673, average loss: 2.9516
[09/26 15:50:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 15:50:16 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 15:50:16 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 15:50:23 visual_prompt]: Epoch 2 / 100: avg data time: 5.81e-02, avg batch time: 0.5005, average train loss: 2.8134
[09/26 15:50:24 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1671, average loss: 2.3782
[09/26 15:50:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 53.50	
[09/26 15:50:24 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 15:50:31 visual_prompt]: Epoch 3 / 100: avg data time: 6.58e-02, avg batch time: 0.5084, average train loss: 2.2729
[09/26 15:50:33 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1671, average loss: 2.2275
[09/26 15:50:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.00	
[09/26 15:50:33 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 15:50:40 visual_prompt]: Epoch 4 / 100: avg data time: 5.20e-02, avg batch time: 0.4955, average train loss: 2.2459
[09/26 15:50:41 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1673, average loss: 2.2435
[09/26 15:50:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 15:50:41 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 15:50:48 visual_prompt]: Epoch 5 / 100: avg data time: 6.00e-02, avg batch time: 0.5040, average train loss: 2.2160
[09/26 15:50:50 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1676, average loss: 2.2279
[09/26 15:50:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 15:50:50 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 15:50:57 visual_prompt]: Epoch 6 / 100: avg data time: 5.46e-02, avg batch time: 0.4997, average train loss: 2.2405
[09/26 15:50:58 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1673, average loss: 2.1880
[09/26 15:50:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 63.50	
[09/26 15:50:58 visual_prompt]: Best epoch 6: best metric: 0.175
[09/26 15:50:58 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 15:51:05 visual_prompt]: Epoch 7 / 100: avg data time: 6.10e-02, avg batch time: 0.5040, average train loss: 2.2093
[09/26 15:51:07 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1678, average loss: 2.2362
[09/26 15:51:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 15:51:07 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 15:51:13 visual_prompt]: Epoch 8 / 100: avg data time: 5.05e-02, avg batch time: 0.4943, average train loss: 2.2388
[09/26 15:51:15 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1671, average loss: 2.2782
[09/26 15:51:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.00	
[09/26 15:51:15 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 15:51:22 visual_prompt]: Epoch 9 / 100: avg data time: 5.36e-02, avg batch time: 0.4979, average train loss: 2.2562
[09/26 15:51:24 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1674, average loss: 2.2384
[09/26 15:51:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 15:51:24 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 15:51:30 visual_prompt]: Epoch 10 / 100: avg data time: 5.88e-02, avg batch time: 0.5027, average train loss: 2.2269
[09/26 15:51:32 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1676, average loss: 2.2101
[09/26 15:51:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 15:51:32 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 15:51:39 visual_prompt]: Epoch 11 / 100: avg data time: 6.24e-02, avg batch time: 0.5061, average train loss: 2.2713
[09/26 15:51:40 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1673, average loss: 2.2836
[09/26 15:51:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 15:51:40 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 15:51:47 visual_prompt]: Epoch 12 / 100: avg data time: 5.75e-02, avg batch time: 0.4995, average train loss: 2.2620
[09/26 15:51:49 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1669, average loss: 2.2437
[09/26 15:51:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 15:51:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 15:51:56 visual_prompt]: Epoch 13 / 100: avg data time: 4.64e-02, avg batch time: 0.4908, average train loss: 2.2641
[09/26 15:51:57 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1669, average loss: 2.2462
[09/26 15:51:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 15:51:57 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 15:52:04 visual_prompt]: Epoch 14 / 100: avg data time: 5.70e-02, avg batch time: 0.5011, average train loss: 2.2476
[09/26 15:52:06 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1672, average loss: 2.2527
[09/26 15:52:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 15:52:06 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 15:52:13 visual_prompt]: Epoch 15 / 100: avg data time: 5.40e-02, avg batch time: 0.4974, average train loss: 2.2413
[09/26 15:52:14 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1672, average loss: 2.2453
[09/26 15:52:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 50.50	
[09/26 15:52:14 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 15:52:21 visual_prompt]: Epoch 16 / 100: avg data time: 6.36e-02, avg batch time: 0.5079, average train loss: 2.2508
[09/26 15:52:23 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1674, average loss: 2.2376
[09/26 15:52:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 15:52:23 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 15:52:29 visual_prompt]: Epoch 17 / 100: avg data time: 5.55e-02, avg batch time: 0.4997, average train loss: 2.2296
[09/26 15:52:31 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1674, average loss: 2.1944
[09/26 15:52:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 15:52:31 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 15:52:38 visual_prompt]: Epoch 18 / 100: avg data time: 5.77e-02, avg batch time: 0.5010, average train loss: 2.2378
[09/26 15:52:40 visual_prompt]: Inference (val):avg data time: 3.98e-05, avg batch time: 0.1674, average loss: 2.2487
[09/26 15:52:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 15:52:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 15:52:46 visual_prompt]: Epoch 19 / 100: avg data time: 4.97e-02, avg batch time: 0.4948, average train loss: 2.2229
[09/26 15:52:48 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1673, average loss: 2.2090
[09/26 15:52:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 15:52:48 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 15:52:55 visual_prompt]: Epoch 20 / 100: avg data time: 6.13e-02, avg batch time: 0.5060, average train loss: 2.2376
[09/26 15:52:57 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1675, average loss: 2.2312
[09/26 15:52:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 61.00	
[09/26 15:52:57 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 15:53:03 visual_prompt]: Epoch 21 / 100: avg data time: 6.49e-02, avg batch time: 0.5080, average train loss: 2.2312
[09/26 15:53:05 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1673, average loss: 2.2134
[09/26 15:53:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 15:53:05 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 15:53:12 visual_prompt]: Epoch 22 / 100: avg data time: 5.42e-02, avg batch time: 0.4969, average train loss: 2.2388
[09/26 15:53:13 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1672, average loss: 2.2342
[09/26 15:53:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 15:53:13 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 15:53:20 visual_prompt]: Epoch 23 / 100: avg data time: 4.98e-02, avg batch time: 0.4950, average train loss: 2.2433
[09/26 15:53:22 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1675, average loss: 2.2154
[09/26 15:53:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 15:53:22 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 15:53:29 visual_prompt]: Epoch 24 / 100: avg data time: 6.12e-02, avg batch time: 0.5035, average train loss: 2.2556
[09/26 15:53:30 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1673, average loss: 2.2118
[09/26 15:53:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 15:53:30 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 15:53:37 visual_prompt]: Epoch 25 / 100: avg data time: 5.47e-02, avg batch time: 0.4988, average train loss: 2.2360
[09/26 15:53:39 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1673, average loss: 2.2488
[09/26 15:53:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 61.50	
[09/26 15:53:39 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 15:53:46 visual_prompt]: Epoch 26 / 100: avg data time: 4.78e-02, avg batch time: 0.4922, average train loss: 2.2562
[09/26 15:53:47 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1670, average loss: 2.2206
[09/26 15:53:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.50	
[09/26 15:53:47 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 15:53:54 visual_prompt]: Epoch 27 / 100: avg data time: 6.52e-02, avg batch time: 0.5092, average train loss: 2.2298
[09/26 15:53:56 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1671, average loss: 2.2491
[09/26 15:53:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/26 15:53:56 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 15:54:03 visual_prompt]: Epoch 28 / 100: avg data time: 6.05e-02, avg batch time: 0.5056, average train loss: 2.2226
[09/26 15:54:04 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1673, average loss: 2.2417
[09/26 15:54:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 15:54:04 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 15:54:11 visual_prompt]: Epoch 29 / 100: avg data time: 6.14e-02, avg batch time: 0.5051, average train loss: 2.2318
[09/26 15:54:13 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1673, average loss: 2.2162
[09/26 15:54:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 15:54:13 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 15:54:20 visual_prompt]: Epoch 30 / 100: avg data time: 5.89e-02, avg batch time: 0.5018, average train loss: 2.2263
[09/26 15:54:21 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1674, average loss: 2.2392
[09/26 15:54:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/26 15:54:21 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 15:54:28 visual_prompt]: Epoch 31 / 100: avg data time: 5.74e-02, avg batch time: 0.5012, average train loss: 2.2219
[09/26 15:54:30 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1673, average loss: 2.2080
[09/26 15:54:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 15:54:30 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 15:54:36 visual_prompt]: Epoch 32 / 100: avg data time: 4.80e-02, avg batch time: 0.4919, average train loss: 2.2143
[09/26 15:54:38 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1672, average loss: 2.2284
[09/26 15:54:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 15:54:38 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 15:54:45 visual_prompt]: Epoch 33 / 100: avg data time: 5.58e-02, avg batch time: 0.4999, average train loss: 2.2313
[09/26 15:54:46 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1671, average loss: 2.2174
[09/26 15:54:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 15:54:46 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 15:54:53 visual_prompt]: Epoch 34 / 100: avg data time: 6.94e-02, avg batch time: 0.5119, average train loss: 2.2201
[09/26 15:54:55 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.1673, average loss: 2.2227
[09/26 15:54:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.50	
[09/26 15:54:55 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 15:55:02 visual_prompt]: Epoch 35 / 100: avg data time: 4.87e-02, avg batch time: 0.4934, average train loss: 2.2153
[09/26 15:55:03 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1669, average loss: 2.2111
[09/26 15:55:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 15:55:03 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 15:55:10 visual_prompt]: Epoch 36 / 100: avg data time: 5.05e-02, avg batch time: 0.4957, average train loss: 2.2114
[09/26 15:55:12 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1667, average loss: 2.2099
[09/26 15:55:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 15:55:12 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 15:55:19 visual_prompt]: Epoch 37 / 100: avg data time: 5.88e-02, avg batch time: 0.5014, average train loss: 2.2156
[09/26 15:55:20 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.1668, average loss: 2.2078
[09/26 15:55:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 15:55:20 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 15:55:27 visual_prompt]: Epoch 38 / 100: avg data time: 6.04e-02, avg batch time: 0.5022, average train loss: 2.2420
[09/26 15:55:29 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1673, average loss: 2.2556
[09/26 15:55:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 15:55:29 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 15:55:36 visual_prompt]: Epoch 39 / 100: avg data time: 4.96e-02, avg batch time: 0.4941, average train loss: 2.2441
[09/26 15:55:37 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1673, average loss: 2.2457
[09/26 15:55:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 15:55:37 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 15:55:44 visual_prompt]: Epoch 40 / 100: avg data time: 6.24e-02, avg batch time: 0.5064, average train loss: 2.2397
[09/26 15:55:46 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1677, average loss: 2.2923
[09/26 15:55:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 15:55:46 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 15:55:53 visual_prompt]: Epoch 41 / 100: avg data time: 5.92e-02, avg batch time: 0.5031, average train loss: 2.2470
[09/26 15:55:54 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1675, average loss: 2.2247
[09/26 15:55:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 15:55:54 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 15:56:01 visual_prompt]: Epoch 42 / 100: avg data time: 4.70e-02, avg batch time: 0.4924, average train loss: 2.2101
[09/26 15:56:03 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.1672, average loss: 2.2073
[09/26 15:56:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 15:56:03 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 15:56:09 visual_prompt]: Epoch 43 / 100: avg data time: 4.73e-02, avg batch time: 0.4921, average train loss: 2.2089
[09/26 15:56:11 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1674, average loss: 2.2255
[09/26 15:56:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/26 15:56:11 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 15:56:18 visual_prompt]: Epoch 44 / 100: avg data time: 5.04e-02, avg batch time: 0.4931, average train loss: 2.2201
[09/26 15:56:19 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1677, average loss: 2.2333
[09/26 15:56:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 63.50	
[09/26 15:56:19 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 15:56:26 visual_prompt]: Epoch 45 / 100: avg data time: 5.94e-02, avg batch time: 0.5031, average train loss: 2.2209
[09/26 15:56:28 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1671, average loss: 2.6062
[09/26 15:56:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/26 15:56:28 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 15:56:35 visual_prompt]: Epoch 46 / 100: avg data time: 5.09e-02, avg batch time: 0.4936, average train loss: 2.2714
[09/26 15:56:36 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1672, average loss: 2.2645
[09/26 15:56:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.00	
[09/26 15:56:36 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 15:56:43 visual_prompt]: Epoch 47 / 100: avg data time: 6.59e-02, avg batch time: 0.5082, average train loss: 2.2474
[09/26 15:56:45 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1670, average loss: 2.1905
[09/26 15:56:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 64.00	
[09/26 15:56:45 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 15:56:52 visual_prompt]: Epoch 48 / 100: avg data time: 5.72e-02, avg batch time: 0.5001, average train loss: 2.2252
[09/26 15:56:53 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.1670, average loss: 2.2342
[09/26 15:56:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 15:56:53 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 15:57:00 visual_prompt]: Epoch 49 / 100: avg data time: 5.90e-02, avg batch time: 0.5034, average train loss: 2.2173
[09/26 15:57:02 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1672, average loss: 2.2524
[09/26 15:57:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.00	
[09/26 15:57:02 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 15:57:09 visual_prompt]: Epoch 50 / 100: avg data time: 4.93e-02, avg batch time: 0.4919, average train loss: 2.1989
[09/26 15:57:10 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1671, average loss: 2.2257
[09/26 15:57:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/26 15:57:10 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 15:57:17 visual_prompt]: Epoch 51 / 100: avg data time: 4.94e-02, avg batch time: 0.4940, average train loss: 2.2085
[09/26 15:57:19 visual_prompt]: Inference (val):avg data time: 3.65e-05, avg batch time: 0.1672, average loss: 2.2169
[09/26 15:57:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 15:57:19 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 15:57:25 visual_prompt]: Epoch 52 / 100: avg data time: 5.15e-02, avg batch time: 0.4953, average train loss: 2.2220
[09/26 15:57:27 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1671, average loss: 2.1921
[09/26 15:57:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.50	
[09/26 15:57:27 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 15:57:34 visual_prompt]: Epoch 53 / 100: avg data time: 6.07e-02, avg batch time: 0.5040, average train loss: 2.2294
[09/26 15:57:35 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1673, average loss: 2.2186
[09/26 15:57:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 49.50	
[09/26 15:57:35 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 15:57:42 visual_prompt]: Epoch 54 / 100: avg data time: 4.76e-02, avg batch time: 0.4927, average train loss: 2.2246
[09/26 15:57:44 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1673, average loss: 2.1950
[09/26 15:57:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 62.00	
[09/26 15:57:44 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 15:57:51 visual_prompt]: Epoch 55 / 100: avg data time: 7.07e-02, avg batch time: 0.5129, average train loss: 2.2074
[09/26 15:57:52 visual_prompt]: Inference (val):avg data time: 4.79e-05, avg batch time: 0.1670, average loss: 2.2214
[09/26 15:57:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 61.50	
[09/26 15:57:52 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 15:57:59 visual_prompt]: Epoch 56 / 100: avg data time: 6.05e-02, avg batch time: 0.5040, average train loss: 2.2116
[09/26 15:58:01 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1669, average loss: 2.2207
[09/26 15:58:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 15:58:01 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 15:58:08 visual_prompt]: Epoch 57 / 100: avg data time: 4.77e-02, avg batch time: 0.4940, average train loss: 2.2122
[09/26 15:58:09 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1674, average loss: 2.2164
[09/26 15:58:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 51.50	
[09/26 15:58:09 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 15:58:16 visual_prompt]: Epoch 58 / 100: avg data time: 5.26e-02, avg batch time: 0.4964, average train loss: 2.2155
[09/26 15:58:18 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1671, average loss: 2.1911
[09/26 15:58:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 15:58:18 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 15:58:25 visual_prompt]: Epoch 59 / 100: avg data time: 6.09e-02, avg batch time: 0.5052, average train loss: 2.2125
[09/26 15:58:26 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1670, average loss: 2.2048
[09/26 15:58:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.50	
[09/26 15:58:26 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 15:58:33 visual_prompt]: Epoch 60 / 100: avg data time: 4.86e-02, avg batch time: 0.4970, average train loss: 2.2056
[09/26 15:58:35 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1669, average loss: 2.2004
[09/26 15:58:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 15:58:35 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 15:58:41 visual_prompt]: Epoch 61 / 100: avg data time: 5.33e-02, avg batch time: 0.4973, average train loss: 2.2103
[09/26 15:58:43 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1669, average loss: 2.2238
[09/26 15:58:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 15:58:43 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 15:58:50 visual_prompt]: Epoch 62 / 100: avg data time: 5.94e-02, avg batch time: 0.5037, average train loss: 2.2191
[09/26 15:58:51 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1668, average loss: 2.2083
[09/26 15:58:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 15:58:51 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 15:58:58 visual_prompt]: Epoch 63 / 100: avg data time: 5.58e-02, avg batch time: 0.4991, average train loss: 2.2073
[09/26 15:59:00 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1669, average loss: 2.1964
[09/26 15:59:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 15:59:00 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 15:59:07 visual_prompt]: Epoch 64 / 100: avg data time: 6.49e-02, avg batch time: 0.5074, average train loss: 2.2020
[09/26 15:59:08 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1670, average loss: 2.2077
[09/26 15:59:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 15:59:08 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 15:59:15 visual_prompt]: Epoch 65 / 100: avg data time: 6.06e-02, avg batch time: 0.5025, average train loss: 2.1928
[09/26 15:59:17 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1669, average loss: 2.1926
[09/26 15:59:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 15:59:17 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 15:59:23 visual_prompt]: Epoch 66 / 100: avg data time: 4.96e-02, avg batch time: 0.4945, average train loss: 2.2065
[09/26 15:59:25 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1672, average loss: 2.1960
[09/26 15:59:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 15:59:25 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 15:59:32 visual_prompt]: Epoch 67 / 100: avg data time: 4.85e-02, avg batch time: 0.4919, average train loss: 2.1998
[09/26 15:59:33 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1673, average loss: 2.1959
[09/26 15:59:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.50	
[09/26 15:59:33 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 15:59:40 visual_prompt]: Epoch 68 / 100: avg data time: 5.04e-02, avg batch time: 0.4946, average train loss: 2.1969
[09/26 15:59:42 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1671, average loss: 2.1893
[09/26 15:59:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 59.00	
[09/26 15:59:42 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 15:59:48 visual_prompt]: Epoch 69 / 100: avg data time: 4.76e-02, avg batch time: 0.4931, average train loss: 2.1880
[09/26 15:59:50 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1671, average loss: 2.1967
[09/26 15:59:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 15:59:50 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 15:59:57 visual_prompt]: Epoch 70 / 100: avg data time: 5.55e-02, avg batch time: 0.4996, average train loss: 2.1799
[09/26 15:59:58 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1673, average loss: 2.2986
[09/26 15:59:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 15:59:58 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 16:00:05 visual_prompt]: Epoch 71 / 100: avg data time: 4.81e-02, avg batch time: 0.4932, average train loss: 2.2275
[09/26 16:00:07 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1672, average loss: 2.2106
[09/26 16:00:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/26 16:00:07 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 16:00:13 visual_prompt]: Epoch 72 / 100: avg data time: 5.57e-02, avg batch time: 0.5005, average train loss: 2.1913
[09/26 16:00:15 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1674, average loss: 2.1635
[09/26 16:00:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 66.00	
[09/26 16:00:15 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 16:00:22 visual_prompt]: Epoch 73 / 100: avg data time: 4.51e-02, avg batch time: 0.4898, average train loss: 2.1799
[09/26 16:00:23 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1673, average loss: 2.2394
[09/26 16:00:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 16:00:23 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 16:00:30 visual_prompt]: Epoch 74 / 100: avg data time: 6.23e-02, avg batch time: 0.5043, average train loss: 2.1727
[09/26 16:00:32 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1672, average loss: 2.1855
[09/26 16:00:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 66.00	
[09/26 16:00:32 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 16:00:38 visual_prompt]: Epoch 75 / 100: avg data time: 4.96e-02, avg batch time: 0.4926, average train loss: 2.1663
[09/26 16:00:40 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1672, average loss: 2.1421
[09/26 16:00:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 70.50	
[09/26 16:00:40 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 16:00:47 visual_prompt]: Epoch 76 / 100: avg data time: 5.58e-02, avg batch time: 0.4988, average train loss: 2.1460
[09/26 16:00:48 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1672, average loss: 2.1796
[09/26 16:00:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 67.00	
[09/26 16:00:48 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 16:00:55 visual_prompt]: Epoch 77 / 100: avg data time: 5.57e-02, avg batch time: 0.4991, average train loss: 2.2144
[09/26 16:00:57 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1674, average loss: 2.1946
[09/26 16:00:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 16:00:57 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 16:01:04 visual_prompt]: Epoch 78 / 100: avg data time: 5.37e-02, avg batch time: 0.4984, average train loss: 2.2006
[09/26 16:01:05 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1673, average loss: 2.1893
[09/26 16:01:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 16:01:05 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 16:01:12 visual_prompt]: Epoch 79 / 100: avg data time: 6.37e-02, avg batch time: 0.5065, average train loss: 2.2079
[09/26 16:01:14 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1676, average loss: 2.1971
[09/26 16:01:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 58.00	
[09/26 16:01:14 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 16:01:20 visual_prompt]: Epoch 80 / 100: avg data time: 4.64e-02, avg batch time: 0.4907, average train loss: 2.1908
[09/26 16:01:22 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1672, average loss: 2.1877
[09/26 16:01:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 62.00	
[09/26 16:01:22 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 16:01:29 visual_prompt]: Epoch 81 / 100: avg data time: 4.88e-02, avg batch time: 0.4925, average train loss: 2.1743
[09/26 16:01:30 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.1670, average loss: 2.1639
[09/26 16:01:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 74.00	
[09/26 16:01:30 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 16:01:37 visual_prompt]: Epoch 82 / 100: avg data time: 5.27e-02, avg batch time: 0.4973, average train loss: 2.1447
[09/26 16:01:39 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1676, average loss: 2.3617
[09/26 16:01:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 62.50	
[09/26 16:01:39 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 16:01:46 visual_prompt]: Epoch 83 / 100: avg data time: 4.88e-02, avg batch time: 0.4946, average train loss: 2.2061
[09/26 16:01:47 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1673, average loss: 2.1856
[09/26 16:01:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.00	
[09/26 16:01:47 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 16:01:54 visual_prompt]: Epoch 84 / 100: avg data time: 5.96e-02, avg batch time: 0.5028, average train loss: 2.1785
[09/26 16:01:56 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1674, average loss: 2.1911
[09/26 16:01:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 16:01:56 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 16:02:03 visual_prompt]: Epoch 85 / 100: avg data time: 5.43e-02, avg batch time: 0.4989, average train loss: 2.2002
[09/26 16:02:04 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1669, average loss: 2.1942
[09/26 16:02:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 16:02:04 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 16:02:11 visual_prompt]: Epoch 86 / 100: avg data time: 4.79e-02, avg batch time: 0.4918, average train loss: 2.1848
[09/26 16:02:13 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1671, average loss: 2.1662
[09/26 16:02:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 66.50	
[09/26 16:02:13 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 16:02:19 visual_prompt]: Epoch 87 / 100: avg data time: 4.88e-02, avg batch time: 0.4935, average train loss: 2.1743
[09/26 16:02:21 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1673, average loss: 2.1875
[09/26 16:02:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 16:02:21 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 16:02:28 visual_prompt]: Epoch 88 / 100: avg data time: 5.92e-02, avg batch time: 0.5022, average train loss: 2.1828
[09/26 16:02:29 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1674, average loss: 2.1831
[09/26 16:02:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 65.50	
[09/26 16:02:29 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 16:02:36 visual_prompt]: Epoch 89 / 100: avg data time: 4.62e-02, avg batch time: 0.4906, average train loss: 2.1516
[09/26 16:02:38 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1672, average loss: 2.1817
[09/26 16:02:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 16:02:38 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 16:02:45 visual_prompt]: Epoch 90 / 100: avg data time: 5.30e-02, avg batch time: 0.4958, average train loss: 2.0812
[09/26 16:02:46 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1673, average loss: 2.1830
[09/26 16:02:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 59.50	
[09/26 16:02:46 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 16:02:53 visual_prompt]: Epoch 91 / 100: avg data time: 5.00e-02, avg batch time: 0.4936, average train loss: 2.1075
[09/26 16:02:54 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1675, average loss: 2.1026
[09/26 16:02:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 70.50	
[09/26 16:02:54 visual_prompt]: Best epoch 91: best metric: 0.180
[09/26 16:02:54 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 16:03:01 visual_prompt]: Epoch 92 / 100: avg data time: 6.13e-02, avg batch time: 0.5055, average train loss: 2.0548
[09/26 16:03:03 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1674, average loss: 1.9609
[09/26 16:03:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 81.50	
[09/26 16:03:03 visual_prompt]: Best epoch 92: best metric: 0.190
[09/26 16:03:03 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 16:03:10 visual_prompt]: Epoch 93 / 100: avg data time: 4.70e-02, avg batch time: 0.4930, average train loss: 1.9424
[09/26 16:03:11 visual_prompt]: Inference (val):avg data time: 1.97e-05, avg batch time: 0.1674, average loss: 2.4497
[09/26 16:03:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 60.00	
[09/26 16:03:11 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 16:03:18 visual_prompt]: Epoch 94 / 100: avg data time: 4.75e-02, avg batch time: 0.4914, average train loss: 1.9975
[09/26 16:03:20 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1674, average loss: 1.9546
[09/26 16:03:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 80.00	
[09/26 16:03:20 visual_prompt]: Best epoch 94: best metric: 0.210
[09/26 16:03:20 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 16:03:26 visual_prompt]: Epoch 95 / 100: avg data time: 4.63e-02, avg batch time: 0.4898, average train loss: 1.8866
[09/26 16:03:28 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1671, average loss: 1.8694
[09/26 16:03:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 85.50	
[09/26 16:03:28 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 16:03:35 visual_prompt]: Epoch 96 / 100: avg data time: 5.43e-02, avg batch time: 0.4978, average train loss: 1.8668
[09/26 16:03:36 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1672, average loss: 1.9218
[09/26 16:03:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 83.50	
[09/26 16:03:36 visual_prompt]: Best epoch 96: best metric: 0.215
[09/26 16:03:36 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 16:03:43 visual_prompt]: Epoch 97 / 100: avg data time: 5.01e-02, avg batch time: 0.4932, average train loss: 1.8085
[09/26 16:03:45 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1671, average loss: 1.9290
[09/26 16:03:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 82.00	
[09/26 16:03:45 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 16:03:52 visual_prompt]: Epoch 98 / 100: avg data time: 6.42e-02, avg batch time: 0.5076, average train loss: 1.7803
[09/26 16:03:53 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1674, average loss: 1.8707
[09/26 16:03:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 85.00	
[09/26 16:03:53 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 16:04:00 visual_prompt]: Epoch 99 / 100: avg data time: 5.65e-02, avg batch time: 0.4997, average train loss: 1.7720
[09/26 16:04:02 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1672, average loss: 1.8618
[09/26 16:04:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 85.50	
[09/26 16:04:02 visual_prompt]: Best epoch 99: best metric: 0.220
[09/26 16:04:02 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 16:04:08 visual_prompt]: Epoch 100 / 100: avg data time: 5.51e-02, avg batch time: 0.4984, average train loss: 1.7398
[09/26 16:04:10 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1678, average loss: 1.8580
[09/26 16:04:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 84.00	
[09/26 16:04:10 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 16:04:10 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 16:04:10 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 16:04:10 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 16:04:10 visual_prompt]: Training with config:
[09/26 16:04:10 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 16:04:10 visual_prompt]: Loading training data...
[09/26 16:04:10 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 16:04:11 visual_prompt]: Number of images: 800
[09/26 16:04:11 visual_prompt]: Number of classes: 9 / 9
[09/26 16:04:11 visual_prompt]: Loading validation data...
[09/26 16:04:11 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 16:04:11 visual_prompt]: Number of images: 200
[09/26 16:04:11 visual_prompt]: Number of classes: 9 / 9
[09/26 16:04:11 visual_prompt]: Constructing models...
[09/26 16:04:14 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 16:04:14 visual_prompt]: tuned percent:0.542
[09/26 16:04:14 visual_prompt]: Device used for model: 0
[09/26 16:04:14 visual_prompt]: Setting up Evaluator...
[09/26 16:04:14 visual_prompt]: Setting up Trainer...
[09/26 16:04:14 visual_prompt]: 	Setting up the optimizer...
[09/26 16:04:14 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 16:04:21 visual_prompt]: Epoch 1 / 100: avg data time: 6.28e-02, avg batch time: 0.5073, average train loss: 2.8855
[09/26 16:04:22 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1668, average loss: 2.9516
[09/26 16:04:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 16:04:22 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 16:04:22 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 16:04:29 visual_prompt]: Epoch 2 / 100: avg data time: 5.70e-02, avg batch time: 0.4997, average train loss: 2.7236
[09/26 16:04:31 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1670, average loss: 2.4740
[09/26 16:04:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.00	top5: 54.00	
[09/26 16:04:31 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 16:04:38 visual_prompt]: Epoch 3 / 100: avg data time: 5.72e-02, avg batch time: 0.4996, average train loss: 2.3028
[09/26 16:04:39 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1672, average loss: 2.2942
[09/26 16:04:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 16:04:39 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 16:04:46 visual_prompt]: Epoch 4 / 100: avg data time: 5.82e-02, avg batch time: 0.5031, average train loss: 2.2560
[09/26 16:04:48 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1670, average loss: 2.2583
[09/26 16:04:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 16:04:48 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 16:04:55 visual_prompt]: Epoch 5 / 100: avg data time: 5.25e-02, avg batch time: 0.4977, average train loss: 2.2620
[09/26 16:04:56 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1673, average loss: 2.2441
[09/26 16:04:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 16:04:56 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 16:05:03 visual_prompt]: Epoch 6 / 100: avg data time: 5.55e-02, avg batch time: 0.4999, average train loss: 2.2261
[09/26 16:05:05 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.1673, average loss: 2.2094
[09/26 16:05:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/26 16:05:05 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 16:05:12 visual_prompt]: Epoch 7 / 100: avg data time: 5.83e-02, avg batch time: 0.5017, average train loss: 2.1891
[09/26 16:05:13 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1674, average loss: 2.1935
[09/26 16:05:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 66.00	
[09/26 16:05:13 visual_prompt]: Best epoch 7: best metric: 0.165
[09/26 16:05:13 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 16:05:20 visual_prompt]: Epoch 8 / 100: avg data time: 5.46e-02, avg batch time: 0.4991, average train loss: 2.1445
[09/26 16:05:21 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1672, average loss: 2.1088
[09/26 16:05:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 64.50	
[09/26 16:05:21 visual_prompt]: Best epoch 8: best metric: 0.185
[09/26 16:05:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 16:05:28 visual_prompt]: Epoch 9 / 100: avg data time: 5.97e-02, avg batch time: 0.5030, average train loss: 2.1567
[09/26 16:05:30 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1675, average loss: 2.1473
[09/26 16:05:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 70.00	
[09/26 16:05:30 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 16:05:37 visual_prompt]: Epoch 10 / 100: avg data time: 5.63e-02, avg batch time: 0.5006, average train loss: 2.0720
[09/26 16:05:38 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1675, average loss: 2.2534
[09/26 16:05:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 67.50	
[09/26 16:05:38 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 16:05:45 visual_prompt]: Epoch 11 / 100: avg data time: 5.26e-02, avg batch time: 0.4977, average train loss: 2.0135
[09/26 16:05:47 visual_prompt]: Inference (val):avg data time: 2.10e-05, avg batch time: 0.1674, average loss: 2.0369
[09/26 16:05:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 83.50	
[09/26 16:05:47 visual_prompt]: Best epoch 11: best metric: 0.215
[09/26 16:05:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 16:05:54 visual_prompt]: Epoch 12 / 100: avg data time: 4.64e-02, avg batch time: 0.4923, average train loss: 1.8501
[09/26 16:05:55 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1672, average loss: 1.9565
[09/26 16:05:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 82.00	
[09/26 16:05:55 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 16:06:02 visual_prompt]: Epoch 13 / 100: avg data time: 4.88e-02, avg batch time: 0.4928, average train loss: 1.8033
[09/26 16:06:04 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1674, average loss: 1.8489
[09/26 16:06:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 87.00	
[09/26 16:06:04 visual_prompt]: Best epoch 13: best metric: 0.290
[09/26 16:06:04 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 16:06:10 visual_prompt]: Epoch 14 / 100: avg data time: 5.01e-02, avg batch time: 0.4936, average train loss: 1.8603
[09/26 16:06:12 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1675, average loss: 2.1605
[09/26 16:06:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 72.50	
[09/26 16:06:12 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 16:06:19 visual_prompt]: Epoch 15 / 100: avg data time: 5.37e-02, avg batch time: 0.4969, average train loss: 1.7182
[09/26 16:06:20 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1673, average loss: 1.8723
[09/26 16:06:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 88.00	
[09/26 16:06:20 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 16:06:27 visual_prompt]: Epoch 16 / 100: avg data time: 5.25e-02, avg batch time: 0.4967, average train loss: 1.7502
[09/26 16:06:29 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1674, average loss: 2.1375
[09/26 16:06:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 76.00	
[09/26 16:06:29 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 16:06:36 visual_prompt]: Epoch 17 / 100: avg data time: 4.94e-02, avg batch time: 0.4940, average train loss: 1.7521
[09/26 16:06:37 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1676, average loss: 1.8847
[09/26 16:06:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 86.50	
[09/26 16:06:37 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 16:06:44 visual_prompt]: Epoch 18 / 100: avg data time: 4.60e-02, avg batch time: 0.4899, average train loss: 1.6682
[09/26 16:06:45 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1675, average loss: 2.2833
[09/26 16:06:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 78.50	
[09/26 16:06:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 16:06:52 visual_prompt]: Epoch 19 / 100: avg data time: 6.05e-02, avg batch time: 0.5041, average train loss: 1.6233
[09/26 16:06:54 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1673, average loss: 1.8122
[09/26 16:06:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 91.50	
[09/26 16:06:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 16:07:01 visual_prompt]: Epoch 20 / 100: avg data time: 5.52e-02, avg batch time: 0.4997, average train loss: 1.6997
[09/26 16:07:02 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1673, average loss: 1.9907
[09/26 16:07:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 86.00	
[09/26 16:07:02 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 16:07:09 visual_prompt]: Epoch 21 / 100: avg data time: 4.86e-02, avg batch time: 0.4936, average train loss: 1.5305
[09/26 16:07:11 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1678, average loss: 1.9431
[09/26 16:07:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 86.00	
[09/26 16:07:11 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 16:07:17 visual_prompt]: Epoch 22 / 100: avg data time: 5.52e-02, avg batch time: 0.5002, average train loss: 1.8256
[09/26 16:07:19 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1674, average loss: 2.3079
[09/26 16:07:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 83.00	
[09/26 16:07:19 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 16:07:26 visual_prompt]: Epoch 23 / 100: avg data time: 5.40e-02, avg batch time: 0.4987, average train loss: 1.6968
[09/26 16:07:28 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1677, average loss: 1.7637
[09/26 16:07:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 94.00	
[09/26 16:07:28 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 16:07:34 visual_prompt]: Epoch 24 / 100: avg data time: 5.29e-02, avg batch time: 0.4972, average train loss: 1.5567
[09/26 16:07:36 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1674, average loss: 1.8630
[09/26 16:07:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 89.00	
[09/26 16:07:36 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 16:07:43 visual_prompt]: Epoch 25 / 100: avg data time: 5.41e-02, avg batch time: 0.4996, average train loss: 1.4670
[09/26 16:07:44 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1677, average loss: 2.2734
[09/26 16:07:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 80.00	
[09/26 16:07:44 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 16:07:51 visual_prompt]: Epoch 26 / 100: avg data time: 4.78e-02, avg batch time: 0.4952, average train loss: 1.8208
[09/26 16:07:53 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1674, average loss: 1.7379
[09/26 16:07:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 91.00	
[09/26 16:07:53 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 16:07:59 visual_prompt]: Epoch 27 / 100: avg data time: 5.11e-02, avg batch time: 0.4962, average train loss: 1.4629
[09/26 16:08:01 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1675, average loss: 1.8325
[09/26 16:08:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 90.50	
[09/26 16:08:01 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 16:08:08 visual_prompt]: Epoch 28 / 100: avg data time: 5.89e-02, avg batch time: 0.5027, average train loss: 2.6547
[09/26 16:08:09 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1679, average loss: 2.1053
[09/26 16:08:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 74.50	
[09/26 16:08:09 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 16:08:16 visual_prompt]: Epoch 29 / 100: avg data time: 4.82e-02, avg batch time: 0.4944, average train loss: 1.8578
[09/26 16:08:18 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1674, average loss: 1.8654
[09/26 16:08:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 86.50	
[09/26 16:08:18 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 16:08:25 visual_prompt]: Epoch 30 / 100: avg data time: 5.54e-02, avg batch time: 0.4994, average train loss: 1.7819
[09/26 16:08:26 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1671, average loss: 1.9552
[09/26 16:08:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 87.00	
[09/26 16:08:26 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 16:08:33 visual_prompt]: Epoch 31 / 100: avg data time: 5.07e-02, avg batch time: 0.4952, average train loss: 1.5967
[09/26 16:08:35 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1679, average loss: 2.0101
[09/26 16:08:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 85.00	
[09/26 16:08:35 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 16:08:42 visual_prompt]: Epoch 32 / 100: avg data time: 5.09e-02, avg batch time: 0.4993, average train loss: 1.5719
[09/26 16:08:43 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1673, average loss: 2.0940
[09/26 16:08:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 85.50	
[09/26 16:08:43 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 16:08:50 visual_prompt]: Epoch 33 / 100: avg data time: 4.59e-02, avg batch time: 0.4902, average train loss: 1.5491
[09/26 16:08:51 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1676, average loss: 1.6875
[09/26 16:08:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 93.00	
[09/26 16:08:51 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 16:08:58 visual_prompt]: Epoch 34 / 100: avg data time: 5.54e-02, avg batch time: 0.4999, average train loss: 1.3913
[09/26 16:09:00 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1680, average loss: 1.9024
[09/26 16:09:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 94.50	
[09/26 16:09:00 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 16:09:07 visual_prompt]: Epoch 35 / 100: avg data time: 5.22e-02, avg batch time: 0.4991, average train loss: 1.4912
[09/26 16:09:08 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1673, average loss: 1.6750
[09/26 16:09:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 91.50	
[09/26 16:09:08 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 16:09:15 visual_prompt]: Epoch 36 / 100: avg data time: 4.52e-02, avg batch time: 0.4905, average train loss: 1.4414
[09/26 16:09:16 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1673, average loss: 1.9656
[09/26 16:09:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 16:09:16 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 16:09:23 visual_prompt]: Epoch 37 / 100: avg data time: 4.69e-02, avg batch time: 0.4932, average train loss: 1.4730
[09/26 16:09:25 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1676, average loss: 2.2299
[09/26 16:09:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 81.00	
[09/26 16:09:25 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 16:09:32 visual_prompt]: Epoch 38 / 100: avg data time: 5.34e-02, avg batch time: 0.4983, average train loss: 1.5033
[09/26 16:09:33 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1672, average loss: 1.8092
[09/26 16:09:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 93.00	
[09/26 16:09:33 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 16:09:40 visual_prompt]: Epoch 39 / 100: avg data time: 4.98e-02, avg batch time: 0.4953, average train loss: 1.4832
[09/26 16:09:42 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1678, average loss: 1.9418
[09/26 16:09:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 87.50	
[09/26 16:09:42 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 16:09:49 visual_prompt]: Epoch 40 / 100: avg data time: 5.90e-02, avg batch time: 0.5051, average train loss: 1.2490
[09/26 16:09:50 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1676, average loss: 2.0138
[09/26 16:09:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 90.00	
[09/26 16:09:50 visual_prompt]: Best epoch 40: best metric: 0.295
[09/26 16:09:50 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 16:09:57 visual_prompt]: Epoch 41 / 100: avg data time: 4.69e-02, avg batch time: 0.4918, average train loss: 1.3266
[09/26 16:09:59 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1675, average loss: 1.5876
[09/26 16:09:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 93.00	
[09/26 16:09:59 visual_prompt]: Best epoch 41: best metric: 0.360
[09/26 16:09:59 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 16:10:05 visual_prompt]: Epoch 42 / 100: avg data time: 4.78e-02, avg batch time: 0.4961, average train loss: 1.4096
[09/26 16:10:07 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1676, average loss: 1.7442
[09/26 16:10:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 92.00	
[09/26 16:10:07 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 16:10:14 visual_prompt]: Epoch 43 / 100: avg data time: 4.78e-02, avg batch time: 0.4925, average train loss: 1.2899
[09/26 16:10:15 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1673, average loss: 1.8562
[09/26 16:10:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 92.50	
[09/26 16:10:15 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 16:10:22 visual_prompt]: Epoch 44 / 100: avg data time: 4.67e-02, avg batch time: 0.4915, average train loss: 1.2166
[09/26 16:10:24 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1678, average loss: 1.7546
[09/26 16:10:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 94.50	
[09/26 16:10:24 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 16:10:31 visual_prompt]: Epoch 45 / 100: avg data time: 5.26e-02, avg batch time: 0.4976, average train loss: 1.1561
[09/26 16:10:32 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1677, average loss: 2.4163
[09/26 16:10:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 85.50	
[09/26 16:10:32 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 16:10:39 visual_prompt]: Epoch 46 / 100: avg data time: 4.91e-02, avg batch time: 0.4957, average train loss: 1.1635
[09/26 16:10:40 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1676, average loss: 1.9089
[09/26 16:10:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 94.00	
[09/26 16:10:40 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 16:10:47 visual_prompt]: Epoch 47 / 100: avg data time: 5.08e-02, avg batch time: 0.4960, average train loss: 1.1819
[09/26 16:10:49 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1676, average loss: 1.7046
[09/26 16:10:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.50	
[09/26 16:10:49 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 16:10:56 visual_prompt]: Epoch 48 / 100: avg data time: 5.52e-02, avg batch time: 0.4997, average train loss: 1.1278
[09/26 16:10:57 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1675, average loss: 1.9939
[09/26 16:10:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 89.50	
[09/26 16:10:57 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 16:11:04 visual_prompt]: Epoch 49 / 100: avg data time: 5.54e-02, avg batch time: 0.4994, average train loss: 1.0438
[09/26 16:11:06 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.1677, average loss: 1.8445
[09/26 16:11:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 96.50	
[09/26 16:11:06 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 16:11:13 visual_prompt]: Epoch 50 / 100: avg data time: 6.04e-02, avg batch time: 0.5040, average train loss: 1.4464
[09/26 16:11:14 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1677, average loss: 1.6160
[09/26 16:11:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 92.50	
[09/26 16:11:14 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 16:11:21 visual_prompt]: Epoch 51 / 100: avg data time: 5.25e-02, avg batch time: 0.4982, average train loss: 1.1847
[09/26 16:11:23 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1680, average loss: 1.9046
[09/26 16:11:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 93.00	
[09/26 16:11:23 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 16:11:30 visual_prompt]: Epoch 52 / 100: avg data time: 5.38e-02, avg batch time: 0.4984, average train loss: 1.0776
[09/26 16:11:31 visual_prompt]: Inference (val):avg data time: 4.09e-05, avg batch time: 0.1674, average loss: 1.9057
[09/26 16:11:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 94.50	
[09/26 16:11:31 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 16:11:38 visual_prompt]: Epoch 53 / 100: avg data time: 4.96e-02, avg batch time: 0.4949, average train loss: 1.0421
[09/26 16:11:40 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1677, average loss: 2.0394
[09/26 16:11:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 91.50	
[09/26 16:11:40 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 16:11:46 visual_prompt]: Epoch 54 / 100: avg data time: 5.00e-02, avg batch time: 0.4949, average train loss: 0.9415
[09/26 16:11:48 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1674, average loss: 1.7131
[09/26 16:11:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.50	top5: 96.50	
[09/26 16:11:48 visual_prompt]: Best epoch 54: best metric: 0.385
[09/26 16:11:48 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 16:11:55 visual_prompt]: Epoch 55 / 100: avg data time: 5.52e-02, avg batch time: 0.5005, average train loss: 0.8134
[09/26 16:11:56 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.1677, average loss: 2.6631
[09/26 16:11:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.00	
[09/26 16:11:56 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 16:12:03 visual_prompt]: Epoch 56 / 100: avg data time: 6.44e-02, avg batch time: 0.5086, average train loss: 0.9181
[09/26 16:12:05 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1674, average loss: 1.9880
[09/26 16:12:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 94.50	
[09/26 16:12:05 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 16:12:12 visual_prompt]: Epoch 57 / 100: avg data time: 4.65e-02, avg batch time: 0.4915, average train loss: 1.4886
[09/26 16:12:13 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1674, average loss: 1.5450
[09/26 16:12:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 96.00	
[09/26 16:12:13 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 16:12:20 visual_prompt]: Epoch 58 / 100: avg data time: 6.07e-02, avg batch time: 0.5038, average train loss: 1.1494
[09/26 16:12:22 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1675, average loss: 1.6924
[09/26 16:12:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 93.50	
[09/26 16:12:22 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 16:12:29 visual_prompt]: Epoch 59 / 100: avg data time: 6.08e-02, avg batch time: 0.5057, average train loss: 0.9049
[09/26 16:12:30 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1674, average loss: 2.2065
[09/26 16:12:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 94.00	
[09/26 16:12:30 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 16:12:37 visual_prompt]: Epoch 60 / 100: avg data time: 4.56e-02, avg batch time: 0.4924, average train loss: 0.8557
[09/26 16:12:39 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1674, average loss: 2.2813
[09/26 16:12:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 95.50	
[09/26 16:12:39 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 16:12:46 visual_prompt]: Epoch 61 / 100: avg data time: 4.74e-02, avg batch time: 0.4933, average train loss: 0.8288
[09/26 16:12:47 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1674, average loss: 2.8556
[09/26 16:12:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.00	
[09/26 16:12:47 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 16:12:54 visual_prompt]: Epoch 62 / 100: avg data time: 5.11e-02, avg batch time: 0.4971, average train loss: 0.8159
[09/26 16:12:56 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1675, average loss: 2.3156
[09/26 16:12:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 94.00	
[09/26 16:12:56 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 16:13:02 visual_prompt]: Epoch 63 / 100: avg data time: 4.70e-02, avg batch time: 0.4926, average train loss: 0.7861
[09/26 16:13:04 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1678, average loss: 2.3480
[09/26 16:13:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 94.50	
[09/26 16:13:04 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 16:13:11 visual_prompt]: Epoch 64 / 100: avg data time: 5.00e-02, avg batch time: 0.4951, average train loss: 0.6114
[09/26 16:13:12 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1674, average loss: 2.9064
[09/26 16:13:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 95.00	
[09/26 16:13:12 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 16:13:19 visual_prompt]: Epoch 65 / 100: avg data time: 5.39e-02, avg batch time: 0.4993, average train loss: 0.7400
[09/26 16:13:21 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1677, average loss: 2.2287
[09/26 16:13:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.50	
[09/26 16:13:21 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 16:13:28 visual_prompt]: Epoch 66 / 100: avg data time: 5.30e-02, avg batch time: 0.4969, average train loss: 0.7827
[09/26 16:13:29 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1672, average loss: 2.3189
[09/26 16:13:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.50	
[09/26 16:13:29 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 16:13:36 visual_prompt]: Epoch 67 / 100: avg data time: 4.39e-02, avg batch time: 0.4897, average train loss: 0.5761
[09/26 16:13:38 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1677, average loss: 2.8618
[09/26 16:13:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 95.00	
[09/26 16:13:38 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 16:13:44 visual_prompt]: Epoch 68 / 100: avg data time: 4.57e-02, avg batch time: 0.4925, average train loss: 0.7277
[09/26 16:13:46 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1676, average loss: 2.9546
[09/26 16:13:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 90.00	
[09/26 16:13:46 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 16:13:53 visual_prompt]: Epoch 69 / 100: avg data time: 5.44e-02, avg batch time: 0.4992, average train loss: 0.8133
[09/26 16:13:54 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1678, average loss: 2.2608
[09/26 16:13:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.00	
[09/26 16:13:54 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 16:14:01 visual_prompt]: Epoch 70 / 100: avg data time: 4.83e-02, avg batch time: 0.4955, average train loss: 0.5724
[09/26 16:14:03 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1681, average loss: 2.6660
[09/26 16:14:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 94.50	
[09/26 16:14:03 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 16:14:10 visual_prompt]: Epoch 71 / 100: avg data time: 5.46e-02, avg batch time: 0.5001, average train loss: 0.4137
[09/26 16:14:11 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1682, average loss: 2.9574
[09/26 16:14:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.50	
[09/26 16:14:11 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 16:14:18 visual_prompt]: Epoch 72 / 100: avg data time: 4.64e-02, avg batch time: 0.4950, average train loss: 0.4299
[09/26 16:14:19 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1684, average loss: 3.0663
[09/26 16:14:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 97.00	
[09/26 16:14:19 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 16:14:26 visual_prompt]: Epoch 73 / 100: avg data time: 4.68e-02, avg batch time: 0.4949, average train loss: 0.3659
[09/26 16:14:28 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1688, average loss: 2.9426
[09/26 16:14:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 97.00	
[09/26 16:14:28 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 16:14:35 visual_prompt]: Epoch 74 / 100: avg data time: 6.40e-02, avg batch time: 0.5132, average train loss: 0.3317
[09/26 16:14:36 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1693, average loss: 3.4684
[09/26 16:14:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 94.00	
[09/26 16:14:36 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 16:14:43 visual_prompt]: Epoch 75 / 100: avg data time: 5.24e-02, avg batch time: 0.5039, average train loss: 0.3311
[09/26 16:14:45 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1699, average loss: 3.3098
[09/26 16:14:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 93.00	
[09/26 16:14:45 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 16:14:52 visual_prompt]: Epoch 76 / 100: avg data time: 6.00e-02, avg batch time: 0.5118, average train loss: 0.3216
[09/26 16:14:54 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1705, average loss: 3.5480
[09/26 16:14:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 92.00	
[09/26 16:14:54 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 16:15:00 visual_prompt]: Epoch 77 / 100: avg data time: 4.87e-02, avg batch time: 0.5029, average train loss: 0.3298
[09/26 16:15:02 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1714, average loss: 3.5158
[09/26 16:15:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 92.00	
[09/26 16:15:02 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 16:15:09 visual_prompt]: Epoch 78 / 100: avg data time: 4.82e-02, avg batch time: 0.5051, average train loss: 0.2680
[09/26 16:15:11 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1715, average loss: 3.4757
[09/26 16:15:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 93.50	
[09/26 16:15:11 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 16:15:18 visual_prompt]: Epoch 79 / 100: avg data time: 5.29e-02, avg batch time: 0.5102, average train loss: 0.2216
[09/26 16:15:19 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1729, average loss: 3.6301
[09/26 16:15:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 92.00	
[09/26 16:15:19 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 16:15:26 visual_prompt]: Epoch 80 / 100: avg data time: 5.07e-02, avg batch time: 0.5112, average train loss: 0.1779
[09/26 16:15:28 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1733, average loss: 3.5641
[09/26 16:15:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 93.50	
[09/26 16:15:28 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 16:15:35 visual_prompt]: Epoch 81 / 100: avg data time: 5.38e-02, avg batch time: 0.5137, average train loss: 0.1078
[09/26 16:15:36 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1735, average loss: 3.5423
[09/26 16:15:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 96.00	
[09/26 16:15:36 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 16:15:43 visual_prompt]: Epoch 82 / 100: avg data time: 4.82e-02, avg batch time: 0.5101, average train loss: 0.0782
[09/26 16:15:45 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1737, average loss: 3.6129
[09/26 16:15:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 95.50	
[09/26 16:15:45 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 16:15:52 visual_prompt]: Epoch 83 / 100: avg data time: 5.68e-02, avg batch time: 0.5189, average train loss: 0.0700
[09/26 16:15:54 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1739, average loss: 3.9419
[09/26 16:15:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 93.50	
[09/26 16:15:54 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 16:16:01 visual_prompt]: Epoch 84 / 100: avg data time: 5.16e-02, avg batch time: 0.5121, average train loss: 0.0483
[09/26 16:16:02 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1735, average loss: 3.8614
[09/26 16:16:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 95.00	
[09/26 16:16:02 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 16:16:09 visual_prompt]: Epoch 85 / 100: avg data time: 5.01e-02, avg batch time: 0.5113, average train loss: 0.0367
[09/26 16:16:11 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1735, average loss: 3.9041
[09/26 16:16:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 95.50	
[09/26 16:16:11 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 16:16:18 visual_prompt]: Epoch 86 / 100: avg data time: 6.01e-02, avg batch time: 0.5202, average train loss: 0.0340
[09/26 16:16:20 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1734, average loss: 4.1209
[09/26 16:16:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.50	
[09/26 16:16:20 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 16:16:27 visual_prompt]: Epoch 87 / 100: avg data time: 5.57e-02, avg batch time: 0.5154, average train loss: 0.0272
[09/26 16:16:28 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1726, average loss: 3.9161
[09/26 16:16:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 97.00	
[09/26 16:16:28 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 16:16:35 visual_prompt]: Epoch 88 / 100: avg data time: 5.34e-02, avg batch time: 0.5120, average train loss: 0.0266
[09/26 16:16:37 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1719, average loss: 4.1955
[09/26 16:16:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 94.50	
[09/26 16:16:37 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 16:16:44 visual_prompt]: Epoch 89 / 100: avg data time: 5.43e-02, avg batch time: 0.5121, average train loss: 0.0252
[09/26 16:16:45 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1723, average loss: 4.0656
[09/26 16:16:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 96.00	
[09/26 16:16:45 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 16:16:52 visual_prompt]: Epoch 90 / 100: avg data time: 4.68e-02, avg batch time: 0.5062, average train loss: 0.0218
[09/26 16:16:54 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1717, average loss: 4.2002
[09/26 16:16:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.50	
[09/26 16:16:54 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 16:17:01 visual_prompt]: Epoch 91 / 100: avg data time: 6.03e-02, avg batch time: 0.5168, average train loss: 0.0191
[09/26 16:17:03 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1713, average loss: 4.1075
[09/26 16:17:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 96.00	
[09/26 16:17:03 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 16:17:10 visual_prompt]: Epoch 92 / 100: avg data time: 4.66e-02, avg batch time: 0.5047, average train loss: 0.0160
[09/26 16:17:11 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1710, average loss: 4.1806
[09/26 16:17:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 95.50	
[09/26 16:17:11 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 16:17:18 visual_prompt]: Epoch 93 / 100: avg data time: 4.98e-02, avg batch time: 0.5038, average train loss: 0.0182
[09/26 16:17:20 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1709, average loss: 4.1603
[09/26 16:17:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 96.00	
[09/26 16:17:20 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 16:17:26 visual_prompt]: Epoch 94 / 100: avg data time: 5.78e-02, avg batch time: 0.5112, average train loss: 0.0191
[09/26 16:17:28 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1705, average loss: 4.1830
[09/26 16:17:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 95.50	
[09/26 16:17:28 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 16:17:35 visual_prompt]: Epoch 95 / 100: avg data time: 4.96e-02, avg batch time: 0.5026, average train loss: 0.0148
[09/26 16:17:37 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1705, average loss: 4.2545
[09/26 16:17:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 95.00	
[09/26 16:17:37 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 16:17:44 visual_prompt]: Epoch 96 / 100: avg data time: 6.26e-02, avg batch time: 0.5143, average train loss: 0.0145
[09/26 16:17:45 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1702, average loss: 4.2676
[09/26 16:17:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.00	
[09/26 16:17:45 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 16:17:52 visual_prompt]: Epoch 97 / 100: avg data time: 5.48e-02, avg batch time: 0.5058, average train loss: 0.0181
[09/26 16:17:54 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1703, average loss: 4.2443
[09/26 16:17:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.00	
[09/26 16:17:54 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 16:18:01 visual_prompt]: Epoch 98 / 100: avg data time: 5.12e-02, avg batch time: 0.5026, average train loss: 0.0149
[09/26 16:18:02 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1698, average loss: 4.2387
[09/26 16:18:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.00	
[09/26 16:18:02 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 16:18:09 visual_prompt]: Epoch 99 / 100: avg data time: 4.72e-02, avg batch time: 0.5003, average train loss: 0.0132
[09/26 16:18:10 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1698, average loss: 4.2371
[09/26 16:18:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.00	
[09/26 16:18:10 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 16:18:17 visual_prompt]: Epoch 100 / 100: avg data time: 4.88e-02, avg batch time: 0.4992, average train loss: 0.0139
[09/26 16:18:19 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1692, average loss: 4.2361
[09/26 16:18:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.00	
[09/26 16:18:19 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 16:18:19 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 16:18:19 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 16:18:19 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 16:18:19 visual_prompt]: Training with config:
[09/26 16:18:19 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 16:18:19 visual_prompt]: Loading training data...
[09/26 16:18:19 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 16:18:20 visual_prompt]: Number of images: 800
[09/26 16:18:20 visual_prompt]: Number of classes: 9 / 9
[09/26 16:18:20 visual_prompt]: Loading validation data...
[09/26 16:18:20 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 16:18:20 visual_prompt]: Number of images: 200
[09/26 16:18:20 visual_prompt]: Number of classes: 9 / 9
[09/26 16:18:20 visual_prompt]: Constructing models...
[09/26 16:18:23 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 16:18:23 visual_prompt]: tuned percent:0.542
[09/26 16:18:23 visual_prompt]: Device used for model: 0
[09/26 16:18:23 visual_prompt]: Setting up Evaluator...
[09/26 16:18:23 visual_prompt]: Setting up Trainer...
[09/26 16:18:23 visual_prompt]: 	Setting up the optimizer...
[09/26 16:18:23 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 16:18:30 visual_prompt]: Epoch 1 / 100: avg data time: 4.83e-02, avg batch time: 0.4958, average train loss: 2.8714
[09/26 16:18:31 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1682, average loss: 2.9516
[09/26 16:18:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 16:18:31 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 16:18:31 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 16:18:38 visual_prompt]: Epoch 2 / 100: avg data time: 5.49e-02, avg batch time: 0.5009, average train loss: 2.8622
[09/26 16:18:40 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1682, average loss: 2.4053
[09/26 16:18:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.50	top5: 56.00	
[09/26 16:18:40 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 16:18:47 visual_prompt]: Epoch 3 / 100: avg data time: 4.77e-02, avg batch time: 0.4943, average train loss: 2.2711
[09/26 16:18:48 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1683, average loss: 2.1991
[09/26 16:18:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/26 16:18:48 visual_prompt]: Best epoch 3: best metric: 0.145
[09/26 16:18:48 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 16:18:55 visual_prompt]: Epoch 4 / 100: avg data time: 5.21e-02, avg batch time: 0.4997, average train loss: 2.2311
[09/26 16:18:56 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1683, average loss: 2.2747
[09/26 16:18:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 54.50	
[09/26 16:18:56 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 16:19:03 visual_prompt]: Epoch 5 / 100: avg data time: 5.79e-02, avg batch time: 0.5039, average train loss: 2.2259
[09/26 16:19:05 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1679, average loss: 2.2194
[09/26 16:19:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 64.50	
[09/26 16:19:05 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 16:19:12 visual_prompt]: Epoch 6 / 100: avg data time: 5.63e-02, avg batch time: 0.5037, average train loss: 2.2190
[09/26 16:19:13 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1682, average loss: 2.3408
[09/26 16:19:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 54.00	
[09/26 16:19:13 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 16:19:20 visual_prompt]: Epoch 7 / 100: avg data time: 4.81e-02, avg batch time: 0.4933, average train loss: 2.2050
[09/26 16:19:22 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1677, average loss: 2.2318
[09/26 16:19:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 59.00	
[09/26 16:19:22 visual_prompt]: Best epoch 7: best metric: 0.155
[09/26 16:19:22 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 16:19:29 visual_prompt]: Epoch 8 / 100: avg data time: 5.00e-02, avg batch time: 0.4963, average train loss: 2.2035
[09/26 16:19:30 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1684, average loss: 2.1248
[09/26 16:19:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 67.50	
[09/26 16:19:30 visual_prompt]: Best epoch 8: best metric: 0.200
[09/26 16:19:30 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 16:19:37 visual_prompt]: Epoch 9 / 100: avg data time: 4.76e-02, avg batch time: 0.4938, average train loss: 2.0473
[09/26 16:19:38 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1678, average loss: 2.1601
[09/26 16:19:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 70.00	
[09/26 16:19:38 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 16:19:45 visual_prompt]: Epoch 10 / 100: avg data time: 5.34e-02, avg batch time: 0.5016, average train loss: 2.0477
[09/26 16:19:47 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1679, average loss: 2.0296
[09/26 16:19:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 81.00	
[09/26 16:19:47 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 16:19:54 visual_prompt]: Epoch 11 / 100: avg data time: 5.92e-02, avg batch time: 0.5048, average train loss: 2.0085
[09/26 16:19:55 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1677, average loss: 2.1132
[09/26 16:19:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 74.00	
[09/26 16:19:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 16:20:02 visual_prompt]: Epoch 12 / 100: avg data time: 5.39e-02, avg batch time: 0.4993, average train loss: 2.0225
[09/26 16:20:04 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1677, average loss: 2.0501
[09/26 16:20:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 80.50	
[09/26 16:20:04 visual_prompt]: Best epoch 12: best metric: 0.215
[09/26 16:20:04 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 16:20:10 visual_prompt]: Epoch 13 / 100: avg data time: 5.12e-02, avg batch time: 0.4963, average train loss: 1.8249
[09/26 16:20:12 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1676, average loss: 1.9780
[09/26 16:20:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 83.50	
[09/26 16:20:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 16:20:19 visual_prompt]: Epoch 14 / 100: avg data time: 5.64e-02, avg batch time: 0.5034, average train loss: 1.7910
[09/26 16:20:21 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1680, average loss: 2.0476
[09/26 16:20:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 84.50	
[09/26 16:20:21 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 16:20:28 visual_prompt]: Epoch 15 / 100: avg data time: 6.50e-02, avg batch time: 0.5108, average train loss: 1.6457
[09/26 16:20:29 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1675, average loss: 1.7970
[09/26 16:20:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 90.50	
[09/26 16:20:29 visual_prompt]: Best epoch 15: best metric: 0.235
[09/26 16:20:29 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 16:20:36 visual_prompt]: Epoch 16 / 100: avg data time: 6.72e-02, avg batch time: 0.5120, average train loss: 1.6262
[09/26 16:20:38 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1674, average loss: 2.4122
[09/26 16:20:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 80.50	
[09/26 16:20:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 16:20:45 visual_prompt]: Epoch 17 / 100: avg data time: 5.86e-02, avg batch time: 0.5043, average train loss: 1.5792
[09/26 16:20:46 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1676, average loss: 1.8319
[09/26 16:20:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 93.00	
[09/26 16:20:46 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 16:20:53 visual_prompt]: Epoch 18 / 100: avg data time: 5.07e-02, avg batch time: 0.4949, average train loss: 1.4897
[09/26 16:20:55 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1684, average loss: 1.8131
[09/26 16:20:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 89.50	
[09/26 16:20:55 visual_prompt]: Best epoch 18: best metric: 0.305
[09/26 16:20:55 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 16:21:01 visual_prompt]: Epoch 19 / 100: avg data time: 4.79e-02, avg batch time: 0.4960, average train loss: 1.4221
[09/26 16:21:03 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1676, average loss: 1.8228
[09/26 16:21:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 92.50	
[09/26 16:21:03 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 16:21:10 visual_prompt]: Epoch 20 / 100: avg data time: 5.41e-02, avg batch time: 0.4997, average train loss: 1.4817
[09/26 16:21:11 visual_prompt]: Inference (val):avg data time: 2.14e-05, avg batch time: 0.1675, average loss: 1.6902
[09/26 16:21:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 92.50	
[09/26 16:21:11 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 16:21:18 visual_prompt]: Epoch 21 / 100: avg data time: 4.95e-02, avg batch time: 0.4975, average train loss: 1.4028
[09/26 16:21:20 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1675, average loss: 1.9909
[09/26 16:21:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 91.00	
[09/26 16:21:20 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 16:21:26 visual_prompt]: Epoch 22 / 100: avg data time: 5.95e-02, avg batch time: 0.5050, average train loss: 1.3749
[09/26 16:21:28 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1675, average loss: 2.6699
[09/26 16:21:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 81.50	
[09/26 16:21:28 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 16:21:35 visual_prompt]: Epoch 23 / 100: avg data time: 4.73e-02, avg batch time: 0.4929, average train loss: 1.4763
[09/26 16:21:36 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1679, average loss: 1.8656
[09/26 16:21:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 91.00	
[09/26 16:21:36 visual_prompt]: Best epoch 23: best metric: 0.310
[09/26 16:21:36 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 16:21:43 visual_prompt]: Epoch 24 / 100: avg data time: 6.22e-02, avg batch time: 0.5063, average train loss: 1.2673
[09/26 16:21:45 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1675, average loss: 1.7845
[09/26 16:21:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 16:21:45 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 16:21:52 visual_prompt]: Epoch 25 / 100: avg data time: 6.14e-02, avg batch time: 0.5065, average train loss: 1.1112
[09/26 16:21:53 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1675, average loss: 2.0165
[09/26 16:21:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 16:21:53 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 16:22:00 visual_prompt]: Epoch 26 / 100: avg data time: 4.92e-02, avg batch time: 0.4951, average train loss: 1.1336
[09/26 16:22:02 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1680, average loss: 2.6661
[09/26 16:22:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 83.50	
[09/26 16:22:02 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 16:22:09 visual_prompt]: Epoch 27 / 100: avg data time: 5.57e-02, avg batch time: 0.5009, average train loss: 1.1295
[09/26 16:22:10 visual_prompt]: Inference (val):avg data time: 1.98e-05, avg batch time: 0.1673, average loss: 2.1123
[09/26 16:22:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 93.50	
[09/26 16:22:10 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 16:22:17 visual_prompt]: Epoch 28 / 100: avg data time: 5.66e-02, avg batch time: 0.5007, average train loss: 1.0759
[09/26 16:22:19 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1676, average loss: 2.0267
[09/26 16:22:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.00	
[09/26 16:22:19 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 16:22:26 visual_prompt]: Epoch 29 / 100: avg data time: 4.97e-02, avg batch time: 0.4950, average train loss: 0.9734
[09/26 16:22:27 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1679, average loss: 2.1704
[09/26 16:22:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 94.00	
[09/26 16:22:27 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 16:22:34 visual_prompt]: Epoch 30 / 100: avg data time: 5.72e-02, avg batch time: 0.5022, average train loss: 0.9343
[09/26 16:22:36 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1674, average loss: 2.2384
[09/26 16:22:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 90.50	
[09/26 16:22:36 visual_prompt]: Best epoch 30: best metric: 0.315
[09/26 16:22:36 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 16:22:42 visual_prompt]: Epoch 31 / 100: avg data time: 4.94e-02, avg batch time: 0.4948, average train loss: 0.8922
[09/26 16:22:44 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1678, average loss: 2.0950
[09/26 16:22:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 95.00	
[09/26 16:22:44 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 16:22:51 visual_prompt]: Epoch 32 / 100: avg data time: 4.73e-02, avg batch time: 0.4930, average train loss: 0.8925
[09/26 16:22:52 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1676, average loss: 2.2357
[09/26 16:22:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 16:22:52 visual_prompt]: Best epoch 32: best metric: 0.340
[09/26 16:22:52 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 16:22:59 visual_prompt]: Epoch 33 / 100: avg data time: 4.55e-02, avg batch time: 0.4934, average train loss: 0.9116
[09/26 16:23:01 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1676, average loss: 2.0511
[09/26 16:23:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 93.50	
[09/26 16:23:01 visual_prompt]: Best epoch 33: best metric: 0.360
[09/26 16:23:01 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 16:23:07 visual_prompt]: Epoch 34 / 100: avg data time: 4.75e-02, avg batch time: 0.4920, average train loss: 0.8722
[09/26 16:23:09 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1676, average loss: 2.4708
[09/26 16:23:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 90.50	
[09/26 16:23:09 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 16:23:16 visual_prompt]: Epoch 35 / 100: avg data time: 4.75e-02, avg batch time: 0.4937, average train loss: 0.7062
[09/26 16:23:17 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1673, average loss: 2.6452
[09/26 16:23:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 94.50	
[09/26 16:23:17 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 16:23:24 visual_prompt]: Epoch 36 / 100: avg data time: 6.61e-02, avg batch time: 0.5106, average train loss: 0.6838
[09/26 16:23:26 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1679, average loss: 2.9962
[09/26 16:23:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 90.50	
[09/26 16:23:26 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 16:23:33 visual_prompt]: Epoch 37 / 100: avg data time: 5.21e-02, avg batch time: 0.4983, average train loss: 0.7113
[09/26 16:23:35 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1677, average loss: 3.1314
[09/26 16:23:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 89.00	
[09/26 16:23:35 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 16:23:41 visual_prompt]: Epoch 38 / 100: avg data time: 6.02e-02, avg batch time: 0.5053, average train loss: 0.6925
[09/26 16:23:43 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1675, average loss: 2.8550
[09/26 16:23:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 91.50	
[09/26 16:23:43 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 16:23:50 visual_prompt]: Epoch 39 / 100: avg data time: 4.82e-02, avg batch time: 0.4931, average train loss: 0.6734
[09/26 16:23:51 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1678, average loss: 3.2192
[09/26 16:23:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 88.50	
[09/26 16:23:51 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 16:23:58 visual_prompt]: Epoch 40 / 100: avg data time: 4.84e-02, avg batch time: 0.4941, average train loss: 0.5437
[09/26 16:24:00 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1676, average loss: 2.6902
[09/26 16:24:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.50	
[09/26 16:24:00 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 16:24:07 visual_prompt]: Epoch 41 / 100: avg data time: 4.92e-02, avg batch time: 0.4966, average train loss: 0.4582
[09/26 16:24:08 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1676, average loss: 3.0542
[09/26 16:24:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 94.50	
[09/26 16:24:08 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 16:24:15 visual_prompt]: Epoch 42 / 100: avg data time: 5.20e-02, avg batch time: 0.4975, average train loss: 0.4978
[09/26 16:24:17 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1679, average loss: 3.7762
[09/26 16:24:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 89.00	
[09/26 16:24:17 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 16:24:23 visual_prompt]: Epoch 43 / 100: avg data time: 5.03e-02, avg batch time: 0.4948, average train loss: 0.5831
[09/26 16:24:25 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1677, average loss: 2.9682
[09/26 16:24:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 93.00	
[09/26 16:24:25 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 16:24:32 visual_prompt]: Epoch 44 / 100: avg data time: 4.87e-02, avg batch time: 0.4962, average train loss: 0.4490
[09/26 16:24:33 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1674, average loss: 2.9002
[09/26 16:24:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 93.50	
[09/26 16:24:33 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 16:24:40 visual_prompt]: Epoch 45 / 100: avg data time: 5.15e-02, avg batch time: 0.4970, average train loss: 0.3944
[09/26 16:24:42 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1679, average loss: 3.2412
[09/26 16:24:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 93.00	
[09/26 16:24:42 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 16:24:49 visual_prompt]: Epoch 46 / 100: avg data time: 6.25e-02, avg batch time: 0.5077, average train loss: 0.2978
[09/26 16:24:50 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1679, average loss: 3.8111
[09/26 16:24:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 90.50	
[09/26 16:24:50 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 16:24:57 visual_prompt]: Epoch 47 / 100: avg data time: 6.71e-02, avg batch time: 0.5128, average train loss: 0.3894
[09/26 16:24:59 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1676, average loss: 4.2721
[09/26 16:24:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 89.00	
[09/26 16:24:59 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 16:25:06 visual_prompt]: Epoch 48 / 100: avg data time: 5.55e-02, avg batch time: 0.5009, average train loss: 0.5222
[09/26 16:25:07 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1677, average loss: 3.8345
[09/26 16:25:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 91.00	
[09/26 16:25:07 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 16:25:14 visual_prompt]: Epoch 49 / 100: avg data time: 5.96e-02, avg batch time: 0.5046, average train loss: 0.4882
[09/26 16:25:16 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1677, average loss: 3.3886
[09/26 16:25:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 92.50	
[09/26 16:25:16 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 16:25:22 visual_prompt]: Epoch 50 / 100: avg data time: 5.74e-02, avg batch time: 0.5027, average train loss: 0.3760
[09/26 16:25:24 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1677, average loss: 3.7420
[09/26 16:25:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 88.00	
[09/26 16:25:24 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 16:25:31 visual_prompt]: Epoch 51 / 100: avg data time: 4.93e-02, avg batch time: 0.4973, average train loss: 0.3593
[09/26 16:25:33 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1677, average loss: 3.3414
[09/26 16:25:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 93.50	
[09/26 16:25:33 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 16:25:39 visual_prompt]: Epoch 52 / 100: avg data time: 5.75e-02, avg batch time: 0.5038, average train loss: 0.2273
[09/26 16:25:41 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1675, average loss: 3.4586
[09/26 16:25:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 93.50	
[09/26 16:25:41 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 16:25:48 visual_prompt]: Epoch 53 / 100: avg data time: 6.17e-02, avg batch time: 0.5072, average train loss: 0.1606
[09/26 16:25:50 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1674, average loss: 3.9153
[09/26 16:25:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 93.50	
[09/26 16:25:50 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 16:25:56 visual_prompt]: Epoch 54 / 100: avg data time: 5.04e-02, avg batch time: 0.4963, average train loss: 0.1406
[09/26 16:25:58 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1678, average loss: 3.6273
[09/26 16:25:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 94.00	
[09/26 16:25:58 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 16:26:05 visual_prompt]: Epoch 55 / 100: avg data time: 5.08e-02, avg batch time: 0.4953, average train loss: 0.1329
[09/26 16:26:06 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1681, average loss: 4.0159
[09/26 16:26:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 93.50	
[09/26 16:26:06 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 16:26:13 visual_prompt]: Epoch 56 / 100: avg data time: 6.26e-02, avg batch time: 0.5073, average train loss: 0.1709
[09/26 16:26:15 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1676, average loss: 4.0718
[09/26 16:26:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 94.50	
[09/26 16:26:15 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 16:26:22 visual_prompt]: Epoch 57 / 100: avg data time: 5.21e-02, avg batch time: 0.4992, average train loss: 0.1647
[09/26 16:26:23 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1677, average loss: 4.6574
[09/26 16:26:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 91.50	
[09/26 16:26:23 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 16:26:30 visual_prompt]: Epoch 58 / 100: avg data time: 4.89e-02, avg batch time: 0.4964, average train loss: 0.1959
[09/26 16:26:32 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1677, average loss: 3.9406
[09/26 16:26:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 93.50	
[09/26 16:26:32 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 16:26:39 visual_prompt]: Epoch 59 / 100: avg data time: 4.70e-02, avg batch time: 0.4945, average train loss: 0.1497
[09/26 16:26:40 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1673, average loss: 3.9318
[09/26 16:26:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.50	
[09/26 16:26:40 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 16:26:47 visual_prompt]: Epoch 60 / 100: avg data time: 4.84e-02, avg batch time: 0.4962, average train loss: 0.1132
[09/26 16:26:49 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1677, average loss: 4.5061
[09/26 16:26:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 93.00	
[09/26 16:26:49 visual_prompt]: Best epoch 60: best metric: 0.390
[09/26 16:26:49 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 16:26:55 visual_prompt]: Epoch 61 / 100: avg data time: 4.78e-02, avg batch time: 0.4936, average train loss: 0.0882
[09/26 16:26:57 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1677, average loss: 4.4855
[09/26 16:26:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 92.00	
[09/26 16:26:57 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 16:27:04 visual_prompt]: Epoch 62 / 100: avg data time: 6.30e-02, avg batch time: 0.5087, average train loss: 0.0871
[09/26 16:27:05 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1678, average loss: 4.5750
[09/26 16:27:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 92.50	
[09/26 16:27:05 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 16:27:12 visual_prompt]: Epoch 63 / 100: avg data time: 4.59e-02, avg batch time: 0.4923, average train loss: 0.0701
[09/26 16:27:14 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1681, average loss: 4.4441
[09/26 16:27:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 92.00	
[09/26 16:27:14 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 16:27:21 visual_prompt]: Epoch 64 / 100: avg data time: 5.32e-02, avg batch time: 0.4983, average train loss: 0.0890
[09/26 16:27:22 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1676, average loss: 4.6655
[09/26 16:27:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 94.00	
[09/26 16:27:22 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 16:27:29 visual_prompt]: Epoch 65 / 100: avg data time: 5.76e-02, avg batch time: 0.5033, average train loss: 0.1068
[09/26 16:27:31 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1677, average loss: 4.3583
[09/26 16:27:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 92.00	
[09/26 16:27:31 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 16:27:37 visual_prompt]: Epoch 66 / 100: avg data time: 5.02e-02, avg batch time: 0.4951, average train loss: 0.1000
[09/26 16:27:39 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1680, average loss: 4.2361
[09/26 16:27:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 94.00	
[09/26 16:27:39 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 16:27:46 visual_prompt]: Epoch 67 / 100: avg data time: 5.28e-02, avg batch time: 0.5011, average train loss: 0.0665
[09/26 16:27:47 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1676, average loss: 4.7708
[09/26 16:27:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 91.00	
[09/26 16:27:47 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 16:27:54 visual_prompt]: Epoch 68 / 100: avg data time: 4.96e-02, avg batch time: 0.4950, average train loss: 0.0560
[09/26 16:27:56 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1679, average loss: 4.7387
[09/26 16:27:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 93.00	
[09/26 16:27:56 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 16:28:03 visual_prompt]: Epoch 69 / 100: avg data time: 5.07e-02, avg batch time: 0.4968, average train loss: 0.0358
[09/26 16:28:04 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1678, average loss: 4.8827
[09/26 16:28:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 93.50	
[09/26 16:28:04 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 16:28:11 visual_prompt]: Epoch 70 / 100: avg data time: 5.08e-02, avg batch time: 0.4974, average train loss: 0.0295
[09/26 16:28:13 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1680, average loss: 4.8962
[09/26 16:28:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 94.00	
[09/26 16:28:13 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 16:28:19 visual_prompt]: Epoch 71 / 100: avg data time: 4.85e-02, avg batch time: 0.4942, average train loss: 0.0187
[09/26 16:28:21 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1675, average loss: 4.8477
[09/26 16:28:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 94.50	
[09/26 16:28:21 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 16:28:28 visual_prompt]: Epoch 72 / 100: avg data time: 4.89e-02, avg batch time: 0.4931, average train loss: 0.0090
[09/26 16:28:29 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1682, average loss: 4.9278
[09/26 16:28:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 95.00	
[09/26 16:28:29 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 16:28:36 visual_prompt]: Epoch 73 / 100: avg data time: 4.79e-02, avg batch time: 0.4931, average train loss: 0.0080
[09/26 16:28:38 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1676, average loss: 5.0229
[09/26 16:28:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 16:28:38 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 16:28:44 visual_prompt]: Epoch 74 / 100: avg data time: 4.96e-02, avg batch time: 0.4943, average train loss: 0.0110
[09/26 16:28:46 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1679, average loss: 5.2396
[09/26 16:28:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 93.50	
[09/26 16:28:46 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 16:28:53 visual_prompt]: Epoch 75 / 100: avg data time: 5.85e-02, avg batch time: 0.5055, average train loss: 0.0068
[09/26 16:28:54 visual_prompt]: Inference (val):avg data time: 3.44e-05, avg batch time: 0.1681, average loss: 5.4399
[09/26 16:28:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.00	
[09/26 16:28:54 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 16:29:01 visual_prompt]: Epoch 76 / 100: avg data time: 5.35e-02, avg batch time: 0.4998, average train loss: 0.0068
[09/26 16:29:03 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1678, average loss: 5.3073
[09/26 16:29:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.50	
[09/26 16:29:03 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 16:29:10 visual_prompt]: Epoch 77 / 100: avg data time: 5.73e-02, avg batch time: 0.5015, average train loss: 0.0052
[09/26 16:29:11 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1678, average loss: 5.2703
[09/26 16:29:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.50	
[09/26 16:29:11 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 16:29:18 visual_prompt]: Epoch 78 / 100: avg data time: 5.79e-02, avg batch time: 0.5025, average train loss: 0.0037
[09/26 16:29:20 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1679, average loss: 5.2902
[09/26 16:29:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.50	
[09/26 16:29:20 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 16:29:27 visual_prompt]: Epoch 79 / 100: avg data time: 4.74e-02, avg batch time: 0.4942, average train loss: 0.0071
[09/26 16:29:28 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1676, average loss: 5.4109
[09/26 16:29:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 16:29:28 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 16:29:35 visual_prompt]: Epoch 80 / 100: avg data time: 4.83e-02, avg batch time: 0.4935, average train loss: 0.0048
[09/26 16:29:37 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1679, average loss: 5.4786
[09/26 16:29:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 94.00	
[09/26 16:29:37 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 16:29:43 visual_prompt]: Epoch 81 / 100: avg data time: 5.48e-02, avg batch time: 0.4998, average train loss: 0.0043
[09/26 16:29:45 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1675, average loss: 5.4648
[09/26 16:29:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 94.00	
[09/26 16:29:45 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 16:29:52 visual_prompt]: Epoch 82 / 100: avg data time: 4.40e-02, avg batch time: 0.4936, average train loss: 0.0037
[09/26 16:29:53 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1680, average loss: 5.4202
[09/26 16:29:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 94.00	
[09/26 16:29:53 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 16:30:00 visual_prompt]: Epoch 83 / 100: avg data time: 4.45e-02, avg batch time: 0.4922, average train loss: 0.0041
[09/26 16:30:02 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1676, average loss: 5.3732
[09/26 16:30:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 94.50	
[09/26 16:30:02 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 16:30:08 visual_prompt]: Epoch 84 / 100: avg data time: 4.62e-02, avg batch time: 0.4934, average train loss: 0.0026
[09/26 16:30:10 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1682, average loss: 5.3375
[09/26 16:30:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 94.50	
[09/26 16:30:10 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 16:30:17 visual_prompt]: Epoch 85 / 100: avg data time: 5.97e-02, avg batch time: 0.5048, average train loss: 0.0029
[09/26 16:30:18 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1674, average loss: 5.3223
[09/26 16:30:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 16:30:18 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 16:30:25 visual_prompt]: Epoch 86 / 100: avg data time: 5.69e-02, avg batch time: 0.5026, average train loss: 0.0047
[09/26 16:30:27 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1676, average loss: 5.3255
[09/26 16:30:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.50	
[09/26 16:30:27 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 16:30:34 visual_prompt]: Epoch 87 / 100: avg data time: 5.16e-02, avg batch time: 0.4996, average train loss: 0.0035
[09/26 16:30:35 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1676, average loss: 5.3530
[09/26 16:30:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.00	
[09/26 16:30:35 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 16:30:42 visual_prompt]: Epoch 88 / 100: avg data time: 5.59e-02, avg batch time: 0.5010, average train loss: 0.0044
[09/26 16:30:44 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1679, average loss: 5.3695
[09/26 16:30:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.00	
[09/26 16:30:44 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 16:30:51 visual_prompt]: Epoch 89 / 100: avg data time: 5.90e-02, avg batch time: 0.5045, average train loss: 0.0037
[09/26 16:30:52 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1674, average loss: 5.3771
[09/26 16:30:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.00	
[09/26 16:30:52 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 16:30:59 visual_prompt]: Epoch 90 / 100: avg data time: 5.11e-02, avg batch time: 0.4968, average train loss: 0.0021
[09/26 16:31:01 visual_prompt]: Inference (val):avg data time: 3.80e-05, avg batch time: 0.1676, average loss: 5.3866
[09/26 16:31:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.00	
[09/26 16:31:01 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 16:31:07 visual_prompt]: Epoch 91 / 100: avg data time: 5.00e-02, avg batch time: 0.4959, average train loss: 0.0042
[09/26 16:31:09 visual_prompt]: Inference (val):avg data time: 2.11e-05, avg batch time: 0.1678, average loss: 5.3962
[09/26 16:31:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.00	
[09/26 16:31:09 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 16:31:16 visual_prompt]: Epoch 92 / 100: avg data time: 4.92e-02, avg batch time: 0.4940, average train loss: 0.0026
[09/26 16:31:17 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1678, average loss: 5.3984
[09/26 16:31:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.00	
[09/26 16:31:17 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 16:31:24 visual_prompt]: Epoch 93 / 100: avg data time: 4.77e-02, avg batch time: 0.4923, average train loss: 0.0028
[09/26 16:31:26 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1676, average loss: 5.3975
[09/26 16:31:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.00	
[09/26 16:31:26 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 16:31:33 visual_prompt]: Epoch 94 / 100: avg data time: 5.92e-02, avg batch time: 0.5042, average train loss: 0.0024
[09/26 16:31:34 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1679, average loss: 5.3984
[09/26 16:31:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.00	
[09/26 16:31:34 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 16:31:41 visual_prompt]: Epoch 95 / 100: avg data time: 5.33e-02, avg batch time: 0.4983, average train loss: 0.0026
[09/26 16:31:43 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1678, average loss: 5.3985
[09/26 16:31:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.00	
[09/26 16:31:43 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 16:31:50 visual_prompt]: Epoch 96 / 100: avg data time: 5.85e-02, avg batch time: 0.5043, average train loss: 0.0030
[09/26 16:31:51 visual_prompt]: Inference (val):avg data time: 2.26e-05, avg batch time: 0.1679, average loss: 5.3993
[09/26 16:31:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.00	
[09/26 16:31:51 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 16:31:58 visual_prompt]: Epoch 97 / 100: avg data time: 4.99e-02, avg batch time: 0.4957, average train loss: 0.0028
[09/26 16:31:59 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1678, average loss: 5.3998
[09/26 16:31:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 16:31:59 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 16:32:06 visual_prompt]: Epoch 98 / 100: avg data time: 4.87e-02, avg batch time: 0.4936, average train loss: 0.0045
[09/26 16:32:08 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1678, average loss: 5.3991
[09/26 16:32:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 16:32:08 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 16:32:15 visual_prompt]: Epoch 99 / 100: avg data time: 5.01e-02, avg batch time: 0.4982, average train loss: 0.0028
[09/26 16:32:16 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1677, average loss: 5.3989
[09/26 16:32:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 16:32:16 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 16:32:23 visual_prompt]: Epoch 100 / 100: avg data time: 5.21e-02, avg batch time: 0.4973, average train loss: 0.0027
[09/26 16:32:25 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1676, average loss: 5.3989
[09/26 16:32:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.00	
[09/26 16:32:25 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 16:32:25 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 16:32:25 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 16:32:25 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 16:32:25 visual_prompt]: Training with config:
[09/26 16:32:25 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 16:32:25 visual_prompt]: Loading training data...
[09/26 16:32:25 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 16:32:26 visual_prompt]: Number of images: 800
[09/26 16:32:26 visual_prompt]: Number of classes: 9 / 9
[09/26 16:32:26 visual_prompt]: Loading validation data...
[09/26 16:32:26 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 16:32:26 visual_prompt]: Number of images: 200
[09/26 16:32:26 visual_prompt]: Number of classes: 9 / 9
[09/26 16:32:26 visual_prompt]: Constructing models...
[09/26 16:32:28 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 16:32:28 visual_prompt]: tuned percent:0.542
[09/26 16:32:29 visual_prompt]: Device used for model: 0
[09/26 16:32:29 visual_prompt]: Setting up Evaluator...
[09/26 16:32:29 visual_prompt]: Setting up Trainer...
[09/26 16:32:29 visual_prompt]: 	Setting up the optimizer...
[09/26 16:32:29 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 16:32:35 visual_prompt]: Epoch 1 / 100: avg data time: 5.18e-02, avg batch time: 0.4943, average train loss: 2.8714
[09/26 16:32:37 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1671, average loss: 2.9516
[09/26 16:32:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 16:32:37 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 16:32:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.05
[09/26 16:32:44 visual_prompt]: Epoch 2 / 100: avg data time: 4.95e-02, avg batch time: 0.4937, average train loss: 2.8281
[09/26 16:32:45 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1673, average loss: 2.3556
[09/26 16:32:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/26 16:32:45 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.1
[09/26 16:32:52 visual_prompt]: Epoch 3 / 100: avg data time: 4.60e-02, avg batch time: 0.4919, average train loss: 2.2708
[09/26 16:32:53 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1671, average loss: 2.2273
[09/26 16:32:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 16:32:53 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.15
[09/26 16:33:00 visual_prompt]: Epoch 4 / 100: avg data time: 4.93e-02, avg batch time: 0.4929, average train loss: 2.2552
[09/26 16:33:02 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1674, average loss: 2.2844
[09/26 16:33:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 49.50	
[09/26 16:33:02 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.2
[09/26 16:33:08 visual_prompt]: Epoch 5 / 100: avg data time: 5.15e-02, avg batch time: 0.4966, average train loss: 2.2299
[09/26 16:33:10 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1673, average loss: 2.2227
[09/26 16:33:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 55.50	
[09/26 16:33:10 visual_prompt]: Best epoch 5: best metric: 0.140
[09/26 16:33:10 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.25
[09/26 16:33:17 visual_prompt]: Epoch 6 / 100: avg data time: 6.15e-02, avg batch time: 0.5048, average train loss: 2.2219
[09/26 16:33:19 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1675, average loss: 2.2032
[09/26 16:33:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 60.00	
[09/26 16:33:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.3
[09/26 16:33:25 visual_prompt]: Epoch 7 / 100: avg data time: 4.87e-02, avg batch time: 0.4931, average train loss: 2.2130
[09/26 16:33:27 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1673, average loss: 2.2944
[09/26 16:33:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 60.00	
[09/26 16:33:27 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.35
[09/26 16:33:34 visual_prompt]: Epoch 8 / 100: avg data time: 5.72e-02, avg batch time: 0.5034, average train loss: 2.1955
[09/26 16:33:35 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1672, average loss: 2.1229
[09/26 16:33:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 70.00	
[09/26 16:33:35 visual_prompt]: Best epoch 8: best metric: 0.185
[09/26 16:33:35 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.4
[09/26 16:33:42 visual_prompt]: Epoch 9 / 100: avg data time: 6.13e-02, avg batch time: 0.5066, average train loss: 2.0876
[09/26 16:33:44 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1676, average loss: 2.0260
[09/26 16:33:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 76.50	
[09/26 16:33:44 visual_prompt]: Best epoch 9: best metric: 0.205
[09/26 16:33:44 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.45
[09/26 16:33:51 visual_prompt]: Epoch 10 / 100: avg data time: 5.19e-02, avg batch time: 0.4978, average train loss: 2.0053
[09/26 16:33:52 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1679, average loss: 1.9849
[09/26 16:33:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 77.00	
[09/26 16:33:52 visual_prompt]: Best epoch 10: best metric: 0.230
[09/26 16:33:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.5
[09/26 16:33:59 visual_prompt]: Epoch 11 / 100: avg data time: 4.75e-02, avg batch time: 0.4941, average train loss: 2.1150
[09/26 16:34:01 visual_prompt]: Inference (val):avg data time: 1.94e-05, avg batch time: 0.1676, average loss: 2.2544
[09/26 16:34:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 74.00	
[09/26 16:34:01 visual_prompt]: Best epoch 11: best metric: 0.250
[09/26 16:34:01 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.49984770675477397
[09/26 16:34:07 visual_prompt]: Epoch 12 / 100: avg data time: 5.20e-02, avg batch time: 0.4984, average train loss: 1.9947
[09/26 16:34:09 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1677, average loss: 2.0845
[09/26 16:34:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 81.50	
[09/26 16:34:09 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.49939101256495605
[09/26 16:34:16 visual_prompt]: Epoch 13 / 100: avg data time: 4.60e-02, avg batch time: 0.4916, average train loss: 1.8522
[09/26 16:34:17 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1679, average loss: 2.0923
[09/26 16:34:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 84.50	
[09/26 16:34:17 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.4986304738420683
[09/26 16:34:24 visual_prompt]: Epoch 14 / 100: avg data time: 4.65e-02, avg batch time: 0.4921, average train loss: 1.8530
[09/26 16:34:26 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1681, average loss: 1.8355
[09/26 16:34:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 89.00	
[09/26 16:34:26 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.4975670171853926
[09/26 16:34:33 visual_prompt]: Epoch 15 / 100: avg data time: 5.29e-02, avg batch time: 0.4977, average train loss: 1.7798
[09/26 16:34:34 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1677, average loss: 1.8993
[09/26 16:34:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 90.00	
[09/26 16:34:34 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.49620193825305203
[09/26 16:34:41 visual_prompt]: Epoch 16 / 100: avg data time: 5.62e-02, avg batch time: 0.5010, average train loss: 1.6230
[09/26 16:34:43 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1675, average loss: 1.6806
[09/26 16:34:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 91.50	
[09/26 16:34:43 visual_prompt]: Best epoch 16: best metric: 0.300
[09/26 16:34:43 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.4945369001834514
[09/26 16:34:50 visual_prompt]: Epoch 17 / 100: avg data time: 5.58e-02, avg batch time: 0.5033, average train loss: 1.5248
[09/26 16:34:51 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1676, average loss: 1.8076
[09/26 16:34:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 94.50	
[09/26 16:34:51 visual_prompt]: Best epoch 17: best metric: 0.305
[09/26 16:34:51 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.4925739315689991
[09/26 16:34:58 visual_prompt]: Epoch 18 / 100: avg data time: 4.74e-02, avg batch time: 0.4935, average train loss: 1.6480
[09/26 16:34:59 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1674, average loss: 1.8088
[09/26 16:34:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 91.50	
[09/26 16:34:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.4903154239845797
[09/26 16:35:06 visual_prompt]: Epoch 19 / 100: avg data time: 5.73e-02, avg batch time: 0.5037, average train loss: 1.3983
[09/26 16:35:08 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1677, average loss: 1.9184
[09/26 16:35:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 92.00	
[09/26 16:35:08 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.4877641290737884
[09/26 16:35:15 visual_prompt]: Epoch 20 / 100: avg data time: 5.88e-02, avg batch time: 0.5034, average train loss: 1.3610
[09/26 16:35:16 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1680, average loss: 3.4862
[09/26 16:35:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 71.00	
[09/26 16:35:16 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.4849231551964771
[09/26 16:35:23 visual_prompt]: Epoch 21 / 100: avg data time: 5.96e-02, avg batch time: 0.5055, average train loss: 1.7079
[09/26 16:35:25 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1677, average loss: 1.8212
[09/26 16:35:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 90.00	
[09/26 16:35:25 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.48179596364169686
[09/26 16:35:32 visual_prompt]: Epoch 22 / 100: avg data time: 4.70e-02, avg batch time: 0.4915, average train loss: 1.4135
[09/26 16:35:33 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1676, average loss: 1.6314
[09/26 16:35:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 95.50	
[09/26 16:35:33 visual_prompt]: Best epoch 22: best metric: 0.320
[09/26 16:35:33 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.4783863644106502
[09/26 16:35:40 visual_prompt]: Epoch 23 / 100: avg data time: 4.77e-02, avg batch time: 0.4942, average train loss: 1.2329
[09/26 16:35:42 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1674, average loss: 1.6645
[09/26 16:35:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 95.50	
[09/26 16:35:42 visual_prompt]: Best epoch 23: best metric: 0.325
[09/26 16:35:42 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.47469851157479176
[09/26 16:35:48 visual_prompt]: Epoch 24 / 100: avg data time: 5.30e-02, avg batch time: 0.4999, average train loss: 1.1987
[09/26 16:35:50 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1676, average loss: 1.7992
[09/26 16:35:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 94.00	
[09/26 16:35:50 visual_prompt]: Best epoch 24: best metric: 0.360
[09/26 16:35:50 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.47073689821473175
[09/26 16:35:57 visual_prompt]: Epoch 25 / 100: avg data time: 5.73e-02, avg batch time: 0.5024, average train loss: 1.2692
[09/26 16:35:58 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1678, average loss: 1.8000
[09/26 16:35:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 96.00	
[09/26 16:35:58 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.4665063509461097
[09/26 16:36:05 visual_prompt]: Epoch 26 / 100: avg data time: 4.78e-02, avg batch time: 0.4930, average train loss: 1.2756
[09/26 16:36:07 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1677, average loss: 2.0174
[09/26 16:36:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 89.50	
[09/26 16:36:07 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.46201202403910646
[09/26 16:36:14 visual_prompt]: Epoch 27 / 100: avg data time: 4.68e-02, avg batch time: 0.4917, average train loss: 1.2317
[09/26 16:36:15 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1672, average loss: 1.9203
[09/26 16:36:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 93.00	
[09/26 16:36:15 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.4572593931387604
[09/26 16:36:22 visual_prompt]: Epoch 28 / 100: avg data time: 6.92e-02, avg batch time: 0.5129, average train loss: 1.0931
[09/26 16:36:24 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1675, average loss: 1.7717
[09/26 16:36:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 96.00	
[09/26 16:36:24 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.45225424859373686
[09/26 16:36:30 visual_prompt]: Epoch 29 / 100: avg data time: 4.94e-02, avg batch time: 0.4947, average train loss: 0.9564
[09/26 16:36:32 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1672, average loss: 2.2931
[09/26 16:36:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 92.50	
[09/26 16:36:32 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.44700268840168045
[09/26 16:36:39 visual_prompt]: Epoch 30 / 100: avg data time: 4.63e-02, avg batch time: 0.4913, average train loss: 1.1190
[09/26 16:36:40 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1678, average loss: 2.9617
[09/26 16:36:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.00	
[09/26 16:36:40 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.4415111107797445
[09/26 16:36:47 visual_prompt]: Epoch 31 / 100: avg data time: 5.11e-02, avg batch time: 0.4973, average train loss: 1.0825
[09/26 16:36:49 visual_prompt]: Inference (val):avg data time: 2.19e-05, avg batch time: 0.1672, average loss: 2.3023
[09/26 16:36:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 90.50	
[09/26 16:36:49 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.43578620636934856
[09/26 16:36:56 visual_prompt]: Epoch 32 / 100: avg data time: 4.85e-02, avg batch time: 0.4942, average train loss: 0.9363
[09/26 16:36:57 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1675, average loss: 2.1511
[09/26 16:36:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 94.50	
[09/26 16:36:57 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.42983495008466277
[09/26 16:37:04 visual_prompt]: Epoch 33 / 100: avg data time: 6.41e-02, avg batch time: 0.5103, average train loss: 0.8857
[09/26 16:37:06 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1676, average loss: 2.2983
[09/26 16:37:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 94.00	
[09/26 16:37:06 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.4236645926147493
[09/26 16:37:12 visual_prompt]: Epoch 34 / 100: avg data time: 5.01e-02, avg batch time: 0.4976, average train loss: 0.9006
[09/26 16:37:14 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1674, average loss: 2.3530
[09/26 16:37:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 93.50	
[09/26 16:37:14 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.41728265158971456
[09/26 16:37:21 visual_prompt]: Epoch 35 / 100: avg data time: 5.49e-02, avg batch time: 0.4987, average train loss: 0.8876
[09/26 16:37:22 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1671, average loss: 2.6456
[09/26 16:37:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 91.50	
[09/26 16:37:22 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.4106969024216348
[09/26 16:37:29 visual_prompt]: Epoch 36 / 100: avg data time: 6.28e-02, avg batch time: 0.5086, average train loss: 1.0412
[09/26 16:37:31 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1673, average loss: 2.1415
[09/26 16:37:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 95.50	
[09/26 16:37:31 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.40391536883141455
[09/26 16:37:38 visual_prompt]: Epoch 37 / 100: avg data time: 5.36e-02, avg batch time: 0.4980, average train loss: 0.8156
[09/26 16:37:39 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1675, average loss: 2.1706
[09/26 16:37:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 96.00	
[09/26 16:37:39 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.3969463130731183
[09/26 16:37:46 visual_prompt]: Epoch 38 / 100: avg data time: 6.45e-02, avg batch time: 0.5082, average train loss: 0.6060
[09/26 16:37:48 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1676, average loss: 2.3650
[09/26 16:37:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 94.50	
[09/26 16:37:48 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.3897982258676867
[09/26 16:37:55 visual_prompt]: Epoch 39 / 100: avg data time: 4.75e-02, avg batch time: 0.4934, average train loss: 0.6224
[09/26 16:37:56 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1673, average loss: 2.6006
[09/26 16:37:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 95.00	
[09/26 16:37:56 visual_prompt]: Best epoch 39: best metric: 0.370
[09/26 16:37:56 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.3824798160583012
[09/26 16:38:03 visual_prompt]: Epoch 40 / 100: avg data time: 5.79e-02, avg batch time: 0.5041, average train loss: 0.5869
[09/26 16:38:05 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1674, average loss: 3.3521
[09/26 16:38:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 89.50	
[09/26 16:38:05 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.375
[09/26 16:38:12 visual_prompt]: Epoch 41 / 100: avg data time: 5.22e-02, avg batch time: 0.4962, average train loss: 0.6614
[09/26 16:38:13 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1673, average loss: 3.1378
[09/26 16:38:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 93.50	
[09/26 16:38:13 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.3673678906964727
[09/26 16:38:20 visual_prompt]: Epoch 42 / 100: avg data time: 4.62e-02, avg batch time: 0.4934, average train loss: 0.6918
[09/26 16:38:22 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1672, average loss: 3.2476
[09/26 16:38:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 91.00	
[09/26 16:38:22 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.35959278669726935
[09/26 16:38:28 visual_prompt]: Epoch 43 / 100: avg data time: 5.37e-02, avg batch time: 0.5002, average train loss: 0.6825
[09/26 16:38:30 visual_prompt]: Inference (val):avg data time: 2.12e-05, avg batch time: 0.1671, average loss: 2.9419
[09/26 16:38:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 91.00	
[09/26 16:38:30 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.3516841607689501
[09/26 16:38:37 visual_prompt]: Epoch 44 / 100: avg data time: 4.53e-02, avg batch time: 0.4908, average train loss: 0.4801
[09/26 16:38:38 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1675, average loss: 2.8951
[09/26 16:38:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 94.50	
[09/26 16:38:38 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.34365164835397805
[09/26 16:38:45 visual_prompt]: Epoch 45 / 100: avg data time: 6.26e-02, avg batch time: 0.5079, average train loss: 0.3638
[09/26 16:38:47 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1673, average loss: 3.1157
[09/26 16:38:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 93.00	
[09/26 16:38:47 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.3355050358314172
[09/26 16:38:54 visual_prompt]: Epoch 46 / 100: avg data time: 5.83e-02, avg batch time: 0.5050, average train loss: 0.4020
[09/26 16:38:56 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1676, average loss: 3.2665
[09/26 16:38:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 96.00	
[09/26 16:38:56 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.32725424859373686
[09/26 16:39:02 visual_prompt]: Epoch 47 / 100: avg data time: 6.43e-02, avg batch time: 0.5099, average train loss: 0.3923
[09/26 16:39:04 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1672, average loss: 3.4648
[09/26 16:39:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 94.00	
[09/26 16:39:04 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.3189093389542498
[09/26 16:39:11 visual_prompt]: Epoch 48 / 100: avg data time: 5.92e-02, avg batch time: 0.5034, average train loss: 0.3472
[09/26 16:39:13 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1674, average loss: 3.3825
[09/26 16:39:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 94.50	
[09/26 16:39:13 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.3104804738999169
[09/26 16:39:19 visual_prompt]: Epoch 49 / 100: avg data time: 6.15e-02, avg batch time: 0.5063, average train loss: 0.2936
[09/26 16:39:21 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1676, average loss: 3.9402
[09/26 16:39:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 92.00	
[09/26 16:39:21 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.3019779227044398
[09/26 16:39:28 visual_prompt]: Epoch 50 / 100: avg data time: 6.47e-02, avg batch time: 0.5086, average train loss: 0.3871
[09/26 16:39:30 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1675, average loss: 3.6434
[09/26 16:39:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 93.50	
[09/26 16:39:30 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.29341204441673263
[09/26 16:39:37 visual_prompt]: Epoch 51 / 100: avg data time: 5.89e-02, avg batch time: 0.5042, average train loss: 0.3327
[09/26 16:39:38 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1671, average loss: 3.0627
[09/26 16:39:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 97.50	
[09/26 16:39:38 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.28479327524001635
[09/26 16:39:45 visual_prompt]: Epoch 52 / 100: avg data time: 6.22e-02, avg batch time: 0.5069, average train loss: 0.2780
[09/26 16:39:47 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1673, average loss: 4.4021
[09/26 16:39:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 90.50	
[09/26 16:39:47 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.2761321158169134
[09/26 16:39:54 visual_prompt]: Epoch 53 / 100: avg data time: 6.67e-02, avg batch time: 0.5111, average train loss: 0.3129
[09/26 16:39:55 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1672, average loss: 3.4109
[09/26 16:39:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.50	
[09/26 16:39:55 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.2674391184360313
[09/26 16:40:02 visual_prompt]: Epoch 54 / 100: avg data time: 5.10e-02, avg batch time: 0.4953, average train loss: 0.2986
[09/26 16:40:04 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1677, average loss: 3.4420
[09/26 16:40:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 95.00	
[09/26 16:40:04 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.2587248741756253
[09/26 16:40:11 visual_prompt]: Epoch 55 / 100: avg data time: 6.62e-02, avg batch time: 0.5100, average train loss: 0.3995
[09/26 16:40:13 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1673, average loss: 4.0202
[09/26 16:40:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 91.00	
[09/26 16:40:13 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.25
[09/26 16:40:20 visual_prompt]: Epoch 56 / 100: avg data time: 6.14e-02, avg batch time: 0.5065, average train loss: 0.3421
[09/26 16:40:21 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1673, average loss: 3.5361
[09/26 16:40:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 16:40:21 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.24127512582437483
[09/26 16:40:28 visual_prompt]: Epoch 57 / 100: avg data time: 6.36e-02, avg batch time: 0.5086, average train loss: 0.1941
[09/26 16:40:30 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1674, average loss: 3.5572
[09/26 16:40:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 95.00	
[09/26 16:40:30 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.23256088156396867
[09/26 16:40:37 visual_prompt]: Epoch 58 / 100: avg data time: 6.23e-02, avg batch time: 0.5056, average train loss: 0.1483
[09/26 16:40:38 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1675, average loss: 4.0535
[09/26 16:40:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 92.50	
[09/26 16:40:38 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.22386788418308667
[09/26 16:40:45 visual_prompt]: Epoch 59 / 100: avg data time: 5.67e-02, avg batch time: 0.5010, average train loss: 0.1039
[09/26 16:40:47 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1673, average loss: 4.1174
[09/26 16:40:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 94.00	
[09/26 16:40:47 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.2152067247599837
[09/26 16:40:54 visual_prompt]: Epoch 60 / 100: avg data time: 5.50e-02, avg batch time: 0.5003, average train loss: 0.0942
[09/26 16:40:55 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1675, average loss: 4.6509
[09/26 16:40:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 91.50	
[09/26 16:40:55 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.20658795558326742
[09/26 16:41:02 visual_prompt]: Epoch 61 / 100: avg data time: 5.84e-02, avg batch time: 0.5035, average train loss: 0.0847
[09/26 16:41:04 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1671, average loss: 4.6448
[09/26 16:41:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 93.50	
[09/26 16:41:04 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.1980220772955602
[09/26 16:41:11 visual_prompt]: Epoch 62 / 100: avg data time: 5.10e-02, avg batch time: 0.4966, average train loss: 0.0672
[09/26 16:41:12 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1672, average loss: 4.9228
[09/26 16:41:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 93.00	
[09/26 16:41:12 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.1895195261000831
[09/26 16:41:19 visual_prompt]: Epoch 63 / 100: avg data time: 6.06e-02, avg batch time: 0.5048, average train loss: 0.0912
[09/26 16:41:21 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1673, average loss: 4.8406
[09/26 16:41:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.00	
[09/26 16:41:21 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.18109066104575022
[09/26 16:41:28 visual_prompt]: Epoch 64 / 100: avg data time: 6.07e-02, avg batch time: 0.5058, average train loss: 0.0759
[09/26 16:41:30 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1672, average loss: 4.7493
[09/26 16:41:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 92.50	
[09/26 16:41:30 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.17274575140626316
[09/26 16:41:36 visual_prompt]: Epoch 65 / 100: avg data time: 5.25e-02, avg batch time: 0.4956, average train loss: 0.0822
[09/26 16:41:38 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1675, average loss: 4.7750
[09/26 16:41:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 94.00	
[09/26 16:41:38 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.16449496416858284
[09/26 16:41:45 visual_prompt]: Epoch 66 / 100: avg data time: 4.78e-02, avg batch time: 0.4953, average train loss: 0.0736
[09/26 16:41:46 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1675, average loss: 5.2594
[09/26 16:41:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 93.50	
[09/26 16:41:46 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.15634835164602198
[09/26 16:41:53 visual_prompt]: Epoch 67 / 100: avg data time: 6.53e-02, avg batch time: 0.5102, average train loss: 0.0728
[09/26 16:41:55 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.1674, average loss: 4.7739
[09/26 16:41:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 97.00	
[09/26 16:41:55 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.14831583923105
[09/26 16:42:02 visual_prompt]: Epoch 68 / 100: avg data time: 6.25e-02, avg batch time: 0.5062, average train loss: 0.0561
[09/26 16:42:04 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1674, average loss: 5.3453
[09/26 16:42:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 94.00	
[09/26 16:42:04 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.14040721330273062
[09/26 16:42:10 visual_prompt]: Epoch 69 / 100: avg data time: 5.68e-02, avg batch time: 0.5018, average train loss: 0.0785
[09/26 16:42:12 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1676, average loss: 5.1113
[09/26 16:42:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 95.50	
[09/26 16:42:12 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.13263210930352737
[09/26 16:42:19 visual_prompt]: Epoch 70 / 100: avg data time: 6.46e-02, avg batch time: 0.5094, average train loss: 0.0593
[09/26 16:42:21 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1673, average loss: 4.4982
[09/26 16:42:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 97.00	
[09/26 16:42:21 visual_prompt]: Best epoch 70: best metric: 0.375
[09/26 16:42:21 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.12500000000000006
[09/26 16:42:28 visual_prompt]: Epoch 71 / 100: avg data time: 6.40e-02, avg batch time: 0.5082, average train loss: 0.0619
[09/26 16:42:29 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.1673, average loss: 5.5261
[09/26 16:42:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 93.50	
[09/26 16:42:29 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.1175201839416988
[09/26 16:42:36 visual_prompt]: Epoch 72 / 100: avg data time: 7.62e-02, avg batch time: 0.5199, average train loss: 0.0534
[09/26 16:42:38 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1677, average loss: 5.1674
[09/26 16:42:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 95.00	
[09/26 16:42:38 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.11020177413231333
[09/26 16:42:45 visual_prompt]: Epoch 73 / 100: avg data time: 5.91e-02, avg batch time: 0.5042, average train loss: 0.0285
[09/26 16:42:46 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1674, average loss: 5.4382
[09/26 16:42:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 93.00	
[09/26 16:42:46 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.10305368692688174
[09/26 16:42:53 visual_prompt]: Epoch 74 / 100: avg data time: 5.90e-02, avg batch time: 0.5041, average train loss: 0.0222
[09/26 16:42:55 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1674, average loss: 5.4196
[09/26 16:42:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 94.50	
[09/26 16:42:55 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.09608463116858543
[09/26 16:43:02 visual_prompt]: Epoch 75 / 100: avg data time: 6.01e-02, avg batch time: 0.5038, average train loss: 0.0253
[09/26 16:43:04 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1675, average loss: 5.6101
[09/26 16:43:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 94.00	
[09/26 16:43:04 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.08930309757836516
[09/26 16:43:11 visual_prompt]: Epoch 76 / 100: avg data time: 6.70e-02, avg batch time: 0.5107, average train loss: 0.0240
[09/26 16:43:12 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1673, average loss: 5.8018
[09/26 16:43:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 93.50	
[09/26 16:43:12 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.08271734841028552
[09/26 16:43:19 visual_prompt]: Epoch 77 / 100: avg data time: 6.71e-02, avg batch time: 0.5108, average train loss: 0.0155
[09/26 16:43:21 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1678, average loss: 5.8357
[09/26 16:43:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 93.00	
[09/26 16:43:21 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.07633540738525066
[09/26 16:43:28 visual_prompt]: Epoch 78 / 100: avg data time: 6.11e-02, avg batch time: 0.5071, average train loss: 0.0222
[09/26 16:43:29 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1675, average loss: 5.5868
[09/26 16:43:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 93.00	
[09/26 16:43:29 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.07016504991533726
[09/26 16:43:36 visual_prompt]: Epoch 79 / 100: avg data time: 5.78e-02, avg batch time: 0.5024, average train loss: 0.0344
[09/26 16:43:38 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1671, average loss: 5.7800
[09/26 16:43:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 92.50	
[09/26 16:43:38 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.06421379363065141
[09/26 16:43:45 visual_prompt]: Epoch 80 / 100: avg data time: 5.02e-02, avg batch time: 0.4956, average train loss: 0.0183
[09/26 16:43:46 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1673, average loss: 6.2823
[09/26 16:43:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 93.50	
[09/26 16:43:46 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.058488889220255524
[09/26 16:43:53 visual_prompt]: Epoch 81 / 100: avg data time: 6.53e-02, avg batch time: 0.5098, average train loss: 0.0165
[09/26 16:43:55 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1675, average loss: 5.9441
[09/26 16:43:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 93.50	
[09/26 16:43:55 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.052997311598319524
[09/26 16:44:02 visual_prompt]: Epoch 82 / 100: avg data time: 5.57e-02, avg batch time: 0.5015, average train loss: 0.0111
[09/26 16:44:03 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1671, average loss: 5.9071
[09/26 16:44:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 94.00	
[09/26 16:44:03 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.047745751406263165
[09/26 16:44:10 visual_prompt]: Epoch 83 / 100: avg data time: 5.71e-02, avg batch time: 0.5008, average train loss: 0.0111
[09/26 16:44:12 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1678, average loss: 6.0916
[09/26 16:44:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.50	
[09/26 16:44:12 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.042740606861239594
[09/26 16:44:19 visual_prompt]: Epoch 84 / 100: avg data time: 6.31e-02, avg batch time: 0.5069, average train loss: 0.0095
[09/26 16:44:21 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1672, average loss: 5.9481
[09/26 16:44:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.50	
[09/26 16:44:21 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.03798797596089351
[09/26 16:44:28 visual_prompt]: Epoch 85 / 100: avg data time: 6.13e-02, avg batch time: 0.5060, average train loss: 0.0137
[09/26 16:44:29 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1675, average loss: 5.8555
[09/26 16:44:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 95.00	
[09/26 16:44:29 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.03349364905389032
[09/26 16:44:36 visual_prompt]: Epoch 86 / 100: avg data time: 6.36e-02, avg batch time: 0.5070, average train loss: 0.0085
[09/26 16:44:38 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1676, average loss: 5.8955
[09/26 16:44:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 94.50	
[09/26 16:44:38 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.029263101785268253
[09/26 16:44:45 visual_prompt]: Epoch 87 / 100: avg data time: 5.90e-02, avg batch time: 0.5038, average train loss: 0.0096
[09/26 16:44:46 visual_prompt]: Inference (val):avg data time: 4.53e-05, avg batch time: 0.1674, average loss: 6.1741
[09/26 16:44:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.00	
[09/26 16:44:46 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.025301488425208296
[09/26 16:44:53 visual_prompt]: Epoch 88 / 100: avg data time: 4.81e-02, avg batch time: 0.4934, average train loss: 0.0056
[09/26 16:44:55 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1676, average loss: 6.1391
[09/26 16:44:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.00	
[09/26 16:44:55 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.021613635589349756
[09/26 16:45:02 visual_prompt]: Epoch 89 / 100: avg data time: 6.15e-02, avg batch time: 0.5049, average train loss: 0.0062
[09/26 16:45:03 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1674, average loss: 5.9851
[09/26 16:45:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 93.50	
[09/26 16:45:03 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.01820403635830317
[09/26 16:45:10 visual_prompt]: Epoch 90 / 100: avg data time: 6.48e-02, avg batch time: 0.5091, average train loss: 0.0068
[09/26 16:45:12 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1674, average loss: 6.0021
[09/26 16:45:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 93.50	
[09/26 16:45:12 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.01507684480352292
[09/26 16:45:19 visual_prompt]: Epoch 91 / 100: avg data time: 5.91e-02, avg batch time: 0.5033, average train loss: 0.0067
[09/26 16:45:20 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1672, average loss: 5.9909
[09/26 16:45:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 94.00	
[09/26 16:45:20 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.012235870926211617
[09/26 16:45:27 visual_prompt]: Epoch 92 / 100: avg data time: 6.34e-02, avg batch time: 0.5075, average train loss: 0.0052
[09/26 16:45:29 visual_prompt]: Inference (val):avg data time: 5.14e-05, avg batch time: 0.1671, average loss: 5.9705
[09/26 16:45:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 94.00	
[09/26 16:45:29 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.009684576015420276
[09/26 16:45:36 visual_prompt]: Epoch 93 / 100: avg data time: 6.90e-02, avg batch time: 0.5131, average train loss: 0.0065
[09/26 16:45:38 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1672, average loss: 5.9699
[09/26 16:45:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 94.00	
[09/26 16:45:38 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.007426068431000882
[09/26 16:45:45 visual_prompt]: Epoch 94 / 100: avg data time: 6.29e-02, avg batch time: 0.5086, average train loss: 0.0108
[09/26 16:45:46 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1676, average loss: 5.9926
[09/26 16:45:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 94.50	
[09/26 16:45:46 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.005463099816548578
[09/26 16:45:53 visual_prompt]: Epoch 95 / 100: avg data time: 6.03e-02, avg batch time: 0.5040, average train loss: 0.0099
[09/26 16:45:55 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1673, average loss: 6.0061
[09/26 16:45:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 94.50	
[09/26 16:45:55 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.003798061746947995
[09/26 16:46:02 visual_prompt]: Epoch 96 / 100: avg data time: 6.34e-02, avg batch time: 0.5085, average train loss: 0.0059
[09/26 16:46:03 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1674, average loss: 6.0134
[09/26 16:46:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 94.50	
[09/26 16:46:03 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0024329828146074095
[09/26 16:46:10 visual_prompt]: Epoch 97 / 100: avg data time: 6.25e-02, avg batch time: 0.5068, average train loss: 0.0058
[09/26 16:46:12 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1674, average loss: 6.0158
[09/26 16:46:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 94.50	
[09/26 16:46:12 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0013695261579316775
[09/26 16:46:19 visual_prompt]: Epoch 98 / 100: avg data time: 5.65e-02, avg batch time: 0.5009, average train loss: 0.0081
[09/26 16:46:20 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1680, average loss: 6.0081
[09/26 16:46:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 94.50	
[09/26 16:46:20 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0006089874350439506
[09/26 16:46:27 visual_prompt]: Epoch 99 / 100: avg data time: 6.00e-02, avg batch time: 0.5042, average train loss: 0.0083
[09/26 16:46:29 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1675, average loss: 6.0070
[09/26 16:46:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 94.50	
[09/26 16:46:29 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.00015229324522605947
[09/26 16:46:36 visual_prompt]: Epoch 100 / 100: avg data time: 5.93e-02, avg batch time: 0.5044, average train loss: 0.0082
[09/26 16:46:38 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1674, average loss: 6.0072
[09/26 16:46:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 94.50	
[09/26 16:46:38 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 16:46:38 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 16:46:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 16:46:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 16:46:38 visual_prompt]: Training with config:
[09/26 16:46:38 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 16:46:38 visual_prompt]: Loading training data...
[09/26 16:46:38 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 16:46:39 visual_prompt]: Number of images: 800
[09/26 16:46:39 visual_prompt]: Number of classes: 9 / 9
[09/26 16:46:39 visual_prompt]: Loading validation data...
[09/26 16:46:39 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 16:46:39 visual_prompt]: Number of images: 200
[09/26 16:46:39 visual_prompt]: Number of classes: 9 / 9
[09/26 16:46:39 visual_prompt]: Constructing models...
[09/26 16:46:42 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 16:46:42 visual_prompt]: tuned percent:0.542
[09/26 16:46:42 visual_prompt]: Device used for model: 0
[09/26 16:46:42 visual_prompt]: Setting up Evaluator...
[09/26 16:46:42 visual_prompt]: Setting up Trainer...
[09/26 16:46:42 visual_prompt]: 	Setting up the optimizer...
[09/26 16:46:42 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 16:46:49 visual_prompt]: Epoch 1 / 100: avg data time: 6.32e-02, avg batch time: 0.5068, average train loss: 2.8701
[09/26 16:46:50 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1671, average loss: 2.9516
[09/26 16:46:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 16:46:50 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 16:46:50 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 16:46:57 visual_prompt]: Epoch 2 / 100: avg data time: 6.02e-02, avg batch time: 0.5023, average train loss: 2.6153
[09/26 16:46:59 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1670, average loss: 2.4786
[09/26 16:46:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 16:46:59 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 16:47:06 visual_prompt]: Epoch 3 / 100: avg data time: 6.63e-02, avg batch time: 0.5109, average train loss: 2.2903
[09/26 16:47:07 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1671, average loss: 2.2486
[09/26 16:47:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 16:47:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 16:47:14 visual_prompt]: Epoch 4 / 100: avg data time: 6.01e-02, avg batch time: 0.5046, average train loss: 2.2347
[09/26 16:47:16 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1674, average loss: 2.1997
[09/26 16:47:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/26 16:47:16 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 16:47:23 visual_prompt]: Epoch 5 / 100: avg data time: 6.17e-02, avg batch time: 0.5049, average train loss: 2.2114
[09/26 16:47:25 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1674, average loss: 2.2258
[09/26 16:47:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 16:47:25 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 16:47:32 visual_prompt]: Epoch 6 / 100: avg data time: 6.75e-02, avg batch time: 0.5107, average train loss: 2.1878
[09/26 16:47:33 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1672, average loss: 2.2242
[09/26 16:47:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/26 16:47:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 16:47:40 visual_prompt]: Epoch 7 / 100: avg data time: 6.00e-02, avg batch time: 0.5045, average train loss: 2.1841
[09/26 16:47:42 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1673, average loss: 2.1742
[09/26 16:47:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 62.00	
[09/26 16:47:42 visual_prompt]: Best epoch 7: best metric: 0.175
[09/26 16:47:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 16:47:49 visual_prompt]: Epoch 8 / 100: avg data time: 5.97e-02, avg batch time: 0.5042, average train loss: 2.1423
[09/26 16:47:50 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1672, average loss: 2.1768
[09/26 16:47:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 60.50	
[09/26 16:47:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 16:47:57 visual_prompt]: Epoch 9 / 100: avg data time: 5.24e-02, avg batch time: 0.4988, average train loss: 2.2103
[09/26 16:47:59 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1673, average loss: 2.2014
[09/26 16:47:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 60.50	
[09/26 16:47:59 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 16:48:06 visual_prompt]: Epoch 10 / 100: avg data time: 6.30e-02, avg batch time: 0.5072, average train loss: 2.2496
[09/26 16:48:07 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1672, average loss: 2.2022
[09/26 16:48:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 16:48:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 16:48:14 visual_prompt]: Epoch 11 / 100: avg data time: 6.84e-02, avg batch time: 0.5112, average train loss: 2.2345
[09/26 16:48:16 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1675, average loss: 2.1956
[09/26 16:48:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 16:48:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 16:48:23 visual_prompt]: Epoch 12 / 100: avg data time: 6.06e-02, avg batch time: 0.5038, average train loss: 2.2078
[09/26 16:48:25 visual_prompt]: Inference (val):avg data time: 4.83e-05, avg batch time: 0.1669, average loss: 2.2145
[09/26 16:48:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 59.00	
[09/26 16:48:25 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 16:48:31 visual_prompt]: Epoch 13 / 100: avg data time: 6.01e-02, avg batch time: 0.5031, average train loss: 2.2365
[09/26 16:48:33 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1672, average loss: 2.2325
[09/26 16:48:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 16:48:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 16:48:40 visual_prompt]: Epoch 14 / 100: avg data time: 5.99e-02, avg batch time: 0.5034, average train loss: 2.2129
[09/26 16:48:42 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1674, average loss: 2.1957
[09/26 16:48:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 16:48:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 16:48:48 visual_prompt]: Epoch 15 / 100: avg data time: 5.55e-02, avg batch time: 0.4991, average train loss: 2.2111
[09/26 16:48:50 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1674, average loss: 2.2118
[09/26 16:48:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 16:48:50 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 16:48:57 visual_prompt]: Epoch 16 / 100: avg data time: 6.21e-02, avg batch time: 0.5060, average train loss: 2.2110
[09/26 16:48:59 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1672, average loss: 2.1910
[09/26 16:48:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 63.00	
[09/26 16:48:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 16:49:06 visual_prompt]: Epoch 17 / 100: avg data time: 6.06e-02, avg batch time: 0.5056, average train loss: 2.2042
[09/26 16:49:07 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1673, average loss: 2.2006
[09/26 16:49:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.00	
[09/26 16:49:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 16:49:14 visual_prompt]: Epoch 18 / 100: avg data time: 5.79e-02, avg batch time: 0.5022, average train loss: 2.1993
[09/26 16:49:16 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1674, average loss: 2.2048
[09/26 16:49:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/26 16:49:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 16:49:23 visual_prompt]: Epoch 19 / 100: avg data time: 6.68e-02, avg batch time: 0.5112, average train loss: 2.2076
[09/26 16:49:24 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1671, average loss: 2.2009
[09/26 16:49:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 16:49:24 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 16:49:31 visual_prompt]: Epoch 20 / 100: avg data time: 6.73e-02, avg batch time: 0.5109, average train loss: 2.2179
[09/26 16:49:33 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1669, average loss: 2.2193
[09/26 16:49:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 61.50	
[09/26 16:49:33 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 16:49:40 visual_prompt]: Epoch 21 / 100: avg data time: 6.83e-02, avg batch time: 0.5110, average train loss: 2.2086
[09/26 16:49:42 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1668, average loss: 2.2186
[09/26 16:49:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 54.50	
[09/26 16:49:42 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 16:49:49 visual_prompt]: Epoch 22 / 100: avg data time: 6.50e-02, avg batch time: 0.5083, average train loss: 2.2165
[09/26 16:49:50 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1666, average loss: 2.2034
[09/26 16:49:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.00	
[09/26 16:49:50 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 16:49:57 visual_prompt]: Epoch 23 / 100: avg data time: 6.00e-02, avg batch time: 0.5025, average train loss: 2.2085
[09/26 16:49:59 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1670, average loss: 2.2280
[09/26 16:49:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 16:49:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 16:50:06 visual_prompt]: Epoch 24 / 100: avg data time: 5.73e-02, avg batch time: 0.5006, average train loss: 2.2101
[09/26 16:50:07 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1669, average loss: 2.2269
[09/26 16:50:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.50	
[09/26 16:50:07 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 16:50:14 visual_prompt]: Epoch 25 / 100: avg data time: 6.50e-02, avg batch time: 0.5075, average train loss: 2.2063
[09/26 16:50:16 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1669, average loss: 2.1967
[09/26 16:50:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/26 16:50:16 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 16:50:23 visual_prompt]: Epoch 26 / 100: avg data time: 6.55e-02, avg batch time: 0.5083, average train loss: 2.2180
[09/26 16:50:24 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1667, average loss: 2.2125
[09/26 16:50:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/26 16:50:24 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 16:50:31 visual_prompt]: Epoch 27 / 100: avg data time: 6.87e-02, avg batch time: 0.5111, average train loss: 2.2106
[09/26 16:50:33 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.1668, average loss: 2.2165
[09/26 16:50:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/26 16:50:33 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 16:50:40 visual_prompt]: Epoch 28 / 100: avg data time: 6.06e-02, avg batch time: 0.5034, average train loss: 2.2112
[09/26 16:50:42 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1670, average loss: 2.2031
[09/26 16:50:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 16:50:42 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 16:50:48 visual_prompt]: Epoch 29 / 100: avg data time: 6.26e-02, avg batch time: 0.5045, average train loss: 2.2110
[09/26 16:50:50 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1666, average loss: 2.2024
[09/26 16:50:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 16:50:50 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 16:50:57 visual_prompt]: Epoch 30 / 100: avg data time: 6.10e-02, avg batch time: 0.5021, average train loss: 2.2191
[09/26 16:50:59 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1663, average loss: 2.2110
[09/26 16:50:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 16:50:59 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 16:51:06 visual_prompt]: Epoch 31 / 100: avg data time: 6.61e-02, avg batch time: 0.5083, average train loss: 2.2092
[09/26 16:51:07 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1663, average loss: 2.2097
[09/26 16:51:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 16:51:07 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 16:51:14 visual_prompt]: Epoch 32 / 100: avg data time: 5.67e-02, avg batch time: 0.4990, average train loss: 2.2020
[09/26 16:51:16 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1661, average loss: 2.2369
[09/26 16:51:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 16:51:16 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 16:51:22 visual_prompt]: Epoch 33 / 100: avg data time: 5.79e-02, avg batch time: 0.4994, average train loss: 2.2132
[09/26 16:51:24 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1660, average loss: 2.2063
[09/26 16:51:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/26 16:51:24 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 16:51:31 visual_prompt]: Epoch 34 / 100: avg data time: 6.12e-02, avg batch time: 0.5050, average train loss: 2.2103
[09/26 16:51:33 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1663, average loss: 2.2055
[09/26 16:51:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 16:51:33 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 16:51:40 visual_prompt]: Epoch 35 / 100: avg data time: 6.13e-02, avg batch time: 0.5038, average train loss: 2.2048
[09/26 16:51:41 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1661, average loss: 2.2090
[09/26 16:51:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 61.50	
[09/26 16:51:41 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 16:51:48 visual_prompt]: Epoch 36 / 100: avg data time: 5.82e-02, avg batch time: 0.5019, average train loss: 2.2069
[09/26 16:51:50 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1665, average loss: 2.1991
[09/26 16:51:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.00	
[09/26 16:51:50 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 16:51:57 visual_prompt]: Epoch 37 / 100: avg data time: 6.37e-02, avg batch time: 0.5061, average train loss: 2.2010
[09/26 16:51:58 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1665, average loss: 2.2073
[09/26 16:51:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 16:51:58 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 16:52:05 visual_prompt]: Epoch 38 / 100: avg data time: 7.28e-02, avg batch time: 0.5149, average train loss: 2.2085
[09/26 16:52:07 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1674, average loss: 2.2020
[09/26 16:52:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 16:52:07 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 16:52:14 visual_prompt]: Epoch 39 / 100: avg data time: 5.63e-02, avg batch time: 0.4985, average train loss: 2.2016
[09/26 16:52:15 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1667, average loss: 2.1988
[09/26 16:52:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 58.00	
[09/26 16:52:15 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 16:52:22 visual_prompt]: Epoch 40 / 100: avg data time: 6.29e-02, avg batch time: 0.5044, average train loss: 2.1974
[09/26 16:52:24 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1666, average loss: 2.1836
[09/26 16:52:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 63.00	
[09/26 16:52:24 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 16:52:31 visual_prompt]: Epoch 41 / 100: avg data time: 5.92e-02, avg batch time: 0.5025, average train loss: 2.2069
[09/26 16:52:33 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1663, average loss: 2.2043
[09/26 16:52:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 16:52:33 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 16:52:39 visual_prompt]: Epoch 42 / 100: avg data time: 6.31e-02, avg batch time: 0.5049, average train loss: 2.2051
[09/26 16:52:41 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1666, average loss: 2.1934
[09/26 16:52:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/26 16:52:41 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 16:52:48 visual_prompt]: Epoch 43 / 100: avg data time: 5.77e-02, avg batch time: 0.4999, average train loss: 2.2045
[09/26 16:52:50 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1667, average loss: 2.2089
[09/26 16:52:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 16:52:50 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 16:52:57 visual_prompt]: Epoch 44 / 100: avg data time: 6.24e-02, avg batch time: 0.5060, average train loss: 2.2082
[09/26 16:52:58 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1674, average loss: 2.1978
[09/26 16:52:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 63.50	
[09/26 16:52:58 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 16:53:05 visual_prompt]: Epoch 45 / 100: avg data time: 7.04e-02, avg batch time: 0.5132, average train loss: 2.2103
[09/26 16:53:07 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1676, average loss: 2.2235
[09/26 16:53:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 16:53:07 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 16:53:14 visual_prompt]: Epoch 46 / 100: avg data time: 6.57e-02, avg batch time: 0.5086, average train loss: 2.2100
[09/26 16:53:15 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1664, average loss: 2.2007
[09/26 16:53:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.00	
[09/26 16:53:15 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 16:53:22 visual_prompt]: Epoch 47 / 100: avg data time: 6.13e-02, avg batch time: 0.5049, average train loss: 2.2013
[09/26 16:53:24 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1667, average loss: 2.1886
[09/26 16:53:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 61.50	
[09/26 16:53:24 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 16:53:31 visual_prompt]: Epoch 48 / 100: avg data time: 5.94e-02, avg batch time: 0.5024, average train loss: 2.1961
[09/26 16:53:32 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1666, average loss: 2.1889
[09/26 16:53:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 16:53:32 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 16:53:39 visual_prompt]: Epoch 49 / 100: avg data time: 6.54e-02, avg batch time: 0.5077, average train loss: 2.2063
[09/26 16:53:41 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1671, average loss: 2.1944
[09/26 16:53:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 60.50	
[09/26 16:53:41 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 16:53:48 visual_prompt]: Epoch 50 / 100: avg data time: 6.55e-02, avg batch time: 0.5087, average train loss: 2.2035
[09/26 16:53:50 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1671, average loss: 2.1899
[09/26 16:53:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 16:53:50 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 16:53:57 visual_prompt]: Epoch 51 / 100: avg data time: 6.46e-02, avg batch time: 0.5072, average train loss: 2.1960
[09/26 16:53:58 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1675, average loss: 2.1689
[09/26 16:53:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 56.50	
[09/26 16:53:58 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 16:54:05 visual_prompt]: Epoch 52 / 100: avg data time: 6.49e-02, avg batch time: 0.5085, average train loss: 2.1907
[09/26 16:54:07 visual_prompt]: Inference (val):avg data time: 5.62e-05, avg batch time: 0.1675, average loss: 2.1709
[09/26 16:54:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 64.00	
[09/26 16:54:07 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 16:54:14 visual_prompt]: Epoch 53 / 100: avg data time: 6.31e-02, avg batch time: 0.5051, average train loss: 2.2081
[09/26 16:54:15 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1673, average loss: 2.1735
[09/26 16:54:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 68.00	
[09/26 16:54:15 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 16:54:22 visual_prompt]: Epoch 54 / 100: avg data time: 6.12e-02, avg batch time: 0.5044, average train loss: 2.1898
[09/26 16:54:24 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1669, average loss: 2.2175
[09/26 16:54:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/26 16:54:24 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 16:54:31 visual_prompt]: Epoch 55 / 100: avg data time: 6.39e-02, avg batch time: 0.5067, average train loss: 2.2119
[09/26 16:54:32 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1669, average loss: 2.1987
[09/26 16:54:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 16:54:32 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 16:54:39 visual_prompt]: Epoch 56 / 100: avg data time: 5.75e-02, avg batch time: 0.4999, average train loss: 2.2080
[09/26 16:54:41 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.1669, average loss: 2.1954
[09/26 16:54:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 16:54:41 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 16:54:48 visual_prompt]: Epoch 57 / 100: avg data time: 5.77e-02, avg batch time: 0.5000, average train loss: 2.1995
[09/26 16:54:50 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1666, average loss: 2.1994
[09/26 16:54:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 16:54:50 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 16:54:56 visual_prompt]: Epoch 58 / 100: avg data time: 6.19e-02, avg batch time: 0.5044, average train loss: 2.2086
[09/26 16:54:58 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1667, average loss: 2.1945
[09/26 16:54:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 16:54:58 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 16:55:05 visual_prompt]: Epoch 59 / 100: avg data time: 5.99e-02, avg batch time: 0.5020, average train loss: 2.1976
[09/26 16:55:07 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.1675, average loss: 2.1947
[09/26 16:55:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 16:55:07 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 16:55:14 visual_prompt]: Epoch 60 / 100: avg data time: 6.34e-02, avg batch time: 0.5051, average train loss: 2.2016
[09/26 16:55:15 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1672, average loss: 2.1958
[09/26 16:55:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 58.00	
[09/26 16:55:15 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 16:55:22 visual_prompt]: Epoch 61 / 100: avg data time: 6.46e-02, avg batch time: 0.5085, average train loss: 2.2058
[09/26 16:55:24 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1667, average loss: 2.1997
[09/26 16:55:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 16:55:24 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 16:55:31 visual_prompt]: Epoch 62 / 100: avg data time: 6.39e-02, avg batch time: 0.5062, average train loss: 2.1983
[09/26 16:55:32 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1674, average loss: 2.1949
[09/26 16:55:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 16:55:32 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 16:55:39 visual_prompt]: Epoch 63 / 100: avg data time: 6.23e-02, avg batch time: 0.5044, average train loss: 2.1901
[09/26 16:55:41 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.1663, average loss: 2.1780
[09/26 16:55:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 16:55:41 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 16:55:48 visual_prompt]: Epoch 64 / 100: avg data time: 5.92e-02, avg batch time: 0.5011, average train loss: 2.1742
[09/26 16:55:49 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1676, average loss: 2.1960
[09/26 16:55:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 16:55:49 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 16:55:56 visual_prompt]: Epoch 65 / 100: avg data time: 6.19e-02, avg batch time: 0.5051, average train loss: 2.2005
[09/26 16:55:58 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1676, average loss: 2.2012
[09/26 16:55:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 16:55:58 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 16:56:05 visual_prompt]: Epoch 66 / 100: avg data time: 6.71e-02, avg batch time: 0.5114, average train loss: 2.2025
[09/26 16:56:07 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1676, average loss: 2.1966
[09/26 16:56:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 16:56:07 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 16:56:14 visual_prompt]: Epoch 67 / 100: avg data time: 6.29e-02, avg batch time: 0.5067, average train loss: 2.1983
[09/26 16:56:15 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1674, average loss: 2.1987
[09/26 16:56:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 16:56:15 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 16:56:22 visual_prompt]: Epoch 68 / 100: avg data time: 6.17e-02, avg batch time: 0.5040, average train loss: 2.1998
[09/26 16:56:24 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1673, average loss: 2.1939
[09/26 16:56:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.00	
[09/26 16:56:24 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 16:56:31 visual_prompt]: Epoch 69 / 100: avg data time: 6.37e-02, avg batch time: 0.5065, average train loss: 2.1915
[09/26 16:56:32 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1674, average loss: 2.1954
[09/26 16:56:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/26 16:56:32 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 16:56:39 visual_prompt]: Epoch 70 / 100: avg data time: 6.37e-02, avg batch time: 0.5057, average train loss: 2.1885
[09/26 16:56:41 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1671, average loss: 2.1916
[09/26 16:56:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 16:56:41 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 16:56:48 visual_prompt]: Epoch 71 / 100: avg data time: 5.85e-02, avg batch time: 0.5000, average train loss: 2.1811
[09/26 16:56:49 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1673, average loss: 2.1651
[09/26 16:56:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 67.00	
[09/26 16:56:49 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 16:56:56 visual_prompt]: Epoch 72 / 100: avg data time: 5.71e-02, avg batch time: 0.5004, average train loss: 2.1821
[09/26 16:56:58 visual_prompt]: Inference (val):avg data time: 4.68e-05, avg batch time: 0.1671, average loss: 2.2437
[09/26 16:56:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/26 16:56:58 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 16:57:05 visual_prompt]: Epoch 73 / 100: avg data time: 5.94e-02, avg batch time: 0.5032, average train loss: 2.1864
[09/26 16:57:06 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1673, average loss: 2.1698
[09/26 16:57:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 64.50	
[09/26 16:57:06 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 16:57:13 visual_prompt]: Epoch 74 / 100: avg data time: 6.06e-02, avg batch time: 0.5025, average train loss: 2.1713
[09/26 16:57:15 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1670, average loss: 2.1979
[09/26 16:57:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.00	
[09/26 16:57:15 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 16:57:22 visual_prompt]: Epoch 75 / 100: avg data time: 4.99e-02, avg batch time: 0.4941, average train loss: 2.2003
[09/26 16:57:23 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1668, average loss: 2.1970
[09/26 16:57:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 16:57:23 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 16:57:30 visual_prompt]: Epoch 76 / 100: avg data time: 5.94e-02, avg batch time: 0.5015, average train loss: 2.1872
[09/26 16:57:32 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1671, average loss: 2.1874
[09/26 16:57:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 61.50	
[09/26 16:57:32 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 16:57:39 visual_prompt]: Epoch 77 / 100: avg data time: 5.84e-02, avg batch time: 0.5004, average train loss: 2.1727
[09/26 16:57:40 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1671, average loss: 2.1687
[09/26 16:57:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 16:57:40 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 16:57:47 visual_prompt]: Epoch 78 / 100: avg data time: 6.06e-02, avg batch time: 0.5024, average train loss: 2.1577
[09/26 16:57:49 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1668, average loss: 2.2237
[09/26 16:57:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 62.00	
[09/26 16:57:49 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 16:57:55 visual_prompt]: Epoch 79 / 100: avg data time: 5.75e-02, avg batch time: 0.4993, average train loss: 2.1424
[09/26 16:57:57 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1673, average loss: 2.2152
[09/26 16:57:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 55.50	
[09/26 16:57:57 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 16:58:04 visual_prompt]: Epoch 80 / 100: avg data time: 5.35e-02, avg batch time: 0.4954, average train loss: 2.1663
[09/26 16:58:05 visual_prompt]: Inference (val):avg data time: 4.88e-05, avg batch time: 0.1671, average loss: 2.1110
[09/26 16:58:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 71.00	
[09/26 16:58:05 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 16:58:12 visual_prompt]: Epoch 81 / 100: avg data time: 6.33e-02, avg batch time: 0.5068, average train loss: 2.1456
[09/26 16:58:14 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1668, average loss: 2.1219
[09/26 16:58:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 73.00	
[09/26 16:58:14 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 16:58:21 visual_prompt]: Epoch 82 / 100: avg data time: 6.35e-02, avg batch time: 0.5062, average train loss: 2.0912
[09/26 16:58:23 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1672, average loss: 2.2360
[09/26 16:58:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 67.00	
[09/26 16:58:23 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 16:58:30 visual_prompt]: Epoch 83 / 100: avg data time: 6.62e-02, avg batch time: 0.5081, average train loss: 2.0683
[09/26 16:58:31 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1673, average loss: 2.1857
[09/26 16:58:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 69.00	
[09/26 16:58:31 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 16:58:38 visual_prompt]: Epoch 84 / 100: avg data time: 6.02e-02, avg batch time: 0.5040, average train loss: 2.0752
[09/26 16:58:40 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1673, average loss: 2.0405
[09/26 16:58:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 76.00	
[09/26 16:58:40 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 16:58:47 visual_prompt]: Epoch 85 / 100: avg data time: 6.28e-02, avg batch time: 0.5053, average train loss: 2.0299
[09/26 16:58:48 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1669, average loss: 2.1116
[09/26 16:58:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 72.50	
[09/26 16:58:48 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 16:58:55 visual_prompt]: Epoch 86 / 100: avg data time: 5.63e-02, avg batch time: 0.5006, average train loss: 1.9929
[09/26 16:58:57 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1668, average loss: 2.0785
[09/26 16:58:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 75.00	
[09/26 16:58:57 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 16:59:04 visual_prompt]: Epoch 87 / 100: avg data time: 6.44e-02, avg batch time: 0.5076, average train loss: 1.9900
[09/26 16:59:05 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1674, average loss: 1.9959
[09/26 16:59:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 79.00	
[09/26 16:59:05 visual_prompt]: Best epoch 87: best metric: 0.225
[09/26 16:59:05 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 16:59:12 visual_prompt]: Epoch 88 / 100: avg data time: 6.33e-02, avg batch time: 0.5060, average train loss: 1.9418
[09/26 16:59:14 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1669, average loss: 2.0568
[09/26 16:59:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 72.00	
[09/26 16:59:14 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 16:59:21 visual_prompt]: Epoch 89 / 100: avg data time: 6.79e-02, avg batch time: 0.5121, average train loss: 1.9016
[09/26 16:59:23 visual_prompt]: Inference (val):avg data time: 4.97e-05, avg batch time: 0.1672, average loss: 2.0652
[09/26 16:59:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 71.50	
[09/26 16:59:23 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 16:59:30 visual_prompt]: Epoch 90 / 100: avg data time: 6.12e-02, avg batch time: 0.5035, average train loss: 1.8981
[09/26 16:59:31 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1671, average loss: 1.9186
[09/26 16:59:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 82.00	
[09/26 16:59:31 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 16:59:38 visual_prompt]: Epoch 91 / 100: avg data time: 6.32e-02, avg batch time: 0.5061, average train loss: 1.8369
[09/26 16:59:40 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1672, average loss: 1.8918
[09/26 16:59:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 82.00	
[09/26 16:59:40 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 16:59:47 visual_prompt]: Epoch 92 / 100: avg data time: 6.69e-02, avg batch time: 0.5091, average train loss: 1.7993
[09/26 16:59:48 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1675, average loss: 1.8565
[09/26 16:59:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 86.00	
[09/26 16:59:48 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 16:59:55 visual_prompt]: Epoch 93 / 100: avg data time: 5.91e-02, avg batch time: 0.5018, average train loss: 1.7469
[09/26 16:59:57 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1673, average loss: 2.0968
[09/26 16:59:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 75.00	
[09/26 16:59:57 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 17:00:04 visual_prompt]: Epoch 94 / 100: avg data time: 6.42e-02, avg batch time: 0.5065, average train loss: 1.7554
[09/26 17:00:05 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1670, average loss: 1.9103
[09/26 17:00:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 86.50	
[09/26 17:00:05 visual_prompt]: Best epoch 94: best metric: 0.245
[09/26 17:00:05 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 17:00:12 visual_prompt]: Epoch 95 / 100: avg data time: 5.37e-02, avg batch time: 0.4983, average train loss: 1.7064
[09/26 17:00:14 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1672, average loss: 1.9474
[09/26 17:00:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 81.50	
[09/26 17:00:14 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 17:00:21 visual_prompt]: Epoch 96 / 100: avg data time: 6.66e-02, avg batch time: 0.5096, average train loss: 1.6923
[09/26 17:00:22 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.1670, average loss: 1.8486
[09/26 17:00:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 86.50	
[09/26 17:00:22 visual_prompt]: Best epoch 96: best metric: 0.250
[09/26 17:00:22 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 17:00:29 visual_prompt]: Epoch 97 / 100: avg data time: 6.59e-02, avg batch time: 0.5078, average train loss: 1.6589
[09/26 17:00:31 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1672, average loss: 1.8928
[09/26 17:00:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 88.00	
[09/26 17:00:31 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 17:00:38 visual_prompt]: Epoch 98 / 100: avg data time: 6.22e-02, avg batch time: 0.5053, average train loss: 1.6297
[09/26 17:00:39 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1670, average loss: 1.8576
[09/26 17:00:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 88.50	
[09/26 17:00:39 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 17:00:46 visual_prompt]: Epoch 99 / 100: avg data time: 4.82e-02, avg batch time: 0.4930, average train loss: 1.6278
[09/26 17:00:48 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1670, average loss: 1.8636
[09/26 17:00:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 88.50	
[09/26 17:00:48 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 17:00:55 visual_prompt]: Epoch 100 / 100: avg data time: 6.27e-02, avg batch time: 0.5047, average train loss: 1.6092
[09/26 17:00:56 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1670, average loss: 1.8739
[09/26 17:00:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 88.50	
[09/26 17:00:56 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 17:00:56 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 17:00:56 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 17:00:56 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 17:00:56 visual_prompt]: Training with config:
[09/26 17:00:56 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 17:00:56 visual_prompt]: Loading training data...
[09/26 17:00:56 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 17:00:58 visual_prompt]: Number of images: 800
[09/26 17:00:58 visual_prompt]: Number of classes: 9 / 9
[09/26 17:00:58 visual_prompt]: Loading validation data...
[09/26 17:00:58 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 17:00:58 visual_prompt]: Number of images: 200
[09/26 17:00:58 visual_prompt]: Number of classes: 9 / 9
[09/26 17:00:58 visual_prompt]: Constructing models...
[09/26 17:01:00 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 17:01:00 visual_prompt]: tuned percent:0.542
[09/26 17:01:01 visual_prompt]: Device used for model: 0
[09/26 17:01:01 visual_prompt]: Setting up Evaluator...
[09/26 17:01:01 visual_prompt]: Setting up Trainer...
[09/26 17:01:01 visual_prompt]: 	Setting up the optimizer...
[09/26 17:01:01 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 17:01:07 visual_prompt]: Epoch 1 / 100: avg data time: 6.28e-02, avg batch time: 0.5053, average train loss: 2.8847
[09/26 17:01:09 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1673, average loss: 2.9516
[09/26 17:01:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 17:01:09 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 17:01:09 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 17:01:16 visual_prompt]: Epoch 2 / 100: avg data time: 6.49e-02, avg batch time: 0.5071, average train loss: 2.6402
[09/26 17:01:18 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1671, average loss: 2.4576
[09/26 17:01:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 51.50	
[09/26 17:01:18 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 17:01:25 visual_prompt]: Epoch 3 / 100: avg data time: 6.08e-02, avg batch time: 0.5037, average train loss: 2.2850
[09/26 17:01:26 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1671, average loss: 2.2259
[09/26 17:01:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/26 17:01:26 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 17:01:33 visual_prompt]: Epoch 4 / 100: avg data time: 6.40e-02, avg batch time: 0.5072, average train loss: 2.2353
[09/26 17:01:35 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1669, average loss: 2.2090
[09/26 17:01:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 54.50	
[09/26 17:01:35 visual_prompt]: Best epoch 4: best metric: 0.145
[09/26 17:01:35 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 17:01:42 visual_prompt]: Epoch 5 / 100: avg data time: 6.18e-02, avg batch time: 0.5036, average train loss: 2.2288
[09/26 17:01:43 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1672, average loss: 2.2288
[09/26 17:01:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 17:01:43 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 17:01:50 visual_prompt]: Epoch 6 / 100: avg data time: 6.10e-02, avg batch time: 0.5039, average train loss: 2.2239
[09/26 17:01:52 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1668, average loss: 2.2607
[09/26 17:01:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.50	
[09/26 17:01:52 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 17:01:59 visual_prompt]: Epoch 7 / 100: avg data time: 6.52e-02, avg batch time: 0.5079, average train loss: 2.1974
[09/26 17:02:01 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1672, average loss: 2.1997
[09/26 17:02:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 60.50	
[09/26 17:02:01 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 17:02:08 visual_prompt]: Epoch 8 / 100: avg data time: 6.81e-02, avg batch time: 0.5103, average train loss: 2.1316
[09/26 17:02:09 visual_prompt]: Inference (val):avg data time: 5.48e-05, avg batch time: 0.1670, average loss: 2.2188
[09/26 17:02:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 67.00	
[09/26 17:02:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 17:02:16 visual_prompt]: Epoch 9 / 100: avg data time: 6.31e-02, avg batch time: 0.5047, average train loss: 2.1444
[09/26 17:02:18 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.1670, average loss: 2.1519
[09/26 17:02:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 70.00	
[09/26 17:02:18 visual_prompt]: Best epoch 9: best metric: 0.195
[09/26 17:02:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 17:02:25 visual_prompt]: Epoch 10 / 100: avg data time: 5.22e-02, avg batch time: 0.4955, average train loss: 2.0043
[09/26 17:02:26 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1671, average loss: 2.0729
[09/26 17:02:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 75.50	
[09/26 17:02:26 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 17:02:33 visual_prompt]: Epoch 11 / 100: avg data time: 6.31e-02, avg batch time: 0.5057, average train loss: 2.0023
[09/26 17:02:35 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1672, average loss: 2.0236
[09/26 17:02:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 80.00	
[09/26 17:02:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 17:02:42 visual_prompt]: Epoch 12 / 100: avg data time: 5.50e-02, avg batch time: 0.4983, average train loss: 1.8380
[09/26 17:02:43 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1672, average loss: 2.0400
[09/26 17:02:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 82.50	
[09/26 17:02:43 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 17:02:50 visual_prompt]: Epoch 13 / 100: avg data time: 6.42e-02, avg batch time: 0.5065, average train loss: 1.7762
[09/26 17:02:52 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1675, average loss: 1.8619
[09/26 17:02:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 88.00	
[09/26 17:02:52 visual_prompt]: Best epoch 13: best metric: 0.230
[09/26 17:02:52 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 17:02:59 visual_prompt]: Epoch 14 / 100: avg data time: 6.64e-02, avg batch time: 0.5092, average train loss: 1.7497
[09/26 17:03:00 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1669, average loss: 1.7764
[09/26 17:03:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 89.00	
[09/26 17:03:00 visual_prompt]: Best epoch 14: best metric: 0.240
[09/26 17:03:00 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 17:03:07 visual_prompt]: Epoch 15 / 100: avg data time: 5.82e-02, avg batch time: 0.5031, average train loss: 1.6792
[09/26 17:03:09 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1674, average loss: 1.8210
[09/26 17:03:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 87.00	
[09/26 17:03:09 visual_prompt]: Best epoch 15: best metric: 0.285
[09/26 17:03:09 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 17:03:16 visual_prompt]: Epoch 16 / 100: avg data time: 6.20e-02, avg batch time: 0.5045, average train loss: 1.7206
[09/26 17:03:18 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1674, average loss: 1.9322
[09/26 17:03:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 87.50	
[09/26 17:03:18 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 17:03:24 visual_prompt]: Epoch 17 / 100: avg data time: 5.32e-02, avg batch time: 0.4981, average train loss: 1.6348
[09/26 17:03:26 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1675, average loss: 1.8219
[09/26 17:03:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 89.00	
[09/26 17:03:26 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 17:03:33 visual_prompt]: Epoch 18 / 100: avg data time: 6.43e-02, avg batch time: 0.5074, average train loss: 1.5780
[09/26 17:03:35 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1676, average loss: 2.1190
[09/26 17:03:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 82.00	
[09/26 17:03:35 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 17:03:42 visual_prompt]: Epoch 19 / 100: avg data time: 6.54e-02, avg batch time: 0.5075, average train loss: 1.5410
[09/26 17:03:43 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.1674, average loss: 1.9689
[09/26 17:03:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 84.50	
[09/26 17:03:43 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 17:03:50 visual_prompt]: Epoch 20 / 100: avg data time: 5.31e-02, avg batch time: 0.4978, average train loss: 1.5252
[09/26 17:03:52 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1674, average loss: 1.8564
[09/26 17:03:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 89.50	
[09/26 17:03:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 17:03:59 visual_prompt]: Epoch 21 / 100: avg data time: 5.99e-02, avg batch time: 0.5037, average train loss: 1.4679
[09/26 17:04:00 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1675, average loss: 2.2728
[09/26 17:04:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 82.50	
[09/26 17:04:00 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 17:04:07 visual_prompt]: Epoch 22 / 100: avg data time: 6.62e-02, avg batch time: 0.5091, average train loss: 1.5039
[09/26 17:04:09 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1673, average loss: 2.3405
[09/26 17:04:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 80.50	
[09/26 17:04:09 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 17:04:16 visual_prompt]: Epoch 23 / 100: avg data time: 6.51e-02, avg batch time: 0.5079, average train loss: 1.5311
[09/26 17:04:17 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1670, average loss: 1.6674
[09/26 17:04:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 94.00	
[09/26 17:04:17 visual_prompt]: Best epoch 23: best metric: 0.290
[09/26 17:04:17 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 17:04:24 visual_prompt]: Epoch 24 / 100: avg data time: 6.65e-02, avg batch time: 0.5095, average train loss: 1.3208
[09/26 17:04:26 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1673, average loss: 1.7392
[09/26 17:04:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 92.00	
[09/26 17:04:26 visual_prompt]: Best epoch 24: best metric: 0.295
[09/26 17:04:26 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 17:04:33 visual_prompt]: Epoch 25 / 100: avg data time: 6.35e-02, avg batch time: 0.5062, average train loss: 1.2816
[09/26 17:04:35 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1671, average loss: 2.1222
[09/26 17:04:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 87.50	
[09/26 17:04:35 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 17:04:42 visual_prompt]: Epoch 26 / 100: avg data time: 6.01e-02, avg batch time: 0.5036, average train loss: 1.2356
[09/26 17:04:43 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1674, average loss: 1.9608
[09/26 17:04:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.50	
[09/26 17:04:43 visual_prompt]: Best epoch 26: best metric: 0.300
[09/26 17:04:43 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 17:04:50 visual_prompt]: Epoch 27 / 100: avg data time: 6.31e-02, avg batch time: 0.5092, average train loss: 1.2918
[09/26 17:04:52 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1676, average loss: 1.8272
[09/26 17:04:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 93.50	
[09/26 17:04:52 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 17:04:59 visual_prompt]: Epoch 28 / 100: avg data time: 6.16e-02, avg batch time: 0.5043, average train loss: 1.2461
[09/26 17:05:00 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1675, average loss: 1.7524
[09/26 17:05:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 92.50	
[09/26 17:05:00 visual_prompt]: Best epoch 28: best metric: 0.340
[09/26 17:05:00 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 17:05:07 visual_prompt]: Epoch 29 / 100: avg data time: 6.05e-02, avg batch time: 0.5024, average train loss: 1.1048
[09/26 17:05:09 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1677, average loss: 1.8976
[09/26 17:05:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 96.50	
[09/26 17:05:09 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 17:05:16 visual_prompt]: Epoch 30 / 100: avg data time: 4.98e-02, avg batch time: 0.4939, average train loss: 1.0889
[09/26 17:05:17 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1674, average loss: 1.9395
[09/26 17:05:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 90.50	
[09/26 17:05:17 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 17:05:24 visual_prompt]: Epoch 31 / 100: avg data time: 6.48e-02, avg batch time: 0.5071, average train loss: 1.1922
[09/26 17:05:26 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1674, average loss: 2.5646
[09/26 17:05:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 84.00	
[09/26 17:05:26 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 17:05:33 visual_prompt]: Epoch 32 / 100: avg data time: 5.11e-02, avg batch time: 0.4953, average train loss: 1.0877
[09/26 17:05:34 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1678, average loss: 2.2514
[09/26 17:05:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 91.00	
[09/26 17:05:34 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 17:05:41 visual_prompt]: Epoch 33 / 100: avg data time: 5.61e-02, avg batch time: 0.4997, average train loss: 0.9850
[09/26 17:05:43 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1673, average loss: 2.7880
[09/26 17:05:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 88.00	
[09/26 17:05:43 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 17:05:50 visual_prompt]: Epoch 34 / 100: avg data time: 6.58e-02, avg batch time: 0.5091, average train loss: 1.1266
[09/26 17:05:51 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1672, average loss: 1.9114
[09/26 17:05:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 93.00	
[09/26 17:05:51 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 17:05:58 visual_prompt]: Epoch 35 / 100: avg data time: 6.01e-02, avg batch time: 0.5051, average train loss: 1.1675
[09/26 17:06:00 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1672, average loss: 2.1150
[09/26 17:06:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 87.50	
[09/26 17:06:00 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 17:06:07 visual_prompt]: Epoch 36 / 100: avg data time: 5.50e-02, avg batch time: 0.4974, average train loss: 0.9033
[09/26 17:06:08 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1672, average loss: 2.5191
[09/26 17:06:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 91.00	
[09/26 17:06:08 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 17:06:15 visual_prompt]: Epoch 37 / 100: avg data time: 6.02e-02, avg batch time: 0.5033, average train loss: 0.8384
[09/26 17:06:17 visual_prompt]: Inference (val):avg data time: 4.54e-05, avg batch time: 0.1673, average loss: 1.8660
[09/26 17:06:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 89.00	
[09/26 17:06:17 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 17:06:24 visual_prompt]: Epoch 38 / 100: avg data time: 6.67e-02, avg batch time: 0.5086, average train loss: 1.1607
[09/26 17:06:26 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1673, average loss: 2.2611
[09/26 17:06:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 89.50	
[09/26 17:06:26 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 17:06:32 visual_prompt]: Epoch 39 / 100: avg data time: 5.30e-02, avg batch time: 0.4968, average train loss: 0.9888
[09/26 17:06:34 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1674, average loss: 2.2730
[09/26 17:06:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 93.00	
[09/26 17:06:34 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 17:06:41 visual_prompt]: Epoch 40 / 100: avg data time: 5.97e-02, avg batch time: 0.5025, average train loss: 0.8273
[09/26 17:06:43 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1673, average loss: 2.2891
[09/26 17:06:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 92.50	
[09/26 17:06:43 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 17:06:50 visual_prompt]: Epoch 41 / 100: avg data time: 6.35e-02, avg batch time: 0.5066, average train loss: 0.7988
[09/26 17:06:51 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1674, average loss: 2.5377
[09/26 17:06:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 94.00	
[09/26 17:06:51 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 17:06:58 visual_prompt]: Epoch 42 / 100: avg data time: 6.16e-02, avg batch time: 0.5042, average train loss: 0.8229
[09/26 17:07:00 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1677, average loss: 2.5260
[09/26 17:07:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 89.50	
[09/26 17:07:00 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 17:07:07 visual_prompt]: Epoch 43 / 100: avg data time: 6.07e-02, avg batch time: 0.5035, average train loss: 0.6980
[09/26 17:07:08 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1675, average loss: 2.4434
[09/26 17:07:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 94.00	
[09/26 17:07:08 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 17:07:15 visual_prompt]: Epoch 44 / 100: avg data time: 6.08e-02, avg batch time: 0.5033, average train loss: 0.6255
[09/26 17:07:17 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1672, average loss: 2.9063
[09/26 17:07:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.50	
[09/26 17:07:17 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 17:07:24 visual_prompt]: Epoch 45 / 100: avg data time: 5.45e-02, avg batch time: 0.4980, average train loss: 0.5258
[09/26 17:07:25 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1673, average loss: 2.8880
[09/26 17:07:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 93.00	
[09/26 17:07:25 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 17:07:32 visual_prompt]: Epoch 46 / 100: avg data time: 6.43e-02, avg batch time: 0.5064, average train loss: 0.4925
[09/26 17:07:34 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1670, average loss: 3.1420
[09/26 17:07:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 90.50	
[09/26 17:07:34 visual_prompt]: Best epoch 46: best metric: 0.375
[09/26 17:07:34 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 17:07:41 visual_prompt]: Epoch 47 / 100: avg data time: 4.87e-02, avg batch time: 0.4927, average train loss: 0.5168
[09/26 17:07:42 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1673, average loss: 3.0513
[09/26 17:07:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 91.50	
[09/26 17:07:42 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 17:07:49 visual_prompt]: Epoch 48 / 100: avg data time: 6.77e-02, avg batch time: 0.5119, average train loss: 0.5103
[09/26 17:07:51 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1671, average loss: 3.0440
[09/26 17:07:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 92.00	
[09/26 17:07:51 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 17:07:58 visual_prompt]: Epoch 49 / 100: avg data time: 6.61e-02, avg batch time: 0.5083, average train loss: 0.7183
[09/26 17:08:00 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1672, average loss: 2.6675
[09/26 17:08:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 90.50	
[09/26 17:08:00 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 17:08:07 visual_prompt]: Epoch 50 / 100: avg data time: 6.77e-02, avg batch time: 0.5107, average train loss: 0.6513
[09/26 17:08:08 visual_prompt]: Inference (val):avg data time: 4.89e-05, avg batch time: 0.1675, average loss: 2.7945
[09/26 17:08:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 93.00	
[09/26 17:08:08 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 17:08:15 visual_prompt]: Epoch 51 / 100: avg data time: 5.62e-02, avg batch time: 0.4996, average train loss: 0.5538
[09/26 17:08:17 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1677, average loss: 2.8370
[09/26 17:08:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 90.50	
[09/26 17:08:17 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 17:08:24 visual_prompt]: Epoch 52 / 100: avg data time: 5.92e-02, avg batch time: 0.5025, average train loss: 0.4828
[09/26 17:08:25 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1674, average loss: 3.3677
[09/26 17:08:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 88.00	
[09/26 17:08:25 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 17:08:32 visual_prompt]: Epoch 53 / 100: avg data time: 6.72e-02, avg batch time: 0.5100, average train loss: 0.4066
[09/26 17:08:34 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1674, average loss: 3.4564
[09/26 17:08:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.00	
[09/26 17:08:34 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 17:08:41 visual_prompt]: Epoch 54 / 100: avg data time: 6.31e-02, avg batch time: 0.5061, average train loss: 0.2891
[09/26 17:08:43 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1672, average loss: 3.6285
[09/26 17:08:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 87.50	
[09/26 17:08:43 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 17:08:50 visual_prompt]: Epoch 55 / 100: avg data time: 6.09e-02, avg batch time: 0.5047, average train loss: 0.3221
[09/26 17:08:51 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.1673, average loss: 3.4193
[09/26 17:08:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.00	
[09/26 17:08:51 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 17:08:58 visual_prompt]: Epoch 56 / 100: avg data time: 6.11e-02, avg batch time: 0.5040, average train loss: 0.3094
[09/26 17:09:00 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1675, average loss: 3.5290
[09/26 17:09:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 88.50	
[09/26 17:09:00 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 17:09:06 visual_prompt]: Epoch 57 / 100: avg data time: 4.87e-02, avg batch time: 0.4925, average train loss: 0.2883
[09/26 17:09:08 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1675, average loss: 3.5921
[09/26 17:09:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 90.00	
[09/26 17:09:08 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 17:09:15 visual_prompt]: Epoch 58 / 100: avg data time: 6.53e-02, avg batch time: 0.5078, average train loss: 0.2608
[09/26 17:09:17 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1670, average loss: 3.3172
[09/26 17:09:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 92.00	
[09/26 17:09:17 visual_prompt]: Best epoch 58: best metric: 0.380
[09/26 17:09:17 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 17:09:24 visual_prompt]: Epoch 59 / 100: avg data time: 5.64e-02, avg batch time: 0.5004, average train loss: 0.2365
[09/26 17:09:25 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1679, average loss: 3.6853
[09/26 17:09:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 90.00	
[09/26 17:09:25 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 17:09:32 visual_prompt]: Epoch 60 / 100: avg data time: 5.35e-02, avg batch time: 0.4977, average train loss: 0.1940
[09/26 17:09:34 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1677, average loss: 3.9590
[09/26 17:09:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 89.00	
[09/26 17:09:34 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 17:09:41 visual_prompt]: Epoch 61 / 100: avg data time: 6.07e-02, avg batch time: 0.5038, average train loss: 0.2222
[09/26 17:09:42 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1675, average loss: 3.6777
[09/26 17:09:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 90.00	
[09/26 17:09:42 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 17:09:49 visual_prompt]: Epoch 62 / 100: avg data time: 6.52e-02, avg batch time: 0.5089, average train loss: 0.1625
[09/26 17:09:51 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1679, average loss: 3.5548
[09/26 17:09:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 17:09:51 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 17:09:58 visual_prompt]: Epoch 63 / 100: avg data time: 5.76e-02, avg batch time: 0.5021, average train loss: 0.1582
[09/26 17:10:00 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1675, average loss: 3.7548
[09/26 17:10:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 94.50	
[09/26 17:10:00 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 17:10:07 visual_prompt]: Epoch 64 / 100: avg data time: 6.22e-02, avg batch time: 0.5045, average train loss: 0.1166
[09/26 17:10:08 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1677, average loss: 4.2240
[09/26 17:10:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 88.50	
[09/26 17:10:08 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 17:10:15 visual_prompt]: Epoch 65 / 100: avg data time: 6.02e-02, avg batch time: 0.5043, average train loss: 0.1122
[09/26 17:10:17 visual_prompt]: Inference (val):avg data time: 4.62e-05, avg batch time: 0.1678, average loss: 4.1795
[09/26 17:10:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 89.00	
[09/26 17:10:17 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 17:10:24 visual_prompt]: Epoch 66 / 100: avg data time: 6.19e-02, avg batch time: 0.5053, average train loss: 0.1307
[09/26 17:10:25 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1676, average loss: 4.2191
[09/26 17:10:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 90.00	
[09/26 17:10:25 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 17:10:32 visual_prompt]: Epoch 67 / 100: avg data time: 6.73e-02, avg batch time: 0.5101, average train loss: 0.1147
[09/26 17:10:34 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1676, average loss: 3.7735
[09/26 17:10:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 92.50	
[09/26 17:10:34 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 17:10:41 visual_prompt]: Epoch 68 / 100: avg data time: 5.83e-02, avg batch time: 0.5009, average train loss: 0.1052
[09/26 17:10:43 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1670, average loss: 3.7504
[09/26 17:10:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 91.50	
[09/26 17:10:43 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 17:10:49 visual_prompt]: Epoch 69 / 100: avg data time: 5.98e-02, avg batch time: 0.5029, average train loss: 0.0805
[09/26 17:10:51 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1671, average loss: 3.8905
[09/26 17:10:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 88.50	
[09/26 17:10:51 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 17:10:58 visual_prompt]: Epoch 70 / 100: avg data time: 6.49e-02, avg batch time: 0.5085, average train loss: 0.0489
[09/26 17:11:00 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1670, average loss: 4.2831
[09/26 17:11:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 89.50	
[09/26 17:11:00 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 17:11:07 visual_prompt]: Epoch 71 / 100: avg data time: 6.49e-02, avg batch time: 0.5069, average train loss: 0.0475
[09/26 17:11:08 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1673, average loss: 3.9310
[09/26 17:11:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 88.00	
[09/26 17:11:08 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 17:11:15 visual_prompt]: Epoch 72 / 100: avg data time: 5.65e-02, avg batch time: 0.4988, average train loss: 0.0354
[09/26 17:11:17 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1674, average loss: 4.4974
[09/26 17:11:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 87.50	
[09/26 17:11:17 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 17:11:24 visual_prompt]: Epoch 73 / 100: avg data time: 5.49e-02, avg batch time: 0.4992, average train loss: 0.0500
[09/26 17:11:25 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1671, average loss: 3.9887
[09/26 17:11:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 88.50	
[09/26 17:11:25 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 17:11:32 visual_prompt]: Epoch 74 / 100: avg data time: 6.41e-02, avg batch time: 0.5069, average train loss: 0.0314
[09/26 17:11:34 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1668, average loss: 3.9791
[09/26 17:11:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 89.00	
[09/26 17:11:34 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 17:11:41 visual_prompt]: Epoch 75 / 100: avg data time: 6.64e-02, avg batch time: 0.5102, average train loss: 0.0188
[09/26 17:11:42 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1671, average loss: 3.9713
[09/26 17:11:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 91.50	
[09/26 17:11:42 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 17:11:49 visual_prompt]: Epoch 76 / 100: avg data time: 6.39e-02, avg batch time: 0.5071, average train loss: 0.0149
[09/26 17:11:51 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.1668, average loss: 4.0199
[09/26 17:11:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 90.50	
[09/26 17:11:51 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 17:11:58 visual_prompt]: Epoch 77 / 100: avg data time: 6.68e-02, avg batch time: 0.5096, average train loss: 0.0127
[09/26 17:12:00 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1672, average loss: 4.1681
[09/26 17:12:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 89.00	
[09/26 17:12:00 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 17:12:06 visual_prompt]: Epoch 78 / 100: avg data time: 5.10e-02, avg batch time: 0.4957, average train loss: 0.0109
[09/26 17:12:08 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1686, average loss: 4.0868
[09/26 17:12:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 89.00	
[09/26 17:12:08 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 17:12:15 visual_prompt]: Epoch 79 / 100: avg data time: 6.35e-02, avg batch time: 0.5064, average train loss: 0.0104
[09/26 17:12:17 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1673, average loss: 4.0550
[09/26 17:12:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 88.50	
[09/26 17:12:17 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 17:12:24 visual_prompt]: Epoch 80 / 100: avg data time: 6.29e-02, avg batch time: 0.5075, average train loss: 0.0095
[09/26 17:12:25 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1677, average loss: 4.1189
[09/26 17:12:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 88.50	
[09/26 17:12:25 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 17:12:32 visual_prompt]: Epoch 81 / 100: avg data time: 5.77e-02, avg batch time: 0.5005, average train loss: 0.0073
[09/26 17:12:34 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1672, average loss: 4.0978
[09/26 17:12:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 88.50	
[09/26 17:12:34 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 17:12:41 visual_prompt]: Epoch 82 / 100: avg data time: 5.49e-02, avg batch time: 0.4972, average train loss: 0.0066
[09/26 17:12:42 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1674, average loss: 4.1019
[09/26 17:12:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 88.50	
[09/26 17:12:42 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 17:12:49 visual_prompt]: Epoch 83 / 100: avg data time: 6.33e-02, avg batch time: 0.5081, average train loss: 0.0065
[09/26 17:12:51 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1672, average loss: 4.0969
[09/26 17:12:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.00	
[09/26 17:12:51 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 17:12:58 visual_prompt]: Epoch 84 / 100: avg data time: 6.25e-02, avg batch time: 0.5055, average train loss: 0.0083
[09/26 17:12:59 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1674, average loss: 4.0885
[09/26 17:12:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 89.00	
[09/26 17:12:59 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 17:13:06 visual_prompt]: Epoch 85 / 100: avg data time: 6.34e-02, avg batch time: 0.5068, average train loss: 0.0059
[09/26 17:13:08 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1676, average loss: 4.0816
[09/26 17:13:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 88.50	
[09/26 17:13:08 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 17:13:15 visual_prompt]: Epoch 86 / 100: avg data time: 6.31e-02, avg batch time: 0.5068, average train loss: 0.0061
[09/26 17:13:16 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.1669, average loss: 4.0952
[09/26 17:13:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 88.50	
[09/26 17:13:16 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 17:13:23 visual_prompt]: Epoch 87 / 100: avg data time: 6.20e-02, avg batch time: 0.5054, average train loss: 0.0059
[09/26 17:13:25 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1672, average loss: 4.1035
[09/26 17:13:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 88.50	
[09/26 17:13:25 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 17:13:32 visual_prompt]: Epoch 88 / 100: avg data time: 5.93e-02, avg batch time: 0.5058, average train loss: 0.0049
[09/26 17:13:33 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1672, average loss: 4.1105
[09/26 17:13:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.00	
[09/26 17:13:33 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 17:13:40 visual_prompt]: Epoch 89 / 100: avg data time: 6.31e-02, avg batch time: 0.5064, average train loss: 0.0058
[09/26 17:13:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1677, average loss: 4.1197
[09/26 17:13:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 89.00	
[09/26 17:13:42 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 17:13:49 visual_prompt]: Epoch 90 / 100: avg data time: 5.79e-02, avg batch time: 0.5007, average train loss: 0.0070
[09/26 17:13:51 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1675, average loss: 4.1210
[09/26 17:13:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.00	
[09/26 17:13:51 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 17:13:58 visual_prompt]: Epoch 91 / 100: avg data time: 6.25e-02, avg batch time: 0.5067, average train loss: 0.0054
[09/26 17:13:59 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1676, average loss: 4.1212
[09/26 17:13:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 88.50	
[09/26 17:13:59 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 17:14:06 visual_prompt]: Epoch 92 / 100: avg data time: 5.61e-02, avg batch time: 0.5009, average train loss: 0.0056
[09/26 17:14:08 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1674, average loss: 4.1242
[09/26 17:14:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.00	
[09/26 17:14:08 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 17:14:15 visual_prompt]: Epoch 93 / 100: avg data time: 5.62e-02, avg batch time: 0.5005, average train loss: 0.0046
[09/26 17:14:16 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1676, average loss: 4.1266
[09/26 17:14:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 88.50	
[09/26 17:14:16 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 17:14:23 visual_prompt]: Epoch 94 / 100: avg data time: 7.16e-02, avg batch time: 0.5150, average train loss: 0.0046
[09/26 17:14:25 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1674, average loss: 4.1265
[09/26 17:14:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 88.50	
[09/26 17:14:25 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 17:14:32 visual_prompt]: Epoch 95 / 100: avg data time: 6.41e-02, avg batch time: 0.5065, average train loss: 0.0053
[09/26 17:14:33 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1675, average loss: 4.1272
[09/26 17:14:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 88.50	
[09/26 17:14:33 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 17:14:40 visual_prompt]: Epoch 96 / 100: avg data time: 6.86e-02, avg batch time: 0.5121, average train loss: 0.0049
[09/26 17:14:42 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1673, average loss: 4.1274
[09/26 17:14:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 88.50	
[09/26 17:14:42 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 17:14:49 visual_prompt]: Epoch 97 / 100: avg data time: 6.49e-02, avg batch time: 0.5076, average train loss: 0.0045
[09/26 17:14:51 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1675, average loss: 4.1276
[09/26 17:14:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.00	
[09/26 17:14:51 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 17:14:57 visual_prompt]: Epoch 98 / 100: avg data time: 5.03e-02, avg batch time: 0.4949, average train loss: 0.0054
[09/26 17:14:59 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1673, average loss: 4.1278
[09/26 17:14:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.00	
[09/26 17:14:59 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 17:15:06 visual_prompt]: Epoch 99 / 100: avg data time: 6.51e-02, avg batch time: 0.5070, average train loss: 0.0046
[09/26 17:15:08 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1671, average loss: 4.1279
[09/26 17:15:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.00	
[09/26 17:15:08 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 17:15:14 visual_prompt]: Epoch 100 / 100: avg data time: 6.46e-02, avg batch time: 0.5072, average train loss: 0.0052
[09/26 17:15:16 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1674, average loss: 4.1279
[09/26 17:15:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.00	
[09/26 17:15:16 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 17:15:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 17:15:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 17:15:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 17:15:16 visual_prompt]: Training with config:
[09/26 17:15:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 17:15:16 visual_prompt]: Loading training data...
[09/26 17:15:16 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 17:15:17 visual_prompt]: Number of images: 800
[09/26 17:15:17 visual_prompt]: Number of classes: 9 / 9
[09/26 17:15:17 visual_prompt]: Loading validation data...
[09/26 17:15:17 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 17:15:18 visual_prompt]: Number of images: 200
[09/26 17:15:18 visual_prompt]: Number of classes: 9 / 9
[09/26 17:15:18 visual_prompt]: Constructing models...
[09/26 17:15:20 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 17:15:20 visual_prompt]: tuned percent:0.542
[09/26 17:15:20 visual_prompt]: Device used for model: 0
[09/26 17:15:20 visual_prompt]: Setting up Evaluator...
[09/26 17:15:20 visual_prompt]: Setting up Trainer...
[09/26 17:15:20 visual_prompt]: 	Setting up the optimizer...
[09/26 17:15:20 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 17:15:27 visual_prompt]: Epoch 1 / 100: avg data time: 6.39e-02, avg batch time: 0.5061, average train loss: 2.8779
[09/26 17:15:29 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1671, average loss: 2.9516
[09/26 17:15:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 17:15:29 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 17:15:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 17:15:36 visual_prompt]: Epoch 2 / 100: avg data time: 5.94e-02, avg batch time: 0.5005, average train loss: 2.6527
[09/26 17:15:37 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1673, average loss: 2.4732
[09/26 17:15:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.50	top5: 56.00	
[09/26 17:15:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 17:15:44 visual_prompt]: Epoch 3 / 100: avg data time: 5.87e-02, avg batch time: 0.5019, average train loss: 2.3102
[09/26 17:15:46 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1672, average loss: 2.2263
[09/26 17:15:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 53.00	
[09/26 17:15:46 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 17:15:53 visual_prompt]: Epoch 4 / 100: avg data time: 5.97e-02, avg batch time: 0.5028, average train loss: 2.2153
[09/26 17:15:54 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1670, average loss: 2.2035
[09/26 17:15:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 54.50	
[09/26 17:15:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 17:16:01 visual_prompt]: Epoch 5 / 100: avg data time: 5.14e-02, avg batch time: 0.4943, average train loss: 2.2057
[09/26 17:16:03 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1668, average loss: 2.2233
[09/26 17:16:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.00	
[09/26 17:16:03 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 17:16:10 visual_prompt]: Epoch 6 / 100: avg data time: 6.62e-02, avg batch time: 0.5080, average train loss: 2.2252
[09/26 17:16:11 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1667, average loss: 2.2408
[09/26 17:16:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 17:16:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 17:16:18 visual_prompt]: Epoch 7 / 100: avg data time: 6.62e-02, avg batch time: 0.5078, average train loss: 2.1789
[09/26 17:16:20 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1669, average loss: 2.1407
[09/26 17:16:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 70.00	
[09/26 17:16:20 visual_prompt]: Best epoch 7: best metric: 0.145
[09/26 17:16:20 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 17:16:27 visual_prompt]: Epoch 8 / 100: avg data time: 5.98e-02, avg batch time: 0.5012, average train loss: 2.1175
[09/26 17:16:29 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1669, average loss: 2.2507
[09/26 17:16:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 63.50	
[09/26 17:16:29 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 17:16:35 visual_prompt]: Epoch 9 / 100: avg data time: 5.70e-02, avg batch time: 0.4997, average train loss: 2.0689
[09/26 17:16:37 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1669, average loss: 2.0793
[09/26 17:16:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 74.50	
[09/26 17:16:37 visual_prompt]: Best epoch 9: best metric: 0.185
[09/26 17:16:37 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 17:16:44 visual_prompt]: Epoch 10 / 100: avg data time: 6.58e-02, avg batch time: 0.5087, average train loss: 1.9872
[09/26 17:16:46 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1669, average loss: 2.2085
[09/26 17:16:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 75.00	
[09/26 17:16:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 17:16:53 visual_prompt]: Epoch 11 / 100: avg data time: 6.58e-02, avg batch time: 0.5083, average train loss: 1.9409
[09/26 17:16:54 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1672, average loss: 1.9995
[09/26 17:16:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 76.50	
[09/26 17:16:54 visual_prompt]: Best epoch 11: best metric: 0.195
[09/26 17:16:54 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 17:17:01 visual_prompt]: Epoch 12 / 100: avg data time: 6.61e-02, avg batch time: 0.5091, average train loss: 1.8625
[09/26 17:17:03 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1671, average loss: 1.9986
[09/26 17:17:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 80.50	
[09/26 17:17:03 visual_prompt]: Best epoch 12: best metric: 0.215
[09/26 17:17:03 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 17:17:10 visual_prompt]: Epoch 13 / 100: avg data time: 6.48e-02, avg batch time: 0.5097, average train loss: 1.8274
[09/26 17:17:11 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1672, average loss: 2.1361
[09/26 17:17:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 77.50	
[09/26 17:17:11 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 17:17:18 visual_prompt]: Epoch 14 / 100: avg data time: 6.66e-02, avg batch time: 0.5089, average train loss: 1.7159
[09/26 17:17:20 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1672, average loss: 1.8611
[09/26 17:17:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 85.00	
[09/26 17:17:20 visual_prompt]: Best epoch 14: best metric: 0.260
[09/26 17:17:20 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 17:17:27 visual_prompt]: Epoch 15 / 100: avg data time: 6.16e-02, avg batch time: 0.5046, average train loss: 1.6691
[09/26 17:17:29 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1674, average loss: 1.9197
[09/26 17:17:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 84.50	
[09/26 17:17:29 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 17:17:36 visual_prompt]: Epoch 16 / 100: avg data time: 6.46e-02, avg batch time: 0.5080, average train loss: 1.6009
[09/26 17:17:37 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1677, average loss: 2.6025
[09/26 17:17:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 67.50	
[09/26 17:17:37 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 17:17:44 visual_prompt]: Epoch 17 / 100: avg data time: 6.22e-02, avg batch time: 0.5042, average train loss: 1.7549
[09/26 17:17:46 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1675, average loss: 1.9782
[09/26 17:17:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 84.50	
[09/26 17:17:46 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 17:17:53 visual_prompt]: Epoch 18 / 100: avg data time: 5.55e-02, avg batch time: 0.4987, average train loss: 1.5815
[09/26 17:17:54 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1673, average loss: 1.8150
[09/26 17:17:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 87.00	
[09/26 17:17:54 visual_prompt]: Best epoch 18: best metric: 0.270
[09/26 17:17:54 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 17:18:01 visual_prompt]: Epoch 19 / 100: avg data time: 5.97e-02, avg batch time: 0.5060, average train loss: 1.4170
[09/26 17:18:03 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1675, average loss: 1.8999
[09/26 17:18:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 89.50	
[09/26 17:18:03 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 17:18:10 visual_prompt]: Epoch 20 / 100: avg data time: 6.31e-02, avg batch time: 0.5059, average train loss: 1.4661
[09/26 17:18:11 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1670, average loss: 1.7473
[09/26 17:18:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 92.50	
[09/26 17:18:11 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 17:18:18 visual_prompt]: Epoch 21 / 100: avg data time: 5.73e-02, avg batch time: 0.5002, average train loss: 1.2953
[09/26 17:18:20 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1673, average loss: 1.8164
[09/26 17:18:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 93.00	
[09/26 17:18:20 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 17:18:27 visual_prompt]: Epoch 22 / 100: avg data time: 6.04e-02, avg batch time: 0.5049, average train loss: 1.2415
[09/26 17:18:28 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1674, average loss: 1.9879
[09/26 17:18:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 87.50	
[09/26 17:18:28 visual_prompt]: Best epoch 22: best metric: 0.285
[09/26 17:18:28 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 17:18:35 visual_prompt]: Epoch 23 / 100: avg data time: 6.25e-02, avg batch time: 0.5055, average train loss: 1.3145
[09/26 17:18:37 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1676, average loss: 1.9965
[09/26 17:18:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 90.00	
[09/26 17:18:37 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 17:18:44 visual_prompt]: Epoch 24 / 100: avg data time: 5.41e-02, avg batch time: 0.4977, average train loss: 1.5127
[09/26 17:18:45 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1673, average loss: 1.8223
[09/26 17:18:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.00	
[09/26 17:18:45 visual_prompt]: Best epoch 24: best metric: 0.295
[09/26 17:18:45 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 17:18:52 visual_prompt]: Epoch 25 / 100: avg data time: 6.12e-02, avg batch time: 0.5047, average train loss: 1.3354
[09/26 17:18:54 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1674, average loss: 1.8401
[09/26 17:18:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 93.50	
[09/26 17:18:54 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 17:19:01 visual_prompt]: Epoch 26 / 100: avg data time: 6.19e-02, avg batch time: 0.5052, average train loss: 1.2839
[09/26 17:19:02 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1674, average loss: 1.7917
[09/26 17:19:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 94.00	
[09/26 17:19:02 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 17:19:09 visual_prompt]: Epoch 27 / 100: avg data time: 6.57e-02, avg batch time: 0.5089, average train loss: 1.2284
[09/26 17:19:11 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1674, average loss: 1.8190
[09/26 17:19:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 94.00	
[09/26 17:19:11 visual_prompt]: Best epoch 27: best metric: 0.300
[09/26 17:19:11 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 17:19:18 visual_prompt]: Epoch 28 / 100: avg data time: 6.32e-02, avg batch time: 0.5056, average train loss: 1.1221
[09/26 17:19:20 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1677, average loss: 2.0325
[09/26 17:19:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 91.50	
[09/26 17:19:20 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 17:19:27 visual_prompt]: Epoch 29 / 100: avg data time: 6.16e-02, avg batch time: 0.5036, average train loss: 1.0003
[09/26 17:19:28 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1677, average loss: 2.3319
[09/26 17:19:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 88.00	
[09/26 17:19:28 visual_prompt]: Best epoch 29: best metric: 0.320
[09/26 17:19:28 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 17:19:35 visual_prompt]: Epoch 30 / 100: avg data time: 6.44e-02, avg batch time: 0.5080, average train loss: 0.9644
[09/26 17:19:37 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1673, average loss: 2.3337
[09/26 17:19:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 17:19:37 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 17:19:44 visual_prompt]: Epoch 31 / 100: avg data time: 6.73e-02, avg batch time: 0.5099, average train loss: 0.9810
[09/26 17:19:45 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1672, average loss: 2.1785
[09/26 17:19:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 93.00	
[09/26 17:19:45 visual_prompt]: Best epoch 31: best metric: 0.325
[09/26 17:19:45 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 17:19:52 visual_prompt]: Epoch 32 / 100: avg data time: 6.72e-02, avg batch time: 0.5091, average train loss: 0.8829
[09/26 17:19:54 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1670, average loss: 2.6355
[09/26 17:19:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 89.50	
[09/26 17:19:54 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 17:20:01 visual_prompt]: Epoch 33 / 100: avg data time: 5.98e-02, avg batch time: 0.5037, average train loss: 0.9095
[09/26 17:20:03 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1675, average loss: 2.7163
[09/26 17:20:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 91.00	
[09/26 17:20:03 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 17:20:09 visual_prompt]: Epoch 34 / 100: avg data time: 6.23e-02, avg batch time: 0.5055, average train loss: 1.0716
[09/26 17:20:11 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1674, average loss: 2.3262
[09/26 17:20:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 94.00	
[09/26 17:20:11 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 17:20:18 visual_prompt]: Epoch 35 / 100: avg data time: 6.40e-02, avg batch time: 0.5063, average train loss: 1.0066
[09/26 17:20:20 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1673, average loss: 2.2506
[09/26 17:20:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 94.00	
[09/26 17:20:20 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 17:20:27 visual_prompt]: Epoch 36 / 100: avg data time: 6.42e-02, avg batch time: 0.5064, average train loss: 0.8583
[09/26 17:20:28 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1671, average loss: 2.3518
[09/26 17:20:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.00	
[09/26 17:20:28 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 17:20:35 visual_prompt]: Epoch 37 / 100: avg data time: 6.47e-02, avg batch time: 0.5069, average train loss: 0.7093
[09/26 17:20:37 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1670, average loss: 2.6616
[09/26 17:20:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 89.50	
[09/26 17:20:37 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 17:20:44 visual_prompt]: Epoch 38 / 100: avg data time: 6.38e-02, avg batch time: 0.5066, average train loss: 0.6558
[09/26 17:20:45 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1675, average loss: 2.6526
[09/26 17:20:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 93.50	
[09/26 17:20:45 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 17:20:52 visual_prompt]: Epoch 39 / 100: avg data time: 6.29e-02, avg batch time: 0.5048, average train loss: 0.5753
[09/26 17:20:54 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1671, average loss: 2.9998
[09/26 17:20:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 90.00	
[09/26 17:20:54 visual_prompt]: Best epoch 39: best metric: 0.335
[09/26 17:20:54 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 17:21:01 visual_prompt]: Epoch 40 / 100: avg data time: 6.34e-02, avg batch time: 0.5067, average train loss: 0.6983
[09/26 17:21:03 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1674, average loss: 2.8439
[09/26 17:21:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 17:21:03 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 17:21:09 visual_prompt]: Epoch 41 / 100: avg data time: 5.25e-02, avg batch time: 0.4972, average train loss: 0.6030
[09/26 17:21:11 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1675, average loss: 2.8855
[09/26 17:21:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 91.00	
[09/26 17:21:11 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 17:21:18 visual_prompt]: Epoch 42 / 100: avg data time: 6.46e-02, avg batch time: 0.5072, average train loss: 0.5853
[09/26 17:21:20 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1673, average loss: 2.8032
[09/26 17:21:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 92.00	
[09/26 17:21:20 visual_prompt]: Best epoch 42: best metric: 0.355
[09/26 17:21:20 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 17:21:27 visual_prompt]: Epoch 43 / 100: avg data time: 6.78e-02, avg batch time: 0.5109, average train loss: 0.5544
[09/26 17:21:28 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1672, average loss: 3.5547
[09/26 17:21:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 90.00	
[09/26 17:21:28 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 17:21:35 visual_prompt]: Epoch 44 / 100: avg data time: 6.33e-02, avg batch time: 0.5064, average train loss: 0.6542
[09/26 17:21:37 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1673, average loss: 3.1585
[09/26 17:21:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 17:21:37 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 17:21:44 visual_prompt]: Epoch 45 / 100: avg data time: 6.64e-02, avg batch time: 0.5091, average train loss: 0.6189
[09/26 17:21:45 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1674, average loss: 3.2658
[09/26 17:21:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 87.50	
[09/26 17:21:45 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 17:21:52 visual_prompt]: Epoch 46 / 100: avg data time: 6.02e-02, avg batch time: 0.5045, average train loss: 0.4425
[09/26 17:21:54 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1673, average loss: 3.5513
[09/26 17:21:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 87.50	
[09/26 17:21:54 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 17:22:01 visual_prompt]: Epoch 47 / 100: avg data time: 5.96e-02, avg batch time: 0.5029, average train loss: 0.4069
[09/26 17:22:03 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1673, average loss: 3.3295
[09/26 17:22:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 89.00	
[09/26 17:22:03 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 17:22:09 visual_prompt]: Epoch 48 / 100: avg data time: 5.07e-02, avg batch time: 0.4957, average train loss: 0.3627
[09/26 17:22:11 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1676, average loss: 3.2593
[09/26 17:22:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 91.50	
[09/26 17:22:11 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 17:22:18 visual_prompt]: Epoch 49 / 100: avg data time: 6.26e-02, avg batch time: 0.5062, average train loss: 0.3301
[09/26 17:22:20 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1674, average loss: 3.4875
[09/26 17:22:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.50	
[09/26 17:22:20 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 17:22:26 visual_prompt]: Epoch 50 / 100: avg data time: 5.63e-02, avg batch time: 0.4994, average train loss: 0.3520
[09/26 17:22:28 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1672, average loss: 3.7484
[09/26 17:22:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.00	
[09/26 17:22:28 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 17:22:35 visual_prompt]: Epoch 51 / 100: avg data time: 6.03e-02, avg batch time: 0.5050, average train loss: 0.3622
[09/26 17:22:37 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1674, average loss: 3.9036
[09/26 17:22:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 89.00	
[09/26 17:22:37 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 17:22:44 visual_prompt]: Epoch 52 / 100: avg data time: 6.09e-02, avg batch time: 0.5045, average train loss: 0.4204
[09/26 17:22:45 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1675, average loss: 4.8225
[09/26 17:22:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 81.50	
[09/26 17:22:45 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 17:22:52 visual_prompt]: Epoch 53 / 100: avg data time: 6.40e-02, avg batch time: 0.5078, average train loss: 0.4637
[09/26 17:22:54 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1676, average loss: 3.5090
[09/26 17:22:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 90.50	
[09/26 17:22:54 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 17:23:01 visual_prompt]: Epoch 54 / 100: avg data time: 5.83e-02, avg batch time: 0.5018, average train loss: 0.2612
[09/26 17:23:02 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1672, average loss: 3.3389
[09/26 17:23:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 89.50	
[09/26 17:23:02 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 17:23:09 visual_prompt]: Epoch 55 / 100: avg data time: 6.61e-02, avg batch time: 0.5092, average train loss: 0.2132
[09/26 17:23:11 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1672, average loss: 3.8055
[09/26 17:23:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 92.00	
[09/26 17:23:11 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 17:23:18 visual_prompt]: Epoch 56 / 100: avg data time: 6.75e-02, avg batch time: 0.5105, average train loss: 0.1356
[09/26 17:23:20 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1671, average loss: 3.8007
[09/26 17:23:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 92.50	
[09/26 17:23:20 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 17:23:27 visual_prompt]: Epoch 57 / 100: avg data time: 6.34e-02, avg batch time: 0.5088, average train loss: 0.1131
[09/26 17:23:28 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1674, average loss: 4.1901
[09/26 17:23:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 90.00	
[09/26 17:23:28 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 17:23:35 visual_prompt]: Epoch 58 / 100: avg data time: 6.18e-02, avg batch time: 0.5057, average train loss: 0.0958
[09/26 17:23:37 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1674, average loss: 4.4380
[09/26 17:23:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.50	
[09/26 17:23:37 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 17:23:44 visual_prompt]: Epoch 59 / 100: avg data time: 5.94e-02, avg batch time: 0.5046, average train loss: 0.1263
[09/26 17:23:45 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1674, average loss: 4.6906
[09/26 17:23:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 89.50	
[09/26 17:23:45 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 17:23:52 visual_prompt]: Epoch 60 / 100: avg data time: 6.18e-02, avg batch time: 0.5047, average train loss: 0.1086
[09/26 17:23:54 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1670, average loss: 4.5876
[09/26 17:23:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 88.50	
[09/26 17:23:54 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 17:24:01 visual_prompt]: Epoch 61 / 100: avg data time: 5.91e-02, avg batch time: 0.5017, average train loss: 0.1201
[09/26 17:24:02 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.1673, average loss: 4.7724
[09/26 17:24:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 91.00	
[09/26 17:24:02 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 17:24:09 visual_prompt]: Epoch 62 / 100: avg data time: 5.72e-02, avg batch time: 0.5010, average train loss: 0.0778
[09/26 17:24:11 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1667, average loss: 4.6718
[09/26 17:24:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.00	
[09/26 17:24:11 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 17:24:18 visual_prompt]: Epoch 63 / 100: avg data time: 6.16e-02, avg batch time: 0.5038, average train loss: 0.0860
[09/26 17:24:19 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1669, average loss: 4.7873
[09/26 17:24:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 92.50	
[09/26 17:24:19 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 17:24:26 visual_prompt]: Epoch 64 / 100: avg data time: 6.47e-02, avg batch time: 0.5067, average train loss: 0.0589
[09/26 17:24:28 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1667, average loss: 4.8710
[09/26 17:24:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 91.00	
[09/26 17:24:28 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 17:24:35 visual_prompt]: Epoch 65 / 100: avg data time: 6.77e-02, avg batch time: 0.5097, average train loss: 0.0315
[09/26 17:24:37 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1668, average loss: 4.8927
[09/26 17:24:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.50	
[09/26 17:24:37 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 17:24:44 visual_prompt]: Epoch 66 / 100: avg data time: 6.55e-02, avg batch time: 0.5075, average train loss: 0.0341
[09/26 17:24:45 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1670, average loss: 5.2596
[09/26 17:24:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 91.50	
[09/26 17:24:45 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 17:24:52 visual_prompt]: Epoch 67 / 100: avg data time: 6.69e-02, avg batch time: 0.5089, average train loss: 0.0371
[09/26 17:24:54 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1677, average loss: 5.0153
[09/26 17:24:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 94.50	
[09/26 17:24:54 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 17:25:01 visual_prompt]: Epoch 68 / 100: avg data time: 6.70e-02, avg batch time: 0.5093, average train loss: 0.0339
[09/26 17:25:03 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1677, average loss: 5.0763
[09/26 17:25:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 91.00	
[09/26 17:25:03 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 17:25:09 visual_prompt]: Epoch 69 / 100: avg data time: 5.84e-02, avg batch time: 0.5028, average train loss: 0.0423
[09/26 17:25:11 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1671, average loss: 5.3139
[09/26 17:25:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 88.00	
[09/26 17:25:11 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 17:25:18 visual_prompt]: Epoch 70 / 100: avg data time: 6.24e-02, avg batch time: 0.5053, average train loss: 0.0372
[09/26 17:25:20 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1674, average loss: 5.4737
[09/26 17:25:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 17:25:20 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 17:25:26 visual_prompt]: Epoch 71 / 100: avg data time: 5.80e-02, avg batch time: 0.5001, average train loss: 0.0324
[09/26 17:25:28 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1674, average loss: 5.3690
[09/26 17:25:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 93.00	
[09/26 17:25:28 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 17:25:35 visual_prompt]: Epoch 72 / 100: avg data time: 6.54e-02, avg batch time: 0.5079, average train loss: 0.0188
[09/26 17:25:37 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1675, average loss: 5.3134
[09/26 17:25:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.50	
[09/26 17:25:37 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 17:25:44 visual_prompt]: Epoch 73 / 100: avg data time: 6.49e-02, avg batch time: 0.5072, average train loss: 0.0123
[09/26 17:25:45 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1675, average loss: 5.5054
[09/26 17:25:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 93.00	
[09/26 17:25:45 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 17:25:52 visual_prompt]: Epoch 74 / 100: avg data time: 5.72e-02, avg batch time: 0.5008, average train loss: 0.0095
[09/26 17:25:54 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1674, average loss: 5.5037
[09/26 17:25:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 91.50	
[09/26 17:25:54 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 17:26:01 visual_prompt]: Epoch 75 / 100: avg data time: 6.30e-02, avg batch time: 0.5057, average train loss: 0.0078
[09/26 17:26:02 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1673, average loss: 5.4894
[09/26 17:26:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 91.50	
[09/26 17:26:02 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 17:26:09 visual_prompt]: Epoch 76 / 100: avg data time: 5.80e-02, avg batch time: 0.5000, average train loss: 0.0062
[09/26 17:26:11 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1675, average loss: 5.5286
[09/26 17:26:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 91.50	
[09/26 17:26:11 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 17:26:18 visual_prompt]: Epoch 77 / 100: avg data time: 5.24e-02, avg batch time: 0.4978, average train loss: 0.0090
[09/26 17:26:19 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1668, average loss: 5.5735
[09/26 17:26:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 91.50	
[09/26 17:26:19 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 17:26:26 visual_prompt]: Epoch 78 / 100: avg data time: 6.17e-02, avg batch time: 0.5047, average train loss: 0.0078
[09/26 17:26:28 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1673, average loss: 5.6010
[09/26 17:26:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 91.50	
[09/26 17:26:28 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 17:26:35 visual_prompt]: Epoch 79 / 100: avg data time: 6.08e-02, avg batch time: 0.5039, average train loss: 0.0071
[09/26 17:26:36 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1670, average loss: 5.5476
[09/26 17:26:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 92.50	
[09/26 17:26:36 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 17:26:43 visual_prompt]: Epoch 80 / 100: avg data time: 5.57e-02, avg batch time: 0.4996, average train loss: 0.0076
[09/26 17:26:45 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1673, average loss: 5.5689
[09/26 17:26:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 91.50	
[09/26 17:26:45 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 17:26:52 visual_prompt]: Epoch 81 / 100: avg data time: 6.74e-02, avg batch time: 0.5110, average train loss: 0.0066
[09/26 17:26:53 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1672, average loss: 5.5904
[09/26 17:26:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 91.50	
[09/26 17:26:53 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 17:27:00 visual_prompt]: Epoch 82 / 100: avg data time: 6.14e-02, avg batch time: 0.5035, average train loss: 0.0104
[09/26 17:27:02 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1673, average loss: 5.7358
[09/26 17:27:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 91.00	
[09/26 17:27:02 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 17:27:09 visual_prompt]: Epoch 83 / 100: avg data time: 6.36e-02, avg batch time: 0.5087, average train loss: 0.0082
[09/26 17:27:11 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1672, average loss: 5.7369
[09/26 17:27:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 91.50	
[09/26 17:27:11 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 17:27:18 visual_prompt]: Epoch 84 / 100: avg data time: 6.26e-02, avg batch time: 0.5057, average train loss: 0.0056
[09/26 17:27:19 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1675, average loss: 5.6831
[09/26 17:27:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 92.00	
[09/26 17:27:19 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 17:27:26 visual_prompt]: Epoch 85 / 100: avg data time: 5.45e-02, avg batch time: 0.4982, average train loss: 0.0074
[09/26 17:27:28 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1673, average loss: 5.8426
[09/26 17:27:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 91.50	
[09/26 17:27:28 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 17:27:35 visual_prompt]: Epoch 86 / 100: avg data time: 6.60e-02, avg batch time: 0.5099, average train loss: 0.0091
[09/26 17:27:36 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1677, average loss: 5.8835
[09/26 17:27:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 91.50	
[09/26 17:27:36 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 17:27:43 visual_prompt]: Epoch 87 / 100: avg data time: 6.12e-02, avg batch time: 0.5046, average train loss: 0.0065
[09/26 17:27:45 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1676, average loss: 5.7460
[09/26 17:27:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 92.00	
[09/26 17:27:45 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 17:27:52 visual_prompt]: Epoch 88 / 100: avg data time: 6.34e-02, avg batch time: 0.5071, average train loss: 0.0054
[09/26 17:27:53 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.1672, average loss: 5.7089
[09/26 17:27:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 92.00	
[09/26 17:27:53 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 17:28:00 visual_prompt]: Epoch 89 / 100: avg data time: 6.36e-02, avg batch time: 0.5061, average train loss: 0.0056
[09/26 17:28:02 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1678, average loss: 5.7162
[09/26 17:28:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 92.00	
[09/26 17:28:02 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 17:28:09 visual_prompt]: Epoch 90 / 100: avg data time: 6.60e-02, avg batch time: 0.5102, average train loss: 0.0127
[09/26 17:28:11 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1671, average loss: 5.7783
[09/26 17:28:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 92.00	
[09/26 17:28:11 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 17:28:18 visual_prompt]: Epoch 91 / 100: avg data time: 6.64e-02, avg batch time: 0.5104, average train loss: 0.0060
[09/26 17:28:19 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1673, average loss: 5.8066
[09/26 17:28:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 91.00	
[09/26 17:28:19 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 17:28:26 visual_prompt]: Epoch 92 / 100: avg data time: 6.19e-02, avg batch time: 0.5046, average train loss: 0.0072
[09/26 17:28:28 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1670, average loss: 5.8003
[09/26 17:28:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 91.50	
[09/26 17:28:28 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 17:28:35 visual_prompt]: Epoch 93 / 100: avg data time: 5.84e-02, avg batch time: 0.5016, average train loss: 0.0053
[09/26 17:28:36 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1671, average loss: 5.7958
[09/26 17:28:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 91.50	
[09/26 17:28:36 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 17:28:43 visual_prompt]: Epoch 94 / 100: avg data time: 5.99e-02, avg batch time: 0.5034, average train loss: 0.0042
[09/26 17:28:45 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1671, average loss: 5.7870
[09/26 17:28:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 17:28:45 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 17:28:52 visual_prompt]: Epoch 95 / 100: avg data time: 5.80e-02, avg batch time: 0.5008, average train loss: 0.0048
[09/26 17:28:53 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1672, average loss: 5.7842
[09/26 17:28:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 17:28:53 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 17:29:00 visual_prompt]: Epoch 96 / 100: avg data time: 5.65e-02, avg batch time: 0.5020, average train loss: 0.0054
[09/26 17:29:02 visual_prompt]: Inference (val):avg data time: 4.43e-05, avg batch time: 0.1670, average loss: 5.7882
[09/26 17:29:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 17:29:02 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 17:29:09 visual_prompt]: Epoch 97 / 100: avg data time: 6.86e-02, avg batch time: 0.5123, average train loss: 0.0043
[09/26 17:29:11 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1676, average loss: 5.7902
[09/26 17:29:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 17:29:11 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 17:29:17 visual_prompt]: Epoch 98 / 100: avg data time: 6.14e-02, avg batch time: 0.5039, average train loss: 0.0050
[09/26 17:29:19 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1673, average loss: 5.7917
[09/26 17:29:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 17:29:19 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 17:29:26 visual_prompt]: Epoch 99 / 100: avg data time: 5.11e-02, avg batch time: 0.4960, average train loss: 0.0034
[09/26 17:29:28 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1677, average loss: 5.7919
[09/26 17:29:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 17:29:28 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 17:29:34 visual_prompt]: Epoch 100 / 100: avg data time: 6.44e-02, avg batch time: 0.5074, average train loss: 0.0034
[09/26 17:29:36 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1676, average loss: 5.7919
[09/26 17:29:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 17:29:36 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 17:29:36 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 17:29:36 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 17:29:36 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 17:29:36 visual_prompt]: Training with config:
[09/26 17:29:36 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.25_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.25, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 17:29:36 visual_prompt]: Loading training data...
[09/26 17:29:36 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 17:29:37 visual_prompt]: Number of images: 800
[09/26 17:29:37 visual_prompt]: Number of classes: 9 / 9
[09/26 17:29:37 visual_prompt]: Loading validation data...
[09/26 17:29:37 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 17:29:38 visual_prompt]: Number of images: 200
[09/26 17:29:38 visual_prompt]: Number of classes: 9 / 9
[09/26 17:29:38 visual_prompt]: Constructing models...
[09/26 17:29:40 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 17:29:40 visual_prompt]: tuned percent:0.542
[09/26 17:29:40 visual_prompt]: Device used for model: 0
[09/26 17:29:40 visual_prompt]: Setting up Evaluator...
[09/26 17:29:40 visual_prompt]: Setting up Trainer...
[09/26 17:29:40 visual_prompt]: 	Setting up the optimizer...
[09/26 17:29:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 17:29:47 visual_prompt]: Epoch 1 / 100: avg data time: 6.90e-02, avg batch time: 0.5130, average train loss: 2.8694
[09/26 17:29:49 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1675, average loss: 2.9516
[09/26 17:29:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 17:29:49 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 17:29:49 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.025
[09/26 17:29:56 visual_prompt]: Epoch 2 / 100: avg data time: 6.14e-02, avg batch time: 0.5038, average train loss: 2.6613
[09/26 17:29:58 visual_prompt]: Inference (val):avg data time: 4.95e-05, avg batch time: 0.1670, average loss: 2.4720
[09/26 17:29:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/26 17:29:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.05
[09/26 17:30:04 visual_prompt]: Epoch 3 / 100: avg data time: 6.71e-02, avg batch time: 0.5094, average train loss: 2.2855
[09/26 17:30:06 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1674, average loss: 2.2259
[09/26 17:30:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.00	
[09/26 17:30:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.075
[09/26 17:30:13 visual_prompt]: Epoch 4 / 100: avg data time: 6.67e-02, avg batch time: 0.5086, average train loss: 2.2255
[09/26 17:30:15 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1669, average loss: 2.2376
[09/26 17:30:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 56.50	
[09/26 17:30:15 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.1
[09/26 17:30:22 visual_prompt]: Epoch 5 / 100: avg data time: 6.45e-02, avg batch time: 0.5074, average train loss: 2.2305
[09/26 17:30:23 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1670, average loss: 2.2135
[09/26 17:30:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/26 17:30:23 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.125
[09/26 17:30:30 visual_prompt]: Epoch 6 / 100: avg data time: 6.43e-02, avg batch time: 0.5071, average train loss: 2.2190
[09/26 17:30:32 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1670, average loss: 2.2118
[09/26 17:30:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.00	
[09/26 17:30:32 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.15
[09/26 17:30:39 visual_prompt]: Epoch 7 / 100: avg data time: 6.12e-02, avg batch time: 0.5053, average train loss: 2.1895
[09/26 17:30:41 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1671, average loss: 2.3056
[09/26 17:30:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/26 17:30:41 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.175
[09/26 17:30:47 visual_prompt]: Epoch 8 / 100: avg data time: 5.57e-02, avg batch time: 0.4990, average train loss: 2.1683
[09/26 17:30:49 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1671, average loss: 2.1558
[09/26 17:30:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 65.50	
[09/26 17:30:49 visual_prompt]: Best epoch 8: best metric: 0.160
[09/26 17:30:49 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.2
[09/26 17:30:56 visual_prompt]: Epoch 9 / 100: avg data time: 5.99e-02, avg batch time: 0.5032, average train loss: 2.0843
[09/26 17:30:58 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1673, average loss: 2.0560
[09/26 17:30:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 77.00	
[09/26 17:30:58 visual_prompt]: Best epoch 9: best metric: 0.205
[09/26 17:30:58 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.225
[09/26 17:31:05 visual_prompt]: Epoch 10 / 100: avg data time: 6.58e-02, avg batch time: 0.5082, average train loss: 1.9848
[09/26 17:31:06 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1673, average loss: 2.1915
[09/26 17:31:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 72.00	
[09/26 17:31:06 visual_prompt]: Best epoch 10: best metric: 0.235
[09/26 17:31:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.25
[09/26 17:31:13 visual_prompt]: Epoch 11 / 100: avg data time: 5.99e-02, avg batch time: 0.5034, average train loss: 2.0130
[09/26 17:31:15 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1672, average loss: 2.1483
[09/26 17:31:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 73.50	
[09/26 17:31:15 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.24992385337738698
[09/26 17:31:22 visual_prompt]: Epoch 12 / 100: avg data time: 6.14e-02, avg batch time: 0.5035, average train loss: 1.8833
[09/26 17:31:23 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1671, average loss: 1.9981
[09/26 17:31:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 77.50	
[09/26 17:31:23 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.24969550628247802
[09/26 17:31:30 visual_prompt]: Epoch 13 / 100: avg data time: 6.15e-02, avg batch time: 0.5042, average train loss: 1.7793
[09/26 17:31:32 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1672, average loss: 1.8941
[09/26 17:31:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 84.50	
[09/26 17:31:32 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.24931523692103416
[09/26 17:31:39 visual_prompt]: Epoch 14 / 100: avg data time: 5.21e-02, avg batch time: 0.4961, average train loss: 1.7260
[09/26 17:31:40 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1673, average loss: 1.8853
[09/26 17:31:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 84.00	
[09/26 17:31:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.2487835085926963
[09/26 17:31:47 visual_prompt]: Epoch 15 / 100: avg data time: 5.90e-02, avg batch time: 0.5024, average train loss: 1.6706
[09/26 17:31:49 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1676, average loss: 1.7893
[09/26 17:31:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 90.50	
[09/26 17:31:49 visual_prompt]: Best epoch 15: best metric: 0.255
[09/26 17:31:49 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.24810096912652602
[09/26 17:31:56 visual_prompt]: Epoch 16 / 100: avg data time: 6.24e-02, avg batch time: 0.5049, average train loss: 1.5448
[09/26 17:31:57 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1674, average loss: 1.9214
[09/26 17:31:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 90.50	
[09/26 17:31:57 visual_prompt]: Best epoch 16: best metric: 0.285
[09/26 17:31:57 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.2472684500917257
[09/26 17:32:04 visual_prompt]: Epoch 17 / 100: avg data time: 6.12e-02, avg batch time: 0.5035, average train loss: 1.6075
[09/26 17:32:06 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.1671, average loss: 1.9677
[09/26 17:32:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 85.50	
[09/26 17:32:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.24628696578449955
[09/26 17:32:13 visual_prompt]: Epoch 18 / 100: avg data time: 5.24e-02, avg batch time: 0.4964, average train loss: 1.6804
[09/26 17:32:15 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1674, average loss: 1.7106
[09/26 17:32:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 88.50	
[09/26 17:32:15 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.24515771199228986
[09/26 17:32:21 visual_prompt]: Epoch 19 / 100: avg data time: 4.93e-02, avg batch time: 0.4946, average train loss: 1.4557
[09/26 17:32:23 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1673, average loss: 2.3539
[09/26 17:32:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 79.50	
[09/26 17:32:23 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.2438820645368942
[09/26 17:32:30 visual_prompt]: Epoch 20 / 100: avg data time: 6.46e-02, avg batch time: 0.5091, average train loss: 1.4610
[09/26 17:32:32 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1673, average loss: 1.7878
[09/26 17:32:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 92.00	
[09/26 17:32:32 visual_prompt]: Best epoch 20: best metric: 0.290
[09/26 17:32:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.24246157759823855
[09/26 17:32:39 visual_prompt]: Epoch 21 / 100: avg data time: 6.54e-02, avg batch time: 0.5100, average train loss: 1.3422
[09/26 17:32:40 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1674, average loss: 1.7459
[09/26 17:32:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 94.50	
[09/26 17:32:40 visual_prompt]: Best epoch 21: best metric: 0.295
[09/26 17:32:40 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.24089798182084843
[09/26 17:32:47 visual_prompt]: Epoch 22 / 100: avg data time: 6.31e-02, avg batch time: 0.5059, average train loss: 1.3335
[09/26 17:32:49 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1674, average loss: 2.5450
[09/26 17:32:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 79.50	
[09/26 17:32:49 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.2391931822053251
[09/26 17:32:56 visual_prompt]: Epoch 23 / 100: avg data time: 5.98e-02, avg batch time: 0.5034, average train loss: 1.4995
[09/26 17:32:57 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.1675, average loss: 1.6939
[09/26 17:32:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 93.00	
[09/26 17:32:57 visual_prompt]: Best epoch 23: best metric: 0.300
[09/26 17:32:57 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.23734925578739588
[09/26 17:33:04 visual_prompt]: Epoch 24 / 100: avg data time: 6.03e-02, avg batch time: 0.5026, average train loss: 1.2920
[09/26 17:33:06 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1671, average loss: 1.8340
[09/26 17:33:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 91.50	
[09/26 17:33:06 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.23536844910736587
[09/26 17:33:13 visual_prompt]: Epoch 25 / 100: avg data time: 5.51e-02, avg batch time: 0.4987, average train loss: 1.1910
[09/26 17:33:15 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1674, average loss: 2.2535
[09/26 17:33:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 86.50	
[09/26 17:33:15 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.23325317547305485
[09/26 17:33:21 visual_prompt]: Epoch 26 / 100: avg data time: 5.98e-02, avg batch time: 0.5019, average train loss: 1.0979
[09/26 17:33:23 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1676, average loss: 1.7342
[09/26 17:33:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 95.50	
[09/26 17:33:23 visual_prompt]: Best epoch 26: best metric: 0.335
[09/26 17:33:23 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.23100601201955323
[09/26 17:33:30 visual_prompt]: Epoch 27 / 100: avg data time: 5.21e-02, avg batch time: 0.4970, average train loss: 1.0094
[09/26 17:33:32 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1676, average loss: 3.3713
[09/26 17:33:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 77.50	
[09/26 17:33:32 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.2286296965693802
[09/26 17:33:38 visual_prompt]: Epoch 28 / 100: avg data time: 6.32e-02, avg batch time: 0.5079, average train loss: 1.1928
[09/26 17:33:40 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1676, average loss: 2.0541
[09/26 17:33:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 88.50	
[09/26 17:33:40 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.22612712429686843
[09/26 17:33:47 visual_prompt]: Epoch 29 / 100: avg data time: 6.58e-02, avg batch time: 0.5103, average train loss: 0.9464
[09/26 17:33:49 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.1674, average loss: 2.0928
[09/26 17:33:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 91.50	
[09/26 17:33:49 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.22350134420084022
[09/26 17:33:56 visual_prompt]: Epoch 30 / 100: avg data time: 6.74e-02, avg batch time: 0.5107, average train loss: 0.9770
[09/26 17:33:57 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1675, average loss: 2.1244
[09/26 17:33:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 92.50	
[09/26 17:33:57 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.22075555538987224
[09/26 17:34:04 visual_prompt]: Epoch 31 / 100: avg data time: 6.46e-02, avg batch time: 0.5068, average train loss: 0.9435
[09/26 17:34:06 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1674, average loss: 2.0655
[09/26 17:34:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 92.00	
[09/26 17:34:06 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.21789310318467428
[09/26 17:34:13 visual_prompt]: Epoch 32 / 100: avg data time: 6.84e-02, avg batch time: 0.5108, average train loss: 0.9100
[09/26 17:34:15 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1674, average loss: 3.5287
[09/26 17:34:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 79.00	
[09/26 17:34:15 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.21491747504233139
[09/26 17:34:22 visual_prompt]: Epoch 33 / 100: avg data time: 6.44e-02, avg batch time: 0.5072, average train loss: 0.8238
[09/26 17:34:23 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1673, average loss: 2.3222
[09/26 17:34:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 91.50	
[09/26 17:34:23 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.21183229630737466
[09/26 17:34:30 visual_prompt]: Epoch 34 / 100: avg data time: 6.25e-02, avg batch time: 0.5046, average train loss: 0.7746
[09/26 17:34:32 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1671, average loss: 2.3668
[09/26 17:34:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 92.00	
[09/26 17:34:32 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.20864132579485728
[09/26 17:34:39 visual_prompt]: Epoch 35 / 100: avg data time: 6.61e-02, avg batch time: 0.5093, average train loss: 0.7045
[09/26 17:34:40 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1669, average loss: 2.7898
[09/26 17:34:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 89.00	
[09/26 17:34:40 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.2053484512108174
[09/26 17:34:47 visual_prompt]: Epoch 36 / 100: avg data time: 6.58e-02, avg batch time: 0.5077, average train loss: 0.6292
[09/26 17:34:49 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1674, average loss: 2.8127
[09/26 17:34:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 91.50	
[09/26 17:34:49 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.20195768441570727
[09/26 17:34:56 visual_prompt]: Epoch 37 / 100: avg data time: 6.19e-02, avg batch time: 0.5043, average train loss: 0.5564
[09/26 17:34:57 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1669, average loss: 2.9980
[09/26 17:34:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.00	
[09/26 17:34:58 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.19847315653655914
[09/26 17:35:04 visual_prompt]: Epoch 38 / 100: avg data time: 5.51e-02, avg batch time: 0.4995, average train loss: 0.6679
[09/26 17:35:06 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1669, average loss: 2.6489
[09/26 17:35:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 95.50	
[09/26 17:35:06 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.19489911293384335
[09/26 17:35:13 visual_prompt]: Epoch 39 / 100: avg data time: 6.24e-02, avg batch time: 0.5059, average train loss: 0.5764
[09/26 17:35:15 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1670, average loss: 2.8305
[09/26 17:35:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 91.50	
[09/26 17:35:15 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.1912399080291506
[09/26 17:35:21 visual_prompt]: Epoch 40 / 100: avg data time: 6.44e-02, avg batch time: 0.5062, average train loss: 0.5352
[09/26 17:35:23 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1669, average loss: 3.1294
[09/26 17:35:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 92.50	
[09/26 17:35:23 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.1875
[09/26 17:35:30 visual_prompt]: Epoch 41 / 100: avg data time: 5.51e-02, avg batch time: 0.4999, average train loss: 0.4710
[09/26 17:35:32 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1671, average loss: 3.3974
[09/26 17:35:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.00	
[09/26 17:35:32 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.18368394534823634
[09/26 17:35:39 visual_prompt]: Epoch 42 / 100: avg data time: 6.68e-02, avg batch time: 0.5096, average train loss: 0.4149
[09/26 17:35:40 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1671, average loss: 3.8890
[09/26 17:35:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 86.00	
[09/26 17:35:40 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.17979639334863468
[09/26 17:35:47 visual_prompt]: Epoch 43 / 100: avg data time: 6.43e-02, avg batch time: 0.5065, average train loss: 0.4415
[09/26 17:35:49 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1671, average loss: 3.4990
[09/26 17:35:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 88.00	
[09/26 17:35:49 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.17584208038447505
[09/26 17:35:56 visual_prompt]: Epoch 44 / 100: avg data time: 6.12e-02, avg batch time: 0.5038, average train loss: 0.4061
[09/26 17:35:57 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1674, average loss: 3.8699
[09/26 17:35:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.00	
[09/26 17:35:57 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.17182582417698902
[09/26 17:36:04 visual_prompt]: Epoch 45 / 100: avg data time: 5.84e-02, avg batch time: 0.5019, average train loss: 0.4395
[09/26 17:36:06 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1673, average loss: 4.1187
[09/26 17:36:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 86.00	
[09/26 17:36:06 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.1677525179157086
[09/26 17:36:13 visual_prompt]: Epoch 46 / 100: avg data time: 4.94e-02, avg batch time: 0.4951, average train loss: 0.3241
[09/26 17:36:14 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1671, average loss: 3.2296
[09/26 17:36:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 91.50	
[09/26 17:36:14 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.16362712429686843
[09/26 17:36:21 visual_prompt]: Epoch 47 / 100: avg data time: 6.66e-02, avg batch time: 0.5101, average train loss: 0.2605
[09/26 17:36:23 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1675, average loss: 4.0421
[09/26 17:36:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 17:36:23 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.1594546694771249
[09/26 17:36:30 visual_prompt]: Epoch 48 / 100: avg data time: 5.97e-02, avg batch time: 0.5043, average train loss: 0.3117
[09/26 17:36:32 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1674, average loss: 4.2837
[09/26 17:36:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 88.00	
[09/26 17:36:32 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.15524023694995845
[09/26 17:36:38 visual_prompt]: Epoch 49 / 100: avg data time: 6.41e-02, avg batch time: 0.5087, average train loss: 0.3919
[09/26 17:36:40 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1674, average loss: 4.6183
[09/26 17:36:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 85.50	
[09/26 17:36:40 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.1509889613522199
[09/26 17:36:47 visual_prompt]: Epoch 50 / 100: avg data time: 5.51e-02, avg batch time: 0.4989, average train loss: 0.2956
[09/26 17:36:49 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1678, average loss: 3.6909
[09/26 17:36:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 88.00	
[09/26 17:36:49 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.14670602220836632
[09/26 17:36:56 visual_prompt]: Epoch 51 / 100: avg data time: 6.36e-02, avg batch time: 0.5066, average train loss: 0.2448
[09/26 17:36:57 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1673, average loss: 4.1153
[09/26 17:36:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 90.00	
[09/26 17:36:57 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.14239663762000818
[09/26 17:37:04 visual_prompt]: Epoch 52 / 100: avg data time: 5.93e-02, avg batch time: 0.5041, average train loss: 0.2083
[09/26 17:37:06 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1673, average loss: 4.0440
[09/26 17:37:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.50	
[09/26 17:37:06 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.1380660579084567
[09/26 17:37:13 visual_prompt]: Epoch 53 / 100: avg data time: 6.28e-02, avg batch time: 0.5061, average train loss: 0.1939
[09/26 17:37:14 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1675, average loss: 4.3897
[09/26 17:37:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 92.00	
[09/26 17:37:14 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.13371955921801565
[09/26 17:37:21 visual_prompt]: Epoch 54 / 100: avg data time: 6.04e-02, avg batch time: 0.5041, average train loss: 0.1385
[09/26 17:37:23 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1675, average loss: 4.8262
[09/26 17:37:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 90.00	
[09/26 17:37:23 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.12936243708781264
[09/26 17:37:30 visual_prompt]: Epoch 55 / 100: avg data time: 6.23e-02, avg batch time: 0.5053, average train loss: 0.1097
[09/26 17:37:31 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1672, average loss: 5.4069
[09/26 17:37:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 87.50	
[09/26 17:37:31 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.125
[09/26 17:37:38 visual_prompt]: Epoch 56 / 100: avg data time: 6.19e-02, avg batch time: 0.5041, average train loss: 0.1102
[09/26 17:37:40 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1670, average loss: 5.0293
[09/26 17:37:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 92.00	
[09/26 17:37:40 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.12063756291218741
[09/26 17:37:47 visual_prompt]: Epoch 57 / 100: avg data time: 6.51e-02, avg batch time: 0.5080, average train loss: 0.0595
[09/26 17:37:49 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1679, average loss: 5.5635
[09/26 17:37:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 93.00	
[09/26 17:37:49 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.11628044078198434
[09/26 17:37:55 visual_prompt]: Epoch 58 / 100: avg data time: 6.09e-02, avg batch time: 0.5054, average train loss: 0.0714
[09/26 17:37:57 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1672, average loss: 5.3672
[09/26 17:37:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 94.00	
[09/26 17:37:57 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.11193394209154334
[09/26 17:38:04 visual_prompt]: Epoch 59 / 100: avg data time: 6.06e-02, avg batch time: 0.5064, average train loss: 0.0858
[09/26 17:38:06 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1672, average loss: 5.7787
[09/26 17:38:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 91.50	
[09/26 17:38:06 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.10760336237999185
[09/26 17:38:13 visual_prompt]: Epoch 60 / 100: avg data time: 6.31e-02, avg batch time: 0.5067, average train loss: 0.0756
[09/26 17:38:14 visual_prompt]: Inference (val):avg data time: 4.50e-05, avg batch time: 0.1676, average loss: 5.5375
[09/26 17:38:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 92.00	
[09/26 17:38:14 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.10329397779163371
[09/26 17:38:21 visual_prompt]: Epoch 61 / 100: avg data time: 6.47e-02, avg batch time: 0.5068, average train loss: 0.0811
[09/26 17:38:23 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1670, average loss: 5.5918
[09/26 17:38:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 17:38:23 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.0990110386477801
[09/26 17:38:30 visual_prompt]: Epoch 62 / 100: avg data time: 5.97e-02, avg batch time: 0.5032, average train loss: 0.0719
[09/26 17:38:31 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1676, average loss: 6.5041
[09/26 17:38:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 86.50	
[09/26 17:38:31 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.09475976305004155
[09/26 17:38:38 visual_prompt]: Epoch 63 / 100: avg data time: 6.42e-02, avg batch time: 0.5072, average train loss: 0.0714
[09/26 17:38:40 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1674, average loss: 6.0568
[09/26 17:38:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 87.50	
[09/26 17:38:40 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.09054533052287511
[09/26 17:38:47 visual_prompt]: Epoch 64 / 100: avg data time: 6.11e-02, avg batch time: 0.5046, average train loss: 0.0835
[09/26 17:38:49 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1673, average loss: 5.7364
[09/26 17:38:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 89.50	
[09/26 17:38:49 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.08637287570313158
[09/26 17:38:55 visual_prompt]: Epoch 65 / 100: avg data time: 5.89e-02, avg batch time: 0.5020, average train loss: 0.0569
[09/26 17:38:57 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1676, average loss: 5.8695
[09/26 17:38:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 17:38:57 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.08224748208429142
[09/26 17:39:04 visual_prompt]: Epoch 66 / 100: avg data time: 6.52e-02, avg batch time: 0.5088, average train loss: 0.0453
[09/26 17:39:06 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1669, average loss: 6.1532
[09/26 17:39:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 88.50	
[09/26 17:39:06 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.07817417582301099
[09/26 17:39:13 visual_prompt]: Epoch 67 / 100: avg data time: 6.59e-02, avg batch time: 0.5091, average train loss: 0.0476
[09/26 17:39:14 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1675, average loss: 5.8668
[09/26 17:39:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 91.00	
[09/26 17:39:14 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.074157919615525
[09/26 17:39:21 visual_prompt]: Epoch 68 / 100: avg data time: 6.08e-02, avg batch time: 0.5032, average train loss: 0.0378
[09/26 17:39:23 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1674, average loss: 6.3769
[09/26 17:39:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 89.50	
[09/26 17:39:23 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.07020360665136531
[09/26 17:39:30 visual_prompt]: Epoch 69 / 100: avg data time: 6.62e-02, avg batch time: 0.5083, average train loss: 0.0182
[09/26 17:39:31 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1671, average loss: 6.2520
[09/26 17:39:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 17:39:31 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.06631605465176368
[09/26 17:39:38 visual_prompt]: Epoch 70 / 100: avg data time: 6.00e-02, avg batch time: 0.5029, average train loss: 0.0100
[09/26 17:39:40 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1675, average loss: 6.6056
[09/26 17:39:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 90.00	
[09/26 17:39:40 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.06250000000000003
[09/26 17:39:47 visual_prompt]: Epoch 71 / 100: avg data time: 7.04e-02, avg batch time: 0.5125, average train loss: 0.0093
[09/26 17:39:49 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1672, average loss: 6.5437
[09/26 17:39:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 91.50	
[09/26 17:39:49 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.0587600919708494
[09/26 17:39:55 visual_prompt]: Epoch 72 / 100: avg data time: 5.13e-02, avg batch time: 0.4947, average train loss: 0.0082
[09/26 17:39:57 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1672, average loss: 6.5942
[09/26 17:39:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 89.50	
[09/26 17:39:57 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.055100887066156665
[09/26 17:40:04 visual_prompt]: Epoch 73 / 100: avg data time: 6.27e-02, avg batch time: 0.5050, average train loss: 0.0080
[09/26 17:40:06 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1671, average loss: 6.5756
[09/26 17:40:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 89.50	
[09/26 17:40:06 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.05152684346344087
[09/26 17:40:12 visual_prompt]: Epoch 74 / 100: avg data time: 6.02e-02, avg batch time: 0.5021, average train loss: 0.0077
[09/26 17:40:14 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1673, average loss: 6.8119
[09/26 17:40:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.00	
[09/26 17:40:14 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.048042315584292714
[09/26 17:40:21 visual_prompt]: Epoch 75 / 100: avg data time: 6.80e-02, avg batch time: 0.5108, average train loss: 0.0045
[09/26 17:40:23 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1671, average loss: 6.8137
[09/26 17:40:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 91.00	
[09/26 17:40:23 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.04465154878918258
[09/26 17:40:29 visual_prompt]: Epoch 76 / 100: avg data time: 5.20e-02, avg batch time: 0.4949, average train loss: 0.0142
[09/26 17:40:31 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1672, average loss: 6.7150
[09/26 17:40:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.00	
[09/26 17:40:31 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.04135867420514276
[09/26 17:40:38 visual_prompt]: Epoch 77 / 100: avg data time: 6.61e-02, avg batch time: 0.5081, average train loss: 0.0043
[09/26 17:40:40 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1671, average loss: 6.7301
[09/26 17:40:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 89.50	
[09/26 17:40:40 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.03816770369262533
[09/26 17:40:47 visual_prompt]: Epoch 78 / 100: avg data time: 6.42e-02, avg batch time: 0.5066, average train loss: 0.0050
[09/26 17:40:48 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1673, average loss: 6.7988
[09/26 17:40:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 90.00	
[09/26 17:40:48 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.03508252495766863
[09/26 17:40:55 visual_prompt]: Epoch 79 / 100: avg data time: 6.09e-02, avg batch time: 0.5038, average train loss: 0.0050
[09/26 17:40:57 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1675, average loss: 6.8721
[09/26 17:40:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 90.00	
[09/26 17:40:57 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.032106896815325706
[09/26 17:41:04 visual_prompt]: Epoch 80 / 100: avg data time: 5.85e-02, avg batch time: 0.5013, average train loss: 0.0054
[09/26 17:41:05 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1669, average loss: 6.8808
[09/26 17:41:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 89.50	
[09/26 17:41:05 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.029244444610127762
[09/26 17:41:12 visual_prompt]: Epoch 81 / 100: avg data time: 6.22e-02, avg batch time: 0.5054, average train loss: 0.0045
[09/26 17:41:14 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1675, average loss: 6.8293
[09/26 17:41:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 89.50	
[09/26 17:41:14 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.026498655799159762
[09/26 17:41:21 visual_prompt]: Epoch 82 / 100: avg data time: 5.01e-02, avg batch time: 0.4952, average train loss: 0.0048
[09/26 17:41:22 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1673, average loss: 6.7970
[09/26 17:41:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 89.50	
[09/26 17:41:22 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.023872875703131582
[09/26 17:41:29 visual_prompt]: Epoch 83 / 100: avg data time: 6.39e-02, avg batch time: 0.5069, average train loss: 0.0057
[09/26 17:41:31 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1675, average loss: 6.7467
[09/26 17:41:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.00	
[09/26 17:41:31 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.021370303430619797
[09/26 17:41:38 visual_prompt]: Epoch 84 / 100: avg data time: 6.20e-02, avg batch time: 0.5046, average train loss: 0.0047
[09/26 17:41:39 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1671, average loss: 6.8661
[09/26 17:41:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 89.50	
[09/26 17:41:39 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.018993987980446755
[09/26 17:41:47 visual_prompt]: Epoch 85 / 100: avg data time: 4.82e-02, avg batch time: 0.4941, average train loss: 0.0050
[09/26 17:41:49 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1673, average loss: 6.8616
[09/26 17:41:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 90.00	
[09/26 17:41:49 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.01674682452694516
[09/26 17:41:56 visual_prompt]: Epoch 86 / 100: avg data time: 6.09e-02, avg batch time: 0.5046, average train loss: 0.0040
[09/26 17:41:58 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1673, average loss: 6.8379
[09/26 17:41:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 90.00	
[09/26 17:41:58 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.014631550892634126
[09/26 17:42:04 visual_prompt]: Epoch 87 / 100: avg data time: 5.33e-02, avg batch time: 0.4968, average train loss: 0.0052
[09/26 17:42:06 visual_prompt]: Inference (val):avg data time: 4.51e-05, avg batch time: 0.1673, average loss: 6.7736
[09/26 17:42:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 89.50	
[09/26 17:42:06 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.012650744212604148
[09/26 17:42:13 visual_prompt]: Epoch 88 / 100: avg data time: 6.18e-02, avg batch time: 0.5041, average train loss: 0.0030
[09/26 17:42:15 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1676, average loss: 6.7682
[09/26 17:42:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 17:42:15 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.010806817794674878
[09/26 17:42:21 visual_prompt]: Epoch 89 / 100: avg data time: 5.75e-02, avg batch time: 0.5002, average train loss: 0.0033
[09/26 17:42:23 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1670, average loss: 6.8279
[09/26 17:42:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 90.00	
[09/26 17:42:23 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.009102018179151586
[09/26 17:42:30 visual_prompt]: Epoch 90 / 100: avg data time: 6.79e-02, avg batch time: 0.5107, average train loss: 0.0032
[09/26 17:42:32 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1677, average loss: 6.8397
[09/26 17:42:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.50	
[09/26 17:42:32 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.00753842240176146
[09/26 17:42:39 visual_prompt]: Epoch 91 / 100: avg data time: 6.47e-02, avg batch time: 0.5078, average train loss: 0.0080
[09/26 17:42:40 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1670, average loss: 6.8546
[09/26 17:42:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 90.00	
[09/26 17:42:40 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.006117935463105809
[09/26 17:42:47 visual_prompt]: Epoch 92 / 100: avg data time: 5.26e-02, avg batch time: 0.4972, average train loss: 0.0025
[09/26 17:42:49 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1676, average loss: 6.8545
[09/26 17:42:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 90.00	
[09/26 17:42:49 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.004842288007710138
[09/26 17:42:56 visual_prompt]: Epoch 93 / 100: avg data time: 6.65e-02, avg batch time: 0.5100, average train loss: 0.0026
[09/26 17:42:58 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1676, average loss: 6.8645
[09/26 17:42:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.50	
[09/26 17:42:58 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.003713034215500441
[09/26 17:43:04 visual_prompt]: Epoch 94 / 100: avg data time: 5.97e-02, avg batch time: 0.5039, average train loss: 0.0033
[09/26 17:43:06 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1679, average loss: 6.8741
[09/26 17:43:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.50	
[09/26 17:43:06 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.002731549908274289
[09/26 17:43:13 visual_prompt]: Epoch 95 / 100: avg data time: 5.98e-02, avg batch time: 0.5020, average train loss: 0.0044
[09/26 17:43:15 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1675, average loss: 6.8797
[09/26 17:43:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.50	
[09/26 17:43:15 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0018990308734739975
[09/26 17:43:21 visual_prompt]: Epoch 96 / 100: avg data time: 5.98e-02, avg batch time: 0.5026, average train loss: 0.0035
[09/26 17:43:23 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1673, average loss: 6.8785
[09/26 17:43:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.50	
[09/26 17:43:23 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.0012164914073037048
[09/26 17:43:30 visual_prompt]: Epoch 97 / 100: avg data time: 5.11e-02, avg batch time: 0.4950, average train loss: 0.0027
[09/26 17:43:32 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1672, average loss: 6.8789
[09/26 17:43:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.50	
[09/26 17:43:32 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0006847630789658388
[09/26 17:43:38 visual_prompt]: Epoch 98 / 100: avg data time: 6.25e-02, avg batch time: 0.5064, average train loss: 0.0042
[09/26 17:43:40 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1677, average loss: 6.8794
[09/26 17:43:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.50	
[09/26 17:43:40 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.0003044937175219753
[09/26 17:43:47 visual_prompt]: Epoch 99 / 100: avg data time: 6.76e-02, avg batch time: 0.5102, average train loss: 0.0034
[09/26 17:43:49 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1676, average loss: 6.8795
[09/26 17:43:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.50	
[09/26 17:43:49 visual_prompt]: Training 100 / 100 epoch, with learning rate 7.614662261302974e-05
[09/26 17:43:56 visual_prompt]: Epoch 100 / 100: avg data time: 5.76e-02, avg batch time: 0.5004, average train loss: 0.0034
[09/26 17:43:57 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1672, average loss: 6.8794
[09/26 17:43:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 90.50	
[09/26 17:43:57 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 17:43:57 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 17:43:57 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 17:43:57 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 17:43:57 visual_prompt]: Training with config:
[09/26 17:43:57 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 17:43:57 visual_prompt]: Loading training data...
[09/26 17:43:57 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 17:43:58 visual_prompt]: Number of images: 800
[09/26 17:43:58 visual_prompt]: Number of classes: 9 / 9
[09/26 17:43:58 visual_prompt]: Loading validation data...
[09/26 17:43:58 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 17:43:59 visual_prompt]: Number of images: 200
[09/26 17:43:59 visual_prompt]: Number of classes: 9 / 9
[09/26 17:43:59 visual_prompt]: Constructing models...
[09/26 17:44:01 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 17:44:01 visual_prompt]: tuned percent:0.542
[09/26 17:44:01 visual_prompt]: Device used for model: 0
[09/26 17:44:01 visual_prompt]: Setting up Evaluator...
[09/26 17:44:01 visual_prompt]: Setting up Trainer...
[09/26 17:44:01 visual_prompt]: 	Setting up the optimizer...
[09/26 17:44:01 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 17:44:08 visual_prompt]: Epoch 1 / 100: avg data time: 5.72e-02, avg batch time: 0.5008, average train loss: 2.8707
[09/26 17:44:10 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1672, average loss: 2.9516
[09/26 17:44:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 17:44:10 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 17:44:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 17:44:17 visual_prompt]: Epoch 2 / 100: avg data time: 6.33e-02, avg batch time: 0.5053, average train loss: 2.4998
[09/26 17:44:19 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1671, average loss: 2.2216
[09/26 17:44:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 58.00	
[09/26 17:44:19 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 17:44:25 visual_prompt]: Epoch 3 / 100: avg data time: 5.26e-02, avg batch time: 0.4954, average train loss: 2.2675
[09/26 17:44:27 visual_prompt]: Inference (val):avg data time: 4.10e-05, avg batch time: 0.1671, average loss: 2.2177
[09/26 17:44:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 17:44:27 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 17:44:34 visual_prompt]: Epoch 4 / 100: avg data time: 6.17e-02, avg batch time: 0.5045, average train loss: 2.2227
[09/26 17:44:36 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1672, average loss: 2.2075
[09/26 17:44:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.00	
[09/26 17:44:36 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 17:44:43 visual_prompt]: Epoch 5 / 100: avg data time: 6.62e-02, avg batch time: 0.5080, average train loss: 2.2019
[09/26 17:44:44 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1668, average loss: 2.1980
[09/26 17:44:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 58.00	
[09/26 17:44:44 visual_prompt]: Best epoch 5: best metric: 0.135
[09/26 17:44:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 17:44:51 visual_prompt]: Epoch 6 / 100: avg data time: 6.57e-02, avg batch time: 0.5089, average train loss: 2.1962
[09/26 17:44:53 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.1669, average loss: 2.1995
[09/26 17:44:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 17:44:53 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 17:45:00 visual_prompt]: Epoch 7 / 100: avg data time: 6.41e-02, avg batch time: 0.5064, average train loss: 2.1954
[09/26 17:45:01 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1670, average loss: 2.2040
[09/26 17:45:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 58.00	
[09/26 17:45:01 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 17:45:08 visual_prompt]: Epoch 8 / 100: avg data time: 5.33e-02, avg batch time: 0.4971, average train loss: 2.1676
[09/26 17:45:10 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1668, average loss: 2.1355
[09/26 17:45:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 70.00	
[09/26 17:45:10 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 17:45:17 visual_prompt]: Epoch 9 / 100: avg data time: 5.80e-02, avg batch time: 0.5018, average train loss: 2.1102
[09/26 17:45:18 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1669, average loss: 2.0955
[09/26 17:45:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 71.00	
[09/26 17:45:18 visual_prompt]: Best epoch 9: best metric: 0.180
[09/26 17:45:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 17:45:25 visual_prompt]: Epoch 10 / 100: avg data time: 5.84e-02, avg batch time: 0.5006, average train loss: 2.0667
[09/26 17:45:27 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1672, average loss: 2.0571
[09/26 17:45:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 76.50	
[09/26 17:45:27 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 17:45:34 visual_prompt]: Epoch 11 / 100: avg data time: 5.97e-02, avg batch time: 0.5021, average train loss: 1.9874
[09/26 17:45:35 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1668, average loss: 2.0652
[09/26 17:45:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 76.50	
[09/26 17:45:35 visual_prompt]: Best epoch 11: best metric: 0.185
[09/26 17:45:35 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 17:45:42 visual_prompt]: Epoch 12 / 100: avg data time: 5.53e-02, avg batch time: 0.5000, average train loss: 1.9323
[09/26 17:45:44 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1672, average loss: 2.0993
[09/26 17:45:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 74.00	
[09/26 17:45:44 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 17:45:51 visual_prompt]: Epoch 13 / 100: avg data time: 6.49e-02, avg batch time: 0.5068, average train loss: 1.9200
[09/26 17:45:53 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1672, average loss: 1.9090
[09/26 17:45:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 83.50	
[09/26 17:45:53 visual_prompt]: Best epoch 13: best metric: 0.215
[09/26 17:45:53 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 17:45:59 visual_prompt]: Epoch 14 / 100: avg data time: 6.37e-02, avg batch time: 0.5072, average train loss: 1.9059
[09/26 17:46:01 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1675, average loss: 2.1375
[09/26 17:46:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 72.00	
[09/26 17:46:01 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 17:46:08 visual_prompt]: Epoch 15 / 100: avg data time: 6.73e-02, avg batch time: 0.5101, average train loss: 1.8284
[09/26 17:46:10 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1672, average loss: 1.9889
[09/26 17:46:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 75.00	
[09/26 17:46:10 visual_prompt]: Best epoch 15: best metric: 0.230
[09/26 17:46:10 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 17:46:17 visual_prompt]: Epoch 16 / 100: avg data time: 5.97e-02, avg batch time: 0.5017, average train loss: 1.9506
[09/26 17:46:18 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1674, average loss: 1.8655
[09/26 17:46:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 88.50	
[09/26 17:46:18 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 17:46:25 visual_prompt]: Epoch 17 / 100: avg data time: 6.13e-02, avg batch time: 0.5037, average train loss: 1.9197
[09/26 17:46:27 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1675, average loss: 2.4882
[09/26 17:46:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 64.50	
[09/26 17:46:27 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 17:46:34 visual_prompt]: Epoch 18 / 100: avg data time: 6.27e-02, avg batch time: 0.5053, average train loss: 1.8792
[09/26 17:46:35 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1675, average loss: 1.8506
[09/26 17:46:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 86.00	
[09/26 17:46:35 visual_prompt]: Best epoch 18: best metric: 0.240
[09/26 17:46:35 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 17:46:42 visual_prompt]: Epoch 19 / 100: avg data time: 6.05e-02, avg batch time: 0.5059, average train loss: 1.8529
[09/26 17:46:44 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1676, average loss: 1.9180
[09/26 17:46:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 81.00	
[09/26 17:46:44 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 17:46:51 visual_prompt]: Epoch 20 / 100: avg data time: 5.82e-02, avg batch time: 0.5007, average train loss: 1.8626
[09/26 17:46:52 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1676, average loss: 2.2449
[09/26 17:46:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 74.50	
[09/26 17:46:52 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 17:46:59 visual_prompt]: Epoch 21 / 100: avg data time: 6.38e-02, avg batch time: 0.5068, average train loss: 1.9873
[09/26 17:47:01 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1673, average loss: 1.9805
[09/26 17:47:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 86.50	
[09/26 17:47:01 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 17:47:08 visual_prompt]: Epoch 22 / 100: avg data time: 6.30e-02, avg batch time: 0.5065, average train loss: 1.8695
[09/26 17:47:10 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1676, average loss: 1.8322
[09/26 17:47:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 86.00	
[09/26 17:47:10 visual_prompt]: Best epoch 22: best metric: 0.290
[09/26 17:47:10 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 17:47:17 visual_prompt]: Epoch 23 / 100: avg data time: 6.26e-02, avg batch time: 0.5068, average train loss: 1.7834
[09/26 17:47:18 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1671, average loss: 2.0236
[09/26 17:47:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 82.00	
[09/26 17:47:18 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 17:47:25 visual_prompt]: Epoch 24 / 100: avg data time: 6.03e-02, avg batch time: 0.5034, average train loss: 1.9533
[09/26 17:47:27 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1675, average loss: 2.1430
[09/26 17:47:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 76.50	
[09/26 17:47:27 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 17:47:34 visual_prompt]: Epoch 25 / 100: avg data time: 6.89e-02, avg batch time: 0.5119, average train loss: 2.0393
[09/26 17:47:35 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1669, average loss: 1.9891
[09/26 17:47:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 82.00	
[09/26 17:47:35 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 17:47:42 visual_prompt]: Epoch 26 / 100: avg data time: 6.44e-02, avg batch time: 0.5085, average train loss: 1.9212
[09/26 17:47:44 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1675, average loss: 1.9423
[09/26 17:47:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 86.00	
[09/26 17:47:44 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 17:47:51 visual_prompt]: Epoch 27 / 100: avg data time: 6.43e-02, avg batch time: 0.5066, average train loss: 1.8476
[09/26 17:47:52 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1672, average loss: 1.8454
[09/26 17:47:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 83.50	
[09/26 17:47:52 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 17:47:59 visual_prompt]: Epoch 28 / 100: avg data time: 6.17e-02, avg batch time: 0.5039, average train loss: 1.9427
[09/26 17:48:01 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1671, average loss: 2.1656
[09/26 17:48:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 68.50	
[09/26 17:48:01 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 17:48:08 visual_prompt]: Epoch 29 / 100: avg data time: 6.21e-02, avg batch time: 0.5041, average train loss: 1.9997
[09/26 17:48:10 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1668, average loss: 2.0837
[09/26 17:48:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 77.00	
[09/26 17:48:10 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 17:48:16 visual_prompt]: Epoch 30 / 100: avg data time: 6.22e-02, avg batch time: 0.5056, average train loss: 1.9355
[09/26 17:48:18 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1672, average loss: 1.9752
[09/26 17:48:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 83.00	
[09/26 17:48:18 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 17:48:25 visual_prompt]: Epoch 31 / 100: avg data time: 5.54e-02, avg batch time: 0.5004, average train loss: 1.7852
[09/26 17:48:27 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.1667, average loss: 1.8608
[09/26 17:48:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 85.50	
[09/26 17:48:27 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 17:48:34 visual_prompt]: Epoch 32 / 100: avg data time: 6.63e-02, avg batch time: 0.5089, average train loss: 1.9872
[09/26 17:48:35 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1666, average loss: 2.1275
[09/26 17:48:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 65.50	
[09/26 17:48:35 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 17:48:42 visual_prompt]: Epoch 33 / 100: avg data time: 5.31e-02, avg batch time: 0.4960, average train loss: 1.9898
[09/26 17:48:44 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1667, average loss: 1.9090
[09/26 17:48:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 84.50	
[09/26 17:48:44 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 17:48:51 visual_prompt]: Epoch 34 / 100: avg data time: 6.07e-02, avg batch time: 0.5038, average train loss: 1.9563
[09/26 17:48:52 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1667, average loss: 1.8957
[09/26 17:48:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 87.50	
[09/26 17:48:52 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 17:48:59 visual_prompt]: Epoch 35 / 100: avg data time: 6.41e-02, avg batch time: 0.5063, average train loss: 1.7821
[09/26 17:49:01 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1666, average loss: 1.9491
[09/26 17:49:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 86.00	
[09/26 17:49:01 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 17:49:08 visual_prompt]: Epoch 36 / 100: avg data time: 6.68e-02, avg batch time: 0.5085, average train loss: 1.8106
[09/26 17:49:09 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1669, average loss: 2.0172
[09/26 17:49:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 81.50	
[09/26 17:49:09 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 17:49:16 visual_prompt]: Epoch 37 / 100: avg data time: 6.47e-02, avg batch time: 0.5079, average train loss: 1.8436
[09/26 17:49:18 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1667, average loss: 1.8126
[09/26 17:49:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 89.00	
[09/26 17:49:18 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 17:49:25 visual_prompt]: Epoch 38 / 100: avg data time: 6.62e-02, avg batch time: 0.5101, average train loss: 1.8134
[09/26 17:49:27 visual_prompt]: Inference (val):avg data time: 4.44e-05, avg batch time: 0.1666, average loss: 1.9545
[09/26 17:49:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 84.00	
[09/26 17:49:27 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 17:49:34 visual_prompt]: Epoch 39 / 100: avg data time: 6.60e-02, avg batch time: 0.5076, average train loss: 1.9841
[09/26 17:49:35 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1673, average loss: 2.2086
[09/26 17:49:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/26 17:49:35 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 17:49:42 visual_prompt]: Epoch 40 / 100: avg data time: 6.34e-02, avg batch time: 0.5051, average train loss: 2.1543
[09/26 17:49:44 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1667, average loss: 2.1485
[09/26 17:49:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 70.50	
[09/26 17:49:44 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 17:49:51 visual_prompt]: Epoch 41 / 100: avg data time: 6.30e-02, avg batch time: 0.5048, average train loss: 2.0739
[09/26 17:49:52 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1672, average loss: 2.2448
[09/26 17:49:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 17:49:52 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 17:49:59 visual_prompt]: Epoch 42 / 100: avg data time: 6.09e-02, avg batch time: 0.5041, average train loss: 2.2172
[09/26 17:50:01 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1674, average loss: 2.1990
[09/26 17:50:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 17:50:01 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 17:50:08 visual_prompt]: Epoch 43 / 100: avg data time: 6.43e-02, avg batch time: 0.5064, average train loss: 2.1984
[09/26 17:50:09 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1672, average loss: 2.1910
[09/26 17:50:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 60.00	
[09/26 17:50:10 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 17:50:16 visual_prompt]: Epoch 44 / 100: avg data time: 6.70e-02, avg batch time: 0.5096, average train loss: 2.2086
[09/26 17:50:18 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1674, average loss: 2.2070
[09/26 17:50:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 17:50:18 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 17:50:25 visual_prompt]: Epoch 45 / 100: avg data time: 5.64e-02, avg batch time: 0.4984, average train loss: 2.1995
[09/26 17:50:26 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1674, average loss: 2.1937
[09/26 17:50:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 17:50:26 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 17:50:33 visual_prompt]: Epoch 46 / 100: avg data time: 6.21e-02, avg batch time: 0.5049, average train loss: 2.1919
[09/26 17:50:35 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1670, average loss: 2.1906
[09/26 17:50:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 17:50:35 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 17:50:42 visual_prompt]: Epoch 47 / 100: avg data time: 5.51e-02, avg batch time: 0.5000, average train loss: 2.1970
[09/26 17:50:44 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1671, average loss: 2.1918
[09/26 17:50:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 17:50:44 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 17:50:50 visual_prompt]: Epoch 48 / 100: avg data time: 6.04e-02, avg batch time: 0.5029, average train loss: 2.1949
[09/26 17:50:52 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1670, average loss: 2.2064
[09/26 17:50:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/26 17:50:52 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 17:50:59 visual_prompt]: Epoch 49 / 100: avg data time: 6.24e-02, avg batch time: 0.5051, average train loss: 2.2006
[09/26 17:51:01 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1667, average loss: 2.1933
[09/26 17:51:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/26 17:51:01 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 17:51:08 visual_prompt]: Epoch 50 / 100: avg data time: 6.22e-02, avg batch time: 0.5037, average train loss: 2.1952
[09/26 17:51:09 visual_prompt]: Inference (val):avg data time: 4.66e-05, avg batch time: 0.1669, average loss: 2.1986
[09/26 17:51:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 59.00	
[09/26 17:51:09 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 17:51:16 visual_prompt]: Epoch 51 / 100: avg data time: 6.07e-02, avg batch time: 0.5041, average train loss: 2.1988
[09/26 17:51:18 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1669, average loss: 2.1945
[09/26 17:51:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 17:51:18 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 17:51:25 visual_prompt]: Epoch 52 / 100: avg data time: 5.77e-02, avg batch time: 0.5011, average train loss: 2.1966
[09/26 17:51:26 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1670, average loss: 2.2022
[09/26 17:51:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 17:51:26 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 17:51:33 visual_prompt]: Epoch 53 / 100: avg data time: 6.29e-02, avg batch time: 0.5053, average train loss: 2.1975
[09/26 17:51:35 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1669, average loss: 2.1957
[09/26 17:51:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/26 17:51:35 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 17:51:42 visual_prompt]: Epoch 54 / 100: avg data time: 5.98e-02, avg batch time: 0.5034, average train loss: 2.1961
[09/26 17:51:43 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1674, average loss: 2.2036
[09/26 17:51:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/26 17:51:43 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 17:51:50 visual_prompt]: Epoch 55 / 100: avg data time: 5.86e-02, avg batch time: 0.5014, average train loss: 2.1944
[09/26 17:51:52 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1670, average loss: 2.1955
[09/26 17:51:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 58.00	
[09/26 17:51:52 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 17:51:59 visual_prompt]: Epoch 56 / 100: avg data time: 5.97e-02, avg batch time: 0.5022, average train loss: 2.1889
[09/26 17:52:00 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1668, average loss: 2.1915
[09/26 17:52:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/26 17:52:00 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 17:52:07 visual_prompt]: Epoch 57 / 100: avg data time: 6.35e-02, avg batch time: 0.5066, average train loss: 2.1934
[09/26 17:52:09 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1669, average loss: 2.1977
[09/26 17:52:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/26 17:52:09 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 17:52:16 visual_prompt]: Epoch 58 / 100: avg data time: 6.18e-02, avg batch time: 0.5059, average train loss: 2.1753
[09/26 17:52:17 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.1667, average loss: 2.1620
[09/26 17:52:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 68.00	
[09/26 17:52:17 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 17:52:24 visual_prompt]: Epoch 59 / 100: avg data time: 6.47e-02, avg batch time: 0.5079, average train loss: 2.2041
[09/26 17:52:26 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1663, average loss: 2.1927
[09/26 17:52:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/26 17:52:26 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 17:52:33 visual_prompt]: Epoch 60 / 100: avg data time: 6.25e-02, avg batch time: 0.5048, average train loss: 2.2009
[09/26 17:52:35 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1668, average loss: 2.1876
[09/26 17:52:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/26 17:52:35 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 17:52:42 visual_prompt]: Epoch 61 / 100: avg data time: 6.20e-02, avg batch time: 0.5049, average train loss: 2.1811
[09/26 17:52:43 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1668, average loss: 2.1929
[09/26 17:52:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.50	
[09/26 17:52:43 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 17:52:50 visual_prompt]: Epoch 62 / 100: avg data time: 6.85e-02, avg batch time: 0.5108, average train loss: 2.1643
[09/26 17:52:52 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1666, average loss: 2.1280
[09/26 17:52:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 64.50	
[09/26 17:52:52 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 17:52:59 visual_prompt]: Epoch 63 / 100: avg data time: 6.09e-02, avg batch time: 0.5038, average train loss: 2.1655
[09/26 17:53:00 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.1675, average loss: 2.1874
[09/26 17:53:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 59.50	
[09/26 17:53:00 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 17:53:07 visual_prompt]: Epoch 64 / 100: avg data time: 5.55e-02, avg batch time: 0.4995, average train loss: 2.0710
[09/26 17:53:09 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1665, average loss: 2.1892
[09/26 17:53:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 70.00	
[09/26 17:53:09 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 17:53:16 visual_prompt]: Epoch 65 / 100: avg data time: 6.70e-02, avg batch time: 0.5092, average train loss: 2.0821
[09/26 17:53:17 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.1672, average loss: 2.0843
[09/26 17:53:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 79.50	
[09/26 17:53:17 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 17:53:24 visual_prompt]: Epoch 66 / 100: avg data time: 6.05e-02, avg batch time: 0.5024, average train loss: 2.1402
[09/26 17:53:26 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1671, average loss: 2.4578
[09/26 17:53:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 53.50	
[09/26 17:53:26 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 17:53:33 visual_prompt]: Epoch 67 / 100: avg data time: 6.55e-02, avg batch time: 0.5083, average train loss: 2.1506
[09/26 17:53:35 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1673, average loss: 2.0789
[09/26 17:53:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 74.50	
[09/26 17:53:35 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 17:53:41 visual_prompt]: Epoch 68 / 100: avg data time: 5.89e-02, avg batch time: 0.5020, average train loss: 2.0442
[09/26 17:53:43 visual_prompt]: Inference (val):avg data time: 4.56e-05, avg batch time: 0.1674, average loss: 2.0450
[09/26 17:53:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 77.50	
[09/26 17:53:43 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 17:53:50 visual_prompt]: Epoch 69 / 100: avg data time: 6.69e-02, avg batch time: 0.5101, average train loss: 2.0102
[09/26 17:53:52 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1674, average loss: 2.0258
[09/26 17:53:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 76.00	
[09/26 17:53:52 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 17:53:59 visual_prompt]: Epoch 70 / 100: avg data time: 6.10e-02, avg batch time: 0.5031, average train loss: 2.0079
[09/26 17:54:00 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.1675, average loss: 1.9779
[09/26 17:54:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 84.50	
[09/26 17:54:00 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 17:54:07 visual_prompt]: Epoch 71 / 100: avg data time: 6.43e-02, avg batch time: 0.5071, average train loss: 1.9696
[09/26 17:54:09 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1671, average loss: 1.9299
[09/26 17:54:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 83.00	
[09/26 17:54:09 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 17:54:16 visual_prompt]: Epoch 72 / 100: avg data time: 6.26e-02, avg batch time: 0.5048, average train loss: 2.0153
[09/26 17:54:17 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1673, average loss: 1.9908
[09/26 17:54:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 81.00	
[09/26 17:54:17 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 17:54:24 visual_prompt]: Epoch 73 / 100: avg data time: 5.89e-02, avg batch time: 0.5026, average train loss: 1.9243
[09/26 17:54:26 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1673, average loss: 1.9810
[09/26 17:54:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 77.00	
[09/26 17:54:26 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 17:54:33 visual_prompt]: Epoch 74 / 100: avg data time: 5.76e-02, avg batch time: 0.5015, average train loss: 1.8904
[09/26 17:54:34 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1674, average loss: 1.9130
[09/26 17:54:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 84.50	
[09/26 17:54:34 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 17:54:41 visual_prompt]: Epoch 75 / 100: avg data time: 6.50e-02, avg batch time: 0.5074, average train loss: 1.9221
[09/26 17:54:43 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1671, average loss: 1.9916
[09/26 17:54:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 80.00	
[09/26 17:54:43 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 17:54:50 visual_prompt]: Epoch 76 / 100: avg data time: 6.47e-02, avg batch time: 0.5073, average train loss: 1.9316
[09/26 17:54:52 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1673, average loss: 2.0098
[09/26 17:54:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 77.50	
[09/26 17:54:52 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 17:54:58 visual_prompt]: Epoch 77 / 100: avg data time: 6.14e-02, avg batch time: 0.5044, average train loss: 1.8947
[09/26 17:55:00 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1672, average loss: 1.8219
[09/26 17:55:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 85.00	
[09/26 17:55:00 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 17:55:07 visual_prompt]: Epoch 78 / 100: avg data time: 6.56e-02, avg batch time: 0.5082, average train loss: 1.8107
[09/26 17:55:09 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1676, average loss: 2.0398
[09/26 17:55:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 78.50	
[09/26 17:55:09 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 17:55:16 visual_prompt]: Epoch 79 / 100: avg data time: 6.12e-02, avg batch time: 0.5058, average train loss: 1.8083
[09/26 17:55:17 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1674, average loss: 1.8826
[09/26 17:55:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 84.50	
[09/26 17:55:17 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 17:55:24 visual_prompt]: Epoch 80 / 100: avg data time: 6.04e-02, avg batch time: 0.5033, average train loss: 1.7458
[09/26 17:55:26 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1674, average loss: 1.8787
[09/26 17:55:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 84.00	
[09/26 17:55:26 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 17:55:33 visual_prompt]: Epoch 81 / 100: avg data time: 5.84e-02, avg batch time: 0.5013, average train loss: 1.7145
[09/26 17:55:34 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1673, average loss: 1.8843
[09/26 17:55:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 84.00	
[09/26 17:55:34 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 17:55:41 visual_prompt]: Epoch 82 / 100: avg data time: 6.35e-02, avg batch time: 0.5058, average train loss: 1.6970
[09/26 17:55:43 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1673, average loss: 1.9454
[09/26 17:55:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 83.00	
[09/26 17:55:43 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 17:55:50 visual_prompt]: Epoch 83 / 100: avg data time: 6.44e-02, avg batch time: 0.5084, average train loss: 1.7452
[09/26 17:55:52 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1671, average loss: 1.8258
[09/26 17:55:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 86.50	
[09/26 17:55:52 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 17:55:58 visual_prompt]: Epoch 84 / 100: avg data time: 6.31e-02, avg batch time: 0.5049, average train loss: 1.6627
[09/26 17:56:00 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1674, average loss: 1.9426
[09/26 17:56:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 82.50	
[09/26 17:56:00 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 17:56:07 visual_prompt]: Epoch 85 / 100: avg data time: 6.56e-02, avg batch time: 0.5093, average train loss: 1.6427
[09/26 17:56:09 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1672, average loss: 1.8358
[09/26 17:56:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 87.00	
[09/26 17:56:09 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 17:56:16 visual_prompt]: Epoch 86 / 100: avg data time: 6.78e-02, avg batch time: 0.5104, average train loss: 1.5794
[09/26 17:56:17 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1670, average loss: 1.8222
[09/26 17:56:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 86.00	
[09/26 17:56:17 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 17:56:24 visual_prompt]: Epoch 87 / 100: avg data time: 6.51e-02, avg batch time: 0.5076, average train loss: 1.5231
[09/26 17:56:26 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1673, average loss: 1.8718
[09/26 17:56:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 86.50	
[09/26 17:56:26 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 17:56:33 visual_prompt]: Epoch 88 / 100: avg data time: 6.37e-02, avg batch time: 0.5059, average train loss: 1.4733
[09/26 17:56:34 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.1674, average loss: 2.0755
[09/26 17:56:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 81.50	
[09/26 17:56:34 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 17:56:41 visual_prompt]: Epoch 89 / 100: avg data time: 5.32e-02, avg batch time: 0.4969, average train loss: 1.4953
[09/26 17:56:43 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1673, average loss: 1.8710
[09/26 17:56:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 88.00	
[09/26 17:56:43 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 17:56:50 visual_prompt]: Epoch 90 / 100: avg data time: 5.04e-02, avg batch time: 0.4948, average train loss: 1.4225
[09/26 17:56:51 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1671, average loss: 1.8864
[09/26 17:56:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 87.00	
[09/26 17:56:51 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 17:56:58 visual_prompt]: Epoch 91 / 100: avg data time: 6.51e-02, avg batch time: 0.5074, average train loss: 1.4105
[09/26 17:57:00 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1675, average loss: 1.8881
[09/26 17:57:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 87.50	
[09/26 17:57:00 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 17:57:07 visual_prompt]: Epoch 92 / 100: avg data time: 5.95e-02, avg batch time: 0.5044, average train loss: 1.3583
[09/26 17:57:08 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1675, average loss: 1.8791
[09/26 17:57:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 88.00	
[09/26 17:57:08 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 17:57:15 visual_prompt]: Epoch 93 / 100: avg data time: 4.93e-02, avg batch time: 0.4939, average train loss: 1.3373
[09/26 17:57:17 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1679, average loss: 1.8585
[09/26 17:57:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 87.00	
[09/26 17:57:17 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 17:57:24 visual_prompt]: Epoch 94 / 100: avg data time: 5.81e-02, avg batch time: 0.5028, average train loss: 1.3104
[09/26 17:57:25 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1680, average loss: 1.9076
[09/26 17:57:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 86.50	
[09/26 17:57:25 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 17:57:32 visual_prompt]: Epoch 95 / 100: avg data time: 5.92e-02, avg batch time: 0.5043, average train loss: 1.2864
[09/26 17:57:34 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1676, average loss: 1.9039
[09/26 17:57:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.00	
[09/26 17:57:34 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 17:57:41 visual_prompt]: Epoch 96 / 100: avg data time: 5.91e-02, avg batch time: 0.5032, average train loss: 1.2716
[09/26 17:57:42 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1693, average loss: 1.9081
[09/26 17:57:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 88.50	
[09/26 17:57:42 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 17:57:49 visual_prompt]: Epoch 97 / 100: avg data time: 4.99e-02, avg batch time: 0.4937, average train loss: 1.2512
[09/26 17:57:51 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1675, average loss: 1.8891
[09/26 17:57:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 89.00	
[09/26 17:57:51 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 17:57:58 visual_prompt]: Epoch 98 / 100: avg data time: 6.37e-02, avg batch time: 0.5059, average train loss: 1.2192
[09/26 17:58:00 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1674, average loss: 1.9031
[09/26 17:58:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 87.50	
[09/26 17:58:00 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 17:58:06 visual_prompt]: Epoch 99 / 100: avg data time: 5.61e-02, avg batch time: 0.5007, average train loss: 1.2277
[09/26 17:58:08 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1676, average loss: 1.9218
[09/26 17:58:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 88.00	
[09/26 17:58:08 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 17:58:15 visual_prompt]: Epoch 100 / 100: avg data time: 6.21e-02, avg batch time: 0.5050, average train loss: 1.2221
[09/26 17:58:17 visual_prompt]: Inference (val):avg data time: 4.94e-05, avg batch time: 0.1677, average loss: 1.9177
[09/26 17:58:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 88.50	
[09/26 17:58:17 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 17:58:17 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 17:58:17 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 17:58:17 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 17:58:17 visual_prompt]: Training with config:
[09/26 17:58:17 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 17:58:17 visual_prompt]: Loading training data...
[09/26 17:58:17 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 17:58:18 visual_prompt]: Number of images: 800
[09/26 17:58:18 visual_prompt]: Number of classes: 9 / 9
[09/26 17:58:18 visual_prompt]: Loading validation data...
[09/26 17:58:18 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 17:58:18 visual_prompt]: Number of images: 200
[09/26 17:58:18 visual_prompt]: Number of classes: 9 / 9
[09/26 17:58:18 visual_prompt]: Constructing models...
[09/26 17:58:21 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 17:58:21 visual_prompt]: tuned percent:0.542
[09/26 17:58:21 visual_prompt]: Device used for model: 0
[09/26 17:58:21 visual_prompt]: Setting up Evaluator...
[09/26 17:58:21 visual_prompt]: Setting up Trainer...
[09/26 17:58:21 visual_prompt]: 	Setting up the optimizer...
[09/26 17:58:21 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 17:58:28 visual_prompt]: Epoch 1 / 100: avg data time: 6.34e-02, avg batch time: 0.5052, average train loss: 2.8705
[09/26 17:58:29 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1671, average loss: 2.9516
[09/26 17:58:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 17:58:29 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 17:58:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 17:58:36 visual_prompt]: Epoch 2 / 100: avg data time: 6.79e-02, avg batch time: 0.5110, average train loss: 2.4276
[09/26 17:58:38 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1672, average loss: 2.2198
[09/26 17:58:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 17:58:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 17:58:45 visual_prompt]: Epoch 3 / 100: avg data time: 6.12e-02, avg batch time: 0.5039, average train loss: 2.2345
[09/26 17:58:46 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1672, average loss: 2.2183
[09/26 17:58:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.50	top5: 55.00	
[09/26 17:58:46 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 17:58:53 visual_prompt]: Epoch 4 / 100: avg data time: 6.52e-02, avg batch time: 0.5069, average train loss: 2.2211
[09/26 17:58:55 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1673, average loss: 2.1969
[09/26 17:58:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 56.50	
[09/26 17:58:55 visual_prompt]: Best epoch 4: best metric: 0.135
[09/26 17:58:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 17:59:02 visual_prompt]: Epoch 5 / 100: avg data time: 5.91e-02, avg batch time: 0.5014, average train loss: 2.1998
[09/26 17:59:04 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1672, average loss: 2.2145
[09/26 17:59:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.50	top5: 55.00	
[09/26 17:59:04 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 17:59:10 visual_prompt]: Epoch 6 / 100: avg data time: 6.17e-02, avg batch time: 0.5029, average train loss: 2.2030
[09/26 17:59:12 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1669, average loss: 2.2052
[09/26 17:59:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.00	
[09/26 17:59:12 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 17:59:19 visual_prompt]: Epoch 7 / 100: avg data time: 5.52e-02, avg batch time: 0.4974, average train loss: 2.1941
[09/26 17:59:21 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1676, average loss: 2.1976
[09/26 17:59:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 58.00	
[09/26 17:59:21 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 17:59:28 visual_prompt]: Epoch 8 / 100: avg data time: 6.93e-02, avg batch time: 0.5112, average train loss: 2.1538
[09/26 17:59:29 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1670, average loss: 2.1674
[09/26 17:59:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 63.50	
[09/26 17:59:29 visual_prompt]: Best epoch 8: best metric: 0.150
[09/26 17:59:29 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 17:59:36 visual_prompt]: Epoch 9 / 100: avg data time: 5.66e-02, avg batch time: 0.4997, average train loss: 2.0932
[09/26 17:59:38 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1671, average loss: 2.1774
[09/26 17:59:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 65.00	
[09/26 17:59:38 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 17:59:45 visual_prompt]: Epoch 10 / 100: avg data time: 6.54e-02, avg batch time: 0.5068, average train loss: 2.0487
[09/26 17:59:46 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1666, average loss: 2.0396
[09/26 17:59:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 77.50	
[09/26 17:59:46 visual_prompt]: Best epoch 10: best metric: 0.200
[09/26 17:59:46 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 17:59:53 visual_prompt]: Epoch 11 / 100: avg data time: 6.20e-02, avg batch time: 0.5046, average train loss: 1.9387
[09/26 17:59:55 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1669, average loss: 2.2506
[09/26 17:59:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 72.50	
[09/26 17:59:55 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 18:00:02 visual_prompt]: Epoch 12 / 100: avg data time: 6.13e-02, avg batch time: 0.5053, average train loss: 1.9773
[09/26 18:00:03 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1670, average loss: 1.9638
[09/26 18:00:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 74.00	
[09/26 18:00:03 visual_prompt]: Best epoch 12: best metric: 0.210
[09/26 18:00:03 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 18:00:10 visual_prompt]: Epoch 13 / 100: avg data time: 6.03e-02, avg batch time: 0.5033, average train loss: 1.8931
[09/26 18:00:12 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1671, average loss: 2.2386
[09/26 18:00:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 76.00	
[09/26 18:00:12 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 18:00:19 visual_prompt]: Epoch 14 / 100: avg data time: 6.26e-02, avg batch time: 0.5053, average train loss: 1.9199
[09/26 18:00:20 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1673, average loss: 1.9427
[09/26 18:00:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 78.50	
[09/26 18:00:20 visual_prompt]: Best epoch 14: best metric: 0.245
[09/26 18:00:20 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 18:00:27 visual_prompt]: Epoch 15 / 100: avg data time: 6.28e-02, avg batch time: 0.5071, average train loss: 1.8095
[09/26 18:00:29 visual_prompt]: Inference (val):avg data time: 4.45e-05, avg batch time: 0.1671, average loss: 1.9943
[09/26 18:00:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 82.00	
[09/26 18:00:29 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 18:00:36 visual_prompt]: Epoch 16 / 100: avg data time: 5.92e-02, avg batch time: 0.5031, average train loss: 1.7120
[09/26 18:00:38 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1675, average loss: 1.8564
[09/26 18:00:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 84.50	
[09/26 18:00:38 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 18:00:45 visual_prompt]: Epoch 17 / 100: avg data time: 6.67e-02, avg batch time: 0.5099, average train loss: 1.6543
[09/26 18:00:46 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1671, average loss: 1.9416
[09/26 18:00:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 83.50	
[09/26 18:00:46 visual_prompt]: Best epoch 17: best metric: 0.270
[09/26 18:00:46 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 18:00:53 visual_prompt]: Epoch 18 / 100: avg data time: 6.47e-02, avg batch time: 0.5073, average train loss: 1.5782
[09/26 18:00:55 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1674, average loss: 2.1320
[09/26 18:00:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 83.00	
[09/26 18:00:55 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 18:01:02 visual_prompt]: Epoch 19 / 100: avg data time: 6.20e-02, avg batch time: 0.5077, average train loss: 1.5200
[09/26 18:01:04 visual_prompt]: Inference (val):avg data time: 4.68e-05, avg batch time: 0.1673, average loss: 2.0839
[09/26 18:01:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 84.00	
[09/26 18:01:04 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 18:01:10 visual_prompt]: Epoch 20 / 100: avg data time: 6.29e-02, avg batch time: 0.5060, average train loss: 1.5123
[09/26 18:01:12 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1675, average loss: 2.1013
[09/26 18:01:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 87.00	
[09/26 18:01:12 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 18:01:19 visual_prompt]: Epoch 21 / 100: avg data time: 6.12e-02, avg batch time: 0.5034, average train loss: 1.5530
[09/26 18:01:21 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1674, average loss: 2.2046
[09/26 18:01:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 81.50	
[09/26 18:01:21 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 18:01:28 visual_prompt]: Epoch 22 / 100: avg data time: 5.97e-02, avg batch time: 0.5029, average train loss: 1.5297
[09/26 18:01:29 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1675, average loss: 2.0453
[09/26 18:01:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 83.00	
[09/26 18:01:29 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 18:01:36 visual_prompt]: Epoch 23 / 100: avg data time: 6.17e-02, avg batch time: 0.5052, average train loss: 1.3751
[09/26 18:01:38 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1673, average loss: 1.8893
[09/26 18:01:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.50	
[09/26 18:01:38 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 18:01:45 visual_prompt]: Epoch 24 / 100: avg data time: 5.36e-02, avg batch time: 0.4987, average train loss: 1.2483
[09/26 18:01:46 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1675, average loss: 1.9531
[09/26 18:01:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 91.00	
[09/26 18:01:46 visual_prompt]: Best epoch 24: best metric: 0.285
[09/26 18:01:46 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 18:01:53 visual_prompt]: Epoch 25 / 100: avg data time: 6.21e-02, avg batch time: 0.5072, average train loss: 1.2368
[09/26 18:01:55 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1670, average loss: 1.9896
[09/26 18:01:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 92.50	
[09/26 18:01:55 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 18:02:01 visual_prompt]: Epoch 26 / 100: avg data time: 4.96e-02, avg batch time: 0.4939, average train loss: 1.2789
[09/26 18:02:03 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1672, average loss: 2.2130
[09/26 18:02:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 91.00	
[09/26 18:02:03 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 18:02:10 visual_prompt]: Epoch 27 / 100: avg data time: 6.25e-02, avg batch time: 0.5057, average train loss: 1.1922
[09/26 18:02:12 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1669, average loss: 2.0381
[09/26 18:02:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.00	
[09/26 18:02:12 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 18:02:19 visual_prompt]: Epoch 28 / 100: avg data time: 6.34e-02, avg batch time: 0.5062, average train loss: 1.0739
[09/26 18:02:20 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1668, average loss: 1.9970
[09/26 18:02:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 90.50	
[09/26 18:02:20 visual_prompt]: Best epoch 28: best metric: 0.320
[09/26 18:02:20 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 18:02:27 visual_prompt]: Epoch 29 / 100: avg data time: 6.85e-02, avg batch time: 0.5104, average train loss: 1.1253
[09/26 18:02:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1670, average loss: 1.9656
[09/26 18:02:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.50	
[09/26 18:02:29 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 18:02:36 visual_prompt]: Epoch 30 / 100: avg data time: 5.08e-02, avg batch time: 0.4931, average train loss: 1.0073
[09/26 18:02:37 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1677, average loss: 2.3967
[09/26 18:02:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 90.50	
[09/26 18:02:37 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 18:02:44 visual_prompt]: Epoch 31 / 100: avg data time: 6.34e-02, avg batch time: 0.5058, average train loss: 1.0413
[09/26 18:02:46 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1676, average loss: 2.3705
[09/26 18:02:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 85.50	
[09/26 18:02:46 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 18:02:53 visual_prompt]: Epoch 32 / 100: avg data time: 5.94e-02, avg batch time: 0.5023, average train loss: 1.0176
[09/26 18:02:54 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1674, average loss: 2.1986
[09/26 18:02:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 89.50	
[09/26 18:02:54 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 18:03:01 visual_prompt]: Epoch 33 / 100: avg data time: 6.44e-02, avg batch time: 0.5067, average train loss: 0.8536
[09/26 18:03:03 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.1675, average loss: 2.4149
[09/26 18:03:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 87.00	
[09/26 18:03:03 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 18:03:10 visual_prompt]: Epoch 34 / 100: avg data time: 5.83e-02, avg batch time: 0.5015, average train loss: 0.8630
[09/26 18:03:12 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1676, average loss: 2.7274
[09/26 18:03:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 86.50	
[09/26 18:03:12 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 18:03:18 visual_prompt]: Epoch 35 / 100: avg data time: 6.33e-02, avg batch time: 0.5057, average train loss: 0.8454
[09/26 18:03:20 visual_prompt]: Inference (val):avg data time: 4.45e-05, avg batch time: 0.1674, average loss: 2.5058
[09/26 18:03:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 91.00	
[09/26 18:03:20 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 18:03:27 visual_prompt]: Epoch 36 / 100: avg data time: 5.00e-02, avg batch time: 0.4941, average train loss: 0.7610
[09/26 18:03:29 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1674, average loss: 2.4966
[09/26 18:03:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 92.00	
[09/26 18:03:29 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 18:03:36 visual_prompt]: Epoch 37 / 100: avg data time: 7.14e-02, avg batch time: 0.5140, average train loss: 0.7813
[09/26 18:03:37 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1676, average loss: 2.6175
[09/26 18:03:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 89.00	
[09/26 18:03:37 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 18:03:44 visual_prompt]: Epoch 38 / 100: avg data time: 6.28e-02, avg batch time: 0.5069, average train loss: 0.7958
[09/26 18:03:46 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1673, average loss: 2.6776
[09/26 18:03:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.00	
[09/26 18:03:46 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 18:03:53 visual_prompt]: Epoch 39 / 100: avg data time: 6.84e-02, avg batch time: 0.5108, average train loss: 0.7024
[09/26 18:03:54 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1675, average loss: 2.8043
[09/26 18:03:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.50	
[09/26 18:03:54 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 18:04:01 visual_prompt]: Epoch 40 / 100: avg data time: 5.66e-02, avg batch time: 0.4997, average train loss: 0.6093
[09/26 18:04:03 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1673, average loss: 3.2862
[09/26 18:04:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 87.50	
[09/26 18:04:03 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 18:04:10 visual_prompt]: Epoch 41 / 100: avg data time: 6.06e-02, avg batch time: 0.5026, average train loss: 0.6001
[09/26 18:04:11 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1673, average loss: 2.9253
[09/26 18:04:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 93.00	
[09/26 18:04:11 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 18:04:18 visual_prompt]: Epoch 42 / 100: avg data time: 5.63e-02, avg batch time: 0.5002, average train loss: 0.7215
[09/26 18:04:20 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1672, average loss: 2.9049
[09/26 18:04:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 91.00	
[09/26 18:04:20 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 18:04:27 visual_prompt]: Epoch 43 / 100: avg data time: 6.23e-02, avg batch time: 0.5047, average train loss: 0.6708
[09/26 18:04:29 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1674, average loss: 3.2077
[09/26 18:04:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 86.50	
[09/26 18:04:29 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 18:04:35 visual_prompt]: Epoch 44 / 100: avg data time: 6.17e-02, avg batch time: 0.5053, average train loss: 0.6310
[09/26 18:04:37 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1673, average loss: 2.6795
[09/26 18:04:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 90.50	
[09/26 18:04:37 visual_prompt]: Best epoch 44: best metric: 0.355
[09/26 18:04:37 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 18:04:44 visual_prompt]: Epoch 45 / 100: avg data time: 6.42e-02, avg batch time: 0.5072, average train loss: 0.4734
[09/26 18:04:46 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.1675, average loss: 3.1714
[09/26 18:04:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.00	
[09/26 18:04:46 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 18:04:53 visual_prompt]: Epoch 46 / 100: avg data time: 6.13e-02, avg batch time: 0.5042, average train loss: 0.3812
[09/26 18:04:54 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1675, average loss: 3.3109
[09/26 18:04:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.00	
[09/26 18:04:54 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 18:05:01 visual_prompt]: Epoch 47 / 100: avg data time: 6.02e-02, avg batch time: 0.5027, average train loss: 0.3366
[09/26 18:05:03 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1672, average loss: 4.9621
[09/26 18:05:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 82.00	
[09/26 18:05:03 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 18:05:10 visual_prompt]: Epoch 48 / 100: avg data time: 5.96e-02, avg batch time: 0.5024, average train loss: 0.5654
[09/26 18:05:11 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1674, average loss: 3.0956
[09/26 18:05:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.00	
[09/26 18:05:11 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 18:05:18 visual_prompt]: Epoch 49 / 100: avg data time: 4.91e-02, avg batch time: 0.4920, average train loss: 0.4925
[09/26 18:05:20 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1673, average loss: 2.9982
[09/26 18:05:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 18:05:20 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 18:05:27 visual_prompt]: Epoch 50 / 100: avg data time: 6.87e-02, avg batch time: 0.5122, average train loss: 0.5067
[09/26 18:05:28 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1672, average loss: 3.3796
[09/26 18:05:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.00	
[09/26 18:05:28 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 18:05:35 visual_prompt]: Epoch 51 / 100: avg data time: 5.99e-02, avg batch time: 0.5028, average train loss: 0.4090
[09/26 18:05:37 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1672, average loss: 3.5319
[09/26 18:05:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 91.50	
[09/26 18:05:37 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 18:05:44 visual_prompt]: Epoch 52 / 100: avg data time: 6.47e-02, avg batch time: 0.5077, average train loss: 0.2585
[09/26 18:05:45 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1674, average loss: 3.7997
[09/26 18:05:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 88.00	
[09/26 18:05:45 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 18:05:52 visual_prompt]: Epoch 53 / 100: avg data time: 4.79e-02, avg batch time: 0.4933, average train loss: 0.3417
[09/26 18:05:54 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1674, average loss: 3.4422
[09/26 18:05:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 89.00	
[09/26 18:05:54 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 18:06:01 visual_prompt]: Epoch 54 / 100: avg data time: 6.36e-02, avg batch time: 0.5079, average train loss: 0.3424
[09/26 18:06:03 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1677, average loss: 3.1727
[09/26 18:06:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 92.00	
[09/26 18:06:03 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 18:06:10 visual_prompt]: Epoch 55 / 100: avg data time: 6.32e-02, avg batch time: 0.5069, average train loss: 0.2347
[09/26 18:06:11 visual_prompt]: Inference (val):avg data time: 4.71e-05, avg batch time: 0.1672, average loss: 3.5942
[09/26 18:06:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.50	
[09/26 18:06:11 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 18:06:18 visual_prompt]: Epoch 56 / 100: avg data time: 6.33e-02, avg batch time: 0.5057, average train loss: 0.2032
[09/26 18:06:20 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1672, average loss: 3.8560
[09/26 18:06:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 92.50	
[09/26 18:06:20 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 18:06:27 visual_prompt]: Epoch 57 / 100: avg data time: 6.54e-02, avg batch time: 0.5086, average train loss: 0.2213
[09/26 18:06:28 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1677, average loss: 3.9680
[09/26 18:06:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 89.50	
[09/26 18:06:28 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 18:06:35 visual_prompt]: Epoch 58 / 100: avg data time: 6.37e-02, avg batch time: 0.5064, average train loss: 0.1764
[09/26 18:06:37 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1674, average loss: 3.8343
[09/26 18:06:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 90.00	
[09/26 18:06:37 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 18:06:44 visual_prompt]: Epoch 59 / 100: avg data time: 6.08e-02, avg batch time: 0.5034, average train loss: 0.1135
[09/26 18:06:45 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1672, average loss: 4.3215
[09/26 18:06:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 90.00	
[09/26 18:06:45 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 18:06:52 visual_prompt]: Epoch 60 / 100: avg data time: 6.33e-02, avg batch time: 0.5070, average train loss: 0.1328
[09/26 18:06:54 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1669, average loss: 4.1146
[09/26 18:06:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 94.50	
[09/26 18:06:54 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 18:07:01 visual_prompt]: Epoch 61 / 100: avg data time: 5.90e-02, avg batch time: 0.5036, average train loss: 0.1031
[09/26 18:07:03 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1671, average loss: 3.9687
[09/26 18:07:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 93.00	
[09/26 18:07:03 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 18:07:10 visual_prompt]: Epoch 62 / 100: avg data time: 5.97e-02, avg batch time: 0.5040, average train loss: 0.0731
[09/26 18:07:11 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.1668, average loss: 4.1696
[09/26 18:07:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 90.50	
[09/26 18:07:11 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 18:07:18 visual_prompt]: Epoch 63 / 100: avg data time: 6.40e-02, avg batch time: 0.5070, average train loss: 0.0801
[09/26 18:07:20 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1675, average loss: 4.1027
[09/26 18:07:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 93.00	
[09/26 18:07:20 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 18:07:27 visual_prompt]: Epoch 64 / 100: avg data time: 6.71e-02, avg batch time: 0.5091, average train loss: 0.0751
[09/26 18:07:28 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1675, average loss: 4.1804
[09/26 18:07:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 93.50	
[09/26 18:07:28 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 18:07:35 visual_prompt]: Epoch 65 / 100: avg data time: 6.15e-02, avg batch time: 0.5045, average train loss: 0.0769
[09/26 18:07:37 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1675, average loss: 4.2925
[09/26 18:07:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 92.00	
[09/26 18:07:37 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 18:07:44 visual_prompt]: Epoch 66 / 100: avg data time: 6.18e-02, avg batch time: 0.5041, average train loss: 0.0593
[09/26 18:07:46 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1674, average loss: 4.4413
[09/26 18:07:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 90.00	
[09/26 18:07:46 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 18:07:52 visual_prompt]: Epoch 67 / 100: avg data time: 5.11e-02, avg batch time: 0.4952, average train loss: 0.0494
[09/26 18:07:54 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1672, average loss: 4.2888
[09/26 18:07:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 93.00	
[09/26 18:07:54 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 18:08:01 visual_prompt]: Epoch 68 / 100: avg data time: 6.59e-02, avg batch time: 0.5080, average train loss: 0.0399
[09/26 18:08:03 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1674, average loss: 4.5318
[09/26 18:08:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 90.50	
[09/26 18:08:03 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 18:08:10 visual_prompt]: Epoch 69 / 100: avg data time: 6.03e-02, avg batch time: 0.5036, average train loss: 0.0316
[09/26 18:08:11 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1674, average loss: 4.2846
[09/26 18:08:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 94.00	
[09/26 18:08:11 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 18:08:18 visual_prompt]: Epoch 70 / 100: avg data time: 6.59e-02, avg batch time: 0.5091, average train loss: 0.0297
[09/26 18:08:20 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1674, average loss: 4.2745
[09/26 18:08:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 92.00	
[09/26 18:08:20 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 18:08:27 visual_prompt]: Epoch 71 / 100: avg data time: 5.75e-02, avg batch time: 0.5010, average train loss: 0.0453
[09/26 18:08:28 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1671, average loss: 4.8744
[09/26 18:08:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 88.00	
[09/26 18:08:28 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 18:08:35 visual_prompt]: Epoch 72 / 100: avg data time: 5.74e-02, avg batch time: 0.5000, average train loss: 0.0339
[09/26 18:08:37 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1675, average loss: 4.5525
[09/26 18:08:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 91.50	
[09/26 18:08:37 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 18:08:44 visual_prompt]: Epoch 73 / 100: avg data time: 6.31e-02, avg batch time: 0.5070, average train loss: 0.0258
[09/26 18:08:46 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1673, average loss: 4.5489
[09/26 18:08:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 90.00	
[09/26 18:08:46 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 18:08:52 visual_prompt]: Epoch 74 / 100: avg data time: 5.91e-02, avg batch time: 0.5018, average train loss: 0.0174
[09/26 18:08:54 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1672, average loss: 4.3882
[09/26 18:08:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 91.50	
[09/26 18:08:54 visual_prompt]: Best epoch 74: best metric: 0.360
[09/26 18:08:54 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 18:09:01 visual_prompt]: Epoch 75 / 100: avg data time: 6.06e-02, avg batch time: 0.5038, average train loss: 0.0167
[09/26 18:09:03 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1675, average loss: 4.3184
[09/26 18:09:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 92.00	
[09/26 18:09:03 visual_prompt]: Best epoch 75: best metric: 0.365
[09/26 18:09:03 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 18:09:10 visual_prompt]: Epoch 76 / 100: avg data time: 6.30e-02, avg batch time: 0.5061, average train loss: 0.0098
[09/26 18:09:11 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1675, average loss: 4.4463
[09/26 18:09:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 91.50	
[09/26 18:09:11 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 18:09:18 visual_prompt]: Epoch 77 / 100: avg data time: 6.35e-02, avg batch time: 0.5069, average train loss: 0.0128
[09/26 18:09:20 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1669, average loss: 4.4380
[09/26 18:09:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 91.00	
[09/26 18:09:20 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 18:09:27 visual_prompt]: Epoch 78 / 100: avg data time: 6.28e-02, avg batch time: 0.5059, average train loss: 0.0084
[09/26 18:09:28 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1670, average loss: 4.4294
[09/26 18:09:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 91.50	
[09/26 18:09:28 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 18:09:35 visual_prompt]: Epoch 79 / 100: avg data time: 6.10e-02, avg batch time: 0.5031, average train loss: 0.0071
[09/26 18:09:37 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1672, average loss: 4.4481
[09/26 18:09:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:09:37 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 18:09:44 visual_prompt]: Epoch 80 / 100: avg data time: 6.51e-02, avg batch time: 0.5073, average train loss: 0.0103
[09/26 18:09:46 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1671, average loss: 4.4944
[09/26 18:09:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 92.00	
[09/26 18:09:46 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 18:09:53 visual_prompt]: Epoch 81 / 100: avg data time: 6.27e-02, avg batch time: 0.5056, average train loss: 0.0114
[09/26 18:09:54 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1675, average loss: 4.4867
[09/26 18:09:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:09:54 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 18:10:01 visual_prompt]: Epoch 82 / 100: avg data time: 6.31e-02, avg batch time: 0.5059, average train loss: 0.0071
[09/26 18:10:03 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1672, average loss: 4.4600
[09/26 18:10:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:10:03 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 18:10:10 visual_prompt]: Epoch 83 / 100: avg data time: 6.37e-02, avg batch time: 0.5057, average train loss: 0.0051
[09/26 18:10:11 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1671, average loss: 4.4858
[09/26 18:10:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 92.00	
[09/26 18:10:11 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 18:10:18 visual_prompt]: Epoch 84 / 100: avg data time: 6.29e-02, avg batch time: 0.5055, average train loss: 0.0054
[09/26 18:10:20 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1673, average loss: 4.5170
[09/26 18:10:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:10:20 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 18:10:27 visual_prompt]: Epoch 85 / 100: avg data time: 5.18e-02, avg batch time: 0.4988, average train loss: 0.0067
[09/26 18:10:29 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1672, average loss: 4.5118
[09/26 18:10:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 92.00	
[09/26 18:10:29 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 18:10:35 visual_prompt]: Epoch 86 / 100: avg data time: 6.19e-02, avg batch time: 0.5066, average train loss: 0.0054
[09/26 18:10:37 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1675, average loss: 4.4977
[09/26 18:10:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 92.00	
[09/26 18:10:37 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 18:10:44 visual_prompt]: Epoch 87 / 100: avg data time: 6.18e-02, avg batch time: 0.5050, average train loss: 0.0078
[09/26 18:10:46 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1676, average loss: 4.4985
[09/26 18:10:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 92.00	
[09/26 18:10:46 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 18:10:53 visual_prompt]: Epoch 88 / 100: avg data time: 6.41e-02, avg batch time: 0.5080, average train loss: 0.0062
[09/26 18:10:54 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1674, average loss: 4.5121
[09/26 18:10:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 92.00	
[09/26 18:10:54 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 18:11:01 visual_prompt]: Epoch 89 / 100: avg data time: 6.20e-02, avg batch time: 0.5050, average train loss: 0.0078
[09/26 18:11:03 visual_prompt]: Inference (val):avg data time: 3.41e-05, avg batch time: 0.1676, average loss: 4.5260
[09/26 18:11:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 92.00	
[09/26 18:11:03 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 18:11:10 visual_prompt]: Epoch 90 / 100: avg data time: 6.18e-02, avg batch time: 0.5060, average train loss: 0.0059
[09/26 18:11:12 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1673, average loss: 4.5579
[09/26 18:11:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:11:12 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 18:11:18 visual_prompt]: Epoch 91 / 100: avg data time: 6.24e-02, avg batch time: 0.5051, average train loss: 0.0050
[09/26 18:11:20 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1676, average loss: 4.5685
[09/26 18:11:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:11:20 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 18:11:27 visual_prompt]: Epoch 92 / 100: avg data time: 5.54e-02, avg batch time: 0.4990, average train loss: 0.0055
[09/26 18:11:29 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1677, average loss: 4.5713
[09/26 18:11:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:11:29 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 18:11:35 visual_prompt]: Epoch 93 / 100: avg data time: 6.55e-02, avg batch time: 0.5089, average train loss: 0.0058
[09/26 18:11:37 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1679, average loss: 4.5608
[09/26 18:11:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:11:37 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 18:11:44 visual_prompt]: Epoch 94 / 100: avg data time: 6.25e-02, avg batch time: 0.5054, average train loss: 0.0056
[09/26 18:11:46 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1670, average loss: 4.5587
[09/26 18:11:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:11:46 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 18:11:53 visual_prompt]: Epoch 95 / 100: avg data time: 6.10e-02, avg batch time: 0.5053, average train loss: 0.0061
[09/26 18:11:54 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1672, average loss: 4.5618
[09/26 18:11:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:11:54 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 18:12:01 visual_prompt]: Epoch 96 / 100: avg data time: 6.47e-02, avg batch time: 0.5087, average train loss: 0.0055
[09/26 18:12:03 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1670, average loss: 4.5634
[09/26 18:12:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:12:03 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 18:12:10 visual_prompt]: Epoch 97 / 100: avg data time: 6.01e-02, avg batch time: 0.5025, average train loss: 0.0053
[09/26 18:12:12 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1670, average loss: 4.5639
[09/26 18:12:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:12:12 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 18:12:18 visual_prompt]: Epoch 98 / 100: avg data time: 6.40e-02, avg batch time: 0.5091, average train loss: 0.0063
[09/26 18:12:20 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1679, average loss: 4.5639
[09/26 18:12:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:12:20 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 18:12:27 visual_prompt]: Epoch 99 / 100: avg data time: 6.00e-02, avg batch time: 0.5027, average train loss: 0.0068
[09/26 18:12:29 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1676, average loss: 4.5637
[09/26 18:12:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:12:29 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 18:12:36 visual_prompt]: Epoch 100 / 100: avg data time: 6.39e-02, avg batch time: 0.5069, average train loss: 0.0048
[09/26 18:12:37 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1677, average loss: 4.5637
[09/26 18:12:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 92.00	
[09/26 18:12:37 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 18:12:37 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 18:12:37 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 18:12:37 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 18:12:37 visual_prompt]: Training with config:
[09/26 18:12:37 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 18:12:37 visual_prompt]: Loading training data...
[09/26 18:12:37 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 18:12:38 visual_prompt]: Number of images: 800
[09/26 18:12:38 visual_prompt]: Number of classes: 9 / 9
[09/26 18:12:38 visual_prompt]: Loading validation data...
[09/26 18:12:38 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 18:12:39 visual_prompt]: Number of images: 200
[09/26 18:12:39 visual_prompt]: Number of classes: 9 / 9
[09/26 18:12:39 visual_prompt]: Constructing models...
[09/26 18:12:41 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 18:12:41 visual_prompt]: tuned percent:0.542
[09/26 18:12:41 visual_prompt]: Device used for model: 0
[09/26 18:12:41 visual_prompt]: Setting up Evaluator...
[09/26 18:12:41 visual_prompt]: Setting up Trainer...
[09/26 18:12:41 visual_prompt]: 	Setting up the optimizer...
[09/26 18:12:41 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 18:12:48 visual_prompt]: Epoch 1 / 100: avg data time: 6.55e-02, avg batch time: 0.5086, average train loss: 2.8841
[09/26 18:12:50 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1670, average loss: 2.9516
[09/26 18:12:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 18:12:50 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 18:12:50 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 18:12:57 visual_prompt]: Epoch 2 / 100: avg data time: 6.11e-02, avg batch time: 0.5033, average train loss: 2.4363
[09/26 18:12:59 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1674, average loss: 2.2260
[09/26 18:12:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.50	
[09/26 18:12:59 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 18:13:06 visual_prompt]: Epoch 3 / 100: avg data time: 6.99e-02, avg batch time: 0.5122, average train loss: 2.2453
[09/26 18:13:07 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1671, average loss: 2.2107
[09/26 18:13:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 18:13:07 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 18:13:14 visual_prompt]: Epoch 4 / 100: avg data time: 6.50e-02, avg batch time: 0.5070, average train loss: 2.2228
[09/26 18:13:16 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1665, average loss: 2.2078
[09/26 18:13:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/26 18:13:16 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 18:13:23 visual_prompt]: Epoch 5 / 100: avg data time: 6.66e-02, avg batch time: 0.5083, average train loss: 2.2010
[09/26 18:13:24 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1666, average loss: 2.1980
[09/26 18:13:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 18:13:24 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 18:13:31 visual_prompt]: Epoch 6 / 100: avg data time: 5.74e-02, avg batch time: 0.4998, average train loss: 2.1976
[09/26 18:13:33 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1669, average loss: 2.2096
[09/26 18:13:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.00	
[09/26 18:13:33 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 18:13:40 visual_prompt]: Epoch 7 / 100: avg data time: 6.70e-02, avg batch time: 0.5087, average train loss: 2.1879
[09/26 18:13:42 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1671, average loss: 2.1993
[09/26 18:13:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 65.00	
[09/26 18:13:42 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 18:13:49 visual_prompt]: Epoch 8 / 100: avg data time: 6.20e-02, avg batch time: 0.5032, average train loss: 2.1727
[09/26 18:13:50 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1668, average loss: 2.1698
[09/26 18:13:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 63.00	
[09/26 18:13:50 visual_prompt]: Best epoch 8: best metric: 0.140
[09/26 18:13:50 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 18:13:57 visual_prompt]: Epoch 9 / 100: avg data time: 5.43e-02, avg batch time: 0.4976, average train loss: 2.1152
[09/26 18:13:59 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1671, average loss: 2.1080
[09/26 18:13:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 72.00	
[09/26 18:13:59 visual_prompt]: Best epoch 9: best metric: 0.150
[09/26 18:13:59 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 18:14:06 visual_prompt]: Epoch 10 / 100: avg data time: 6.27e-02, avg batch time: 0.5046, average train loss: 2.0589
[09/26 18:14:07 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1667, average loss: 2.0933
[09/26 18:14:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 78.50	
[09/26 18:14:07 visual_prompt]: Best epoch 10: best metric: 0.175
[09/26 18:14:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 18:14:14 visual_prompt]: Epoch 11 / 100: avg data time: 7.01e-02, avg batch time: 0.5122, average train loss: 2.0583
[09/26 18:14:16 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1671, average loss: 2.4276
[09/26 18:14:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 53.50	
[09/26 18:14:16 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 18:14:23 visual_prompt]: Epoch 12 / 100: avg data time: 6.49e-02, avg batch time: 0.5076, average train loss: 2.0663
[09/26 18:14:24 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1670, average loss: 2.0184
[09/26 18:14:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 77.50	
[09/26 18:14:24 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 18:14:31 visual_prompt]: Epoch 13 / 100: avg data time: 6.80e-02, avg batch time: 0.5112, average train loss: 1.9499
[09/26 18:14:33 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1672, average loss: 2.0384
[09/26 18:14:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 78.00	
[09/26 18:14:33 visual_prompt]: Best epoch 13: best metric: 0.210
[09/26 18:14:33 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 18:14:40 visual_prompt]: Epoch 14 / 100: avg data time: 6.73e-02, avg batch time: 0.5102, average train loss: 1.9113
[09/26 18:14:42 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1670, average loss: 1.9932
[09/26 18:14:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 82.00	
[09/26 18:14:42 visual_prompt]: Best epoch 14: best metric: 0.225
[09/26 18:14:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 18:14:49 visual_prompt]: Epoch 15 / 100: avg data time: 6.80e-02, avg batch time: 0.5109, average train loss: 1.8583
[09/26 18:14:51 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1671, average loss: 1.9529
[09/26 18:14:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 81.50	
[09/26 18:14:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 18:14:57 visual_prompt]: Epoch 16 / 100: avg data time: 6.15e-02, avg batch time: 0.5041, average train loss: 1.7536
[09/26 18:14:59 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1672, average loss: 1.9833
[09/26 18:14:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 82.50	
[09/26 18:14:59 visual_prompt]: Best epoch 16: best metric: 0.250
[09/26 18:14:59 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 18:15:06 visual_prompt]: Epoch 17 / 100: avg data time: 6.16e-02, avg batch time: 0.5047, average train loss: 1.6869
[09/26 18:15:08 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1674, average loss: 1.9734
[09/26 18:15:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 83.50	
[09/26 18:15:08 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 18:15:15 visual_prompt]: Epoch 18 / 100: avg data time: 6.54e-02, avg batch time: 0.5076, average train loss: 1.6401
[09/26 18:15:16 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1670, average loss: 2.1971
[09/26 18:15:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 80.50	
[09/26 18:15:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 18:15:23 visual_prompt]: Epoch 19 / 100: avg data time: 6.38e-02, avg batch time: 0.5071, average train loss: 1.7056
[09/26 18:15:25 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.1669, average loss: 1.7691
[09/26 18:15:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 88.50	
[09/26 18:15:25 visual_prompt]: Best epoch 19: best metric: 0.295
[09/26 18:15:25 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 18:15:32 visual_prompt]: Epoch 20 / 100: avg data time: 5.46e-02, avg batch time: 0.4981, average train loss: 1.6295
[09/26 18:15:33 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1674, average loss: 1.9275
[09/26 18:15:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 86.50	
[09/26 18:15:33 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 18:15:40 visual_prompt]: Epoch 21 / 100: avg data time: 6.41e-02, avg batch time: 0.5065, average train loss: 1.5201
[09/26 18:15:42 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1678, average loss: 1.8700
[09/26 18:15:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 87.00	
[09/26 18:15:42 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 18:15:49 visual_prompt]: Epoch 22 / 100: avg data time: 5.96e-02, avg batch time: 0.5039, average train loss: 1.4340
[09/26 18:15:50 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1675, average loss: 1.9529
[09/26 18:15:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 85.50	
[09/26 18:15:51 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 18:15:57 visual_prompt]: Epoch 23 / 100: avg data time: 6.02e-02, avg batch time: 0.5040, average train loss: 1.4827
[09/26 18:15:59 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1674, average loss: 1.8565
[09/26 18:15:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 88.50	
[09/26 18:15:59 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 18:16:06 visual_prompt]: Epoch 24 / 100: avg data time: 5.44e-02, avg batch time: 0.4996, average train loss: 1.5118
[09/26 18:16:08 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1672, average loss: 1.8533
[09/26 18:16:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 89.00	
[09/26 18:16:08 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 18:16:15 visual_prompt]: Epoch 25 / 100: avg data time: 6.84e-02, avg batch time: 0.5106, average train loss: 1.3408
[09/26 18:16:16 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1675, average loss: 2.0129
[09/26 18:16:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 89.00	
[09/26 18:16:16 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 18:16:23 visual_prompt]: Epoch 26 / 100: avg data time: 6.10e-02, avg batch time: 0.5037, average train loss: 1.2852
[09/26 18:16:25 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1674, average loss: 1.9297
[09/26 18:16:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 86.50	
[09/26 18:16:25 visual_prompt]: Best epoch 26: best metric: 0.300
[09/26 18:16:25 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 18:16:32 visual_prompt]: Epoch 27 / 100: avg data time: 6.41e-02, avg batch time: 0.5083, average train loss: 1.2112
[09/26 18:16:33 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1675, average loss: 1.8539
[09/26 18:16:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 89.50	
[09/26 18:16:33 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 18:16:40 visual_prompt]: Epoch 28 / 100: avg data time: 5.94e-02, avg batch time: 0.5021, average train loss: 1.2375
[09/26 18:16:42 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1675, average loss: 2.2269
[09/26 18:16:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 83.50	
[09/26 18:16:42 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 18:16:49 visual_prompt]: Epoch 29 / 100: avg data time: 6.28e-02, avg batch time: 0.5056, average train loss: 1.2356
[09/26 18:16:51 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1675, average loss: 1.8800
[09/26 18:16:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 91.00	
[09/26 18:16:51 visual_prompt]: Best epoch 29: best metric: 0.335
[09/26 18:16:51 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 18:16:58 visual_prompt]: Epoch 30 / 100: avg data time: 6.22e-02, avg batch time: 0.5052, average train loss: 1.0986
[09/26 18:16:59 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1675, average loss: 2.2404
[09/26 18:16:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 87.00	
[09/26 18:16:59 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 18:17:06 visual_prompt]: Epoch 31 / 100: avg data time: 6.30e-02, avg batch time: 0.5064, average train loss: 1.0557
[09/26 18:17:08 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1677, average loss: 1.9870
[09/26 18:17:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 95.50	
[09/26 18:17:08 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 18:17:15 visual_prompt]: Epoch 32 / 100: avg data time: 6.92e-02, avg batch time: 0.5113, average train loss: 1.0865
[09/26 18:17:16 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1674, average loss: 2.3174
[09/26 18:17:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 85.50	
[09/26 18:17:16 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 18:17:23 visual_prompt]: Epoch 33 / 100: avg data time: 6.14e-02, avg batch time: 0.5068, average train loss: 0.9702
[09/26 18:17:25 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1673, average loss: 2.0493
[09/26 18:17:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 92.00	
[09/26 18:17:25 visual_prompt]: Best epoch 33: best metric: 0.340
[09/26 18:17:25 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 18:17:32 visual_prompt]: Epoch 34 / 100: avg data time: 5.94e-02, avg batch time: 0.5019, average train loss: 0.9699
[09/26 18:17:34 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1673, average loss: 2.3406
[09/26 18:17:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 90.00	
[09/26 18:17:34 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 18:17:40 visual_prompt]: Epoch 35 / 100: avg data time: 6.25e-02, avg batch time: 0.5051, average train loss: 1.0175
[09/26 18:17:42 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1675, average loss: 2.1169
[09/26 18:17:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.50	
[09/26 18:17:42 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 18:17:49 visual_prompt]: Epoch 36 / 100: avg data time: 6.18e-02, avg batch time: 0.5058, average train loss: 0.9550
[09/26 18:17:51 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.1672, average loss: 2.0897
[09/26 18:17:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 91.00	
[09/26 18:17:51 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 18:17:58 visual_prompt]: Epoch 37 / 100: avg data time: 6.50e-02, avg batch time: 0.5079, average train loss: 0.9486
[09/26 18:17:59 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1673, average loss: 2.2640
[09/26 18:17:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 91.50	
[09/26 18:17:59 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 18:18:06 visual_prompt]: Epoch 38 / 100: avg data time: 6.54e-02, avg batch time: 0.5073, average train loss: 0.8683
[09/26 18:18:08 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1671, average loss: 2.3251
[09/26 18:18:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 89.50	
[09/26 18:18:08 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 18:18:15 visual_prompt]: Epoch 39 / 100: avg data time: 6.27e-02, avg batch time: 0.5046, average train loss: 0.7307
[09/26 18:18:16 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1667, average loss: 2.6141
[09/26 18:18:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 86.00	
[09/26 18:18:16 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 18:18:23 visual_prompt]: Epoch 40 / 100: avg data time: 5.14e-02, avg batch time: 0.4961, average train loss: 0.6187
[09/26 18:18:25 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1671, average loss: 2.9583
[09/26 18:18:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.00	
[09/26 18:18:25 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 18:18:32 visual_prompt]: Epoch 41 / 100: avg data time: 6.03e-02, avg batch time: 0.5044, average train loss: 0.5753
[09/26 18:18:33 visual_prompt]: Inference (val):avg data time: 4.20e-05, avg batch time: 0.1669, average loss: 2.8170
[09/26 18:18:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 91.50	
[09/26 18:18:33 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 18:18:40 visual_prompt]: Epoch 42 / 100: avg data time: 6.14e-02, avg batch time: 0.5036, average train loss: 0.5928
[09/26 18:18:42 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1670, average loss: 3.2560
[09/26 18:18:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 92.00	
[09/26 18:18:42 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 18:18:49 visual_prompt]: Epoch 43 / 100: avg data time: 5.47e-02, avg batch time: 0.4979, average train loss: 0.7165
[09/26 18:18:50 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1671, average loss: 2.8530
[09/26 18:18:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 90.00	
[09/26 18:18:50 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 18:18:57 visual_prompt]: Epoch 44 / 100: avg data time: 6.11e-02, avg batch time: 0.5046, average train loss: 0.6214
[09/26 18:18:59 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1672, average loss: 2.7289
[09/26 18:18:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 91.00	
[09/26 18:18:59 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 18:19:06 visual_prompt]: Epoch 45 / 100: avg data time: 6.79e-02, avg batch time: 0.5107, average train loss: 0.5326
[09/26 18:19:08 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1669, average loss: 2.9809
[09/26 18:19:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 91.50	
[09/26 18:19:08 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 18:19:15 visual_prompt]: Epoch 46 / 100: avg data time: 6.15e-02, avg batch time: 0.5032, average train loss: 0.4904
[09/26 18:19:16 visual_prompt]: Inference (val):avg data time: 4.63e-05, avg batch time: 0.1671, average loss: 3.0338
[09/26 18:19:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.50	
[09/26 18:19:16 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 18:19:23 visual_prompt]: Epoch 47 / 100: avg data time: 5.75e-02, avg batch time: 0.4996, average train loss: 0.4711
[09/26 18:19:25 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1669, average loss: 3.3820
[09/26 18:19:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 91.50	
[09/26 18:19:25 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 18:19:32 visual_prompt]: Epoch 48 / 100: avg data time: 6.49e-02, avg batch time: 0.5080, average train loss: 0.4602
[09/26 18:19:33 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1672, average loss: 3.2241
[09/26 18:19:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 92.00	
[09/26 18:19:33 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 18:19:40 visual_prompt]: Epoch 49 / 100: avg data time: 6.75e-02, avg batch time: 0.5095, average train loss: 0.3844
[09/26 18:19:42 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.1674, average loss: 3.3154
[09/26 18:19:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 89.00	
[09/26 18:19:42 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 18:19:49 visual_prompt]: Epoch 50 / 100: avg data time: 6.44e-02, avg batch time: 0.5082, average train loss: 0.3073
[09/26 18:19:51 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1675, average loss: 3.9698
[09/26 18:19:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 86.50	
[09/26 18:19:51 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 18:19:57 visual_prompt]: Epoch 51 / 100: avg data time: 6.39e-02, avg batch time: 0.5066, average train loss: 0.2903
[09/26 18:19:59 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.1671, average loss: 4.3232
[09/26 18:19:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 87.50	
[09/26 18:19:59 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 18:20:06 visual_prompt]: Epoch 52 / 100: avg data time: 5.55e-02, avg batch time: 0.5024, average train loss: 0.3001
[09/26 18:20:08 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1674, average loss: 4.2878
[09/26 18:20:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 85.50	
[09/26 18:20:08 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 18:20:15 visual_prompt]: Epoch 53 / 100: avg data time: 6.75e-02, avg batch time: 0.5096, average train loss: 0.3008
[09/26 18:20:16 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1672, average loss: 4.1496
[09/26 18:20:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 94.00	
[09/26 18:20:16 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 18:20:23 visual_prompt]: Epoch 54 / 100: avg data time: 6.22e-02, avg batch time: 0.5042, average train loss: 0.3689
[09/26 18:20:25 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1673, average loss: 4.2740
[09/26 18:20:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 84.50	
[09/26 18:20:25 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 18:20:32 visual_prompt]: Epoch 55 / 100: avg data time: 6.55e-02, avg batch time: 0.5085, average train loss: 0.2726
[09/26 18:20:33 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1673, average loss: 4.1243
[09/26 18:20:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 92.50	
[09/26 18:20:33 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 18:20:40 visual_prompt]: Epoch 56 / 100: avg data time: 5.81e-02, avg batch time: 0.5046, average train loss: 0.3087
[09/26 18:20:42 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1674, average loss: 4.1972
[09/26 18:20:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 88.50	
[09/26 18:20:42 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 18:20:49 visual_prompt]: Epoch 57 / 100: avg data time: 6.67e-02, avg batch time: 0.5097, average train loss: 0.2900
[09/26 18:20:51 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1675, average loss: 4.2379
[09/26 18:20:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 89.00	
[09/26 18:20:51 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 18:20:58 visual_prompt]: Epoch 58 / 100: avg data time: 6.10e-02, avg batch time: 0.5050, average train loss: 0.1669
[09/26 18:20:59 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1674, average loss: 4.9651
[09/26 18:20:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 86.00	
[09/26 18:20:59 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 18:21:06 visual_prompt]: Epoch 59 / 100: avg data time: 6.00e-02, avg batch time: 0.5039, average train loss: 0.2018
[09/26 18:21:08 visual_prompt]: Inference (val):avg data time: 4.43e-05, avg batch time: 0.1675, average loss: 4.5749
[09/26 18:21:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 90.50	
[09/26 18:21:08 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 18:21:15 visual_prompt]: Epoch 60 / 100: avg data time: 6.01e-02, avg batch time: 0.5038, average train loss: 0.3184
[09/26 18:21:16 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1676, average loss: 4.8334
[09/26 18:21:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 85.00	
[09/26 18:21:16 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 18:21:23 visual_prompt]: Epoch 61 / 100: avg data time: 5.82e-02, avg batch time: 0.5005, average train loss: 0.1807
[09/26 18:21:25 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1675, average loss: 4.7360
[09/26 18:21:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.00	
[09/26 18:21:25 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 18:21:32 visual_prompt]: Epoch 62 / 100: avg data time: 6.36e-02, avg batch time: 0.5059, average train loss: 0.1279
[09/26 18:21:33 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1675, average loss: 4.5847
[09/26 18:21:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 90.50	
[09/26 18:21:33 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 18:21:40 visual_prompt]: Epoch 63 / 100: avg data time: 6.07e-02, avg batch time: 0.5026, average train loss: 0.1105
[09/26 18:21:42 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1673, average loss: 4.8580
[09/26 18:21:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 91.00	
[09/26 18:21:42 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 18:21:49 visual_prompt]: Epoch 64 / 100: avg data time: 6.51e-02, avg batch time: 0.5077, average train loss: 0.0848
[09/26 18:21:50 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1675, average loss: 5.1736
[09/26 18:21:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 87.50	
[09/26 18:21:50 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 18:21:57 visual_prompt]: Epoch 65 / 100: avg data time: 5.70e-02, avg batch time: 0.4999, average train loss: 0.0646
[09/26 18:21:59 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1672, average loss: 5.1730
[09/26 18:21:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 88.00	
[09/26 18:21:59 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 18:22:06 visual_prompt]: Epoch 66 / 100: avg data time: 6.24e-02, avg batch time: 0.5063, average train loss: 0.1104
[09/26 18:22:08 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1673, average loss: 5.4331
[09/26 18:22:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 87.00	
[09/26 18:22:08 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 18:22:14 visual_prompt]: Epoch 67 / 100: avg data time: 5.87e-02, avg batch time: 0.5012, average train loss: 0.0800
[09/26 18:22:16 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1673, average loss: 5.3583
[09/26 18:22:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 88.50	
[09/26 18:22:16 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 18:22:23 visual_prompt]: Epoch 68 / 100: avg data time: 6.67e-02, avg batch time: 0.5094, average train loss: 0.0609
[09/26 18:22:25 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1674, average loss: 5.4496
[09/26 18:22:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 18:22:25 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 18:22:32 visual_prompt]: Epoch 69 / 100: avg data time: 5.71e-02, avg batch time: 0.4993, average train loss: 0.0544
[09/26 18:22:33 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.1673, average loss: 5.6791
[09/26 18:22:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 89.00	
[09/26 18:22:33 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 18:22:40 visual_prompt]: Epoch 70 / 100: avg data time: 6.41e-02, avg batch time: 0.5072, average train loss: 0.0511
[09/26 18:22:42 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1670, average loss: 5.5885
[09/26 18:22:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 90.50	
[09/26 18:22:42 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 18:22:49 visual_prompt]: Epoch 71 / 100: avg data time: 6.32e-02, avg batch time: 0.5049, average train loss: 0.0671
[09/26 18:22:50 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1676, average loss: 5.5151
[09/26 18:22:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 91.50	
[09/26 18:22:50 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 18:22:57 visual_prompt]: Epoch 72 / 100: avg data time: 6.27e-02, avg batch time: 0.5052, average train loss: 0.0416
[09/26 18:22:59 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1670, average loss: 5.9683
[09/26 18:22:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 89.50	
[09/26 18:22:59 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 18:23:06 visual_prompt]: Epoch 73 / 100: avg data time: 6.41e-02, avg batch time: 0.5070, average train loss: 0.0225
[09/26 18:23:08 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1673, average loss: 5.9830
[09/26 18:23:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 89.50	
[09/26 18:23:08 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 18:23:15 visual_prompt]: Epoch 74 / 100: avg data time: 6.38e-02, avg batch time: 0.5067, average train loss: 0.0223
[09/26 18:23:16 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1668, average loss: 6.1337
[09/26 18:23:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 90.00	
[09/26 18:23:16 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 18:23:23 visual_prompt]: Epoch 75 / 100: avg data time: 6.34e-02, avg batch time: 0.5062, average train loss: 0.0191
[09/26 18:23:25 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1671, average loss: 6.0528
[09/26 18:23:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 90.00	
[09/26 18:23:25 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 18:23:32 visual_prompt]: Epoch 76 / 100: avg data time: 6.49e-02, avg batch time: 0.5068, average train loss: 0.0183
[09/26 18:23:33 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1673, average loss: 6.3369
[09/26 18:23:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 88.50	
[09/26 18:23:33 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 18:23:40 visual_prompt]: Epoch 77 / 100: avg data time: 5.88e-02, avg batch time: 0.5012, average train loss: 0.0208
[09/26 18:23:42 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1669, average loss: 6.4435
[09/26 18:23:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.00	
[09/26 18:23:42 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 18:23:49 visual_prompt]: Epoch 78 / 100: avg data time: 6.58e-02, avg batch time: 0.5097, average train loss: 0.0159
[09/26 18:23:50 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1668, average loss: 6.4036
[09/26 18:23:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.50	
[09/26 18:23:50 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 18:23:57 visual_prompt]: Epoch 79 / 100: avg data time: 5.55e-02, avg batch time: 0.4985, average train loss: 0.0107
[09/26 18:23:59 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.1670, average loss: 6.3927
[09/26 18:23:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 89.00	
[09/26 18:23:59 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 18:24:06 visual_prompt]: Epoch 80 / 100: avg data time: 6.78e-02, avg batch time: 0.5119, average train loss: 0.0101
[09/26 18:24:08 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1674, average loss: 6.4211
[09/26 18:24:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.00	
[09/26 18:24:08 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 18:24:14 visual_prompt]: Epoch 81 / 100: avg data time: 6.03e-02, avg batch time: 0.5034, average train loss: 0.0091
[09/26 18:24:16 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1671, average loss: 6.4671
[09/26 18:24:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.00	
[09/26 18:24:16 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 18:24:23 visual_prompt]: Epoch 82 / 100: avg data time: 6.74e-02, avg batch time: 0.5102, average train loss: 0.0130
[09/26 18:24:25 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1674, average loss: 6.4896
[09/26 18:24:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 88.50	
[09/26 18:24:25 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 18:24:31 visual_prompt]: Epoch 83 / 100: avg data time: 5.17e-02, avg batch time: 0.4968, average train loss: 0.0083
[09/26 18:24:33 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1671, average loss: 6.4829
[09/26 18:24:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 89.00	
[09/26 18:24:33 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 18:24:40 visual_prompt]: Epoch 84 / 100: avg data time: 5.81e-02, avg batch time: 0.5009, average train loss: 0.0113
[09/26 18:24:42 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1675, average loss: 6.4532
[09/26 18:24:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 89.00	
[09/26 18:24:42 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 18:24:49 visual_prompt]: Epoch 85 / 100: avg data time: 6.09e-02, avg batch time: 0.5055, average train loss: 0.0082
[09/26 18:24:50 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1677, average loss: 6.4967
[09/26 18:24:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 88.50	
[09/26 18:24:50 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 18:24:57 visual_prompt]: Epoch 86 / 100: avg data time: 6.03e-02, avg batch time: 0.5036, average train loss: 0.0091
[09/26 18:24:59 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1673, average loss: 6.5391
[09/26 18:24:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 88.50	
[09/26 18:24:59 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 18:25:06 visual_prompt]: Epoch 87 / 100: avg data time: 4.89e-02, avg batch time: 0.4929, average train loss: 0.0099
[09/26 18:25:07 visual_prompt]: Inference (val):avg data time: 4.66e-05, avg batch time: 0.1673, average loss: 6.5939
[09/26 18:25:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 88.50	
[09/26 18:25:07 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 18:25:14 visual_prompt]: Epoch 88 / 100: avg data time: 6.43e-02, avg batch time: 0.5086, average train loss: 0.0086
[09/26 18:25:16 visual_prompt]: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1675, average loss: 6.6274
[09/26 18:25:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 88.50	
[09/26 18:25:16 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 18:25:23 visual_prompt]: Epoch 89 / 100: avg data time: 6.71e-02, avg batch time: 0.5094, average train loss: 0.0084
[09/26 18:25:24 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1675, average loss: 6.6063
[09/26 18:25:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 88.50	
[09/26 18:25:24 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 18:25:31 visual_prompt]: Epoch 90 / 100: avg data time: 6.40e-02, avg batch time: 0.5082, average train loss: 0.0081
[09/26 18:25:33 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1677, average loss: 6.5749
[09/26 18:25:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 18:25:33 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 18:25:40 visual_prompt]: Epoch 91 / 100: avg data time: 6.49e-02, avg batch time: 0.5080, average train loss: 0.0074
[09/26 18:25:42 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1678, average loss: 6.5694
[09/26 18:25:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 18:25:42 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 18:25:49 visual_prompt]: Epoch 92 / 100: avg data time: 6.42e-02, avg batch time: 0.5087, average train loss: 0.0072
[09/26 18:25:50 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1673, average loss: 6.5657
[09/26 18:25:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 18:25:50 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 18:25:57 visual_prompt]: Epoch 93 / 100: avg data time: 5.87e-02, avg batch time: 0.5021, average train loss: 0.0062
[09/26 18:25:59 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1672, average loss: 6.5683
[09/26 18:25:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 18:25:59 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 18:26:06 visual_prompt]: Epoch 94 / 100: avg data time: 5.96e-02, avg batch time: 0.5029, average train loss: 0.0093
[09/26 18:26:07 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1676, average loss: 6.5706
[09/26 18:26:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 18:26:07 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 18:26:14 visual_prompt]: Epoch 95 / 100: avg data time: 6.85e-02, avg batch time: 0.5112, average train loss: 0.0098
[09/26 18:26:16 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1669, average loss: 6.5691
[09/26 18:26:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 18:26:16 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 18:26:23 visual_prompt]: Epoch 96 / 100: avg data time: 5.94e-02, avg batch time: 0.5014, average train loss: 0.0055
[09/26 18:26:25 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.1670, average loss: 6.5688
[09/26 18:26:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 18:26:25 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 18:26:32 visual_prompt]: Epoch 97 / 100: avg data time: 6.52e-02, avg batch time: 0.5096, average train loss: 0.0086
[09/26 18:26:33 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1669, average loss: 6.5726
[09/26 18:26:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 18:26:33 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 18:26:40 visual_prompt]: Epoch 98 / 100: avg data time: 6.24e-02, avg batch time: 0.5053, average train loss: 0.0066
[09/26 18:26:42 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1674, average loss: 6.5748
[09/26 18:26:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 18:26:42 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 18:26:49 visual_prompt]: Epoch 99 / 100: avg data time: 6.05e-02, avg batch time: 0.5037, average train loss: 0.0080
[09/26 18:26:50 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1675, average loss: 6.5760
[09/26 18:26:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 88.50	
[09/26 18:26:50 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 18:26:57 visual_prompt]: Epoch 100 / 100: avg data time: 6.40e-02, avg batch time: 0.5069, average train loss: 0.0117
[09/26 18:26:59 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1681, average loss: 6.5765
[09/26 18:26:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 88.50	
[09/26 18:26:59 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 18:26:59 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 18:26:59 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 18:26:59 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 18:26:59 visual_prompt]: Training with config:
[09/26 18:26:59 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.1_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 18:26:59 visual_prompt]: Loading training data...
[09/26 18:26:59 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 18:27:00 visual_prompt]: Number of images: 800
[09/26 18:27:00 visual_prompt]: Number of classes: 9 / 9
[09/26 18:27:00 visual_prompt]: Loading validation data...
[09/26 18:27:00 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 18:27:01 visual_prompt]: Number of images: 200
[09/26 18:27:01 visual_prompt]: Number of classes: 9 / 9
[09/26 18:27:01 visual_prompt]: Constructing models...
[09/26 18:27:03 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 18:27:03 visual_prompt]: tuned percent:0.542
[09/26 18:27:03 visual_prompt]: Device used for model: 0
[09/26 18:27:03 visual_prompt]: Setting up Evaluator...
[09/26 18:27:03 visual_prompt]: Setting up Trainer...
[09/26 18:27:03 visual_prompt]: 	Setting up the optimizer...
[09/26 18:27:03 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 18:27:10 visual_prompt]: Epoch 1 / 100: avg data time: 6.11e-02, avg batch time: 0.5040, average train loss: 2.8792
[09/26 18:27:12 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1671, average loss: 2.9516
[09/26 18:27:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 18:27:12 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 18:27:12 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.010000000000000002
[09/26 18:27:19 visual_prompt]: Epoch 2 / 100: avg data time: 6.04e-02, avg batch time: 0.5040, average train loss: 2.4408
[09/26 18:27:20 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1672, average loss: 2.2371
[09/26 18:27:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.00	
[09/26 18:27:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.020000000000000004
[09/26 18:27:27 visual_prompt]: Epoch 3 / 100: avg data time: 6.32e-02, avg batch time: 0.5078, average train loss: 2.2297
[09/26 18:27:29 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1672, average loss: 2.2221
[09/26 18:27:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 56.00	
[09/26 18:27:29 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.03
[09/26 18:27:36 visual_prompt]: Epoch 4 / 100: avg data time: 6.38e-02, avg batch time: 0.5057, average train loss: 2.2172
[09/26 18:27:38 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1675, average loss: 2.1965
[09/26 18:27:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 61.50	
[09/26 18:27:38 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.04000000000000001
[09/26 18:27:45 visual_prompt]: Epoch 5 / 100: avg data time: 6.68e-02, avg batch time: 0.5107, average train loss: 2.2048
[09/26 18:27:46 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1672, average loss: 2.2076
[09/26 18:27:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 56.50	
[09/26 18:27:46 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.05
[09/26 18:27:53 visual_prompt]: Epoch 6 / 100: avg data time: 6.43e-02, avg batch time: 0.5069, average train loss: 2.2083
[09/26 18:27:55 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1673, average loss: 2.2193
[09/26 18:27:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 18:27:55 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.06
[09/26 18:28:02 visual_prompt]: Epoch 7 / 100: avg data time: 6.25e-02, avg batch time: 0.5062, average train loss: 2.2112
[09/26 18:28:03 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1672, average loss: 2.2002
[09/26 18:28:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 56.50	
[09/26 18:28:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.06999999999999999
[09/26 18:28:10 visual_prompt]: Epoch 8 / 100: avg data time: 6.48e-02, avg batch time: 0.5066, average train loss: 2.1703
[09/26 18:28:12 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1673, average loss: 2.1549
[09/26 18:28:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 65.00	
[09/26 18:28:12 visual_prompt]: Best epoch 8: best metric: 0.145
[09/26 18:28:12 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.08000000000000002
[09/26 18:28:19 visual_prompt]: Epoch 9 / 100: avg data time: 6.36e-02, avg batch time: 0.5062, average train loss: 2.1302
[09/26 18:28:21 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1675, average loss: 2.1000
[09/26 18:28:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 73.50	
[09/26 18:28:21 visual_prompt]: Best epoch 9: best metric: 0.170
[09/26 18:28:21 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.09000000000000001
[09/26 18:28:27 visual_prompt]: Epoch 10 / 100: avg data time: 5.75e-02, avg batch time: 0.4996, average train loss: 2.0628
[09/26 18:28:29 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1673, average loss: 2.1250
[09/26 18:28:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 71.50	
[09/26 18:28:29 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.1
[09/26 18:28:36 visual_prompt]: Epoch 11 / 100: avg data time: 5.91e-02, avg batch time: 0.5023, average train loss: 2.0848
[09/26 18:28:38 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1670, average loss: 2.2025
[09/26 18:28:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 73.50	
[09/26 18:28:38 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0999695413509548
[09/26 18:28:45 visual_prompt]: Epoch 12 / 100: avg data time: 6.38e-02, avg batch time: 0.5061, average train loss: 1.9815
[09/26 18:28:46 visual_prompt]: Inference (val):avg data time: 5.22e-05, avg batch time: 0.1674, average loss: 2.1480
[09/26 18:28:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 71.50	
[09/26 18:28:46 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.09987820251299122
[09/26 18:28:53 visual_prompt]: Epoch 13 / 100: avg data time: 5.73e-02, avg batch time: 0.4999, average train loss: 1.9094
[09/26 18:28:55 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1674, average loss: 2.1218
[09/26 18:28:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 72.00	
[09/26 18:28:55 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.09972609476841367
[09/26 18:29:02 visual_prompt]: Epoch 14 / 100: avg data time: 6.26e-02, avg batch time: 0.5066, average train loss: 1.8624
[09/26 18:29:03 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1670, average loss: 2.0639
[09/26 18:29:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 75.50	
[09/26 18:29:03 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.09951340343707853
[09/26 18:29:10 visual_prompt]: Epoch 15 / 100: avg data time: 4.84e-02, avg batch time: 0.4944, average train loss: 1.7674
[09/26 18:29:12 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1670, average loss: 1.9280
[09/26 18:29:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 82.50	
[09/26 18:29:12 visual_prompt]: Best epoch 15: best metric: 0.205
[09/26 18:29:12 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.09924038765061041
[09/26 18:29:19 visual_prompt]: Epoch 16 / 100: avg data time: 5.81e-02, avg batch time: 0.5018, average train loss: 1.7074
[09/26 18:29:20 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1672, average loss: 2.0234
[09/26 18:29:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 82.00	
[09/26 18:29:20 visual_prompt]: Best epoch 16: best metric: 0.255
[09/26 18:29:20 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.09890738003669029
[09/26 18:29:27 visual_prompt]: Epoch 17 / 100: avg data time: 6.96e-02, avg batch time: 0.5132, average train loss: 1.6433
[09/26 18:29:29 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1676, average loss: 1.9569
[09/26 18:29:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 85.00	
[09/26 18:29:29 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.09851478631379983
[09/26 18:29:36 visual_prompt]: Epoch 18 / 100: avg data time: 5.82e-02, avg batch time: 0.5032, average train loss: 1.6753
[09/26 18:29:37 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1672, average loss: 1.9752
[09/26 18:29:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 81.50	
[09/26 18:29:37 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.09806308479691594
[09/26 18:29:44 visual_prompt]: Epoch 19 / 100: avg data time: 5.94e-02, avg batch time: 0.5029, average train loss: 1.6775
[09/26 18:29:46 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.1670, average loss: 1.9150
[09/26 18:29:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 87.00	
[09/26 18:29:46 visual_prompt]: Best epoch 19: best metric: 0.260
[09/26 18:29:46 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.09755282581475769
[09/26 18:29:53 visual_prompt]: Epoch 20 / 100: avg data time: 6.19e-02, avg batch time: 0.5050, average train loss: 1.5644
[09/26 18:29:54 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1671, average loss: 1.8557
[09/26 18:29:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 87.00	
[09/26 18:29:54 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.09698463103929543
[09/26 18:30:01 visual_prompt]: Epoch 21 / 100: avg data time: 5.15e-02, avg batch time: 0.4945, average train loss: 1.4532
[09/26 18:30:03 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1670, average loss: 1.9930
[09/26 18:30:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 86.50	
[09/26 18:30:03 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.09635919272833937
[09/26 18:30:10 visual_prompt]: Epoch 22 / 100: avg data time: 6.26e-02, avg batch time: 0.5057, average train loss: 1.5395
[09/26 18:30:11 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1672, average loss: 1.8352
[09/26 18:30:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 18:30:11 visual_prompt]: Best epoch 22: best metric: 0.270
[09/26 18:30:11 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.09567727288213004
[09/26 18:30:18 visual_prompt]: Epoch 23 / 100: avg data time: 6.18e-02, avg batch time: 0.5045, average train loss: 1.5548
[09/26 18:30:20 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1680, average loss: 1.7930
[09/26 18:30:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 92.00	
[09/26 18:30:20 visual_prompt]: Best epoch 23: best metric: 0.280
[09/26 18:30:20 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.09493970231495835
[09/26 18:30:27 visual_prompt]: Epoch 24 / 100: avg data time: 6.57e-02, avg batch time: 0.5094, average train loss: 1.4593
[09/26 18:30:29 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1675, average loss: 1.8403
[09/26 18:30:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 91.00	
[09/26 18:30:29 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.09414737964294635
[09/26 18:30:36 visual_prompt]: Epoch 25 / 100: avg data time: 5.45e-02, avg batch time: 0.5028, average train loss: 1.3009
[09/26 18:30:37 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1674, average loss: 1.7712
[09/26 18:30:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 92.50	
[09/26 18:30:37 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.09330127018922195
[09/26 18:30:44 visual_prompt]: Epoch 26 / 100: avg data time: 4.93e-02, avg batch time: 0.4936, average train loss: 1.2599
[09/26 18:30:46 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1671, average loss: 2.0037
[09/26 18:30:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 90.00	
[09/26 18:30:46 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0924024048078213
[09/26 18:30:53 visual_prompt]: Epoch 27 / 100: avg data time: 6.68e-02, avg batch time: 0.5104, average train loss: 1.2540
[09/26 18:30:54 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1669, average loss: 1.9485
[09/26 18:30:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 91.00	
[09/26 18:30:54 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.09145187862775209
[09/26 18:31:01 visual_prompt]: Epoch 28 / 100: avg data time: 6.22e-02, avg batch time: 0.5053, average train loss: 1.2863
[09/26 18:31:03 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1671, average loss: 1.9689
[09/26 18:31:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 92.00	
[09/26 18:31:03 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.09045084971874738
[09/26 18:31:10 visual_prompt]: Epoch 29 / 100: avg data time: 6.38e-02, avg batch time: 0.5069, average train loss: 1.1810
[09/26 18:31:11 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1671, average loss: 1.9270
[09/26 18:31:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 93.50	
[09/26 18:31:11 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.0894005376803361
[09/26 18:31:18 visual_prompt]: Epoch 30 / 100: avg data time: 6.25e-02, avg batch time: 0.5070, average train loss: 1.1861
[09/26 18:31:20 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1671, average loss: 1.9297
[09/26 18:31:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 92.50	
[09/26 18:31:20 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.0883022221559489
[09/26 18:31:27 visual_prompt]: Epoch 31 / 100: avg data time: 6.26e-02, avg batch time: 0.5063, average train loss: 1.1073
[09/26 18:31:29 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1670, average loss: 1.9962
[09/26 18:31:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 92.00	
[09/26 18:31:29 visual_prompt]: Best epoch 31: best metric: 0.300
[09/26 18:31:29 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.08715724127386971
[09/26 18:31:36 visual_prompt]: Epoch 32 / 100: avg data time: 6.45e-02, avg batch time: 0.5079, average train loss: 0.9614
[09/26 18:31:37 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1670, average loss: 2.0264
[09/26 18:31:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 95.00	
[09/26 18:31:37 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.08596699001693256
[09/26 18:31:44 visual_prompt]: Epoch 33 / 100: avg data time: 6.33e-02, avg batch time: 0.5066, average train loss: 0.8442
[09/26 18:31:46 visual_prompt]: Inference (val):avg data time: 5.46e-05, avg batch time: 0.1676, average loss: 2.1586
[09/26 18:31:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 93.50	
[09/26 18:31:46 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.08473291852294987
[09/26 18:31:53 visual_prompt]: Epoch 34 / 100: avg data time: 5.07e-02, avg batch time: 0.4933, average train loss: 0.8503
[09/26 18:31:54 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.1675, average loss: 2.3346
[09/26 18:31:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 93.50	
[09/26 18:31:54 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.08345653031794292
[09/26 18:32:01 visual_prompt]: Epoch 35 / 100: avg data time: 5.22e-02, avg batch time: 0.4995, average train loss: 0.8504
[09/26 18:32:03 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1676, average loss: 2.4707
[09/26 18:32:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.00	
[09/26 18:32:03 visual_prompt]: Best epoch 35: best metric: 0.305
[09/26 18:32:03 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.08213938048432697
[09/26 18:32:10 visual_prompt]: Epoch 36 / 100: avg data time: 5.39e-02, avg batch time: 0.4973, average train loss: 0.7940
[09/26 18:32:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1671, average loss: 2.4781
[09/26 18:32:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 90.50	
[09/26 18:32:11 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.08078307376628291
[09/26 18:32:18 visual_prompt]: Epoch 37 / 100: avg data time: 6.15e-02, avg batch time: 0.5051, average train loss: 0.8007
[09/26 18:32:20 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1675, average loss: 2.8828
[09/26 18:32:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 90.00	
[09/26 18:32:20 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.07938926261462366
[09/26 18:32:27 visual_prompt]: Epoch 38 / 100: avg data time: 6.46e-02, avg batch time: 0.5069, average train loss: 0.8174
[09/26 18:32:29 visual_prompt]: Inference (val):avg data time: 4.62e-05, avg batch time: 0.1674, average loss: 2.6534
[09/26 18:32:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 90.50	
[09/26 18:32:29 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.07795964517353735
[09/26 18:32:35 visual_prompt]: Epoch 39 / 100: avg data time: 6.19e-02, avg batch time: 0.5059, average train loss: 0.7694
[09/26 18:32:37 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1674, average loss: 2.5749
[09/26 18:32:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 91.50	
[09/26 18:32:37 visual_prompt]: Best epoch 39: best metric: 0.320
[09/26 18:32:37 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.07649596321166025
[09/26 18:32:44 visual_prompt]: Epoch 40 / 100: avg data time: 6.45e-02, avg batch time: 0.5074, average train loss: 0.6644
[09/26 18:32:46 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1674, average loss: 2.5478
[09/26 18:32:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 95.00	
[09/26 18:32:46 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.07500000000000001
[09/26 18:32:53 visual_prompt]: Epoch 41 / 100: avg data time: 6.06e-02, avg batch time: 0.5033, average train loss: 0.6288
[09/26 18:32:54 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1676, average loss: 2.7650
[09/26 18:32:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 91.00	
[09/26 18:32:54 visual_prompt]: Best epoch 41: best metric: 0.330
[09/26 18:32:54 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.07347357813929455
[09/26 18:33:01 visual_prompt]: Epoch 42 / 100: avg data time: 6.45e-02, avg batch time: 0.5070, average train loss: 0.5263
[09/26 18:33:03 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1674, average loss: 3.2489
[09/26 18:33:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 87.00	
[09/26 18:33:03 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.07191855733945388
[09/26 18:33:10 visual_prompt]: Epoch 43 / 100: avg data time: 6.56e-02, avg batch time: 0.5081, average train loss: 0.5470
[09/26 18:33:11 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1674, average loss: 3.0761
[09/26 18:33:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.50	
[09/26 18:33:11 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.07033683215379002
[09/26 18:33:18 visual_prompt]: Epoch 44 / 100: avg data time: 6.73e-02, avg batch time: 0.5109, average train loss: 0.4622
[09/26 18:33:20 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1673, average loss: 3.6709
[09/26 18:33:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 89.50	
[09/26 18:33:20 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0687303296707956
[09/26 18:33:27 visual_prompt]: Epoch 45 / 100: avg data time: 4.97e-02, avg batch time: 0.4951, average train loss: 0.3909
[09/26 18:33:29 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1675, average loss: 4.0696
[09/26 18:33:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 18:33:29 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.06710100716628345
[09/26 18:33:35 visual_prompt]: Epoch 46 / 100: avg data time: 4.66e-02, avg batch time: 0.4899, average train loss: 0.4421
[09/26 18:33:37 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1672, average loss: 3.7432
[09/26 18:33:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 87.50	
[09/26 18:33:37 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.06545084971874737
[09/26 18:33:44 visual_prompt]: Epoch 47 / 100: avg data time: 6.07e-02, avg batch time: 0.5044, average train loss: 0.4024
[09/26 18:33:46 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1675, average loss: 3.4489
[09/26 18:33:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 94.00	
[09/26 18:33:46 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.06378186779084996
[09/26 18:33:52 visual_prompt]: Epoch 48 / 100: avg data time: 6.14e-02, avg batch time: 0.5033, average train loss: 0.3333
[09/26 18:33:54 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1670, average loss: 3.9315
[09/26 18:33:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 91.00	
[09/26 18:33:54 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.06209609477998338
[09/26 18:34:01 visual_prompt]: Epoch 49 / 100: avg data time: 6.41e-02, avg batch time: 0.5075, average train loss: 0.3099
[09/26 18:34:03 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1673, average loss: 3.9005
[09/26 18:34:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 91.00	
[09/26 18:34:03 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.06039558454088796
[09/26 18:34:09 visual_prompt]: Epoch 50 / 100: avg data time: 6.12e-02, avg batch time: 0.5040, average train loss: 0.2486
[09/26 18:34:11 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1677, average loss: 4.2829
[09/26 18:34:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 90.00	
[09/26 18:34:11 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.058682408883346526
[09/26 18:34:18 visual_prompt]: Epoch 51 / 100: avg data time: 6.51e-02, avg batch time: 0.5093, average train loss: 0.3554
[09/26 18:34:20 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1673, average loss: 4.3856
[09/26 18:34:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 90.00	
[09/26 18:34:20 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.05695865504800327
[09/26 18:34:27 visual_prompt]: Epoch 52 / 100: avg data time: 6.57e-02, avg batch time: 0.5090, average train loss: 0.2784
[09/26 18:34:28 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1674, average loss: 4.2541
[09/26 18:34:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 89.00	
[09/26 18:34:28 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.05522642316338268
[09/26 18:34:35 visual_prompt]: Epoch 53 / 100: avg data time: 5.45e-02, avg batch time: 0.5000, average train loss: 0.2419
[09/26 18:34:37 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1673, average loss: 3.9180
[09/26 18:34:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 18:34:37 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.05348782368720626
[09/26 18:34:44 visual_prompt]: Epoch 54 / 100: avg data time: 6.23e-02, avg batch time: 0.5062, average train loss: 0.1927
[09/26 18:34:45 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1676, average loss: 4.1814
[09/26 18:34:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 18:34:45 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.05174497483512506
[09/26 18:34:52 visual_prompt]: Epoch 55 / 100: avg data time: 5.51e-02, avg batch time: 0.4984, average train loss: 0.1772
[09/26 18:34:54 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1676, average loss: 4.7063
[09/26 18:34:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 91.00	
[09/26 18:34:54 visual_prompt]: Best epoch 55: best metric: 0.335
[09/26 18:34:54 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.05
[09/26 18:35:01 visual_prompt]: Epoch 56 / 100: avg data time: 5.50e-02, avg batch time: 0.4982, average train loss: 0.2324
[09/26 18:35:02 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.1670, average loss: 4.3087
[09/26 18:35:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 92.00	
[09/26 18:35:02 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.04825502516487497
[09/26 18:35:09 visual_prompt]: Epoch 57 / 100: avg data time: 5.15e-02, avg batch time: 0.4961, average train loss: 0.2745
[09/26 18:35:11 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1671, average loss: 4.9833
[09/26 18:35:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 88.00	
[09/26 18:35:11 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.04651217631279374
[09/26 18:35:18 visual_prompt]: Epoch 58 / 100: avg data time: 5.70e-02, avg batch time: 0.5010, average train loss: 0.2457
[09/26 18:35:19 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1672, average loss: 4.0049
[09/26 18:35:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 92.00	
[09/26 18:35:19 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.04477357683661734
[09/26 18:35:26 visual_prompt]: Epoch 59 / 100: avg data time: 6.33e-02, avg batch time: 0.5072, average train loss: 0.2014
[09/26 18:35:28 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1671, average loss: 4.3551
[09/26 18:35:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 90.50	
[09/26 18:35:28 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.04304134495199674
[09/26 18:35:35 visual_prompt]: Epoch 60 / 100: avg data time: 6.14e-02, avg batch time: 0.5045, average train loss: 0.1249
[09/26 18:35:37 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1671, average loss: 4.4112
[09/26 18:35:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 92.00	
[09/26 18:35:37 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.041317591116653486
[09/26 18:35:43 visual_prompt]: Epoch 61 / 100: avg data time: 6.35e-02, avg batch time: 0.5076, average train loss: 0.0878
[09/26 18:35:45 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1671, average loss: 4.5927
[09/26 18:35:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 92.50	
[09/26 18:35:45 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.039604415459112044
[09/26 18:35:52 visual_prompt]: Epoch 62 / 100: avg data time: 6.45e-02, avg batch time: 0.5075, average train loss: 0.0602
[09/26 18:35:54 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1679, average loss: 4.8941
[09/26 18:35:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 91.50	
[09/26 18:35:54 visual_prompt]: Best epoch 62: best metric: 0.340
[09/26 18:35:54 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.037903905220016625
[09/26 18:36:01 visual_prompt]: Epoch 63 / 100: avg data time: 6.37e-02, avg batch time: 0.5075, average train loss: 0.0486
[09/26 18:36:02 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1670, average loss: 5.0513
[09/26 18:36:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 91.50	
[09/26 18:36:02 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.036218132209150045
[09/26 18:36:09 visual_prompt]: Epoch 64 / 100: avg data time: 5.62e-02, avg batch time: 0.5009, average train loss: 0.0474
[09/26 18:36:11 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1672, average loss: 5.1823
[09/26 18:36:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 92.50	
[09/26 18:36:11 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.03454915028125263
[09/26 18:36:18 visual_prompt]: Epoch 65 / 100: avg data time: 6.57e-02, avg batch time: 0.5096, average train loss: 0.0582
[09/26 18:36:20 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1672, average loss: 5.2457
[09/26 18:36:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 91.00	
[09/26 18:36:20 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.03289899283371657
[09/26 18:36:26 visual_prompt]: Epoch 66 / 100: avg data time: 6.12e-02, avg batch time: 0.5043, average train loss: 0.0666
[09/26 18:36:28 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1676, average loss: 5.3122
[09/26 18:36:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 18:36:28 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0312696703292044
[09/26 18:36:35 visual_prompt]: Epoch 67 / 100: avg data time: 6.18e-02, avg batch time: 0.5053, average train loss: 0.0443
[09/26 18:36:37 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1676, average loss: 5.4740
[09/26 18:36:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 91.50	
[09/26 18:36:37 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.02966316784621
[09/26 18:36:44 visual_prompt]: Epoch 68 / 100: avg data time: 6.13e-02, avg batch time: 0.5044, average train loss: 0.0360
[09/26 18:36:45 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1676, average loss: 5.4403
[09/26 18:36:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 91.50	
[09/26 18:36:45 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.028081442660546126
[09/26 18:36:52 visual_prompt]: Epoch 69 / 100: avg data time: 6.11e-02, avg batch time: 0.5041, average train loss: 0.0439
[09/26 18:36:54 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1676, average loss: 5.4698
[09/26 18:36:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 89.50	
[09/26 18:36:54 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.026526421860705474
[09/26 18:37:01 visual_prompt]: Epoch 70 / 100: avg data time: 6.41e-02, avg batch time: 0.5060, average train loss: 0.0386
[09/26 18:37:02 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1673, average loss: 5.6279
[09/26 18:37:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 89.50	
[09/26 18:37:02 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.025000000000000012
[09/26 18:37:09 visual_prompt]: Epoch 71 / 100: avg data time: 6.46e-02, avg batch time: 0.5068, average train loss: 0.0198
[09/26 18:37:11 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.1672, average loss: 5.5483
[09/26 18:37:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 91.00	
[09/26 18:37:11 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.02350403678833976
[09/26 18:37:18 visual_prompt]: Epoch 72 / 100: avg data time: 6.40e-02, avg batch time: 0.5081, average train loss: 0.0242
[09/26 18:37:20 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1674, average loss: 5.7147
[09/26 18:37:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 90.50	
[09/26 18:37:20 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.022040354826462667
[09/26 18:37:27 visual_prompt]: Epoch 73 / 100: avg data time: 6.34e-02, avg batch time: 0.5057, average train loss: 0.0212
[09/26 18:37:28 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1674, average loss: 5.7859
[09/26 18:37:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:37:28 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.02061073738537635
[09/26 18:37:35 visual_prompt]: Epoch 74 / 100: avg data time: 5.75e-02, avg batch time: 0.4998, average train loss: 0.0242
[09/26 18:37:37 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1675, average loss: 6.0523
[09/26 18:37:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 89.00	
[09/26 18:37:37 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.019216926233717086
[09/26 18:37:44 visual_prompt]: Epoch 75 / 100: avg data time: 6.24e-02, avg batch time: 0.5055, average train loss: 0.0156
[09/26 18:37:45 visual_prompt]: Inference (val):avg data time: 4.57e-05, avg batch time: 0.1676, average loss: 5.8625
[09/26 18:37:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 90.50	
[09/26 18:37:45 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.017860619515673033
[09/26 18:37:52 visual_prompt]: Epoch 76 / 100: avg data time: 5.26e-02, avg batch time: 0.4944, average train loss: 0.0144
[09/26 18:37:54 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1676, average loss: 5.8559
[09/26 18:37:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 90.50	
[09/26 18:37:54 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.016543469682057107
[09/26 18:38:00 visual_prompt]: Epoch 77 / 100: avg data time: 5.74e-02, avg batch time: 0.5009, average train loss: 0.0131
[09/26 18:38:02 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1673, average loss: 5.9690
[09/26 18:38:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:38:02 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.015267081477050132
[09/26 18:38:09 visual_prompt]: Epoch 78 / 100: avg data time: 7.11e-02, avg batch time: 0.5132, average train loss: 0.0104
[09/26 18:38:11 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1677, average loss: 5.8659
[09/26 18:38:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 90.50	
[09/26 18:38:11 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.014033009983067453
[09/26 18:38:18 visual_prompt]: Epoch 79 / 100: avg data time: 5.39e-02, avg batch time: 0.4960, average train loss: 0.0104
[09/26 18:38:19 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1676, average loss: 5.9326
[09/26 18:38:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 91.00	
[09/26 18:38:19 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.012842758726130284
[09/26 18:38:26 visual_prompt]: Epoch 80 / 100: avg data time: 6.56e-02, avg batch time: 0.5092, average train loss: 0.0064
[09/26 18:38:28 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.1669, average loss: 6.0542
[09/26 18:38:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 90.50	
[09/26 18:38:28 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.011697777844051106
[09/26 18:38:35 visual_prompt]: Epoch 81 / 100: avg data time: 6.64e-02, avg batch time: 0.5089, average train loss: 0.0059
[09/26 18:38:36 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1673, average loss: 6.0488
[09/26 18:38:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 91.00	
[09/26 18:38:36 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.010599462319663905
[09/26 18:38:43 visual_prompt]: Epoch 82 / 100: avg data time: 6.57e-02, avg batch time: 0.5093, average train loss: 0.0093
[09/26 18:38:45 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1671, average loss: 6.0128
[09/26 18:38:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 91.00	
[09/26 18:38:45 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.009549150281252633
[09/26 18:38:52 visual_prompt]: Epoch 83 / 100: avg data time: 5.82e-02, avg batch time: 0.5015, average train loss: 0.0056
[09/26 18:38:54 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1672, average loss: 6.0238
[09/26 18:38:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 91.00	
[09/26 18:38:54 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.008548121372247919
[09/26 18:39:01 visual_prompt]: Epoch 84 / 100: avg data time: 6.63e-02, avg batch time: 0.5092, average train loss: 0.0053
[09/26 18:39:02 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1673, average loss: 6.0696
[09/26 18:39:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 90.00	
[09/26 18:39:02 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.007597595192178702
[09/26 18:39:09 visual_prompt]: Epoch 85 / 100: avg data time: 6.38e-02, avg batch time: 0.5065, average train loss: 0.0061
[09/26 18:39:11 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1671, average loss: 6.1153
[09/26 18:39:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 90.50	
[09/26 18:39:11 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.006698729810778065
[09/26 18:39:18 visual_prompt]: Epoch 86 / 100: avg data time: 6.53e-02, avg batch time: 0.5089, average train loss: 0.0058
[09/26 18:39:19 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1673, average loss: 6.1132
[09/26 18:39:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 90.50	
[09/26 18:39:19 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.005852620357053651
[09/26 18:39:26 visual_prompt]: Epoch 87 / 100: avg data time: 6.20e-02, avg batch time: 0.5058, average train loss: 0.0055
[09/26 18:39:28 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1675, average loss: 6.0657
[09/26 18:39:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:39:28 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00506029768504166
[09/26 18:39:35 visual_prompt]: Epoch 88 / 100: avg data time: 6.33e-02, avg batch time: 0.5080, average train loss: 0.0059
[09/26 18:39:37 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1673, average loss: 6.0686
[09/26 18:39:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:39:37 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.004322727117869951
[09/26 18:39:43 visual_prompt]: Epoch 89 / 100: avg data time: 5.83e-02, avg batch time: 0.5017, average train loss: 0.0077
[09/26 18:39:45 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.1675, average loss: 6.0797
[09/26 18:39:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:39:45 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0036408072716606343
[09/26 18:39:52 visual_prompt]: Epoch 90 / 100: avg data time: 6.76e-02, avg batch time: 0.5120, average train loss: 0.0049
[09/26 18:39:54 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1684, average loss: 6.1015
[09/26 18:39:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:39:54 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0030153689607045845
[09/26 18:40:01 visual_prompt]: Epoch 91 / 100: avg data time: 6.43e-02, avg batch time: 0.5066, average train loss: 0.0083
[09/26 18:40:02 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1670, average loss: 6.1266
[09/26 18:40:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:40:02 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0024471741852423235
[09/26 18:40:09 visual_prompt]: Epoch 92 / 100: avg data time: 5.90e-02, avg batch time: 0.5030, average train loss: 0.0046
[09/26 18:40:11 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1678, average loss: 6.1520
[09/26 18:40:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:40:11 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0019369152030840554
[09/26 18:40:18 visual_prompt]: Epoch 93 / 100: avg data time: 6.67e-02, avg batch time: 0.5102, average train loss: 0.0053
[09/26 18:40:20 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1672, average loss: 6.1510
[09/26 18:40:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 90.50	
[09/26 18:40:20 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0014852136862001765
[09/26 18:40:26 visual_prompt]: Epoch 94 / 100: avg data time: 5.73e-02, avg batch time: 0.5013, average train loss: 0.0061
[09/26 18:40:28 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1671, average loss: 6.1414
[09/26 18:40:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:40:28 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0010926199633097156
[09/26 18:40:35 visual_prompt]: Epoch 95 / 100: avg data time: 6.09e-02, avg batch time: 0.5054, average train loss: 0.0044
[09/26 18:40:37 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1678, average loss: 6.1359
[09/26 18:40:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:40:37 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.0007596123493895991
[09/26 18:40:44 visual_prompt]: Epoch 96 / 100: avg data time: 7.13e-02, avg batch time: 0.5149, average train loss: 0.0044
[09/26 18:40:45 visual_prompt]: Inference (val):avg data time: 4.14e-05, avg batch time: 0.1667, average loss: 6.1383
[09/26 18:40:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:40:45 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00048659656292148194
[09/26 18:40:52 visual_prompt]: Epoch 97 / 100: avg data time: 6.06e-02, avg batch time: 0.5030, average train loss: 0.0073
[09/26 18:40:54 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1668, average loss: 6.1361
[09/26 18:40:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:40:54 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.0002739052315863355
[09/26 18:41:01 visual_prompt]: Epoch 98 / 100: avg data time: 5.08e-02, avg batch time: 0.4941, average train loss: 0.0059
[09/26 18:41:02 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1675, average loss: 6.1362
[09/26 18:41:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:41:02 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.00012179748700879012
[09/26 18:41:09 visual_prompt]: Epoch 99 / 100: avg data time: 5.04e-02, avg batch time: 0.4957, average train loss: 0.0052
[09/26 18:41:11 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1675, average loss: 6.1363
[09/26 18:41:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:41:11 visual_prompt]: Training 100 / 100 epoch, with learning rate 3.0458649045211895e-05
[09/26 18:41:18 visual_prompt]: Epoch 100 / 100: avg data time: 6.00e-02, avg batch time: 0.5026, average train loss: 0.0051
[09/26 18:41:19 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1677, average loss: 6.1363
[09/26 18:41:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:41:19 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 18:41:19 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 18:41:19 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 18:41:19 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 18:41:19 visual_prompt]: Training with config:
[09/26 18:41:19 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 18:41:19 visual_prompt]: Loading training data...
[09/26 18:41:19 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 18:41:21 visual_prompt]: Number of images: 800
[09/26 18:41:21 visual_prompt]: Number of classes: 9 / 9
[09/26 18:41:21 visual_prompt]: Loading validation data...
[09/26 18:41:21 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 18:41:21 visual_prompt]: Number of images: 200
[09/26 18:41:21 visual_prompt]: Number of classes: 9 / 9
[09/26 18:41:21 visual_prompt]: Constructing models...
[09/26 18:41:24 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 18:41:24 visual_prompt]: tuned percent:0.542
[09/26 18:41:24 visual_prompt]: Device used for model: 0
[09/26 18:41:24 visual_prompt]: Setting up Evaluator...
[09/26 18:41:24 visual_prompt]: Setting up Trainer...
[09/26 18:41:24 visual_prompt]: 	Setting up the optimizer...
[09/26 18:41:24 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 18:41:31 visual_prompt]: Epoch 1 / 100: avg data time: 6.16e-02, avg batch time: 0.5054, average train loss: 2.8719
[09/26 18:41:32 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1671, average loss: 2.9516
[09/26 18:41:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 18:41:32 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 18:41:32 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 18:41:39 visual_prompt]: Epoch 2 / 100: avg data time: 6.12e-02, avg batch time: 0.5037, average train loss: 2.4222
[09/26 18:41:41 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1672, average loss: 2.2467
[09/26 18:41:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.50	top5: 58.50	
[09/26 18:41:41 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 18:41:47 visual_prompt]: Epoch 3 / 100: avg data time: 5.18e-02, avg batch time: 0.4944, average train loss: 2.2281
[09/26 18:41:49 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1676, average loss: 2.2146
[09/26 18:41:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/26 18:41:49 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 18:41:56 visual_prompt]: Epoch 4 / 100: avg data time: 6.14e-02, avg batch time: 0.5040, average train loss: 2.2167
[09/26 18:41:58 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1672, average loss: 2.2029
[09/26 18:41:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.00	
[09/26 18:41:58 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 18:42:05 visual_prompt]: Epoch 5 / 100: avg data time: 6.30e-02, avg batch time: 0.5051, average train loss: 2.2066
[09/26 18:42:06 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.1671, average loss: 2.2118
[09/26 18:42:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 58.00	
[09/26 18:42:06 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 18:42:13 visual_prompt]: Epoch 6 / 100: avg data time: 7.30e-02, avg batch time: 0.5149, average train loss: 2.2144
[09/26 18:42:15 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1673, average loss: 2.1998
[09/26 18:42:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 61.50	
[09/26 18:42:15 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 18:42:22 visual_prompt]: Epoch 7 / 100: avg data time: 5.71e-02, avg batch time: 0.4991, average train loss: 2.2201
[09/26 18:42:23 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1669, average loss: 2.2219
[09/26 18:42:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.00	
[09/26 18:42:23 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 18:42:30 visual_prompt]: Epoch 8 / 100: avg data time: 7.10e-02, avg batch time: 0.5130, average train loss: 2.2114
[09/26 18:42:32 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1672, average loss: 2.2207
[09/26 18:42:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 18:42:32 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 18:42:39 visual_prompt]: Epoch 9 / 100: avg data time: 5.84e-02, avg batch time: 0.5014, average train loss: 2.2005
[09/26 18:42:41 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1670, average loss: 2.1843
[09/26 18:42:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 62.00	
[09/26 18:42:41 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 18:42:47 visual_prompt]: Epoch 10 / 100: avg data time: 5.54e-02, avg batch time: 0.4984, average train loss: 2.1788
[09/26 18:42:49 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1671, average loss: 2.1855
[09/26 18:42:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 63.50	
[09/26 18:42:49 visual_prompt]: Best epoch 10: best metric: 0.140
[09/26 18:42:49 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 18:42:56 visual_prompt]: Epoch 11 / 100: avg data time: 5.56e-02, avg batch time: 0.5002, average train loss: 2.1099
[09/26 18:42:58 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1671, average loss: 2.1456
[09/26 18:42:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 68.50	
[09/26 18:42:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 18:43:05 visual_prompt]: Epoch 12 / 100: avg data time: 6.15e-02, avg batch time: 0.5034, average train loss: 2.0796
[09/26 18:43:06 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1670, average loss: 2.6779
[09/26 18:43:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 67.00	
[09/26 18:43:06 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 18:43:13 visual_prompt]: Epoch 13 / 100: avg data time: 6.30e-02, avg batch time: 0.5060, average train loss: 2.1862
[09/26 18:43:15 visual_prompt]: Inference (val):avg data time: 4.60e-05, avg batch time: 0.1671, average loss: 2.2005
[09/26 18:43:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 66.50	
[09/26 18:43:15 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 18:43:22 visual_prompt]: Epoch 14 / 100: avg data time: 6.56e-02, avg batch time: 0.5091, average train loss: 2.1532
[09/26 18:43:23 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1670, average loss: 2.1671
[09/26 18:43:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 66.50	
[09/26 18:43:23 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 18:43:30 visual_prompt]: Epoch 15 / 100: avg data time: 6.74e-02, avg batch time: 0.5100, average train loss: 2.0604
[09/26 18:43:32 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1669, average loss: 2.1163
[09/26 18:43:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 77.00	
[09/26 18:43:32 visual_prompt]: Best epoch 15: best metric: 0.155
[09/26 18:43:32 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 18:43:39 visual_prompt]: Epoch 16 / 100: avg data time: 6.54e-02, avg batch time: 0.5080, average train loss: 1.9840
[09/26 18:43:41 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1669, average loss: 1.9973
[09/26 18:43:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 80.50	
[09/26 18:43:41 visual_prompt]: Best epoch 16: best metric: 0.195
[09/26 18:43:41 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 18:43:48 visual_prompt]: Epoch 17 / 100: avg data time: 6.25e-02, avg batch time: 0.5049, average train loss: 2.0395
[09/26 18:43:49 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1674, average loss: 2.0668
[09/26 18:43:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 74.50	
[09/26 18:43:49 visual_prompt]: Best epoch 17: best metric: 0.215
[09/26 18:43:49 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 18:43:56 visual_prompt]: Epoch 18 / 100: avg data time: 5.83e-02, avg batch time: 0.5009, average train loss: 1.8896
[09/26 18:43:58 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1673, average loss: 2.1186
[09/26 18:43:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 73.50	
[09/26 18:43:58 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 18:44:05 visual_prompt]: Epoch 19 / 100: avg data time: 6.53e-02, avg batch time: 0.5084, average train loss: 1.7859
[09/26 18:44:06 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1675, average loss: 1.9909
[09/26 18:44:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 79.00	
[09/26 18:44:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 18:44:13 visual_prompt]: Epoch 20 / 100: avg data time: 4.79e-02, avg batch time: 0.4926, average train loss: 1.7088
[09/26 18:44:15 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1670, average loss: 2.9314
[09/26 18:44:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 60.50	
[09/26 18:44:15 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 18:44:22 visual_prompt]: Epoch 21 / 100: avg data time: 6.32e-02, avg batch time: 0.5069, average train loss: 2.0687
[09/26 18:44:24 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1671, average loss: 1.9873
[09/26 18:44:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 79.00	
[09/26 18:44:24 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 18:44:30 visual_prompt]: Epoch 22 / 100: avg data time: 4.98e-02, avg batch time: 0.4936, average train loss: 1.8369
[09/26 18:44:32 visual_prompt]: Inference (val):avg data time: 5.52e-05, avg batch time: 0.1672, average loss: 1.8387
[09/26 18:44:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 87.00	
[09/26 18:44:32 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 18:44:39 visual_prompt]: Epoch 23 / 100: avg data time: 5.84e-02, avg batch time: 0.5026, average train loss: 1.7766
[09/26 18:44:41 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1670, average loss: 1.8307
[09/26 18:44:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 91.50	
[09/26 18:44:41 visual_prompt]: Best epoch 23: best metric: 0.250
[09/26 18:44:41 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 18:44:47 visual_prompt]: Epoch 24 / 100: avg data time: 5.88e-02, avg batch time: 0.5018, average train loss: 1.7129
[09/26 18:44:49 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1671, average loss: 1.7841
[09/26 18:44:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 88.50	
[09/26 18:44:49 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 18:44:56 visual_prompt]: Epoch 25 / 100: avg data time: 5.69e-02, avg batch time: 0.5008, average train loss: 1.6913
[09/26 18:44:58 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1668, average loss: 2.1248
[09/26 18:44:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 77.00	
[09/26 18:44:58 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 18:45:05 visual_prompt]: Epoch 26 / 100: avg data time: 6.37e-02, avg batch time: 0.5070, average train loss: 1.6581
[09/26 18:45:06 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1674, average loss: 1.7666
[09/26 18:45:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 91.50	
[09/26 18:45:06 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 18:45:13 visual_prompt]: Epoch 27 / 100: avg data time: 6.34e-02, avg batch time: 0.5066, average train loss: 1.5904
[09/26 18:45:15 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1669, average loss: 2.0822
[09/26 18:45:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 80.00	
[09/26 18:45:15 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 18:45:22 visual_prompt]: Epoch 28 / 100: avg data time: 6.48e-02, avg batch time: 0.5075, average train loss: 1.6078
[09/26 18:45:23 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1670, average loss: 1.9193
[09/26 18:45:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 86.00	
[09/26 18:45:23 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 18:45:30 visual_prompt]: Epoch 29 / 100: avg data time: 5.98e-02, avg batch time: 0.5035, average train loss: 1.5358
[09/26 18:45:32 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1669, average loss: 2.0666
[09/26 18:45:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 83.00	
[09/26 18:45:32 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 18:45:39 visual_prompt]: Epoch 30 / 100: avg data time: 6.18e-02, avg batch time: 0.5048, average train loss: 1.5327
[09/26 18:45:41 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1672, average loss: 1.8940
[09/26 18:45:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 86.50	
[09/26 18:45:41 visual_prompt]: Best epoch 30: best metric: 0.270
[09/26 18:45:41 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 18:45:48 visual_prompt]: Epoch 31 / 100: avg data time: 6.68e-02, avg batch time: 0.5085, average train loss: 1.5977
[09/26 18:45:49 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1669, average loss: 1.9429
[09/26 18:45:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 86.50	
[09/26 18:45:49 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 18:45:56 visual_prompt]: Epoch 32 / 100: avg data time: 5.94e-02, avg batch time: 0.5042, average train loss: 1.5717
[09/26 18:45:58 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1669, average loss: 1.7428
[09/26 18:45:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.50	
[09/26 18:45:58 visual_prompt]: Best epoch 32: best metric: 0.275
[09/26 18:45:58 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 18:46:05 visual_prompt]: Epoch 33 / 100: avg data time: 6.05e-02, avg batch time: 0.5033, average train loss: 1.4069
[09/26 18:46:06 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1668, average loss: 1.7760
[09/26 18:46:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 90.50	
[09/26 18:46:06 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 18:46:13 visual_prompt]: Epoch 34 / 100: avg data time: 5.30e-02, avg batch time: 0.4971, average train loss: 1.3657
[09/26 18:46:15 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1669, average loss: 2.0852
[09/26 18:46:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 85.50	
[09/26 18:46:15 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 18:46:22 visual_prompt]: Epoch 35 / 100: avg data time: 6.17e-02, avg batch time: 0.5051, average train loss: 1.4791
[09/26 18:46:23 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1668, average loss: 1.9065
[09/26 18:46:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 87.50	
[09/26 18:46:23 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 18:46:30 visual_prompt]: Epoch 36 / 100: avg data time: 6.04e-02, avg batch time: 0.5025, average train loss: 1.5390
[09/26 18:46:32 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1668, average loss: 1.9665
[09/26 18:46:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 88.00	
[09/26 18:46:32 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 18:46:39 visual_prompt]: Epoch 37 / 100: avg data time: 6.55e-02, avg batch time: 0.5101, average train loss: 1.5302
[09/26 18:46:41 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1665, average loss: 1.7174
[09/26 18:46:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 88.50	
[09/26 18:46:41 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 18:46:48 visual_prompt]: Epoch 38 / 100: avg data time: 6.19e-02, avg batch time: 0.5033, average train loss: 1.3845
[09/26 18:46:49 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1666, average loss: 2.1036
[09/26 18:46:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 82.50	
[09/26 18:46:49 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 18:46:56 visual_prompt]: Epoch 39 / 100: avg data time: 6.26e-02, avg batch time: 0.5055, average train loss: 1.5273
[09/26 18:46:58 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1673, average loss: 1.6359
[09/26 18:46:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 96.50	
[09/26 18:46:58 visual_prompt]: Best epoch 39: best metric: 0.300
[09/26 18:46:58 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 18:47:05 visual_prompt]: Epoch 40 / 100: avg data time: 6.32e-02, avg batch time: 0.5066, average train loss: 1.4309
[09/26 18:47:06 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1673, average loss: 1.7930
[09/26 18:47:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 89.50	
[09/26 18:47:06 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 18:47:13 visual_prompt]: Epoch 41 / 100: avg data time: 6.12e-02, avg batch time: 0.5043, average train loss: 1.3106
[09/26 18:47:15 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1676, average loss: 1.8928
[09/26 18:47:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 88.50	
[09/26 18:47:15 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 18:47:22 visual_prompt]: Epoch 42 / 100: avg data time: 6.82e-02, avg batch time: 0.5118, average train loss: 1.2600
[09/26 18:47:24 visual_prompt]: Inference (val):avg data time: 4.19e-05, avg batch time: 0.1672, average loss: 1.8131
[09/26 18:47:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 91.50	
[09/26 18:47:24 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 18:47:31 visual_prompt]: Epoch 43 / 100: avg data time: 6.54e-02, avg batch time: 0.5101, average train loss: 1.3146
[09/26 18:47:32 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1673, average loss: 2.1317
[09/26 18:47:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 82.00	
[09/26 18:47:32 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 18:47:39 visual_prompt]: Epoch 44 / 100: avg data time: 6.28e-02, avg batch time: 0.5056, average train loss: 1.3852
[09/26 18:47:41 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1670, average loss: 1.7943
[09/26 18:47:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 89.50	
[09/26 18:47:41 visual_prompt]: Best epoch 44: best metric: 0.345
[09/26 18:47:41 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 18:47:48 visual_prompt]: Epoch 45 / 100: avg data time: 6.43e-02, avg batch time: 0.5075, average train loss: 1.2217
[09/26 18:47:49 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1672, average loss: 1.8178
[09/26 18:47:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 92.50	
[09/26 18:47:49 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 18:47:56 visual_prompt]: Epoch 46 / 100: avg data time: 5.79e-02, avg batch time: 0.5018, average train loss: 1.0897
[09/26 18:47:58 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1670, average loss: 2.3429
[09/26 18:47:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 84.00	
[09/26 18:47:58 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 18:48:05 visual_prompt]: Epoch 47 / 100: avg data time: 5.38e-02, avg batch time: 0.4975, average train loss: 1.1667
[09/26 18:48:06 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1672, average loss: 1.8632
[09/26 18:48:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 92.50	
[09/26 18:48:06 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 18:48:13 visual_prompt]: Epoch 48 / 100: avg data time: 6.09e-02, avg batch time: 0.5054, average train loss: 1.1182
[09/26 18:48:15 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1671, average loss: 2.0436
[09/26 18:48:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 91.50	
[09/26 18:48:15 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 18:48:22 visual_prompt]: Epoch 49 / 100: avg data time: 6.72e-02, avg batch time: 0.5097, average train loss: 1.0322
[09/26 18:48:24 visual_prompt]: Inference (val):avg data time: 4.64e-05, avg batch time: 0.1673, average loss: 1.9395
[09/26 18:48:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 93.00	
[09/26 18:48:24 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 18:48:30 visual_prompt]: Epoch 50 / 100: avg data time: 6.38e-02, avg batch time: 0.5067, average train loss: 1.0755
[09/26 18:48:32 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1677, average loss: 2.0854
[09/26 18:48:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 91.00	
[09/26 18:48:32 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 18:48:39 visual_prompt]: Epoch 51 / 100: avg data time: 6.35e-02, avg batch time: 0.5063, average train loss: 1.1289
[09/26 18:48:41 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1673, average loss: 1.8153
[09/26 18:48:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 92.50	
[09/26 18:48:41 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 18:48:48 visual_prompt]: Epoch 52 / 100: avg data time: 6.36e-02, avg batch time: 0.5073, average train loss: 1.0132
[09/26 18:48:49 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1670, average loss: 2.0417
[09/26 18:48:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 92.50	
[09/26 18:48:49 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 18:48:56 visual_prompt]: Epoch 53 / 100: avg data time: 5.99e-02, avg batch time: 0.5030, average train loss: 0.8675
[09/26 18:48:58 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1673, average loss: 2.0119
[09/26 18:48:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 93.00	
[09/26 18:48:58 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 18:49:05 visual_prompt]: Epoch 54 / 100: avg data time: 5.81e-02, avg batch time: 0.5031, average train loss: 0.9532
[09/26 18:49:06 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1673, average loss: 2.6226
[09/26 18:49:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 82.50	
[09/26 18:49:06 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 18:49:13 visual_prompt]: Epoch 55 / 100: avg data time: 5.28e-02, avg batch time: 0.4961, average train loss: 0.9152
[09/26 18:49:15 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1677, average loss: 2.1138
[09/26 18:49:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 91.00	
[09/26 18:49:15 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 18:49:22 visual_prompt]: Epoch 56 / 100: avg data time: 6.24e-02, avg batch time: 0.5069, average train loss: 0.8095
[09/26 18:49:23 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1671, average loss: 1.9852
[09/26 18:49:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 94.50	
[09/26 18:49:23 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 18:49:30 visual_prompt]: Epoch 57 / 100: avg data time: 5.31e-02, avg batch time: 0.4967, average train loss: 0.7598
[09/26 18:49:32 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1674, average loss: 2.3910
[09/26 18:49:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 89.50	
[09/26 18:49:32 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 18:49:39 visual_prompt]: Epoch 58 / 100: avg data time: 5.06e-02, avg batch time: 0.4972, average train loss: 0.7944
[09/26 18:49:40 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1672, average loss: 2.4665
[09/26 18:49:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 91.50	
[09/26 18:49:40 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 18:49:47 visual_prompt]: Epoch 59 / 100: avg data time: 6.27e-02, avg batch time: 0.5054, average train loss: 0.7393
[09/26 18:49:49 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1672, average loss: 2.0475
[09/26 18:49:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 94.00	
[09/26 18:49:49 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 18:49:56 visual_prompt]: Epoch 60 / 100: avg data time: 6.31e-02, avg batch time: 0.5067, average train loss: 0.7291
[09/26 18:49:57 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1674, average loss: 2.2791
[09/26 18:49:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.50	
[09/26 18:49:57 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 18:50:04 visual_prompt]: Epoch 61 / 100: avg data time: 7.29e-02, avg batch time: 0.5160, average train loss: 0.7189
[09/26 18:50:06 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1669, average loss: 2.0914
[09/26 18:50:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.00	
[09/26 18:50:06 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 18:50:13 visual_prompt]: Epoch 62 / 100: avg data time: 6.08e-02, avg batch time: 0.5039, average train loss: 0.6311
[09/26 18:50:15 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1669, average loss: 2.0883
[09/26 18:50:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 94.00	
[09/26 18:50:15 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 18:50:22 visual_prompt]: Epoch 63 / 100: avg data time: 6.13e-02, avg batch time: 0.5044, average train loss: 0.6353
[09/26 18:50:23 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1671, average loss: 2.3562
[09/26 18:50:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 94.00	
[09/26 18:50:23 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 18:50:30 visual_prompt]: Epoch 64 / 100: avg data time: 5.72e-02, avg batch time: 0.5010, average train loss: 0.5340
[09/26 18:50:32 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1671, average loss: 2.3366
[09/26 18:50:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 91.50	
[09/26 18:50:32 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 18:50:39 visual_prompt]: Epoch 65 / 100: avg data time: 5.72e-02, avg batch time: 0.4999, average train loss: 0.4956
[09/26 18:50:40 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1672, average loss: 2.3833
[09/26 18:50:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 92.50	
[09/26 18:50:40 visual_prompt]: Best epoch 65: best metric: 0.350
[09/26 18:50:40 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 18:50:47 visual_prompt]: Epoch 66 / 100: avg data time: 6.17e-02, avg batch time: 0.5052, average train loss: 0.4832
[09/26 18:50:49 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1666, average loss: 2.4656
[09/26 18:50:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.00	
[09/26 18:50:49 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 18:50:56 visual_prompt]: Epoch 67 / 100: avg data time: 6.03e-02, avg batch time: 0.5023, average train loss: 0.4517
[09/26 18:50:57 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1670, average loss: 2.5155
[09/26 18:50:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 91.00	
[09/26 18:50:57 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 18:51:04 visual_prompt]: Epoch 68 / 100: avg data time: 5.96e-02, avg batch time: 0.5028, average train loss: 0.5137
[09/26 18:51:06 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1668, average loss: 2.3881
[09/26 18:51:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 92.00	
[09/26 18:51:06 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 18:51:13 visual_prompt]: Epoch 69 / 100: avg data time: 6.32e-02, avg batch time: 0.5057, average train loss: 0.4330
[09/26 18:51:14 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1676, average loss: 2.2717
[09/26 18:51:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 95.00	
[09/26 18:51:14 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 18:51:21 visual_prompt]: Epoch 70 / 100: avg data time: 6.06e-02, avg batch time: 0.5052, average train loss: 0.3366
[09/26 18:51:23 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1678, average loss: 2.3608
[09/26 18:51:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 94.00	
[09/26 18:51:23 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 18:51:30 visual_prompt]: Epoch 71 / 100: avg data time: 5.96e-02, avg batch time: 0.5026, average train loss: 0.2930
[09/26 18:51:32 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1676, average loss: 2.6039
[09/26 18:51:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 92.50	
[09/26 18:51:32 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 18:51:38 visual_prompt]: Epoch 72 / 100: avg data time: 5.17e-02, avg batch time: 0.4955, average train loss: 0.2680
[09/26 18:51:40 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1674, average loss: 2.5453
[09/26 18:51:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 93.00	
[09/26 18:51:40 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 18:51:47 visual_prompt]: Epoch 73 / 100: avg data time: 6.67e-02, avg batch time: 0.5094, average train loss: 0.3914
[09/26 18:51:49 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1674, average loss: 2.9831
[09/26 18:51:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 91.00	
[09/26 18:51:49 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 18:51:56 visual_prompt]: Epoch 74 / 100: avg data time: 6.11e-02, avg batch time: 0.5044, average train loss: 0.3822
[09/26 18:51:57 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1673, average loss: 2.6448
[09/26 18:51:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 92.50	
[09/26 18:51:57 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 18:52:04 visual_prompt]: Epoch 75 / 100: avg data time: 6.33e-02, avg batch time: 0.5056, average train loss: 0.2788
[09/26 18:52:06 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1672, average loss: 2.6797
[09/26 18:52:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 93.00	
[09/26 18:52:06 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 18:52:13 visual_prompt]: Epoch 76 / 100: avg data time: 5.79e-02, avg batch time: 0.5010, average train loss: 0.2209
[09/26 18:52:14 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1672, average loss: 2.8454
[09/26 18:52:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 91.50	
[09/26 18:52:14 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 18:52:21 visual_prompt]: Epoch 77 / 100: avg data time: 6.06e-02, avg batch time: 0.5058, average train loss: 0.1782
[09/26 18:52:23 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1676, average loss: 2.7645
[09/26 18:52:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 92.00	
[09/26 18:52:23 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 18:52:30 visual_prompt]: Epoch 78 / 100: avg data time: 6.35e-02, avg batch time: 0.5076, average train loss: 0.1313
[09/26 18:52:32 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1674, average loss: 2.8111
[09/26 18:52:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 92.00	
[09/26 18:52:32 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 18:52:39 visual_prompt]: Epoch 79 / 100: avg data time: 7.02e-02, avg batch time: 0.5129, average train loss: 0.1333
[09/26 18:52:40 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1675, average loss: 2.8487
[09/26 18:52:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 91.50	
[09/26 18:52:40 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 18:52:47 visual_prompt]: Epoch 80 / 100: avg data time: 5.35e-02, avg batch time: 0.4965, average train loss: 0.1085
[09/26 18:52:49 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1672, average loss: 2.9316
[09/26 18:52:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 90.00	
[09/26 18:52:49 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 18:52:56 visual_prompt]: Epoch 81 / 100: avg data time: 6.34e-02, avg batch time: 0.5054, average train loss: 0.1023
[09/26 18:52:57 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1672, average loss: 3.0048
[09/26 18:52:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 90.50	
[09/26 18:52:57 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 18:53:04 visual_prompt]: Epoch 82 / 100: avg data time: 5.97e-02, avg batch time: 0.5038, average train loss: 0.0804
[09/26 18:53:06 visual_prompt]: Inference (val):avg data time: 4.70e-05, avg batch time: 0.1674, average loss: 2.9564
[09/26 18:53:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 90.50	
[09/26 18:53:06 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 18:53:13 visual_prompt]: Epoch 83 / 100: avg data time: 5.98e-02, avg batch time: 0.5025, average train loss: 0.0764
[09/26 18:53:15 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.1671, average loss: 2.9992
[09/26 18:53:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 90.50	
[09/26 18:53:15 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 18:53:21 visual_prompt]: Epoch 84 / 100: avg data time: 5.82e-02, avg batch time: 0.5014, average train loss: 0.0539
[09/26 18:53:23 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1677, average loss: 2.9906
[09/26 18:53:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 90.50	
[09/26 18:53:23 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 18:53:30 visual_prompt]: Epoch 85 / 100: avg data time: 5.88e-02, avg batch time: 0.5025, average train loss: 0.0551
[09/26 18:53:32 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1674, average loss: 3.0087
[09/26 18:53:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 90.00	
[09/26 18:53:32 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 18:53:39 visual_prompt]: Epoch 86 / 100: avg data time: 6.21e-02, avg batch time: 0.5046, average train loss: 0.0520
[09/26 18:53:40 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1677, average loss: 3.0180
[09/26 18:53:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:53:40 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 18:53:47 visual_prompt]: Epoch 87 / 100: avg data time: 6.68e-02, avg batch time: 0.5095, average train loss: 0.0456
[09/26 18:53:49 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1673, average loss: 3.0043
[09/26 18:53:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 90.50	
[09/26 18:53:49 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 18:53:56 visual_prompt]: Epoch 88 / 100: avg data time: 7.21e-02, avg batch time: 0.5143, average train loss: 0.0486
[09/26 18:53:57 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1674, average loss: 3.0794
[09/26 18:53:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.00	
[09/26 18:53:57 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 18:54:04 visual_prompt]: Epoch 89 / 100: avg data time: 5.43e-02, avg batch time: 0.4980, average train loss: 0.0477
[09/26 18:54:06 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1674, average loss: 3.0320
[09/26 18:54:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 90.50	
[09/26 18:54:06 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 18:54:13 visual_prompt]: Epoch 90 / 100: avg data time: 6.55e-02, avg batch time: 0.5090, average train loss: 0.0433
[09/26 18:54:14 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1677, average loss: 3.0226
[09/26 18:54:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 89.50	
[09/26 18:54:14 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 18:54:21 visual_prompt]: Epoch 91 / 100: avg data time: 6.29e-02, avg batch time: 0.5063, average train loss: 0.0409
[09/26 18:54:23 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1674, average loss: 3.0407
[09/26 18:54:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.50	
[09/26 18:54:23 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 18:54:30 visual_prompt]: Epoch 92 / 100: avg data time: 6.70e-02, avg batch time: 0.5098, average train loss: 0.0385
[09/26 18:54:32 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1670, average loss: 3.0414
[09/26 18:54:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 89.50	
[09/26 18:54:32 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 18:54:39 visual_prompt]: Epoch 93 / 100: avg data time: 6.29e-02, avg batch time: 0.5084, average train loss: 0.0403
[09/26 18:54:40 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1678, average loss: 3.0258
[09/26 18:54:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 90.00	
[09/26 18:54:40 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 18:54:47 visual_prompt]: Epoch 94 / 100: avg data time: 6.61e-02, avg batch time: 0.5105, average train loss: 0.0396
[09/26 18:54:49 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1677, average loss: 3.0191
[09/26 18:54:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 89.50	
[09/26 18:54:49 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 18:54:56 visual_prompt]: Epoch 95 / 100: avg data time: 5.72e-02, avg batch time: 0.5010, average train loss: 0.0390
[09/26 18:54:58 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.1678, average loss: 3.0267
[09/26 18:54:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 89.50	
[09/26 18:54:58 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 18:55:05 visual_prompt]: Epoch 96 / 100: avg data time: 6.81e-02, avg batch time: 0.5124, average train loss: 0.0379
[09/26 18:55:06 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1673, average loss: 3.0276
[09/26 18:55:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 89.50	
[09/26 18:55:06 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 18:55:13 visual_prompt]: Epoch 97 / 100: avg data time: 5.68e-02, avg batch time: 0.5005, average train loss: 0.0381
[09/26 18:55:15 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1675, average loss: 3.0273
[09/26 18:55:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 89.50	
[09/26 18:55:15 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 18:55:22 visual_prompt]: Epoch 98 / 100: avg data time: 6.43e-02, avg batch time: 0.5068, average train loss: 0.0378
[09/26 18:55:23 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1676, average loss: 3.0277
[09/26 18:55:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.00	top5: 89.50	
[09/26 18:55:23 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 18:55:30 visual_prompt]: Epoch 99 / 100: avg data time: 5.77e-02, avg batch time: 0.5015, average train loss: 0.0386
[09/26 18:55:32 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1674, average loss: 3.0285
[09/26 18:55:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.50	
[09/26 18:55:32 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 18:55:39 visual_prompt]: Epoch 100 / 100: avg data time: 5.60e-02, avg batch time: 0.4979, average train loss: 0.0403
[09/26 18:55:40 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1673, average loss: 3.0288
[09/26 18:55:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 89.50	
[09/26 18:55:40 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 18:55:40 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 18:55:40 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 18:55:40 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 18:55:40 visual_prompt]: Training with config:
[09/26 18:55:40 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 18:55:40 visual_prompt]: Loading training data...
[09/26 18:55:40 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 18:55:42 visual_prompt]: Number of images: 800
[09/26 18:55:42 visual_prompt]: Number of classes: 9 / 9
[09/26 18:55:42 visual_prompt]: Loading validation data...
[09/26 18:55:42 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 18:55:42 visual_prompt]: Number of images: 200
[09/26 18:55:42 visual_prompt]: Number of classes: 9 / 9
[09/26 18:55:42 visual_prompt]: Constructing models...
[09/26 18:55:45 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 18:55:45 visual_prompt]: tuned percent:0.542
[09/26 18:55:45 visual_prompt]: Device used for model: 0
[09/26 18:55:45 visual_prompt]: Setting up Evaluator...
[09/26 18:55:45 visual_prompt]: Setting up Trainer...
[09/26 18:55:45 visual_prompt]: 	Setting up the optimizer...
[09/26 18:55:45 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 18:55:52 visual_prompt]: Epoch 1 / 100: avg data time: 7.10e-02, avg batch time: 0.5138, average train loss: 2.8633
[09/26 18:55:53 visual_prompt]: Inference (val):avg data time: 4.48e-05, avg batch time: 0.1673, average loss: 2.9516
[09/26 18:55:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 18:55:53 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 18:55:53 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 18:56:00 visual_prompt]: Epoch 2 / 100: avg data time: 6.19e-02, avg batch time: 0.5053, average train loss: 2.4149
[09/26 18:56:02 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1674, average loss: 2.2297
[09/26 18:56:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 55.00	
[09/26 18:56:02 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 18:56:09 visual_prompt]: Epoch 3 / 100: avg data time: 6.21e-02, avg batch time: 0.5043, average train loss: 2.2402
[09/26 18:56:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1675, average loss: 2.2384
[09/26 18:56:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 53.50	
[09/26 18:56:11 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 18:56:17 visual_prompt]: Epoch 4 / 100: avg data time: 4.98e-02, avg batch time: 0.4919, average train loss: 2.2378
[09/26 18:56:19 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1677, average loss: 2.2030
[09/26 18:56:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 60.00	
[09/26 18:56:19 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 18:56:26 visual_prompt]: Epoch 5 / 100: avg data time: 6.42e-02, avg batch time: 0.5064, average train loss: 2.2210
[09/26 18:56:28 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1674, average loss: 2.2093
[09/26 18:56:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.00	top5: 59.50	
[09/26 18:56:28 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 18:56:34 visual_prompt]: Epoch 6 / 100: avg data time: 6.41e-02, avg batch time: 0.5083, average train loss: 2.2106
[09/26 18:56:36 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1674, average loss: 2.1974
[09/26 18:56:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 59.00	
[09/26 18:56:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 18:56:43 visual_prompt]: Epoch 7 / 100: avg data time: 6.32e-02, avg batch time: 0.5067, average train loss: 2.2138
[09/26 18:56:45 visual_prompt]: Inference (val):avg data time: 4.43e-05, avg batch time: 0.1678, average loss: 2.2141
[09/26 18:56:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/26 18:56:45 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 18:56:52 visual_prompt]: Epoch 8 / 100: avg data time: 6.72e-02, avg batch time: 0.5101, average train loss: 2.2144
[09/26 18:56:53 visual_prompt]: Inference (val):avg data time: 4.35e-05, avg batch time: 0.1675, average loss: 2.2156
[09/26 18:56:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 63.50	
[09/26 18:56:53 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 18:57:00 visual_prompt]: Epoch 9 / 100: avg data time: 7.19e-02, avg batch time: 0.5143, average train loss: 2.1874
[09/26 18:57:02 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1673, average loss: 2.1804
[09/26 18:57:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 67.00	
[09/26 18:57:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 18:57:09 visual_prompt]: Epoch 10 / 100: avg data time: 6.67e-02, avg batch time: 0.5084, average train loss: 2.1429
[09/26 18:57:11 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1675, average loss: 2.1361
[09/26 18:57:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 65.50	
[09/26 18:57:11 visual_prompt]: Best epoch 10: best metric: 0.150
[09/26 18:57:11 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 18:57:18 visual_prompt]: Epoch 11 / 100: avg data time: 6.20e-02, avg batch time: 0.5046, average train loss: 2.0990
[09/26 18:57:19 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1675, average loss: 2.0855
[09/26 18:57:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 73.50	
[09/26 18:57:19 visual_prompt]: Best epoch 11: best metric: 0.155
[09/26 18:57:19 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 18:57:26 visual_prompt]: Epoch 12 / 100: avg data time: 6.36e-02, avg batch time: 0.5080, average train loss: 2.0276
[09/26 18:57:28 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1679, average loss: 2.0206
[09/26 18:57:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 76.00	
[09/26 18:57:28 visual_prompt]: Best epoch 12: best metric: 0.180
[09/26 18:57:28 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 18:57:35 visual_prompt]: Epoch 13 / 100: avg data time: 5.48e-02, avg batch time: 0.4978, average train loss: 1.9857
[09/26 18:57:36 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1677, average loss: 2.1754
[09/26 18:57:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 74.00	
[09/26 18:57:36 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 18:57:43 visual_prompt]: Epoch 14 / 100: avg data time: 6.58e-02, avg batch time: 0.5082, average train loss: 1.8855
[09/26 18:57:45 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1675, average loss: 2.1098
[09/26 18:57:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 76.50	
[09/26 18:57:45 visual_prompt]: Best epoch 14: best metric: 0.210
[09/26 18:57:45 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 18:57:52 visual_prompt]: Epoch 15 / 100: avg data time: 6.34e-02, avg batch time: 0.5067, average train loss: 1.9494
[09/26 18:57:53 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1676, average loss: 1.9171
[09/26 18:57:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 85.50	
[09/26 18:57:53 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 18:58:00 visual_prompt]: Epoch 16 / 100: avg data time: 6.16e-02, avg batch time: 0.5043, average train loss: 1.8101
[09/26 18:58:02 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1675, average loss: 1.9380
[09/26 18:58:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 82.00	
[09/26 18:58:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 18:58:09 visual_prompt]: Epoch 17 / 100: avg data time: 6.23e-02, avg batch time: 0.5061, average train loss: 1.7813
[09/26 18:58:10 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1675, average loss: 2.0213
[09/26 18:58:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 81.00	
[09/26 18:58:11 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 18:58:18 visual_prompt]: Epoch 18 / 100: avg data time: 6.67e-02, avg batch time: 0.5111, average train loss: 1.6820
[09/26 18:58:19 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1678, average loss: 1.8941
[09/26 18:58:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 84.00	
[09/26 18:58:19 visual_prompt]: Best epoch 18: best metric: 0.215
[09/26 18:58:19 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 18:58:26 visual_prompt]: Epoch 19 / 100: avg data time: 6.12e-02, avg batch time: 0.5048, average train loss: 1.6604
[09/26 18:58:28 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1676, average loss: 1.8742
[09/26 18:58:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 84.00	
[09/26 18:58:28 visual_prompt]: Best epoch 19: best metric: 0.250
[09/26 18:58:28 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 18:58:35 visual_prompt]: Epoch 20 / 100: avg data time: 6.66e-02, avg batch time: 0.5105, average train loss: 1.6164
[09/26 18:58:36 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1674, average loss: 2.1101
[09/26 18:58:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 82.00	
[09/26 18:58:36 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 18:58:43 visual_prompt]: Epoch 21 / 100: avg data time: 6.22e-02, avg batch time: 0.5053, average train loss: 1.5915
[09/26 18:58:45 visual_prompt]: Inference (val):avg data time: 4.29e-05, avg batch time: 0.1674, average loss: 2.2711
[09/26 18:58:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 81.50	
[09/26 18:58:45 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 18:58:52 visual_prompt]: Epoch 22 / 100: avg data time: 6.11e-02, avg batch time: 0.5053, average train loss: 1.5555
[09/26 18:58:54 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1677, average loss: 2.0012
[09/26 18:58:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 82.50	
[09/26 18:58:54 visual_prompt]: Best epoch 22: best metric: 0.295
[09/26 18:58:54 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 18:59:01 visual_prompt]: Epoch 23 / 100: avg data time: 6.68e-02, avg batch time: 0.5098, average train loss: 1.4223
[09/26 18:59:02 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1677, average loss: 2.0037
[09/26 18:59:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 87.00	
[09/26 18:59:02 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 18:59:09 visual_prompt]: Epoch 24 / 100: avg data time: 6.23e-02, avg batch time: 0.5065, average train loss: 1.3646
[09/26 18:59:11 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1680, average loss: 2.1334
[09/26 18:59:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 84.50	
[09/26 18:59:11 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 18:59:18 visual_prompt]: Epoch 25 / 100: avg data time: 6.28e-02, avg batch time: 0.5059, average train loss: 1.4465
[09/26 18:59:19 visual_prompt]: Inference (val):avg data time: 4.16e-05, avg batch time: 0.1673, average loss: 2.1422
[09/26 18:59:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 86.00	
[09/26 18:59:19 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 18:59:26 visual_prompt]: Epoch 26 / 100: avg data time: 5.96e-02, avg batch time: 0.5035, average train loss: 1.4473
[09/26 18:59:28 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1676, average loss: 2.0710
[09/26 18:59:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 84.00	
[09/26 18:59:28 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 18:59:35 visual_prompt]: Epoch 27 / 100: avg data time: 5.62e-02, avg batch time: 0.5013, average train loss: 1.4098
[09/26 18:59:36 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1677, average loss: 2.1920
[09/26 18:59:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 84.00	
[09/26 18:59:36 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 18:59:43 visual_prompt]: Epoch 28 / 100: avg data time: 5.44e-02, avg batch time: 0.4990, average train loss: 1.3810
[09/26 18:59:45 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1677, average loss: 1.9839
[09/26 18:59:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 89.50	
[09/26 18:59:45 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 18:59:52 visual_prompt]: Epoch 29 / 100: avg data time: 6.40e-02, avg batch time: 0.5069, average train loss: 1.2381
[09/26 18:59:53 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1676, average loss: 2.2764
[09/26 18:59:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 85.00	
[09/26 18:59:53 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 19:00:00 visual_prompt]: Epoch 30 / 100: avg data time: 6.41e-02, avg batch time: 0.5069, average train loss: 1.0865
[09/26 19:00:02 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1676, average loss: 2.2758
[09/26 19:00:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 90.00	
[09/26 19:00:02 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 19:00:09 visual_prompt]: Epoch 31 / 100: avg data time: 6.46e-02, avg batch time: 0.5086, average train loss: 1.1619
[09/26 19:00:11 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1675, average loss: 2.4112
[09/26 19:00:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 87.50	
[09/26 19:00:11 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 19:00:17 visual_prompt]: Epoch 32 / 100: avg data time: 5.10e-02, avg batch time: 0.4961, average train loss: 1.0437
[09/26 19:00:19 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1679, average loss: 2.4819
[09/26 19:00:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 87.00	
[09/26 19:00:19 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 19:00:26 visual_prompt]: Epoch 33 / 100: avg data time: 6.68e-02, avg batch time: 0.5100, average train loss: 1.0109
[09/26 19:00:28 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1677, average loss: 2.3638
[09/26 19:00:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 92.50	
[09/26 19:00:28 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 19:00:35 visual_prompt]: Epoch 34 / 100: avg data time: 6.59e-02, avg batch time: 0.5090, average train loss: 1.0327
[09/26 19:00:36 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1678, average loss: 2.7867
[09/26 19:00:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 83.50	
[09/26 19:00:36 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 19:00:43 visual_prompt]: Epoch 35 / 100: avg data time: 6.41e-02, avg batch time: 0.5061, average train loss: 1.0523
[09/26 19:00:45 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1680, average loss: 2.6781
[09/26 19:00:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 91.00	
[09/26 19:00:45 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 19:00:52 visual_prompt]: Epoch 36 / 100: avg data time: 5.92e-02, avg batch time: 0.5040, average train loss: 0.9715
[09/26 19:00:54 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1674, average loss: 2.3471
[09/26 19:00:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 90.00	
[09/26 19:00:54 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 19:01:01 visual_prompt]: Epoch 37 / 100: avg data time: 6.52e-02, avg batch time: 0.5071, average train loss: 0.8278
[09/26 19:01:02 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1674, average loss: 2.8812
[09/26 19:01:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 90.00	
[09/26 19:01:02 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 19:01:09 visual_prompt]: Epoch 38 / 100: avg data time: 6.29e-02, avg batch time: 0.5066, average train loss: 0.8132
[09/26 19:01:11 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1680, average loss: 2.6266
[09/26 19:01:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 91.50	
[09/26 19:01:11 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 19:01:18 visual_prompt]: Epoch 39 / 100: avg data time: 6.16e-02, avg batch time: 0.5044, average train loss: 0.8180
[09/26 19:01:19 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1677, average loss: 2.7634
[09/26 19:01:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 87.50	
[09/26 19:01:19 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 19:01:26 visual_prompt]: Epoch 40 / 100: avg data time: 6.64e-02, avg batch time: 0.5087, average train loss: 0.8550
[09/26 19:01:28 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1677, average loss: 2.8536
[09/26 19:01:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 89.00	
[09/26 19:01:28 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 19:01:35 visual_prompt]: Epoch 41 / 100: avg data time: 6.47e-02, avg batch time: 0.5075, average train loss: 0.7214
[09/26 19:01:36 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1673, average loss: 3.1661
[09/26 19:01:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 88.00	
[09/26 19:01:36 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 19:01:43 visual_prompt]: Epoch 42 / 100: avg data time: 6.41e-02, avg batch time: 0.5060, average train loss: 0.5549
[09/26 19:01:45 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1674, average loss: 2.9016
[09/26 19:01:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 92.00	
[09/26 19:01:45 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 19:01:52 visual_prompt]: Epoch 43 / 100: avg data time: 6.81e-02, avg batch time: 0.5111, average train loss: 0.4996
[09/26 19:01:54 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1672, average loss: 3.2405
[09/26 19:01:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 91.50	
[09/26 19:01:54 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 19:02:01 visual_prompt]: Epoch 44 / 100: avg data time: 5.67e-02, avg batch time: 0.4985, average train loss: 0.5260
[09/26 19:02:02 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1677, average loss: 3.4164
[09/26 19:02:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.00	
[09/26 19:02:02 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 19:02:09 visual_prompt]: Epoch 45 / 100: avg data time: 6.43e-02, avg batch time: 0.5070, average train loss: 0.5741
[09/26 19:02:11 visual_prompt]: Inference (val):avg data time: 4.22e-05, avg batch time: 0.1673, average loss: 3.2994
[09/26 19:02:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 91.50	
[09/26 19:02:11 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 19:02:18 visual_prompt]: Epoch 46 / 100: avg data time: 6.08e-02, avg batch time: 0.5046, average train loss: 0.4843
[09/26 19:02:19 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1675, average loss: 3.6865
[09/26 19:02:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.00	
[09/26 19:02:19 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 19:02:26 visual_prompt]: Epoch 47 / 100: avg data time: 6.19e-02, avg batch time: 0.5049, average train loss: 0.4208
[09/26 19:02:28 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1673, average loss: 4.1408
[09/26 19:02:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 87.50	
[09/26 19:02:28 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 19:02:35 visual_prompt]: Epoch 48 / 100: avg data time: 6.15e-02, avg batch time: 0.5055, average train loss: 0.3768
[09/26 19:02:37 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1677, average loss: 3.7556
[09/26 19:02:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 19:02:37 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 19:02:44 visual_prompt]: Epoch 49 / 100: avg data time: 6.26e-02, avg batch time: 0.5053, average train loss: 0.2864
[09/26 19:02:45 visual_prompt]: Inference (val):avg data time: 2.16e-05, avg batch time: 0.1671, average loss: 3.8644
[09/26 19:02:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 90.50	
[09/26 19:02:45 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 19:02:52 visual_prompt]: Epoch 50 / 100: avg data time: 5.34e-02, avg batch time: 0.4971, average train loss: 0.3089
[09/26 19:02:54 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1673, average loss: 4.3526
[09/26 19:02:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 87.50	
[09/26 19:02:54 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 19:03:01 visual_prompt]: Epoch 51 / 100: avg data time: 6.33e-02, avg batch time: 0.5071, average train loss: 0.2997
[09/26 19:03:02 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1679, average loss: 4.1422
[09/26 19:03:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.50	
[09/26 19:03:02 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 19:03:09 visual_prompt]: Epoch 52 / 100: avg data time: 6.21e-02, avg batch time: 0.5050, average train loss: 0.2358
[09/26 19:03:11 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1675, average loss: 4.1130
[09/26 19:03:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 91.00	
[09/26 19:03:11 visual_prompt]: Best epoch 52: best metric: 0.320
[09/26 19:03:11 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 19:03:18 visual_prompt]: Epoch 53 / 100: avg data time: 6.43e-02, avg batch time: 0.5079, average train loss: 0.2719
[09/26 19:03:20 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1676, average loss: 4.0133
[09/26 19:03:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 89.00	
[09/26 19:03:20 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 19:03:27 visual_prompt]: Epoch 54 / 100: avg data time: 6.62e-02, avg batch time: 0.5108, average train loss: 0.2083
[09/26 19:03:28 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1676, average loss: 4.2110
[09/26 19:03:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 90.00	
[09/26 19:03:28 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 19:03:35 visual_prompt]: Epoch 55 / 100: avg data time: 5.89e-02, avg batch time: 0.5023, average train loss: 0.1830
[09/26 19:03:37 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1677, average loss: 4.3426
[09/26 19:03:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 89.00	
[09/26 19:03:37 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 19:03:44 visual_prompt]: Epoch 56 / 100: avg data time: 6.17e-02, avg batch time: 0.5042, average train loss: 0.2582
[09/26 19:03:45 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1677, average loss: 4.5325
[09/26 19:03:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 90.00	
[09/26 19:03:45 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 19:03:52 visual_prompt]: Epoch 57 / 100: avg data time: 6.45e-02, avg batch time: 0.5089, average train loss: 0.2403
[09/26 19:03:54 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1674, average loss: 4.7302
[09/26 19:03:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 90.00	
[09/26 19:03:54 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 19:04:01 visual_prompt]: Epoch 58 / 100: avg data time: 6.36e-02, avg batch time: 0.5076, average train loss: 0.1919
[09/26 19:04:02 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1672, average loss: 4.4068
[09/26 19:04:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 87.00	
[09/26 19:04:02 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 19:04:09 visual_prompt]: Epoch 59 / 100: avg data time: 6.06e-02, avg batch time: 0.5044, average train loss: 0.1534
[09/26 19:04:11 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1674, average loss: 4.7353
[09/26 19:04:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 87.50	
[09/26 19:04:11 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 19:04:18 visual_prompt]: Epoch 60 / 100: avg data time: 5.30e-02, avg batch time: 0.4967, average train loss: 0.1348
[09/26 19:04:20 visual_prompt]: Inference (val):avg data time: 3.20e-05, avg batch time: 0.1673, average loss: 4.6001
[09/26 19:04:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 91.50	
[09/26 19:04:20 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 19:04:27 visual_prompt]: Epoch 61 / 100: avg data time: 6.08e-02, avg batch time: 0.5048, average train loss: 0.0851
[09/26 19:04:28 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1672, average loss: 4.5774
[09/26 19:04:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 19:04:28 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 19:04:35 visual_prompt]: Epoch 62 / 100: avg data time: 6.26e-02, avg batch time: 0.5056, average train loss: 0.0676
[09/26 19:04:37 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1676, average loss: 4.5937
[09/26 19:04:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 89.00	
[09/26 19:04:37 visual_prompt]: Best epoch 62: best metric: 0.330
[09/26 19:04:37 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 19:04:44 visual_prompt]: Epoch 63 / 100: avg data time: 6.51e-02, avg batch time: 0.5075, average train loss: 0.0843
[09/26 19:04:45 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1669, average loss: 4.7144
[09/26 19:04:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 90.50	
[09/26 19:04:45 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 19:04:52 visual_prompt]: Epoch 64 / 100: avg data time: 6.14e-02, avg batch time: 0.5059, average train loss: 0.0647
[09/26 19:04:54 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1682, average loss: 4.8088
[09/26 19:04:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 87.50	
[09/26 19:04:54 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 19:05:01 visual_prompt]: Epoch 65 / 100: avg data time: 6.62e-02, avg batch time: 0.5095, average train loss: 0.0517
[09/26 19:05:03 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1680, average loss: 4.8859
[09/26 19:05:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 88.00	
[09/26 19:05:03 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 19:05:09 visual_prompt]: Epoch 66 / 100: avg data time: 5.83e-02, avg batch time: 0.5020, average train loss: 0.0478
[09/26 19:05:11 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1679, average loss: 4.8863
[09/26 19:05:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 89.50	
[09/26 19:05:11 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 19:05:18 visual_prompt]: Epoch 67 / 100: avg data time: 6.21e-02, avg batch time: 0.5055, average train loss: 0.0392
[09/26 19:05:20 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1676, average loss: 4.7331
[09/26 19:05:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 91.00	
[09/26 19:05:20 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 19:05:27 visual_prompt]: Epoch 68 / 100: avg data time: 6.13e-02, avg batch time: 0.5047, average train loss: 0.0651
[09/26 19:05:28 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1678, average loss: 5.0357
[09/26 19:05:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 90.00	
[09/26 19:05:28 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 19:05:35 visual_prompt]: Epoch 69 / 100: avg data time: 6.43e-02, avg batch time: 0.5068, average train loss: 0.0360
[09/26 19:05:37 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1676, average loss: 5.3928
[09/26 19:05:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 85.00	
[09/26 19:05:37 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 19:05:44 visual_prompt]: Epoch 70 / 100: avg data time: 5.75e-02, avg batch time: 0.5014, average train loss: 0.0257
[09/26 19:05:45 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1676, average loss: 4.8716
[09/26 19:05:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.50	top5: 89.00	
[09/26 19:05:45 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 19:05:52 visual_prompt]: Epoch 71 / 100: avg data time: 5.52e-02, avg batch time: 0.4981, average train loss: 0.0225
[09/26 19:05:54 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1677, average loss: 4.9934
[09/26 19:05:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 88.00	
[09/26 19:05:54 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 19:06:01 visual_prompt]: Epoch 72 / 100: avg data time: 6.49e-02, avg batch time: 0.5083, average train loss: 0.0195
[09/26 19:06:02 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1675, average loss: 5.2349
[09/26 19:06:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 86.50	
[09/26 19:06:02 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 19:06:09 visual_prompt]: Epoch 73 / 100: avg data time: 6.47e-02, avg batch time: 0.5078, average train loss: 0.0150
[09/26 19:06:11 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1675, average loss: 5.3229
[09/26 19:06:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.50	
[09/26 19:06:11 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 19:06:18 visual_prompt]: Epoch 74 / 100: avg data time: 5.04e-02, avg batch time: 0.4941, average train loss: 0.0157
[09/26 19:06:19 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1673, average loss: 4.9849
[09/26 19:06:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.50	
[09/26 19:06:19 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 19:06:26 visual_prompt]: Epoch 75 / 100: avg data time: 6.64e-02, avg batch time: 0.5090, average train loss: 0.0191
[09/26 19:06:28 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1672, average loss: 5.5374
[09/26 19:06:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 87.00	
[09/26 19:06:28 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 19:06:35 visual_prompt]: Epoch 76 / 100: avg data time: 6.58e-02, avg batch time: 0.5081, average train loss: 0.0233
[09/26 19:06:37 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1672, average loss: 4.9572
[09/26 19:06:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 91.00	
[09/26 19:06:37 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 19:06:44 visual_prompt]: Epoch 77 / 100: avg data time: 6.17e-02, avg batch time: 0.5044, average train loss: 0.0131
[09/26 19:06:45 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1673, average loss: 5.0936
[09/26 19:06:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 89.00	
[09/26 19:06:45 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 19:06:52 visual_prompt]: Epoch 78 / 100: avg data time: 5.17e-02, avg batch time: 0.4946, average train loss: 0.0145
[09/26 19:06:54 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1673, average loss: 5.1636
[09/26 19:06:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 89.00	
[09/26 19:06:54 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 19:07:01 visual_prompt]: Epoch 79 / 100: avg data time: 5.77e-02, avg batch time: 0.5014, average train loss: 0.0145
[09/26 19:07:02 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1674, average loss: 5.1551
[09/26 19:07:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 91.00	
[09/26 19:07:02 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 19:07:09 visual_prompt]: Epoch 80 / 100: avg data time: 6.19e-02, avg batch time: 0.5062, average train loss: 0.0102
[09/26 19:07:11 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.1673, average loss: 5.1774
[09/26 19:07:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 90.50	
[09/26 19:07:11 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 19:07:18 visual_prompt]: Epoch 81 / 100: avg data time: 6.10e-02, avg batch time: 0.5047, average train loss: 0.0117
[09/26 19:07:19 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1676, average loss: 5.0842
[09/26 19:07:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 90.50	
[09/26 19:07:19 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 19:07:26 visual_prompt]: Epoch 82 / 100: avg data time: 5.38e-02, avg batch time: 0.4976, average train loss: 0.0095
[09/26 19:07:28 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1677, average loss: 5.0317
[09/26 19:07:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 90.00	
[09/26 19:07:28 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 19:07:35 visual_prompt]: Epoch 83 / 100: avg data time: 6.80e-02, avg batch time: 0.5113, average train loss: 0.0113
[09/26 19:07:36 visual_prompt]: Inference (val):avg data time: 4.39e-05, avg batch time: 0.1678, average loss: 5.0748
[09/26 19:07:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.00	
[09/26 19:07:36 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 19:07:43 visual_prompt]: Epoch 84 / 100: avg data time: 6.15e-02, avg batch time: 0.5038, average train loss: 0.0100
[09/26 19:07:45 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1675, average loss: 5.2256
[09/26 19:07:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 90.00	
[09/26 19:07:45 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 19:07:52 visual_prompt]: Epoch 85 / 100: avg data time: 6.22e-02, avg batch time: 0.5068, average train loss: 0.0073
[09/26 19:07:54 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1673, average loss: 5.2466
[09/26 19:07:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 90.00	
[09/26 19:07:54 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 19:08:01 visual_prompt]: Epoch 86 / 100: avg data time: 6.36e-02, avg batch time: 0.5071, average train loss: 0.0090
[09/26 19:08:02 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1678, average loss: 5.2191
[09/26 19:08:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.00	
[09/26 19:08:02 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 19:08:09 visual_prompt]: Epoch 87 / 100: avg data time: 5.50e-02, avg batch time: 0.5013, average train loss: 0.0079
[09/26 19:08:11 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1679, average loss: 5.1869
[09/26 19:08:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:08:11 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 19:08:18 visual_prompt]: Epoch 88 / 100: avg data time: 5.39e-02, avg batch time: 0.4990, average train loss: 0.0069
[09/26 19:08:19 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1680, average loss: 5.1821
[09/26 19:08:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:08:19 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 19:08:26 visual_prompt]: Epoch 89 / 100: avg data time: 6.56e-02, avg batch time: 0.5085, average train loss: 0.0066
[09/26 19:08:28 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1677, average loss: 5.1793
[09/26 19:08:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:08:28 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 19:08:35 visual_prompt]: Epoch 90 / 100: avg data time: 5.95e-02, avg batch time: 0.5049, average train loss: 0.0053
[09/26 19:08:36 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1674, average loss: 5.1837
[09/26 19:08:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 90.50	
[09/26 19:08:36 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 19:08:43 visual_prompt]: Epoch 91 / 100: avg data time: 5.73e-02, avg batch time: 0.5013, average train loss: 0.0084
[09/26 19:08:45 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1671, average loss: 5.1980
[09/26 19:08:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 90.50	
[09/26 19:08:45 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 19:08:52 visual_prompt]: Epoch 92 / 100: avg data time: 6.56e-02, avg batch time: 0.5094, average train loss: 0.0068
[09/26 19:08:54 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1677, average loss: 5.1928
[09/26 19:08:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:08:54 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 19:09:01 visual_prompt]: Epoch 93 / 100: avg data time: 6.71e-02, avg batch time: 0.5113, average train loss: 0.0085
[09/26 19:09:02 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1676, average loss: 5.1929
[09/26 19:09:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:09:02 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 19:09:09 visual_prompt]: Epoch 94 / 100: avg data time: 6.36e-02, avg batch time: 0.5071, average train loss: 0.0061
[09/26 19:09:11 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1669, average loss: 5.1923
[09/26 19:09:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:09:11 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 19:09:18 visual_prompt]: Epoch 95 / 100: avg data time: 6.20e-02, avg batch time: 0.5052, average train loss: 0.0083
[09/26 19:09:19 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1675, average loss: 5.1869
[09/26 19:09:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:09:19 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 19:09:26 visual_prompt]: Epoch 96 / 100: avg data time: 6.19e-02, avg batch time: 0.5054, average train loss: 0.0074
[09/26 19:09:28 visual_prompt]: Inference (val):avg data time: 4.71e-05, avg batch time: 0.1673, average loss: 5.1850
[09/26 19:09:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:09:28 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 19:09:35 visual_prompt]: Epoch 97 / 100: avg data time: 6.59e-02, avg batch time: 0.5109, average train loss: 0.0063
[09/26 19:09:36 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1678, average loss: 5.1864
[09/26 19:09:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:09:36 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 19:09:43 visual_prompt]: Epoch 98 / 100: avg data time: 6.35e-02, avg batch time: 0.5072, average train loss: 0.0061
[09/26 19:09:45 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1673, average loss: 5.1880
[09/26 19:09:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:09:45 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 19:09:52 visual_prompt]: Epoch 99 / 100: avg data time: 4.96e-02, avg batch time: 0.4959, average train loss: 0.0074
[09/26 19:09:54 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1668, average loss: 5.1884
[09/26 19:09:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:09:54 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 19:10:00 visual_prompt]: Epoch 100 / 100: avg data time: 5.71e-02, avg batch time: 0.4998, average train loss: 0.0074
[09/26 19:10:02 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1670, average loss: 5.1885
[09/26 19:10:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 90.50	
[09/26 19:10:02 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 19:10:02 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 19:10:02 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 19:10:02 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 19:10:02 visual_prompt]: Training with config:
[09/26 19:10:02 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 19:10:02 visual_prompt]: Loading training data...
[09/26 19:10:02 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 19:10:03 visual_prompt]: Number of images: 800
[09/26 19:10:03 visual_prompt]: Number of classes: 9 / 9
[09/26 19:10:03 visual_prompt]: Loading validation data...
[09/26 19:10:03 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 19:10:03 visual_prompt]: Number of images: 200
[09/26 19:10:03 visual_prompt]: Number of classes: 9 / 9
[09/26 19:10:03 visual_prompt]: Constructing models...
[09/26 19:10:06 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 19:10:06 visual_prompt]: tuned percent:0.542
[09/26 19:10:06 visual_prompt]: Device used for model: 0
[09/26 19:10:06 visual_prompt]: Setting up Evaluator...
[09/26 19:10:06 visual_prompt]: Setting up Trainer...
[09/26 19:10:06 visual_prompt]: 	Setting up the optimizer...
[09/26 19:10:06 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 19:10:13 visual_prompt]: Epoch 1 / 100: avg data time: 5.68e-02, avg batch time: 0.5066, average train loss: 2.8744
[09/26 19:10:15 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1665, average loss: 2.9516
[09/26 19:10:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 19:10:15 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 19:10:15 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 19:10:22 visual_prompt]: Epoch 2 / 100: avg data time: 6.22e-02, avg batch time: 0.5044, average train loss: 2.4382
[09/26 19:10:24 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1673, average loss: 2.2323
[09/26 19:10:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/26 19:10:24 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 19:10:30 visual_prompt]: Epoch 3 / 100: avg data time: 6.20e-02, avg batch time: 0.5056, average train loss: 2.2354
[09/26 19:10:32 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1673, average loss: 2.2258
[09/26 19:10:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 53.50	
[09/26 19:10:32 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 19:10:39 visual_prompt]: Epoch 4 / 100: avg data time: 6.04e-02, avg batch time: 0.5034, average train loss: 2.2121
[09/26 19:10:41 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1673, average loss: 2.1974
[09/26 19:10:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.00	top5: 60.00	
[09/26 19:10:41 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 19:10:47 visual_prompt]: Epoch 5 / 100: avg data time: 5.05e-02, avg batch time: 0.4948, average train loss: 2.2074
[09/26 19:10:49 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1674, average loss: 2.2006
[09/26 19:10:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.50	top5: 60.00	
[09/26 19:10:49 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 19:10:56 visual_prompt]: Epoch 6 / 100: avg data time: 5.41e-02, avg batch time: 0.4970, average train loss: 2.2061
[09/26 19:10:57 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1674, average loss: 2.2214
[09/26 19:10:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 54.50	
[09/26 19:10:57 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 19:11:04 visual_prompt]: Epoch 7 / 100: avg data time: 5.82e-02, avg batch time: 0.5014, average train loss: 2.2082
[09/26 19:11:06 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1674, average loss: 2.1990
[09/26 19:11:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 57.50	
[09/26 19:11:06 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 19:11:13 visual_prompt]: Epoch 8 / 100: avg data time: 6.41e-02, avg batch time: 0.5063, average train loss: 2.2136
[09/26 19:11:14 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1673, average loss: 2.1959
[09/26 19:11:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/26 19:11:14 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 19:11:21 visual_prompt]: Epoch 9 / 100: avg data time: 5.07e-02, avg batch time: 0.4946, average train loss: 2.2081
[09/26 19:11:23 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1674, average loss: 2.2019
[09/26 19:11:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/26 19:11:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 19:11:30 visual_prompt]: Epoch 10 / 100: avg data time: 6.66e-02, avg batch time: 0.5096, average train loss: 2.1599
[09/26 19:11:31 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1669, average loss: 2.1321
[09/26 19:11:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 70.50	
[09/26 19:11:31 visual_prompt]: Best epoch 10: best metric: 0.190
[09/26 19:11:31 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 19:11:38 visual_prompt]: Epoch 11 / 100: avg data time: 6.69e-02, avg batch time: 0.5101, average train loss: 2.1030
[09/26 19:11:40 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1669, average loss: 2.0853
[09/26 19:11:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 77.50	
[09/26 19:11:40 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 19:11:47 visual_prompt]: Epoch 12 / 100: avg data time: 5.65e-02, avg batch time: 0.5003, average train loss: 2.0227
[09/26 19:11:49 visual_prompt]: Inference (val):avg data time: 4.75e-05, avg batch time: 0.1673, average loss: 2.0427
[09/26 19:11:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 79.00	
[09/26 19:11:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 19:11:56 visual_prompt]: Epoch 13 / 100: avg data time: 6.35e-02, avg batch time: 0.5057, average train loss: 1.9665
[09/26 19:11:57 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1672, average loss: 2.0537
[09/26 19:11:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 80.00	
[09/26 19:11:57 visual_prompt]: Best epoch 13: best metric: 0.205
[09/26 19:11:57 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 19:12:04 visual_prompt]: Epoch 14 / 100: avg data time: 6.21e-02, avg batch time: 0.5046, average train loss: 1.8623
[09/26 19:12:06 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1674, average loss: 2.0097
[09/26 19:12:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 77.50	
[09/26 19:12:06 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 19:12:13 visual_prompt]: Epoch 15 / 100: avg data time: 6.76e-02, avg batch time: 0.5099, average train loss: 1.9574
[09/26 19:12:15 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1672, average loss: 1.9990
[09/26 19:12:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 78.00	
[09/26 19:12:15 visual_prompt]: Best epoch 15: best metric: 0.225
[09/26 19:12:15 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 19:12:21 visual_prompt]: Epoch 16 / 100: avg data time: 6.73e-02, avg batch time: 0.5115, average train loss: 1.9226
[09/26 19:12:23 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1672, average loss: 2.0314
[09/26 19:12:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 78.50	
[09/26 19:12:23 visual_prompt]: Best epoch 16: best metric: 0.230
[09/26 19:12:23 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 19:12:30 visual_prompt]: Epoch 17 / 100: avg data time: 5.89e-02, avg batch time: 0.5037, average train loss: 1.7961
[09/26 19:12:32 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1672, average loss: 1.9012
[09/26 19:12:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 83.50	
[09/26 19:12:32 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 19:12:39 visual_prompt]: Epoch 18 / 100: avg data time: 6.79e-02, avg batch time: 0.5120, average train loss: 1.7263
[09/26 19:12:40 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1670, average loss: 1.9046
[09/26 19:12:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 83.50	
[09/26 19:12:40 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 19:12:47 visual_prompt]: Epoch 19 / 100: avg data time: 6.26e-02, avg batch time: 0.5048, average train loss: 1.6971
[09/26 19:12:49 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1673, average loss: 2.1011
[09/26 19:12:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 80.00	
[09/26 19:12:49 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 19:12:56 visual_prompt]: Epoch 20 / 100: avg data time: 5.83e-02, avg batch time: 0.5017, average train loss: 1.6034
[09/26 19:12:57 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1671, average loss: 2.0295
[09/26 19:12:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 81.50	
[09/26 19:12:57 visual_prompt]: Best epoch 20: best metric: 0.260
[09/26 19:12:57 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 19:13:04 visual_prompt]: Epoch 21 / 100: avg data time: 6.06e-02, avg batch time: 0.5041, average train loss: 1.5889
[09/26 19:13:06 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1674, average loss: 2.3411
[09/26 19:13:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 79.50	
[09/26 19:13:06 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 19:13:13 visual_prompt]: Epoch 22 / 100: avg data time: 6.89e-02, avg batch time: 0.5129, average train loss: 1.5250
[09/26 19:13:15 visual_prompt]: Inference (val):avg data time: 4.18e-05, avg batch time: 0.1677, average loss: 2.1512
[09/26 19:13:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 81.00	
[09/26 19:13:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 19:13:22 visual_prompt]: Epoch 23 / 100: avg data time: 5.66e-02, avg batch time: 0.5012, average train loss: 1.4725
[09/26 19:13:23 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1674, average loss: 2.0270
[09/26 19:13:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 86.00	
[09/26 19:13:23 visual_prompt]: Best epoch 23: best metric: 0.290
[09/26 19:13:23 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 19:13:30 visual_prompt]: Epoch 24 / 100: avg data time: 5.66e-02, avg batch time: 0.5024, average train loss: 1.5124
[09/26 19:13:32 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1673, average loss: 2.0124
[09/26 19:13:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 83.50	
[09/26 19:13:32 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 19:13:39 visual_prompt]: Epoch 25 / 100: avg data time: 6.56e-02, avg batch time: 0.5086, average train loss: 1.3652
[09/26 19:13:40 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1678, average loss: 2.3379
[09/26 19:13:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 83.00	
[09/26 19:13:40 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 19:13:47 visual_prompt]: Epoch 26 / 100: avg data time: 6.42e-02, avg batch time: 0.5075, average train loss: 1.3161
[09/26 19:13:49 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1672, average loss: 2.0384
[09/26 19:13:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 88.50	
[09/26 19:13:49 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 19:13:56 visual_prompt]: Epoch 27 / 100: avg data time: 5.84e-02, avg batch time: 0.5034, average train loss: 1.2537
[09/26 19:13:58 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1674, average loss: 2.2725
[09/26 19:13:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 83.50	
[09/26 19:13:58 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 19:14:05 visual_prompt]: Epoch 28 / 100: avg data time: 6.28e-02, avg batch time: 0.5074, average train loss: 1.2048
[09/26 19:14:06 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1675, average loss: 2.3934
[09/26 19:14:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 82.50	
[09/26 19:14:06 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 19:14:13 visual_prompt]: Epoch 29 / 100: avg data time: 6.14e-02, avg batch time: 0.5045, average train loss: 1.1242
[09/26 19:14:15 visual_prompt]: Inference (val):avg data time: 4.36e-05, avg batch time: 0.1670, average loss: 2.3751
[09/26 19:14:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 85.00	
[09/26 19:14:15 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 19:14:22 visual_prompt]: Epoch 30 / 100: avg data time: 5.09e-02, avg batch time: 0.4988, average train loss: 1.0187
[09/26 19:14:23 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1669, average loss: 2.4299
[09/26 19:14:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 88.50	
[09/26 19:14:23 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 19:14:30 visual_prompt]: Epoch 31 / 100: avg data time: 6.30e-02, avg batch time: 0.5067, average train loss: 1.1319
[09/26 19:14:32 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1671, average loss: 2.3631
[09/26 19:14:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 87.50	
[09/26 19:14:32 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 19:14:39 visual_prompt]: Epoch 32 / 100: avg data time: 6.19e-02, avg batch time: 0.5072, average train loss: 1.0995
[09/26 19:14:41 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1670, average loss: 2.9801
[09/26 19:14:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 83.00	
[09/26 19:14:41 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 19:14:48 visual_prompt]: Epoch 33 / 100: avg data time: 6.82e-02, avg batch time: 0.5116, average train loss: 1.0513
[09/26 19:14:49 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1669, average loss: 2.3251
[09/26 19:14:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 90.00	
[09/26 19:14:49 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 19:14:56 visual_prompt]: Epoch 34 / 100: avg data time: 6.78e-02, avg batch time: 0.5114, average train loss: 0.9224
[09/26 19:14:58 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1671, average loss: 2.7857
[09/26 19:14:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 87.00	
[09/26 19:14:58 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 19:15:05 visual_prompt]: Epoch 35 / 100: avg data time: 5.66e-02, avg batch time: 0.5010, average train loss: 0.7116
[09/26 19:15:06 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1677, average loss: 3.3342
[09/26 19:15:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 85.50	
[09/26 19:15:06 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 19:15:13 visual_prompt]: Epoch 36 / 100: avg data time: 6.31e-02, avg batch time: 0.5059, average train loss: 0.7262
[09/26 19:15:15 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1674, average loss: 3.3465
[09/26 19:15:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 87.00	
[09/26 19:15:15 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 19:15:22 visual_prompt]: Epoch 37 / 100: avg data time: 6.18e-02, avg batch time: 0.5058, average train loss: 0.8574
[09/26 19:15:23 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1676, average loss: 2.8575
[09/26 19:15:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 87.00	
[09/26 19:15:23 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 19:15:30 visual_prompt]: Epoch 38 / 100: avg data time: 5.74e-02, avg batch time: 0.5031, average train loss: 0.7756
[09/26 19:15:32 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1676, average loss: 2.8256
[09/26 19:15:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 86.50	
[09/26 19:15:32 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 19:15:39 visual_prompt]: Epoch 39 / 100: avg data time: 6.37e-02, avg batch time: 0.5070, average train loss: 0.6651
[09/26 19:15:41 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1675, average loss: 3.1439
[09/26 19:15:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 87.50	
[09/26 19:15:41 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 19:15:47 visual_prompt]: Epoch 40 / 100: avg data time: 6.54e-02, avg batch time: 0.5075, average train loss: 0.5757
[09/26 19:15:49 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1673, average loss: 3.6108
[09/26 19:15:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 90.50	
[09/26 19:15:49 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 19:15:56 visual_prompt]: Epoch 41 / 100: avg data time: 6.56e-02, avg batch time: 0.5090, average train loss: 0.6567
[09/26 19:15:58 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1676, average loss: 3.7729
[09/26 19:15:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 83.00	
[09/26 19:15:58 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 19:16:05 visual_prompt]: Epoch 42 / 100: avg data time: 6.82e-02, avg batch time: 0.5111, average train loss: 0.5788
[09/26 19:16:06 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1673, average loss: 2.9068
[09/26 19:16:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 90.00	
[09/26 19:16:06 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 19:16:13 visual_prompt]: Epoch 43 / 100: avg data time: 5.70e-02, avg batch time: 0.5004, average train loss: 0.5443
[09/26 19:16:15 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1677, average loss: 3.3675
[09/26 19:16:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 89.00	
[09/26 19:16:15 visual_prompt]: Best epoch 43: best metric: 0.305
[09/26 19:16:15 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 19:16:22 visual_prompt]: Epoch 44 / 100: avg data time: 6.59e-02, avg batch time: 0.5090, average train loss: 0.4480
[09/26 19:16:24 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1671, average loss: 3.3184
[09/26 19:16:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 91.00	
[09/26 19:16:24 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 19:16:30 visual_prompt]: Epoch 45 / 100: avg data time: 5.75e-02, avg batch time: 0.5003, average train loss: 0.3524
[09/26 19:16:32 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1675, average loss: 4.2478
[09/26 19:16:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 87.00	
[09/26 19:16:32 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 19:16:39 visual_prompt]: Epoch 46 / 100: avg data time: 5.97e-02, avg batch time: 0.5040, average train loss: 0.3347
[09/26 19:16:41 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1674, average loss: 3.9528
[09/26 19:16:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 89.50	
[09/26 19:16:41 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 19:16:48 visual_prompt]: Epoch 47 / 100: avg data time: 6.38e-02, avg batch time: 0.5068, average train loss: 0.3096
[09/26 19:16:49 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1672, average loss: 4.3756
[09/26 19:16:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 90.00	
[09/26 19:16:49 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 19:16:56 visual_prompt]: Epoch 48 / 100: avg data time: 6.32e-02, avg batch time: 0.5064, average train loss: 0.3610
[09/26 19:16:58 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1675, average loss: 4.5784
[09/26 19:16:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 85.50	
[09/26 19:16:58 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 19:17:05 visual_prompt]: Epoch 49 / 100: avg data time: 5.61e-02, avg batch time: 0.4984, average train loss: 0.3856
[09/26 19:17:06 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1682, average loss: 4.5159
[09/26 19:17:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 86.50	
[09/26 19:17:06 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 19:17:13 visual_prompt]: Epoch 50 / 100: avg data time: 6.36e-02, avg batch time: 0.5075, average train loss: 0.2899
[09/26 19:17:15 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1673, average loss: 4.4770
[09/26 19:17:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 87.50	
[09/26 19:17:15 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 19:17:22 visual_prompt]: Epoch 51 / 100: avg data time: 6.42e-02, avg batch time: 0.5110, average train loss: 0.3507
[09/26 19:17:23 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1674, average loss: 4.6535
[09/26 19:17:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 86.00	
[09/26 19:17:23 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 19:17:30 visual_prompt]: Epoch 52 / 100: avg data time: 5.90e-02, avg batch time: 0.5052, average train loss: 0.2735
[09/26 19:17:32 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1673, average loss: 4.2143
[09/26 19:17:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 90.00	
[09/26 19:17:32 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 19:17:39 visual_prompt]: Epoch 53 / 100: avg data time: 6.26e-02, avg batch time: 0.5065, average train loss: 0.1917
[09/26 19:17:41 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1674, average loss: 5.1944
[09/26 19:17:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 87.50	
[09/26 19:17:41 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 19:17:47 visual_prompt]: Epoch 54 / 100: avg data time: 4.99e-02, avg batch time: 0.4943, average train loss: 0.1954
[09/26 19:17:49 visual_prompt]: Inference (val):avg data time: 3.46e-05, avg batch time: 0.1675, average loss: 5.1045
[09/26 19:17:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 87.50	
[09/26 19:17:49 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 19:17:56 visual_prompt]: Epoch 55 / 100: avg data time: 5.87e-02, avg batch time: 0.5031, average train loss: 0.1887
[09/26 19:17:58 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.1675, average loss: 5.1920
[09/26 19:17:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 87.00	
[09/26 19:17:58 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 19:18:05 visual_prompt]: Epoch 56 / 100: avg data time: 5.80e-02, avg batch time: 0.5036, average train loss: 0.1717
[09/26 19:18:06 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1675, average loss: 5.3955
[09/26 19:18:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 87.50	
[09/26 19:18:06 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 19:18:13 visual_prompt]: Epoch 57 / 100: avg data time: 6.41e-02, avg batch time: 0.5097, average train loss: 0.1722
[09/26 19:18:15 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1676, average loss: 5.2277
[09/26 19:18:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 88.00	
[09/26 19:18:15 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 19:18:22 visual_prompt]: Epoch 58 / 100: avg data time: 6.39e-02, avg batch time: 0.5093, average train loss: 0.1717
[09/26 19:18:23 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1672, average loss: 5.3986
[09/26 19:18:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 86.50	
[09/26 19:18:23 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 19:18:30 visual_prompt]: Epoch 59 / 100: avg data time: 6.50e-02, avg batch time: 0.5127, average train loss: 0.1417
[09/26 19:18:32 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1676, average loss: 5.2337
[09/26 19:18:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 86.50	
[09/26 19:18:32 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 19:18:39 visual_prompt]: Epoch 60 / 100: avg data time: 6.84e-02, avg batch time: 0.5118, average train loss: 0.1310
[09/26 19:18:41 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1670, average loss: 5.4393
[09/26 19:18:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 86.50	
[09/26 19:18:41 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 19:18:48 visual_prompt]: Epoch 61 / 100: avg data time: 6.76e-02, avg batch time: 0.5111, average train loss: 0.0850
[09/26 19:18:49 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1672, average loss: 5.5640
[09/26 19:18:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 87.50	
[09/26 19:18:49 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 19:18:56 visual_prompt]: Epoch 62 / 100: avg data time: 6.24e-02, avg batch time: 0.5067, average train loss: 0.0719
[09/26 19:18:58 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1676, average loss: 5.8569
[09/26 19:18:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 86.50	
[09/26 19:18:58 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 19:19:05 visual_prompt]: Epoch 63 / 100: avg data time: 6.21e-02, avg batch time: 0.5048, average train loss: 0.0794
[09/26 19:19:07 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1672, average loss: 5.9873
[09/26 19:19:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 88.00	
[09/26 19:19:07 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 19:19:14 visual_prompt]: Epoch 64 / 100: avg data time: 6.36e-02, avg batch time: 0.5088, average train loss: 0.0670
[09/26 19:19:15 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1672, average loss: 6.3143
[09/26 19:19:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 86.00	
[09/26 19:19:15 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 19:19:22 visual_prompt]: Epoch 65 / 100: avg data time: 6.60e-02, avg batch time: 0.5093, average train loss: 0.0490
[09/26 19:19:24 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1667, average loss: 5.7982
[09/26 19:19:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 86.50	
[09/26 19:19:24 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 19:19:31 visual_prompt]: Epoch 66 / 100: avg data time: 5.43e-02, avg batch time: 0.4987, average train loss: 0.0482
[09/26 19:19:32 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1674, average loss: 5.7989
[09/26 19:19:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 87.00	
[09/26 19:19:32 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 19:19:39 visual_prompt]: Epoch 67 / 100: avg data time: 6.63e-02, avg batch time: 0.5104, average train loss: 0.0230
[09/26 19:19:41 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1668, average loss: 5.9499
[09/26 19:19:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 87.00	
[09/26 19:19:41 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 19:19:48 visual_prompt]: Epoch 68 / 100: avg data time: 6.59e-02, avg batch time: 0.5084, average train loss: 0.0363
[09/26 19:19:50 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1676, average loss: 5.9371
[09/26 19:19:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 87.00	
[09/26 19:19:50 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 19:19:57 visual_prompt]: Epoch 69 / 100: avg data time: 6.23e-02, avg batch time: 0.5059, average train loss: 0.0246
[09/26 19:19:58 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1675, average loss: 6.1292
[09/26 19:19:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 85.50	
[09/26 19:19:58 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 19:20:05 visual_prompt]: Epoch 70 / 100: avg data time: 5.55e-02, avg batch time: 0.4992, average train loss: 0.0228
[09/26 19:20:07 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1675, average loss: 6.1765
[09/26 19:20:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 87.00	
[09/26 19:20:07 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 19:20:14 visual_prompt]: Epoch 71 / 100: avg data time: 6.61e-02, avg batch time: 0.5098, average train loss: 0.0284
[09/26 19:20:15 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1675, average loss: 6.0194
[09/26 19:20:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 88.00	
[09/26 19:20:15 visual_prompt]: Best epoch 71: best metric: 0.310
[09/26 19:20:15 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 19:20:22 visual_prompt]: Epoch 72 / 100: avg data time: 6.24e-02, avg batch time: 0.5048, average train loss: 0.0276
[09/26 19:20:24 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1676, average loss: 6.2518
[09/26 19:20:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.50	
[09/26 19:20:24 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 19:20:31 visual_prompt]: Epoch 73 / 100: avg data time: 6.86e-02, avg batch time: 0.5107, average train loss: 0.0244
[09/26 19:20:33 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1676, average loss: 6.4221
[09/26 19:20:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 87.50	
[09/26 19:20:33 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 19:20:40 visual_prompt]: Epoch 74 / 100: avg data time: 6.78e-02, avg batch time: 0.5109, average train loss: 0.0165
[09/26 19:20:41 visual_prompt]: Inference (val):avg data time: 4.52e-05, avg batch time: 0.1673, average loss: 6.4330
[09/26 19:20:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 86.50	
[09/26 19:20:41 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 19:20:48 visual_prompt]: Epoch 75 / 100: avg data time: 6.81e-02, avg batch time: 0.5107, average train loss: 0.0129
[09/26 19:20:50 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1671, average loss: 6.4077
[09/26 19:20:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 87.50	
[09/26 19:20:50 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 19:20:57 visual_prompt]: Epoch 76 / 100: avg data time: 6.13e-02, avg batch time: 0.5050, average train loss: 0.0145
[09/26 19:20:59 visual_prompt]: Inference (val):avg data time: 4.17e-05, avg batch time: 0.1671, average loss: 6.3845
[09/26 19:20:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 87.50	
[09/26 19:20:59 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 19:21:06 visual_prompt]: Epoch 77 / 100: avg data time: 6.63e-02, avg batch time: 0.5090, average train loss: 0.0136
[09/26 19:21:07 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1675, average loss: 6.2576
[09/26 19:21:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 87.50	
[09/26 19:21:07 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 19:21:14 visual_prompt]: Epoch 78 / 100: avg data time: 4.74e-02, avg batch time: 0.4930, average train loss: 0.0096
[09/26 19:21:16 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1671, average loss: 6.3335
[09/26 19:21:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 87.50	
[09/26 19:21:16 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 19:21:23 visual_prompt]: Epoch 79 / 100: avg data time: 5.89e-02, avg batch time: 0.5018, average train loss: 0.0099
[09/26 19:21:24 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1672, average loss: 6.4570
[09/26 19:21:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 88.00	
[09/26 19:21:24 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 19:21:31 visual_prompt]: Epoch 80 / 100: avg data time: 6.30e-02, avg batch time: 0.5078, average train loss: 0.0096
[09/26 19:21:33 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1672, average loss: 6.5889
[09/26 19:21:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 87.50	
[09/26 19:21:33 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 19:21:40 visual_prompt]: Epoch 81 / 100: avg data time: 5.86e-02, avg batch time: 0.5009, average train loss: 0.0066
[09/26 19:21:41 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1674, average loss: 6.4790
[09/26 19:21:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 88.00	
[09/26 19:21:41 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 19:21:48 visual_prompt]: Epoch 82 / 100: avg data time: 5.94e-02, avg batch time: 0.5015, average train loss: 0.0107
[09/26 19:21:50 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1673, average loss: 6.5180
[09/26 19:21:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 87.50	
[09/26 19:21:50 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 19:21:57 visual_prompt]: Epoch 83 / 100: avg data time: 6.04e-02, avg batch time: 0.5041, average train loss: 0.0112
[09/26 19:21:58 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1670, average loss: 6.4301
[09/26 19:21:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 88.00	
[09/26 19:21:58 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 19:22:05 visual_prompt]: Epoch 84 / 100: avg data time: 4.90e-02, avg batch time: 0.4935, average train loss: 0.0066
[09/26 19:22:07 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1671, average loss: 6.4663
[09/26 19:22:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 88.00	
[09/26 19:22:07 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 19:22:14 visual_prompt]: Epoch 85 / 100: avg data time: 5.91e-02, avg batch time: 0.5025, average train loss: 0.0089
[09/26 19:22:15 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1674, average loss: 6.4643
[09/26 19:22:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 88.00	
[09/26 19:22:15 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 19:22:22 visual_prompt]: Epoch 86 / 100: avg data time: 6.19e-02, avg batch time: 0.5058, average train loss: 0.0090
[09/26 19:22:24 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1670, average loss: 6.4641
[09/26 19:22:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 88.00	
[09/26 19:22:24 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 19:22:31 visual_prompt]: Epoch 87 / 100: avg data time: 5.39e-02, avg batch time: 0.4975, average train loss: 0.0063
[09/26 19:22:33 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1673, average loss: 6.4739
[09/26 19:22:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 87.50	
[09/26 19:22:33 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 19:22:39 visual_prompt]: Epoch 88 / 100: avg data time: 6.17e-02, avg batch time: 0.5050, average train loss: 0.0066
[09/26 19:22:41 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1671, average loss: 6.5330
[09/26 19:22:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 87.50	
[09/26 19:22:41 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 19:22:48 visual_prompt]: Epoch 89 / 100: avg data time: 6.44e-02, avg batch time: 0.5079, average train loss: 0.0101
[09/26 19:22:50 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.1672, average loss: 6.6871
[09/26 19:22:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.00	
[09/26 19:22:50 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 19:22:57 visual_prompt]: Epoch 90 / 100: avg data time: 6.29e-02, avg batch time: 0.5069, average train loss: 0.0081
[09/26 19:22:58 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1675, average loss: 6.7032
[09/26 19:22:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 87.50	
[09/26 19:22:58 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 19:23:05 visual_prompt]: Epoch 91 / 100: avg data time: 5.65e-02, avg batch time: 0.5016, average train loss: 0.0057
[09/26 19:23:07 visual_prompt]: Inference (val):avg data time: 4.24e-05, avg batch time: 0.1673, average loss: 6.6887
[09/26 19:23:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 87.50	
[09/26 19:23:07 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 19:23:14 visual_prompt]: Epoch 92 / 100: avg data time: 5.95e-02, avg batch time: 0.5031, average train loss: 0.0057
[09/26 19:23:15 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1674, average loss: 6.6730
[09/26 19:23:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.50	
[09/26 19:23:15 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 19:23:22 visual_prompt]: Epoch 93 / 100: avg data time: 6.03e-02, avg batch time: 0.5039, average train loss: 0.0060
[09/26 19:23:24 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1674, average loss: 6.6683
[09/26 19:23:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.50	
[09/26 19:23:24 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 19:23:31 visual_prompt]: Epoch 94 / 100: avg data time: 6.54e-02, avg batch time: 0.5104, average train loss: 0.0061
[09/26 19:23:33 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1679, average loss: 6.6665
[09/26 19:23:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.50	
[09/26 19:23:33 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 19:23:40 visual_prompt]: Epoch 95 / 100: avg data time: 5.83e-02, avg batch time: 0.5012, average train loss: 0.0057
[09/26 19:23:41 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1679, average loss: 6.6639
[09/26 19:23:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.50	
[09/26 19:23:41 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 19:23:48 visual_prompt]: Epoch 96 / 100: avg data time: 4.93e-02, avg batch time: 0.4957, average train loss: 0.0114
[09/26 19:23:50 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1674, average loss: 6.6471
[09/26 19:23:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.50	
[09/26 19:23:50 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 19:23:57 visual_prompt]: Epoch 97 / 100: avg data time: 6.23e-02, avg batch time: 0.5050, average train loss: 0.0100
[09/26 19:23:58 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1677, average loss: 6.6392
[09/26 19:23:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.50	
[09/26 19:23:58 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 19:24:05 visual_prompt]: Epoch 98 / 100: avg data time: 6.19e-02, avg batch time: 0.5065, average train loss: 0.0070
[09/26 19:24:07 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1676, average loss: 6.6393
[09/26 19:24:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.50	
[09/26 19:24:07 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 19:24:14 visual_prompt]: Epoch 99 / 100: avg data time: 6.19e-02, avg batch time: 0.5090, average train loss: 0.0091
[09/26 19:24:15 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1677, average loss: 6.6391
[09/26 19:24:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.50	
[09/26 19:24:15 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 19:24:22 visual_prompt]: Epoch 100 / 100: avg data time: 4.93e-02, avg batch time: 0.4951, average train loss: 0.0076
[09/26 19:24:24 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1676, average loss: 6.6394
[09/26 19:24:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 87.50	
[09/26 19:24:24 visual_prompt]: Rank of current process: 0. World size: 1
[09/26 19:24:24 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[09/26 19:24:24 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir'])
[09/26 19:24:24 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/26 19:24:24 visual_prompt]: Training with config:
[09/26 19:24:24 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/prompt50/crop224/val/seed0/lr0.05_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 9, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[09/26 19:24:24 visual_prompt]: Loading training data...
[09/26 19:24:24 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset train...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split train[:800], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 19:24:25 visual_prompt]: Number of images: 800
[09/26 19:24:25 visual_prompt]: Number of classes: 9 / 9
[09/26 19:24:25 visual_prompt]: Loading validation data...
[09/26 19:24:25 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  599]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  573]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   49]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/26 19:24:25 visual_prompt]: Number of images: 200
[09/26 19:24:25 visual_prompt]: Number of classes: 9 / 9
[09/26 19:24:25 visual_prompt]: Constructing models...
[09/26 19:24:28 visual_prompt]: Total Parameters: 86266377	 Gradient Parameters: 467721
[09/26 19:24:28 visual_prompt]: tuned percent:0.542
[09/26 19:24:28 visual_prompt]: Device used for model: 0
[09/26 19:24:28 visual_prompt]: Setting up Evaluator...
[09/26 19:24:28 visual_prompt]: Setting up Trainer...
[09/26 19:24:28 visual_prompt]: 	Setting up the optimizer...
[09/26 19:24:28 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/26 19:24:35 visual_prompt]: Epoch 1 / 100: avg data time: 6.28e-02, avg batch time: 0.5074, average train loss: 2.8735
[09/26 19:24:37 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1674, average loss: 2.9516
[09/26 19:24:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/26 19:24:37 visual_prompt]: Best epoch 1: best metric: 0.130
[09/26 19:24:37 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[09/26 19:24:43 visual_prompt]: Epoch 2 / 100: avg data time: 6.17e-02, avg batch time: 0.5042, average train loss: 2.3896
[09/26 19:24:45 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1672, average loss: 2.2345
[09/26 19:24:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.00	
[09/26 19:24:45 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[09/26 19:24:52 visual_prompt]: Epoch 3 / 100: avg data time: 6.19e-02, avg batch time: 0.5045, average train loss: 2.2442
[09/26 19:24:54 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1671, average loss: 2.2207
[09/26 19:24:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 49.50	
[09/26 19:24:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.015
[09/26 19:25:00 visual_prompt]: Epoch 4 / 100: avg data time: 5.68e-02, avg batch time: 0.5002, average train loss: 2.2201
[09/26 19:25:02 visual_prompt]: Inference (val):avg data time: 4.42e-05, avg batch time: 0.1675, average loss: 2.2165
[09/26 19:25:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.50	top5: 58.00	
[09/26 19:25:02 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[09/26 19:25:09 visual_prompt]: Epoch 5 / 100: avg data time: 6.28e-02, avg batch time: 0.5056, average train loss: 2.2154
[09/26 19:25:11 visual_prompt]: Inference (val):avg data time: 4.37e-05, avg batch time: 0.1675, average loss: 2.2033
[09/26 19:25:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/26 19:25:11 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.025
[09/26 19:25:18 visual_prompt]: Epoch 6 / 100: avg data time: 6.08e-02, avg batch time: 0.5040, average train loss: 2.2080
[09/26 19:25:19 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1673, average loss: 2.2171
[09/26 19:25:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/26 19:25:19 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.03
[09/26 19:25:26 visual_prompt]: Epoch 7 / 100: avg data time: 6.27e-02, avg batch time: 0.5052, average train loss: 2.2053
[09/26 19:25:28 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1671, average loss: 2.1933
[09/26 19:25:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 58.00	
[09/26 19:25:28 visual_prompt]: Best epoch 7: best metric: 0.155
[09/26 19:25:28 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[09/26 19:25:35 visual_prompt]: Epoch 8 / 100: avg data time: 6.72e-02, avg batch time: 0.5097, average train loss: 2.2009
[09/26 19:25:36 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1673, average loss: 2.2213
[09/26 19:25:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 57.00	
[09/26 19:25:36 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[09/26 19:25:43 visual_prompt]: Epoch 9 / 100: avg data time: 6.26e-02, avg batch time: 0.5038, average train loss: 2.2130
[09/26 19:25:45 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1673, average loss: 2.2163
[09/26 19:25:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.50	
[09/26 19:25:45 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[09/26 19:25:52 visual_prompt]: Epoch 10 / 100: avg data time: 6.09e-02, avg batch time: 0.5043, average train loss: 2.1772
[09/26 19:25:53 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1671, average loss: 2.1768
[09/26 19:25:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 69.00	
[09/26 19:25:53 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.05
[09/26 19:26:00 visual_prompt]: Epoch 11 / 100: avg data time: 5.68e-02, avg batch time: 0.4995, average train loss: 2.1398
[09/26 19:26:02 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1668, average loss: 2.1365
[09/26 19:26:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 70.50	
[09/26 19:26:02 visual_prompt]: Best epoch 11: best metric: 0.195
[09/26 19:26:02 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[09/26 19:26:09 visual_prompt]: Epoch 12 / 100: avg data time: 6.80e-02, avg batch time: 0.5113, average train loss: 2.0705
[09/26 19:26:10 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1669, average loss: 2.2930
[09/26 19:26:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 62.50	
[09/26 19:26:10 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[09/26 19:26:17 visual_prompt]: Epoch 13 / 100: avg data time: 6.22e-02, avg batch time: 0.5048, average train loss: 1.9874
[09/26 19:26:19 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1671, average loss: 2.0408
[09/26 19:26:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 77.50	
[09/26 19:26:19 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[09/26 19:26:26 visual_prompt]: Epoch 14 / 100: avg data time: 5.51e-02, avg batch time: 0.4993, average train loss: 1.9666
[09/26 19:26:28 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1676, average loss: 1.9562
[09/26 19:26:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 78.50	
[09/26 19:26:28 visual_prompt]: Best epoch 14: best metric: 0.205
[09/26 19:26:28 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[09/26 19:26:34 visual_prompt]: Epoch 15 / 100: avg data time: 5.59e-02, avg batch time: 0.4989, average train loss: 1.8658
[09/26 19:26:36 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1671, average loss: 2.1225
[09/26 19:26:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 76.50	
[09/26 19:26:36 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[09/26 19:26:43 visual_prompt]: Epoch 16 / 100: avg data time: 6.12e-02, avg batch time: 0.5031, average train loss: 1.8816
[09/26 19:26:45 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1672, average loss: 2.0030
[09/26 19:26:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 80.50	
[09/26 19:26:45 visual_prompt]: Best epoch 16: best metric: 0.225
[09/26 19:26:45 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[09/26 19:26:52 visual_prompt]: Epoch 17 / 100: avg data time: 7.01e-02, avg batch time: 0.5133, average train loss: 1.8151
[09/26 19:26:53 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1672, average loss: 2.2475
[09/26 19:26:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 72.50	
[09/26 19:26:53 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[09/26 19:27:00 visual_prompt]: Epoch 18 / 100: avg data time: 6.37e-02, avg batch time: 0.5080, average train loss: 1.8422
[09/26 19:27:02 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1673, average loss: 2.2099
[09/26 19:27:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 75.50	
[09/26 19:27:02 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[09/26 19:27:09 visual_prompt]: Epoch 19 / 100: avg data time: 6.04e-02, avg batch time: 0.5047, average train loss: 1.7215
[09/26 19:27:10 visual_prompt]: Inference (val):avg data time: 4.49e-05, avg batch time: 0.1673, average loss: 2.0796
[09/26 19:27:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 81.50	
[09/26 19:27:10 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[09/26 19:27:17 visual_prompt]: Epoch 20 / 100: avg data time: 4.75e-02, avg batch time: 0.4923, average train loss: 1.7279
[09/26 19:27:19 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1678, average loss: 1.9077
[09/26 19:27:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 83.50	
[09/26 19:27:19 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[09/26 19:27:25 visual_prompt]: Epoch 21 / 100: avg data time: 5.03e-02, avg batch time: 0.4971, average train loss: 1.6667
[09/26 19:27:27 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.1669, average loss: 1.9941
[09/26 19:27:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 82.50	
[09/26 19:27:27 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[09/26 19:27:34 visual_prompt]: Epoch 22 / 100: avg data time: 6.53e-02, avg batch time: 0.5074, average train loss: 1.5798
[09/26 19:27:36 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1674, average loss: 2.0104
[09/26 19:27:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 83.00	
[09/26 19:27:36 visual_prompt]: Best epoch 22: best metric: 0.255
[09/26 19:27:36 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[09/26 19:27:43 visual_prompt]: Epoch 23 / 100: avg data time: 6.38e-02, avg batch time: 0.5059, average train loss: 1.5583
[09/26 19:27:44 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1674, average loss: 2.0093
[09/26 19:27:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 84.00	
[09/26 19:27:44 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[09/26 19:27:51 visual_prompt]: Epoch 24 / 100: avg data time: 4.88e-02, avg batch time: 0.4944, average train loss: 1.5289
[09/26 19:27:53 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1674, average loss: 2.2058
[09/26 19:27:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 82.00	
[09/26 19:27:53 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[09/26 19:28:00 visual_prompt]: Epoch 25 / 100: avg data time: 6.57e-02, avg batch time: 0.5090, average train loss: 1.4394
[09/26 19:28:01 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1672, average loss: 1.9702
[09/26 19:28:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 84.00	
[09/26 19:28:01 visual_prompt]: Best epoch 25: best metric: 0.260
[09/26 19:28:01 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[09/26 19:28:08 visual_prompt]: Epoch 26 / 100: avg data time: 6.46e-02, avg batch time: 0.5079, average train loss: 1.4010
[09/26 19:28:10 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1679, average loss: 2.0468
[09/26 19:28:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 87.50	
[09/26 19:28:10 visual_prompt]: Best epoch 26: best metric: 0.275
[09/26 19:28:10 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.04620120240391065
[09/26 19:28:17 visual_prompt]: Epoch 27 / 100: avg data time: 5.95e-02, avg batch time: 0.5036, average train loss: 1.3331
[09/26 19:28:18 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1681, average loss: 2.3320
[09/26 19:28:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 84.50	
[09/26 19:28:18 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.04572593931387604
[09/26 19:28:25 visual_prompt]: Epoch 28 / 100: avg data time: 5.00e-02, avg batch time: 0.4934, average train loss: 1.3879
[09/26 19:28:27 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1676, average loss: 2.0911
[09/26 19:28:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 87.50	
[09/26 19:28:27 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.04522542485937369
[09/26 19:28:34 visual_prompt]: Epoch 29 / 100: avg data time: 6.33e-02, avg batch time: 0.5068, average train loss: 1.2835
[09/26 19:28:35 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1676, average loss: 2.1979
[09/26 19:28:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 88.00	
[09/26 19:28:35 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.04470026884016805
[09/26 19:28:42 visual_prompt]: Epoch 30 / 100: avg data time: 6.16e-02, avg batch time: 0.5046, average train loss: 1.3226
[09/26 19:28:44 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1676, average loss: 2.0467
[09/26 19:28:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 89.00	
[09/26 19:28:44 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.04415111107797445
[09/26 19:28:51 visual_prompt]: Epoch 31 / 100: avg data time: 6.42e-02, avg batch time: 0.5072, average train loss: 1.1826
[09/26 19:28:53 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1677, average loss: 2.4054
[09/26 19:28:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 85.50	
[09/26 19:28:53 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.04357862063693486
[09/26 19:28:59 visual_prompt]: Epoch 32 / 100: avg data time: 6.05e-02, avg batch time: 0.5049, average train loss: 1.1612
[09/26 19:29:01 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1677, average loss: 2.4903
[09/26 19:29:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 88.00	
[09/26 19:29:01 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.04298349500846628
[09/26 19:29:08 visual_prompt]: Epoch 33 / 100: avg data time: 6.00e-02, avg batch time: 0.5031, average train loss: 1.0723
[09/26 19:29:10 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1676, average loss: 2.5715
[09/26 19:29:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 86.50	
[09/26 19:29:10 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.042366459261474934
[09/26 19:29:17 visual_prompt]: Epoch 34 / 100: avg data time: 6.07e-02, avg batch time: 0.5044, average train loss: 0.9843
[09/26 19:29:18 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1673, average loss: 2.7619
[09/26 19:29:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 87.00	
[09/26 19:29:18 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.04172826515897146
[09/26 19:29:25 visual_prompt]: Epoch 35 / 100: avg data time: 6.19e-02, avg batch time: 0.5052, average train loss: 0.9143
[09/26 19:29:27 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1675, average loss: 3.3501
[09/26 19:29:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 82.50	
[09/26 19:29:27 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.04106969024216348
[09/26 19:29:34 visual_prompt]: Epoch 36 / 100: avg data time: 6.20e-02, avg batch time: 0.5043, average train loss: 0.8900
[09/26 19:29:35 visual_prompt]: Inference (val):avg data time: 4.12e-05, avg batch time: 0.1676, average loss: 2.5513
[09/26 19:29:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 88.50	
[09/26 19:29:35 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.040391536883141455
[09/26 19:29:42 visual_prompt]: Epoch 37 / 100: avg data time: 5.67e-02, avg batch time: 0.5009, average train loss: 0.9064
[09/26 19:29:44 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1674, average loss: 2.7274
[09/26 19:29:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 92.00	
[09/26 19:29:44 visual_prompt]: Best epoch 37: best metric: 0.305
[09/26 19:29:44 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.03969463130731183
[09/26 19:29:51 visual_prompt]: Epoch 38 / 100: avg data time: 5.41e-02, avg batch time: 0.4986, average train loss: 0.9124
[09/26 19:29:52 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1671, average loss: 2.9481
[09/26 19:29:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 86.00	
[09/26 19:29:52 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.03897982258676867
[09/26 19:29:59 visual_prompt]: Epoch 39 / 100: avg data time: 6.29e-02, avg batch time: 0.5061, average train loss: 0.9128
[09/26 19:30:01 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1675, average loss: 2.7649
[09/26 19:30:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 86.50	
[09/26 19:30:01 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.03824798160583012
[09/26 19:30:08 visual_prompt]: Epoch 40 / 100: avg data time: 5.83e-02, avg batch time: 0.5014, average train loss: 0.9382
[09/26 19:30:09 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1678, average loss: 2.8456
[09/26 19:30:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 85.00	
[09/26 19:30:09 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.037500000000000006
[09/26 19:30:16 visual_prompt]: Epoch 41 / 100: avg data time: 6.65e-02, avg batch time: 0.5099, average train loss: 0.7477
[09/26 19:30:18 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1672, average loss: 2.7574
[09/26 19:30:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 91.00	
[09/26 19:30:18 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.03673678906964727
[09/26 19:30:25 visual_prompt]: Epoch 42 / 100: avg data time: 5.36e-02, avg batch time: 0.4989, average train loss: 0.6645
[09/26 19:30:27 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1672, average loss: 3.4790
[09/26 19:30:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 85.00	
[09/26 19:30:27 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.03595927866972694
[09/26 19:30:34 visual_prompt]: Epoch 43 / 100: avg data time: 6.90e-02, avg batch time: 0.5136, average train loss: 0.6675
[09/26 19:30:36 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1674, average loss: 3.2447
[09/26 19:30:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 89.50	
[09/26 19:30:36 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.03516841607689501
[09/26 19:30:43 visual_prompt]: Epoch 44 / 100: avg data time: 8.18e-02, avg batch time: 0.5251, average train loss: 0.7501
[09/26 19:30:44 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1675, average loss: 3.7868
[09/26 19:30:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 81.50	
[09/26 19:30:44 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0343651648353978
[09/26 19:30:51 visual_prompt]: Epoch 45 / 100: avg data time: 6.36e-02, avg batch time: 0.5082, average train loss: 0.7654
[09/26 19:30:54 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1673, average loss: 3.2538
[09/26 19:30:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 83.50	
[09/26 19:30:54 visual_prompt]: Training 46 / 100 epoch, with learning rate 0.033550503583141725
[09/26 19:31:01 visual_prompt]: Epoch 46 / 100: avg data time: 7.11e-02, avg batch time: 0.5165, average train loss: 0.6375
[09/26 19:31:02 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1674, average loss: 3.5019
[09/26 19:31:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 81.50	
[09/26 19:31:02 visual_prompt]: Training 47 / 100 epoch, with learning rate 0.032725424859373686
[09/26 19:31:09 visual_prompt]: Epoch 47 / 100: avg data time: 5.67e-02, avg batch time: 0.4988, average train loss: 0.5062
[09/26 19:31:11 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1674, average loss: 3.5639
[09/26 19:31:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 85.50	
[09/26 19:31:11 visual_prompt]: Training 48 / 100 epoch, with learning rate 0.03189093389542498
[09/26 19:31:18 visual_prompt]: Epoch 48 / 100: avg data time: 6.52e-02, avg batch time: 0.5083, average train loss: 0.4737
[09/26 19:31:20 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1677, average loss: 3.5701
[09/26 19:31:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 88.00	
[09/26 19:31:20 visual_prompt]: Training 49 / 100 epoch, with learning rate 0.03104804738999169
[09/26 19:31:26 visual_prompt]: Epoch 49 / 100: avg data time: 6.21e-02, avg batch time: 0.5045, average train loss: 0.3570
[09/26 19:31:28 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1672, average loss: 3.6736
[09/26 19:31:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 90.00	
[09/26 19:31:28 visual_prompt]: Training 50 / 100 epoch, with learning rate 0.03019779227044398
[09/26 19:31:35 visual_prompt]: Epoch 50 / 100: avg data time: 5.62e-02, avg batch time: 0.5003, average train loss: 0.3842
[09/26 19:31:37 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1675, average loss: 4.1248
[09/26 19:31:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 88.50	
[09/26 19:31:37 visual_prompt]: Training 51 / 100 epoch, with learning rate 0.029341204441673263
[09/26 19:31:44 visual_prompt]: Epoch 51 / 100: avg data time: 6.62e-02, avg batch time: 0.5081, average train loss: 0.7250
[09/26 19:31:45 visual_prompt]: Inference (val):avg data time: 4.02e-05, avg batch time: 0.1673, average loss: 3.6495
[09/26 19:31:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 87.50	
[09/26 19:31:45 visual_prompt]: Training 52 / 100 epoch, with learning rate 0.028479327524001636
[09/26 19:31:52 visual_prompt]: Epoch 52 / 100: avg data time: 5.95e-02, avg batch time: 0.5025, average train loss: 0.7261
[09/26 19:31:54 visual_prompt]: Inference (val):avg data time: 4.25e-05, avg batch time: 0.1674, average loss: 3.4547
[09/26 19:31:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 85.00	
[09/26 19:31:54 visual_prompt]: Training 53 / 100 epoch, with learning rate 0.02761321158169134
[09/26 19:32:01 visual_prompt]: Epoch 53 / 100: avg data time: 6.19e-02, avg batch time: 0.5050, average train loss: 0.4879
[09/26 19:32:02 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1672, average loss: 3.3408
[09/26 19:32:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 88.00	
[09/26 19:32:02 visual_prompt]: Training 54 / 100 epoch, with learning rate 0.02674391184360313
[09/26 19:32:09 visual_prompt]: Epoch 54 / 100: avg data time: 6.53e-02, avg batch time: 0.5088, average train loss: 0.3654
[09/26 19:32:11 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1677, average loss: 3.4929
[09/26 19:32:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 87.00	
[09/26 19:32:11 visual_prompt]: Training 55 / 100 epoch, with learning rate 0.02587248741756253
[09/26 19:32:18 visual_prompt]: Epoch 55 / 100: avg data time: 5.45e-02, avg batch time: 0.4972, average train loss: 0.2952
[09/26 19:32:19 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1676, average loss: 3.9386
[09/26 19:32:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 87.00	
[09/26 19:32:19 visual_prompt]: Training 56 / 100 epoch, with learning rate 0.025
[09/26 19:32:26 visual_prompt]: Epoch 56 / 100: avg data time: 5.90e-02, avg batch time: 0.5037, average train loss: 0.2264
[09/26 19:32:28 visual_prompt]: Inference (val):avg data time: 4.26e-05, avg batch time: 0.1673, average loss: 4.0116
[09/26 19:32:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 89.00	
[09/26 19:32:28 visual_prompt]: Training 57 / 100 epoch, with learning rate 0.024127512582437484
[09/26 19:32:35 visual_prompt]: Epoch 57 / 100: avg data time: 6.11e-02, avg batch time: 0.5050, average train loss: 0.1702
[09/26 19:32:37 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1677, average loss: 4.5707
[09/26 19:32:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 86.50	
[09/26 19:32:37 visual_prompt]: Training 58 / 100 epoch, with learning rate 0.02325608815639687
[09/26 19:32:43 visual_prompt]: Epoch 58 / 100: avg data time: 5.78e-02, avg batch time: 0.5026, average train loss: 0.1367
[09/26 19:32:45 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1675, average loss: 4.6758
[09/26 19:32:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 88.00	
[09/26 19:32:45 visual_prompt]: Training 59 / 100 epoch, with learning rate 0.02238678841830867
[09/26 19:32:52 visual_prompt]: Epoch 59 / 100: avg data time: 6.38e-02, avg batch time: 0.5064, average train loss: 0.1146
[09/26 19:32:54 visual_prompt]: Inference (val):avg data time: 4.27e-05, avg batch time: 0.1680, average loss: 5.2971
[09/26 19:32:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 84.50	
[09/26 19:32:54 visual_prompt]: Training 60 / 100 epoch, with learning rate 0.02152067247599837
[09/26 19:33:01 visual_prompt]: Epoch 60 / 100: avg data time: 6.41e-02, avg batch time: 0.5081, average train loss: 0.1171
[09/26 19:33:02 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1680, average loss: 5.1590
[09/26 19:33:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 87.00	
[09/26 19:33:02 visual_prompt]: Training 61 / 100 epoch, with learning rate 0.020658795558326743
[09/26 19:33:09 visual_prompt]: Epoch 61 / 100: avg data time: 6.22e-02, avg batch time: 0.5048, average train loss: 0.1016
[09/26 19:33:11 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1671, average loss: 5.5527
[09/26 19:33:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 86.50	
[09/26 19:33:11 visual_prompt]: Training 62 / 100 epoch, with learning rate 0.019802207729556022
[09/26 19:33:18 visual_prompt]: Epoch 62 / 100: avg data time: 5.94e-02, avg batch time: 0.5029, average train loss: 0.0822
[09/26 19:33:19 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1674, average loss: 5.5289
[09/26 19:33:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 88.50	
[09/26 19:33:19 visual_prompt]: Training 63 / 100 epoch, with learning rate 0.018951952610008312
[09/26 19:33:26 visual_prompt]: Epoch 63 / 100: avg data time: 4.88e-02, avg batch time: 0.4922, average train loss: 0.0952
[09/26 19:33:28 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1673, average loss: 5.6653
[09/26 19:33:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 89.00	
[09/26 19:33:28 visual_prompt]: Training 64 / 100 epoch, with learning rate 0.018109066104575022
[09/26 19:33:35 visual_prompt]: Epoch 64 / 100: avg data time: 5.30e-02, avg batch time: 0.4983, average train loss: 0.1137
[09/26 19:33:36 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1674, average loss: 5.6492
[09/26 19:33:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 85.50	
[09/26 19:33:36 visual_prompt]: Training 65 / 100 epoch, with learning rate 0.017274575140626316
[09/26 19:33:43 visual_prompt]: Epoch 65 / 100: avg data time: 5.81e-02, avg batch time: 0.5013, average train loss: 0.1012
[09/26 19:33:45 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1672, average loss: 5.7005
[09/26 19:33:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 87.00	
[09/26 19:33:45 visual_prompt]: Training 66 / 100 epoch, with learning rate 0.016449496416858285
[09/26 19:33:52 visual_prompt]: Epoch 66 / 100: avg data time: 5.68e-02, avg batch time: 0.5009, average train loss: 0.0694
[09/26 19:33:53 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1676, average loss: 5.8168
[09/26 19:33:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 87.50	
[09/26 19:33:53 visual_prompt]: Training 67 / 100 epoch, with learning rate 0.0156348351646022
[09/26 19:34:00 visual_prompt]: Epoch 67 / 100: avg data time: 6.17e-02, avg batch time: 0.5054, average train loss: 0.0655
[09/26 19:34:02 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1679, average loss: 5.8022
[09/26 19:34:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 89.50	
[09/26 19:34:02 visual_prompt]: Training 68 / 100 epoch, with learning rate 0.014831583923105
[09/26 19:34:09 visual_prompt]: Epoch 68 / 100: avg data time: 6.85e-02, avg batch time: 0.5108, average train loss: 0.0644
[09/26 19:34:11 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1680, average loss: 5.7887
[09/26 19:34:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 19:34:11 visual_prompt]: Training 69 / 100 epoch, with learning rate 0.014040721330273063
[09/26 19:34:17 visual_prompt]: Epoch 69 / 100: avg data time: 5.76e-02, avg batch time: 0.5003, average train loss: 0.0834
[09/26 19:34:19 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1674, average loss: 6.0197
[09/26 19:34:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 86.50	
[09/26 19:34:19 visual_prompt]: Training 70 / 100 epoch, with learning rate 0.013263210930352737
[09/26 19:34:26 visual_prompt]: Epoch 70 / 100: avg data time: 6.08e-02, avg batch time: 0.5031, average train loss: 0.0550
[09/26 19:34:28 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1675, average loss: 6.0456
[09/26 19:34:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.50	
[09/26 19:34:28 visual_prompt]: Training 71 / 100 epoch, with learning rate 0.012500000000000006
[09/26 19:34:34 visual_prompt]: Epoch 71 / 100: avg data time: 5.21e-02, avg batch time: 0.4956, average train loss: 0.0483
[09/26 19:34:36 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1675, average loss: 5.9392
[09/26 19:34:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 89.50	
[09/26 19:34:36 visual_prompt]: Training 72 / 100 epoch, with learning rate 0.01175201839416988
[09/26 19:34:43 visual_prompt]: Epoch 72 / 100: avg data time: 5.04e-02, avg batch time: 0.4937, average train loss: 0.0369
[09/26 19:34:44 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1675, average loss: 6.0627
[09/26 19:34:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 88.50	
[09/26 19:34:44 visual_prompt]: Training 73 / 100 epoch, with learning rate 0.011020177413231334
[09/26 19:34:51 visual_prompt]: Epoch 73 / 100: avg data time: 5.22e-02, avg batch time: 0.4963, average train loss: 0.0339
[09/26 19:34:53 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1671, average loss: 6.2003
[09/26 19:34:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 86.50	
[09/26 19:34:53 visual_prompt]: Training 74 / 100 epoch, with learning rate 0.010305368692688175
[09/26 19:35:00 visual_prompt]: Epoch 74 / 100: avg data time: 5.68e-02, avg batch time: 0.4991, average train loss: 0.0428
[09/26 19:35:01 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1676, average loss: 6.1353
[09/26 19:35:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 86.50	
[09/26 19:35:01 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.009608463116858543
[09/26 19:35:08 visual_prompt]: Epoch 75 / 100: avg data time: 4.77e-02, avg batch time: 0.4932, average train loss: 0.0221
[09/26 19:35:10 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1676, average loss: 6.0748
[09/26 19:35:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 88.00	
[09/26 19:35:10 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.008930309757836517
[09/26 19:35:16 visual_prompt]: Epoch 76 / 100: avg data time: 5.15e-02, avg batch time: 0.4946, average train loss: 0.0205
[09/26 19:35:18 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1673, average loss: 6.2533
[09/26 19:35:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 87.50	
[09/26 19:35:18 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.008271734841028553
[09/26 19:35:25 visual_prompt]: Epoch 77 / 100: avg data time: 4.99e-02, avg batch time: 0.4947, average train loss: 0.0199
[09/26 19:35:26 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1673, average loss: 6.1853
[09/26 19:35:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 88.00	
[09/26 19:35:26 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.007633540738525066
[09/26 19:35:33 visual_prompt]: Epoch 78 / 100: avg data time: 5.91e-02, avg batch time: 0.5033, average train loss: 0.0197
[09/26 19:35:35 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1678, average loss: 6.3102
[09/26 19:35:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 88.50	
[09/26 19:35:35 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.0070165049915337264
[09/26 19:35:42 visual_prompt]: Epoch 79 / 100: avg data time: 4.98e-02, avg batch time: 0.4962, average train loss: 0.0220
[09/26 19:35:43 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1678, average loss: 6.3123
[09/26 19:35:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 87.50	
[09/26 19:35:43 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.006421379363065142
[09/26 19:35:50 visual_prompt]: Epoch 80 / 100: avg data time: 5.08e-02, avg batch time: 0.4982, average train loss: 0.0215
[09/26 19:35:52 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1682, average loss: 6.3329
[09/26 19:35:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 88.00	
[09/26 19:35:52 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.005848888922025553
[09/26 19:35:59 visual_prompt]: Epoch 81 / 100: avg data time: 5.24e-02, avg batch time: 0.5017, average train loss: 0.0190
[09/26 19:36:00 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1690, average loss: 6.3632
[09/26 19:36:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 88.00	
[09/26 19:36:00 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.005299731159831953
[09/26 19:36:07 visual_prompt]: Epoch 82 / 100: avg data time: 5.99e-02, avg batch time: 0.5094, average train loss: 0.0190
[09/26 19:36:09 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1687, average loss: 6.3586
[09/26 19:36:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 87.50	
[09/26 19:36:09 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.004774575140626317
[09/26 19:36:16 visual_prompt]: Epoch 83 / 100: avg data time: 5.60e-02, avg batch time: 0.5079, average train loss: 0.0137
[09/26 19:36:17 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1700, average loss: 6.5425
[09/26 19:36:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 87.00	
[09/26 19:36:17 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.004274060686123959
[09/26 19:36:24 visual_prompt]: Epoch 84 / 100: avg data time: 5.93e-02, avg batch time: 0.5127, average train loss: 0.0117
[09/26 19:36:26 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1706, average loss: 6.5450
[09/26 19:36:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 87.00	
[09/26 19:36:26 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.003798797596089351
[09/26 19:36:33 visual_prompt]: Epoch 85 / 100: avg data time: 5.09e-02, avg batch time: 0.5051, average train loss: 0.0188
[09/26 19:36:34 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1711, average loss: 6.4819
[09/26 19:36:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 87.00	
[09/26 19:36:34 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.0033493649053890325
[09/26 19:36:41 visual_prompt]: Epoch 86 / 100: avg data time: 6.00e-02, avg batch time: 0.5160, average train loss: 0.0099
[09/26 19:36:43 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1715, average loss: 6.4835
[09/26 19:36:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 87.00	
[09/26 19:36:43 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.0029263101785268254
[09/26 19:36:50 visual_prompt]: Epoch 87 / 100: avg data time: 5.18e-02, avg batch time: 0.5088, average train loss: 0.0122
[09/26 19:36:52 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1716, average loss: 6.4969
[09/26 19:36:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 19:36:52 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.00253014884252083
[09/26 19:36:59 visual_prompt]: Epoch 88 / 100: avg data time: 5.10e-02, avg batch time: 0.5083, average train loss: 0.0106
[09/26 19:37:00 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1718, average loss: 6.5179
[09/26 19:37:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 88.00	
[09/26 19:37:00 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.0021613635589349755
[09/26 19:37:07 visual_prompt]: Epoch 89 / 100: avg data time: 4.95e-02, avg batch time: 0.5078, average train loss: 0.0116
[09/26 19:37:09 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1718, average loss: 6.5819
[09/26 19:37:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 87.50	
[09/26 19:37:09 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.0018204036358303172
[09/26 19:37:16 visual_prompt]: Epoch 90 / 100: avg data time: 6.32e-02, avg batch time: 0.5206, average train loss: 0.0186
[09/26 19:37:18 visual_prompt]: Inference (val):avg data time: 4.66e-05, avg batch time: 0.1713, average loss: 6.5524
[09/26 19:37:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 19:37:18 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.0015076844803522923
[09/26 19:37:25 visual_prompt]: Epoch 91 / 100: avg data time: 5.77e-02, avg batch time: 0.5152, average train loss: 0.0129
[09/26 19:37:26 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1714, average loss: 6.5378
[09/26 19:37:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 19:37:26 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.0012235870926211618
[09/26 19:37:33 visual_prompt]: Epoch 92 / 100: avg data time: 5.24e-02, avg batch time: 0.5089, average train loss: 0.0117
[09/26 19:37:35 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1717, average loss: 6.5449
[09/26 19:37:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 88.00	
[09/26 19:37:35 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.0009684576015420277
[09/26 19:37:42 visual_prompt]: Epoch 93 / 100: avg data time: 4.98e-02, avg batch time: 0.5056, average train loss: 0.0116
[09/26 19:37:43 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1711, average loss: 6.5440
[09/26 19:37:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 19:37:43 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.0007426068431000883
[09/26 19:37:50 visual_prompt]: Epoch 94 / 100: avg data time: 5.21e-02, avg batch time: 0.5074, average train loss: 0.0092
[09/26 19:37:52 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1710, average loss: 6.5453
[09/26 19:37:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 19:37:52 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.0005463099816548578
[09/26 19:37:59 visual_prompt]: Epoch 95 / 100: avg data time: 5.83e-02, avg batch time: 0.5124, average train loss: 0.0127
[09/26 19:38:01 visual_prompt]: Inference (val):avg data time: 4.23e-05, avg batch time: 0.1710, average loss: 6.5452
[09/26 19:38:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 19:38:01 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.00037980617469479954
[09/26 19:38:08 visual_prompt]: Epoch 96 / 100: avg data time: 6.50e-02, avg batch time: 0.5189, average train loss: 0.0188
[09/26 19:38:09 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1702, average loss: 6.5555
[09/26 19:38:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 19:38:09 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.00024329828146074097
[09/26 19:38:16 visual_prompt]: Epoch 97 / 100: avg data time: 5.74e-02, avg batch time: 0.5106, average train loss: 0.0095
[09/26 19:38:18 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1700, average loss: 6.5592
[09/26 19:38:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 19:38:18 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.00013695261579316776
[09/26 19:38:25 visual_prompt]: Epoch 98 / 100: avg data time: 4.84e-02, avg batch time: 0.5022, average train loss: 0.0101
[09/26 19:38:27 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1700, average loss: 6.5590
[09/26 19:38:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 19:38:27 visual_prompt]: Training 99 / 100 epoch, with learning rate 6.089874350439506e-05
[09/26 19:38:34 visual_prompt]: Epoch 99 / 100: avg data time: 5.89e-02, avg batch time: 0.5129, average train loss: 0.0115
[09/26 19:38:35 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1699, average loss: 6.5594
[09/26 19:38:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
[09/26 19:38:35 visual_prompt]: Training 100 / 100 epoch, with learning rate 1.5229324522605948e-05
[09/26 19:38:42 visual_prompt]: Epoch 100 / 100: avg data time: 6.02e-02, avg batch time: 0.5125, average train loss: 0.0070
[09/26 19:38:44 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1696, average loss: 6.5595
[09/26 19:38:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 88.00	
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 289, in <module>
    if __name__ == '__main__':
        ^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 284, in main
    # already ran
^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 113, in train
    seed(cfg)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_vtab.py", line 136, in seed
    torch.manual_seed(SEED)
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/random.py", line 36, in manual_seed
    seed = int(seed)
           ^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'tuple'

/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/13 19:14:08 visual_prompt]: Rank of current process: 0. World size: 1
[11/13 19:14:08 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/13 19:14:08 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/13 19:14:08 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/13 19:14:08 visual_prompt]: Training with config:
[11/13 19:14:08 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/13 19:14:08 visual_prompt]: Loading training data...
[11/13 19:14:08 visual_prompt]: Constructing mammo-cbis dataset train...
[11/13 19:14:08 visual_prompt]: Loading validation data...
[11/13 19:14:08 visual_prompt]: Constructing mammo-cbis dataset val...
[11/13 19:14:08 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/13 19:14:10 visual_prompt]: Enable all parameters update during training
[11/13 19:14:10 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/13 19:14:10 visual_prompt]: tuned percent:100.000
[11/13 19:14:11 visual_prompt]: Device used for model: 0
[11/13 19:14:11 visual_prompt]: Setting up Evaluator...
[11/13 19:14:11 visual_prompt]: Setting up Trainer...
[11/13 19:14:11 visual_prompt]: 	Setting up the optimizer...
[11/13 19:14:11 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/13 19:20:43 visual_prompt]: Epoch 1 / 100: avg data time: 1.07e+01, avg batch time: 11.2214, average train loss: 6.9791
[11/13 19:21:27 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1766, average loss: 6.3857
[11/13 19:21:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/13 19:21:27 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/13 19:27:53 visual_prompt]: Epoch 2 / 100: avg data time: 1.05e+01, avg batch time: 11.0261, average train loss: 3.1853
[11/13 19:28:37 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1741, average loss: 0.8753
[11/13 19:28:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.69	
[11/13 19:28:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/13 19:35:03 visual_prompt]: Epoch 3 / 100: avg data time: 1.05e+01, avg batch time: 11.0304, average train loss: 0.9599
[11/13 19:35:47 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1805, average loss: 0.7334
[11/13 19:35:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.81	
[11/13 19:35:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/13 19:42:13 visual_prompt]: Epoch 4 / 100: avg data time: 1.05e+01, avg batch time: 11.0275, average train loss: 0.7770
[11/13 19:42:57 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1699, average loss: 0.6492
[11/13 19:42:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.12	
[11/13 19:42:57 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/13 19:49:23 visual_prompt]: Epoch 5 / 100: avg data time: 1.05e+01, avg batch time: 11.0258, average train loss: 0.7371
[11/13 19:50:06 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1737, average loss: 0.7053
[11/13 19:50:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 69.47	
[11/13 19:50:06 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/13 19:56:33 visual_prompt]: Epoch 6 / 100: avg data time: 1.05e+01, avg batch time: 11.0431, average train loss: 0.7302
[11/13 19:57:16 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1773, average loss: 0.6638
[11/13 19:57:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.38	
[11/13 19:57:16 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/13 20:03:43 visual_prompt]: Epoch 7 / 100: avg data time: 1.05e+01, avg batch time: 11.0401, average train loss: 0.6865
[11/13 20:04:26 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1736, average loss: 0.6231
[11/13 20:04:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.25	
[11/13 20:04:26 visual_prompt]: Best epoch 7: best metric: -0.623
[11/13 20:04:26 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/13 20:10:53 visual_prompt]: Epoch 8 / 100: avg data time: 1.05e+01, avg batch time: 11.0429, average train loss: 0.6633
[11/13 20:11:37 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1767, average loss: 0.6259
[11/13 20:11:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.63	
[11/13 20:11:37 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/13 20:18:03 visual_prompt]: Epoch 9 / 100: avg data time: 1.05e+01, avg batch time: 11.0242, average train loss: 0.6471
[11/13 20:18:47 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1748, average loss: 0.8654
[11/13 20:18:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 71.31	
[11/13 20:18:47 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/13 20:25:13 visual_prompt]: Epoch 10 / 100: avg data time: 1.05e+01, avg batch time: 11.0286, average train loss: 0.6220
[11/13 20:25:56 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1767, average loss: 0.8537
[11/13 20:25:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 69.46	
[11/13 20:25:56 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/13 20:32:23 visual_prompt]: Epoch 11 / 100: avg data time: 1.05e+01, avg batch time: 11.0371, average train loss: 0.5798
[11/13 20:33:06 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1681, average loss: 0.9848
[11/13 20:33:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 70.86	
[11/13 20:33:07 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/13 20:39:33 visual_prompt]: Epoch 12 / 100: avg data time: 1.05e+01, avg batch time: 11.0299, average train loss: 0.5611
[11/13 20:40:16 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1685, average loss: 0.8405
[11/13 20:40:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 73.09	
[11/13 20:40:16 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/13 20:46:43 visual_prompt]: Epoch 13 / 100: avg data time: 1.05e+01, avg batch time: 11.0343, average train loss: 0.5306
[11/13 20:47:26 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1757, average loss: 0.8182
[11/13 20:47:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.86	
[11/13 20:47:26 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/13 20:53:53 visual_prompt]: Epoch 14 / 100: avg data time: 1.05e+01, avg batch time: 11.0551, average train loss: 0.5167
[11/13 20:54:37 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1696, average loss: 0.7376
[11/13 20:54:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.13	
[11/13 20:54:37 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/13 21:01:04 visual_prompt]: Epoch 15 / 100: avg data time: 1.05e+01, avg batch time: 11.0508, average train loss: 0.4869
[11/13 21:01:48 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1797, average loss: 0.6656
[11/13 21:01:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.43	
[11/13 21:01:48 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/13 21:08:14 visual_prompt]: Epoch 16 / 100: avg data time: 1.05e+01, avg batch time: 11.0339, average train loss: 0.4100
[11/13 21:08:57 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1698, average loss: 0.7873
[11/13 21:08:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.13	
[11/13 21:08:57 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/13 21:15:24 visual_prompt]: Epoch 17 / 100: avg data time: 1.05e+01, avg batch time: 11.0336, average train loss: 0.4407
[11/13 21:16:07 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1757, average loss: 1.0832
[11/13 21:16:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 69.68	
[11/13 21:16:07 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/13 21:22:34 visual_prompt]: Epoch 18 / 100: avg data time: 1.05e+01, avg batch time: 11.0328, average train loss: 0.3789
[11/13 21:23:17 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1709, average loss: 0.7727
[11/13 21:23:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 72.00	
[11/13 21:23:17 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/13 21:29:44 visual_prompt]: Epoch 19 / 100: avg data time: 1.05e+01, avg batch time: 11.0512, average train loss: 0.2827
[11/13 21:30:28 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1788, average loss: 0.8068
[11/13 21:30:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.25	
[11/13 21:30:28 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/13 21:36:54 visual_prompt]: Epoch 20 / 100: avg data time: 1.05e+01, avg batch time: 11.0401, average train loss: 0.2560
[11/13 21:37:38 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1761, average loss: 1.2641
[11/13 21:37:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.72	
[11/13 21:37:38 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/13 21:44:04 visual_prompt]: Epoch 21 / 100: avg data time: 1.05e+01, avg batch time: 11.0386, average train loss: 0.2641
[11/13 21:44:48 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1726, average loss: 1.0899
[11/13 21:44:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.41	
[11/13 21:44:48 visual_prompt]: Stopping early.
[11/13 21:44:48 visual_prompt]: Rank of current process: 0. World size: 1
[11/13 21:44:48 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/13 21:44:48 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/13 21:44:48 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/13 21:44:48 visual_prompt]: Training with config:
[11/13 21:44:48 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/13 21:44:48 visual_prompt]: Loading training data...
[11/13 21:44:48 visual_prompt]: Constructing mammo-cbis dataset train...
[11/13 21:44:48 visual_prompt]: Loading validation data...
[11/13 21:44:48 visual_prompt]: Constructing mammo-cbis dataset val...
[11/13 21:44:48 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/13 21:44:49 visual_prompt]: Enable all parameters update during training
[11/13 21:44:49 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/13 21:44:49 visual_prompt]: tuned percent:100.000
[11/13 21:44:50 visual_prompt]: Device used for model: 0
[11/13 21:44:50 visual_prompt]: Setting up Evaluator...
[11/13 21:44:50 visual_prompt]: Setting up Trainer...
[11/13 21:44:50 visual_prompt]: 	Setting up the optimizer...
[11/13 21:44:50 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/13 21:51:13 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9545, average train loss: 6.9791
[11/13 21:51:57 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1765, average loss: 6.3857
[11/13 21:51:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/13 21:51:57 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/13 21:58:18 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.8985, average train loss: 3.1853
[11/13 21:59:01 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1723, average loss: 0.8753
[11/13 21:59:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.69	
[11/13 21:59:01 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/13 22:05:23 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.8990, average train loss: 0.9599
[11/13 22:06:06 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1751, average loss: 0.7334
[11/13 22:06:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.81	
[11/13 22:06:06 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/13 22:12:28 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.8947, average train loss: 0.7770
[11/13 22:13:11 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1787, average loss: 0.6492
[11/13 22:13:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.12	
[11/13 22:13:11 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/13 22:19:32 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.8840, average train loss: 0.7371
[11/13 22:20:16 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1720, average loss: 0.7053
[11/13 22:20:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 69.47	
[11/13 22:20:16 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/13 22:26:38 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9055, average train loss: 0.7302
[11/13 22:27:21 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1722, average loss: 0.6638
[11/13 22:27:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.38	
[11/13 22:27:21 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/13 22:33:43 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9027, average train loss: 0.6865
[11/13 22:34:26 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1706, average loss: 0.6231
[11/13 22:34:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.25	
[11/13 22:34:26 visual_prompt]: Best epoch 7: best metric: -0.623
[11/13 22:34:26 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/13 22:40:48 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9043, average train loss: 0.6633
[11/13 22:41:31 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1724, average loss: 0.6259
[11/13 22:41:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.63	
[11/13 22:41:31 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/13 22:47:53 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.9048, average train loss: 0.6471
[11/13 22:48:36 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1699, average loss: 0.8654
[11/13 22:48:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 71.31	
[11/13 22:48:36 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/13 22:54:58 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.9010, average train loss: 0.6220
[11/13 22:55:41 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1775, average loss: 0.8537
[11/13 22:55:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 69.46	
[11/13 22:55:41 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/13 23:02:03 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.9109, average train loss: 0.5798
[11/13 23:02:47 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1780, average loss: 0.9848
[11/13 23:02:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 70.86	
[11/13 23:02:47 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/13 23:09:08 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.9031, average train loss: 0.5611
[11/13 23:09:52 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1776, average loss: 0.8405
[11/13 23:09:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 73.09	
[11/13 23:09:52 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/13 23:16:14 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.9091, average train loss: 0.5306
[11/13 23:16:57 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1757, average loss: 0.8182
[11/13 23:16:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.86	
[11/13 23:16:57 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/13 23:23:19 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.9102, average train loss: 0.5167
[11/13 23:24:03 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1673, average loss: 0.7376
[11/13 23:24:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.13	
[11/13 23:24:03 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/13 23:30:25 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9158, average train loss: 0.4869
[11/13 23:31:08 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1753, average loss: 0.6656
[11/13 23:31:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.43	
[11/13 23:31:08 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/13 23:37:30 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.8979, average train loss: 0.4100
[11/13 23:38:13 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.1767, average loss: 0.7873
[11/13 23:38:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.13	
[11/13 23:38:13 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/13 23:44:35 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.8946, average train loss: 0.4407
[11/13 23:45:18 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1677, average loss: 1.0832
[11/13 23:45:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 69.68	
[11/13 23:45:18 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/13 23:51:39 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.8916, average train loss: 0.3789
[11/13 23:52:23 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1775, average loss: 0.7727
[11/13 23:52:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 72.00	
[11/13 23:52:23 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/13 23:58:45 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9133, average train loss: 0.2827
[11/13 23:59:28 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1712, average loss: 0.8068
[11/13 23:59:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.25	
[11/13 23:59:28 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/14 00:05:50 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.8945, average train loss: 0.2560
[11/14 00:06:33 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1752, average loss: 1.2641
[11/14 00:06:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.72	
[11/14 00:06:33 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/14 00:12:55 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.9062, average train loss: 0.2641
[11/14 00:13:38 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1748, average loss: 1.0899
[11/14 00:13:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.41	
[11/14 00:13:38 visual_prompt]: Stopping early.
[11/14 00:13:38 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 00:13:38 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 00:13:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/14 00:13:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 00:13:38 visual_prompt]: Training with config:
[11/14 00:13:38 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 00:13:38 visual_prompt]: Loading training data...
[11/14 00:13:38 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 00:13:38 visual_prompt]: Loading validation data...
[11/14 00:13:38 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 00:13:38 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 00:13:40 visual_prompt]: Enable all parameters update during training
[11/14 00:13:40 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 00:13:40 visual_prompt]: tuned percent:100.000
[11/14 00:13:40 visual_prompt]: Device used for model: 0
[11/14 00:13:40 visual_prompt]: Setting up Evaluator...
[11/14 00:13:40 visual_prompt]: Setting up Trainer...
[11/14 00:13:40 visual_prompt]: 	Setting up the optimizer...
[11/14 00:13:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 00:20:01 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.8929, average train loss: 6.9791
[11/14 00:20:45 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.1784, average loss: 6.3857
[11/14 00:20:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 00:20:45 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/14 00:27:06 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.8895, average train loss: 3.1853
[11/14 00:27:50 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1695, average loss: 0.8753
[11/14 00:27:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.69	
[11/14 00:27:50 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/14 00:34:11 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.8879, average train loss: 0.9599
[11/14 00:34:54 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1756, average loss: 0.7334
[11/14 00:34:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 62.81	
[11/14 00:34:54 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/14 00:41:16 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.8876, average train loss: 0.7770
[11/14 00:41:59 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1787, average loss: 0.6492
[11/14 00:41:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 66.12	
[11/14 00:41:59 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/14 00:48:20 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.8859, average train loss: 0.7371
[11/14 00:49:04 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1698, average loss: 0.7053
[11/14 00:49:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 69.47	
[11/14 00:49:04 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/14 00:55:25 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.8932, average train loss: 0.7302
[11/14 00:56:09 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1711, average loss: 0.6638
[11/14 00:56:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 68.38	
[11/14 00:56:09 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/14 01:02:30 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.8993, average train loss: 0.6865
[11/14 01:03:14 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1686, average loss: 0.6231
[11/14 01:03:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 69.25	
[11/14 01:03:14 visual_prompt]: Best epoch 7: best metric: -0.623
[11/14 01:03:14 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/14 01:09:36 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9025, average train loss: 0.6633
[11/14 01:10:19 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1699, average loss: 0.6259
[11/14 01:10:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 70.63	
[11/14 01:10:19 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/14 01:16:40 visual_prompt]: Epoch 9 / 100: avg data time: 1.03e+01, avg batch time: 10.8736, average train loss: 0.6471
[11/14 01:17:23 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1683, average loss: 0.8654
[11/14 01:17:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 71.31	
[11/14 01:17:23 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/14 01:23:44 visual_prompt]: Epoch 10 / 100: avg data time: 1.03e+01, avg batch time: 10.8781, average train loss: 0.6220
[11/14 01:24:28 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1757, average loss: 0.8537
[11/14 01:24:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 69.46	
[11/14 01:24:28 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/14 01:30:49 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.8832, average train loss: 0.5798
[11/14 01:31:32 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1763, average loss: 0.9848
[11/14 01:31:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 70.86	
[11/14 01:31:32 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/14 01:37:54 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.8884, average train loss: 0.5611
[11/14 01:38:37 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1773, average loss: 0.8405
[11/14 01:38:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 73.09	
[11/14 01:38:37 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/14 01:44:58 visual_prompt]: Epoch 13 / 100: avg data time: 1.03e+01, avg batch time: 10.8734, average train loss: 0.5306
[11/14 01:45:41 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1772, average loss: 0.8182
[11/14 01:45:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 70.86	
[11/14 01:45:41 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/14 01:52:03 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.8883, average train loss: 0.5167
[11/14 01:52:46 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1721, average loss: 0.7376
[11/14 01:52:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 69.13	
[11/14 01:52:46 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/14 01:59:08 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9001, average train loss: 0.4869
[11/14 01:59:51 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1728, average loss: 0.6656
[11/14 01:59:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 70.43	
[11/14 01:59:51 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/14 02:06:12 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.8868, average train loss: 0.4100
[11/14 02:06:56 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1807, average loss: 0.7873
[11/14 02:06:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 64.13	
[11/14 02:06:56 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/14 02:13:18 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.8941, average train loss: 0.4407
[11/14 02:14:01 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1767, average loss: 1.0832
[11/14 02:14:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 69.68	
[11/14 02:14:01 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/14 02:20:22 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.8866, average train loss: 0.3789
[11/14 02:21:06 visual_prompt]: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1698, average loss: 0.7727
[11/14 02:21:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.85	rocauc: 72.00	
[11/14 02:21:06 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/14 02:27:28 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9075, average train loss: 0.2827
[11/14 02:28:11 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1739, average loss: 0.8068
[11/14 02:28:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.25	
[11/14 02:28:11 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/14 02:34:32 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.8883, average train loss: 0.2560
[11/14 02:35:16 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1804, average loss: 1.2641
[11/14 02:35:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.72	
[11/14 02:35:16 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/14 02:41:37 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.8970, average train loss: 0.2641
[11/14 02:42:21 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1785, average loss: 1.0899
[11/14 02:42:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 71.41	
[11/14 02:42:21 visual_prompt]: Stopping early.
[11/14 02:42:21 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 02:42:21 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 02:42:21 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/14 02:42:21 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 02:42:21 visual_prompt]: Training with config:
[11/14 02:42:21 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0005_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 02:42:21 visual_prompt]: Loading training data...
[11/14 02:42:21 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 02:42:21 visual_prompt]: Loading validation data...
[11/14 02:42:21 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 02:42:21 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 02:42:23 visual_prompt]: Enable all parameters update during training
[11/14 02:42:23 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 02:42:23 visual_prompt]: tuned percent:100.000
[11/14 02:42:23 visual_prompt]: Device used for model: 0
[11/14 02:42:23 visual_prompt]: Setting up Evaluator...
[11/14 02:42:23 visual_prompt]: Setting up Trainer...
[11/14 02:42:23 visual_prompt]: 	Setting up the optimizer...
[11/14 02:42:23 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 02:48:45 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9010, average train loss: 6.9791
[11/14 02:49:28 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1698, average loss: 6.3857
[11/14 02:49:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 02:49:28 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.0001
[11/14 02:55:46 visual_prompt]: Epoch 2 / 100: avg data time: 1.02e+01, avg batch time: 10.7808, average train loss: 3.7696
[11/14 02:56:29 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1769, average loss: 0.8737
[11/14 02:56:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 52.91	
[11/14 02:56:29 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.0002
[11/14 03:02:51 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9041, average train loss: 0.9347
[11/14 03:03:34 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1698, average loss: 0.7837
[11/14 03:03:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 53.09	
[11/14 03:03:34 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.0003
[11/14 03:09:56 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.8833, average train loss: 0.8079
[11/14 03:10:39 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1770, average loss: 0.7091
[11/14 03:10:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 53.89	
[11/14 03:10:39 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.0004
[11/14 03:17:00 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.8874, average train loss: 0.7853
[11/14 03:17:44 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1791, average loss: 0.7754
[11/14 03:17:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 56.30	
[11/14 03:17:44 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0005
[11/14 03:24:06 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9041, average train loss: 0.7465
[11/14 03:24:49 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1767, average loss: 0.6865
[11/14 03:24:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 58.04	
[11/14 03:24:49 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/14 03:31:11 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.8964, average train loss: 0.7096
[11/14 03:31:54 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1742, average loss: 0.6843
[11/14 03:31:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 59.42	
[11/14 03:31:54 visual_prompt]: Best epoch 7: best metric: -0.684
[11/14 03:31:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/14 03:38:16 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9056, average train loss: 0.7223
[11/14 03:39:00 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1740, average loss: 0.6571
[11/14 03:39:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 59.36	
[11/14 03:39:00 visual_prompt]: Best epoch 8: best metric: -0.657
[11/14 03:39:00 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/14 03:45:21 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.8952, average train loss: 0.6881
[11/14 03:46:05 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1697, average loss: 0.7079
[11/14 03:46:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.67	
[11/14 03:46:05 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/14 03:52:26 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.8926, average train loss: 0.7266
[11/14 03:53:10 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1782, average loss: 0.8433
[11/14 03:53:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.97	rocauc: 59.97	
[11/14 03:53:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/14 03:59:31 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.8900, average train loss: 0.6806
[11/14 04:00:14 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1740, average loss: 0.6778
[11/14 04:00:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 61.03	
[11/14 04:00:14 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/14 04:06:36 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.8889, average train loss: 0.6837
[11/14 04:07:19 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1753, average loss: 0.9075
[11/14 04:07:19 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 61.03	
[11/14 04:07:19 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/14 04:13:36 visual_prompt]: Epoch 13 / 100: avg data time: 1.02e+01, avg batch time: 10.7685, average train loss: 0.6630
[11/14 04:14:20 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1784, average loss: 0.6550
[11/14 04:14:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.04	rocauc: 61.25	
[11/14 04:14:20 visual_prompt]: Best epoch 13: best metric: -0.655
[11/14 04:14:20 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/14 04:20:41 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.8892, average train loss: 0.6617
[11/14 04:21:25 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1736, average loss: 0.6443
[11/14 04:21:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 61.14	
[11/14 04:21:25 visual_prompt]: Best epoch 14: best metric: -0.644
[11/14 04:21:25 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/14 04:27:47 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9112, average train loss: 0.6704
[11/14 04:28:30 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1767, average loss: 0.7420
[11/14 04:28:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 61.24	
[11/14 04:28:30 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/14 04:34:52 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.8907, average train loss: 0.6734
[11/14 04:35:35 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1767, average loss: 0.6462
[11/14 04:35:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 62.21	
[11/14 04:35:35 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/14 04:41:57 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.8905, average train loss: 0.6687
[11/14 04:42:40 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1762, average loss: 0.7409
[11/14 04:42:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.45	
[11/14 04:42:40 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/14 04:49:02 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.8958, average train loss: 0.6681
[11/14 04:49:45 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1731, average loss: 0.7146
[11/14 04:49:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 62.45	
[11/14 04:49:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/14 04:56:07 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9125, average train loss: 0.6348
[11/14 04:56:51 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1744, average loss: 0.6495
[11/14 04:56:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 62.18	
[11/14 04:56:51 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/14 05:03:13 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.9071, average train loss: 0.6323
[11/14 05:03:56 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1725, average loss: 0.6532
[11/14 05:03:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 61.87	
[11/14 05:03:56 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/14 05:10:18 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.9018, average train loss: 0.6121
[11/14 05:11:02 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1760, average loss: 0.6540
[11/14 05:11:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 61.38	
[11/14 05:11:02 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/14 05:17:23 visual_prompt]: Epoch 22 / 100: avg data time: 1.04e+01, avg batch time: 10.9031, average train loss: 0.6386
[11/14 05:18:07 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1740, average loss: 0.7101
[11/14 05:18:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 60.68	
[11/14 05:18:07 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.00046152381310523384
[11/14 05:24:28 visual_prompt]: Epoch 23 / 100: avg data time: 1.04e+01, avg batch time: 10.8926, average train loss: 0.6404
[11/14 05:25:12 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1696, average loss: 0.7702
[11/14 05:25:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.74	
[11/14 05:25:12 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.000457002207787005
[11/14 05:31:34 visual_prompt]: Epoch 24 / 100: avg data time: 1.04e+01, avg batch time: 10.9068, average train loss: 0.6111
[11/14 05:32:17 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1678, average loss: 0.6646
[11/14 05:32:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 61.47	
[11/14 05:32:17 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0004522542485937369
[11/14 05:38:39 visual_prompt]: Epoch 25 / 100: avg data time: 1.04e+01, avg batch time: 10.8944, average train loss: 0.6119
[11/14 05:39:22 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1766, average loss: 0.6632
[11/14 05:39:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 60.90	
[11/14 05:39:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.00044728512734909845
[11/14 05:45:44 visual_prompt]: Epoch 26 / 100: avg data time: 1.04e+01, avg batch time: 10.9032, average train loss: 0.6072
[11/14 05:46:28 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1755, average loss: 0.6658
[11/14 05:46:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 62.08	
[11/14 05:46:28 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.0004421002777142148
[11/14 05:52:49 visual_prompt]: Epoch 27 / 100: avg data time: 1.04e+01, avg batch time: 10.8931, average train loss: 0.6085
[11/14 05:53:33 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1776, average loss: 0.6609
[11/14 05:53:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 62.12	
[11/14 05:53:33 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.0004367053692460385
[11/14 05:59:54 visual_prompt]: Epoch 28 / 100: avg data time: 1.04e+01, avg batch time: 10.8980, average train loss: 0.6329
[11/14 06:00:38 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1741, average loss: 0.6926
[11/14 06:00:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 60.52	
[11/14 06:00:38 visual_prompt]: Stopping early.
[11/14 06:00:38 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 06:00:38 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 06:00:38 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/14 06:00:38 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 06:00:38 visual_prompt]: Training with config:
[11/14 06:00:38 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.01/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 06:00:38 visual_prompt]: Loading training data...
[11/14 06:00:38 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 06:00:38 visual_prompt]: Loading validation data...
[11/14 06:00:38 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 06:00:38 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 06:00:40 visual_prompt]: Enable all parameters update during training
[11/14 06:00:40 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 06:00:40 visual_prompt]: tuned percent:100.000
[11/14 06:00:40 visual_prompt]: Device used for model: 0
[11/14 06:00:40 visual_prompt]: Setting up Evaluator...
[11/14 06:00:40 visual_prompt]: Setting up Trainer...
[11/14 06:00:40 visual_prompt]: 	Setting up the optimizer...
[11/14 06:00:40 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 06:07:02 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9072, average train loss: 6.9791
[11/14 06:07:45 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1734, average loss: 6.3857
[11/14 06:07:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 06:07:45 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/14 06:14:07 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.8879, average train loss: 2.2494
[11/14 06:14:50 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1700, average loss: 0.8412
[11/14 06:14:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 51.72	
[11/14 06:14:50 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/14 06:21:12 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9001, average train loss: 0.8962
[11/14 06:21:55 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1746, average loss: 0.7044
[11/14 06:21:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.92	
[11/14 06:21:55 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/14 06:28:17 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.8978, average train loss: 0.7820
[11/14 06:29:01 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1767, average loss: 0.7135
[11/14 06:29:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.72	
[11/14 06:29:01 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/14 06:35:22 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.8889, average train loss: 0.7455
[11/14 06:36:06 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1748, average loss: 0.7162
[11/14 06:36:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.65	
[11/14 06:36:06 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/14 06:42:27 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9031, average train loss: 0.6861
[11/14 06:43:11 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1689, average loss: 0.6859
[11/14 06:43:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.65	
[11/14 06:43:11 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/14 06:49:33 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9030, average train loss: 0.6416
[11/14 06:50:17 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1802, average loss: 0.6250
[11/14 06:50:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 68.25	
[11/14 06:50:17 visual_prompt]: Best epoch 7: best metric: -0.625
[11/14 06:50:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/14 06:56:39 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9218, average train loss: 0.6677
[11/14 06:57:23 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1680, average loss: 0.8172
[11/14 06:57:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.84	
[11/14 06:57:23 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/14 07:03:44 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.8908, average train loss: 0.5855
[11/14 07:04:28 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1759, average loss: 0.6318
[11/14 07:04:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 68.24	
[11/14 07:04:28 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/14 07:10:49 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.8877, average train loss: 0.5394
[11/14 07:11:32 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1749, average loss: 0.6388
[11/14 07:11:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.99	
[11/14 07:11:32 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/14 07:17:54 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.8911, average train loss: 0.4939
[11/14 07:18:38 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1777, average loss: 0.8244
[11/14 07:18:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.68	
[11/14 07:18:38 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/14 07:24:59 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.8982, average train loss: 0.5270
[11/14 07:25:43 visual_prompt]: Inference (val):avg data time: 3.64e-05, avg batch time: 0.1776, average loss: 0.8798
[11/14 07:25:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 69.06	
[11/14 07:25:43 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/14 07:32:04 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.8919, average train loss: 0.4742
[11/14 07:32:48 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1735, average loss: 0.7313
[11/14 07:32:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.39	
[11/14 07:32:48 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/14 07:39:10 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.9053, average train loss: 0.4177
[11/14 07:39:54 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1792, average loss: 0.6655
[11/14 07:39:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.63	
[11/14 07:39:54 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/14 07:46:16 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9143, average train loss: 0.3513
[11/14 07:46:59 visual_prompt]: Inference (val):avg data time: 3.39e-05, avg batch time: 0.1782, average loss: 0.7925
[11/14 07:46:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.87	
[11/14 07:46:59 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/14 07:53:21 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.9077, average train loss: 0.2968
[11/14 07:54:05 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1746, average loss: 1.1919
[11/14 07:54:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 63.99	
[11/14 07:54:05 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/14 08:00:22 visual_prompt]: Epoch 17 / 100: avg data time: 1.02e+01, avg batch time: 10.7773, average train loss: 0.3106
[11/14 08:01:06 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1762, average loss: 1.0007
[11/14 08:01:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 66.76	
[11/14 08:01:06 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/14 08:07:27 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.8890, average train loss: 0.3144
[11/14 08:08:11 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1729, average loss: 0.9425
[11/14 08:08:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.84	
[11/14 08:08:11 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/14 08:14:33 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9148, average train loss: 0.2037
[11/14 08:15:17 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1794, average loss: 1.0811
[11/14 08:15:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 72.10	
[11/14 08:15:17 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/14 08:21:38 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.8952, average train loss: 0.1522
[11/14 08:22:22 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1764, average loss: 1.3032
[11/14 08:22:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 64.65	
[11/14 08:22:22 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/14 08:28:44 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.9126, average train loss: 0.1307
[11/14 08:29:28 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1702, average loss: 1.3926
[11/14 08:29:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 64.94	
[11/14 08:29:28 visual_prompt]: Stopping early.
[11/14 08:29:28 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 08:29:28 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 08:29:28 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/14 08:29:28 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 08:29:28 visual_prompt]: Training with config:
[11/14 08:29:28 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 08:29:28 visual_prompt]: Loading training data...
[11/14 08:29:28 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 08:29:28 visual_prompt]: Loading validation data...
[11/14 08:29:28 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 08:29:28 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 08:29:29 visual_prompt]: Enable all parameters update during training
[11/14 08:29:29 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 08:29:29 visual_prompt]: tuned percent:100.000
[11/14 08:29:29 visual_prompt]: Device used for model: 0
[11/14 08:29:29 visual_prompt]: Setting up Evaluator...
[11/14 08:29:29 visual_prompt]: Setting up Trainer...
[11/14 08:29:29 visual_prompt]: 	Setting up the optimizer...
[11/14 08:29:29 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 08:35:52 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9364, average train loss: 6.9791
[11/14 08:36:36 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1737, average loss: 6.3857
[11/14 08:36:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 08:36:36 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/14 08:42:58 visual_prompt]: Epoch 2 / 100: avg data time: 1.04e+01, avg batch time: 10.9032, average train loss: 2.2494
[11/14 08:43:41 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1762, average loss: 0.8412
[11/14 08:43:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 51.72	
[11/14 08:43:41 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/14 08:50:03 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9000, average train loss: 0.8962
[11/14 08:50:47 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1706, average loss: 0.7044
[11/14 08:50:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.92	
[11/14 08:50:47 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/14 08:57:08 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.8864, average train loss: 0.7820
[11/14 08:57:52 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1726, average loss: 0.7135
[11/14 08:57:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.72	
[11/14 08:57:52 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/14 09:04:13 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.8942, average train loss: 0.7455
[11/14 09:04:57 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1736, average loss: 0.7162
[11/14 09:04:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.65	
[11/14 09:04:57 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/14 09:11:18 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.8987, average train loss: 0.6861
[11/14 09:12:02 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1776, average loss: 0.6859
[11/14 09:12:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.65	
[11/14 09:12:02 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/14 09:18:20 visual_prompt]: Epoch 7 / 100: avg data time: 1.03e+01, avg batch time: 10.7867, average train loss: 0.6416
[11/14 09:19:03 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1758, average loss: 0.6250
[11/14 09:19:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 68.25	
[11/14 09:19:03 visual_prompt]: Best epoch 7: best metric: -0.625
[11/14 09:19:03 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/14 09:25:26 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9123, average train loss: 0.6677
[11/14 09:26:09 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1770, average loss: 0.8172
[11/14 09:26:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.84	
[11/14 09:26:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/14 09:32:31 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.9004, average train loss: 0.5855
[11/14 09:33:15 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1698, average loss: 0.6318
[11/14 09:33:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 68.24	
[11/14 09:33:15 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/14 09:39:36 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.8906, average train loss: 0.5394
[11/14 09:40:20 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1788, average loss: 0.6388
[11/14 09:40:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.99	
[11/14 09:40:20 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/14 09:46:41 visual_prompt]: Epoch 11 / 100: avg data time: 1.04e+01, avg batch time: 10.8949, average train loss: 0.4939
[11/14 09:47:25 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1741, average loss: 0.8244
[11/14 09:47:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.68	
[11/14 09:47:25 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/14 09:53:47 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.9035, average train loss: 0.5270
[11/14 09:54:30 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1754, average loss: 0.8798
[11/14 09:54:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 69.06	
[11/14 09:54:30 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/14 10:00:52 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.8894, average train loss: 0.4742
[11/14 10:01:35 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1804, average loss: 0.7313
[11/14 10:01:35 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.39	
[11/14 10:01:35 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/14 10:07:57 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.8946, average train loss: 0.4177
[11/14 10:08:40 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1720, average loss: 0.6655
[11/14 10:08:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.63	
[11/14 10:08:40 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/14 10:15:02 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9084, average train loss: 0.3513
[11/14 10:15:46 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1692, average loss: 0.7925
[11/14 10:15:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.87	
[11/14 10:15:46 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/14 10:22:07 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.8946, average train loss: 0.2968
[11/14 10:22:51 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1799, average loss: 1.1919
[11/14 10:22:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 63.99	
[11/14 10:22:51 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/14 10:29:13 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.9004, average train loss: 0.3106
[11/14 10:29:57 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1746, average loss: 1.0007
[11/14 10:29:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 66.76	
[11/14 10:29:57 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/14 10:36:18 visual_prompt]: Epoch 18 / 100: avg data time: 1.04e+01, avg batch time: 10.8913, average train loss: 0.3144
[11/14 10:37:01 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1755, average loss: 0.9425
[11/14 10:37:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.84	
[11/14 10:37:01 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/14 10:43:24 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9183, average train loss: 0.2037
[11/14 10:44:07 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1736, average loss: 1.0811
[11/14 10:44:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 72.10	
[11/14 10:44:07 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/14 10:50:29 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.8946, average train loss: 0.1522
[11/14 10:51:13 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1774, average loss: 1.3032
[11/14 10:51:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 64.65	
[11/14 10:51:13 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/14 10:57:35 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.9086, average train loss: 0.1307
[11/14 10:58:18 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1715, average loss: 1.3926
[11/14 10:58:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 64.94	
[11/14 10:58:18 visual_prompt]: Stopping early.
[11/14 10:58:19 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 10:58:19 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 10:58:19 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/14 10:58:19 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 10:58:19 visual_prompt]: Training with config:
[11/14 10:58:19 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 10:58:19 visual_prompt]: Loading training data...
[11/14 10:58:19 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 10:58:19 visual_prompt]: Loading validation data...
[11/14 10:58:19 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 10:58:19 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 10:58:20 visual_prompt]: Enable all parameters update during training
[11/14 10:58:20 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 10:58:20 visual_prompt]: tuned percent:100.000
[11/14 10:58:21 visual_prompt]: Device used for model: 0
[11/14 10:58:21 visual_prompt]: Setting up Evaluator...
[11/14 10:58:21 visual_prompt]: Setting up Trainer...
[11/14 10:58:21 visual_prompt]: 	Setting up the optimizer...
[11/14 10:58:21 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 11:04:42 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9064, average train loss: 6.9791
[11/14 11:05:26 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1757, average loss: 6.3857
[11/14 11:05:26 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 11:05:26 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/14 11:12:37 visual_prompt]: Epoch 2 / 100: avg data time: 1.18e+01, avg batch time: 12.3101, average train loss: 2.2494
[11/14 11:13:21 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1677, average loss: 0.8412
[11/14 11:13:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 51.72	
[11/14 11:13:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/14 11:19:43 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9231, average train loss: 0.8962
[11/14 11:20:27 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1700, average loss: 0.7044
[11/14 11:20:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.92	
[11/14 11:20:27 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/14 11:26:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.9017, average train loss: 0.7820
[11/14 11:27:32 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1786, average loss: 0.7135
[11/14 11:27:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.72	
[11/14 11:27:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/14 11:33:56 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.9505, average train loss: 0.7455
[11/14 11:34:40 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1758, average loss: 0.7162
[11/14 11:34:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 61.65	
[11/14 11:34:40 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/14 11:41:01 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.8960, average train loss: 0.6861
[11/14 11:41:45 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1749, average loss: 0.6859
[11/14 11:41:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 65.65	
[11/14 11:41:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/14 11:48:06 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.8939, average train loss: 0.6416
[11/14 11:48:51 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1708, average loss: 0.6250
[11/14 11:48:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.48	rocauc: 68.25	
[11/14 11:48:51 visual_prompt]: Best epoch 7: best metric: -0.625
[11/14 11:48:51 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/14 11:55:13 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9245, average train loss: 0.6677
[11/14 11:55:57 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1767, average loss: 0.8172
[11/14 11:55:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 67.84	
[11/14 11:55:57 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/14 12:02:18 visual_prompt]: Epoch 9 / 100: avg data time: 1.04e+01, avg batch time: 10.8895, average train loss: 0.5855
[11/14 12:03:02 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1731, average loss: 0.6318
[11/14 12:03:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 70.33	rocauc: 68.24	
[11/14 12:03:02 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/14 12:09:23 visual_prompt]: Epoch 10 / 100: avg data time: 1.04e+01, avg batch time: 10.8877, average train loss: 0.5394
[11/14 12:10:07 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1726, average loss: 0.6388
[11/14 12:10:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.99	
[11/14 12:10:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/14 12:16:28 visual_prompt]: Epoch 11 / 100: avg data time: 1.03e+01, avg batch time: 10.8810, average train loss: 0.4939
[11/14 12:17:12 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1784, average loss: 0.8244
[11/14 12:17:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 68.68	
[11/14 12:17:12 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/14 12:23:34 visual_prompt]: Epoch 12 / 100: avg data time: 1.04e+01, avg batch time: 10.9003, average train loss: 0.5270
[11/14 12:24:17 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1740, average loss: 0.8798
[11/14 12:24:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 69.06	
[11/14 12:24:17 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/14 12:30:38 visual_prompt]: Epoch 13 / 100: avg data time: 1.04e+01, avg batch time: 10.8855, average train loss: 0.4742
[11/14 12:31:22 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1765, average loss: 0.7313
[11/14 12:31:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.39	
[11/14 12:31:22 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/14 12:37:43 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.8926, average train loss: 0.4177
[11/14 12:38:27 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1698, average loss: 0.6655
[11/14 12:38:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 68.29	rocauc: 68.63	
[11/14 12:38:27 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/14 12:44:49 visual_prompt]: Epoch 15 / 100: avg data time: 1.04e+01, avg batch time: 10.9127, average train loss: 0.3513
[11/14 12:45:33 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1782, average loss: 0.7925
[11/14 12:45:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 66.87	
[11/14 12:45:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/14 12:51:55 visual_prompt]: Epoch 16 / 100: avg data time: 1.04e+01, avg batch time: 10.9021, average train loss: 0.2968
[11/14 12:52:39 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1684, average loss: 1.1919
[11/14 12:52:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 63.99	
[11/14 12:52:39 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/14 12:59:01 visual_prompt]: Epoch 17 / 100: avg data time: 1.04e+01, avg batch time: 10.9078, average train loss: 0.3106
[11/14 12:59:44 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1764, average loss: 1.0007
[11/14 12:59:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 66.76	
[11/14 12:59:44 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/14 13:06:02 visual_prompt]: Epoch 18 / 100: avg data time: 1.03e+01, avg batch time: 10.7845, average train loss: 0.3144
[11/14 13:06:46 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1688, average loss: 0.9425
[11/14 13:06:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.82	rocauc: 67.84	
[11/14 13:06:46 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/14 13:13:08 visual_prompt]: Epoch 19 / 100: avg data time: 1.04e+01, avg batch time: 10.9212, average train loss: 0.2037
[11/14 13:13:52 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1733, average loss: 1.0811
[11/14 13:13:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 67.89	rocauc: 72.10	
[11/14 13:13:52 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/14 13:20:14 visual_prompt]: Epoch 20 / 100: avg data time: 1.04e+01, avg batch time: 10.9065, average train loss: 0.1522
[11/14 13:20:57 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1700, average loss: 1.3032
[11/14 13:20:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 64.65	
[11/14 13:20:57 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/14 13:27:19 visual_prompt]: Epoch 21 / 100: avg data time: 1.04e+01, avg batch time: 10.9086, average train loss: 0.1307
[11/14 13:28:03 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1750, average loss: 1.3926
[11/14 13:28:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 64.94	
[11/14 13:28:03 visual_prompt]: Stopping early.
[11/14 13:28:03 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 13:28:03 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 13:28:03 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/14 13:28:03 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 13:28:03 visual_prompt]: Training with config:
[11/14 13:28:03 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/val/seed0/lr0.0001_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 13:28:03 visual_prompt]: Loading training data...
[11/14 13:28:03 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 13:28:03 visual_prompt]: Loading validation data...
[11/14 13:28:03 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 13:28:03 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 13:28:05 visual_prompt]: Enable all parameters update during training
[11/14 13:28:05 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 13:28:05 visual_prompt]: tuned percent:100.000
[11/14 13:28:05 visual_prompt]: Device used for model: 0
[11/14 13:28:05 visual_prompt]: Setting up Evaluator...
[11/14 13:28:05 visual_prompt]: Setting up Trainer...
[11/14 13:28:05 visual_prompt]: 	Setting up the optimizer...
[11/14 13:28:05 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/14 13:34:27 visual_prompt]: Epoch 1 / 100: avg data time: 1.04e+01, avg batch time: 10.9078, average train loss: 6.9791
[11/14 13:35:11 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1767, average loss: 6.3857
[11/14 13:35:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.22	
[11/14 13:35:11 visual_prompt]: Training 2 / 100 epoch, with learning rate 2e-05
[11/14 13:41:37 visual_prompt]: Epoch 2 / 100: avg data time: 1.05e+01, avg batch time: 11.0265, average train loss: 2.4083
[11/14 13:42:20 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1669, average loss: 1.1167
[11/14 13:42:20 visual_prompt]: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 50.01	
[11/14 13:42:20 visual_prompt]: Training 3 / 100 epoch, with learning rate 4e-05
[11/14 13:48:43 visual_prompt]: Epoch 3 / 100: avg data time: 1.04e+01, avg batch time: 10.9221, average train loss: 1.1687
[11/14 13:49:27 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1758, average loss: 1.2126
[11/14 13:49:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 53.98	
[11/14 13:49:27 visual_prompt]: Training 4 / 100 epoch, with learning rate 6e-05
[11/14 13:55:49 visual_prompt]: Epoch 4 / 100: avg data time: 1.04e+01, avg batch time: 10.9067, average train loss: 1.0270
[11/14 13:56:32 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1711, average loss: 0.9335
[11/14 13:56:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 54.21	
[11/14 13:56:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 8e-05
[11/14 14:02:54 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e+01, avg batch time: 10.8999, average train loss: 0.9454
[11/14 14:03:38 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1718, average loss: 0.9480
[11/14 14:03:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 51.83	
[11/14 14:03:38 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.0001
[11/14 14:10:00 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e+01, avg batch time: 10.9054, average train loss: 0.9100
[11/14 14:10:43 visual_prompt]: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1699, average loss: 0.9315
[11/14 14:10:43 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 52.85	
[11/14 14:10:43 visual_prompt]: Training 7 / 100 epoch, with learning rate 9.997266286704631e-05
[11/14 14:17:05 visual_prompt]: Epoch 7 / 100: avg data time: 1.04e+01, avg batch time: 10.9048, average train loss: 0.8703
[11/14 14:17:49 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1695, average loss: 1.0649
[11/14 14:17:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 52.85	
[11/14 14:17:49 visual_prompt]: Training 8 / 100 epoch, with learning rate 9.989068136093873e-05
[11/14 14:24:12 visual_prompt]: Epoch 8 / 100: avg data time: 1.04e+01, avg batch time: 10.9342, average train loss: 0.8369
[11/14 14:24:56 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1758, average loss: 0.7743
[11/14 14:24:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 53.98	
[11/14 14:24:56 visual_prompt]: Best epoch 8: best metric: -0.774
[11/14 14:24:56 visual_prompt]: Training 9 / 100 epoch, with learning rate 9.975414512725057e-05
[11/14 14:31:37 visual_prompt]: Epoch 9 / 100: avg data time: 1.09e+01, avg batch time: 11.4697, average train loss: 0.7901
[11/14 14:32:29 visual_prompt]: Inference (val):avg data time: 7.41e-05, avg batch time: 0.1784, average loss: 0.9431
[11/14 14:32:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 54.30	
[11/14 14:32:29 visual_prompt]: Training 10 / 100 epoch, with learning rate 9.956320346634876e-05
[11/14 14:39:27 visual_prompt]: Epoch 10 / 100: avg data time: 1.14e+01, avg batch time: 11.9189, average train loss: 0.8237
[11/14 14:40:12 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1801, average loss: 0.7581
[11/14 14:40:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 54.80	
[11/14 14:40:12 visual_prompt]: Best epoch 10: best metric: -0.758
[11/14 14:40:12 visual_prompt]: Training 11 / 100 epoch, with learning rate 9.931806517013612e-05
[11/14 14:46:45 visual_prompt]: Epoch 11 / 100: avg data time: 1.07e+01, avg batch time: 11.2376, average train loss: 0.7639
[11/14 14:47:30 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1798, average loss: 0.9742
[11/14 14:47:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.16	
[11/14 14:47:30 visual_prompt]: Training 12 / 100 epoch, with learning rate 9.901899829374047e-05
[11/14 14:54:04 visual_prompt]: Epoch 12 / 100: avg data time: 1.07e+01, avg batch time: 11.2421, average train loss: 0.7347
[11/14 14:54:49 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1785, average loss: 0.8635
[11/14 14:54:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.80	
[11/14 14:54:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 9.86663298624003e-05
[11/14 15:01:19 visual_prompt]: Epoch 13 / 100: avg data time: 1.06e+01, avg batch time: 11.1314, average train loss: 0.7321
[11/14 15:02:04 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1787, average loss: 0.8714
[11/14 15:02:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.47	
[11/14 15:02:04 visual_prompt]: Training 14 / 100 epoch, with learning rate 9.826044551386744e-05
[11/14 15:08:28 visual_prompt]: Epoch 14 / 100: avg data time: 1.04e+01, avg batch time: 10.9490, average train loss: 0.7440
[11/14 15:09:11 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1744, average loss: 0.7656
[11/14 15:09:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 56.45	
[11/14 15:09:11 visual_prompt]: Training 15 / 100 epoch, with learning rate 9.780178907671789e-05
[11/14 15:15:29 visual_prompt]: Epoch 15 / 100: avg data time: 1.03e+01, avg batch time: 10.8143, average train loss: 0.7128
[11/14 15:16:13 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1733, average loss: 0.8723
[11/14 15:16:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.15	
[11/14 15:16:13 visual_prompt]: Training 16 / 100 epoch, with learning rate 9.729086208503174e-05
[11/14 15:22:30 visual_prompt]: Epoch 16 / 100: avg data time: 1.02e+01, avg batch time: 10.7863, average train loss: 0.7028
[11/14 15:23:14 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1751, average loss: 0.8690
[11/14 15:23:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.74	
[11/14 15:23:14 visual_prompt]: Training 17 / 100 epoch, with learning rate 9.672822322997305e-05
[11/14 15:29:27 visual_prompt]: Epoch 17 / 100: avg data time: 1.01e+01, avg batch time: 10.6682, average train loss: 0.7061
[11/14 15:30:10 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1792, average loss: 0.7900
[11/14 15:30:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 57.69	
[11/14 15:30:10 visual_prompt]: Training 18 / 100 epoch, with learning rate 9.611448774886924e-05
[11/14 15:36:27 visual_prompt]: Epoch 18 / 100: avg data time: 1.02e+01, avg batch time: 10.7645, average train loss: 0.6954
[11/14 15:37:10 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.1795, average loss: 0.8938
[11/14 15:37:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.13	
[11/14 15:37:10 visual_prompt]: Training 19 / 100 epoch, with learning rate 9.545032675245813e-05
[11/14 15:43:30 visual_prompt]: Epoch 19 / 100: avg data time: 1.03e+01, avg batch time: 10.8251, average train loss: 0.7020
[11/14 15:44:13 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1795, average loss: 0.7740
[11/14 15:44:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 58.89	
[11/14 15:44:13 visual_prompt]: Training 20 / 100 epoch, with learning rate 9.473646649103818e-05
[11/14 15:50:31 visual_prompt]: Epoch 20 / 100: avg data time: 1.03e+01, avg batch time: 10.7928, average train loss: 0.6806
[11/14 15:51:14 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1788, average loss: 0.7090
[11/14 15:51:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 58.81	
[11/14 15:51:14 visual_prompt]: Best epoch 20: best metric: -0.709
[11/14 15:51:14 visual_prompt]: Training 21 / 100 epoch, with learning rate 9.397368756032445e-05
[11/14 15:57:32 visual_prompt]: Epoch 21 / 100: avg data time: 1.03e+01, avg batch time: 10.8004, average train loss: 0.6853
[11/14 15:58:16 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1780, average loss: 0.7283
[11/14 15:58:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 58.72	
[11/14 15:58:16 visual_prompt]: Training 22 / 100 epoch, with learning rate 9.316282404787871e-05
[11/14 16:04:33 visual_prompt]: Epoch 22 / 100: avg data time: 1.03e+01, avg batch time: 10.7883, average train loss: 0.6825
[11/14 16:05:16 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1787, average loss: 0.7366
[11/14 16:05:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 59.25	
[11/14 16:05:16 visual_prompt]: Training 23 / 100 epoch, with learning rate 9.230476262104677e-05
[11/14 16:11:33 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e+01, avg batch time: 10.7535, average train loss: 0.6956
[11/14 16:12:16 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1754, average loss: 0.7761
[11/14 16:12:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 59.45	
[11/14 16:12:16 visual_prompt]: Training 24 / 100 epoch, with learning rate 9.140044155740101e-05
[11/14 16:18:34 visual_prompt]: Epoch 24 / 100: avg data time: 1.02e+01, avg batch time: 10.7791, average train loss: 0.6743
[11/14 16:19:17 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1798, average loss: 0.7708
[11/14 16:19:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 59.74	
[11/14 16:19:17 visual_prompt]: Training 25 / 100 epoch, with learning rate 9.045084971874738e-05
[11/14 16:25:34 visual_prompt]: Epoch 25 / 100: avg data time: 1.02e+01, avg batch time: 10.7754, average train loss: 0.6717
[11/14 16:26:17 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1788, average loss: 0.7443
[11/14 16:26:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 59.67	
[11/14 16:26:17 visual_prompt]: Training 26 / 100 epoch, with learning rate 8.945702546981969e-05
[11/14 16:32:35 visual_prompt]: Epoch 26 / 100: avg data time: 1.02e+01, avg batch time: 10.7846, average train loss: 0.6576
[11/14 16:33:18 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1725, average loss: 0.8329
[11/14 16:33:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.82	
[11/14 16:33:18 visual_prompt]: Training 27 / 100 epoch, with learning rate 8.842005554284296e-05
[11/14 16:39:50 visual_prompt]: Epoch 27 / 100: avg data time: 1.06e+01, avg batch time: 11.1777, average train loss: 0.6736
[11/14 16:40:33 visual_prompt]: Inference (val):avg data time: 3.61e-05, avg batch time: 0.1741, average loss: 0.7449
[11/14 16:40:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 59.93	
[11/14 16:40:33 visual_prompt]: Training 28 / 100 epoch, with learning rate 8.73410738492077e-05
[11/14 16:46:56 visual_prompt]: Epoch 28 / 100: avg data time: 1.04e+01, avg batch time: 10.9240, average train loss: 0.6646
[11/14 16:47:40 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1776, average loss: 0.8364
[11/14 16:47:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 59.99	
[11/14 16:47:40 visual_prompt]: Training 29 / 100 epoch, with learning rate 8.622126023955446e-05
[11/14 16:54:02 visual_prompt]: Epoch 29 / 100: avg data time: 1.04e+01, avg batch time: 10.9276, average train loss: 0.6529
[11/14 16:54:46 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1786, average loss: 0.7338
[11/14 16:54:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.29	
[11/14 16:54:46 visual_prompt]: Training 30 / 100 epoch, with learning rate 8.506183921362443e-05
[11/14 17:01:08 visual_prompt]: Epoch 30 / 100: avg data time: 1.04e+01, avg batch time: 10.9137, average train loss: 0.6584
[11/14 17:01:52 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1764, average loss: 0.7477
[11/14 17:01:52 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 60.34	
[11/14 17:01:52 visual_prompt]: Training 31 / 100 epoch, with learning rate 8.386407858128706e-05
[11/14 17:08:14 visual_prompt]: Epoch 31 / 100: avg data time: 1.03e+01, avg batch time: 10.8948, average train loss: 0.6417
[11/14 17:08:57 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1807, average loss: 0.7687
[11/14 17:08:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 60.70	
[11/14 17:08:57 visual_prompt]: Training 32 / 100 epoch, with learning rate 8.262928807620843e-05
[11/14 17:15:20 visual_prompt]: Epoch 32 / 100: avg data time: 1.04e+01, avg batch time: 10.9277, average train loss: 0.6558
[11/14 17:16:04 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1802, average loss: 0.7276
[11/14 17:16:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.68	
[11/14 17:16:04 visual_prompt]: Training 33 / 100 epoch, with learning rate 8.135881792367686e-05
[11/14 17:22:27 visual_prompt]: Epoch 33 / 100: avg data time: 1.04e+01, avg batch time: 10.9283, average train loss: 0.6629
[11/14 17:23:10 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1778, average loss: 0.7263
[11/14 17:23:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 60.74	
[11/14 17:23:10 visual_prompt]: Training 34 / 100 epoch, with learning rate 8.005405736415126e-05
[11/14 17:29:32 visual_prompt]: Epoch 34 / 100: avg data time: 1.04e+01, avg batch time: 10.9057, average train loss: 0.6403
[11/14 17:30:16 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1773, average loss: 0.7662
[11/14 17:30:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.58	
[11/14 17:30:16 visual_prompt]: Stopping early.
[11/14 17:30:16 visual_prompt]: Rank of current process: 0. World size: 1
[11/14 17:30:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/14 17:30:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '200', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/14 17:30:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/14 17:30:16 visual_prompt]: Training with config:
[11/14 17:30:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size200/test/seed9805/lrNone_wdNone/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 9805, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': None, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': None, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 200, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 64, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/14 17:30:16 visual_prompt]: Loading training data...
[11/14 17:30:16 visual_prompt]: Constructing mammo-cbis dataset train...
[11/14 17:30:16 visual_prompt]: Loading validation data...
[11/14 17:30:16 visual_prompt]: Constructing mammo-cbis dataset val...
[11/14 17:30:16 visual_prompt]: Loading test data...
[11/14 17:30:16 visual_prompt]: Constructing mammo-cbis dataset test...
[11/14 17:30:16 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 145, 768])
load_pretrained: grid-size from 14 to 12
[11/14 17:30:18 visual_prompt]: Enable all parameters update during training
[11/14 17:30:18 visual_prompt]: Total Parameters: 85760258	 Gradient Parameters: 85760258
[11/14 17:30:18 visual_prompt]: tuned percent:100.000
[11/14 17:30:18 visual_prompt]: Device used for model: 0
[11/14 17:30:18 visual_prompt]: Setting up Evaluator...
[11/14 17:30:18 visual_prompt]: Setting up Trainer...
[11/14 17:30:18 visual_prompt]: 	Setting up the optimizer...
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 99, in <module>
    main(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 94, in main
    train(cfg, args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 43, in train
    trainer = Trainer(cfg, model, evaluator, cur_device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/engine/trainer.py", line 46, in __init__
    self.optimizer = make_optimizer([self.model], cfg.SOLVER)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/solver/optimizer.py", line 36, in make_optimizer
    if train_params.WEIGHT_DECAY > 0:
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: '>' not supported between instances of 'NoneType' and 'int'
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

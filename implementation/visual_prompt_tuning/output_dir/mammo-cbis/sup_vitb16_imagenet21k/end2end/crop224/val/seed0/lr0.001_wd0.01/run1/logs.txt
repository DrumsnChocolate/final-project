[11/07 04:35:58][INFO] visual_prompt:   95: Rank of current process: 0. World size: 1
[11/07 04:35:58][INFO] visual_prompt:   97: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                Quadro RTX 6000
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/07 04:35:58][INFO] visual_prompt:   99: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '1', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '224', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[11/07 04:35:58][INFO] visual_prompt:  101: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/07 04:35:58][INFO] visual_prompt:  108: Training with config:
[11/07 04:35:58][INFO] visual_prompt:  109: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/crop224/val/seed0/lr0.001_wd0.01/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 1, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/07 04:35:58][INFO] visual_prompt:   55: Loading training data...
[11/07 04:35:58][INFO] visual_prompt:   28: Constructing mammo-cbis dataset train...
[11/07 04:35:58][INFO] visual_prompt:   57: Loading validation data...
[11/07 04:35:58][INFO] visual_prompt:   28: Constructing mammo-cbis dataset val...
[11/07 04:35:58][INFO] visual_prompt:   38: Constructing models...
[11/07 04:36:00][INFO] visual_prompt:  153: Enable all parameters update during training
[11/07 04:36:00][INFO] visual_prompt:   52: Total Parameters: 85800194	 Gradient Parameters: 85800194
[11/07 04:36:00][INFO] visual_prompt:   54: tuned percent:100.000
[11/07 04:36:00][INFO] visual_prompt:   40: Device used for model: 0
[11/07 04:36:00][INFO] visual_prompt:   40: Setting up Evaluator...
[11/07 04:36:00][INFO] visual_prompt:   42: Setting up Trainer...
[11/07 04:36:00][INFO] visual_prompt:   45: 	Setting up the optimizer...
[11/07 04:36:00][INFO] visual_prompt:  165: Training 1 / 100 epoch, with learning rate 0.0
[11/07 04:36:24][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 17.0073,	0.2008 s / batch. (data: 7.08e-04). ETA=12:20:00, max mem: 5.0 GB 
[11/07 04:36:46][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 8.7706,	0.1967 s / batch. (data: 6.76e-04). ETA=12:04:27, max mem: 5.0 GB 
[11/07 04:37:07][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.0001,	0.2104 s / batch. (data: 4.61e-02). ETA=12:54:42, max mem: 5.0 GB 
[11/07 04:37:28][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 9.7364,	0.1620 s / batch. (data: 6.71e-04). ETA=9:56:15, max mem: 5.0 GB 
[11/07 04:37:50][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 9.6752,	0.1815 s / batch. (data: 6.69e-04). ETA=11:07:43, max mem: 5.0 GB 
[11/07 04:38:12][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.0000,	0.2526 s / batch. (data: 2.15e-02). ETA=15:28:37, max mem: 5.0 GB 
[11/07 04:38:34][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 10.7484,	0.2182 s / batch. (data: 1.82e-02). ETA=13:21:44, max mem: 5.0 GB 
[11/07 04:38:56][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 10.3468,	0.1889 s / batch. (data: 6.82e-03). ETA=11:33:58, max mem: 5.0 GB 
[11/07 04:39:18][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.0000,	0.2107 s / batch. (data: 5.40e-03). ETA=12:53:45, max mem: 5.0 GB 
[11/07 04:39:38][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.0001,	0.1996 s / batch. (data: 1.04e-02). ETA=12:12:29, max mem: 5.0 GB 
[11/07 04:39:59][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.0000,	0.1016 s / batch. (data: 3.25e-04). ETA=6:12:34, max mem: 5.0 GB 
[11/07 04:40:20][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.0000,	0.2106 s / batch. (data: 1.04e-02). ETA=12:52:02, max mem: 5.0 GB 
[11/07 04:40:40][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.0000,	0.2164 s / batch. (data: 1.04e-02). ETA=13:13:06, max mem: 5.0 GB 
[11/07 04:41:03][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.0027,	0.1633 s / batch. (data: 5.37e-03). ETA=9:58:11, max mem: 5.0 GB 
[11/07 04:41:24][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 5.3648,	0.2185 s / batch. (data: 5.36e-03). ETA=13:20:08, max mem: 5.0 GB 
[11/07 04:41:44][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 5.2697,	0.1700 s / batch. (data: 5.41e-03). ETA=10:22:05, max mem: 5.0 GB 
[11/07 04:42:06][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.0000,	0.2198 s / batch. (data: 2.10e-02). ETA=13:24:06, max mem: 5.0 GB 
[11/07 04:42:27][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.0000,	0.2155 s / batch. (data: 1.55e-02). ETA=13:08:08, max mem: 5.0 GB 
[11/07 04:42:48][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 8.6260,	0.2187 s / batch. (data: 1.50e-02). ETA=13:19:16, max mem: 5.0 GB 
[11/07 04:43:10][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.0002,	0.1966 s / batch. (data: 5.78e-03). ETA=11:58:22, max mem: 5.0 GB 
[11/07 04:43:31][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 14.9015,	0.2268 s / batch. (data: 7.69e-04). ETA=13:48:13, max mem: 5.0 GB 
[11/07 04:43:51][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 7.6808,	0.1049 s / batch. (data: 1.18e-04). ETA=6:22:59, max mem: 5.0 GB 
[11/07 04:43:52][INFO] visual_prompt:  217: Epoch 1 / 100: avg data time: 2.83e-02, avg batch time: 0.2132, average train loss: 5.2946
[11/07 04:44:13][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.001, 0.0953 s / batch. (data: 3.15e-05)max mem: 4.96204 GB 
[11/07 04:44:33][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.002, 0.0533 s / batch. (data: 3.00e-05)max mem: 4.96204 GB 
[11/07 04:44:41][INFO] visual_prompt:  316: Inference (val):avg data time: 3.93e-04, avg batch time: 0.0647, average loss: 4.3337
[11/07 04:44:41][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.86	
[11/07 04:44:41][INFO] visual_prompt:  165: Training 2 / 100 epoch, with learning rate 0.0002
[11/07 04:45:06][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.1393,	0.1958 s / batch. (data: 2.06e-02). ETA=11:54:13, max mem: 5.0 GB 
[11/07 04:45:27][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.8012,	0.2491 s / batch. (data: 1.16e-03). ETA=15:08:11, max mem: 5.0 GB 
[11/07 04:45:49][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.0788,	0.1849 s / batch. (data: 5.40e-03). ETA=11:14:01, max mem: 5.0 GB 
[11/07 04:46:10][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.6753,	0.2007 s / batch. (data: 9.84e-03). ETA=12:11:06, max mem: 5.0 GB 
[11/07 04:46:30][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.0789,	0.2390 s / batch. (data: 1.55e-02). ETA=14:30:12, max mem: 5.0 GB 
[11/07 04:46:50][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.3582,	0.1789 s / batch. (data: 6.63e-04). ETA=10:51:04, max mem: 5.0 GB 
[11/07 04:47:11][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.2549,	0.1640 s / batch. (data: 7.96e-03). ETA=9:56:43, max mem: 5.0 GB 
[11/07 04:47:33][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.0845,	0.1742 s / batch. (data: 1.04e-02). ETA=10:33:36, max mem: 5.0 GB 
[11/07 04:47:55][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.8504,	0.2129 s / batch. (data: 2.56e-02). ETA=12:53:57, max mem: 5.0 GB 
[11/07 04:48:18][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.7034,	0.2020 s / batch. (data: 1.04e-02). ETA=12:13:46, max mem: 5.0 GB 
[11/07 04:48:39][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.0461,	0.2041 s / batch. (data: 2.64e-04). ETA=12:21:15, max mem: 5.0 GB 
[11/07 04:48:59][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.4415,	0.1759 s / batch. (data: 2.44e-04). ETA=10:38:29, max mem: 5.0 GB 
[11/07 04:49:23][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.2197,	0.9300 s / batch. (data: 7.70e-01). ETA=2 days, 8:14:04, max mem: 5.0 GB 
[11/07 04:49:44][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.6966,	0.1914 s / batch. (data: 2.79e-04). ETA=11:34:12, max mem: 5.0 GB 
[11/07 04:50:06][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 1.3098,	0.1819 s / batch. (data: 1.79e-02). ETA=10:59:18, max mem: 5.0 GB 
[11/07 04:50:27][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 1.9869,	0.2088 s / batch. (data: 2.05e-02). ETA=12:36:29, max mem: 5.0 GB 
[11/07 04:50:48][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.3484,	0.1886 s / batch. (data: 1.55e-02). ETA=11:23:04, max mem: 5.0 GB 
[11/07 04:51:08][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.1993,	0.2100 s / batch. (data: 1.04e-02). ETA=12:40:10, max mem: 5.0 GB 
[11/07 04:51:30][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.2669,	0.1438 s / batch. (data: 5.37e-03). ETA=8:40:18, max mem: 5.0 GB 
[11/07 04:51:51][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.7119,	0.1753 s / batch. (data: 7.98e-03). ETA=10:33:47, max mem: 5.0 GB 
[11/07 04:52:11][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.1638,	0.1993 s / batch. (data: 8.25e-03). ETA=12:00:23, max mem: 5.0 GB 
[11/07 04:52:32][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.2001,	0.0585 s / batch. (data: 8.94e-05). ETA=3:31:17, max mem: 5.0 GB 
[11/07 04:52:33][INFO] visual_prompt:  217: Epoch 2 / 100: avg data time: 2.98e-02, avg batch time: 0.2133, average train loss: 1.0977
[11/07 04:52:55][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.269, 0.0890 s / batch. (data: 2.43e-05)max mem: 4.96204 GB 
[11/07 04:53:14][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.294, 0.0615 s / batch. (data: 3.60e-05)max mem: 4.96204 GB 
[11/07 04:53:22][INFO] visual_prompt:  316: Inference (val):avg data time: 3.05e-05, avg batch time: 0.0628, average loss: 0.8391
[11/07 04:53:22][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.00	
[11/07 04:53:22][INFO] visual_prompt:  165: Training 3 / 100 epoch, with learning rate 0.0004
[11/07 04:53:46][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.4928,	0.1642 s / batch. (data: 2.82e-04). ETA=9:52:49, max mem: 5.0 GB 
[11/07 04:54:08][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 2.2591,	0.2006 s / batch. (data: 2.49e-02). ETA=12:04:01, max mem: 5.0 GB 
[11/07 04:54:30][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 1.3486,	0.1700 s / batch. (data: 2.40e-04). ETA=10:13:31, max mem: 5.0 GB 
[11/07 04:54:51][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 1.4217,	0.1837 s / batch. (data: 9.30e-03). ETA=11:02:38, max mem: 5.0 GB 
[11/07 04:55:13][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 2.2470,	0.2163 s / batch. (data: 1.04e-02). ETA=12:59:41, max mem: 5.0 GB 
[11/07 04:55:35][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 1.1179,	0.1662 s / batch. (data: 7.84e-04). ETA=9:58:42, max mem: 5.0 GB 
[11/07 04:55:56][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 1.3942,	0.1710 s / batch. (data: 6.91e-03). ETA=10:15:45, max mem: 5.0 GB 
[11/07 04:56:18][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 2.6094,	0.1803 s / batch. (data: 1.74e-02). ETA=10:48:57, max mem: 5.0 GB 
[11/07 04:56:39][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 1.1515,	0.2088 s / batch. (data: 8.01e-03). ETA=12:31:12, max mem: 5.0 GB 
[11/07 04:57:00][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.1815,	0.1931 s / batch. (data: 1.21e-02). ETA=11:34:16, max mem: 5.0 GB 
[11/07 04:57:21][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.6591,	0.1565 s / batch. (data: 2.50e-04). ETA=9:22:30, max mem: 5.0 GB 
[11/07 04:57:41][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.7950,	0.2078 s / batch. (data: 5.80e-03). ETA=12:26:26, max mem: 5.0 GB 
[11/07 04:58:03][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 1.2591,	0.1131 s / batch. (data: 1.87e-04). ETA=6:46:08, max mem: 5.0 GB 
[11/07 04:58:25][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.3342,	0.1282 s / batch. (data: 5.39e-03). ETA=7:40:10, max mem: 5.0 GB 
[11/07 04:58:45][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.1076,	0.2269 s / batch. (data: 1.05e-02). ETA=13:34:09, max mem: 5.0 GB 
[11/07 04:59:06][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.2285,	0.2206 s / batch. (data: 1.31e-02). ETA=13:11:00, max mem: 5.0 GB 
[11/07 04:59:27][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 1.1235,	0.2206 s / batch. (data: 1.56e-02). ETA=13:10:38, max mem: 5.0 GB 
[11/07 04:59:49][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.3517,	0.2178 s / batch. (data: 5.37e-03). ETA=13:00:18, max mem: 5.0 GB 
[11/07 05:00:09][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.2734,	0.1901 s / batch. (data: 7.96e-03). ETA=11:20:41, max mem: 5.0 GB 
[11/07 05:00:30][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 1.6778,	0.2414 s / batch. (data: 2.11e-02). ETA=14:24:04, max mem: 5.0 GB 
[11/07 05:00:53][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 2.6383,	0.2441 s / batch. (data: 5.85e-03). ETA=14:33:11, max mem: 5.0 GB 
[11/07 05:01:13][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.6687,	0.4060 s / batch. (data: 2.80e-01). ETA=1 day, 0:11:57, max mem: 5.0 GB 
[11/07 05:01:14][INFO] visual_prompt:  217: Epoch 3 / 100: avg data time: 2.92e-02, avg batch time: 0.2133, average train loss: 0.9122
[11/07 05:01:36][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.486, 0.0910 s / batch. (data: 3.60e-05)max mem: 4.96204 GB 
[11/07 05:01:56][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.468, 0.0687 s / batch. (data: 2.93e-05)max mem: 4.96204 GB 
[11/07 05:02:04][INFO] visual_prompt:  316: Inference (val):avg data time: 2.99e-05, avg batch time: 0.0643, average loss: 0.7461
[11/07 05:02:04][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.24	
[11/07 05:02:04][INFO] visual_prompt:  165: Training 4 / 100 epoch, with learning rate 0.0006
[11/07 05:02:29][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 1.6867,	0.2203 s / batch. (data: 3.80e-02). ETA=13:07:29, max mem: 5.0 GB 
[11/07 05:02:49][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.6990,	0.2097 s / batch. (data: 3.08e-02). ETA=12:29:04, max mem: 5.0 GB 
[11/07 05:03:11][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.3313,	0.1698 s / batch. (data: 1.04e-02). ETA=10:06:28, max mem: 5.0 GB 
[11/07 05:03:32][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 1.0073,	0.2143 s / batch. (data: 2.13e-02). ETA=12:44:57, max mem: 5.0 GB 
[11/07 05:03:53][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.6463,	0.1654 s / batch. (data: 2.26e-02). ETA=9:49:58, max mem: 5.0 GB 
[11/07 05:04:14][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.3259,	0.1916 s / batch. (data: 1.64e-02). ETA=11:23:21, max mem: 5.0 GB 
[11/07 05:04:35][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 1.3500,	0.2157 s / batch. (data: 2.41e-04). ETA=12:48:49, max mem: 5.0 GB 
[11/07 05:04:57][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 1.5442,	0.1702 s / batch. (data: 5.42e-03). ETA=10:06:20, max mem: 5.0 GB 
[11/07 05:05:18][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.2893,	0.2439 s / batch. (data: 2.06e-02). ETA=14:28:42, max mem: 5.0 GB 
[11/07 05:05:38][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 1.2215,	0.1960 s / batch. (data: 9.91e-03). ETA=11:37:27, max mem: 5.0 GB 
[11/07 05:06:00][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 2.3748,	0.2006 s / batch. (data: 1.55e-02). ETA=11:53:48, max mem: 5.0 GB 
[11/07 05:06:21][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.2733,	0.1851 s / batch. (data: 5.38e-03). ETA=10:58:17, max mem: 5.0 GB 
[11/07 05:06:42][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 1.2201,	0.2102 s / batch. (data: 5.38e-03). ETA=12:26:59, max mem: 5.0 GB 
[11/07 05:07:04][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.8250,	0.2428 s / batch. (data: 2.10e-02). ETA=14:22:36, max mem: 5.0 GB 
[11/07 05:07:25][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.4628,	0.1707 s / batch. (data: 2.50e-04). ETA=10:06:10, max mem: 5.0 GB 
[11/07 05:07:46][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.2615,	0.2087 s / batch. (data: 1.55e-02). ETA=12:20:39, max mem: 5.0 GB 
[11/07 05:08:08][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.0936,	0.1791 s / batch. (data: 2.56e-04). ETA=10:35:30, max mem: 5.0 GB 
[11/07 05:08:28][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.9399,	0.1756 s / batch. (data: 1.80e-02). ETA=10:22:49, max mem: 5.0 GB 
[11/07 05:08:50][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.6914,	0.2217 s / batch. (data: 2.33e-04). ETA=13:05:43, max mem: 5.0 GB 
[11/07 05:09:11][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 1.1759,	0.1747 s / batch. (data: 2.41e-04). ETA=10:18:58, max mem: 5.0 GB 
[11/07 05:09:33][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 1.2894,	0.1893 s / batch. (data: 7.07e-04). ETA=11:10:23, max mem: 5.0 GB 
[11/07 05:09:54][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.1614,	0.1550 s / batch. (data: 1.15e-04). ETA=9:08:35, max mem: 5.0 GB 
[11/07 05:09:56][INFO] visual_prompt:  217: Epoch 4 / 100: avg data time: 2.90e-02, avg batch time: 0.2132, average train loss: 0.8940
[11/07 05:10:17][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.925, 0.0707 s / batch. (data: 4.98e-05)max mem: 4.96204 GB 
[11/07 05:10:37][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.931, 0.0854 s / batch. (data: 2.72e-05)max mem: 4.96204 GB 
[11/07 05:10:45][INFO] visual_prompt:  316: Inference (val):avg data time: 2.49e-04, avg batch time: 0.0639, average loss: 0.6937
[11/07 05:10:45][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.44	
[11/07 05:10:45][INFO] visual_prompt:  165: Training 5 / 100 epoch, with learning rate 0.0008
[11/07 05:11:09][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 2.0238,	0.1953 s / batch. (data: 2.83e-04). ETA=11:31:03, max mem: 5.0 GB 
[11/07 05:11:30][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 1.0386,	0.1492 s / batch. (data: 5.32e-03). ETA=8:47:23, max mem: 5.0 GB 
[11/07 05:11:52][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 1.4904,	0.2136 s / batch. (data: 2.56e-02). ETA=12:34:56, max mem: 5.0 GB 
[11/07 05:12:14][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 2.6809,	0.1578 s / batch. (data: 9.82e-03). ETA=9:17:24, max mem: 5.0 GB 
[11/07 05:12:36][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.8951,	0.1791 s / batch. (data: 5.77e-03). ETA=10:32:12, max mem: 5.0 GB 
[11/07 05:12:57][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.2169,	0.2087 s / batch. (data: 7.02e-04). ETA=12:16:24, max mem: 5.0 GB 
[11/07 05:13:17][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.8593,	0.1927 s / batch. (data: 2.53e-02). ETA=11:19:43, max mem: 5.0 GB 
[11/07 05:13:39][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.9078,	0.2106 s / batch. (data: 1.55e-02). ETA=12:22:31, max mem: 5.0 GB 
[11/07 05:14:00][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.5003,	0.1694 s / batch. (data: 2.46e-04). ETA=9:57:08, max mem: 5.0 GB 
[11/07 05:14:21][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.7297,	0.2320 s / batch. (data: 7.01e-02). ETA=13:37:11, max mem: 5.0 GB 
[11/07 05:14:42][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.6109,	0.1973 s / batch. (data: 4.13e-02). ETA=11:34:30, max mem: 5.0 GB 
[11/07 05:15:03][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.6024,	0.1731 s / batch. (data: 2.65e-04). ETA=10:09:09, max mem: 5.0 GB 
[11/07 05:15:24][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.4197,	0.1811 s / batch. (data: 6.61e-04). ETA=10:36:51, max mem: 5.0 GB 
[11/07 05:15:45][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.0003,	0.2313 s / batch. (data: 1.54e-02). ETA=13:33:15, max mem: 5.0 GB 
[11/07 05:16:07][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 1.2953,	0.7841 s / batch. (data: 6.42e-01). ETA=1 day, 21:55:20, max mem: 5.0 GB 
[11/07 05:16:29][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.5169,	0.1953 s / batch. (data: 2.72e-04). ETA=11:25:55, max mem: 5.0 GB 
[11/07 05:16:50][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.7794,	0.2103 s / batch. (data: 3.04e-02). ETA=12:18:13, max mem: 5.0 GB 
[11/07 05:17:12][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.8215,	0.2308 s / batch. (data: 2.44e-02). ETA=13:30:03, max mem: 5.0 GB 
[11/07 05:17:34][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.6410,	0.2303 s / batch. (data: 1.59e-02). ETA=13:27:51, max mem: 5.0 GB 
[11/07 05:17:55][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.2228,	0.1717 s / batch. (data: 1.08e-02). ETA=10:02:03, max mem: 5.0 GB 
[11/07 05:18:15][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 1.4759,	0.2167 s / batch. (data: 5.72e-03). ETA=12:39:25, max mem: 5.0 GB 
[11/07 05:18:36][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.4495,	0.0991 s / batch. (data: 1.25e-04). ETA=5:47:06, max mem: 5.0 GB 
[11/07 05:18:37][INFO] visual_prompt:  217: Epoch 5 / 100: avg data time: 2.93e-02, avg batch time: 0.2133, average train loss: 0.8782
[11/07 05:18:58][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.840, 0.1055 s / batch. (data: 2.57e-05)max mem: 4.96204 GB 
[11/07 05:19:18][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.842, 0.0827 s / batch. (data: 2.29e-05)max mem: 4.96204 GB 
[11/07 05:19:26][INFO] visual_prompt:  316: Inference (val):avg data time: 3.36e-04, avg batch time: 0.0631, average loss: 0.6888
[11/07 05:19:26][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.37	
[11/07 05:19:26][INFO] visual_prompt:  165: Training 6 / 100 epoch, with learning rate 0.001
[11/07 05:19:50][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 3.4754,	0.2004 s / batch. (data: 5.38e-03). ETA=11:41:33, max mem: 5.0 GB 
[11/07 05:20:13][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 1.6310,	0.1928 s / batch. (data: 2.57e-02). ETA=11:14:41, max mem: 5.0 GB 
[11/07 05:20:34][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.4592,	0.1844 s / batch. (data: 6.79e-03). ETA=10:44:49, max mem: 5.0 GB 
[11/07 05:20:54][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.8283,	0.1877 s / batch. (data: 2.51e-04). ETA=10:56:10, max mem: 5.0 GB 
[11/07 05:21:16][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.4875,	0.1741 s / batch. (data: 1.08e-02). ETA=10:08:23, max mem: 5.0 GB 
[11/07 05:21:38][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.4917,	0.1902 s / batch. (data: 1.57e-02). ETA=11:04:13, max mem: 5.0 GB 
[11/07 05:22:00][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.7122,	0.2117 s / batch. (data: 3.01e-02). ETA=12:19:07, max mem: 5.0 GB 
[11/07 05:22:21][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 1.0167,	0.1954 s / batch. (data: 6.92e-03). ETA=11:21:45, max mem: 5.0 GB 
[11/07 05:22:42][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.6651,	0.2015 s / batch. (data: 5.39e-03). ETA=11:42:35, max mem: 5.0 GB 
[11/07 05:23:04][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.7879,	0.1914 s / batch. (data: 1.37e-02). ETA=11:07:07, max mem: 5.0 GB 
[11/07 05:23:25][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.2936,	0.1815 s / batch. (data: 1.55e-02). ETA=10:32:18, max mem: 5.0 GB 
[11/07 05:23:45][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.3209,	0.2121 s / batch. (data: 3.73e-02). ETA=12:18:30, max mem: 5.0 GB 
[11/07 05:24:06][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.4799,	0.2070 s / batch. (data: 1.55e-02). ETA=12:00:21, max mem: 5.0 GB 
[11/07 05:24:28][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.3625,	0.2169 s / batch. (data: 1.04e-02). ETA=12:34:45, max mem: 5.0 GB 
[11/07 05:24:49][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.9777,	0.1585 s / batch. (data: 2.80e-04). ETA=9:11:15, max mem: 5.0 GB 
[11/07 05:25:11][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.3323,	0.1799 s / batch. (data: 1.08e-02). ETA=10:25:21, max mem: 5.0 GB 
[11/07 05:25:32][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.6734,	0.1840 s / batch. (data: 1.55e-02). ETA=10:39:02, max mem: 5.0 GB 
[11/07 05:25:55][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.9524,	0.2003 s / batch. (data: 1.04e-02). ETA=11:35:27, max mem: 5.0 GB 
[11/07 05:26:15][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 1.0611,	0.2165 s / batch. (data: 8.86e-03). ETA=12:31:26, max mem: 5.0 GB 
[11/07 05:26:36][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.7980,	0.1573 s / batch. (data: 2.03e-04). ETA=9:05:39, max mem: 5.0 GB 
[11/07 05:26:55][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.7706,	0.1360 s / batch. (data: 1.04e-02). ETA=7:51:41, max mem: 5.0 GB 
[11/07 05:27:17][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.8025,	0.1505 s / batch. (data: 1.12e-04). ETA=8:41:29, max mem: 5.0 GB 
[11/07 05:27:18][INFO] visual_prompt:  217: Epoch 6 / 100: avg data time: 2.95e-02, avg batch time: 0.2132, average train loss: 0.8033
[11/07 05:27:40][INFO] visual_prompt:  303: 	Test 100/246. loss: 1.030, 0.1116 s / batch. (data: 2.88e-05)max mem: 4.96204 GB 
[11/07 05:27:59][INFO] visual_prompt:  303: 	Test 200/246. loss: 1.026, 0.1117 s / batch. (data: 2.36e-05)max mem: 4.96204 GB 
[11/07 05:28:07][INFO] visual_prompt:  316: Inference (val):avg data time: 4.76e-05, avg batch time: 0.0636, average loss: 0.7057
[11/07 05:28:07][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.21	
[11/07 05:28:07][INFO] visual_prompt:  165: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/07 05:28:30][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.1194,	0.1844 s / batch. (data: 7.37e-04). ETA=10:38:41, max mem: 5.0 GB 
[11/07 05:28:54][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.2359,	0.1526 s / batch. (data: 5.37e-03). ETA=8:48:27, max mem: 5.0 GB 
[11/07 05:29:17][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.9825,	0.2199 s / batch. (data: 7.11e-04). ETA=12:40:59, max mem: 5.0 GB 
[11/07 05:29:39][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 1.2486,	0.1775 s / batch. (data: 7.05e-04). ETA=10:13:54, max mem: 5.0 GB 
[11/07 05:30:01][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 1.4225,	0.2199 s / batch. (data: 6.85e-04). ETA=12:40:22, max mem: 5.0 GB 
[11/07 05:30:21][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.8155,	0.1722 s / batch. (data: 9.93e-03). ETA=9:54:59, max mem: 5.0 GB 
[11/07 05:30:43][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.1980,	0.1942 s / batch. (data: 2.49e-02). ETA=11:10:53, max mem: 5.0 GB 
[11/07 05:31:04][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.3860,	0.1937 s / batch. (data: 2.45e-04). ETA=11:08:36, max mem: 5.0 GB 
[11/07 05:31:25][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.6925,	0.2067 s / batch. (data: 1.55e-02). ETA=11:53:22, max mem: 5.0 GB 
[11/07 05:31:46][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.5898,	0.1908 s / batch. (data: 1.34e-02). ETA=10:58:05, max mem: 5.0 GB 
[11/07 05:32:07][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.7698,	0.1591 s / batch. (data: 2.83e-04). ETA=9:08:18, max mem: 5.0 GB 
[11/07 05:32:27][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.4889,	0.1590 s / batch. (data: 2.11e-04). ETA=9:07:41, max mem: 5.0 GB 
[11/07 05:32:48][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.6733,	0.1280 s / batch. (data: 7.04e-04). ETA=7:20:52, max mem: 5.0 GB 
[11/07 05:33:09][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.4502,	0.1972 s / batch. (data: 2.39e-04). ETA=11:18:38, max mem: 5.0 GB 
[11/07 05:33:30][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 1.6218,	0.1909 s / batch. (data: 1.05e-02). ETA=10:56:47, max mem: 5.0 GB 
[11/07 05:33:51][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.4768,	0.2289 s / batch. (data: 9.20e-03). ETA=13:07:18, max mem: 5.0 GB 
[11/07 05:34:12][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.8390,	0.2712 s / batch. (data: 5.82e-03). ETA=15:32:12, max mem: 5.0 GB 
[11/07 05:34:33][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.9950,	0.2133 s / batch. (data: 2.98e-04). ETA=12:12:41, max mem: 5.0 GB 
[11/07 05:34:55][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.8383,	0.2477 s / batch. (data: 3.14e-02). ETA=14:10:39, max mem: 5.0 GB 
[11/07 05:35:16][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.9636,	0.1659 s / batch. (data: 1.04e-02). ETA=9:29:13, max mem: 5.0 GB 
[11/07 05:35:37][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 1.0157,	0.1912 s / batch. (data: 5.37e-03). ETA=10:55:50, max mem: 5.0 GB 
[11/07 05:35:58][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.7969,	0.1342 s / batch. (data: 1.21e-04). ETA=7:40:05, max mem: 5.0 GB 
[11/07 05:35:59][INFO] visual_prompt:  217: Epoch 7 / 100: avg data time: 2.78e-02, avg batch time: 0.2131, average train loss: 0.7624
[11/07 05:36:20][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.898, 0.0658 s / batch. (data: 2.38e-05)max mem: 4.96204 GB 
[11/07 05:36:40][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.892, 0.0771 s / batch. (data: 3.15e-05)max mem: 4.96204 GB 
[11/07 05:36:48][INFO] visual_prompt:  316: Inference (val):avg data time: 1.28e-04, avg batch time: 0.0627, average loss: 0.6903
[11/07 05:36:48][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.25	
[11/07 05:36:48][INFO] visual_prompt:   36: Best epoch 7: best metric: -0.690
[11/07 05:36:48][INFO] visual_prompt:  165: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/07 05:37:12][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.1556,	0.1876 s / batch. (data: 2.45e-04). ETA=10:42:44, max mem: 5.0 GB 
[11/07 05:37:34][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.8309,	0.2149 s / batch. (data: 6.93e-04). ETA=12:16:08, max mem: 5.0 GB 
[11/07 05:37:55][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.8390,	0.1691 s / batch. (data: 1.60e-02). ETA=9:38:50, max mem: 5.0 GB 
[11/07 05:38:16][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.3260,	0.2088 s / batch. (data: 5.33e-03). ETA=11:54:25, max mem: 5.0 GB 
[11/07 05:38:39][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.4819,	0.4401 s / batch. (data: 2.95e-01). ETA=1 day, 1:05:18, max mem: 5.0 GB 
[11/07 05:39:00][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.6748,	0.1926 s / batch. (data: 7.00e-04). ETA=10:58:28, max mem: 5.0 GB 
[11/07 05:39:21][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.2610,	0.2331 s / batch. (data: 2.57e-02). ETA=13:16:31, max mem: 5.0 GB 
[11/07 05:39:42][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.3941,	0.2024 s / batch. (data: 1.04e-02). ETA=11:31:12, max mem: 5.0 GB 
[11/07 05:40:04][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.4259,	0.1824 s / batch. (data: 2.76e-04). ETA=10:22:33, max mem: 5.0 GB 
[11/07 05:40:25][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 1.1341,	0.2008 s / batch. (data: 6.84e-04). ETA=11:25:06, max mem: 5.0 GB 
[11/07 05:40:46][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.6023,	0.1285 s / batch. (data: 2.37e-04). ETA=7:18:19, max mem: 5.0 GB 
[11/07 05:41:08][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.6748,	0.2291 s / batch. (data: 5.38e-03). ETA=13:00:57, max mem: 5.0 GB 
[11/07 05:41:29][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.9671,	0.2552 s / batch. (data: 1.61e-02). ETA=14:29:27, max mem: 5.0 GB 
[11/07 05:41:50][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.8380,	0.2514 s / batch. (data: 1.55e-02). ETA=14:16:10, max mem: 5.0 GB 
[11/07 05:42:12][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 1.2837,	0.1162 s / batch. (data: 3.04e-04). ETA=6:35:28, max mem: 5.0 GB 
[11/07 05:42:34][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 1.1890,	0.2144 s / batch. (data: 2.06e-02). ETA=12:09:32, max mem: 5.0 GB 
[11/07 05:42:55][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 1.4702,	0.1809 s / batch. (data: 2.71e-04). ETA=10:15:00, max mem: 5.0 GB 
[11/07 05:43:16][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.1903,	0.1304 s / batch. (data: 7.98e-03). ETA=7:23:02, max mem: 5.0 GB 
[11/07 05:43:37][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.4417,	0.2579 s / batch. (data: 5.79e-03). ETA=14:36:12, max mem: 5.0 GB 
[11/07 05:43:59][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.9705,	0.1858 s / batch. (data: 5.38e-03). ETA=10:30:43, max mem: 5.0 GB 
[11/07 05:44:20][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.4146,	0.1782 s / batch. (data: 1.75e-02). ETA=10:04:36, max mem: 5.0 GB 
[11/07 05:44:39][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.9488,	0.0516 s / batch. (data: 1.50e-04). ETA=2:55:02, max mem: 5.0 GB 
[11/07 05:44:40][INFO] visual_prompt:  217: Epoch 8 / 100: avg data time: 2.77e-02, avg batch time: 0.2134, average train loss: 0.7614
[11/07 05:45:02][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.733, 0.0862 s / batch. (data: 2.03e-05)max mem: 4.96204 GB 
[11/07 05:45:22][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.733, 0.1286 s / batch. (data: 2.48e-05)max mem: 4.96204 GB 
[11/07 05:45:30][INFO] visual_prompt:  316: Inference (val):avg data time: 2.16e-04, avg batch time: 0.0632, average loss: 0.6901
[11/07 05:45:30][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.75	
[11/07 05:45:30][INFO] visual_prompt:   36: Best epoch 8: best metric: -0.690
[11/07 05:45:30][INFO] visual_prompt:  165: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/07 05:45:54][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 1.0796,	0.1806 s / batch. (data: 1.05e-02). ETA=10:12:04, max mem: 5.0 GB 
[11/07 05:46:17][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.7517,	0.1866 s / batch. (data: 2.86e-04). ETA=10:32:18, max mem: 5.0 GB 
[11/07 05:46:40][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.7709,	0.2107 s / batch. (data: 2.39e-04). ETA=11:53:32, max mem: 5.0 GB 
[11/07 05:47:01][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.6345,	0.2321 s / batch. (data: 1.09e-02). ETA=13:05:32, max mem: 5.0 GB 
[11/07 05:47:23][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.9164,	0.1472 s / batch. (data: 5.34e-03). ETA=8:18:08, max mem: 5.0 GB 
[11/07 05:47:45][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.3013,	0.1895 s / batch. (data: 8.44e-04). ETA=10:40:41, max mem: 5.0 GB 
[11/07 05:48:06][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.3901,	0.1812 s / batch. (data: 1.55e-02). ETA=10:12:31, max mem: 5.0 GB 
[11/07 05:48:26][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.7721,	0.2053 s / batch. (data: 1.55e-02). ETA=11:33:31, max mem: 5.0 GB 
[11/07 05:48:47][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.7586,	0.1914 s / batch. (data: 1.55e-02). ETA=10:46:13, max mem: 5.0 GB 
[11/07 05:49:08][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 1.0669,	0.1917 s / batch. (data: 1.42e-02). ETA=10:46:59, max mem: 5.0 GB 
[11/07 05:49:30][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.3990,	0.6153 s / batch. (data: 4.22e-01). ETA=1 day, 10:35:39, max mem: 5.0 GB 
[11/07 05:49:51][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.4463,	0.2023 s / batch. (data: 1.31e-02). ETA=11:22:15, max mem: 5.0 GB 
[11/07 05:50:10][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.2597,	0.1537 s / batch. (data: 7.92e-04). ETA=8:38:02, max mem: 5.0 GB 
[11/07 05:50:31][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.6847,	0.2235 s / batch. (data: 2.61e-02). ETA=12:32:59, max mem: 5.0 GB 
[11/07 05:50:52][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 1.1391,	0.2139 s / batch. (data: 6.78e-04). ETA=12:00:01, max mem: 5.0 GB 
[11/07 05:51:13][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.5599,	0.1915 s / batch. (data: 2.58e-02). ETA=10:44:28, max mem: 5.0 GB 
[11/07 05:51:34][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 1.4337,	0.2280 s / batch. (data: 7.07e-04). ETA=12:46:50, max mem: 5.0 GB 
[11/07 05:51:54][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.9363,	0.4562 s / batch. (data: 2.96e-01). ETA=1 day, 1:33:44, max mem: 5.0 GB 
[11/07 05:52:14][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.9229,	0.2373 s / batch. (data: 2.47e-02). ETA=13:17:12, max mem: 5.0 GB 
[11/07 05:52:36][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.5587,	0.1895 s / batch. (data: 2.40e-04). ETA=10:36:29, max mem: 5.0 GB 
[11/07 05:52:57][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 1.0290,	0.2102 s / batch. (data: 5.40e-03). ETA=11:45:26, max mem: 5.0 GB 
[11/07 05:53:20][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.6088,	0.2238 s / batch. (data: 1.18e-04). ETA=12:30:59, max mem: 5.0 GB 
[11/07 05:53:22][INFO] visual_prompt:  217: Epoch 9 / 100: avg data time: 3.10e-02, avg batch time: 0.2132, average train loss: 0.7356
[11/07 05:53:43][INFO] visual_prompt:  303: 	Test 100/246. loss: 1.048, 0.0977 s / batch. (data: 2.84e-05)max mem: 4.96204 GB 
[11/07 05:54:03][INFO] visual_prompt:  303: 	Test 200/246. loss: 1.048, 0.1088 s / batch. (data: 3.36e-05)max mem: 4.96204 GB 
[11/07 05:54:11][INFO] visual_prompt:  316: Inference (val):avg data time: 3.15e-05, avg batch time: 0.0627, average loss: 0.7098
[11/07 05:54:11][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.92	
[11/07 05:54:11][INFO] visual_prompt:  165: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/07 05:54:35][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.6151,	0.2310 s / batch. (data: 2.06e-02). ETA=12:54:34, max mem: 5.0 GB 
[11/07 05:54:57][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.7149,	0.3857 s / batch. (data: 2.55e-01). ETA=21:32:47, max mem: 5.0 GB 
[11/07 05:55:18][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 1.0263,	0.2088 s / batch. (data: 5.32e-03). ETA=11:39:33, max mem: 5.0 GB 
[11/07 05:55:39][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 1.0508,	0.1887 s / batch. (data: 1.55e-02). ETA=10:31:52, max mem: 5.0 GB 
[11/07 05:56:00][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.4159,	0.1678 s / batch. (data: 5.77e-04). ETA=9:21:31, max mem: 5.0 GB 
[11/07 05:56:21][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.9675,	0.2566 s / batch. (data: 2.62e-02). ETA=14:18:26, max mem: 5.0 GB 
[11/07 05:56:43][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.5522,	0.2192 s / batch. (data: 3.12e-02). ETA=12:12:59, max mem: 5.0 GB 
[11/07 05:57:06][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.4319,	0.2102 s / batch. (data: 7.35e-04). ETA=11:42:31, max mem: 5.0 GB 
[11/07 05:57:27][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.1828,	1.1060 s / batch. (data: 9.66e-01). ETA=2 days, 13:33:49, max mem: 5.0 GB 
[11/07 05:57:48][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.4579,	0.1729 s / batch. (data: 3.72e-04). ETA=9:37:17, max mem: 5.0 GB 
[11/07 05:58:10][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.6470,	0.2060 s / batch. (data: 6.85e-04). ETA=11:27:13, max mem: 5.0 GB 
[11/07 05:58:30][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 1.0791,	0.2184 s / batch. (data: 2.07e-02). ETA=12:08:24, max mem: 5.0 GB 
[11/07 05:58:51][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.9148,	0.1966 s / batch. (data: 2.06e-02). ETA=10:55:14, max mem: 5.0 GB 
[11/07 05:59:12][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 1.5989,	0.1592 s / batch. (data: 5.40e-03). ETA=8:50:21, max mem: 5.0 GB 
[11/07 05:59:33][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.5250,	0.1793 s / batch. (data: 7.81e-03). ETA=9:57:03, max mem: 5.0 GB 
[11/07 05:59:55][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.6246,	0.2222 s / batch. (data: 5.22e-02). ETA=12:19:24, max mem: 5.0 GB 
[11/07 06:00:16][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.3874,	0.2087 s / batch. (data: 2.23e-04). ETA=11:34:15, max mem: 5.0 GB 
[11/07 06:00:37][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 1.1034,	0.1610 s / batch. (data: 5.33e-03). ETA=8:55:23, max mem: 5.0 GB 
[11/07 06:00:58][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.7878,	0.1853 s / batch. (data: 5.36e-03). ETA=10:15:51, max mem: 5.0 GB 
[11/07 06:01:19][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.6634,	0.2261 s / batch. (data: 2.43e-04). ETA=12:30:52, max mem: 5.0 GB 
[11/07 06:01:42][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.6341,	0.2237 s / batch. (data: 1.09e-02). ETA=12:22:37, max mem: 5.0 GB 
[11/07 06:02:02][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.5619,	0.1180 s / batch. (data: 1.51e-04). ETA=6:31:29, max mem: 5.0 GB 
[11/07 06:02:03][INFO] visual_prompt:  217: Epoch 10 / 100: avg data time: 2.85e-02, avg batch time: 0.2133, average train loss: 0.7254
[11/07 06:02:24][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.613, 0.0968 s / batch. (data: 2.98e-05)max mem: 4.96204 GB 
[11/07 06:02:44][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.613, 0.1086 s / batch. (data: 3.10e-05)max mem: 4.96204 GB 
[11/07 06:02:52][INFO] visual_prompt:  316: Inference (val):avg data time: 1.05e-04, avg batch time: 0.0649, average loss: 0.7048
[11/07 06:02:52][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.03	
[11/07 06:02:52][INFO] visual_prompt:  165: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/07 06:03:16][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.2875,	0.2163 s / batch. (data: 1.55e-02). ETA=11:57:11, max mem: 5.0 GB 
[11/07 06:03:38][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.7569,	0.2270 s / batch. (data: 1.79e-02). ETA=12:32:19, max mem: 5.0 GB 
[11/07 06:04:01][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.7046,	0.2270 s / batch. (data: 5.37e-03). ETA=12:31:54, max mem: 5.0 GB 
[11/07 06:04:23][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.9635,	0.1957 s / batch. (data: 2.54e-02). ETA=10:47:58, max mem: 5.0 GB 
[11/07 06:04:44][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.9682,	0.1530 s / batch. (data: 7.00e-04). ETA=8:26:26, max mem: 5.0 GB 
[11/07 06:05:05][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.5917,	0.1170 s / batch. (data: 5.32e-04). ETA=6:27:09, max mem: 5.0 GB 
[11/07 06:05:27][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 1.0594,	0.9268 s / batch. (data: 8.28e-01). ETA=2 days, 3:04:15, max mem: 5.0 GB 
[11/07 06:05:47][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.6491,	0.2193 s / batch. (data: 1.04e-02). ETA=12:04:38, max mem: 5.0 GB 
[11/07 06:06:08][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.8829,	0.1812 s / batch. (data: 5.76e-03). ETA=9:58:20, max mem: 5.0 GB 
[11/07 06:06:28][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.7404,	0.1967 s / batch. (data: 3.25e-02). ETA=10:49:24, max mem: 5.0 GB 
[11/07 06:06:50][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.4195,	0.2780 s / batch. (data: 3.11e-02). ETA=15:17:12, max mem: 5.0 GB 
[11/07 06:07:10][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.7125,	0.1995 s / batch. (data: 1.66e-02). ETA=10:58:06, max mem: 5.0 GB 
[11/07 06:07:32][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.7946,	0.1820 s / batch. (data: 2.63e-04). ETA=9:59:54, max mem: 5.0 GB 
[11/07 06:07:53][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 1.7125,	0.1749 s / batch. (data: 1.55e-02). ETA=9:36:14, max mem: 5.0 GB 
[11/07 06:08:15][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.4543,	0.2061 s / batch. (data: 6.41e-04). ETA=11:18:47, max mem: 5.0 GB 
[11/07 06:08:36][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 1.1059,	0.2149 s / batch. (data: 5.36e-03). ETA=11:47:16, max mem: 5.0 GB 
[11/07 06:08:57][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.4332,	0.1789 s / batch. (data: 2.60e-04). ETA=9:48:23, max mem: 5.0 GB 
[11/07 06:09:18][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.3884,	0.2101 s / batch. (data: 2.34e-02). ETA=11:30:50, max mem: 5.0 GB 
[11/07 06:09:40][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.4469,	0.1861 s / batch. (data: 1.04e-02). ETA=10:11:25, max mem: 5.0 GB 
[11/07 06:10:01][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.7371,	0.2142 s / batch. (data: 5.40e-03). ETA=11:43:28, max mem: 5.0 GB 
[11/07 06:10:22][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.3644,	0.2153 s / batch. (data: 1.55e-02). ETA=11:46:57, max mem: 5.0 GB 
[11/07 06:10:43][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.7895,	0.1084 s / batch. (data: 1.24e-04). ETA=5:55:41, max mem: 5.0 GB 
[11/07 06:10:44][INFO] visual_prompt:  217: Epoch 11 / 100: avg data time: 2.92e-02, avg batch time: 0.2133, average train loss: 0.7167
[11/07 06:11:06][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.733, 0.0786 s / batch. (data: 2.69e-05)max mem: 4.96204 GB 
[11/07 06:11:25][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.733, 0.0957 s / batch. (data: 2.38e-05)max mem: 4.96204 GB 
[11/07 06:11:34][INFO] visual_prompt:  316: Inference (val):avg data time: 9.83e-05, avg batch time: 0.0636, average loss: 0.6901
[11/07 06:11:34][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.56	
[11/07 06:11:34][INFO] visual_prompt:   36: Best epoch 11: best metric: -0.690
[11/07 06:11:34][INFO] visual_prompt:  165: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/07 06:11:56][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.3699,	0.1725 s / batch. (data: 1.55e-02). ETA=9:25:39, max mem: 5.0 GB 
[11/07 06:12:16][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.2541,	0.1619 s / batch. (data: 1.05e-02). ETA=8:50:34, max mem: 5.0 GB 
[11/07 06:12:39][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.9950,	0.1897 s / batch. (data: 9.66e-03). ETA=10:21:23, max mem: 5.0 GB 
[11/07 06:13:00][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.2365,	0.1973 s / batch. (data: 2.06e-02). ETA=10:46:02, max mem: 5.0 GB 
[11/07 06:13:22][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.4296,	0.1711 s / batch. (data: 6.33e-04). ETA=9:19:52, max mem: 5.0 GB 
[11/07 06:13:44][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 1.0286,	0.1704 s / batch. (data: 7.88e-04). ETA=9:17:33, max mem: 5.0 GB 
[11/07 06:14:06][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.6107,	0.1595 s / batch. (data: 2.52e-04). ETA=8:41:36, max mem: 5.0 GB 
[11/07 06:14:28][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 1.2170,	0.2320 s / batch. (data: 5.69e-03). ETA=12:38:06, max mem: 5.0 GB 
[11/07 06:14:49][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.7222,	0.6334 s / batch. (data: 4.88e-01). ETA=1 day, 10:28:44, max mem: 5.0 GB 
[11/07 06:15:10][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.3871,	0.1994 s / batch. (data: 1.04e-02). ETA=10:50:52, max mem: 5.0 GB 
[11/07 06:15:31][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.8877,	0.2075 s / batch. (data: 2.06e-02). ETA=11:17:03, max mem: 5.0 GB 
[11/07 06:15:52][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 1.1470,	0.2472 s / batch. (data: 1.60e-02). ETA=13:26:13, max mem: 5.0 GB 
[11/07 06:16:12][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.4174,	0.2202 s / batch. (data: 5.37e-03). ETA=11:57:51, max mem: 5.0 GB 
[11/07 06:16:34][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.4779,	0.1925 s / batch. (data: 1.25e-02). ETA=10:27:09, max mem: 5.0 GB 
[11/07 06:16:56][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.4254,	0.4923 s / batch. (data: 3.53e-01). ETA=1 day, 2:42:57, max mem: 5.0 GB 
[11/07 06:17:17][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.7787,	0.1917 s / batch. (data: 6.61e-04). ETA=10:23:49, max mem: 5.0 GB 
[11/07 06:17:38][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.5578,	0.2306 s / batch. (data: 1.65e-02). ETA=12:30:09, max mem: 5.0 GB 
[11/07 06:18:00][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.8393,	0.1423 s / batch. (data: 6.66e-04). ETA=7:42:37, max mem: 5.0 GB 
[11/07 06:18:21][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.5474,	0.2368 s / batch. (data: 1.08e-02). ETA=12:49:19, max mem: 5.0 GB 
[11/07 06:18:41][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 1.0062,	0.1399 s / batch. (data: 1.04e-02). ETA=7:34:21, max mem: 5.0 GB 
[11/07 06:19:03][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.9365,	0.1773 s / batch. (data: 5.38e-03). ETA=9:35:36, max mem: 5.0 GB 
[11/07 06:19:24][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.3871,	0.1084 s / batch. (data: 8.54e-05). ETA=5:51:51, max mem: 5.0 GB 
[11/07 06:19:25][INFO] visual_prompt:  217: Epoch 12 / 100: avg data time: 2.94e-02, avg batch time: 0.2131, average train loss: 0.7121
[11/07 06:19:47][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.405, 0.0537 s / batch. (data: 2.62e-05)max mem: 4.96204 GB 
[11/07 06:20:06][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.404, 0.0755 s / batch. (data: 4.63e-05)max mem: 4.96204 GB 
[11/07 06:20:14][INFO] visual_prompt:  316: Inference (val):avg data time: 2.46e-04, avg batch time: 0.0637, average loss: 0.7860
[11/07 06:20:14][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.87	
[11/07 06:20:15][INFO] visual_prompt:  165: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/07 06:20:39][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.7230,	0.2146 s / batch. (data: 4.44e-02). ETA=11:35:57, max mem: 5.0 GB 
[11/07 06:21:01][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.6570,	0.1995 s / batch. (data: 1.55e-02). ETA=10:46:43, max mem: 5.0 GB 
[11/07 06:21:22][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.6309,	0.0982 s / batch. (data: 2.41e-04). ETA=5:17:57, max mem: 5.0 GB 
[11/07 06:21:42][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 1.0584,	0.1617 s / batch. (data: 9.13e-03). ETA=8:43:22, max mem: 5.0 GB 
[11/07 06:22:03][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.6756,	0.1906 s / batch. (data: 2.78e-04). ETA=10:16:47, max mem: 5.0 GB 
[11/07 06:22:25][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.5066,	0.1981 s / batch. (data: 6.38e-03). ETA=10:40:50, max mem: 5.0 GB 
[11/07 06:22:47][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.8665,	0.1977 s / batch. (data: 2.75e-04). ETA=10:38:58, max mem: 5.0 GB 
[11/07 06:23:09][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.6133,	0.2374 s / batch. (data: 1.05e-02). ETA=12:47:06, max mem: 5.0 GB 
[11/07 06:23:30][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.4481,	0.1909 s / batch. (data: 1.69e-02). ETA=10:16:22, max mem: 5.0 GB 
[11/07 06:23:51][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.4962,	0.2272 s / batch. (data: 1.04e-02). ETA=12:13:25, max mem: 5.0 GB 
[11/07 06:24:12][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.8152,	0.2015 s / batch. (data: 6.93e-04). ETA=10:50:03, max mem: 5.0 GB 
[11/07 06:24:34][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.6355,	0.2002 s / batch. (data: 2.78e-04). ETA=10:45:31, max mem: 5.0 GB 
[11/07 06:24:55][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 1.1509,	0.1852 s / batch. (data: 1.04e-02). ETA=9:56:48, max mem: 5.0 GB 
[11/07 06:25:17][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.5052,	0.2040 s / batch. (data: 1.55e-02). ETA=10:57:03, max mem: 5.0 GB 
[11/07 06:25:39][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.5306,	0.1982 s / batch. (data: 7.44e-03). ETA=10:37:55, max mem: 5.0 GB 
[11/07 06:26:00][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.6638,	0.2183 s / batch. (data: 1.04e-02). ETA=11:42:17, max mem: 5.0 GB 
[11/07 06:26:21][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.6068,	0.2178 s / batch. (data: 6.85e-04). ETA=11:40:30, max mem: 5.0 GB 
[11/07 06:26:41][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 1.0547,	0.2079 s / batch. (data: 2.05e-02). ETA=11:08:23, max mem: 5.0 GB 
[11/07 06:27:03][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.6676,	0.1857 s / batch. (data: 1.46e-02). ETA=9:56:31, max mem: 5.0 GB 
[11/07 06:27:24][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.5656,	0.2041 s / batch. (data: 2.63e-04). ETA=10:55:21, max mem: 5.0 GB 
[11/07 06:27:44][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.7913,	0.2042 s / batch. (data: 1.55e-02). ETA=10:55:19, max mem: 5.0 GB 
[11/07 06:28:05][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.7278,	0.1773 s / batch. (data: 1.10e-04). ETA=9:28:49, max mem: 5.0 GB 
[11/07 06:28:06][INFO] visual_prompt:  217: Epoch 13 / 100: avg data time: 2.88e-02, avg batch time: 0.2132, average train loss: 0.7075
[11/07 06:28:28][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.886, 0.0661 s / batch. (data: 3.39e-05)max mem: 4.96204 GB 
[11/07 06:28:47][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.886, 0.0870 s / batch. (data: 2.43e-05)max mem: 4.96204 GB 
[11/07 06:28:56][INFO] visual_prompt:  316: Inference (val):avg data time: 9.86e-05, avg batch time: 0.0626, average loss: 0.6915
[11/07 06:28:56][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.27	
[11/07 06:28:56][INFO] visual_prompt:  165: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/07 06:29:19][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.9027,	0.1763 s / batch. (data: 1.60e-02). ETA=9:25:09, max mem: 5.0 GB 
[11/07 06:29:41][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.7642,	0.1813 s / batch. (data: 8.07e-03). ETA=9:40:58, max mem: 5.0 GB 
[11/07 06:30:04][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.5944,	0.2206 s / batch. (data: 6.08e-03). ETA=11:46:21, max mem: 5.0 GB 
[11/07 06:30:25][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.8178,	0.1270 s / batch. (data: 6.34e-04). ETA=6:46:21, max mem: 5.0 GB 
[11/07 06:30:47][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.6737,	0.2103 s / batch. (data: 6.88e-03). ETA=11:12:43, max mem: 5.0 GB 
[11/07 06:31:09][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.4576,	0.2359 s / batch. (data: 1.80e-03). ETA=12:34:20, max mem: 5.0 GB 
[11/07 06:31:31][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.8349,	0.2002 s / batch. (data: 1.04e-02). ETA=10:39:43, max mem: 5.0 GB 
[11/07 06:31:53][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.6085,	0.2128 s / batch. (data: 7.26e-03). ETA=11:19:36, max mem: 5.0 GB 
[11/07 06:32:14][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.6647,	0.2431 s / batch. (data: 3.11e-02). ETA=12:56:08, max mem: 5.0 GB 
[11/07 06:32:34][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.5699,	0.2188 s / batch. (data: 1.54e-02). ETA=11:38:10, max mem: 5.0 GB 
[11/07 06:32:55][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 1.1023,	0.2770 s / batch. (data: 4.10e-02). ETA=14:43:14, max mem: 5.0 GB 
[11/07 06:33:16][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.9069,	0.2403 s / batch. (data: 8.59e-02). ETA=12:45:59, max mem: 5.0 GB 
[11/07 06:33:37][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.6878,	0.1908 s / batch. (data: 1.55e-02). ETA=10:07:55, max mem: 5.0 GB 
[11/07 06:33:58][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.8294,	0.1875 s / batch. (data: 1.55e-02). ETA=9:56:59, max mem: 5.0 GB 
[11/07 06:34:19][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.5525,	0.2097 s / batch. (data: 2.88e-04). ETA=11:07:25, max mem: 5.0 GB 
[11/07 06:34:41][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 1.3303,	0.2240 s / batch. (data: 2.77e-04). ETA=11:52:35, max mem: 5.0 GB 
[11/07 06:35:01][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.5754,	0.2555 s / batch. (data: 2.56e-02). ETA=13:32:23, max mem: 5.0 GB 
[11/07 06:35:23][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 1.2945,	0.2608 s / batch. (data: 1.09e-02). ETA=13:48:41, max mem: 5.0 GB 
[11/07 06:35:44][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 1.2849,	0.1713 s / batch. (data: 5.36e-03). ETA=9:04:07, max mem: 5.0 GB 
[11/07 06:36:05][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.7339,	0.1809 s / batch. (data: 7.10e-03). ETA=9:34:15, max mem: 5.0 GB 
[11/07 06:36:26][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.6208,	0.1860 s / batch. (data: 1.04e-02). ETA=9:50:11, max mem: 5.0 GB 
[11/07 06:36:46][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.7996,	0.1704 s / batch. (data: 1.44e-04). ETA=9:00:23, max mem: 5.0 GB 
[11/07 06:36:48][INFO] visual_prompt:  217: Epoch 14 / 100: avg data time: 2.84e-02, avg batch time: 0.2134, average train loss: 0.7077
[11/07 06:37:09][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.800, 0.0388 s / batch. (data: 3.70e-05)max mem: 4.96204 GB 
[11/07 06:37:29][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.800, 0.0711 s / batch. (data: 2.19e-05)max mem: 4.96204 GB 
[11/07 06:37:37][INFO] visual_prompt:  316: Inference (val):avg data time: 3.45e-04, avg batch time: 0.0643, average loss: 0.6884
[11/07 06:37:37][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.55	
[11/07 06:37:37][INFO] visual_prompt:   36: Best epoch 14: best metric: -0.688
[11/07 06:37:37][INFO] visual_prompt:  165: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/07 06:38:02][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.9677,	0.9184 s / batch. (data: 8.14e-01). ETA=2 days, 0:30:08, max mem: 5.0 GB 
[11/07 06:38:24][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.7287,	0.8985 s / batch. (data: 7.61e-01). ETA=1 day, 23:25:43, max mem: 5.0 GB 
[11/07 06:38:45][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.4303,	0.2417 s / batch. (data: 2.44e-02). ETA=12:45:15, max mem: 5.0 GB 
[11/07 06:39:08][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.8677,	0.1779 s / batch. (data: 5.87e-03). ETA=9:22:53, max mem: 5.0 GB 
[11/07 06:39:30][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.7920,	0.1871 s / batch. (data: 1.08e-02). ETA=9:51:37, max mem: 5.0 GB 
[11/07 06:39:52][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.7992,	0.1951 s / batch. (data: 1.03e-02). ETA=10:16:39, max mem: 5.0 GB 
[11/07 06:40:13][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.8823,	0.1978 s / batch. (data: 1.81e-02). ETA=10:24:48, max mem: 5.0 GB 
[11/07 06:40:34][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.7864,	0.2040 s / batch. (data: 5.76e-03). ETA=10:44:11, max mem: 5.0 GB 
[11/07 06:40:55][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.5326,	0.2020 s / batch. (data: 1.81e-02). ETA=10:37:33, max mem: 5.0 GB 
[11/07 06:41:17][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.6584,	0.1876 s / batch. (data: 7.02e-04). ETA=9:51:39, max mem: 5.0 GB 
[11/07 06:41:39][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.7092,	0.1682 s / batch. (data: 2.54e-04). ETA=8:50:09, max mem: 5.0 GB 
[11/07 06:42:01][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.8292,	0.2575 s / batch. (data: 2.57e-02). ETA=13:31:22, max mem: 5.0 GB 
[11/07 06:42:23][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.4521,	0.1852 s / batch. (data: 2.51e-04). ETA=9:43:15, max mem: 5.0 GB 
[11/07 06:42:45][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.7945,	0.1658 s / batch. (data: 3.95e-03). ETA=8:41:57, max mem: 5.0 GB 
[11/07 06:43:08][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.7751,	0.2180 s / batch. (data: 2.59e-02). ETA=11:25:40, max mem: 5.0 GB 
[11/07 06:43:30][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.6166,	0.2270 s / batch. (data: 1.55e-02). ETA=11:53:35, max mem: 5.0 GB 
[11/07 06:43:51][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.7998,	0.1974 s / batch. (data: 5.41e-03). ETA=10:20:20, max mem: 5.0 GB 
[11/07 06:44:13][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.5993,	0.2127 s / batch. (data: 2.56e-02). ETA=11:08:05, max mem: 5.0 GB 
[11/07 06:44:35][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.4660,	0.2023 s / batch. (data: 5.37e-03). ETA=10:35:08, max mem: 5.0 GB 
[11/07 06:44:58][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.6272,	0.1944 s / batch. (data: 6.21e-03). ETA=10:09:47, max mem: 5.0 GB 
[11/07 06:45:18][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.5943,	0.2248 s / batch. (data: 1.55e-02). ETA=11:44:42, max mem: 5.0 GB 
[11/07 06:45:39][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 1.1496,	0.1443 s / batch. (data: 1.19e-04). ETA=7:32:06, max mem: 5.0 GB 
[11/07 06:45:40][INFO] visual_prompt:  217: Epoch 15 / 100: avg data time: 3.28e-02, avg batch time: 0.2183, average train loss: 0.7036
[11/07 06:46:02][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.932, 0.0669 s / batch. (data: 6.39e-05)max mem: 4.96204 GB 
[11/07 06:46:23][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.932, 0.0957 s / batch. (data: 2.22e-05)max mem: 4.96204 GB 
[11/07 06:46:32][INFO] visual_prompt:  316: Inference (val):avg data time: 6.86e-05, avg batch time: 0.0665, average loss: 0.6952
[11/07 06:46:32][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.74	
[11/07 06:46:32][INFO] visual_prompt:  165: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/07 06:46:57][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.4694,	0.2043 s / batch. (data: 2.59e-02). ETA=10:40:01, max mem: 5.0 GB 
[11/07 06:47:19][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 1.0083,	0.1952 s / batch. (data: 1.63e-02). ETA=10:10:54, max mem: 5.0 GB 
[11/07 06:47:41][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.3909,	0.1997 s / batch. (data: 5.40e-03). ETA=10:24:46, max mem: 5.0 GB 
[11/07 06:48:02][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.8952,	0.1415 s / batch. (data: 2.13e-04). ETA=7:22:29, max mem: 5.0 GB 
[11/07 06:48:23][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.6695,	0.1871 s / batch. (data: 5.75e-03). ETA=9:44:39, max mem: 5.0 GB 
[11/07 06:48:44][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.5559,	0.2547 s / batch. (data: 1.09e-02). ETA=13:15:28, max mem: 5.0 GB 
[11/07 06:49:07][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.7890,	0.1972 s / batch. (data: 7.00e-04). ETA=10:15:40, max mem: 5.0 GB 
[11/07 06:49:29][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.6486,	0.1740 s / batch. (data: 7.30e-04). ETA=9:02:57, max mem: 5.0 GB 
[11/07 06:49:51][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.5895,	0.2186 s / batch. (data: 1.05e-02). ETA=11:21:49, max mem: 5.0 GB 
[11/07 06:50:14][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.9913,	0.2284 s / batch. (data: 8.94e-02). ETA=11:51:48, max mem: 5.0 GB 
[11/07 06:50:37][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.6420,	0.2026 s / batch. (data: 2.06e-02). ETA=10:31:06, max mem: 5.0 GB 
[11/07 06:50:59][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.9289,	0.2396 s / batch. (data: 2.88e-02). ETA=12:26:04, max mem: 5.0 GB 
[11/07 06:51:22][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.8247,	0.2306 s / batch. (data: 2.07e-02). ETA=11:57:36, max mem: 5.0 GB 
[11/07 06:51:45][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.9930,	0.1282 s / batch. (data: 2.69e-04). ETA=6:38:44, max mem: 5.0 GB 
[11/07 06:52:07][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.7594,	0.1533 s / batch. (data: 3.22e-04). ETA=7:56:40, max mem: 5.0 GB 
[11/07 06:52:30][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.6898,	0.1226 s / batch. (data: 5.38e-03). ETA=6:20:50, max mem: 5.0 GB 
[11/07 06:52:51][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.7022,	0.2258 s / batch. (data: 5.80e-03). ETA=11:41:01, max mem: 5.0 GB 
[11/07 06:53:13][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.8212,	0.1750 s / batch. (data: 2.83e-04). ETA=9:03:16, max mem: 5.0 GB 
[11/07 06:53:33][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.6050,	0.1829 s / batch. (data: 7.03e-03). ETA=9:27:24, max mem: 5.0 GB 
[11/07 06:53:55][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.8570,	0.2015 s / batch. (data: 2.54e-02). ETA=10:24:49, max mem: 5.0 GB 
[11/07 06:54:18][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.7165,	0.1413 s / batch. (data: 2.71e-04). ETA=7:17:58, max mem: 5.0 GB 
[11/07 06:54:40][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.4928,	0.1263 s / batch. (data: 1.45e-04). ETA=6:31:06, max mem: 5.0 GB 
[11/07 06:54:42][INFO] visual_prompt:  217: Epoch 16 / 100: avg data time: 3.23e-02, avg batch time: 0.2214, average train loss: 0.6984
[11/07 06:55:04][INFO] visual_prompt:  303: 	Test 100/246. loss: 1.003, 0.1128 s / batch. (data: 3.81e-05)max mem: 4.96204 GB 
[11/07 06:55:24][INFO] visual_prompt:  303: 	Test 200/246. loss: 1.003, 0.0527 s / batch. (data: 3.15e-05)max mem: 4.96204 GB 
[11/07 06:55:33][INFO] visual_prompt:  316: Inference (val):avg data time: 1.97e-04, avg batch time: 0.0695, average loss: 0.7033
[11/07 06:55:33][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.17	
[11/07 06:55:33][INFO] visual_prompt:  165: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/07 06:55:57][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.6363,	0.1623 s / batch. (data: 5.37e-03). ETA=8:22:15, max mem: 5.0 GB 
[11/07 06:56:20][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.6554,	0.1396 s / batch. (data: 2.86e-04). ETA=7:11:54, max mem: 5.0 GB 
[11/07 06:56:42][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.7995,	0.1988 s / batch. (data: 8.10e-04). ETA=10:14:36, max mem: 5.0 GB 
[11/07 06:57:03][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.7175,	0.1817 s / batch. (data: 5.40e-03). ETA=9:21:30, max mem: 5.0 GB 
[11/07 06:57:26][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.5609,	0.2256 s / batch. (data: 5.34e-03). ETA=11:36:37, max mem: 5.0 GB 
[11/07 06:57:49][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.7060,	0.1525 s / batch. (data: 7.01e-04). ETA=7:50:53, max mem: 5.0 GB 
[11/07 06:58:11][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.4648,	0.1891 s / batch. (data: 9.23e-03). ETA=9:43:26, max mem: 5.0 GB 
[11/07 06:58:32][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.7927,	0.4600 s / batch. (data: 3.47e-01). ETA=23:38:28, max mem: 5.0 GB 
[11/07 06:58:54][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.7785,	0.2363 s / batch. (data: 5.82e-03). ETA=12:08:18, max mem: 5.0 GB 
[11/07 06:59:16][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.7806,	0.1935 s / batch. (data: 1.53e-02). ETA=9:56:04, max mem: 5.0 GB 
[11/07 06:59:38][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.5921,	0.1954 s / batch. (data: 1.55e-02). ETA=10:01:37, max mem: 5.0 GB 
[11/07 07:00:00][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.8094,	0.2082 s / batch. (data: 2.06e-02). ETA=10:40:44, max mem: 5.0 GB 
[11/07 07:00:22][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.7803,	0.1522 s / batch. (data: 9.35e-03). ETA=7:48:04, max mem: 5.0 GB 
[11/07 07:00:45][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.5151,	0.2322 s / batch. (data: 2.43e-02). ETA=11:53:32, max mem: 5.0 GB 
[11/07 07:01:08][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.4629,	0.2168 s / batch. (data: 1.09e-02). ETA=11:05:50, max mem: 5.0 GB 
[11/07 07:01:31][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.8753,	0.2028 s / batch. (data: 2.56e-02). ETA=10:22:33, max mem: 5.0 GB 
[11/07 07:01:52][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.9717,	0.2201 s / batch. (data: 3.36e-02). ETA=11:15:30, max mem: 5.0 GB 
[11/07 07:02:14][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.9516,	0.2495 s / batch. (data: 1.76e-02). ETA=12:45:04, max mem: 5.0 GB 
[11/07 07:02:37][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.5035,	0.1701 s / batch. (data: 5.34e-03). ETA=8:41:24, max mem: 5.0 GB 
[11/07 07:03:00][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.4312,	0.2150 s / batch. (data: 2.82e-04). ETA=10:58:31, max mem: 5.0 GB 
[11/07 07:03:22][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.5092,	0.2091 s / batch. (data: 2.15e-04). ETA=10:40:05, max mem: 5.0 GB 
[11/07 07:03:44][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.8056,	0.1784 s / batch. (data: 1.20e-04). ETA=9:05:50, max mem: 5.0 GB 
[11/07 07:03:45][INFO] visual_prompt:  217: Epoch 17 / 100: avg data time: 3.04e-02, avg batch time: 0.2226, average train loss: 0.6962
[11/07 07:04:08][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.874, 0.1032 s / batch. (data: 3.50e-05)max mem: 4.96204 GB 
[11/07 07:04:28][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.874, 0.0943 s / batch. (data: 2.43e-05)max mem: 4.96204 GB 
[11/07 07:04:37][INFO] visual_prompt:  316: Inference (val):avg data time: 5.03e-04, avg batch time: 0.0666, average loss: 0.6907
[11/07 07:04:37][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.05	
[11/07 07:04:37][INFO] visual_prompt:  165: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/07 07:05:03][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.7501,	0.2423 s / batch. (data: 2.63e-02). ETA=12:21:04, max mem: 5.0 GB 
[11/07 07:05:24][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.5358,	0.1307 s / batch. (data: 6.38e-04). ETA=6:39:30, max mem: 5.0 GB 
[11/07 07:05:47][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.5017,	0.2149 s / batch. (data: 4.57e-03). ETA=10:56:28, max mem: 5.0 GB 
[11/07 07:06:10][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.5999,	0.2332 s / batch. (data: 2.06e-02). ETA=11:51:56, max mem: 5.0 GB 
[11/07 07:06:33][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.6620,	0.7881 s / batch. (data: 6.61e-01). ETA=1 day, 16:04:52, max mem: 5.0 GB 
[11/07 07:06:54][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.6789,	0.1826 s / batch. (data: 6.04e-03). ETA=9:16:56, max mem: 5.0 GB 
[11/07 07:07:16][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.9681,	0.2580 s / batch. (data: 5.90e-03). ETA=13:06:23, max mem: 5.0 GB 
[11/07 07:07:38][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.6214,	0.2329 s / batch. (data: 1.45e-02). ETA=11:49:41, max mem: 5.0 GB 
[11/07 07:08:00][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.6857,	0.1485 s / batch. (data: 8.15e-03). ETA=7:32:07, max mem: 5.0 GB 
[11/07 07:08:23][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.9058,	0.1734 s / batch. (data: 5.37e-03). ETA=8:47:47, max mem: 5.0 GB 
[11/07 07:08:44][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.6486,	0.1555 s / batch. (data: 9.92e-03). ETA=7:53:00, max mem: 5.0 GB 
[11/07 07:09:05][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.8900,	0.1675 s / batch. (data: 1.63e-02). ETA=8:29:14, max mem: 5.0 GB 
[11/07 07:09:27][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.5236,	0.2495 s / batch. (data: 1.51e-02). ETA=12:37:55, max mem: 5.0 GB 
[11/07 07:09:49][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.6347,	0.2319 s / batch. (data: 1.69e-02). ETA=11:44:04, max mem: 5.0 GB 
[11/07 07:10:12][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.9492,	0.2799 s / batch. (data: 3.00e-03). ETA=14:09:21, max mem: 5.0 GB 
[11/07 07:10:34][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.5987,	0.2372 s / batch. (data: 5.81e-03). ETA=11:59:38, max mem: 5.0 GB 
[11/07 07:10:57][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.6019,	0.1979 s / batch. (data: 3.04e-02). ETA=9:59:49, max mem: 5.0 GB 
[11/07 07:11:20][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.8767,	0.2540 s / batch. (data: 5.37e-03). ETA=12:49:37, max mem: 5.0 GB 
[11/07 07:11:41][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.5725,	0.1897 s / batch. (data: 2.50e-04). ETA=9:34:30, max mem: 5.0 GB 
[11/07 07:12:03][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.8071,	0.2060 s / batch. (data: 1.55e-02). ETA=10:23:27, max mem: 5.0 GB 
[11/07 07:12:24][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.5753,	0.2198 s / batch. (data: 7.29e-04). ETA=11:04:43, max mem: 5.0 GB 
[11/07 07:12:46][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.5579,	0.1470 s / batch. (data: 1.19e-04). ETA=7:24:17, max mem: 5.0 GB 
[11/07 07:12:47][INFO] visual_prompt:  217: Epoch 18 / 100: avg data time: 3.04e-02, avg batch time: 0.2216, average train loss: 0.6959
[11/07 07:13:10][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.822, 0.0579 s / batch. (data: 4.77e-05)max mem: 4.96204 GB 
[11/07 07:13:30][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.822, 0.0933 s / batch. (data: 3.00e-05)max mem: 4.96204 GB 
[11/07 07:13:39][INFO] visual_prompt:  316: Inference (val):avg data time: 6.83e-05, avg batch time: 0.0684, average loss: 0.6886
[11/07 07:13:39][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.04	
[11/07 07:13:39][INFO] visual_prompt:  165: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/07 07:14:03][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.8436,	0.2200 s / batch. (data: 1.04e-02). ETA=11:04:36, max mem: 5.0 GB 
[11/07 07:14:25][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.7897,	0.1994 s / batch. (data: 4.61e-02). ETA=10:02:01, max mem: 5.0 GB 
[11/07 07:14:47][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.8875,	0.2095 s / batch. (data: 2.31e-03). ETA=10:32:10, max mem: 5.0 GB 
[11/07 07:15:09][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.6162,	0.8067 s / batch. (data: 6.67e-01). ETA=1 day, 16:33:16, max mem: 5.0 GB 
[11/07 07:15:30][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.7398,	0.2314 s / batch. (data: 5.40e-03). ETA=11:37:41, max mem: 5.0 GB 
[11/07 07:15:54][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.8358,	0.1902 s / batch. (data: 2.55e-02). ETA=9:33:05, max mem: 5.0 GB 
[11/07 07:16:16][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.7556,	0.2167 s / batch. (data: 5.67e-03). ETA=10:52:42, max mem: 5.0 GB 
[11/07 07:16:38][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.5152,	0.2206 s / batch. (data: 5.36e-03). ETA=11:03:50, max mem: 5.0 GB 
[11/07 07:17:01][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.5930,	0.1622 s / batch. (data: 7.77e-03). ETA=8:07:53, max mem: 5.0 GB 
[11/07 07:17:22][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.5242,	0.2230 s / batch. (data: 2.81e-04). ETA=11:10:33, max mem: 5.0 GB 
[11/07 07:17:46][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.9529,	0.1948 s / batch. (data: 1.55e-02). ETA=9:45:23, max mem: 5.0 GB 
[11/07 07:18:08][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.9198,	0.2060 s / batch. (data: 1.66e-02). ETA=10:18:36, max mem: 5.0 GB 
[11/07 07:18:31][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.7943,	0.1283 s / batch. (data: 1.68e-02). ETA=6:24:58, max mem: 5.0 GB 
[11/07 07:18:52][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.6733,	0.1783 s / batch. (data: 2.06e-02). ETA=8:54:52, max mem: 5.0 GB 
[11/07 07:19:14][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.8200,	0.2381 s / batch. (data: 1.04e-02). ETA=11:53:43, max mem: 5.0 GB 
[11/07 07:19:37][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.6503,	0.3141 s / batch. (data: 2.16e-01). ETA=15:41:07, max mem: 5.0 GB 
[11/07 07:19:59][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.5133,	0.1937 s / batch. (data: 5.38e-03). ETA=9:40:10, max mem: 5.0 GB 
[11/07 07:20:21][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.6679,	0.2124 s / batch. (data: 2.48e-02). ETA=10:35:52, max mem: 5.0 GB 
[11/07 07:20:43][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.4675,	0.2466 s / batch. (data: 7.10e-04). ETA=12:17:43, max mem: 5.0 GB 
[11/07 07:21:05][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.6179,	0.2250 s / batch. (data: 1.56e-02). ETA=11:12:44, max mem: 5.0 GB 
[11/07 07:21:27][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.6366,	0.2085 s / batch. (data: 5.87e-03). ETA=10:23:03, max mem: 5.0 GB 
[11/07 07:21:49][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.5362,	0.1131 s / batch. (data: 1.02e-04). ETA=5:37:54, max mem: 5.0 GB 
[11/07 07:21:50][INFO] visual_prompt:  217: Epoch 19 / 100: avg data time: 3.05e-02, avg batch time: 0.2221, average train loss: 0.6951
[11/07 07:22:12][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.443, 0.0612 s / batch. (data: 3.72e-05)max mem: 4.96204 GB 
[11/07 07:22:33][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.443, 0.0992 s / batch. (data: 3.36e-05)max mem: 4.96204 GB 
[11/07 07:22:42][INFO] visual_prompt:  316: Inference (val):avg data time: 3.99e-05, avg batch time: 0.0678, average loss: 0.7636
[11/07 07:22:42][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.22	
[11/07 07:22:42][INFO] visual_prompt:  165: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/07 07:23:06][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.5643,	0.2132 s / batch. (data: 5.39e-03). ETA=10:36:20, max mem: 5.0 GB 
[11/07 07:23:30][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.9191,	0.1760 s / batch. (data: 1.59e-02). ETA=8:44:52, max mem: 5.0 GB 
[11/07 07:23:52][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.6327,	0.1873 s / batch. (data: 1.70e-02). ETA=9:18:26, max mem: 5.0 GB 
[11/07 07:24:15][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.6536,	0.1728 s / batch. (data: 5.41e-03). ETA=8:34:46, max mem: 5.0 GB 
[11/07 07:24:37][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.5253,	0.1410 s / batch. (data: 2.66e-04). ETA=6:59:51, max mem: 5.0 GB 
[11/07 07:24:59][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.7961,	0.2196 s / batch. (data: 5.97e-03). ETA=10:53:34, max mem: 5.0 GB 
[11/07 07:25:21][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.6373,	0.2290 s / batch. (data: 4.64e-02). ETA=11:21:12, max mem: 5.0 GB 
[11/07 07:25:42][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.5331,	0.2170 s / batch. (data: 2.46e-02). ETA=10:45:03, max mem: 5.0 GB 
[11/07 07:26:04][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.7667,	0.1874 s / batch. (data: 5.37e-03). ETA=9:16:53, max mem: 5.0 GB 
[11/07 07:26:25][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 1.0764,	0.2159 s / batch. (data: 1.15e-02). ETA=10:41:05, max mem: 5.0 GB 
[11/07 07:26:47][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.5108,	0.2061 s / batch. (data: 1.55e-02). ETA=10:11:41, max mem: 5.0 GB 
[11/07 07:27:09][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.7710,	0.1982 s / batch. (data: 2.39e-02). ETA=9:47:56, max mem: 5.0 GB 
[11/07 07:27:32][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.8242,	0.1867 s / batch. (data: 7.10e-04). ETA=9:13:27, max mem: 5.0 GB 
[11/07 07:27:55][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.5682,	0.1908 s / batch. (data: 1.94e-02). ETA=9:25:21, max mem: 5.0 GB 
[11/07 07:28:17][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.7445,	0.1941 s / batch. (data: 9.93e-03). ETA=9:34:51, max mem: 5.0 GB 
[11/07 07:28:38][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.9278,	0.2026 s / batch. (data: 1.05e-02). ETA=9:59:31, max mem: 5.0 GB 
[11/07 07:29:00][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.8476,	0.2306 s / batch. (data: 2.06e-02). ETA=11:22:07, max mem: 5.0 GB 
[11/07 07:29:22][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.5936,	0.2281 s / batch. (data: 1.09e-02). ETA=11:14:16, max mem: 5.0 GB 
[11/07 07:29:44][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.8725,	0.1911 s / batch. (data: 3.47e-02). ETA=9:24:36, max mem: 5.0 GB 
[11/07 07:30:06][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.6131,	0.2504 s / batch. (data: 2.14e-02). ETA=12:19:23, max mem: 5.0 GB 
[11/07 07:30:29][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.6718,	0.2025 s / batch. (data: 7.98e-03). ETA=9:57:37, max mem: 5.0 GB 
[11/07 07:30:51][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.6736,	0.1550 s / batch. (data: 1.58e-04). ETA=7:37:18, max mem: 5.0 GB 
[11/07 07:30:52][INFO] visual_prompt:  217: Epoch 20 / 100: avg data time: 2.93e-02, avg batch time: 0.2216, average train loss: 0.6938
[11/07 07:31:14][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.647, 0.0842 s / batch. (data: 3.60e-05)max mem: 4.96204 GB 
[11/07 07:31:35][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.646, 0.0736 s / batch. (data: 3.31e-05)max mem: 4.96204 GB 
[11/07 07:31:43][INFO] visual_prompt:  316: Inference (val):avg data time: 1.13e-04, avg batch time: 0.0679, average loss: 0.6985
[11/07 07:31:43][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.20	
[11/07 07:31:43][INFO] visual_prompt:  165: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/07 07:32:09][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.6487,	0.9866 s / batch. (data: 8.07e-01). ETA=2 days, 0:28:15, max mem: 5.0 GB 
[11/07 07:32:31][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.6634,	0.1838 s / batch. (data: 5.77e-03). ETA=9:01:21, max mem: 5.0 GB 
[11/07 07:32:55][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 1.0681,	0.1502 s / batch. (data: 1.88e-02). ETA=7:22:09, max mem: 5.0 GB 
[11/07 07:33:18][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.8451,	0.2340 s / batch. (data: 2.71e-02). ETA=11:28:36, max mem: 5.0 GB 
[11/07 07:33:39][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.7397,	0.1960 s / batch. (data: 1.04e-02). ETA=9:36:33, max mem: 5.0 GB 
[11/07 07:34:01][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.9434,	0.2618 s / batch. (data: 2.67e-02). ETA=12:49:38, max mem: 5.0 GB 
[11/07 07:34:24][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.5902,	0.2242 s / batch. (data: 2.10e-02). ETA=10:58:34, max mem: 5.0 GB 
[11/07 07:34:48][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.7074,	0.2290 s / batch. (data: 7.24e-04). ETA=11:12:25, max mem: 5.0 GB 
[11/07 07:35:10][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.8259,	0.1776 s / batch. (data: 3.05e-04). ETA=8:41:16, max mem: 5.0 GB 
[11/07 07:35:31][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.6558,	0.2668 s / batch. (data: 5.65e-02). ETA=13:02:22, max mem: 5.0 GB 
[11/07 07:35:52][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.7166,	0.2122 s / batch. (data: 5.40e-03). ETA=10:22:06, max mem: 5.0 GB 
[11/07 07:36:15][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.7590,	0.2131 s / batch. (data: 5.35e-03). ETA=10:24:16, max mem: 5.0 GB 
[11/07 07:36:38][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.6703,	0.7122 s / batch. (data: 5.63e-01). ETA=1 day, 10:45:09, max mem: 5.0 GB 
[11/07 07:36:59][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.6080,	0.2176 s / batch. (data: 2.10e-02). ETA=10:36:41, max mem: 5.0 GB 
[11/07 07:37:22][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.5322,	0.2442 s / batch. (data: 5.42e-03). ETA=11:54:01, max mem: 5.0 GB 
[11/07 07:37:43][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.5405,	0.1900 s / batch. (data: 2.13e-04). ETA=9:15:20, max mem: 5.0 GB 
[11/07 07:38:04][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.7534,	0.1501 s / batch. (data: 5.33e-03). ETA=7:18:22, max mem: 5.0 GB 
[11/07 07:38:26][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.6612,	0.1424 s / batch. (data: 2.57e-04). ETA=6:55:48, max mem: 5.0 GB 
[11/07 07:38:48][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.5902,	0.2575 s / batch. (data: 1.63e-02). ETA=12:31:19, max mem: 5.0 GB 
[11/07 07:39:10][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.7598,	0.1901 s / batch. (data: 1.56e-02). ETA=9:14:16, max mem: 5.0 GB 
[11/07 07:39:32][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.8507,	0.1865 s / batch. (data: 2.90e-02). ETA=9:03:35, max mem: 5.0 GB 
[11/07 07:39:54][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.5738,	0.1374 s / batch. (data: 1.51e-04). ETA=6:40:17, max mem: 5.0 GB 
[11/07 07:39:55][INFO] visual_prompt:  217: Epoch 21 / 100: avg data time: 3.03e-02, avg batch time: 0.2221, average train loss: 0.6930
[11/07 07:40:17][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.825, 0.0805 s / batch. (data: 2.79e-05)max mem: 4.96204 GB 
[11/07 07:40:37][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.825, 0.0734 s / batch. (data: 4.48e-05)max mem: 4.96204 GB 
[11/07 07:40:46][INFO] visual_prompt:  316: Inference (val):avg data time: 4.92e-05, avg batch time: 0.0682, average loss: 0.6887
[11/07 07:40:46][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.56	
[11/07 07:40:46][INFO] visual_prompt:   42: Stopping early.

[11/21 03:32:36][INFO] visual_prompt:   95: Rank of current process: 0. World size: 1
[11/21 03:32:36][INFO] visual_prompt:   97: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 03:32:36][INFO] visual_prompt:   99: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/21 03:32:36][INFO] visual_prompt:  101: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 03:32:36][INFO] visual_prompt:  108: Training with config:
[11/21 03:32:36][INFO] visual_prompt:  109: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.001_wd0.0001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 03:32:36][INFO] visual_prompt:   55: Loading training data...
[11/21 03:32:36][INFO] visual_prompt:   28: Constructing mammo-cbis dataset train...
[11/21 03:32:36][INFO] visual_prompt:   57: Loading validation data...
[11/21 03:32:36][INFO] visual_prompt:   28: Constructing mammo-cbis dataset val...
[11/21 03:32:36][INFO] visual_prompt:   38: Constructing models...
[11/21 03:32:38][INFO] visual_prompt:  153: Enable all parameters update during training
[11/21 03:32:38][INFO] visual_prompt:   52: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 03:32:38][INFO] visual_prompt:   54: tuned percent:100.000
[11/21 03:32:38][INFO] visual_prompt:   40: Device used for model: 0
[11/21 03:32:38][INFO] visual_prompt:   40: Setting up Evaluator...
[11/21 03:32:38][INFO] visual_prompt:   42: Setting up Trainer...
[11/21 03:32:38][INFO] visual_prompt:   45: 	Setting up the optimizer...
[11/21 03:32:38][INFO] visual_prompt:  165: Training 1 / 100 epoch, with learning rate 0.0
[11/21 03:34:21][INFO] visual_prompt:  204: 	Training 100/553. train loss: 10.8600,	0.9487 s / batch. (data: 1.55e-02). ETA=14:32:50, max mem: 27.1 GB 
[11/21 03:35:59][INFO] visual_prompt:  204: 	Training 200/553. train loss: 9.4496,	0.9400 s / batch. (data: 7.96e-03). ETA=14:23:11, max mem: 27.1 GB 
[11/21 03:37:34][INFO] visual_prompt:  204: 	Training 300/553. train loss: 9.8169,	0.9400 s / batch. (data: 7.96e-03). ETA=14:21:38, max mem: 27.1 GB 
[11/21 03:39:07][INFO] visual_prompt:  204: 	Training 400/553. train loss: 7.5668,	0.8935 s / batch. (data: 3.10e-04). ETA=13:37:32, max mem: 27.1 GB 
[11/21 03:40:43][INFO] visual_prompt:  204: 	Training 500/553. train loss: 3.1665,	0.9080 s / batch. (data: 2.94e-04). ETA=13:49:16, max mem: 27.1 GB 
[11/21 03:41:32][INFO] visual_prompt:  217: Epoch 1 / 100: avg data time: 4.40e-02, avg batch time: 0.9656, average train loss: 7.6130
[11/21 03:42:29][INFO] visual_prompt:  316: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3010, average loss: 6.9126
[11/21 03:42:30][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 03:42:30][INFO] visual_prompt:  165: Training 2 / 100 epoch, with learning rate 0.0002
[11/21 03:44:15][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.3838,	0.9116 s / batch. (data: 7.91e-03). ETA=13:50:14, max mem: 27.1 GB 
[11/21 03:45:48][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9826,	1.6660 s / batch. (data: 7.27e-01). ETA=1 day, 1:14:34, max mem: 27.1 GB 
[11/21 03:47:24][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7348,	0.9320 s / batch. (data: 2.47e-04). ETA=14:05:45, max mem: 27.1 GB 
[11/21 03:49:00][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.0478,	0.9456 s / batch. (data: 6.28e-03). ETA=14:16:32, max mem: 27.1 GB 
[11/21 03:50:35][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.1059,	0.9400 s / batch. (data: 2.91e-04). ETA=14:09:51, max mem: 27.1 GB 
[11/21 03:51:24][INFO] visual_prompt:  217: Epoch 2 / 100: avg data time: 4.62e-02, avg batch time: 0.9665, average train loss: 1.0189
[11/21 03:52:21][INFO] visual_prompt:  316: Inference (val):avg data time: 5.05e-04, avg batch time: 0.2986, average loss: 0.8974
[11/21 03:52:21][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.82	
[11/21 03:52:21][INFO] visual_prompt:  165: Training 3 / 100 epoch, with learning rate 0.0004
[11/21 03:54:06][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6868,	0.9300 s / batch. (data: 2.70e-04). ETA=13:58:29, max mem: 27.1 GB 
[11/21 03:55:42][INFO] visual_prompt:  204: 	Training 200/553. train loss: 3.2300,	0.9415 s / batch. (data: 1.99e-04). ETA=14:07:17, max mem: 27.1 GB 
[11/21 03:57:17][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6914,	0.9297 s / batch. (data: 5.86e-03). ETA=13:55:04, max mem: 27.1 GB 
[11/21 03:58:51][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9229,	0.9321 s / batch. (data: 7.62e-04). ETA=13:55:40, max mem: 27.1 GB 
[11/21 04:00:24][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.2135,	0.9452 s / batch. (data: 2.69e-04). ETA=14:05:51, max mem: 27.1 GB 
[11/21 04:01:13][INFO] visual_prompt:  217: Epoch 3 / 100: avg data time: 3.98e-02, avg batch time: 0.9612, average train loss: 0.8724
[11/21 04:02:10][INFO] visual_prompt:  316: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3000, average loss: 0.7653
[11/21 04:02:10][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.09	
[11/21 04:02:10][INFO] visual_prompt:  165: Training 4 / 100 epoch, with learning rate 0.0006
[11/21 04:03:56][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.8606,	0.9131 s / batch. (data: 6.80e-03). ETA=13:34:50, max mem: 27.1 GB 
[11/21 04:05:29][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9922,	0.9243 s / batch. (data: 1.05e-02). ETA=13:43:16, max mem: 27.1 GB 
[11/21 04:07:04][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.4010,	0.9287 s / batch. (data: 5.41e-03). ETA=13:45:39, max mem: 27.1 GB 
[11/21 04:08:39][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.4662,	0.9351 s / batch. (data: 2.54e-04). ETA=13:49:47, max mem: 27.1 GB 
[11/21 04:10:15][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.9864,	0.9241 s / batch. (data: 8.32e-03). ETA=13:38:29, max mem: 27.1 GB 
[11/21 04:11:05][INFO] visual_prompt:  217: Epoch 4 / 100: avg data time: 4.72e-02, avg batch time: 0.9670, average train loss: 0.8364
[11/21 04:12:03][INFO] visual_prompt:  316: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3014, average loss: 0.7393
[11/21 04:12:03][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 60.90	
[11/21 04:12:03][INFO] visual_prompt:  165: Training 5 / 100 epoch, with learning rate 0.0008
[11/21 04:13:47][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5950,	0.9288 s / batch. (data: 1.62e-02). ETA=13:40:14, max mem: 27.1 GB 
[11/21 04:15:23][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.4999,	0.9051 s / batch. (data: 3.29e-04). ETA=13:17:46, max mem: 27.1 GB 
[11/21 04:16:56][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.0810,	0.9231 s / batch. (data: 2.71e-04). ETA=13:32:09, max mem: 27.1 GB 
[11/21 04:18:32][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.3875,	2.9722 s / batch. (data: 2.05e+00). ETA=1 day, 19:30:01, max mem: 27.1 GB 
[11/21 04:20:08][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7022,	0.9211 s / batch. (data: 3.84e-03). ETA=13:27:20, max mem: 27.1 GB 
[11/21 04:20:57][INFO] visual_prompt:  217: Epoch 5 / 100: avg data time: 4.46e-02, avg batch time: 0.9654, average train loss: 0.8059
[11/21 04:21:54][INFO] visual_prompt:  316: Inference (val):avg data time: 3.73e-05, avg batch time: 0.2981, average loss: 0.7588
[11/21 04:21:54][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.42	
[11/21 04:21:54][INFO] visual_prompt:  165: Training 6 / 100 epoch, with learning rate 0.001
[11/21 04:23:40][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6954,	0.9308 s / batch. (data: 5.39e-03). ETA=13:33:24, max mem: 27.1 GB 
[11/21 04:25:13][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8495,	0.9400 s / batch. (data: 2.92e-04). ETA=13:39:54, max mem: 27.1 GB 
[11/21 04:26:48][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.0836,	0.9257 s / batch. (data: 2.93e-04). ETA=13:25:53, max mem: 27.1 GB 
[11/21 04:28:22][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6880,	1.3080 s / batch. (data: 3.79e-01). ETA=18:56:33, max mem: 27.1 GB 
[11/21 04:30:00][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8342,	0.9209 s / batch. (data: 2.71e-04). ETA=13:18:41, max mem: 27.1 GB 
[11/21 04:30:48][INFO] visual_prompt:  217: Epoch 6 / 100: avg data time: 4.59e-02, avg batch time: 0.9663, average train loss: 0.7977
[11/21 04:31:45][INFO] visual_prompt:  316: Inference (val):avg data time: 1.50e-04, avg batch time: 0.3001, average loss: 0.6731
[11/21 04:31:45][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 60.04	
[11/21 04:31:45][INFO] visual_prompt:  165: Training 7 / 100 epoch, with learning rate 0.000999726628670463
[11/21 04:33:36][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6780,	0.9332 s / batch. (data: 1.05e-02). ETA=13:26:54, max mem: 27.1 GB 
[11/21 04:35:09][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5776,	0.9155 s / batch. (data: 9.92e-03). ETA=13:10:07, max mem: 27.1 GB 
[11/21 04:36:43][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6702,	0.9355 s / batch. (data: 5.95e-03). ETA=13:25:50, max mem: 27.1 GB 
[11/21 04:38:16][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7325,	0.9299 s / batch. (data: 5.39e-03). ETA=13:19:27, max mem: 27.1 GB 
[11/21 04:39:50][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5668,	0.9277 s / batch. (data: 8.07e-03). ETA=13:15:58, max mem: 27.1 GB 
[11/21 04:40:40][INFO] visual_prompt:  217: Epoch 7 / 100: avg data time: 4.49e-02, avg batch time: 0.9661, average train loss: 0.7769
[11/21 04:41:37][INFO] visual_prompt:  316: Inference (val):avg data time: 8.62e-05, avg batch time: 0.3014, average loss: 0.6708
[11/21 04:41:37][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 61.08	
[11/21 04:41:37][INFO] visual_prompt:   36: Best epoch 7: best metric: -0.671
[11/21 04:41:37][INFO] visual_prompt:  165: Training 8 / 100 epoch, with learning rate 0.0009989068136093873
[11/21 04:43:22][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5727,	0.9600 s / batch. (data: 2.88e-04). ETA=13:41:16, max mem: 27.1 GB 
[11/21 04:44:59][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6480,	0.9438 s / batch. (data: 2.11e-02). ETA=13:25:51, max mem: 27.1 GB 
[11/21 04:46:32][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8312,	0.9548 s / batch. (data: 8.08e-03). ETA=13:33:38, max mem: 27.1 GB 
[11/21 04:48:08][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5980,	0.8978 s / batch. (data: 2.94e-04). ETA=12:43:34, max mem: 27.1 GB 
[11/21 04:49:44][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5961,	0.9345 s / batch. (data: 5.37e-03). ETA=13:13:15, max mem: 27.1 GB 
[11/21 04:50:33][INFO] visual_prompt:  217: Epoch 8 / 100: avg data time: 4.89e-02, avg batch time: 0.9684, average train loss: 0.7722
[11/21 04:51:30][INFO] visual_prompt:  316: Inference (val):avg data time: 8.71e-05, avg batch time: 0.3023, average loss: 0.6732
[11/21 04:51:30][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 62.43	
[11/21 04:51:30][INFO] visual_prompt:  165: Training 9 / 100 epoch, with learning rate 0.0009975414512725057
[11/21 04:53:18][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7839,	0.8988 s / batch. (data: 7.97e-03). ETA=12:40:37, max mem: 27.1 GB 
[11/21 04:54:53][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.2177,	0.9361 s / batch. (data: 1.05e-02). ETA=13:10:40, max mem: 27.1 GB 
[11/21 04:56:26][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5714,	0.9220 s / batch. (data: 5.37e-03). ETA=12:57:10, max mem: 27.1 GB 
[11/21 04:57:59][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6399,	0.9236 s / batch. (data: 9.86e-04). ETA=12:57:01, max mem: 27.1 GB 
[11/21 04:59:32][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6732,	0.9331 s / batch. (data: 1.31e-02). ETA=13:03:26, max mem: 27.1 GB 
[11/21 05:00:22][INFO] visual_prompt:  217: Epoch 9 / 100: avg data time: 4.11e-02, avg batch time: 0.9619, average train loss: 0.7845
[11/21 05:01:20][INFO] visual_prompt:  316: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3017, average loss: 0.6995
[11/21 05:01:20][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 60.51	
[11/21 05:01:20][INFO] visual_prompt:  165: Training 10 / 100 epoch, with learning rate 0.0009956320346634876
[11/21 05:03:05][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.8198,	0.9451 s / batch. (data: 5.17e-03). ETA=13:11:04, max mem: 27.1 GB 
[11/21 05:04:42][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7008,	0.9055 s / batch. (data: 2.55e-04). ETA=12:36:24, max mem: 27.1 GB 
[11/21 05:06:15][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8111,	0.9246 s / batch. (data: 5.84e-03). ETA=12:50:52, max mem: 27.1 GB 
[11/21 05:07:48][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6473,	0.9560 s / batch. (data: 2.92e-04). ETA=13:15:26, max mem: 27.1 GB 
[11/21 05:09:23][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6517,	0.9074 s / batch. (data: 3.56e-04). ETA=12:33:26, max mem: 27.1 GB 
[11/21 05:10:14][INFO] visual_prompt:  217: Epoch 10 / 100: avg data time: 4.59e-02, avg batch time: 0.9651, average train loss: 0.7522
[11/21 05:11:11][INFO] visual_prompt:  316: Inference (val):avg data time: 3.21e-04, avg batch time: 0.3036, average loss: 0.7778
[11/21 05:11:11][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 59.54	
[11/21 05:11:11][INFO] visual_prompt:  165: Training 11 / 100 epoch, with learning rate 0.0009931806517013613
[11/21 05:12:59][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.0082,	0.9170 s / batch. (data: 5.12e-03). ETA=12:39:08, max mem: 27.1 GB 
[11/21 05:14:33][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8066,	0.9151 s / batch. (data: 1.05e-02). ETA=12:36:03, max mem: 27.1 GB 
[11/21 05:16:06][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5032,	0.9104 s / batch. (data: 7.74e-04). ETA=12:30:35, max mem: 27.1 GB 
[11/21 05:17:41][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.0039,	0.9059 s / batch. (data: 2.80e-04). ETA=12:25:22, max mem: 27.1 GB 
[11/21 05:19:15][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5149,	0.9360 s / batch. (data: 2.88e-04). ETA=12:48:36, max mem: 27.1 GB 
[11/21 05:20:04][INFO] visual_prompt:  217: Epoch 11 / 100: avg data time: 4.32e-02, avg batch time: 0.9639, average train loss: 0.7361
[11/21 05:21:01][INFO] visual_prompt:  316: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3022, average loss: 0.6923
[11/21 05:21:01][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 55.74	
[11/21 05:21:01][INFO] visual_prompt:  165: Training 12 / 100 epoch, with learning rate 0.0009901899829374047
[11/21 05:22:44][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5596,	0.9336 s / batch. (data: 2.66e-04). ETA=12:44:14, max mem: 27.1 GB 
[11/21 05:24:22][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8426,	0.9040 s / batch. (data: 2.69e-04). ETA=12:18:32, max mem: 27.1 GB 
[11/21 05:25:57][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8374,	0.9241 s / batch. (data: 1.05e-02). ETA=12:33:25, max mem: 27.1 GB 
[11/21 05:27:31][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.0728,	0.9520 s / batch. (data: 2.75e-04). ETA=12:54:35, max mem: 27.1 GB 
[11/21 05:29:06][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6859,	0.9400 s / batch. (data: 7.81e-04). ETA=12:43:13, max mem: 27.1 GB 
[11/21 05:29:56][INFO] visual_prompt:  217: Epoch 12 / 100: avg data time: 4.68e-02, avg batch time: 0.9661, average train loss: 0.7657
[11/21 05:30:53][INFO] visual_prompt:  316: Inference (val):avg data time: 8.71e-05, avg batch time: 0.3006, average loss: 1.0292
[11/21 05:30:53][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.44	
[11/21 05:30:53][INFO] visual_prompt:  165: Training 13 / 100 epoch, with learning rate 0.000986663298624003
[11/21 05:32:40][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5912,	0.9617 s / batch. (data: 3.76e-02). ETA=12:58:23, max mem: 27.1 GB 
[11/21 05:34:13][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9927,	0.9319 s / batch. (data: 5.40e-03). ETA=12:32:41, max mem: 27.1 GB 
[11/21 05:35:49][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6511,	0.9307 s / batch. (data: 1.05e-02). ETA=12:30:12, max mem: 27.1 GB 
[11/21 05:37:25][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7264,	0.9251 s / batch. (data: 2.89e-04). ETA=12:24:07, max mem: 27.1 GB 
[11/21 05:38:59][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7676,	0.9313 s / batch. (data: 7.79e-04). ETA=12:27:35, max mem: 27.1 GB 
[11/21 05:39:48][INFO] visual_prompt:  217: Epoch 13 / 100: avg data time: 4.91e-02, avg batch time: 0.9672, average train loss: 0.7372
[11/21 05:40:46][INFO] visual_prompt:  316: Inference (val):avg data time: 3.51e-05, avg batch time: 0.3024, average loss: 0.7227
[11/21 05:40:46][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 60.19	
[11/21 05:40:46][INFO] visual_prompt:  165: Training 14 / 100 epoch, with learning rate 0.0009826044551386743
[11/21 05:42:33][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6014,	0.9267 s / batch. (data: 2.64e-04). ETA=12:21:33, max mem: 27.1 GB 
[11/21 05:44:10][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.2965,	0.9154 s / batch. (data: 8.77e-04). ETA=12:10:56, max mem: 27.1 GB 
[11/21 05:45:44][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6809,	0.9114 s / batch. (data: 3.24e-04). ETA=12:06:15, max mem: 27.1 GB 
[11/21 05:47:17][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9096,	0.9120 s / batch. (data: 3.05e-04). ETA=12:05:11, max mem: 27.1 GB 
[11/21 05:48:52][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6614,	0.9339 s / batch. (data: 7.93e-03). ETA=12:21:02, max mem: 27.1 GB 
[11/21 05:49:41][INFO] visual_prompt:  217: Epoch 14 / 100: avg data time: 4.80e-02, avg batch time: 0.9677, average train loss: 0.7440
[11/21 05:50:38][INFO] visual_prompt:  316: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3006, average loss: 0.7037
[11/21 05:50:38][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 55.66	
[11/21 05:50:38][INFO] visual_prompt:  165: Training 15 / 100 epoch, with learning rate 0.0009780178907671788
[11/21 05:52:25][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7290,	0.9292 s / batch. (data: 5.39e-03). ETA=12:14:56, max mem: 27.1 GB 
[11/21 05:54:01][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5956,	0.9049 s / batch. (data: 7.95e-03). ETA=11:54:15, max mem: 27.1 GB 
[11/21 05:55:35][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.0009,	0.9200 s / batch. (data: 3.97e-03). ETA=12:04:37, max mem: 27.1 GB 
[11/21 05:57:09][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6230,	0.9199 s / batch. (data: 2.58e-04). ETA=12:03:00, max mem: 27.1 GB 
[11/21 05:58:43][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8899,	0.9218 s / batch. (data: 2.67e-04). ETA=12:02:58, max mem: 27.1 GB 
[11/21 05:59:32][INFO] visual_prompt:  217: Epoch 15 / 100: avg data time: 4.52e-02, avg batch time: 0.9646, average train loss: 0.7319
[11/21 06:00:29][INFO] visual_prompt:  316: Inference (val):avg data time: 3.56e-05, avg batch time: 0.3019, average loss: 0.7418
[11/21 06:00:29][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.13	
[11/21 06:00:29][INFO] visual_prompt:  165: Training 16 / 100 epoch, with learning rate 0.0009729086208503173
[11/21 06:02:13][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7656,	0.9421 s / batch. (data: 5.37e-03). ETA=12:16:27, max mem: 27.1 GB 
[11/21 06:03:46][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6125,	0.9159 s / batch. (data: 7.99e-04). ETA=11:54:31, max mem: 27.1 GB 
[11/21 06:05:24][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9523,	0.9260 s / batch. (data: 2.97e-04). ETA=12:00:47, max mem: 27.1 GB 
[11/21 06:07:00][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6501,	0.9200 s / batch. (data: 7.98e-03). ETA=11:54:35, max mem: 27.1 GB 
[11/21 06:08:32][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6839,	0.9640 s / batch. (data: 5.49e-03). ETA=12:27:11, max mem: 27.1 GB 
[11/21 06:09:22][INFO] visual_prompt:  217: Epoch 16 / 100: avg data time: 4.41e-02, avg batch time: 0.9636, average train loss: 0.7179
[11/21 06:10:19][INFO] visual_prompt:  316: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3007, average loss: 0.7825
[11/21 06:10:19][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.26	
[11/21 06:10:19][INFO] visual_prompt:  165: Training 17 / 100 epoch, with learning rate 0.0009672822322997304
[11/21 06:12:01][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7966,	0.9366 s / batch. (data: 2.74e-04). ETA=12:03:31, max mem: 27.1 GB 
[11/21 06:13:40][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8273,	0.9243 s / batch. (data: 5.44e-03). ETA=11:52:30, max mem: 27.1 GB 
[11/21 06:15:13][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.2650,	0.9131 s / batch. (data: 2.86e-04). ETA=11:42:22, max mem: 27.1 GB 
[11/21 06:16:47][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6766,	0.9621 s / batch. (data: 2.21e-02). ETA=12:18:27, max mem: 27.1 GB 
[11/21 06:18:23][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6942,	0.9062 s / batch. (data: 5.37e-03). ETA=11:34:03, max mem: 27.1 GB 
[11/21 06:19:13][INFO] visual_prompt:  217: Epoch 17 / 100: avg data time: 4.45e-02, avg batch time: 0.9645, average train loss: 0.7223
[11/21 06:20:10][INFO] visual_prompt:  316: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3020, average loss: 0.6803
[11/21 06:20:10][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 57.16	
[11/21 06:20:10][INFO] visual_prompt:  165: Training 18 / 100 epoch, with learning rate 0.0009611448774886924
[11/21 06:21:58][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6996,	0.9175 s / batch. (data: 9.50e-03). ETA=11:40:20, max mem: 27.1 GB 
[11/21 06:23:33][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7133,	0.9400 s / batch. (data: 2.98e-04). ETA=11:55:56, max mem: 27.1 GB 
[11/21 06:25:06][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7091,	0.9285 s / batch. (data: 4.48e-03). ETA=11:45:38, max mem: 27.1 GB 
[11/21 06:26:39][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6981,	0.9097 s / batch. (data: 2.85e-04). ETA=11:29:52, max mem: 27.1 GB 
[11/21 06:28:16][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6865,	0.9226 s / batch. (data: 8.00e-04). ETA=11:38:05, max mem: 27.1 GB 
[11/21 06:29:05][INFO] visual_prompt:  217: Epoch 18 / 100: avg data time: 4.74e-02, avg batch time: 0.9665, average train loss: 0.7198
[11/21 06:30:02][INFO] visual_prompt:  316: Inference (val):avg data time: 3.46e-05, avg batch time: 0.3025, average loss: 0.7417
[11/21 06:30:02][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.88	
[11/21 06:30:02][INFO] visual_prompt:  165: Training 19 / 100 epoch, with learning rate 0.0009545032675245813
[11/21 06:31:47][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7048,	0.9240 s / batch. (data: 7.97e-03). ETA=11:36:45, max mem: 27.1 GB 
[11/21 06:33:24][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7849,	3.6098 s / batch. (data: 2.71e+00). ETA=1 day, 21:16:08, max mem: 27.1 GB 
[11/21 06:34:57][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7007,	0.9280 s / batch. (data: 1.04e-02). ETA=11:36:43, max mem: 27.1 GB 
[11/21 06:36:32][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6829,	0.9172 s / batch. (data: 6.13e-03). ETA=11:27:04, max mem: 27.1 GB 
[11/21 06:38:06][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.4757,	0.9280 s / batch. (data: 2.92e-04). ETA=11:33:35, max mem: 27.1 GB 
[11/21 06:38:55][INFO] visual_prompt:  217: Epoch 19 / 100: avg data time: 4.49e-02, avg batch time: 0.9639, average train loss: 0.7821
[11/21 06:39:53][INFO] visual_prompt:  316: Inference (val):avg data time: 6.57e-04, avg batch time: 0.3021, average loss: 0.9451
[11/21 06:39:53][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.37	
[11/21 06:39:53][INFO] visual_prompt:  165: Training 20 / 100 epoch, with learning rate 0.0009473646649103818
[11/21 06:41:41][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6838,	0.9141 s / batch. (data: 5.24e-03). ETA=11:20:54, max mem: 27.1 GB 
[11/21 06:43:13][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6984,	0.9280 s / batch. (data: 5.43e-03). ETA=11:29:41, max mem: 27.1 GB 
[11/21 06:44:48][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6681,	0.9400 s / batch. (data: 7.98e-03). ETA=11:37:03, max mem: 27.1 GB 
[11/21 06:46:23][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7772,	0.9429 s / batch. (data: 1.09e-02). ETA=11:37:38, max mem: 27.1 GB 
[11/21 06:47:55][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7336,	0.9335 s / batch. (data: 5.40e-03). ETA=11:29:07, max mem: 27.1 GB 
[11/21 06:48:48][INFO] visual_prompt:  217: Epoch 20 / 100: avg data time: 4.88e-02, avg batch time: 0.9671, average train loss: 0.7211
[11/21 06:49:45][INFO] visual_prompt:  316: Inference (val):avg data time: 1.35e-04, avg batch time: 0.3012, average loss: 0.6857
[11/21 06:49:45][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.66	
[11/21 06:49:45][INFO] visual_prompt:  165: Training 21 / 100 epoch, with learning rate 0.0009397368756032445
[11/21 06:51:33][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7179,	0.9043 s / batch. (data: 7.44e-04). ETA=11:05:17, max mem: 27.1 GB 
[11/21 06:53:08][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7321,	0.9480 s / batch. (data: 1.14e-03). ETA=11:35:49, max mem: 27.1 GB 
[11/21 06:54:43][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7022,	0.9309 s / batch. (data: 2.81e-04). ETA=11:21:42, max mem: 27.1 GB 
[11/21 06:56:17][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9374,	0.9240 s / batch. (data: 2.80e-04). ETA=11:15:07, max mem: 27.1 GB 
[11/21 06:57:50][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6295,	0.9213 s / batch. (data: 1.05e-02). ETA=11:11:37, max mem: 27.1 GB 
[11/21 06:58:39][INFO] visual_prompt:  217: Epoch 21 / 100: avg data time: 4.49e-02, avg batch time: 0.9652, average train loss: 0.7111
[11/21 06:59:36][INFO] visual_prompt:  316: Inference (val):avg data time: 1.66e-04, avg batch time: 0.3022, average loss: 0.6965
[11/21 06:59:36][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.38	
[11/21 06:59:36][INFO] visual_prompt:   42: Stopping early.

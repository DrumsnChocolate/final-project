[11/21 22:59:24][INFO] visual_prompt:   95: Rank of current process: 0. World size: 1
[11/21 22:59:24][INFO] visual_prompt:   97: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 22:59:24][INFO] visual_prompt:   99: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/21 22:59:24][INFO] visual_prompt:  101: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 22:59:24][INFO] visual_prompt:  108: Training with config:
[11/21 22:59:24][INFO] visual_prompt:  109: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0005_wd0.0/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 22:59:24][INFO] visual_prompt:   55: Loading training data...
[11/21 22:59:24][INFO] visual_prompt:   28: Constructing mammo-cbis dataset train...
[11/21 22:59:24][INFO] visual_prompt:   57: Loading validation data...
[11/21 22:59:24][INFO] visual_prompt:   28: Constructing mammo-cbis dataset val...
[11/21 22:59:24][INFO] visual_prompt:   38: Constructing models...
[11/21 22:59:26][INFO] visual_prompt:  153: Enable all parameters update during training
[11/21 22:59:26][INFO] visual_prompt:   52: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 22:59:26][INFO] visual_prompt:   54: tuned percent:100.000
[11/21 22:59:26][INFO] visual_prompt:   40: Device used for model: 0
[11/21 22:59:26][INFO] visual_prompt:   40: Setting up Evaluator...
[11/21 22:59:26][INFO] visual_prompt:   42: Setting up Trainer...
[11/21 22:59:26][INFO] visual_prompt:   45: 	Setting up the optimizer...
[11/21 22:59:26][INFO] visual_prompt:  165: Training 1 / 100 epoch, with learning rate 0.0
[11/21 23:01:09][INFO] visual_prompt:  204: 	Training 100/553. train loss: 10.8600,	0.9245 s / batch. (data: 5.41e-03). ETA=14:10:31, max mem: 32.0 GB 
[11/21 23:02:48][INFO] visual_prompt:  204: 	Training 200/553. train loss: 9.4496,	0.8920 s / batch. (data: 3.75e-04). ETA=13:39:09, max mem: 32.0 GB 
[11/21 23:04:24][INFO] visual_prompt:  204: 	Training 300/553. train loss: 9.8169,	0.9080 s / batch. (data: 2.86e-04). ETA=13:52:20, max mem: 32.0 GB 
[11/21 23:05:57][INFO] visual_prompt:  204: 	Training 400/553. train loss: 7.5668,	0.9280 s / batch. (data: 7.94e-03). ETA=14:09:06, max mem: 32.0 GB 
[11/21 23:07:33][INFO] visual_prompt:  204: 	Training 500/553. train loss: 3.1665,	0.9119 s / batch. (data: 5.73e-03). ETA=13:52:54, max mem: 32.0 GB 
[11/21 23:08:22][INFO] visual_prompt:  217: Epoch 1 / 100: avg data time: 5.31e-02, avg batch time: 0.9681, average train loss: 7.6130
[11/21 23:09:19][INFO] visual_prompt:  316: Inference (val):avg data time: 3.75e-05, avg batch time: 0.2999, average loss: 6.9126
[11/21 23:09:19][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 23:09:19][INFO] visual_prompt:  165: Training 2 / 100 epoch, with learning rate 0.0001
[11/21 23:11:06][INFO] visual_prompt:  204: 	Training 100/553. train loss: 2.2479,	0.9098 s / batch. (data: 8.29e-04). ETA=13:48:39, max mem: 32.0 GB 
[11/21 23:12:39][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.4912,	1.8689 s / batch. (data: 9.56e-01). ETA=1 day, 4:19:00, max mem: 32.0 GB 
[11/21 23:14:15][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.0397,	0.9338 s / batch. (data: 1.38e-02). ETA=14:07:22, max mem: 32.0 GB 
[11/21 23:15:51][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7003,	0.9240 s / batch. (data: 7.94e-03). ETA=13:56:56, max mem: 32.0 GB 
[11/21 23:17:27][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7592,	0.9094 s / batch. (data: 2.97e-04). ETA=13:42:14, max mem: 32.0 GB 
[11/21 23:18:15][INFO] visual_prompt:  217: Epoch 2 / 100: avg data time: 5.38e-02, avg batch time: 0.9688, average train loss: 0.9642
[11/21 23:19:13][INFO] visual_prompt:  316: Inference (val):avg data time: 2.88e-04, avg batch time: 0.3027, average loss: 0.9557
[11/21 23:19:13][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.97	
[11/21 23:19:13][INFO] visual_prompt:  165: Training 3 / 100 epoch, with learning rate 0.0002
[11/21 23:20:58][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6841,	0.9597 s / batch. (data: 2.39e-02). ETA=14:25:15, max mem: 32.0 GB 
[11/21 23:22:34][INFO] visual_prompt:  204: 	Training 200/553. train loss: 2.6665,	0.9320 s / batch. (data: 1.59e-02). ETA=13:58:40, max mem: 32.0 GB 
[11/21 23:24:09][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9654,	0.9280 s / batch. (data: 8.12e-04). ETA=13:53:35, max mem: 32.0 GB 
[11/21 23:25:43][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7961,	0.9160 s / batch. (data: 7.82e-04). ETA=13:41:15, max mem: 32.0 GB 
[11/21 23:27:16][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.4374,	0.9374 s / batch. (data: 5.43e-03). ETA=13:58:53, max mem: 32.0 GB 
[11/21 23:28:06][INFO] visual_prompt:  217: Epoch 3 / 100: avg data time: 4.86e-02, avg batch time: 0.9634, average train loss: 0.8697
[11/21 23:29:03][INFO] visual_prompt:  316: Inference (val):avg data time: 3.21e-04, avg batch time: 0.3012, average loss: 0.6843
[11/21 23:29:03][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 58.55	
[11/21 23:29:03][INFO] visual_prompt:  165: Training 4 / 100 epoch, with learning rate 0.0003
[11/21 23:30:48][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6966,	0.9120 s / batch. (data: 3.00e-04). ETA=13:33:49, max mem: 32.0 GB 
[11/21 23:32:24][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.4323,	1.4876 s / batch. (data: 5.84e-01). ETA=22:04:56, max mem: 32.0 GB 
[11/21 23:33:58][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.2915,	0.9240 s / batch. (data: 7.94e-03). ETA=13:41:26, max mem: 32.0 GB 
[11/21 23:35:33][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6585,	0.9200 s / batch. (data: 3.94e-03). ETA=13:36:21, max mem: 32.0 GB 
[11/21 23:37:09][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8498,	0.9560 s / batch. (data: 8.40e-04). ETA=14:06:41, max mem: 32.0 GB 
[11/21 23:38:00][INFO] visual_prompt:  217: Epoch 4 / 100: avg data time: 5.39e-02, avg batch time: 0.9697, average train loss: 0.8451
[11/21 23:38:57][INFO] visual_prompt:  316: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3004, average loss: 0.7263
[11/21 23:38:57][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 60.99	
[11/21 23:38:57][INFO] visual_prompt:  165: Training 5 / 100 epoch, with learning rate 0.0004
[11/21 23:40:42][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.4784,	0.9128 s / batch. (data: 6.19e-03). ETA=13:26:07, max mem: 32.0 GB 
[11/21 23:42:18][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.4395,	0.9120 s / batch. (data: 3.76e-03). ETA=13:23:53, max mem: 32.0 GB 
[11/21 23:43:51][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.8232,	0.9240 s / batch. (data: 3.95e-03). ETA=13:32:56, max mem: 32.0 GB 
[11/21 23:45:28][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9823,	3.2121 s / batch. (data: 2.29e+00). ETA=1 day, 23:00:41, max mem: 32.0 GB 
[11/21 23:47:05][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7993,	0.9419 s / batch. (data: 1.62e-02). ETA=13:45:32, max mem: 32.0 GB 
[11/21 23:47:53][INFO] visual_prompt:  217: Epoch 5 / 100: avg data time: 5.29e-02, avg batch time: 0.9690, average train loss: 0.8008
[11/21 23:48:51][INFO] visual_prompt:  316: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3005, average loss: 0.7207
[11/21 23:48:51][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 63.80	
[11/21 23:48:51][INFO] visual_prompt:  165: Training 6 / 100 epoch, with learning rate 0.0005
[11/21 23:50:37][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.4613,	0.9013 s / batch. (data: 3.43e-04). ETA=13:07:37, max mem: 32.0 GB 
[11/21 23:52:11][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.1862,	0.9320 s / batch. (data: 2.99e-04). ETA=13:32:58, max mem: 32.0 GB 
[11/21 23:53:46][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6365,	0.9040 s / batch. (data: 3.06e-04). ETA=13:07:00, max mem: 32.0 GB 
[11/21 23:55:21][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9402,	1.6598 s / batch. (data: 7.59e-01). ETA=1 day, 0:02:11, max mem: 32.0 GB 
[11/21 23:56:58][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.9939,	0.9280 s / batch. (data: 8.09e-04). ETA=13:24:48, max mem: 32.0 GB 
[11/21 23:57:47][INFO] visual_prompt:  217: Epoch 6 / 100: avg data time: 5.35e-02, avg batch time: 0.9686, average train loss: 0.8545
[11/21 23:58:44][INFO] visual_prompt:  316: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3003, average loss: 0.6561
[11/21 23:58:44][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 64.31	
[11/21 23:58:44][INFO] visual_prompt:  165: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/22 00:00:35][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5306,	0.9080 s / batch. (data: 3.04e-04). ETA=13:05:09, max mem: 32.0 GB 
[11/22 00:02:08][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5438,	0.9515 s / batch. (data: 2.78e-02). ETA=13:41:07, max mem: 32.0 GB 
[11/22 00:03:42][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5863,	0.8964 s / batch. (data: 2.96e-04). ETA=12:52:07, max mem: 32.0 GB 
[11/22 00:05:15][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9098,	0.9040 s / batch. (data: 7.93e-03). ETA=12:57:09, max mem: 32.0 GB 
[11/22 00:06:50][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.4766,	0.9160 s / batch. (data: 3.18e-03). ETA=13:05:56, max mem: 32.0 GB 
[11/22 00:07:40][INFO] visual_prompt:  217: Epoch 7 / 100: avg data time: 5.19e-02, avg batch time: 0.9674, average train loss: 0.8194
[11/22 00:08:37][INFO] visual_prompt:  316: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3026, average loss: 0.6432
[11/22 00:08:37][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 66.45	
[11/22 00:08:37][INFO] visual_prompt:   36: Best epoch 7: best metric: -0.643
[11/22 00:08:37][INFO] visual_prompt:  165: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/22 00:10:23][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.4682,	0.9373 s / batch. (data: 5.42e-03). ETA=13:21:49, max mem: 32.0 GB 
[11/22 00:11:59][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6494,	0.8920 s / batch. (data: 3.93e-03). ETA=12:41:36, max mem: 32.0 GB 
[11/22 00:13:34][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.0332,	0.8901 s / batch. (data: 3.09e-04). ETA=12:38:29, max mem: 32.0 GB 
[11/22 00:15:10][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5901,	0.9400 s / batch. (data: 2.93e-04). ETA=13:19:26, max mem: 32.0 GB 
[11/22 00:16:46][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6567,	0.9600 s / batch. (data: 5.42e-03). ETA=13:34:51, max mem: 32.0 GB 
[11/22 00:17:34][INFO] visual_prompt:  217: Epoch 8 / 100: avg data time: 5.58e-02, avg batch time: 0.9709, average train loss: 0.8263
[11/22 00:18:32][INFO] visual_prompt:  316: Inference (val):avg data time: 3.44e-05, avg batch time: 0.2998, average loss: 0.6388
[11/22 00:18:32][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 66.26	rocauc: 66.48	
[11/22 00:18:32][INFO] visual_prompt:   36: Best epoch 8: best metric: -0.639
[11/22 00:18:32][INFO] visual_prompt:  165: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/22 00:20:20][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.0862,	0.9280 s / batch. (data: 3.95e-03). ETA=13:05:19, max mem: 32.0 GB 
[11/22 00:21:56][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.0051,	0.9479 s / batch. (data: 3.14e-04). ETA=13:20:35, max mem: 32.0 GB 
[11/22 00:23:30][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7270,	0.9001 s / batch. (data: 3.15e-04). ETA=12:38:42, max mem: 32.0 GB 
[11/22 00:25:03][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.2077,	0.9056 s / batch. (data: 5.41e-03). ETA=12:41:53, max mem: 32.0 GB 
[11/22 00:26:35][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.2351,	0.8960 s / batch. (data: 3.17e-04). ETA=12:32:15, max mem: 32.0 GB 
[11/22 00:27:26][INFO] visual_prompt:  217: Epoch 9 / 100: avg data time: 4.94e-02, avg batch time: 0.9651, average train loss: 0.8481
[11/22 00:28:23][INFO] visual_prompt:  316: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3008, average loss: 0.7521
[11/22 00:28:23][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 66.13	
[11/22 00:28:23][INFO] visual_prompt:  165: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/22 00:30:10][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.2014,	0.9401 s / batch. (data: 2.07e-02). ETA=13:06:56, max mem: 32.0 GB 
[11/22 00:31:46][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5852,	0.9292 s / batch. (data: 2.24e-02). ETA=12:56:13, max mem: 32.0 GB 
[11/22 00:33:19][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.4803,	0.9240 s / batch. (data: 3.16e-04). ETA=12:50:20, max mem: 32.0 GB 
[11/22 00:34:52][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.6436,	0.9140 s / batch. (data: 3.94e-03). ETA=12:40:31, max mem: 32.0 GB 
[11/22 00:36:27][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.4665,	1.0798 s / batch. (data: 1.77e-01). ETA=14:56:37, max mem: 32.0 GB 
[11/22 00:37:18][INFO] visual_prompt:  217: Epoch 10 / 100: avg data time: 5.20e-02, avg batch time: 0.9670, average train loss: 0.8030
[11/22 00:38:16][INFO] visual_prompt:  316: Inference (val):avg data time: 3.53e-05, avg batch time: 0.3000, average loss: 0.6475
[11/22 00:38:16][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 62.20	rocauc: 67.34	
[11/22 00:38:16][INFO] visual_prompt:  165: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/22 00:40:05][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.8553,	0.9320 s / batch. (data: 7.96e-03). ETA=12:51:34, max mem: 32.0 GB 
[11/22 00:41:38][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.1456,	0.9238 s / batch. (data: 3.33e-04). ETA=12:43:10, max mem: 32.0 GB 
[11/22 00:43:11][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5686,	0.9320 s / batch. (data: 5.92e-03). ETA=12:48:25, max mem: 32.0 GB 
[11/22 00:44:47][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.4636,	0.9080 s / batch. (data: 7.95e-03). ETA=12:27:07, max mem: 32.0 GB 
[11/22 00:46:21][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5040,	0.9440 s / batch. (data: 7.92e-03). ETA=12:55:11, max mem: 32.0 GB 
[11/22 00:47:10][INFO] visual_prompt:  217: Epoch 11 / 100: avg data time: 4.89e-02, avg batch time: 0.9656, average train loss: 0.7714
[11/22 00:48:08][INFO] visual_prompt:  316: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3037, average loss: 0.8470
[11/22 00:48:08][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 68.44	
[11/22 00:48:08][INFO] visual_prompt:  165: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/22 00:49:50][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.9034,	0.9320 s / batch. (data: 7.89e-04). ETA=12:42:57, max mem: 32.0 GB 
[11/22 00:51:30][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8467,	0.9639 s / batch. (data: 2.79e-02). ETA=13:07:29, max mem: 32.0 GB 
[11/22 00:53:04][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.2364,	0.9125 s / batch. (data: 5.13e-03). ETA=12:23:57, max mem: 32.0 GB 
[11/22 00:54:39][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.2854,	0.9436 s / batch. (data: 1.10e-02). ETA=12:47:41, max mem: 32.0 GB 
[11/22 00:56:13][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5573,	0.9581 s / batch. (data: 1.11e-02). ETA=12:57:57, max mem: 32.0 GB 
[11/22 00:57:04][INFO] visual_prompt:  217: Epoch 12 / 100: avg data time: 5.24e-02, avg batch time: 0.9684, average train loss: 0.8042
[11/22 00:58:01][INFO] visual_prompt:  316: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3003, average loss: 0.8392
[11/22 00:58:01][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 68.21	
[11/22 00:58:01][INFO] visual_prompt:  165: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/22 00:59:48][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5433,	0.9080 s / batch. (data: 8.59e-04). ETA=12:14:56, max mem: 32.0 GB 
[11/22 01:01:22][INFO] visual_prompt:  204: 	Training 200/553. train loss: 2.0095,	0.8955 s / batch. (data: 7.93e-03). ETA=12:03:17, max mem: 32.0 GB 
[11/22 01:02:58][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.8903,	0.9079 s / batch. (data: 2.79e-04). ETA=12:11:50, max mem: 32.0 GB 
[11/22 01:04:34][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7817,	0.9000 s / batch. (data: 5.41e-03). ETA=12:03:57, max mem: 32.0 GB 
[11/22 01:06:08][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.3167,	0.8960 s / batch. (data: 2.90e-04). ETA=11:59:15, max mem: 32.0 GB 
[11/22 01:06:56][INFO] visual_prompt:  217: Epoch 13 / 100: avg data time: 5.36e-02, avg batch time: 0.9676, average train loss: 0.7373
[11/22 01:07:54][INFO] visual_prompt:  316: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3014, average loss: 0.9178
[11/22 01:07:54][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 69.31	
[11/22 01:07:54][INFO] visual_prompt:  165: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/22 01:09:42][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.8809,	0.9141 s / batch. (data: 2.79e-04). ETA=12:11:25, max mem: 32.0 GB 
[11/22 01:11:19][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.0992,	0.9161 s / batch. (data: 8.14e-03). ETA=12:11:33, max mem: 32.0 GB 
[11/22 01:12:53][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9553,	0.9040 s / batch. (data: 3.34e-04). ETA=12:00:20, max mem: 32.0 GB 
[11/22 01:14:27][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.3113,	0.9021 s / batch. (data: 3.90e-03). ETA=11:57:22, max mem: 32.0 GB 
[11/22 01:16:02][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8914,	0.9163 s / batch. (data: 3.26e-04). ETA=12:07:03, max mem: 32.0 GB 
[11/22 01:16:50][INFO] visual_prompt:  217: Epoch 14 / 100: avg data time: 5.43e-02, avg batch time: 0.9697, average train loss: 0.7619
[11/22 01:17:48][INFO] visual_prompt:  316: Inference (val):avg data time: 1.53e-04, avg batch time: 0.3006, average loss: 0.7220
[11/22 01:17:48][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 64.63	rocauc: 68.27	
[11/22 01:17:48][INFO] visual_prompt:  165: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/22 01:19:35][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.2813,	0.9160 s / batch. (data: 3.94e-03). ETA=12:04:31, max mem: 32.0 GB 
[11/22 01:21:11][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7587,	0.9121 s / batch. (data: 2.75e-04). ETA=11:59:56, max mem: 32.0 GB 
[11/22 01:22:46][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.3190,	0.9201 s / batch. (data: 2.86e-04). ETA=12:04:40, max mem: 32.0 GB 
[11/22 01:24:21][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.2632,	0.9159 s / batch. (data: 1.83e-02). ETA=11:59:54, max mem: 32.0 GB 
[11/22 01:25:54][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5304,	0.8905 s / batch. (data: 2.93e-04). ETA=11:38:26, max mem: 32.0 GB 
[11/22 01:26:43][INFO] visual_prompt:  217: Epoch 15 / 100: avg data time: 5.08e-02, avg batch time: 0.9667, average train loss: 0.7417
[11/22 01:27:41][INFO] visual_prompt:  316: Inference (val):avg data time: 3.75e-04, avg batch time: 0.3004, average loss: 1.0012
[11/22 01:27:41][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 47.15	rocauc: 67.51	
[11/22 01:27:41][INFO] visual_prompt:  165: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/22 01:29:25][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.9884,	0.9154 s / batch. (data: 3.94e-03). ETA=11:55:36, max mem: 32.0 GB 
[11/22 01:30:57][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.2472,	0.9760 s / batch. (data: 4.41e-02). ETA=12:41:22, max mem: 32.0 GB 
[11/22 01:32:37][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.3502,	0.9008 s / batch. (data: 3.00e-04). ETA=11:41:10, max mem: 32.0 GB 
[11/22 01:34:12][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.4238,	0.9198 s / batch. (data: 2.90e-04). ETA=11:54:28, max mem: 32.0 GB 
[11/22 01:35:45][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.0746,	0.9240 s / batch. (data: 3.07e-04). ETA=11:56:09, max mem: 32.0 GB 
[11/22 01:36:35][INFO] visual_prompt:  217: Epoch 16 / 100: avg data time: 4.89e-02, avg batch time: 0.9647, average train loss: 0.7367
[11/22 01:37:32][INFO] visual_prompt:  316: Inference (val):avg data time: 3.72e-05, avg batch time: 0.3005, average loss: 0.6860
[11/22 01:37:32][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 67.76	
[11/22 01:37:32][INFO] visual_prompt:  165: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/22 01:39:15][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.9054,	0.9278 s / batch. (data: 9.71e-03). ETA=11:56:46, max mem: 32.0 GB 
[11/22 01:40:53][INFO] visual_prompt:  204: 	Training 200/553. train loss: 2.1884,	0.9067 s / batch. (data: 1.05e-02). ETA=11:38:58, max mem: 32.0 GB 
[11/22 01:42:26][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.3200,	0.8912 s / batch. (data: 3.42e-04). ETA=11:25:32, max mem: 32.0 GB 
[11/22 01:44:02][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5236,	0.9279 s / batch. (data: 3.89e-03). ETA=11:52:12, max mem: 32.0 GB 
[11/22 01:45:36][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8940,	0.9040 s / batch. (data: 2.90e-04). ETA=11:32:20, max mem: 32.0 GB 
[11/22 01:46:26][INFO] visual_prompt:  217: Epoch 17 / 100: avg data time: 5.05e-02, avg batch time: 0.9653, average train loss: 0.7089
[11/22 01:47:24][INFO] visual_prompt:  316: Inference (val):avg data time: 8.88e-05, avg batch time: 0.3013, average loss: 0.7225
[11/22 01:47:24][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 65.45	rocauc: 67.95	
[11/22 01:47:24][INFO] visual_prompt:  165: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/22 01:49:13][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.1829,	0.9120 s / batch. (data: 8.08e-04). ETA=11:36:08, max mem: 32.0 GB 
[11/22 01:50:48][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5540,	0.9439 s / batch. (data: 5.43e-03). ETA=11:58:57, max mem: 32.0 GB 
[11/22 01:52:21][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7571,	0.8998 s / batch. (data: 5.43e-03). ETA=11:23:50, max mem: 32.0 GB 
[11/22 01:53:54][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.4429,	0.9200 s / batch. (data: 3.01e-04). ETA=11:37:38, max mem: 32.0 GB 
[11/22 01:55:32][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.4826,	0.9280 s / batch. (data: 2.92e-04). ETA=11:42:10, max mem: 32.0 GB 
[11/22 01:56:20][INFO] visual_prompt:  217: Epoch 18 / 100: avg data time: 5.43e-02, avg batch time: 0.9692, average train loss: 0.6825
[11/22 01:57:18][INFO] visual_prompt:  316: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3012, average loss: 0.6270
[11/22 01:57:18][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 64.23	rocauc: 68.62	
[11/22 01:57:18][INFO] visual_prompt:   36: Best epoch 18: best metric: -0.627
[11/22 01:57:18][INFO] visual_prompt:  165: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/22 01:59:04][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.2028,	0.9200 s / batch. (data: 3.13e-04). ETA=11:33:47, max mem: 32.0 GB 
[11/22 02:00:41][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8351,	3.1322 s / batch. (data: 2.23e+00). ETA=1 day, 15:16:44, max mem: 32.0 GB 
[11/22 02:02:14][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.2568,	0.9200 s / batch. (data: 2.93e-04). ETA=11:30:41, max mem: 32.0 GB 
[11/22 02:03:49][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.3724,	0.9325 s / batch. (data: 2.94e-04). ETA=11:38:31, max mem: 32.0 GB 
[11/22 02:05:24][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.2418,	0.9484 s / batch. (data: 1.10e-02). ETA=11:48:51, max mem: 32.0 GB 
[11/22 02:06:13][INFO] visual_prompt:  217: Epoch 19 / 100: avg data time: 5.24e-02, avg batch time: 0.9674, average train loss: 0.6236
[11/22 02:07:11][INFO] visual_prompt:  316: Inference (val):avg data time: 2.49e-04, avg batch time: 0.3016, average loss: 0.7040
[11/22 02:07:11][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 66.67	rocauc: 68.52	
[11/22 02:07:11][INFO] visual_prompt:  165: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/22 02:08:59][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7391,	0.9080 s / batch. (data: 2.93e-04). ETA=11:16:20, max mem: 32.0 GB 
[11/22 02:10:32][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8256,	0.9242 s / batch. (data: 2.81e-02). ETA=11:26:51, max mem: 32.0 GB 
[11/22 02:12:06][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.3372,	0.9099 s / batch. (data: 3.49e-04). ETA=11:14:45, max mem: 32.0 GB 
[11/22 02:13:42][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7815,	0.9324 s / batch. (data: 2.44e-02). ETA=11:29:51, max mem: 32.0 GB 
[11/22 02:15:15][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6078,	1.0600 s / batch. (data: 1.40e-01). ETA=13:02:31, max mem: 32.0 GB 
[11/22 02:16:06][INFO] visual_prompt:  217: Epoch 20 / 100: avg data time: 5.26e-02, avg batch time: 0.9675, average train loss: 0.6392
[11/22 02:17:04][INFO] visual_prompt:  316: Inference (val):avg data time: 3.44e-05, avg batch time: 0.2988, average loss: 0.6538
[11/22 02:17:04][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 69.92	rocauc: 70.45	
[11/22 02:17:04][INFO] visual_prompt:  165: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/22 02:18:52][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.1837,	0.8855 s / batch. (data: 3.14e-04). ETA=10:51:23, max mem: 32.0 GB 
[11/22 02:20:27][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5306,	0.9400 s / batch. (data: 3.09e-04). ETA=11:29:58, max mem: 32.0 GB 
[11/22 02:22:03][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5232,	0.9520 s / batch. (data: 3.12e-04). ETA=11:37:10, max mem: 32.0 GB 
[11/22 02:23:37][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7484,	0.9467 s / batch. (data: 1.60e-02). ETA=11:31:41, max mem: 32.0 GB 
[11/22 02:25:11][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.3700,	0.9200 s / batch. (data: 2.89e-04). ETA=11:10:41, max mem: 32.0 GB 
[11/22 02:25:59][INFO] visual_prompt:  217: Epoch 21 / 100: avg data time: 5.29e-02, avg batch time: 0.9687, average train loss: 0.5820
[11/22 02:26:57][INFO] visual_prompt:  316: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3011, average loss: 0.7944
[11/22 02:26:57][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 60.16	rocauc: 67.77	
[11/22 02:26:57][INFO] visual_prompt:  165: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/22 02:28:41][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.1474,	0.9101 s / batch. (data: 3.01e-04). ETA=11:01:09, max mem: 32.0 GB 
[11/22 02:30:19][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8845,	0.9400 s / batch. (data: 8.33e-04). ETA=11:21:17, max mem: 32.0 GB 
[11/22 02:31:55][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.2161,	0.9208 s / batch. (data: 8.59e-04). ETA=11:05:50, max mem: 32.0 GB 
[11/22 02:33:27][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.3273,	0.9073 s / batch. (data: 3.28e-04). ETA=10:54:34, max mem: 32.0 GB 
[11/22 02:35:03][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.4743,	0.8943 s / batch. (data: 2.87e-04). ETA=10:43:42, max mem: 32.0 GB 
[11/22 02:35:52][INFO] visual_prompt:  217: Epoch 22 / 100: avg data time: 5.17e-02, avg batch time: 0.9665, average train loss: 0.5875
[11/22 02:36:49][INFO] visual_prompt:  316: Inference (val):avg data time: 3.68e-05, avg batch time: 0.3011, average loss: 1.0189
[11/22 02:36:49][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 66.91	
[11/22 02:36:49][INFO] visual_prompt:  165: Training 23 / 100 epoch, with learning rate 0.00046152381310523384
[11/22 02:38:37][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.3895,	0.9192 s / batch. (data: 5.96e-03). ETA=10:59:17, max mem: 32.0 GB 
[11/22 02:40:11][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7816,	0.9033 s / batch. (data: 7.93e-04). ETA=10:46:21, max mem: 32.0 GB 
[11/22 02:41:44][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.0780,	0.9354 s / batch. (data: 2.34e-02). ETA=11:07:46, max mem: 32.0 GB 
[11/22 02:43:19][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.0332,	0.9320 s / batch. (data: 8.48e-04). ETA=11:03:47, max mem: 32.0 GB 
[11/22 02:44:51][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.3079,	0.9400 s / batch. (data: 2.70e-04). ETA=11:07:56, max mem: 32.0 GB 
[11/22 02:45:43][INFO] visual_prompt:  217: Epoch 23 / 100: avg data time: 4.84e-02, avg batch time: 0.9647, average train loss: 0.6013
[11/22 02:46:41][INFO] visual_prompt:  316: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3010, average loss: 0.9448
[11/22 02:46:41][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 60.57	rocauc: 69.75	
[11/22 02:46:41][INFO] visual_prompt:  165: Training 24 / 100 epoch, with learning rate 0.000457002207787005
[11/22 02:48:25][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6747,	0.9157 s / batch. (data: 5.94e-03). ETA=10:48:21, max mem: 32.0 GB 
[11/22 02:50:00][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.1957,	0.9199 s / batch. (data: 7.91e-03). ETA=10:49:47, max mem: 32.0 GB 
[11/22 02:51:38][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5821,	0.9111 s / batch. (data: 7.04e-03). ETA=10:42:01, max mem: 32.0 GB 
[11/22 02:53:12][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.2623,	0.9015 s / batch. (data: 2.97e-04). ETA=10:33:46, max mem: 32.0 GB 
[11/22 02:54:49][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5507,	0.8960 s / batch. (data: 3.07e-04). ETA=10:28:24, max mem: 32.0 GB 
[11/22 02:55:37][INFO] visual_prompt:  217: Epoch 24 / 100: avg data time: 5.52e-02, avg batch time: 0.9698, average train loss: 0.5297
[11/22 02:56:35][INFO] visual_prompt:  316: Inference (val):avg data time: 1.52e-04, avg batch time: 0.3028, average loss: 0.7976
[11/22 02:56:35][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 65.91	
[11/22 02:56:35][INFO] visual_prompt:  165: Training 25 / 100 epoch, with learning rate 0.0004522542485937369
[11/22 02:58:17][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5070,	0.9200 s / batch. (data: 3.12e-04). ETA=10:42:53, max mem: 32.0 GB 
[11/22 02:59:56][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.2384,	0.9440 s / batch. (data: 5.45e-03). ETA=10:58:04, max mem: 32.0 GB 
[11/22 03:01:30][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.3558,	0.9526 s / batch. (data: 2.74e-04). ETA=11:02:29, max mem: 32.0 GB 
[11/22 03:03:06][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.3265,	0.9214 s / batch. (data: 2.97e-04). ETA=10:39:16, max mem: 32.0 GB 
[11/22 03:04:40][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.1309,	0.9413 s / batch. (data: 2.65e-04). ETA=10:51:29, max mem: 32.0 GB 
[11/22 03:05:31][INFO] visual_prompt:  217: Epoch 25 / 100: avg data time: 5.39e-02, avg batch time: 0.9689, average train loss: 0.5018
[11/22 03:06:28][INFO] visual_prompt:  316: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3000, average loss: 0.8679
[11/22 03:06:28][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 68.10	
[11/22 03:06:28][INFO] visual_prompt:  165: Training 26 / 100 epoch, with learning rate 0.00044728512734909845
[11/22 03:08:14][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.2529,	0.9016 s / batch. (data: 8.26e-04). ETA=10:21:42, max mem: 32.0 GB 
[11/22 03:09:50][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.0116,	0.9240 s / batch. (data: 3.61e-04). ETA=10:35:37, max mem: 32.0 GB 
[11/22 03:11:27][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.1028,	0.9040 s / batch. (data: 2.93e-04). ETA=10:20:22, max mem: 32.0 GB 
[11/22 03:12:59][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.2029,	0.8950 s / batch. (data: 7.94e-03). ETA=10:12:42, max mem: 32.0 GB 
[11/22 03:14:33][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.3842,	0.9200 s / batch. (data: 7.95e-03). ETA=10:28:15, max mem: 32.0 GB 
[11/22 03:15:23][INFO] visual_prompt:  217: Epoch 26 / 100: avg data time: 5.24e-02, avg batch time: 0.9674, average train loss: 0.4740
[11/22 03:16:21][INFO] visual_prompt:  316: Inference (val):avg data time: 8.59e-05, avg batch time: 0.3024, average loss: 1.0006
[11/22 03:16:21][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 61.38	rocauc: 67.57	
[11/22 03:16:21][INFO] visual_prompt:  165: Training 27 / 100 epoch, with learning rate 0.0004421002777142148
[11/22 03:18:05][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.1457,	0.9263 s / batch. (data: 2.95e-02). ETA=10:30:14, max mem: 32.0 GB 
[11/22 03:19:42][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.2389,	0.9560 s / batch. (data: 2.87e-04). ETA=10:48:50, max mem: 32.0 GB 
[11/22 03:21:15][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.6118,	0.9120 s / batch. (data: 2.84e-04). ETA=10:17:27, max mem: 32.0 GB 
[11/22 03:22:49][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.3014,	0.9200 s / batch. (data: 2.97e-04). ETA=10:21:20, max mem: 32.0 GB 
[11/22 03:24:27][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6208,	0.9200 s / batch. (data: 3.25e-04). ETA=10:19:47, max mem: 32.0 GB 
[11/22 03:25:16][INFO] visual_prompt:  217: Epoch 27 / 100: avg data time: 5.18e-02, avg batch time: 0.9675, average train loss: 0.4734
[11/22 03:26:14][INFO] visual_prompt:  316: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3022, average loss: 0.9710
[11/22 03:26:14][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 67.04	
[11/22 03:26:14][INFO] visual_prompt:  165: Training 28 / 100 epoch, with learning rate 0.0004367053692460385
[11/22 03:27:57][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5488,	0.9040 s / batch. (data: 3.43e-04). ETA=10:06:42, max mem: 32.0 GB 
[11/22 03:29:33][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.1858,	0.9080 s / batch. (data: 3.14e-04). ETA=10:07:53, max mem: 32.0 GB 
[11/22 03:31:09][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7107,	0.9320 s / batch. (data: 3.93e-03). ETA=10:22:23, max mem: 32.0 GB 
[11/22 03:32:44][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.1995,	0.9000 s / batch. (data: 3.91e-04). ETA=9:59:33, max mem: 32.0 GB 
[11/22 03:34:19][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.2876,	0.9360 s / batch. (data: 2.88e-04). ETA=10:21:57, max mem: 32.0 GB 
[11/22 03:35:09][INFO] visual_prompt:  217: Epoch 28 / 100: avg data time: 5.06e-02, avg batch time: 0.9666, average train loss: 0.4697
[11/22 03:36:06][INFO] visual_prompt:  316: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3009, average loss: 0.9926
[11/22 03:36:06][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 67.16	
[11/22 03:36:06][INFO] visual_prompt:  165: Training 29 / 100 epoch, with learning rate 0.0004311063011977723
[11/22 03:37:52][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.3170,	0.9479 s / batch. (data: 5.58e-03). ETA=10:27:27, max mem: 32.0 GB 
[11/22 03:39:29][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.2134,	0.9442 s / batch. (data: 2.84e-02). ETA=10:23:25, max mem: 32.0 GB 
[11/22 03:41:07][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.3062,	0.9240 s / batch. (data: 3.29e-04). ETA=10:08:33, max mem: 32.0 GB 
[11/22 03:42:44][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.8933,	0.9284 s / batch. (data: 5.91e-03). ETA=10:09:52, max mem: 32.0 GB 
[11/22 03:44:15][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.1584,	0.9362 s / batch. (data: 2.04e-02). ETA=10:13:29, max mem: 32.0 GB 
[11/22 03:45:04][INFO] visual_prompt:  217: Epoch 29 / 100: avg data time: 5.70e-02, avg batch time: 0.9723, average train loss: 0.3913
[11/22 03:46:02][INFO] visual_prompt:  316: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3006, average loss: 1.1432
[11/22 03:46:02][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 63.01	rocauc: 65.26	
[11/22 03:46:02][INFO] visual_prompt:  165: Training 30 / 100 epoch, with learning rate 0.00042530919606812215
[11/22 03:47:46][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.2151,	0.9000 s / batch. (data: 3.04e-04). ETA=9:47:27, max mem: 32.0 GB 
[11/22 03:49:22][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.3015,	0.9240 s / batch. (data: 3.19e-04). ETA=10:01:34, max mem: 32.0 GB 
[11/22 03:50:57][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.2153,	0.9074 s / batch. (data: 3.03e-04). ETA=9:49:13, max mem: 32.0 GB 
[11/22 03:52:31][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.0704,	0.9275 s / batch. (data: 2.98e-04). ETA=10:00:44, max mem: 32.0 GB 
[11/22 03:54:06][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.2405,	0.9071 s / batch. (data: 3.22e-04). ETA=9:46:01, max mem: 32.0 GB 
[11/22 03:54:55][INFO] visual_prompt:  217: Epoch 30 / 100: avg data time: 4.78e-02, avg batch time: 0.9633, average train loss: 0.3784
[11/22 03:55:52][INFO] visual_prompt:  316: Inference (val):avg data time: 3.84e-04, avg batch time: 0.3031, average loss: 0.9012
[11/22 03:55:52][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 70.73	rocauc: 71.12	
[11/22 03:55:52][INFO] visual_prompt:  165: Training 31 / 100 epoch, with learning rate 0.0004193203929064353
[11/22 03:57:41][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.0648,	0.8921 s / batch. (data: 3.53e-04). ETA=9:34:02, max mem: 32.0 GB 
[11/22 03:59:13][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.3788,	0.9181 s / batch. (data: 1.56e-02). ETA=9:49:16, max mem: 32.0 GB 
[11/22 04:00:49][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.1475,	0.9240 s / batch. (data: 1.05e-02). ETA=9:51:30, max mem: 32.0 GB 
[11/22 04:02:24][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.0355,	0.8920 s / batch. (data: 3.15e-04). ETA=9:29:32, max mem: 32.0 GB 
[11/22 04:03:57][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.0012,	0.9040 s / batch. (data: 7.96e-03). ETA=9:35:41, max mem: 32.0 GB 
[11/22 04:04:48][INFO] visual_prompt:  217: Epoch 31 / 100: avg data time: 5.23e-02, avg batch time: 0.9675, average train loss: 0.3076
[11/22 04:05:45][INFO] visual_prompt:  316: Inference (val):avg data time: 3.50e-05, avg batch time: 0.3020, average loss: 1.1812
[11/22 04:05:45][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 66.61	
[11/22 04:05:45][INFO] visual_prompt:  165: Training 32 / 100 epoch, with learning rate 0.00041314644038104216
[11/22 04:07:28][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.0251,	0.9480 s / batch. (data: 8.06e-03). ETA=10:01:18, max mem: 32.0 GB 
[11/22 04:09:05][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.0312,	0.9120 s / batch. (data: 8.15e-04). ETA=9:36:56, max mem: 32.0 GB 
[11/22 04:10:38][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.0066,	0.9398 s / batch. (data: 3.25e-04). ETA=9:52:56, max mem: 32.0 GB 
[11/22 04:12:15][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.0386,	0.9200 s / batch. (data: 2.96e-04). ETA=9:38:56, max mem: 32.0 GB 
[11/22 04:13:53][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.0846,	0.9120 s / batch. (data: 3.08e-04). ETA=9:32:23, max mem: 32.0 GB 
[11/22 04:14:42][INFO] visual_prompt:  217: Epoch 32 / 100: avg data time: 5.48e-02, avg batch time: 0.9699, average train loss: 0.2900
[11/22 04:15:39][INFO] visual_prompt:  316: Inference (val):avg data time: 3.64e-05, avg batch time: 0.2995, average loss: 1.3334
[11/22 04:15:39][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 63.39	
[11/22 04:15:39][INFO] visual_prompt:   42: Stopping early.

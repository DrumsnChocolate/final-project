[11/21 15:51:09][INFO] visual_prompt:   95: Rank of current process: 0. World size: 1
[11/21 15:51:09][INFO] visual_prompt:   97: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              1
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/21 15:51:09][INFO] visual_prompt:   99: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/21 15:51:09][INFO] visual_prompt:  101: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/21 15:51:09][INFO] visual_prompt:  108: Training with config:
[11/21 15:51:09][INFO] visual_prompt:  109: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0005_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/21 15:51:09][INFO] visual_prompt:   55: Loading training data...
[11/21 15:51:09][INFO] visual_prompt:   28: Constructing mammo-cbis dataset train...
[11/21 15:51:09][INFO] visual_prompt:   57: Loading validation data...
[11/21 15:51:09][INFO] visual_prompt:   28: Constructing mammo-cbis dataset val...
[11/21 15:51:10][INFO] visual_prompt:   38: Constructing models...
[11/21 15:51:11][INFO] visual_prompt:  153: Enable all parameters update during training
[11/21 15:51:11][INFO] visual_prompt:   52: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/21 15:51:11][INFO] visual_prompt:   54: tuned percent:100.000
[11/21 15:51:12][INFO] visual_prompt:   40: Device used for model: 0
[11/21 15:51:12][INFO] visual_prompt:   40: Setting up Evaluator...
[11/21 15:51:12][INFO] visual_prompt:   42: Setting up Trainer...
[11/21 15:51:12][INFO] visual_prompt:   45: 	Setting up the optimizer...
[11/21 15:51:12][INFO] visual_prompt:  165: Training 1 / 100 epoch, with learning rate 0.0
[11/21 15:52:54][INFO] visual_prompt:  204: 	Training 100/553. train loss: 10.8600,	0.9354 s / batch. (data: 2.97e-04). ETA=14:20:35, max mem: 30.7 GB 
[11/21 15:54:34][INFO] visual_prompt:  204: 	Training 200/553. train loss: 9.4496,	0.9030 s / batch. (data: 1.05e-02). ETA=13:49:17, max mem: 30.7 GB 
[11/21 15:56:10][INFO] visual_prompt:  204: 	Training 300/553. train loss: 9.8169,	0.9151 s / batch. (data: 2.84e-04). ETA=13:58:48, max mem: 30.7 GB 
[11/21 15:57:43][INFO] visual_prompt:  204: 	Training 400/553. train loss: 7.5668,	0.9379 s / batch. (data: 3.05e-04). ETA=14:18:12, max mem: 30.7 GB 
[11/21 15:59:18][INFO] visual_prompt:  204: 	Training 500/553. train loss: 3.1665,	0.9040 s / batch. (data: 2.91e-04). ETA=13:45:40, max mem: 30.7 GB 
[11/21 16:00:07][INFO] visual_prompt:  217: Epoch 1 / 100: avg data time: 4.68e-02, avg batch time: 0.9681, average train loss: 7.6130
[11/21 16:01:05][INFO] visual_prompt:  316: Inference (val):avg data time: 3.51e-05, avg batch time: 0.2998, average loss: 6.9126
[11/21 16:01:05][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/21 16:01:05][INFO] visual_prompt:  165: Training 2 / 100 epoch, with learning rate 0.0001
[11/21 16:02:52][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.8570,	0.9210 s / batch. (data: 5.90e-03). ETA=13:58:49, max mem: 30.7 GB 
[11/21 16:04:25][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.5929,	1.4120 s / batch. (data: 4.84e-01). ETA=21:23:41, max mem: 30.7 GB 
[11/21 16:06:00][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8436,	0.9332 s / batch. (data: 2.12e-02). ETA=14:06:49, max mem: 30.7 GB 
[11/21 16:07:37][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6238,	0.9007 s / batch. (data: 2.80e-04). ETA=13:35:49, max mem: 30.7 GB 
[11/21 16:09:13][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.9023,	0.9016 s / batch. (data: 5.38e-03). ETA=13:35:06, max mem: 30.7 GB 
[11/21 16:10:02][INFO] visual_prompt:  217: Epoch 2 / 100: avg data time: 4.92e-02, avg batch time: 0.9708, average train loss: 0.9653
[11/21 16:10:59][INFO] visual_prompt:  316: Inference (val):avg data time: 3.55e-05, avg batch time: 0.3010, average loss: 1.1172
[11/21 16:10:59][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.32	
[11/21 16:10:59][INFO] visual_prompt:  165: Training 3 / 100 epoch, with learning rate 0.0002
[11/21 16:12:45][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6849,	0.9360 s / batch. (data: 8.00e-04). ETA=14:03:52, max mem: 30.7 GB 
[11/21 16:14:20][INFO] visual_prompt:  204: 	Training 200/553. train loss: 2.5458,	0.9080 s / batch. (data: 2.83e-04). ETA=13:37:05, max mem: 30.7 GB 
[11/21 16:15:55][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9304,	0.9141 s / batch. (data: 5.36e-03). ETA=13:41:02, max mem: 30.7 GB 
[11/21 16:17:30][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7199,	0.9300 s / batch. (data: 1.04e-02). ETA=13:53:45, max mem: 30.7 GB 
[11/21 16:19:03][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.0368,	0.9113 s / batch. (data: 8.01e-03). ETA=13:35:29, max mem: 30.7 GB 
[11/21 16:19:53][INFO] visual_prompt:  217: Epoch 3 / 100: avg data time: 4.35e-02, avg batch time: 0.9638, average train loss: 0.8281
[11/21 16:20:50][INFO] visual_prompt:  316: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3015, average loss: 0.6959
[11/21 16:20:50][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 61.82	
[11/21 16:20:50][INFO] visual_prompt:  165: Training 4 / 100 epoch, with learning rate 0.0003
[11/21 16:22:35][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6732,	0.9000 s / batch. (data: 2.76e-04). ETA=13:23:07, max mem: 30.7 GB 
[11/21 16:24:08][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9950,	1.0877 s / batch. (data: 1.42e-01). ETA=16:08:47, max mem: 30.7 GB 
[11/21 16:25:44][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9635,	0.9241 s / batch. (data: 3.96e-03). ETA=13:41:30, max mem: 30.7 GB 
[11/21 16:27:18][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.4996,	0.9155 s / batch. (data: 7.95e-03). ETA=13:32:20, max mem: 30.7 GB 
[11/21 16:28:54][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.9537,	0.9440 s / batch. (data: 7.97e-03). ETA=13:56:07, max mem: 30.7 GB 
[11/21 16:29:45][INFO] visual_prompt:  217: Epoch 4 / 100: avg data time: 4.68e-02, avg batch time: 0.9679, average train loss: 0.8101
[11/21 16:30:42][INFO] visual_prompt:  316: Inference (val):avg data time: 2.68e-04, avg batch time: 0.3034, average loss: 0.7008
[11/21 16:30:42][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 62.68	
[11/21 16:30:42][INFO] visual_prompt:  165: Training 5 / 100 epoch, with learning rate 0.0004
[11/21 16:32:27][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6717,	0.9440 s / batch. (data: 2.78e-04). ETA=13:53:39, max mem: 30.7 GB 
[11/21 16:34:03][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6559,	0.9065 s / batch. (data: 2.85e-04). ETA=13:19:03, max mem: 30.7 GB 
[11/21 16:35:37][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7608,	0.9493 s / batch. (data: 1.05e-02). ETA=13:55:09, max mem: 30.7 GB 
[11/21 16:37:13][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.2000,	3.8655 s / batch. (data: 2.96e+00). ETA=2 days, 8:34:25, max mem: 30.7 GB 
[11/21 16:38:49][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7459,	0.9231 s / batch. (data: 1.62e-02). ETA=13:29:06, max mem: 30.7 GB 
[11/21 16:39:38][INFO] visual_prompt:  217: Epoch 5 / 100: avg data time: 4.71e-02, avg batch time: 0.9683, average train loss: 0.7726
[11/21 16:40:36][INFO] visual_prompt:  316: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3005, average loss: 0.8611
[11/21 16:40:36][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.62	
[11/21 16:40:36][INFO] visual_prompt:  165: Training 6 / 100 epoch, with learning rate 0.0005
[11/21 16:42:22][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7654,	0.9075 s / batch. (data: 3.43e-04). ETA=13:13:05, max mem: 30.7 GB 
[11/21 16:43:57][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7305,	0.9173 s / batch. (data: 2.11e-02). ETA=13:20:05, max mem: 30.7 GB 
[11/21 16:45:33][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.2277,	0.8997 s / batch. (data: 5.41e-03). ETA=13:03:13, max mem: 30.7 GB 
[11/21 16:47:08][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.8866,	1.2520 s / batch. (data: 3.21e-01). ETA=18:07:53, max mem: 30.7 GB 
[11/21 16:48:45][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.0112,	0.9317 s / batch. (data: 5.38e-03). ETA=13:27:59, max mem: 30.7 GB 
[11/21 16:49:34][INFO] visual_prompt:  217: Epoch 6 / 100: avg data time: 5.29e-02, avg batch time: 0.9733, average train loss: 0.7453
[11/21 16:50:33][INFO] visual_prompt:  316: Inference (val):avg data time: 3.47e-05, avg batch time: 0.3017, average loss: 0.6674
[11/21 16:50:33][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 61.96	
[11/21 16:50:33][INFO] visual_prompt:  165: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/21 16:52:27][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6297,	0.8999 s / batch. (data: 3.04e-04). ETA=12:58:09, max mem: 30.7 GB 
[11/21 16:54:02][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5663,	0.9221 s / batch. (data: 7.32e-04). ETA=13:15:46, max mem: 30.7 GB 
[11/21 16:55:37][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6486,	0.9399 s / batch. (data: 1.04e-02). ETA=13:29:34, max mem: 30.7 GB 
[11/21 16:57:10][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6779,	0.9160 s / batch. (data: 2.88e-04). ETA=13:07:28, max mem: 30.7 GB 
[11/21 16:58:43][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6253,	0.9022 s / batch. (data: 3.03e-04). ETA=12:54:07, max mem: 30.7 GB 
[11/21 16:59:35][INFO] visual_prompt:  217: Epoch 7 / 100: avg data time: 5.98e-02, avg batch time: 0.9804, average train loss: 0.7511
[11/21 17:00:34][INFO] visual_prompt:  316: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3036, average loss: 0.6734
[11/21 17:00:34][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 64.39	
[11/21 17:00:34][INFO] visual_prompt:  165: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/21 17:02:23][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5535,	0.9398 s / batch. (data: 8.09e-03). ETA=13:23:58, max mem: 30.7 GB 
[11/21 17:04:00][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6132,	0.9454 s / batch. (data: 1.06e-02). ETA=13:27:11, max mem: 30.7 GB 
[11/21 17:05:37][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8169,	0.9173 s / batch. (data: 2.67e-03). ETA=13:01:42, max mem: 30.7 GB 
[11/21 17:07:15][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6093,	0.8917 s / batch. (data: 3.16e-04). ETA=12:38:24, max mem: 30.7 GB 
[11/21 17:08:52][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6371,	0.9342 s / batch. (data: 5.35e-03). ETA=13:12:57, max mem: 30.7 GB 
[11/21 17:09:41][INFO] visual_prompt:  217: Epoch 8 / 100: avg data time: 6.81e-02, avg batch time: 0.9886, average train loss: 0.7477
[11/21 17:10:40][INFO] visual_prompt:  316: Inference (val):avg data time: 3.58e-05, avg batch time: 0.3020, average loss: 0.6627
[11/21 17:10:40][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 61.79	rocauc: 66.68	
[11/21 17:10:40][INFO] visual_prompt:   36: Best epoch 8: best metric: -0.663
[11/21 17:10:40][INFO] visual_prompt:  165: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/21 17:12:31][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.8738,	0.9185 s / batch. (data: 5.38e-03). ETA=12:57:19, max mem: 30.7 GB 
[11/21 17:14:08][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.2659,	0.9161 s / batch. (data: 7.95e-03). ETA=12:53:42, max mem: 30.7 GB 
[11/21 17:15:44][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5446,	0.9260 s / batch. (data: 1.39e-02). ETA=13:00:32, max mem: 30.7 GB 
[11/21 17:17:29][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7119,	0.9295 s / batch. (data: 1.13e-02). ETA=13:01:59, max mem: 30.7 GB 
[11/21 17:19:05][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7430,	0.9561 s / batch. (data: 2.85e-04). ETA=13:22:42, max mem: 30.7 GB 
[11/21 17:19:59][INFO] visual_prompt:  217: Epoch 9 / 100: avg data time: 9.08e-02, avg batch time: 1.0109, average train loss: 0.7612
[11/21 17:20:58][INFO] visual_prompt:  316: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3006, average loss: 0.7194
[11/21 17:20:58][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 63.61	
[11/21 17:20:58][INFO] visual_prompt:  165: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/21 17:22:47][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7011,	0.9080 s / batch. (data: 3.03e-04). ETA=12:40:00, max mem: 30.7 GB 
[11/21 17:24:29][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8099,	0.9365 s / batch. (data: 8.43e-03). ETA=13:02:19, max mem: 30.7 GB 
[11/21 17:26:07][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9176,	0.9160 s / batch. (data: 2.74e-04). ETA=12:43:40, max mem: 30.7 GB 
[11/21 17:27:46][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6508,	0.9230 s / batch. (data: 2.54e-04). ETA=12:47:57, max mem: 30.7 GB 
[11/21 17:29:26][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7234,	2.0504 s / batch. (data: 1.12e+00). ETA=1 day, 4:22:39, max mem: 30.7 GB 
[11/21 17:30:18][INFO] visual_prompt:  217: Epoch 10 / 100: avg data time: 9.17e-02, avg batch time: 1.0123, average train loss: 0.7366
[11/21 17:31:17][INFO] visual_prompt:  316: Inference (val):avg data time: 5.09e-04, avg batch time: 0.3032, average loss: 0.7899
[11/21 17:31:17][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 63.05	
[11/21 17:31:17][INFO] visual_prompt:  165: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/21 17:33:10][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.0450,	0.9522 s / batch. (data: 2.41e-02). ETA=13:08:13, max mem: 30.7 GB 
[11/21 17:34:46][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8262,	0.9293 s / batch. (data: 1.56e-02). ETA=12:47:46, max mem: 30.7 GB 
[11/21 17:36:24][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5074,	0.9342 s / batch. (data: 1.05e-02). ETA=12:50:16, max mem: 30.7 GB 
[11/21 17:38:05][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6383,	0.9759 s / batch. (data: 1.08e-02). ETA=13:22:59, max mem: 30.7 GB 
[11/21 17:39:43][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6327,	0.9719 s / batch. (data: 5.43e-03). ETA=13:18:03, max mem: 30.7 GB 
[11/21 17:40:36][INFO] visual_prompt:  217: Epoch 11 / 100: avg data time: 8.88e-02, avg batch time: 1.0107, average train loss: 0.7623
[11/21 17:41:36][INFO] visual_prompt:  316: Inference (val):avg data time: 3.84e-04, avg batch time: 0.3022, average loss: 0.8459
[11/21 17:41:36][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.37	
[11/21 17:41:36][INFO] visual_prompt:  165: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/21 17:43:24][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5506,	0.9351 s / batch. (data: 5.38e-03). ETA=12:45:29, max mem: 30.7 GB 
[11/21 17:45:10][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7810,	0.9200 s / batch. (data: 2.82e-04). ETA=12:31:34, max mem: 30.7 GB 
[11/21 17:46:49][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7985,	0.9116 s / batch. (data: 2.63e-04). ETA=12:23:13, max mem: 30.7 GB 
[11/21 17:48:32][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.0704,	0.9138 s / batch. (data: 7.30e-04). ETA=12:23:26, max mem: 30.7 GB 
[11/21 17:50:14][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6888,	0.9234 s / batch. (data: 5.37e-03). ETA=12:29:44, max mem: 30.7 GB 
[11/21 17:51:06][INFO] visual_prompt:  217: Epoch 12 / 100: avg data time: 1.10e-01, avg batch time: 1.0304, average train loss: 0.7783
[11/21 17:52:05][INFO] visual_prompt:  316: Inference (val):avg data time: 2.84e-04, avg batch time: 0.3005, average loss: 1.0043
[11/21 17:52:05][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.57	
[11/21 17:52:05][INFO] visual_prompt:  165: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/21 17:53:56][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6430,	0.9327 s / batch. (data: 2.07e-02). ETA=12:34:55, max mem: 30.7 GB 
[11/21 17:55:37][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7517,	0.9278 s / batch. (data: 4.91e-03). ETA=12:29:22, max mem: 30.7 GB 
[11/21 17:57:16][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6238,	0.9054 s / batch. (data: 2.66e-04). ETA=12:09:49, max mem: 30.7 GB 
[11/21 17:58:58][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6045,	0.9161 s / batch. (data: 5.59e-03). ETA=12:16:55, max mem: 30.7 GB 
[11/21 18:00:37][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7709,	0.9332 s / batch. (data: 2.72e-04). ETA=12:29:08, max mem: 30.7 GB 
[11/21 18:01:27][INFO] visual_prompt:  217: Epoch 13 / 100: avg data time: 9.51e-02, avg batch time: 1.0158, average train loss: 0.7437
[11/21 18:02:27][INFO] visual_prompt:  316: Inference (val):avg data time: 3.59e-05, avg batch time: 0.3010, average loss: 0.7146
[11/21 18:02:27][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.55	
[11/21 18:02:27][INFO] visual_prompt:  165: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/21 18:04:22][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6832,	0.9147 s / batch. (data: 2.94e-04). ETA=12:11:54, max mem: 30.7 GB 
[11/21 18:06:04][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.3382,	0.9171 s / batch. (data: 5.41e-03). ETA=12:12:17, max mem: 30.7 GB 
[11/21 18:07:45][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6942,	0.9411 s / batch. (data: 2.90e-04). ETA=12:29:55, max mem: 30.7 GB 
[11/21 18:09:26][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9725,	0.9063 s / batch. (data: 3.22e-04). ETA=12:00:39, max mem: 30.7 GB 
[11/21 18:11:05][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7262,	0.9262 s / batch. (data: 2.79e-04). ETA=12:14:56, max mem: 30.7 GB 
[11/21 18:11:58][INFO] visual_prompt:  217: Epoch 14 / 100: avg data time: 1.11e-01, avg batch time: 1.0323, average train loss: 0.7481
[11/21 18:12:57][INFO] visual_prompt:  316: Inference (val):avg data time: 5.03e-04, avg batch time: 0.3018, average loss: 0.6882
[11/21 18:12:57][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 52.44	rocauc: 60.36	
[11/21 18:12:57][INFO] visual_prompt:  165: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/21 18:14:48][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7162,	0.8955 s / batch. (data: 2.84e-04). ETA=11:48:18, max mem: 30.7 GB 
[11/21 18:16:31][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6730,	0.9268 s / batch. (data: 5.40e-03). ETA=12:11:30, max mem: 30.7 GB 
[11/21 18:18:11][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.0156,	0.9614 s / batch. (data: 1.08e-02). ETA=12:37:13, max mem: 30.7 GB 
[11/21 18:19:53][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5667,	0.9495 s / batch. (data: 2.78e-04). ETA=12:26:14, max mem: 30.7 GB 
[11/21 18:21:33][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8838,	0.9067 s / batch. (data: 2.47e-04). ETA=11:51:05, max mem: 30.7 GB 
[11/21 18:22:22][INFO] visual_prompt:  217: Epoch 15 / 100: avg data time: 9.98e-02, avg batch time: 1.0200, average train loss: 0.7251
[11/21 18:23:16][INFO] visual_prompt:  316: Inference (val):avg data time: 1.55e-04, avg batch time: 0.3008, average loss: 0.7262
[11/21 18:23:16][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.19	
[11/21 18:23:16][INFO] visual_prompt:  165: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/21 18:24:58][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7716,	0.9313 s / batch. (data: 8.25e-04). ETA=12:08:02, max mem: 30.7 GB 
[11/21 18:26:30][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5952,	0.9426 s / batch. (data: 1.55e-02). ETA=12:15:19, max mem: 30.7 GB 
[11/21 18:28:03][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9086,	0.9330 s / batch. (data: 8.24e-04). ETA=12:06:17, max mem: 30.7 GB 
[11/21 18:29:35][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5912,	0.9446 s / batch. (data: 2.77e-04). ETA=12:13:42, max mem: 30.7 GB 
[11/21 18:31:07][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7226,	0.9370 s / batch. (data: 7.33e-04). ETA=12:06:16, max mem: 30.7 GB 
[11/21 18:31:56][INFO] visual_prompt:  217: Epoch 16 / 100: avg data time: 2.31e-02, avg batch time: 0.9389, average train loss: 0.7173
[11/21 18:32:50][INFO] visual_prompt:  316: Inference (val):avg data time: 3.16e-05, avg batch time: 0.3033, average loss: 0.8011
[11/21 18:32:50][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.07	
[11/21 18:32:50][INFO] visual_prompt:  165: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/21 18:34:30][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.8178,	0.9522 s / batch. (data: 2.48e-02). ETA=12:15:37, max mem: 30.7 GB 
[11/21 18:36:06][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8274,	0.9374 s / batch. (data: 2.07e-02). ETA=12:02:38, max mem: 30.7 GB 
[11/21 18:37:38][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.2690,	0.9064 s / batch. (data: 2.95e-04). ETA=11:37:12, max mem: 30.7 GB 
[11/21 18:39:14][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5805,	0.9262 s / batch. (data: 5.37e-03). ETA=11:50:54, max mem: 30.7 GB 
[11/21 18:40:48][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7030,	0.9032 s / batch. (data: 2.90e-04). ETA=11:31:43, max mem: 30.7 GB 
[11/21 18:41:38][INFO] visual_prompt:  217: Epoch 17 / 100: avg data time: 3.47e-02, avg batch time: 0.9545, average train loss: 0.7197
[11/21 18:42:36][INFO] visual_prompt:  316: Inference (val):avg data time: 1.61e-04, avg batch time: 0.2990, average loss: 0.6756
[11/21 18:42:36][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 59.14	
[11/21 18:42:36][INFO] visual_prompt:  165: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/21 18:44:24][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6769,	0.9588 s / batch. (data: 7.62e-04). ETA=12:11:51, max mem: 30.7 GB 
[11/21 18:45:59][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7586,	0.9363 s / batch. (data: 3.34e-02). ETA=11:53:10, max mem: 30.7 GB 
[11/21 18:47:32][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6943,	0.9218 s / batch. (data: 7.96e-03). ETA=11:40:32, max mem: 30.7 GB 
[11/21 18:49:05][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7871,	0.9168 s / batch. (data: 2.58e-04). ETA=11:35:13, max mem: 30.7 GB 
[11/21 18:50:42][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6610,	0.9360 s / batch. (data: 7.43e-04). ETA=11:48:15, max mem: 30.7 GB 
[11/21 18:51:32][INFO] visual_prompt:  217: Epoch 18 / 100: avg data time: 4.99e-02, avg batch time: 0.9701, average train loss: 0.7185
[11/21 18:52:30][INFO] visual_prompt:  316: Inference (val):avg data time: 6.32e-04, avg batch time: 0.3025, average loss: 0.7800
[11/21 18:52:30][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.80	
[11/21 18:52:30][INFO] visual_prompt:  165: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/21 18:54:15][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6676,	0.9269 s / batch. (data: 5.92e-03). ETA=11:39:00, max mem: 30.7 GB 
[11/21 18:55:52][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8745,	2.9895 s / batch. (data: 2.08e+00). ETA=1 day, 13:29:23, max mem: 30.7 GB 
[11/21 18:57:24][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6719,	0.9360 s / batch. (data: 7.95e-03). ETA=11:42:41, max mem: 30.7 GB 
[11/21 18:58:59][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6874,	0.9343 s / batch. (data: 1.06e-02). ETA=11:39:51, max mem: 30.7 GB 
[11/21 19:00:34][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5270,	0.9058 s / batch. (data: 3.26e-03). ETA=11:17:01, max mem: 30.7 GB 
[11/21 19:01:23][INFO] visual_prompt:  217: Epoch 19 / 100: avg data time: 4.38e-02, avg batch time: 0.9637, average train loss: 0.7170
[11/21 19:02:20][INFO] visual_prompt:  316: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3006, average loss: 0.8259
[11/21 19:02:20][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.38	
[11/21 19:02:20][INFO] visual_prompt:  165: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/21 19:04:08][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6702,	0.9155 s / batch. (data: 2.99e-04). ETA=11:21:54, max mem: 30.7 GB 
[11/21 19:05:41][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7752,	0.9445 s / batch. (data: 1.64e-02). ETA=11:41:57, max mem: 30.7 GB 
[11/21 19:07:15][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6504,	0.9325 s / batch. (data: 2.75e-04). ETA=11:31:30, max mem: 30.7 GB 
[11/21 19:08:50][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6793,	0.9276 s / batch. (data: 7.86e-04). ETA=11:26:18, max mem: 30.7 GB 
[11/21 19:10:23][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7653,	0.9337 s / batch. (data: 2.84e-04). ETA=11:29:15, max mem: 30.7 GB 
[11/21 19:11:14][INFO] visual_prompt:  217: Epoch 20 / 100: avg data time: 4.61e-02, avg batch time: 0.9651, average train loss: 0.7346
[11/21 19:12:11][INFO] visual_prompt:  316: Inference (val):avg data time: 1.51e-04, avg batch time: 0.3009, average loss: 0.6990
[11/21 19:12:11][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 55.02	
[11/21 19:12:11][INFO] visual_prompt:  165: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/21 19:13:59][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6760,	0.9400 s / batch. (data: 7.82e-04). ETA=11:31:31, max mem: 30.7 GB 
[11/21 19:15:35][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8808,	0.9360 s / batch. (data: 2.73e-04). ETA=11:27:00, max mem: 30.7 GB 
[11/21 19:17:09][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6670,	0.9178 s / batch. (data: 2.74e-04). ETA=11:12:07, max mem: 30.7 GB 
[11/21 19:18:44][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.0172,	0.9134 s / batch. (data: 7.88e-03). ETA=11:07:24, max mem: 30.7 GB 
[11/21 19:20:16][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7626,	0.9327 s / batch. (data: 2.89e-04). ETA=11:19:54, max mem: 30.7 GB 
[11/21 19:21:05][INFO] visual_prompt:  217: Epoch 21 / 100: avg data time: 4.56e-02, avg batch time: 0.9645, average train loss: 0.7164
[11/21 19:22:02][INFO] visual_prompt:  316: Inference (val):avg data time: 8.71e-05, avg batch time: 0.3008, average loss: 0.6921
[11/21 19:22:02][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 50.81	rocauc: 56.27	
[11/21 19:22:02][INFO] visual_prompt:  165: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/21 19:23:46][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6959,	0.9240 s / batch. (data: 5.39e-03). ETA=11:11:14, max mem: 30.7 GB 
[11/21 19:25:24][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9053,	0.9390 s / batch. (data: 7.94e-04). ETA=11:20:34, max mem: 30.7 GB 
[11/21 19:26:59][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6623,	0.9347 s / batch. (data: 5.88e-03). ETA=11:15:55, max mem: 30.7 GB 
[11/21 19:28:32][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.8282,	0.9340 s / batch. (data: 5.97e-03). ETA=11:13:50, max mem: 30.7 GB 
[11/21 19:30:06][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8120,	0.9142 s / batch. (data: 2.85e-04). ETA=10:58:03, max mem: 30.7 GB 
[11/21 19:30:56][INFO] visual_prompt:  217: Epoch 22 / 100: avg data time: 4.43e-02, avg batch time: 0.9643, average train loss: 0.7150
[11/21 19:31:53][INFO] visual_prompt:  316: Inference (val):avg data time: 8.93e-05, avg batch time: 0.3000, average loss: 0.7473
[11/21 19:31:53][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.58	
[11/21 19:31:53][INFO] visual_prompt:   42: Stopping early.

[11/22 11:11:12][INFO] visual_prompt:   95: Rank of current process: 0. World size: 1
[11/22 11:11:12][INFO] visual_prompt:   97: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 11:11:12][INFO] visual_prompt:   99: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/22 11:11:12][INFO] visual_prompt:  101: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/22 11:11:12][INFO] visual_prompt:  108: Training with config:
[11/22 11:11:12][INFO] visual_prompt:  109: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size800/val/seed0/lr0.0005_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.0005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/22 11:11:12][INFO] visual_prompt:   55: Loading training data...
[11/22 11:11:12][INFO] visual_prompt:   28: Constructing mammo-cbis dataset train...
[11/22 11:11:12][INFO] visual_prompt:   57: Loading validation data...
[11/22 11:11:12][INFO] visual_prompt:   28: Constructing mammo-cbis dataset val...
[11/22 11:11:12][INFO] visual_prompt:   38: Constructing models...
[11/22 11:11:14][INFO] visual_prompt:  153: Enable all parameters update during training
[11/22 11:11:14][INFO] visual_prompt:   52: Total Parameters: 87569666	 Gradient Parameters: 87569666
[11/22 11:11:14][INFO] visual_prompt:   54: tuned percent:100.000
[11/22 11:11:14][INFO] visual_prompt:   40: Device used for model: 0
[11/22 11:11:14][INFO] visual_prompt:   40: Setting up Evaluator...
[11/22 11:11:14][INFO] visual_prompt:   42: Setting up Trainer...
[11/22 11:11:14][INFO] visual_prompt:   45: 	Setting up the optimizer...
[11/22 11:11:14][INFO] visual_prompt:  165: Training 1 / 100 epoch, with learning rate 0.0
[11/22 11:12:59][INFO] visual_prompt:  204: 	Training 100/553. train loss: 10.8600,	0.9240 s / batch. (data: 2.77e-04). ETA=14:10:05, max mem: 30.7 GB 
[11/22 11:14:39][INFO] visual_prompt:  204: 	Training 200/553. train loss: 9.4496,	0.9320 s / batch. (data: 3.18e-04). ETA=14:15:52, max mem: 30.7 GB 
[11/22 11:16:17][INFO] visual_prompt:  204: 	Training 300/553. train loss: 9.8169,	0.9300 s / batch. (data: 2.43e-04). ETA=14:12:31, max mem: 30.7 GB 
[11/22 11:17:51][INFO] visual_prompt:  204: 	Training 400/553. train loss: 7.5668,	0.9511 s / batch. (data: 5.38e-03). ETA=14:30:13, max mem: 30.7 GB 
[11/22 11:19:29][INFO] visual_prompt:  204: 	Training 500/553. train loss: 3.1665,	0.9240 s / batch. (data: 2.85e-04). ETA=14:03:54, max mem: 30.7 GB 
[11/22 11:20:18][INFO] visual_prompt:  217: Epoch 1 / 100: avg data time: 5.16e-02, avg batch time: 0.9838, average train loss: 7.6130
[11/22 11:21:17][INFO] visual_prompt:  316: Inference (val):avg data time: 8.44e-05, avg batch time: 0.3044, average loss: 6.9126
[11/22 11:21:17][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.15	
[11/22 11:21:17][INFO] visual_prompt:  165: Training 2 / 100 epoch, with learning rate 0.0001
[11/22 11:23:04][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.8513,	0.9480 s / batch. (data: 7.25e-04). ETA=14:23:27, max mem: 30.7 GB 
[11/22 11:24:39][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.5982,	2.1736 s / batch. (data: 1.26e+00). ETA=1 day, 8:56:03, max mem: 30.7 GB 
[11/22 11:26:16][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8438,	0.9195 s / batch. (data: 5.12e-03). ETA=13:54:23, max mem: 30.7 GB 
[11/22 11:27:54][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6157,	0.9423 s / batch. (data: 5.52e-03). ETA=14:13:31, max mem: 30.7 GB 
[11/22 11:29:31][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.9069,	0.9320 s / batch. (data: 3.30e-04). ETA=14:02:37, max mem: 30.7 GB 
[11/22 11:30:21][INFO] visual_prompt:  217: Epoch 2 / 100: avg data time: 5.22e-02, avg batch time: 0.9834, average train loss: 0.9656
[11/22 11:31:19][INFO] visual_prompt:  316: Inference (val):avg data time: 3.15e-05, avg batch time: 0.3061, average loss: 1.1204
[11/22 11:31:19][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.22	
[11/22 11:31:19][INFO] visual_prompt:  165: Training 3 / 100 epoch, with learning rate 0.0002
[11/22 11:33:06][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6865,	0.9478 s / batch. (data: 5.85e-03). ETA=14:14:32, max mem: 30.7 GB 
[11/22 11:34:43][INFO] visual_prompt:  204: 	Training 200/553. train loss: 2.5361,	0.9263 s / batch. (data: 2.91e-04). ETA=13:53:34, max mem: 30.7 GB 
[11/22 11:36:20][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9344,	0.9286 s / batch. (data: 2.57e-04). ETA=13:54:08, max mem: 30.7 GB 
[11/22 11:37:56][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7066,	0.9440 s / batch. (data: 2.71e-04). ETA=14:06:20, max mem: 30.7 GB 
[11/22 11:39:31][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.0653,	0.9600 s / batch. (data: 2.71e-04). ETA=14:19:07, max mem: 30.7 GB 
[11/22 11:40:21][INFO] visual_prompt:  217: Epoch 3 / 100: avg data time: 4.75e-02, avg batch time: 0.9796, average train loss: 0.8295
[11/22 11:41:19][INFO] visual_prompt:  316: Inference (val):avg data time: 3.22e-05, avg batch time: 0.3045, average loss: 0.6953
[11/22 11:41:19][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 53.66	rocauc: 61.64	
[11/22 11:41:19][INFO] visual_prompt:  165: Training 4 / 100 epoch, with learning rate 0.0003
[11/22 11:43:06][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6718,	0.9400 s / batch. (data: 2.71e-04). ETA=13:58:51, max mem: 30.7 GB 
[11/22 11:44:42][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9921,	1.4685 s / batch. (data: 5.21e-01). ETA=21:47:58, max mem: 30.7 GB 
[11/22 11:46:18][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9739,	0.9513 s / batch. (data: 5.37e-03). ETA=14:05:43, max mem: 30.7 GB 
[11/22 11:47:55][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5086,	0.9527 s / batch. (data: 7.99e-03). ETA=14:05:21, max mem: 30.7 GB 
[11/22 11:49:32][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.9723,	0.9415 s / batch. (data: 3.07e-04). ETA=13:53:54, max mem: 30.7 GB 
[11/22 11:50:24][INFO] visual_prompt:  217: Epoch 4 / 100: avg data time: 5.38e-02, avg batch time: 0.9848, average train loss: 0.8106
[11/22 11:51:22][INFO] visual_prompt:  316: Inference (val):avg data time: 3.20e-05, avg batch time: 0.3074, average loss: 0.6956
[11/22 11:51:22][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 58.94	rocauc: 62.68	
[11/22 11:51:22][INFO] visual_prompt:  165: Training 5 / 100 epoch, with learning rate 0.0004
[11/22 11:53:11][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6789,	0.9073 s / batch. (data: 2.45e-04). ETA=13:21:15, max mem: 30.7 GB 
[11/22 11:54:48][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6411,	0.9328 s / batch. (data: 3.04e-04). ETA=13:42:15, max mem: 30.7 GB 
[11/22 11:56:24][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7581,	0.9469 s / batch. (data: 5.43e-03). ETA=13:53:03, max mem: 30.7 GB 
[11/22 11:58:03][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.1937,	3.9567 s / batch. (data: 3.04e+00). ETA=2 days, 9:54:32, max mem: 30.7 GB 
[11/22 11:59:40][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7503,	0.9360 s / batch. (data: 7.36e-04). ETA=13:40:23, max mem: 30.7 GB 
[11/22 12:00:30][INFO] visual_prompt:  217: Epoch 5 / 100: avg data time: 5.81e-02, avg batch time: 0.9901, average train loss: 0.7724
[11/22 12:01:29][INFO] visual_prompt:  316: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3046, average loss: 0.8424
[11/22 12:01:29][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 44.72	rocauc: 64.18	
[11/22 12:01:29][INFO] visual_prompt:  165: Training 6 / 100 epoch, with learning rate 0.0005
[11/22 12:03:17][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7041,	0.9200 s / batch. (data: 5.42e-03). ETA=13:23:59, max mem: 30.7 GB 
[11/22 12:04:52][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7292,	0.9186 s / batch. (data: 2.88e-04). ETA=13:21:14, max mem: 30.7 GB 
[11/22 12:06:29][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.2312,	0.9418 s / batch. (data: 3.98e-03). ETA=13:39:54, max mem: 30.7 GB 
[11/22 12:08:06][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.8251,	2.1763 s / batch. (data: 1.24e+00). ETA=1 day, 7:30:58, max mem: 30.7 GB 
[11/22 12:09:45][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.0673,	0.9258 s / batch. (data: 7.39e-04). ETA=13:22:53, max mem: 30.7 GB 
[11/22 12:10:35][INFO] visual_prompt:  217: Epoch 6 / 100: avg data time: 5.69e-02, avg batch time: 0.9879, average train loss: 0.7446
[11/22 12:11:34][INFO] visual_prompt:  316: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3039, average loss: 0.6716
[11/22 12:11:34][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 60.98	rocauc: 63.45	
[11/22 12:11:34][INFO] visual_prompt:  165: Training 7 / 100 epoch, with learning rate 0.0004998633143352315
[11/22 12:13:26][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6907,	0.9160 s / batch. (data: 2.67e-04). ETA=13:12:03, max mem: 30.7 GB 
[11/22 12:15:01][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5593,	0.9310 s / batch. (data: 1.13e-02). ETA=13:23:29, max mem: 30.7 GB 
[11/22 12:16:37][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6930,	0.9523 s / batch. (data: 7.49e-04). ETA=13:40:16, max mem: 30.7 GB 
[11/22 12:18:11][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6873,	0.9182 s / batch. (data: 3.08e-04). ETA=13:09:24, max mem: 30.7 GB 
[11/22 12:19:49][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6061,	0.9500 s / batch. (data: 1.05e-02). ETA=13:35:08, max mem: 30.7 GB 
[11/22 12:20:39][INFO] visual_prompt:  217: Epoch 7 / 100: avg data time: 5.44e-02, avg batch time: 0.9861, average train loss: 0.7468
[11/22 12:21:38][INFO] visual_prompt:  316: Inference (val):avg data time: 8.35e-05, avg batch time: 0.3081, average loss: 0.6723
[11/22 12:21:38][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 63.31	
[11/22 12:21:38][INFO] visual_prompt:  165: Training 8 / 100 epoch, with learning rate 0.0004994534068046936
[11/22 12:23:25][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6438,	0.9358 s / batch. (data: 1.05e-02). ETA=13:20:33, max mem: 30.7 GB 
[11/22 12:25:03][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5069,	0.9403 s / batch. (data: 5.37e-03). ETA=13:22:52, max mem: 30.7 GB 
[11/22 12:26:41][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8445,	0.9473 s / batch. (data: 7.71e-04). ETA=13:27:15, max mem: 30.7 GB 
[11/22 12:28:18][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5897,	0.9425 s / batch. (data: 2.24e-04). ETA=13:21:35, max mem: 30.7 GB 
[11/22 12:29:56][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6475,	0.9193 s / batch. (data: 2.81e-04). ETA=13:00:17, max mem: 30.7 GB 
[11/22 12:30:45][INFO] visual_prompt:  217: Epoch 8 / 100: avg data time: 5.83e-02, avg batch time: 0.9892, average train loss: 0.7387
[11/22 12:31:44][INFO] visual_prompt:  316: Inference (val):avg data time: 3.25e-05, avg batch time: 0.3044, average loss: 0.6565
[11/22 12:31:44][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 63.41	rocauc: 66.30	
[11/22 12:31:44][INFO] visual_prompt:   36: Best epoch 8: best metric: -0.657
[11/22 12:31:44][INFO] visual_prompt:  165: Training 9 / 100 epoch, with learning rate 0.0004987707256362529
[11/22 12:33:34][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.8672,	0.9365 s / batch. (data: 1.00e-02). ETA=13:12:32, max mem: 30.7 GB 
[11/22 12:35:12][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.1425,	0.9331 s / batch. (data: 5.39e-03). ETA=13:08:04, max mem: 30.7 GB 
[11/22 12:36:47][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5695,	0.9280 s / batch. (data: 2.62e-04). ETA=13:02:14, max mem: 30.7 GB 
[11/22 12:38:23][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7844,	0.9240 s / batch. (data: 8.29e-04). ETA=12:57:21, max mem: 30.7 GB 
[11/22 12:39:57][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7561,	0.9442 s / batch. (data: 4.13e-03). ETA=13:12:43, max mem: 30.7 GB 
[11/22 12:40:48][INFO] visual_prompt:  217: Epoch 9 / 100: avg data time: 5.39e-02, avg batch time: 0.9846, average train loss: 0.7547
[11/22 12:41:47][INFO] visual_prompt:  316: Inference (val):avg data time: 2.31e-04, avg batch time: 0.3057, average loss: 0.7306
[11/22 12:41:47][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 66.02	
[11/22 12:41:47][INFO] visual_prompt:  165: Training 10 / 100 epoch, with learning rate 0.0004978160173317438
[11/22 12:43:34][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6737,	0.9606 s / batch. (data: 5.39e-03). ETA=13:24:02, max mem: 30.7 GB 
[11/22 12:45:13][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9183,	0.9280 s / batch. (data: 2.68e-04). ETA=12:55:13, max mem: 30.7 GB 
[11/22 12:46:48][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7657,	0.9361 s / batch. (data: 7.84e-04). ETA=13:00:25, max mem: 30.7 GB 
[11/22 12:48:23][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5113,	0.9549 s / batch. (data: 1.48e-02). ETA=13:14:30, max mem: 30.7 GB 
[11/22 12:49:59][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6439,	0.9564 s / batch. (data: 1.50e-02). ETA=13:14:11, max mem: 30.7 GB 
[11/22 12:50:51][INFO] visual_prompt:  217: Epoch 10 / 100: avg data time: 5.25e-02, avg batch time: 0.9841, average train loss: 0.7352
[11/22 12:51:50][INFO] visual_prompt:  316: Inference (val):avg data time: 3.39e-05, avg batch time: 0.3036, average loss: 0.7702
[11/22 12:51:50][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 60.84	
[11/22 12:51:50][INFO] visual_prompt:  165: Training 11 / 100 epoch, with learning rate 0.0004965903258506806
[11/22 12:53:41][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.0376,	0.9200 s / batch. (data: 2.64e-04). ETA=12:41:36, max mem: 30.7 GB 
[11/22 12:55:16][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8345,	0.9440 s / batch. (data: 2.88e-04). ETA=12:59:53, max mem: 30.7 GB 
[11/22 12:56:49][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5044,	0.9480 s / batch. (data: 5.84e-03). ETA=13:01:38, max mem: 30.7 GB 
[11/22 12:58:28][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9051,	0.9312 s / batch. (data: 2.60e-04). ETA=12:46:13, max mem: 30.7 GB 
[11/22 13:00:03][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.4996,	0.9266 s / batch. (data: 2.68e-04). ETA=12:40:53, max mem: 30.7 GB 
[11/22 13:00:53][INFO] visual_prompt:  217: Epoch 11 / 100: avg data time: 5.27e-02, avg batch time: 0.9827, average train loss: 0.7196
[11/22 13:01:52][INFO] visual_prompt:  316: Inference (val):avg data time: 3.02e-05, avg batch time: 0.3058, average loss: 0.6777
[11/22 13:01:52][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 59.35	rocauc: 59.90	
[11/22 13:01:52][INFO] visual_prompt:  165: Training 12 / 100 epoch, with learning rate 0.0004950949914687023
[11/22 13:03:36][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5457,	0.9260 s / batch. (data: 5.37e-03). ETA=12:38:01, max mem: 30.7 GB 
[11/22 13:05:17][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8439,	0.9354 s / batch. (data: 7.80e-03). ETA=12:44:11, max mem: 30.7 GB 
[11/22 13:06:53][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8217,	0.9580 s / batch. (data: 3.11e-04). ETA=13:01:01, max mem: 30.7 GB 
[11/22 13:08:30][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9145,	0.9465 s / batch. (data: 2.57e-04). ETA=12:50:06, max mem: 30.7 GB 
[11/22 13:10:06][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6788,	0.9320 s / batch. (data: 2.68e-04). ETA=12:36:44, max mem: 30.7 GB 
[11/22 13:10:57][INFO] visual_prompt:  217: Epoch 12 / 100: avg data time: 5.58e-02, avg batch time: 0.9859, average train loss: 0.7851
[11/22 13:11:56][INFO] visual_prompt:  316: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3037, average loss: 1.0087
[11/22 13:11:56][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.25	
[11/22 13:11:56][INFO] visual_prompt:  165: Training 13 / 100 epoch, with learning rate 0.0004933316493120015
[11/22 13:13:44][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6654,	0.9030 s / batch. (data: 2.89e-04). ETA=12:10:53, max mem: 30.7 GB 
[11/22 13:15:21][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8363,	0.9160 s / batch. (data: 7.99e-03). ETA=12:19:52, max mem: 30.7 GB 
[11/22 13:16:58][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6305,	0.9241 s / batch. (data: 2.66e-04). ETA=12:24:52, max mem: 30.7 GB 
[11/22 13:18:36][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6475,	0.9320 s / batch. (data: 8.91e-03). ETA=12:29:43, max mem: 30.7 GB 
[11/22 13:20:11][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7910,	0.9351 s / batch. (data: 7.58e-04). ETA=12:30:39, max mem: 30.7 GB 
[11/22 13:21:01][INFO] visual_prompt:  217: Epoch 13 / 100: avg data time: 5.29e-02, avg batch time: 0.9848, average train loss: 0.7442
[11/22 13:21:59][INFO] visual_prompt:  316: Inference (val):avg data time: 4.36e-04, avg batch time: 0.3052, average loss: 0.7244
[11/22 13:21:59][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.42	
[11/22 13:21:59][INFO] visual_prompt:  165: Training 14 / 100 epoch, with learning rate 0.0004913022275693372
[11/22 13:23:49][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6473,	0.9436 s / batch. (data: 2.49e-04). ETA=12:35:03, max mem: 30.7 GB 
[11/22 13:25:28][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.2841,	0.9231 s / batch. (data: 2.62e-04). ETA=12:17:04, max mem: 30.7 GB 
[11/22 13:27:04][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7002,	1.2138 s / batch. (data: 3.00e-01). ETA=16:07:11, max mem: 30.7 GB 
[11/22 13:28:39][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9454,	0.9290 s / batch. (data: 3.04e-04). ETA=12:18:45, max mem: 30.7 GB 
[11/22 13:30:16][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6671,	0.9640 s / batch. (data: 7.95e-03). ETA=12:44:58, max mem: 30.7 GB 
[11/22 13:31:05][INFO] visual_prompt:  217: Epoch 14 / 100: avg data time: 5.80e-02, avg batch time: 0.9871, average train loss: 0.7437
[11/22 13:32:04][INFO] visual_prompt:  316: Inference (val):avg data time: 8.51e-05, avg batch time: 0.3013, average loss: 0.6930
[11/22 13:32:04][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 60.07	
[11/22 13:32:04][INFO] visual_prompt:  165: Training 15 / 100 epoch, with learning rate 0.0004890089453835894
[11/22 13:33:53][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6939,	0.9303 s / batch. (data: 2.69e-04). ETA=12:15:52, max mem: 30.7 GB 
[11/22 13:35:30][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7180,	0.9768 s / batch. (data: 5.85e-03). ETA=12:50:57, max mem: 30.7 GB 
[11/22 13:37:08][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9971,	0.9524 s / batch. (data: 1.05e-02). ETA=12:30:06, max mem: 30.7 GB 
[11/22 13:38:44][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.4975,	0.9539 s / batch. (data: 1.79e-02). ETA=12:29:44, max mem: 30.7 GB 
[11/22 13:40:20][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8928,	0.9109 s / batch. (data: 2.62e-04). ETA=11:54:24, max mem: 30.7 GB 
[11/22 13:41:09][INFO] visual_prompt:  217: Epoch 15 / 100: avg data time: 5.50e-02, avg batch time: 0.9867, average train loss: 0.7284
[11/22 13:42:08][INFO] visual_prompt:  316: Inference (val):avg data time: 3.26e-05, avg batch time: 0.3044, average loss: 0.7401
[11/22 13:42:08][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.80	
[11/22 13:42:08][INFO] visual_prompt:  165: Training 16 / 100 epoch, with learning rate 0.00048645431042515866
[11/22 13:43:54][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7895,	0.9108 s / batch. (data: 3.02e-04). ETA=11:52:00, max mem: 30.7 GB 
[11/22 13:45:28][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5870,	1.1520 s / batch. (data: 2.08e-01). ETA=14:58:39, max mem: 30.7 GB 
[11/22 13:47:10][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9182,	0.9545 s / batch. (data: 6.49e-03). ETA=12:23:01, max mem: 30.7 GB 
[11/22 13:48:47][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6374,	0.9626 s / batch. (data: 1.04e-02). ETA=12:27:44, max mem: 30.7 GB 
[11/22 13:50:21][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7078,	0.9468 s / batch. (data: 5.82e-03). ETA=12:13:53, max mem: 30.7 GB 
[11/22 13:51:12][INFO] visual_prompt:  217: Epoch 16 / 100: avg data time: 5.20e-02, avg batch time: 0.9832, average train loss: 0.7177
[11/22 13:52:12][INFO] visual_prompt:  316: Inference (val):avg data time: 1.24e-04, avg batch time: 0.3049, average loss: 0.8127
[11/22 13:52:12][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.18	
[11/22 13:52:12][INFO] visual_prompt:  165: Training 17 / 100 epoch, with learning rate 0.0004836411161498652
[11/22 13:53:56][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7705,	0.9381 s / batch. (data: 2.46e-04). ETA=12:04:44, max mem: 30.7 GB 
[11/22 13:55:37][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9846,	0.9222 s / batch. (data: 3.47e-04). ETA=11:50:54, max mem: 30.7 GB 
[11/22 13:57:10][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.3557,	0.9389 s / batch. (data: 2.96e-04). ETA=12:02:11, max mem: 30.7 GB 
[11/22 13:58:49][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5184,	0.9484 s / batch. (data: 5.39e-03). ETA=12:07:54, max mem: 30.7 GB 
[11/22 14:00:26][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7559,	0.9277 s / batch. (data: 2.44e-04). ETA=11:50:29, max mem: 30.7 GB 
[11/22 14:01:17][INFO] visual_prompt:  217: Epoch 17 / 100: avg data time: 5.53e-02, avg batch time: 0.9856, average train loss: 0.7242
[11/22 14:02:16][INFO] visual_prompt:  316: Inference (val):avg data time: 3.18e-04, avg batch time: 0.3037, average loss: 0.6796
[11/22 14:02:16][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 58.54	rocauc: 57.46	
[11/22 14:02:16][INFO] visual_prompt:  165: Training 18 / 100 epoch, with learning rate 0.0004805724387443462
[11/22 14:04:06][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7188,	0.9093 s / batch. (data: 7.32e-03). ETA=11:34:05, max mem: 30.7 GB 
[11/22 14:05:43][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7266,	0.9267 s / batch. (data: 2.56e-04). ETA=11:45:48, max mem: 30.7 GB 
[11/22 14:07:18][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7207,	0.9303 s / batch. (data: 7.76e-03). ETA=11:47:00, max mem: 30.7 GB 
[11/22 14:08:53][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7670,	0.9186 s / batch. (data: 2.75e-04). ETA=11:36:33, max mem: 30.7 GB 
[11/22 14:10:33][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6052,	0.9373 s / batch. (data: 7.01e-04). ETA=11:49:13, max mem: 30.7 GB 
[11/22 14:11:22][INFO] visual_prompt:  217: Epoch 18 / 100: avg data time: 5.78e-02, avg batch time: 0.9877, average train loss: 0.7196
[11/22 14:12:21][INFO] visual_prompt:  316: Inference (val):avg data time: 3.09e-05, avg batch time: 0.3018, average loss: 0.7644
[11/22 14:12:21][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.12	
[11/22 14:12:21][INFO] visual_prompt:  165: Training 19 / 100 epoch, with learning rate 0.00047725163376229063
[11/22 14:14:08][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6528,	0.9438 s / batch. (data: 7.75e-04). ETA=11:51:42, max mem: 30.7 GB 
[11/22 14:15:46][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7099,	2.8248 s / batch. (data: 1.92e+00). ETA=1 day, 11:25:28, max mem: 30.7 GB 
[11/22 14:17:22][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6649,	0.9175 s / batch. (data: 3.01e-04). ETA=11:28:49, max mem: 30.7 GB 
[11/22 14:18:58][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6931,	0.9558 s / batch. (data: 5.38e-03). ETA=11:55:59, max mem: 30.7 GB 
[11/22 14:20:34][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.3876,	0.9463 s / batch. (data: 5.38e-03). ETA=11:47:18, max mem: 30.7 GB 
[11/22 14:21:24][INFO] visual_prompt:  217: Epoch 19 / 100: avg data time: 5.04e-02, avg batch time: 0.9821, average train loss: 0.7145
[11/22 14:22:22][INFO] visual_prompt:  316: Inference (val):avg data time: 1.49e-04, avg batch time: 0.3035, average loss: 0.8298
[11/22 14:22:22][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.33	
[11/22 14:22:22][INFO] visual_prompt:  165: Training 20 / 100 epoch, with learning rate 0.0004736823324551909
[11/22 14:24:12][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6449,	0.9222 s / batch. (data: 2.77e-04). ETA=11:26:56, max mem: 30.7 GB 
[11/22 14:25:46][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7310,	0.9256 s / batch. (data: 3.16e-04). ETA=11:27:53, max mem: 30.7 GB 
[11/22 14:27:23][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6181,	0.9400 s / batch. (data: 2.82e-04). ETA=11:37:05, max mem: 30.7 GB 
[11/22 14:28:59][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6592,	0.9250 s / batch. (data: 5.59e-03). ETA=11:24:23, max mem: 30.7 GB 
[11/22 14:30:34][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7501,	0.9214 s / batch. (data: 5.42e-03). ETA=11:20:10, max mem: 30.7 GB 
[11/22 14:31:27][INFO] visual_prompt:  217: Epoch 20 / 100: avg data time: 5.40e-02, avg batch time: 0.9847, average train loss: 0.7239
[11/22 14:32:25][INFO] visual_prompt:  316: Inference (val):avg data time: 1.48e-04, avg batch time: 0.3043, average loss: 0.7013
[11/22 14:32:25][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 54.77	
[11/22 14:32:25][INFO] visual_prompt:  165: Training 21 / 100 epoch, with learning rate 0.00046986843780162223
[11/22 14:34:15][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6826,	0.9361 s / batch. (data: 1.04e-03). ETA=11:28:37, max mem: 30.7 GB 
[11/22 14:35:52][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7575,	0.9226 s / batch. (data: 2.42e-04). ETA=11:17:13, max mem: 30.7 GB 
[11/22 14:37:29][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7348,	0.9370 s / batch. (data: 1.85e-03). ETA=11:26:11, max mem: 30.7 GB 
[11/22 14:39:05][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9551,	0.9186 s / batch. (data: 2.99e-04). ETA=11:11:10, max mem: 30.7 GB 
[11/22 14:40:39][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5895,	0.9151 s / batch. (data: 2.76e-04). ETA=11:07:08, max mem: 30.7 GB 
[11/22 14:41:29][INFO] visual_prompt:  217: Epoch 21 / 100: avg data time: 5.17e-02, avg batch time: 0.9825, average train loss: 0.7049
[11/22 14:42:27][INFO] visual_prompt:  316: Inference (val):avg data time: 3.24e-05, avg batch time: 0.3044, average loss: 0.6910
[11/22 14:42:27][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 52.85	rocauc: 53.35	
[11/22 14:42:27][INFO] visual_prompt:  165: Training 22 / 100 epoch, with learning rate 0.0004658141202393935
[11/22 14:44:13][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7590,	0.9520 s / batch. (data: 7.98e-03). ETA=11:31:36, max mem: 30.7 GB 
[11/22 14:45:52][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9307,	0.9488 s / batch. (data: 5.85e-03). ETA=11:27:41, max mem: 30.7 GB 
[11/22 14:47:30][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5014,	0.9169 s / batch. (data: 2.93e-04). ETA=11:03:02, max mem: 30.7 GB 
[11/22 14:49:04][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.8776,	0.9483 s / batch. (data: 2.77e-04). ETA=11:24:08, max mem: 30.7 GB 
[11/22 14:50:39][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8144,	0.9279 s / batch. (data: 4.27e-04). ETA=11:07:53, max mem: 30.7 GB 
[11/22 14:51:31][INFO] visual_prompt:  217: Epoch 22 / 100: avg data time: 5.09e-02, avg batch time: 0.9826, average train loss: 0.7317
[11/22 14:52:29][INFO] visual_prompt:  316: Inference (val):avg data time: 3.41e-05, avg batch time: 0.3050, average loss: 0.7270
[11/22 14:52:29][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.72	
[11/22 14:52:29][INFO] visual_prompt:  165: Training 23 / 100 epoch, with learning rate 0.00046152381310523384
[11/22 14:54:18][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6756,	0.9480 s / batch. (data: 7.84e-04). ETA=11:19:56, max mem: 30.7 GB 
[11/22 14:55:54][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7566,	0.9579 s / batch. (data: 2.67e-04). ETA=11:25:26, max mem: 30.7 GB 
[11/22 14:57:28][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.3510,	0.9679 s / batch. (data: 8.03e-03). ETA=11:30:58, max mem: 30.7 GB 
[11/22 14:59:04][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.8258,	0.9521 s / batch. (data: 2.71e-04). ETA=11:18:07, max mem: 30.7 GB 
[11/22 15:00:39][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.3822,	0.9066 s / batch. (data: 2.57e-04). ETA=10:44:11, max mem: 30.7 GB 
[11/22 15:01:31][INFO] visual_prompt:  217: Epoch 23 / 100: avg data time: 4.93e-02, avg batch time: 0.9802, average train loss: 0.7102
[11/22 15:02:29][INFO] visual_prompt:  316: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3053, average loss: 0.8528
[11/22 15:02:29][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.85	
[11/22 15:02:29][INFO] visual_prompt:  165: Training 24 / 100 epoch, with learning rate 0.000457002207787005
[11/22 15:04:15][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7599,	0.9481 s / batch. (data: 2.57e-04). ETA=11:11:14, max mem: 30.7 GB 
[11/22 15:05:52][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6438,	0.9360 s / batch. (data: 3.97e-03). ETA=11:01:07, max mem: 30.7 GB 
[11/22 15:07:31][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6012,	0.9225 s / batch. (data: 2.76e-04). ETA=10:50:05, max mem: 30.7 GB 
[11/22 15:09:06][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6673,	0.9279 s / batch. (data: 3.96e-03). ETA=10:52:21, max mem: 30.7 GB 
[11/22 15:10:45][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7354,	0.9358 s / batch. (data: 5.92e-03). ETA=10:56:20, max mem: 30.7 GB 
[11/22 15:11:34][INFO] visual_prompt:  217: Epoch 24 / 100: avg data time: 5.42e-02, avg batch time: 0.9844, average train loss: 0.7096
[11/22 15:12:32][INFO] visual_prompt:  316: Inference (val):avg data time: 3.35e-05, avg batch time: 0.3050, average loss: 0.7463
[11/22 15:12:32][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.46	
[11/22 15:12:32][INFO] visual_prompt:  165: Training 25 / 100 epoch, with learning rate 0.0004522542485937369
[11/22 15:14:16][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7269,	0.9280 s / batch. (data: 5.41e-03). ETA=10:48:28, max mem: 30.7 GB 
[11/22 15:15:56][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.4366,	0.9273 s / batch. (data: 5.38e-03). ETA=10:46:26, max mem: 30.7 GB 
[11/22 15:17:32][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6073,	0.9369 s / batch. (data: 2.96e-04). ETA=10:51:33, max mem: 30.7 GB 
[11/22 15:19:09][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7271,	0.9348 s / batch. (data: 7.97e-03). ETA=10:48:33, max mem: 30.7 GB 
[11/22 15:20:45][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6491,	0.9345 s / batch. (data: 2.56e-04). ETA=10:46:45, max mem: 30.7 GB 
[11/22 15:21:37][INFO] visual_prompt:  217: Epoch 25 / 100: avg data time: 5.53e-02, avg batch time: 0.9859, average train loss: 0.7060
[11/22 15:22:36][INFO] visual_prompt:  316: Inference (val):avg data time: 3.31e-05, avg batch time: 0.3029, average loss: 0.7096
[11/22 15:22:36][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 49.59	rocauc: 56.34	
[11/22 15:22:36][INFO] visual_prompt:  165: Training 26 / 100 epoch, with learning rate 0.00044728512734909845
[11/22 15:24:22][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5507,	0.9384 s / batch. (data: 1.60e-02). ETA=10:47:07, max mem: 30.7 GB 
[11/22 15:26:00][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.4685,	0.9405 s / batch. (data: 3.07e-04). ETA=10:47:00, max mem: 30.7 GB 
[11/22 15:27:38][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6631,	0.9232 s / batch. (data: 5.86e-03). ETA=10:33:31, max mem: 30.7 GB 
[11/22 15:29:12][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6480,	0.9334 s / batch. (data: 3.07e-04). ETA=10:39:01, max mem: 30.7 GB 
[11/22 15:30:48][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5298,	0.9409 s / batch. (data: 7.23e-04). ETA=10:42:35, max mem: 30.7 GB 
[11/22 15:31:38][INFO] visual_prompt:  217: Epoch 26 / 100: avg data time: 5.04e-02, avg batch time: 0.9814, average train loss: 0.7069
[11/22 15:32:37][INFO] visual_prompt:  316: Inference (val):avg data time: 3.29e-05, avg batch time: 0.3064, average loss: 0.8508
[11/22 15:32:37][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.36	
[11/22 15:32:37][INFO] visual_prompt:  165: Training 27 / 100 epoch, with learning rate 0.0004421002777142148
[11/22 15:34:23][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6230,	0.9240 s / batch. (data: 2.83e-04). ETA=10:28:38, max mem: 30.7 GB 
[11/22 15:36:00][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6139,	0.9260 s / batch. (data: 5.38e-03). ETA=10:28:26, max mem: 30.7 GB 
[11/22 15:37:34][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7190,	0.9274 s / batch. (data: 6.90e-04). ETA=10:27:54, max mem: 30.7 GB 
[11/22 15:39:10][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9160,	0.9563 s / batch. (data: 5.81e-03). ETA=10:45:50, max mem: 30.7 GB 
[11/22 15:40:49][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8499,	0.9600 s / batch. (data: 2.74e-04). ETA=10:46:47, max mem: 30.7 GB 
[11/22 15:41:39][INFO] visual_prompt:  217: Epoch 27 / 100: avg data time: 4.99e-02, avg batch time: 0.9803, average train loss: 0.7023
[11/22 15:42:37][INFO] visual_prompt:  316: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3054, average loss: 0.7010
[11/22 15:42:37][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 50.00	rocauc: 56.45	
[11/22 15:42:37][INFO] visual_prompt:  165: Training 28 / 100 epoch, with learning rate 0.0004367053692460385
[11/22 15:44:22][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7943,	0.9585 s / batch. (data: 1.45e-02). ETA=10:43:18, max mem: 30.7 GB 
[11/22 15:46:00][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5091,	0.9378 s / batch. (data: 2.87e-04). ETA=10:27:49, max mem: 30.7 GB 
[11/22 15:47:36][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.2747,	0.9485 s / batch. (data: 1.04e-02). ETA=10:33:26, max mem: 30.7 GB 
[11/22 15:49:14][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6864,	1.3239 s / batch. (data: 3.78e-01). ETA=14:41:56, max mem: 30.7 GB 
[11/22 15:50:50][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.0298,	0.9120 s / batch. (data: 2.66e-04). ETA=10:06:00, max mem: 30.7 GB 
[11/22 15:51:39][INFO] visual_prompt:  217: Epoch 28 / 100: avg data time: 4.88e-02, avg batch time: 0.9796, average train loss: 0.7019
[11/22 15:52:38][INFO] visual_prompt:  316: Inference (val):avg data time: 3.21e-05, avg batch time: 0.3028, average loss: 0.7152
[11/22 15:52:38][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.72	
[11/22 15:52:38][INFO] visual_prompt:  165: Training 29 / 100 epoch, with learning rate 0.0004311063011977723
[11/22 15:54:25][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7407,	0.9295 s / batch. (data: 3.35e-04). ETA=10:15:16, max mem: 30.7 GB 
[11/22 15:56:03][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6046,	0.9149 s / batch. (data: 5.37e-03). ETA=10:04:03, max mem: 30.7 GB 
[11/22 15:57:43][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8130,	0.9157 s / batch. (data: 3.75e-04). ETA=10:03:05, max mem: 30.7 GB 
[11/22 15:59:21][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6579,	0.9244 s / batch. (data: 7.92e-04). ETA=10:07:17, max mem: 30.7 GB 
[11/22 16:00:54][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7277,	0.9725 s / batch. (data: 7.22e-04). ETA=10:37:15, max mem: 30.7 GB 
[11/22 16:01:44][INFO] visual_prompt:  217: Epoch 29 / 100: avg data time: 5.90e-02, avg batch time: 0.9876, average train loss: 0.7047
[11/22 16:02:42][INFO] visual_prompt:  316: Inference (val):avg data time: 8.45e-05, avg batch time: 0.3066, average loss: 0.7228
[11/22 16:02:42][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.16	
[11/22 16:02:42][INFO] visual_prompt:   42: Stopping early.

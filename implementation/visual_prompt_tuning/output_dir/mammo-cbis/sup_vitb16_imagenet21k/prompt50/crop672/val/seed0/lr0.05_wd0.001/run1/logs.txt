[10/29 09:40:12][INFO] visual_prompt:   95: Rank of current process: 0. World size: 1
[10/29 09:40:12][INFO] visual_prompt:   97: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/29 09:40:12][INFO] visual_prompt:   99: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/29 09:40:12][INFO] visual_prompt:  101: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/29 09:40:12][INFO] visual_prompt:  108: Training with config:
[10/29 09:40:12][INFO] visual_prompt:  109: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr0.05_wd0.001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.05, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/29 09:40:12][INFO] visual_prompt:   55: Loading training data...
[10/29 09:40:12][INFO] visual_prompt:   28: Constructing mammo-cbis dataset train...
[10/29 09:40:12][INFO] visual_prompt:   57: Loading validation data...
[10/29 09:40:12][INFO] visual_prompt:   28: Constructing mammo-cbis dataset val...
[10/29 09:40:12][INFO] visual_prompt:   38: Constructing models...
[10/29 09:40:14][INFO] visual_prompt:   52: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/29 09:40:14][INFO] visual_prompt:   54: tuned percent:0.529
[10/29 09:40:15][INFO] visual_prompt:   40: Device used for model: 0
[10/29 09:40:15][INFO] visual_prompt:   40: Setting up Evaluator...
[10/29 09:40:15][INFO] visual_prompt:   42: Setting up Trainer...
[10/29 09:40:15][INFO] visual_prompt:   45: 	Setting up the optimizer...
[10/29 09:40:15][INFO] visual_prompt:  165: Training 1 / 100 epoch, with learning rate 0.0
[10/29 09:41:46][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.8535,	0.5121 s / batch. (data: 1.20e-02). ETA=7:51:08, max mem: 11.4 GB 
[10/29 09:43:13][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.4692,	0.4923 s / batch. (data: 2.63e-04). ETA=7:32:03, max mem: 11.4 GB 
[10/29 09:44:44][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.5173,	2.9272 s / batch. (data: 2.45e+00). ETA=1 day, 20:43:14, max mem: 11.4 GB 
[10/29 09:46:11][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.0673,	0.5131 s / batch. (data: 9.05e-03). ETA=7:49:28, max mem: 11.4 GB 
[10/29 09:47:42][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8113,	0.4960 s / batch. (data: 2.56e-04). ETA=7:33:00, max mem: 11.4 GB 
[10/29 09:48:27][INFO] visual_prompt:  217: Epoch 1 / 100: avg data time: 3.96e-01, avg batch time: 0.8909, average train loss: 1.3966
[10/29 09:49:20][INFO] visual_prompt:  316: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1906, average loss: 1.3454
[10/29 09:49:20][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/29 09:49:20][INFO] visual_prompt:  165: Training 2 / 100 epoch, with learning rate 0.005000000000000001
[10/29 09:50:51][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6972,	1.0201 s / batch. (data: 5.34e-01). ETA=15:29:05, max mem: 11.4 GB 
[10/29 09:52:19][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.3721,	1.9400 s / batch. (data: 1.45e+00). ETA=1 day, 5:23:42, max mem: 11.4 GB 
[10/29 09:53:49][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6404,	1.6360 s / batch. (data: 1.12e+00). ETA=1 day, 0:44:34, max mem: 11.4 GB 
[10/29 09:55:16][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7997,	0.5160 s / batch. (data: 2.82e-04). ETA=7:47:22, max mem: 11.4 GB 
[10/29 09:56:46][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6541,	0.4886 s / batch. (data: 2.75e-04). ETA=7:21:45, max mem: 11.4 GB 
[10/29 09:57:31][INFO] visual_prompt:  217: Epoch 2 / 100: avg data time: 3.93e-01, avg batch time: 0.8877, average train loss: 0.7501
[10/29 09:58:23][INFO] visual_prompt:  316: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1908, average loss: 0.7281
[10/29 09:58:23][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.55	
[10/29 09:58:23][INFO] visual_prompt:  165: Training 3 / 100 epoch, with learning rate 0.010000000000000002
[10/29 09:59:54][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7582,	0.4847 s / batch. (data: 5.37e-03). ETA=7:16:59, max mem: 11.4 GB 
[10/29 10:01:24][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7944,	1.6344 s / batch. (data: 1.16e+00). ETA=1 day, 0:30:47, max mem: 11.4 GB 
[10/29 10:02:50][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5395,	0.5159 s / batch. (data: 7.85e-03). ETA=7:43:25, max mem: 11.4 GB 
[10/29 10:04:20][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6360,	0.4960 s / batch. (data: 2.57e-04). ETA=7:24:41, max mem: 11.4 GB 
[10/29 10:05:50][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7208,	1.9033 s / batch. (data: 1.41e+00). ETA=1 day, 4:23:13, max mem: 11.4 GB 
[10/29 10:06:34][INFO] visual_prompt:  217: Epoch 3 / 100: avg data time: 3.92e-01, avg batch time: 0.8871, average train loss: 0.7409
[10/29 10:07:26][INFO] visual_prompt:  316: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1910, average loss: 0.7305
[10/29 10:07:26][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.83	
[10/29 10:07:26][INFO] visual_prompt:  165: Training 4 / 100 epoch, with learning rate 0.015
[10/29 10:09:00][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7351,	0.5080 s / batch. (data: 7.98e-03). ETA=7:33:17, max mem: 11.4 GB 
[10/29 10:10:29][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5611,	0.4788 s / batch. (data: 2.74e-04). ETA=7:06:29, max mem: 11.4 GB 
[10/29 10:11:57][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6160,	1.3426 s / batch. (data: 8.57e-01). ETA=19:53:34, max mem: 11.4 GB 
[10/29 10:13:23][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7723,	1.9451 s / batch. (data: 1.47e+00). ETA=1 day, 4:46:01, max mem: 11.4 GB 
[10/29 10:14:53][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.4610,	3.6800 s / batch. (data: 3.20e+00). ETA=2 days, 6:19:19, max mem: 11.4 GB 
[10/29 10:15:38][INFO] visual_prompt:  217: Epoch 4 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.7425
[10/29 10:16:31][INFO] visual_prompt:  316: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1917, average loss: 0.6851
[10/29 10:16:31][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.20	
[10/29 10:16:31][INFO] visual_prompt:  165: Training 5 / 100 epoch, with learning rate 0.020000000000000004
[10/29 10:18:01][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.4832,	0.4800 s / batch. (data: 3.08e-04). ETA=7:03:51, max mem: 11.4 GB 
[10/29 10:19:31][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6412,	1.9080 s / batch. (data: 1.42e+00). ETA=1 day, 4:01:50, max mem: 11.4 GB 
[10/29 10:21:00][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8802,	0.4792 s / batch. (data: 2.53e-04). ETA=7:01:38, max mem: 11.4 GB 
[10/29 10:22:27][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5523,	0.4926 s / batch. (data: 1.20e-02). ETA=7:12:32, max mem: 11.4 GB 
[10/29 10:23:57][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5851,	0.5120 s / batch. (data: 2.52e-04). ETA=7:28:45, max mem: 11.4 GB 
[10/29 10:24:44][INFO] visual_prompt:  217: Epoch 5 / 100: avg data time: 3.97e-01, avg batch time: 0.8915, average train loss: 0.7433
[10/29 10:25:36][INFO] visual_prompt:  316: Inference (val):avg data time: 3.90e-05, avg batch time: 0.1908, average loss: 0.6887
[10/29 10:25:36][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.85	
[10/29 10:25:36][INFO] visual_prompt:  165: Training 6 / 100 epoch, with learning rate 0.025
[10/29 10:27:10][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5821,	0.5131 s / batch. (data: 5.83e-03). ETA=7:28:26, max mem: 11.4 GB 
[10/29 10:28:39][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7939,	0.5000 s / batch. (data: 1.20e-02). ETA=7:16:05, max mem: 11.4 GB 
[10/29 10:30:06][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5456,	0.4953 s / batch. (data: 2.87e-04). ETA=7:11:10, max mem: 11.4 GB 
[10/29 10:31:40][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6289,	1.2249 s / batch. (data: 7.22e-01). ETA=17:44:20, max mem: 11.4 GB 
[10/29 10:33:08][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7905,	1.4242 s / batch. (data: 9.37e-01). ETA=20:35:06, max mem: 11.4 GB 
[10/29 10:33:53][INFO] visual_prompt:  217: Epoch 6 / 100: avg data time: 4.02e-01, avg batch time: 0.8972, average train loss: 0.7461
[10/29 10:34:46][INFO] visual_prompt:  316: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1902, average loss: 0.6924
[10/29 10:34:46][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.94	
[10/29 10:34:46][INFO] visual_prompt:  165: Training 7 / 100 epoch, with learning rate 0.03
[10/29 10:36:17][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.4529,	0.5080 s / batch. (data: 2.73e-04). ETA=7:19:16, max mem: 11.4 GB 
[10/29 10:37:46][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5302,	1.6760 s / batch. (data: 1.20e+00). ETA=1 day, 0:06:26, max mem: 11.4 GB 
[10/29 10:39:19][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9286,	2.4962 s / batch. (data: 2.02e+00). ETA=1 day, 11:50:06, max mem: 11.4 GB 
[10/29 10:40:48][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6694,	2.3467 s / batch. (data: 1.87e+00). ETA=1 day, 9:37:24, max mem: 11.4 GB 
[10/29 10:42:14][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7590,	0.4923 s / batch. (data: 2.75e-04). ETA=7:02:22, max mem: 11.4 GB 
[10/29 10:42:59][INFO] visual_prompt:  217: Epoch 7 / 100: avg data time: 3.97e-01, avg batch time: 0.8921, average train loss: 0.7393
[10/29 10:43:52][INFO] visual_prompt:  316: Inference (val):avg data time: 1.85e-04, avg batch time: 0.1920, average loss: 0.7732
[10/29 10:43:52][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.29	
[10/29 10:43:52][INFO] visual_prompt:  165: Training 8 / 100 epoch, with learning rate 0.034999999999999996
[10/29 10:45:23][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7081,	0.9673 s / batch. (data: 4.87e-01). ETA=13:47:30, max mem: 11.4 GB 
[10/29 10:46:53][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.1810,	0.4794 s / batch. (data: 2.43e-04). ETA=6:49:17, max mem: 11.4 GB 
[10/29 10:48:23][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7646,	0.5026 s / batch. (data: 5.39e-03). ETA=7:08:16, max mem: 11.4 GB 
[10/29 10:49:53][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7255,	1.4315 s / batch. (data: 9.43e-01). ETA=20:17:29, max mem: 11.4 GB 
[10/29 10:51:23][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.9127,	2.3103 s / batch. (data: 1.83e+00). ETA=1 day, 8:41:01, max mem: 11.4 GB 
[10/29 10:52:08][INFO] visual_prompt:  217: Epoch 8 / 100: avg data time: 4.02e-01, avg batch time: 0.8962, average train loss: 0.7551
[10/29 10:53:01][INFO] visual_prompt:  316: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1892, average loss: 0.6962
[10/29 10:53:01][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.18	
[10/29 10:53:01][INFO] visual_prompt:  165: Training 9 / 100 epoch, with learning rate 0.04000000000000001
[10/29 10:54:33][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.4322,	0.4923 s / batch. (data: 7.96e-03). ETA=6:56:36, max mem: 11.4 GB 
[10/29 10:56:02][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5753,	0.5000 s / batch. (data: 2.76e-04). ETA=7:02:19, max mem: 11.4 GB 
[10/29 10:57:31][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5802,	2.3542 s / batch. (data: 1.86e+00). ETA=1 day, 9:04:26, max mem: 11.4 GB 
[10/29 10:59:01][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6194,	0.5120 s / batch. (data: 7.97e-03). ETA=7:10:43, max mem: 11.4 GB 
[10/29 11:00:31][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5945,	1.4070 s / batch. (data: 9.12e-01). ETA=19:41:16, max mem: 11.4 GB 
[10/29 11:01:15][INFO] visual_prompt:  217: Epoch 9 / 100: avg data time: 3.98e-01, avg batch time: 0.8930, average train loss: 0.7512
[10/29 11:02:08][INFO] visual_prompt:  316: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1910, average loss: 0.7002
[10/29 11:02:08][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.10	
[10/29 11:02:08][INFO] visual_prompt:  165: Training 10 / 100 epoch, with learning rate 0.045000000000000005
[10/29 11:03:42][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6827,	0.5075 s / batch. (data: 5.38e-03). ETA=7:04:48, max mem: 11.4 GB 
[10/29 11:05:10][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6950,	0.4887 s / batch. (data: 2.58e-04). ETA=6:48:16, max mem: 11.4 GB 
[10/29 11:06:38][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7623,	1.6743 s / batch. (data: 1.17e+00). ETA=23:15:53, max mem: 11.4 GB 
[10/29 11:08:04][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.9120,	0.5722 s / batch. (data: 7.55e-02). ETA=7:56:08, max mem: 11.4 GB 
[10/29 11:09:34][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8730,	0.4792 s / batch. (data: 2.73e-04). ETA=6:37:54, max mem: 11.4 GB 
[10/29 11:10:20][INFO] visual_prompt:  217: Epoch 10 / 100: avg data time: 3.94e-01, avg batch time: 0.8894, average train loss: 0.7498
[10/29 11:11:12][INFO] visual_prompt:  316: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1923, average loss: 0.8428
[10/29 11:11:12][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.47	
[10/29 11:11:12][INFO] visual_prompt:  165: Training 11 / 100 epoch, with learning rate 0.05
[10/29 11:12:46][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6822,	0.4973 s / batch. (data: 5.38e-03). ETA=6:51:39, max mem: 11.4 GB 
[10/29 11:14:16][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.0283,	0.5080 s / batch. (data: 2.77e-04). ETA=6:59:41, max mem: 11.4 GB 
[10/29 11:15:43][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.4260,	2.0883 s / batch. (data: 1.59e+00). ETA=1 day, 4:41:49, max mem: 11.4 GB 
[10/29 11:17:11][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7427,	0.4919 s / batch. (data: 7.55e-04). ETA=6:44:44, max mem: 11.4 GB 
[10/29 11:18:39][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6990,	0.4868 s / batch. (data: 8.81e-03). ETA=6:39:45, max mem: 11.4 GB 
[10/29 11:19:24][INFO] visual_prompt:  217: Epoch 11 / 100: avg data time: 3.93e-01, avg batch time: 0.8886, average train loss: 0.7409
[10/29 11:20:16][INFO] visual_prompt:  316: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1904, average loss: 0.7037
[10/29 11:20:16][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.14	
[10/29 11:20:16][INFO] visual_prompt:  165: Training 12 / 100 epoch, with learning rate 0.0499847706754774
[10/29 11:21:49][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.9702,	0.4893 s / batch. (data: 8.30e-03). ETA=6:40:31, max mem: 11.4 GB 
[10/29 11:23:19][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5719,	0.4920 s / batch. (data: 5.41e-03). ETA=6:41:56, max mem: 11.4 GB 
[10/29 11:24:45][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7246,	0.4915 s / batch. (data: 2.72e-04). ETA=6:40:43, max mem: 11.4 GB 
[10/29 11:26:15][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6917,	0.5017 s / batch. (data: 2.73e-04). ETA=6:48:13, max mem: 11.4 GB 
[10/29 11:27:44][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.4247,	0.4831 s / batch. (data: 5.39e-03). ETA=6:32:12, max mem: 11.4 GB 
[10/29 11:28:28][INFO] visual_prompt:  217: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8896, average train loss: 0.7462
[10/29 11:29:21][INFO] visual_prompt:  316: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1912, average loss: 0.8303
[10/29 11:29:21][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.81	
[10/29 11:29:21][INFO] visual_prompt:  165: Training 13 / 100 epoch, with learning rate 0.04993910125649561
[10/29 11:30:54][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5744,	1.2438 s / batch. (data: 7.43e-01). ETA=16:46:43, max mem: 11.4 GB 
[10/29 11:32:20][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7672,	1.3954 s / batch. (data: 8.83e-01). ETA=18:47:06, max mem: 11.4 GB 
[10/29 11:33:50][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5941,	2.1440 s / batch. (data: 1.65e+00). ETA=1 day, 4:48:11, max mem: 11.4 GB 
[10/29 11:35:17][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6977,	0.4955 s / batch. (data: 7.96e-03). ETA=6:38:33, max mem: 11.4 GB 
[10/29 11:36:46][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6939,	0.5125 s / batch. (data: 1.05e-02). ETA=6:51:23, max mem: 11.4 GB 
[10/29 11:37:32][INFO] visual_prompt:  217: Epoch 13 / 100: avg data time: 3.93e-01, avg batch time: 0.8884, average train loss: 0.7419
[10/29 11:38:24][INFO] visual_prompt:  316: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1893, average loss: 0.6922
[10/29 11:38:24][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.61	
[10/29 11:38:24][INFO] visual_prompt:   36: Best epoch 13: best metric: -0.692
[10/29 11:38:24][INFO] visual_prompt:  165: Training 14 / 100 epoch, with learning rate 0.049863047384206834
[10/29 11:39:57][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.8477,	0.5080 s / batch. (data: 7.94e-03). ETA=6:46:29, max mem: 11.4 GB 
[10/29 11:41:27][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6724,	1.8149 s / batch. (data: 1.33e+00). ETA=1 day, 0:09:13, max mem: 11.4 GB 
[10/29 11:42:55][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6762,	1.3359 s / batch. (data: 8.54e-01). ETA=17:44:30, max mem: 11.4 GB 
[10/29 11:44:22][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6416,	0.5215 s / batch. (data: 2.06e-02). ETA=6:54:39, max mem: 11.4 GB 
[10/29 11:45:51][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.9632,	0.4901 s / batch. (data: 2.29e-04). ETA=6:28:53, max mem: 11.4 GB 
[10/29 11:46:36][INFO] visual_prompt:  217: Epoch 14 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.7278
[10/29 11:47:29][INFO] visual_prompt:  316: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1915, average loss: 0.7771
[10/29 11:47:29][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.77	
[10/29 11:47:29][INFO] visual_prompt:  165: Training 15 / 100 epoch, with learning rate 0.04975670171853926
[10/29 11:49:00][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6976,	0.5201 s / batch. (data: 2.65e-04). ETA=6:51:24, max mem: 11.4 GB 
[10/29 11:50:28][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6563,	0.5048 s / batch. (data: 2.81e-04). ETA=6:38:26, max mem: 11.4 GB 
[10/29 11:51:57][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7683,	0.5190 s / batch. (data: 1.04e-02). ETA=6:48:47, max mem: 11.4 GB 
[10/29 11:53:24][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6147,	0.5925 s / batch. (data: 8.09e-02). ETA=7:45:39, max mem: 11.4 GB 
[10/29 11:54:54][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8391,	0.4787 s / batch. (data: 2.87e-04). ETA=6:15:26, max mem: 11.4 GB 
[10/29 11:55:40][INFO] visual_prompt:  217: Epoch 15 / 100: avg data time: 3.93e-01, avg batch time: 0.8891, average train loss: 0.7285
[10/29 11:56:33][INFO] visual_prompt:  316: Inference (val):avg data time: 1.99e-04, avg batch time: 0.1900, average loss: 0.7075
[10/29 11:56:33][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.48	
[10/29 11:56:33][INFO] visual_prompt:  165: Training 16 / 100 epoch, with learning rate 0.049620193825305206
[10/29 11:58:03][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5637,	0.4960 s / batch. (data: 2.62e-04). ETA=6:27:44, max mem: 11.4 GB 
[10/29 11:59:32][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8473,	0.5006 s / batch. (data: 1.60e-02). ETA=6:30:31, max mem: 11.4 GB 
[10/29 12:01:01][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.1255,	0.4785 s / batch. (data: 2.94e-04). ETA=6:12:29, max mem: 11.4 GB 
[10/29 12:02:30][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7138,	0.4996 s / batch. (data: 2.06e-02). ETA=6:28:05, max mem: 11.4 GB 
[10/29 12:03:58][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7026,	1.9160 s / batch. (data: 1.42e+00). ETA=1 day, 0:45:02, max mem: 11.4 GB 
[10/29 12:04:44][INFO] visual_prompt:  217: Epoch 16 / 100: avg data time: 3.93e-01, avg batch time: 0.8881, average train loss: 0.7237
[10/29 12:05:37][INFO] visual_prompt:  316: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1918, average loss: 0.7588
[10/29 12:05:37][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.23	
[10/29 12:05:37][INFO] visual_prompt:  165: Training 17 / 100 epoch, with learning rate 0.049453690018345146
[10/29 12:07:07][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5557,	0.4791 s / batch. (data: 2.65e-04). ETA=6:10:05, max mem: 11.4 GB 
[10/29 12:08:38][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6224,	0.5040 s / batch. (data: 2.65e-04). ETA=6:28:30, max mem: 11.4 GB 
[10/29 12:10:06][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9705,	0.4908 s / batch. (data: 2.58e-04). ETA=6:17:32, max mem: 11.4 GB 
[10/29 12:11:34][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5705,	1.3866 s / batch. (data: 8.99e-01). ETA=17:44:14, max mem: 11.4 GB 
[10/29 12:13:02][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6609,	2.2080 s / batch. (data: 1.71e+00). ETA=1 day, 4:11:02, max mem: 11.4 GB 
[10/29 12:13:48][INFO] visual_prompt:  217: Epoch 17 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.7161
[10/29 12:14:41][INFO] visual_prompt:  316: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1911, average loss: 0.7185
[10/29 12:14:41][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.41	
[10/29 12:14:41][INFO] visual_prompt:  165: Training 18 / 100 epoch, with learning rate 0.04925739315689991
[10/29 12:16:13][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7142,	0.4960 s / batch. (data: 2.57e-04). ETA=6:18:36, max mem: 11.4 GB 
[10/29 12:17:44][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7519,	0.5322 s / batch. (data: 2.61e-02). ETA=6:45:21, max mem: 11.4 GB 
[10/29 12:19:13][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6267,	0.5284 s / batch. (data: 2.04e-02). ETA=6:41:34, max mem: 11.4 GB 
[10/29 12:20:40][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7104,	0.4846 s / batch. (data: 2.73e-04). ETA=6:07:28, max mem: 11.4 GB 
[10/29 12:22:08][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6983,	0.6360 s / batch. (data: 1.41e-01). ETA=8:01:12, max mem: 11.4 GB 
[10/29 12:22:53][INFO] visual_prompt:  217: Epoch 18 / 100: avg data time: 3.94e-01, avg batch time: 0.8890, average train loss: 0.7208
[10/29 12:23:45][INFO] visual_prompt:  316: Inference (val):avg data time: 3.26e-05, avg batch time: 0.1924, average loss: 0.7070
[10/29 12:23:45][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.96	
[10/29 12:23:45][INFO] visual_prompt:  165: Training 19 / 100 epoch, with learning rate 0.04903154239845797
[10/29 12:25:17][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.9295,	0.4948 s / batch. (data: 2.74e-04). ETA=6:13:08, max mem: 11.4 GB 
[10/29 12:26:46][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7734,	0.4976 s / batch. (data: 3.10e-04). ETA=6:14:23, max mem: 11.4 GB 
[10/29 12:28:14][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.1660,	0.5000 s / batch. (data: 2.70e-04). ETA=6:15:22, max mem: 11.4 GB 
[10/29 12:29:45][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5715,	0.5002 s / batch. (data: 7.42e-04). ETA=6:14:43, max mem: 11.4 GB 
[10/29 12:31:09][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7393,	0.4883 s / batch. (data: 2.99e-04). ETA=6:05:00, max mem: 11.4 GB 
[10/29 12:31:55][INFO] visual_prompt:  217: Epoch 19 / 100: avg data time: 3.90e-01, avg batch time: 0.8861, average train loss: 0.7112
[10/29 12:32:48][INFO] visual_prompt:  316: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1920, average loss: 0.6877
[10/29 12:32:48][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.26	
[10/29 12:32:48][INFO] visual_prompt:   36: Best epoch 19: best metric: -0.688
[10/29 12:32:48][INFO] visual_prompt:  165: Training 20 / 100 epoch, with learning rate 0.048776412907378844
[10/29 12:34:18][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7806,	0.4920 s / batch. (data: 7.96e-03). ETA=6:06:30, max mem: 11.4 GB 
[10/29 12:35:48][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6497,	0.4920 s / batch. (data: 2.92e-04). ETA=6:05:39, max mem: 11.4 GB 
[10/29 12:37:17][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6701,	0.4921 s / batch. (data: 2.82e-04). ETA=6:04:54, max mem: 11.4 GB 
[10/29 12:38:45][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5960,	0.5040 s / batch. (data: 2.60e-04). ETA=6:12:53, max mem: 11.4 GB 
[10/29 12:40:13][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7665,	0.4920 s / batch. (data: 2.66e-04). ETA=6:03:13, max mem: 11.4 GB 
[10/29 12:41:00][INFO] visual_prompt:  217: Epoch 20 / 100: avg data time: 3.94e-01, avg batch time: 0.8894, average train loss: 0.7287
[10/29 12:41:52][INFO] visual_prompt:  316: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1915, average loss: 0.7485
[10/29 12:41:52][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.56	
[10/29 12:41:52][INFO] visual_prompt:  165: Training 21 / 100 epoch, with learning rate 0.048492315519647715
[10/29 12:43:26][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6148,	0.6676 s / batch. (data: 1.90e-01). ETA=8:11:09, max mem: 11.4 GB 
[10/29 12:44:53][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6961,	0.5120 s / batch. (data: 7.97e-03). ETA=6:15:49, max mem: 11.4 GB 
[10/29 12:46:23][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8802,	1.4731 s / batch. (data: 9.62e-01). ETA=17:58:48, max mem: 11.4 GB 
[10/29 12:47:49][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6213,	0.5360 s / batch. (data: 4.81e-02). ETA=6:31:37, max mem: 11.4 GB 
[10/29 12:49:19][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7084,	0.5075 s / batch. (data: 3.09e-04). ETA=6:09:59, max mem: 11.4 GB 
[10/29 12:50:03][INFO] visual_prompt:  217: Epoch 21 / 100: avg data time: 3.94e-01, avg batch time: 0.8878, average train loss: 0.7228
[10/29 12:50:56][INFO] visual_prompt:  316: Inference (val):avg data time: 1.29e-04, avg batch time: 0.1904, average loss: 0.7615
[10/29 12:50:56][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.99	
[10/29 12:50:56][INFO] visual_prompt:  165: Training 22 / 100 epoch, with learning rate 0.048179596364169686
[10/29 12:52:27][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7085,	0.4913 s / batch. (data: 2.79e-04). ETA=5:56:53, max mem: 11.4 GB 
[10/29 12:53:56][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6094,	0.5000 s / batch. (data: 2.56e-04). ETA=6:02:23, max mem: 11.4 GB 
[10/29 12:55:22][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5022,	0.5443 s / batch. (data: 4.26e-02). ETA=6:33:36, max mem: 11.4 GB 
[10/29 12:56:51][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7057,	0.4983 s / batch. (data: 2.70e-04). ETA=5:59:31, max mem: 11.4 GB 
[10/29 12:58:20][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6227,	0.5038 s / batch. (data: 1.04e-02). ETA=6:02:35, max mem: 11.4 GB 
[10/29 12:59:07][INFO] visual_prompt:  217: Epoch 22 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 0.7235
[10/29 13:00:00][INFO] visual_prompt:  316: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1909, average loss: 0.7204
[10/29 13:00:00][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.74	
[10/29 13:00:00][INFO] visual_prompt:  165: Training 23 / 100 epoch, with learning rate 0.04783863644106502
[10/29 13:01:33][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6923,	0.4913 s / batch. (data: 2.62e-04). ETA=5:52:22, max mem: 11.4 GB 
[10/29 13:03:02][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6374,	1.4948 s / batch. (data: 1.01e+00). ETA=17:49:37, max mem: 11.4 GB 
[10/29 13:04:32][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7860,	0.4912 s / batch. (data: 7.12e-04). ETA=5:50:40, max mem: 11.4 GB 
[10/29 13:05:59][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5748,	0.5076 s / batch. (data: 5.83e-03). ETA=6:01:31, max mem: 11.4 GB 
[10/29 13:07:24][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.9828,	0.5443 s / batch. (data: 8.23e-03). ETA=6:26:43, max mem: 11.4 GB 
[10/29 13:08:10][INFO] visual_prompt:  217: Epoch 23 / 100: avg data time: 3.93e-01, avg batch time: 0.8873, average train loss: 0.7234
[10/29 13:09:03][INFO] visual_prompt:  316: Inference (val):avg data time: 2.17e-04, avg batch time: 0.1911, average loss: 0.6916
[10/29 13:09:03][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.15	
[10/29 13:09:03][INFO] visual_prompt:  165: Training 24 / 100 epoch, with learning rate 0.047469851157479176
[10/29 13:10:32][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7899,	0.5028 s / batch. (data: 2.67e-04). ETA=5:56:00, max mem: 11.4 GB 
[10/29 13:12:00][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6643,	0.5013 s / batch. (data: 1.55e-02). ETA=5:54:05, max mem: 11.4 GB 
[10/29 13:13:29][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.7070,	1.5999 s / batch. (data: 1.12e+00). ETA=18:47:24, max mem: 11.4 GB 
[10/29 13:14:59][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7029,	0.5040 s / batch. (data: 1.19e-02). ETA=5:54:17, max mem: 11.4 GB 
[10/29 13:16:29][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8630,	0.9554 s / batch. (data: 4.52e-01). ETA=11:10:05, max mem: 11.4 GB 
[10/29 13:17:15][INFO] visual_prompt:  217: Epoch 24 / 100: avg data time: 3.95e-01, avg batch time: 0.8902, average train loss: 0.7279
[10/29 13:18:08][INFO] visual_prompt:  316: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1919, average loss: 0.6936
[10/29 13:18:08][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 61.69	
[10/29 13:18:08][INFO] visual_prompt:  165: Training 25 / 100 epoch, with learning rate 0.047073689821473176
[10/29 13:19:44][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.5967,	0.5162 s / batch. (data: 1.20e-02). ETA=6:00:43, max mem: 11.4 GB 
[10/29 13:21:11][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.7751,	0.4966 s / batch. (data: 1.05e-02). ETA=5:46:12, max mem: 11.4 GB 
[10/29 13:22:39][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6999,	1.1078 s / batch. (data: 6.16e-01). ETA=12:50:26, max mem: 11.4 GB 
[10/29 13:24:08][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7335,	1.6600 s / batch. (data: 1.15e+00). ETA=19:11:41, max mem: 11.4 GB 
[10/29 13:25:37][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.6985,	2.1534 s / batch. (data: 1.65e+00). ETA=1 day, 0:50:26, max mem: 11.4 GB 
[10/29 13:26:22][INFO] visual_prompt:  217: Epoch 25 / 100: avg data time: 3.98e-01, avg batch time: 0.8933, average train loss: 0.7108
[10/29 13:27:15][INFO] visual_prompt:  316: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1907, average loss: 0.6919
[10/29 13:27:15][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.98	
[10/29 13:27:15][INFO] visual_prompt:  165: Training 26 / 100 epoch, with learning rate 0.046650635094610975
[10/29 13:28:46][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7106,	0.5000 s / batch. (data: 7.98e-03). ETA=5:44:47, max mem: 11.4 GB 
[10/29 13:30:15][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6190,	1.6280 s / batch. (data: 1.12e+00). ETA=18:39:55, max mem: 11.4 GB 
[10/29 13:31:46][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.4986,	0.4803 s / batch. (data: 2.70e-04). ETA=5:29:36, max mem: 11.4 GB 
[10/29 13:33:13][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5682,	0.4902 s / batch. (data: 2.58e-04). ETA=5:35:33, max mem: 11.4 GB 
[10/29 13:34:40][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.7560,	0.5042 s / batch. (data: 2.45e-04). ETA=5:44:19, max mem: 11.4 GB 
[10/29 13:35:25][INFO] visual_prompt:  217: Epoch 26 / 100: avg data time: 3.93e-01, avg batch time: 0.8866, average train loss: 0.7106
[10/29 13:36:18][INFO] visual_prompt:  316: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1912, average loss: 0.7463
[10/29 13:36:18][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.01	
[10/29 13:36:18][INFO] visual_prompt:   42: Stopping early.

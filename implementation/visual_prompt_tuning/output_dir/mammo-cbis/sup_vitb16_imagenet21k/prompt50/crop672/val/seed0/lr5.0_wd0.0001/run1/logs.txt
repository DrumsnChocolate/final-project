[10/26 00:51:41][INFO] visual_prompt:   95: Rank of current process: 0. World size: 1
[10/26 00:51:41][INFO] visual_prompt:   97: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 00:51:41][INFO] visual_prompt:   99: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 00:51:41][INFO] visual_prompt:  101: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 00:51:41][INFO] visual_prompt:  108: Training with config:
[10/26 00:51:41][INFO] visual_prompt:  109: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr5.0_wd0.0001/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 5.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 00:51:41][INFO] visual_prompt:   55: Loading training data...
[10/26 00:51:41][INFO] visual_prompt:   28: Constructing mammo-cbis dataset train...
[10/26 00:51:41][INFO] visual_prompt:   57: Loading validation data...
[10/26 00:51:41][INFO] visual_prompt:   28: Constructing mammo-cbis dataset val...
[10/26 00:51:41][INFO] visual_prompt:   38: Constructing models...
[10/26 00:51:48][INFO] visual_prompt:   52: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/26 00:51:48][INFO] visual_prompt:   54: tuned percent:0.529
[10/26 00:51:48][INFO] visual_prompt:   40: Device used for model: 0
[10/26 00:51:48][INFO] visual_prompt:   40: Setting up Evaluator...
[10/26 00:51:48][INFO] visual_prompt:   42: Setting up Trainer...
[10/26 00:51:48][INFO] visual_prompt:   45: 	Setting up the optimizer...
[10/26 00:51:48][INFO] visual_prompt:  165: Training 1 / 100 epoch, with learning rate 0.0
[10/26 00:53:19][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.8535,	0.4911 s / batch. (data: 2.61e-04). ETA=7:31:47, max mem: 11.4 GB 
[10/26 00:54:46][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.4692,	0.5010 s / batch. (data: 5.40e-03). ETA=7:40:04, max mem: 11.4 GB 
[10/26 00:56:18][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.5173,	2.9991 s / batch. (data: 2.50e+00). ETA=1 day, 21:49:10, max mem: 11.4 GB 
[10/26 00:57:44][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.0673,	0.5045 s / batch. (data: 1.05e-02). ETA=7:41:36, max mem: 11.4 GB 
[10/26 00:59:15][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8113,	0.5309 s / batch. (data: 1.09e-02). ETA=8:04:53, max mem: 11.4 GB 
[10/26 01:00:01][INFO] visual_prompt:  217: Epoch 1 / 100: avg data time: 3.96e-01, avg batch time: 0.8919, average train loss: 1.3966
[10/26 01:00:53][INFO] visual_prompt:  316: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1911, average loss: 1.3454
[10/26 01:00:53][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/26 01:00:53][INFO] visual_prompt:  165: Training 2 / 100 epoch, with learning rate 0.5
[10/26 01:02:25][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.7287,	0.4986 s / batch. (data: 2.89e-04). ETA=7:34:09, max mem: 11.4 GB 
[10/26 01:03:53][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.0029,	0.6762 s / batch. (data: 1.90e-01). ETA=10:14:42, max mem: 11.4 GB 
[10/26 01:05:23][INFO] visual_prompt:  204: 	Training 300/553. train loss: 2.8828,	0.9439 s / batch. (data: 4.65e-01). ETA=14:16:33, max mem: 11.4 GB 
[10/26 01:06:51][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6478,	0.5247 s / batch. (data: 2.08e-02). ETA=7:55:17, max mem: 11.4 GB 
[10/26 01:08:20][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.4853,	0.4789 s / batch. (data: 2.78e-04). ETA=7:12:58, max mem: 11.4 GB 
[10/26 01:09:06][INFO] visual_prompt:  217: Epoch 2 / 100: avg data time: 3.96e-01, avg batch time: 0.8902, average train loss: 2.1185
[10/26 01:09:58][INFO] visual_prompt:  316: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1903, average loss: 6.1366
[10/26 01:09:58][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.25	
[10/26 01:09:58][INFO] visual_prompt:  165: Training 3 / 100 epoch, with learning rate 1.0
[10/26 01:11:28][INFO] visual_prompt:  204: 	Training 100/553. train loss: 7.4872,	1.0720 s / batch. (data: 5.62e-01). ETA=16:06:29, max mem: 11.4 GB 
[10/26 01:12:59][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.8288,	0.5004 s / batch. (data: 1.05e-02). ETA=7:30:19, max mem: 11.4 GB 
[10/26 01:14:25][INFO] visual_prompt:  204: 	Training 300/553. train loss: 3.7283,	0.4920 s / batch. (data: 3.36e-04). ETA=7:21:53, max mem: 11.4 GB 
[10/26 01:15:55][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.3106,	0.5010 s / batch. (data: 3.45e-04). ETA=7:29:09, max mem: 11.4 GB 
[10/26 01:17:26][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.4593,	1.8198 s / batch. (data: 1.34e+00). ETA=1 day, 3:08:30, max mem: 11.4 GB 
[10/26 01:18:10][INFO] visual_prompt:  217: Epoch 3 / 100: avg data time: 3.94e-01, avg batch time: 0.8892, average train loss: 2.3632
[10/26 01:19:02][INFO] visual_prompt:  316: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1902, average loss: 3.7433
[10/26 01:19:02][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.82	
[10/26 01:19:02][INFO] visual_prompt:  165: Training 4 / 100 epoch, with learning rate 1.5
[10/26 01:20:36][INFO] visual_prompt:  204: 	Training 100/553. train loss: 2.9718,	0.4914 s / batch. (data: 2.50e-04). ETA=7:18:30, max mem: 11.4 GB 
[10/26 01:22:04][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9883,	0.4920 s / batch. (data: 2.78e-04). ETA=7:18:12, max mem: 11.4 GB 
[10/26 01:23:34][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.2225,	2.0244 s / batch. (data: 1.53e+00). ETA=1 day, 5:59:45, max mem: 11.4 GB 
[10/26 01:24:59][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.7704,	1.9425 s / batch. (data: 1.46e+00). ETA=1 day, 4:43:42, max mem: 11.4 GB 
[10/26 01:26:29][INFO] visual_prompt:  204: 	Training 500/553. train loss: 4.2522,	3.1840 s / batch. (data: 2.69e+00). ETA=1 day, 23:00:00, max mem: 11.4 GB 
[10/26 01:27:15][INFO] visual_prompt:  217: Epoch 4 / 100: avg data time: 3.97e-01, avg batch time: 0.8909, average train loss: 2.4774
[10/26 01:28:07][INFO] visual_prompt:  316: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1904, average loss: 3.7360
[10/26 01:28:07][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.47	
[10/26 01:28:07][INFO] visual_prompt:  165: Training 5 / 100 epoch, with learning rate 2.0
[10/26 01:29:38][INFO] visual_prompt:  204: 	Training 100/553. train loss: 26.8719,	0.4911 s / batch. (data: 2.62e-04). ETA=7:13:44, max mem: 11.4 GB 
[10/26 01:31:07][INFO] visual_prompt:  204: 	Training 200/553. train loss: 2.2505,	1.6680 s / batch. (data: 1.18e+00). ETA=1 day, 0:30:18, max mem: 11.4 GB 
[10/26 01:32:37][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.6237,	0.4840 s / batch. (data: 2.56e-04). ETA=7:05:48, max mem: 11.4 GB 
[10/26 01:34:05][INFO] visual_prompt:  204: 	Training 400/553. train loss: 8.9198,	0.4999 s / batch. (data: 5.45e-03). ETA=7:18:58, max mem: 11.4 GB 
[10/26 01:35:34][INFO] visual_prompt:  204: 	Training 500/553. train loss: 3.7993,	0.5287 s / batch. (data: 2.07e-02). ETA=7:43:25, max mem: 11.4 GB 
[10/26 01:36:20][INFO] visual_prompt:  217: Epoch 5 / 100: avg data time: 3.95e-01, avg batch time: 0.8908, average train loss: 3.4754
[10/26 01:37:13][INFO] visual_prompt:  316: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1913, average loss: 0.7998
[10/26 01:37:13][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.00	
[10/26 01:37:13][INFO] visual_prompt:  165: Training 6 / 100 epoch, with learning rate 2.5
[10/26 01:38:46][INFO] visual_prompt:  204: 	Training 100/553. train loss: 3.3289,	0.5048 s / batch. (data: 2.39e-04). ETA=7:21:11, max mem: 11.4 GB 
[10/26 01:40:14][INFO] visual_prompt:  204: 	Training 200/553. train loss: 16.2253,	0.4999 s / batch. (data: 7.90e-03). ETA=7:16:03, max mem: 11.4 GB 
[10/26 01:41:41][INFO] visual_prompt:  204: 	Training 300/553. train loss: 6.0368,	0.4789 s / batch. (data: 2.79e-04). ETA=6:56:55, max mem: 11.4 GB 
[10/26 01:43:13][INFO] visual_prompt:  204: 	Training 400/553. train loss: 2.8529,	0.5115 s / batch. (data: 1.14e-02). ETA=7:24:25, max mem: 11.4 GB 
[10/26 01:44:41][INFO] visual_prompt:  204: 	Training 500/553. train loss: 10.0016,	1.2900 s / batch. (data: 8.13e-01). ETA=18:38:47, max mem: 11.4 GB 
[10/26 01:45:25][INFO] visual_prompt:  217: Epoch 6 / 100: avg data time: 3.96e-01, avg batch time: 0.8912, average train loss: 5.9108
[10/26 01:46:18][INFO] visual_prompt:  316: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1883, average loss: 8.2367
[10/26 01:46:18][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.86	
[10/26 01:46:18][INFO] visual_prompt:  165: Training 7 / 100 epoch, with learning rate 3.0
[10/26 01:47:49][INFO] visual_prompt:  204: 	Training 100/553. train loss: 9.5317,	0.5120 s / batch. (data: 7.44e-04). ETA=7:22:43, max mem: 11.4 GB 
[10/26 01:49:18][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6301,	0.4957 s / batch. (data: 2.73e-04). ETA=7:07:49, max mem: 11.4 GB 
[10/26 01:50:52][INFO] visual_prompt:  204: 	Training 300/553. train loss: 4.4946,	2.8398 s / batch. (data: 2.36e+00). ETA=1 day, 16:46:06, max mem: 11.4 GB 
[10/26 01:52:20][INFO] visual_prompt:  204: 	Training 400/553. train loss: 6.7144,	2.3154 s / batch. (data: 1.81e+00). ETA=1 day, 9:10:33, max mem: 11.4 GB 
[10/26 01:53:46][INFO] visual_prompt:  204: 	Training 500/553. train loss: 12.4632,	0.4914 s / batch. (data: 2.73e-04). ETA=7:01:36, max mem: 11.4 GB 
[10/26 01:54:30][INFO] visual_prompt:  217: Epoch 7 / 100: avg data time: 3.95e-01, avg batch time: 0.8902, average train loss: 5.8479
[10/26 01:55:23][INFO] visual_prompt:  316: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1918, average loss: 5.4253
[10/26 01:55:23][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.63	
[10/26 01:55:23][INFO] visual_prompt:  165: Training 8 / 100 epoch, with learning rate 3.5
[10/26 01:56:53][INFO] visual_prompt:  204: 	Training 100/553. train loss: 7.1637,	0.5079 s / batch. (data: 7.96e-03). ETA=7:14:31, max mem: 11.4 GB 
[10/26 01:58:23][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9091,	0.5070 s / batch. (data: 7.97e-03). ETA=7:12:50, max mem: 11.4 GB 
[10/26 01:59:51][INFO] visual_prompt:  204: 	Training 300/553. train loss: 6.6727,	0.5028 s / batch. (data: 9.69e-03). ETA=7:08:25, max mem: 11.4 GB 
[10/26 02:01:21][INFO] visual_prompt:  204: 	Training 400/553. train loss: 4.6011,	1.0804 s / batch. (data: 5.79e-01). ETA=15:18:50, max mem: 11.4 GB 
[10/26 02:02:50][INFO] visual_prompt:  204: 	Training 500/553. train loss: 40.1586,	2.0761 s / batch. (data: 1.57e+00). ETA=1 day, 5:22:12, max mem: 11.4 GB 
[10/26 02:03:35][INFO] visual_prompt:  217: Epoch 8 / 100: avg data time: 3.95e-01, avg batch time: 0.8898, average train loss: 10.9680
[10/26 02:04:27][INFO] visual_prompt:  316: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1908, average loss: 8.0847
[10/26 02:04:27][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.37	
[10/26 02:04:27][INFO] visual_prompt:  165: Training 9 / 100 epoch, with learning rate 4.0
[10/26 02:05:59][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.0000,	0.4840 s / batch. (data: 2.37e-04). ETA=6:49:36, max mem: 11.4 GB 
[10/26 02:07:27][INFO] visual_prompt:  204: 	Training 200/553. train loss: 5.3104,	0.4995 s / batch. (data: 2.26e-04). ETA=7:01:51, max mem: 11.4 GB 
[10/26 02:08:56][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.4660,	2.3200 s / batch. (data: 1.83e+00). ETA=1 day, 8:35:35, max mem: 11.4 GB 
[10/26 02:10:25][INFO] visual_prompt:  204: 	Training 400/553. train loss: 14.9535,	0.4951 s / batch. (data: 7.98e-03). ETA=6:56:30, max mem: 11.4 GB 
[10/26 02:11:55][INFO] visual_prompt:  204: 	Training 500/553. train loss: 2.3561,	1.4998 s / batch. (data: 1.02e+00). ETA=20:59:14, max mem: 11.4 GB 
[10/26 02:12:39][INFO] visual_prompt:  217: Epoch 9 / 100: avg data time: 3.95e-01, avg batch time: 0.8893, average train loss: 9.5286
[10/26 02:13:32][INFO] visual_prompt:  316: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1912, average loss: 11.7752
[10/26 02:13:32][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.14	
[10/26 02:13:32][INFO] visual_prompt:  165: Training 10 / 100 epoch, with learning rate 4.5
[10/26 02:15:05][INFO] visual_prompt:  204: 	Training 100/553. train loss: 3.6801,	0.5101 s / batch. (data: 3.25e-04). ETA=7:06:58, max mem: 11.4 GB 
[10/26 02:16:32][INFO] visual_prompt:  204: 	Training 200/553. train loss: 16.6762,	0.4920 s / batch. (data: 2.81e-04). ETA=6:51:00, max mem: 11.4 GB 
[10/26 02:18:02][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.6767,	1.8607 s / batch. (data: 1.36e+00). ETA=1 day, 1:51:19, max mem: 11.4 GB 
[10/26 02:19:28][INFO] visual_prompt:  204: 	Training 400/553. train loss: 4.6693,	1.3973 s / batch. (data: 9.04e-01). ETA=19:22:35, max mem: 11.4 GB 
[10/26 02:20:58][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.6154,	1.4680 s / batch. (data: 9.70e-01). ETA=20:18:58, max mem: 11.4 GB 
[10/26 02:21:43][INFO] visual_prompt:  217: Epoch 10 / 100: avg data time: 3.93e-01, avg batch time: 0.8886, average train loss: 9.3432
[10/26 02:22:35][INFO] visual_prompt:  316: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1895, average loss: 5.4945
[10/26 02:22:35][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.83	
[10/26 02:22:35][INFO] visual_prompt:  165: Training 11 / 100 epoch, with learning rate 5.0
[10/26 02:24:09][INFO] visual_prompt:  204: 	Training 100/553. train loss: 10.0064,	0.5000 s / batch. (data: 1.20e-02). ETA=6:53:54, max mem: 11.4 GB 
[10/26 02:25:39][INFO] visual_prompt:  204: 	Training 200/553. train loss: 18.7238,	0.4805 s / batch. (data: 2.64e-04). ETA=6:36:59, max mem: 11.4 GB 
[10/26 02:27:07][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.0000,	2.1678 s / batch. (data: 1.68e+00). ETA=1 day, 5:47:19, max mem: 11.4 GB 
[10/26 02:28:35][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.6231,	0.5320 s / batch. (data: 7.09e-04). ETA=7:17:45, max mem: 11.4 GB 
[10/26 02:30:02][INFO] visual_prompt:  204: 	Training 500/553. train loss: 9.7026,	0.4960 s / batch. (data: 2.76e-04). ETA=6:47:17, max mem: 11.4 GB 
[10/26 02:30:47][INFO] visual_prompt:  217: Epoch 11 / 100: avg data time: 3.94e-01, avg batch time: 0.8892, average train loss: 10.0758
[10/26 02:31:40][INFO] visual_prompt:  316: Inference (val):avg data time: 3.21e-05, avg batch time: 0.1908, average loss: 1.1789
[10/26 02:31:40][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 39.85	
[10/26 02:31:40][INFO] visual_prompt:  165: Training 12 / 100 epoch, with learning rate 4.998477067547739
[10/26 02:33:13][INFO] visual_prompt:  204: 	Training 100/553. train loss: 7.8945,	0.4880 s / batch. (data: 2.49e-04). ETA=6:39:28, max mem: 11.4 GB 
[10/26 02:34:42][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.6289,	0.4923 s / batch. (data: 2.51e-04). ETA=6:42:10, max mem: 11.4 GB 
[10/26 02:36:10][INFO] visual_prompt:  204: 	Training 300/553. train loss: 10.4888,	0.5105 s / batch. (data: 5.38e-03). ETA=6:56:13, max mem: 11.4 GB 
[10/26 02:37:39][INFO] visual_prompt:  204: 	Training 400/553. train loss: 6.8556,	0.5048 s / batch. (data: 2.77e-04). ETA=6:50:41, max mem: 11.4 GB 
[10/26 02:39:07][INFO] visual_prompt:  204: 	Training 500/553. train loss: 2.2146,	0.4932 s / batch. (data: 2.61e-04). ETA=6:40:25, max mem: 11.4 GB 
[10/26 02:39:51][INFO] visual_prompt:  217: Epoch 12 / 100: avg data time: 3.94e-01, avg batch time: 0.8893, average train loss: 8.6512
[10/26 02:40:44][INFO] visual_prompt:  316: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1906, average loss: 19.4005
[10/26 02:40:44][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.74	
[10/26 02:40:44][INFO] visual_prompt:  165: Training 13 / 100 epoch, with learning rate 4.993910125649561
[10/26 02:42:17][INFO] visual_prompt:  204: 	Training 100/553. train loss: 6.2042,	0.5173 s / batch. (data: 8.92e-03). ETA=6:58:44, max mem: 11.4 GB 
[10/26 02:43:43][INFO] visual_prompt:  204: 	Training 200/553. train loss: 17.2855,	0.4995 s / batch. (data: 2.76e-04). ETA=6:43:28, max mem: 11.4 GB 
[10/26 02:45:12][INFO] visual_prompt:  204: 	Training 300/553. train loss: 3.2223,	0.7153 s / batch. (data: 2.37e-01). ETA=9:36:36, max mem: 11.4 GB 
[10/26 02:46:39][INFO] visual_prompt:  204: 	Training 400/553. train loss: 2.3358,	0.4816 s / batch. (data: 2.73e-04). ETA=6:27:22, max mem: 11.4 GB 
[10/26 02:48:10][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.0418,	0.5247 s / batch. (data: 5.41e-03). ETA=7:01:12, max mem: 11.4 GB 
[10/26 02:48:55][INFO] visual_prompt:  217: Epoch 13 / 100: avg data time: 3.94e-01, avg batch time: 0.8886, average train loss: 7.9913
[10/26 02:49:48][INFO] visual_prompt:  316: Inference (val):avg data time: 1.50e-04, avg batch time: 0.1903, average loss: 1.1091
[10/26 02:49:48][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.19	
[10/26 02:49:48][INFO] visual_prompt:   36: Best epoch 13: best metric: -1.109
[10/26 02:49:48][INFO] visual_prompt:  165: Training 14 / 100 epoch, with learning rate 4.986304738420683
[10/26 02:51:20][INFO] visual_prompt:  204: 	Training 100/553. train loss: 62.5105,	0.4975 s / batch. (data: 2.69e-04). ETA=6:38:03, max mem: 11.4 GB 
[10/26 02:52:49][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.0000,	1.9040 s / batch. (data: 1.41e+00). ETA=1 day, 1:20:21, max mem: 11.4 GB 
[10/26 02:54:18][INFO] visual_prompt:  204: 	Training 300/553. train loss: 10.3488,	1.3736 s / batch. (data: 8.95e-01). ETA=18:14:34, max mem: 11.4 GB 
[10/26 02:55:45][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.5904,	0.4992 s / batch. (data: 5.47e-03). ETA=6:36:56, max mem: 11.4 GB 
[10/26 02:57:14][INFO] visual_prompt:  204: 	Training 500/553. train loss: 62.2967,	0.4840 s / batch. (data: 2.52e-04). ETA=6:24:06, max mem: 11.4 GB 
[10/26 02:57:59][INFO] visual_prompt:  217: Epoch 14 / 100: avg data time: 3.94e-01, avg batch time: 0.8885, average train loss: 12.3698
[10/26 02:58:51][INFO] visual_prompt:  316: Inference (val):avg data time: 3.10e-05, avg batch time: 0.1914, average loss: 17.9445
[10/26 02:58:51][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.09	
[10/26 02:58:51][INFO] visual_prompt:  165: Training 15 / 100 epoch, with learning rate 4.975670171853926
[10/26 03:00:23][INFO] visual_prompt:  204: 	Training 100/553. train loss: 27.4773,	0.4907 s / batch. (data: 2.79e-04). ETA=6:28:06, max mem: 11.4 GB 
[10/26 03:01:51][INFO] visual_prompt:  204: 	Training 200/553. train loss: 67.8974,	0.4800 s / batch. (data: 2.75e-04). ETA=6:18:51, max mem: 11.4 GB 
[10/26 03:03:21][INFO] visual_prompt:  204: 	Training 300/553. train loss: 33.4701,	0.4836 s / batch. (data: 2.52e-04). ETA=6:20:51, max mem: 11.4 GB 
[10/26 03:04:47][INFO] visual_prompt:  204: 	Training 400/553. train loss: 3.2093,	0.4942 s / batch. (data: 2.71e-04). ETA=6:28:26, max mem: 11.4 GB 
[10/26 03:06:17][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.3904,	0.5233 s / batch. (data: 7.27e-03). ETA=6:50:26, max mem: 11.4 GB 
[10/26 03:07:04][INFO] visual_prompt:  217: Epoch 15 / 100: avg data time: 3.97e-01, avg batch time: 0.8907, average train loss: 22.3954
[10/26 03:07:56][INFO] visual_prompt:  316: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1920, average loss: 10.7320
[10/26 03:07:56][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 42.55	
[10/26 03:07:56][INFO] visual_prompt:  165: Training 16 / 100 epoch, with learning rate 4.962019382530521
[10/26 03:09:27][INFO] visual_prompt:  204: 	Training 100/553. train loss: 8.9266,	0.4781 s / batch. (data: 2.75e-04). ETA=6:13:45, max mem: 11.4 GB 
[10/26 03:10:56][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.7082,	0.4778 s / batch. (data: 2.80e-04). ETA=6:12:43, max mem: 11.4 GB 
[10/26 03:12:26][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.9033,	0.4805 s / batch. (data: 1.14e-03). ETA=6:14:01, max mem: 11.4 GB 
[10/26 03:13:54][INFO] visual_prompt:  204: 	Training 400/553. train loss: 31.5535,	0.4780 s / batch. (data: 2.77e-04). ETA=6:11:19, max mem: 11.4 GB 
[10/26 03:15:23][INFO] visual_prompt:  204: 	Training 500/553. train loss: 3.2071,	1.8838 s / batch. (data: 1.39e+00). ETA=1 day, 0:20:06, max mem: 11.4 GB 
[10/26 03:16:08][INFO] visual_prompt:  217: Epoch 16 / 100: avg data time: 3.96e-01, avg batch time: 0.8895, average train loss: 12.7780
[10/26 03:17:01][INFO] visual_prompt:  316: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1906, average loss: 2.8510
[10/26 03:17:01][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.35	
[10/26 03:17:01][INFO] visual_prompt:  165: Training 17 / 100 epoch, with learning rate 4.945369001834514
[10/26 03:18:32][INFO] visual_prompt:  204: 	Training 100/553. train loss: 7.1212,	0.5040 s / batch. (data: 4.54e-04). ETA=6:29:19, max mem: 11.4 GB 
[10/26 03:20:02][INFO] visual_prompt:  204: 	Training 200/553. train loss: 33.1376,	0.5003 s / batch. (data: 8.49e-03). ETA=6:25:41, max mem: 11.4 GB 
[10/26 03:21:30][INFO] visual_prompt:  204: 	Training 300/553. train loss: 5.7990,	0.5010 s / batch. (data: 2.48e-04). ETA=6:25:24, max mem: 11.4 GB 
[10/26 03:22:58][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6266,	1.0560 s / batch. (data: 5.57e-01). ETA=13:30:32, max mem: 11.4 GB 
[10/26 03:24:25][INFO] visual_prompt:  204: 	Training 500/553. train loss: 10.5542,	0.5526 s / batch. (data: 4.74e-02). ETA=7:03:15, max mem: 11.4 GB 
[10/26 03:25:12][INFO] visual_prompt:  217: Epoch 17 / 100: avg data time: 3.94e-01, avg batch time: 0.8883, average train loss: 10.4214
[10/26 03:26:05][INFO] visual_prompt:  316: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1899, average loss: 8.6570
[10/26 03:26:05][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 45.79	
[10/26 03:26:05][INFO] visual_prompt:  165: Training 18 / 100 epoch, with learning rate 4.925739315689991
[10/26 03:27:36][INFO] visual_prompt:  204: 	Training 100/553. train loss: 7.6962,	0.4941 s / batch. (data: 1.04e-02). ETA=6:17:10, max mem: 11.4 GB 
[10/26 03:29:07][INFO] visual_prompt:  204: 	Training 200/553. train loss: 17.7688,	0.5280 s / batch. (data: 1.19e-02). ETA=6:42:07, max mem: 11.4 GB 
[10/26 03:30:36][INFO] visual_prompt:  204: 	Training 300/553. train loss: 10.3138,	0.4850 s / batch. (data: 5.40e-03). ETA=6:08:35, max mem: 11.4 GB 
[10/26 03:32:04][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.8387,	0.4917 s / batch. (data: 2.70e-04). ETA=6:12:51, max mem: 11.4 GB 
[10/26 03:33:32][INFO] visual_prompt:  204: 	Training 500/553. train loss: 2.2974,	0.4966 s / batch. (data: 5.40e-03). ETA=6:15:44, max mem: 11.4 GB 
[10/26 03:34:16][INFO] visual_prompt:  217: Epoch 18 / 100: avg data time: 3.95e-01, avg batch time: 0.8892, average train loss: 13.6318
[10/26 03:35:09][INFO] visual_prompt:  316: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1920, average loss: 3.8757
[10/26 03:35:09][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.35	
[10/26 03:35:09][INFO] visual_prompt:  165: Training 19 / 100 epoch, with learning rate 4.903154239845797
[10/26 03:36:40][INFO] visual_prompt:  204: 	Training 100/553. train loss: 8.3992,	0.5000 s / batch. (data: 2.81e-04). ETA=6:17:03, max mem: 11.4 GB 
[10/26 03:38:10][INFO] visual_prompt:  204: 	Training 200/553. train loss: 8.2049,	0.4965 s / batch. (data: 5.38e-03). ETA=6:13:35, max mem: 11.4 GB 
[10/26 03:39:39][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.1794,	0.4913 s / batch. (data: 2.67e-04). ETA=6:08:51, max mem: 11.4 GB 
[10/26 03:41:09][INFO] visual_prompt:  204: 	Training 400/553. train loss: 2.6432,	0.4880 s / batch. (data: 2.69e-04). ETA=6:05:34, max mem: 11.4 GB 
[10/26 03:42:33][INFO] visual_prompt:  204: 	Training 500/553. train loss: 4.7819,	0.5000 s / batch. (data: 2.79e-04). ETA=6:13:43, max mem: 11.4 GB 
[10/26 03:43:20][INFO] visual_prompt:  217: Epoch 19 / 100: avg data time: 3.95e-01, avg batch time: 0.8883, average train loss: 7.7568
[10/26 03:44:12][INFO] visual_prompt:  316: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1900, average loss: 16.5289
[10/26 03:44:12][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.20	
[10/26 03:44:12][INFO] visual_prompt:  165: Training 20 / 100 epoch, with learning rate 4.877641290737884
[10/26 03:45:42][INFO] visual_prompt:  204: 	Training 100/553. train loss: 5.7588,	0.5325 s / batch. (data: 2.75e-04). ETA=6:36:38, max mem: 11.4 GB 
[10/26 03:47:13][INFO] visual_prompt:  204: 	Training 200/553. train loss: 5.4121,	0.4900 s / batch. (data: 1.20e-02). ETA=6:04:11, max mem: 11.4 GB 
[10/26 03:48:41][INFO] visual_prompt:  204: 	Training 300/553. train loss: 2.7455,	0.5040 s / batch. (data: 7.96e-03). ETA=6:13:45, max mem: 11.4 GB 
[10/26 03:50:10][INFO] visual_prompt:  204: 	Training 400/553. train loss: 32.9596,	0.5243 s / batch. (data: 1.55e-02). ETA=6:27:56, max mem: 11.4 GB 
[10/26 03:51:38][INFO] visual_prompt:  204: 	Training 500/553. train loss: 8.5920,	0.4962 s / batch. (data: 3.12e-04). ETA=6:06:18, max mem: 11.4 GB 
[10/26 03:52:25][INFO] visual_prompt:  217: Epoch 20 / 100: avg data time: 3.97e-01, avg batch time: 0.8908, average train loss: 12.5349
[10/26 03:53:18][INFO] visual_prompt:  316: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1893, average loss: 4.5171
[10/26 03:53:18][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.13	
[10/26 03:53:18][INFO] visual_prompt:   42: Stopping early.

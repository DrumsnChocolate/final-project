[10/26 17:40:10][INFO] visual_prompt:   95: Rank of current process: 0. World size: 1
[10/26 17:40:10][INFO] visual_prompt:   97: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/26 17:40:10][INFO] visual_prompt:   99: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROPSIZE', '672', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '7', 'SOLVER.CRITERION', 'loss'])
[10/26 17:40:10][INFO] visual_prompt:  101: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[10/26 17:40:10][INFO] visual_prompt:  108: Training with config:
[10/26 17:40:10][INFO] visual_prompt:  109: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/crop672/val/seed0/lr2.5_wd0.0/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 7, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 2.5, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 672, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[10/26 17:40:10][INFO] visual_prompt:   55: Loading training data...
[10/26 17:40:10][INFO] visual_prompt:   28: Constructing mammo-cbis dataset train...
[10/26 17:40:10][INFO] visual_prompt:   57: Loading validation data...
[10/26 17:40:10][INFO] visual_prompt:   28: Constructing mammo-cbis dataset val...
[10/26 17:40:10][INFO] visual_prompt:   38: Constructing models...
[10/26 17:40:12][INFO] visual_prompt:   52: Total Parameters: 87465218	 Gradient Parameters: 462338
[10/26 17:40:12][INFO] visual_prompt:   54: tuned percent:0.529
[10/26 17:40:13][INFO] visual_prompt:   40: Device used for model: 0
[10/26 17:40:13][INFO] visual_prompt:   40: Setting up Evaluator...
[10/26 17:40:13][INFO] visual_prompt:   42: Setting up Trainer...
[10/26 17:40:13][INFO] visual_prompt:   45: 	Setting up the optimizer...
[10/26 17:40:13][INFO] visual_prompt:  165: Training 1 / 100 epoch, with learning rate 0.0
[10/26 17:41:44][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.8535,	0.4783 s / batch. (data: 2.46e-04). ETA=7:20:02, max mem: 11.4 GB 
[10/26 17:43:10][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.4692,	0.5007 s / batch. (data: 5.40e-03). ETA=7:39:48, max mem: 11.4 GB 
[10/26 17:44:42][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.5173,	2.9203 s / batch. (data: 2.44e+00). ETA=1 day, 20:36:53, max mem: 11.4 GB 
[10/26 17:46:08][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.0673,	0.5040 s / batch. (data: 3.00e-04). ETA=7:41:07, max mem: 11.4 GB 
[10/26 17:47:39][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8113,	0.4880 s / batch. (data: 2.73e-04). ETA=7:25:42, max mem: 11.4 GB 
[10/26 17:48:25][INFO] visual_prompt:  217: Epoch 1 / 100: avg data time: 3.95e-01, avg batch time: 0.8899, average train loss: 1.3966
[10/26 17:49:17][INFO] visual_prompt:  316: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1899, average loss: 1.3454
[10/26 17:49:17][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.33	
[10/26 17:49:17][INFO] visual_prompt:  165: Training 2 / 100 epoch, with learning rate 0.25
[10/26 17:50:49][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.5965,	0.5360 s / batch. (data: 3.15e-04). ETA=8:08:10, max mem: 11.4 GB 
[10/26 17:52:18][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.6470,	1.2413 s / batch. (data: 7.64e-01). ETA=18:48:30, max mem: 11.4 GB 
[10/26 17:53:48][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.3288,	1.6920 s / batch. (data: 1.19e+00). ETA=1 day, 1:35:24, max mem: 11.4 GB 
[10/26 17:55:15][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.6423,	0.5000 s / batch. (data: 2.88e-04). ETA=7:32:52, max mem: 11.4 GB 
[10/26 17:56:45][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.4763,	0.4942 s / batch. (data: 5.40e-03). ETA=7:26:46, max mem: 11.4 GB 
[10/26 17:57:30][INFO] visual_prompt:  217: Epoch 2 / 100: avg data time: 3.96e-01, avg batch time: 0.8909, average train loss: 1.2790
[10/26 17:58:22][INFO] visual_prompt:  316: Inference (val):avg data time: 3.16e-05, avg batch time: 0.1922, average loss: 2.2703
[10/26 17:58:22][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.46	
[10/26 17:58:22][INFO] visual_prompt:  165: Training 3 / 100 epoch, with learning rate 0.5
[10/26 17:59:53][INFO] visual_prompt:  204: 	Training 100/553. train loss: 3.0428,	1.0000 s / batch. (data: 5.18e-01). ETA=15:01:34, max mem: 11.4 GB 
[10/26 18:01:22][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.0757,	0.5086 s / batch. (data: 8.56e-03). ETA=7:37:40, max mem: 11.4 GB 
[10/26 18:02:50][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.4341,	0.4840 s / batch. (data: 2.80e-04). ETA=7:14:42, max mem: 11.4 GB 
[10/26 18:04:19][INFO] visual_prompt:  204: 	Training 400/553. train loss: 4.7927,	0.5373 s / batch. (data: 2.13e-02). ETA=8:01:44, max mem: 11.4 GB 
[10/26 18:05:49][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.8023,	1.8680 s / batch. (data: 1.38e+00). ETA=1 day, 3:51:40, max mem: 11.4 GB 
[10/26 18:06:33][INFO] visual_prompt:  217: Epoch 3 / 100: avg data time: 3.93e-01, avg batch time: 0.8876, average train loss: 1.6704
[10/26 18:07:26][INFO] visual_prompt:  316: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1930, average loss: 2.1507
[10/26 18:07:26][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.80	
[10/26 18:07:26][INFO] visual_prompt:  165: Training 4 / 100 epoch, with learning rate 0.75
[10/26 18:08:59][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.7607,	0.4971 s / batch. (data: 5.08e-03). ETA=7:23:35, max mem: 11.4 GB 
[10/26 18:10:28][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.3375,	0.5000 s / batch. (data: 7.96e-03). ETA=7:25:20, max mem: 11.4 GB 
[10/26 18:11:56][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.8326,	1.3712 s / batch. (data: 8.94e-01). ETA=20:19:02, max mem: 11.4 GB 
[10/26 18:13:22][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.8771,	2.0552 s / batch. (data: 1.56e+00). ETA=1 day, 6:23:41, max mem: 11.4 GB 
[10/26 18:14:52][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.5641,	3.5521 s / batch. (data: 3.06e+00). ETA=2 days, 4:26:00, max mem: 11.4 GB 
[10/26 18:15:38][INFO] visual_prompt:  217: Epoch 4 / 100: avg data time: 3.97e-01, avg batch time: 0.8912, average train loss: 1.7824
[10/26 18:16:31][INFO] visual_prompt:  316: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1893, average loss: 0.9729
[10/26 18:16:31][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.02	
[10/26 18:16:31][INFO] visual_prompt:  165: Training 5 / 100 epoch, with learning rate 1.0
[10/26 18:18:03][INFO] visual_prompt:  204: 	Training 100/553. train loss: 3.1976,	0.5080 s / batch. (data: 1.34e-02). ETA=7:28:38, max mem: 11.4 GB 
[10/26 18:19:32][INFO] visual_prompt:  204: 	Training 200/553. train loss: 2.6513,	1.8760 s / batch. (data: 1.37e+00). ETA=1 day, 3:33:37, max mem: 11.4 GB 
[10/26 18:21:02][INFO] visual_prompt:  204: 	Training 300/553. train loss: 7.3855,	0.4800 s / batch. (data: 2.71e-04). ETA=7:02:19, max mem: 11.4 GB 
[10/26 18:22:30][INFO] visual_prompt:  204: 	Training 400/553. train loss: 6.3355,	0.5040 s / batch. (data: 2.77e-04). ETA=7:22:34, max mem: 11.4 GB 
[10/26 18:23:59][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.3502,	0.4886 s / batch. (data: 5.36e-03). ETA=7:08:13, max mem: 11.4 GB 
[10/26 18:24:46][INFO] visual_prompt:  217: Epoch 5 / 100: avg data time: 3.99e-01, avg batch time: 0.8937, average train loss: 2.3528
[10/26 18:25:38][INFO] visual_prompt:  316: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1903, average loss: 3.9034
[10/26 18:25:38][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.23	
[10/26 18:25:38][INFO] visual_prompt:  165: Training 6 / 100 epoch, with learning rate 1.25
[10/26 18:27:11][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.8220,	0.5067 s / batch. (data: 2.68e-04). ETA=7:22:46, max mem: 11.4 GB 
[10/26 18:28:39][INFO] visual_prompt:  204: 	Training 200/553. train loss: 6.3834,	0.5028 s / batch. (data: 2.84e-04). ETA=7:18:34, max mem: 11.4 GB 
[10/26 18:30:06][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.6840,	0.5207 s / batch. (data: 4.65e-03). ETA=7:33:16, max mem: 11.4 GB 
[10/26 18:31:38][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.2502,	0.5120 s / batch. (data: 1.19e-02). ETA=7:24:51, max mem: 11.4 GB 
[10/26 18:33:06][INFO] visual_prompt:  204: 	Training 500/553. train loss: 7.5331,	1.3956 s / batch. (data: 9.19e-01). ETA=20:10:20, max mem: 11.4 GB 
[10/26 18:33:50][INFO] visual_prompt:  217: Epoch 6 / 100: avg data time: 3.95e-01, avg batch time: 0.8902, average train loss: 2.2502
[10/26 18:34:42][INFO] visual_prompt:  316: Inference (val):avg data time: 2.91e-04, avg batch time: 0.1908, average loss: 0.8104
[10/26 18:34:42][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.71	
[10/26 18:34:42][INFO] visual_prompt:  165: Training 7 / 100 epoch, with learning rate 1.5
[10/26 18:36:12][INFO] visual_prompt:  204: 	Training 100/553. train loss: 3.3987,	0.4993 s / batch. (data: 2.73e-04). ETA=7:11:46, max mem: 11.4 GB 
[10/26 18:37:42][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.9239,	0.5000 s / batch. (data: 2.52e-04). ETA=7:11:32, max mem: 11.4 GB 
[10/26 18:39:12][INFO] visual_prompt:  204: 	Training 300/553. train loss: 2.4975,	1.2431 s / batch. (data: 7.49e-01). ETA=17:50:47, max mem: 11.4 GB 
[10/26 18:40:40][INFO] visual_prompt:  204: 	Training 400/553. train loss: 2.2425,	0.5480 s / batch. (data: 5.25e-02). ETA=7:51:07, max mem: 11.4 GB 
[10/26 18:42:08][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.6499,	0.4920 s / batch. (data: 2.35e-04). ETA=7:02:10, max mem: 11.4 GB 
[10/26 18:42:52][INFO] visual_prompt:  217: Epoch 7 / 100: avg data time: 3.90e-01, avg batch time: 0.8856, average train loss: 2.1440
[10/26 18:43:45][INFO] visual_prompt:  316: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1901, average loss: 4.0009
[10/26 18:43:45][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.06	
[10/26 18:43:45][INFO] visual_prompt:  165: Training 8 / 100 epoch, with learning rate 1.75
[10/26 18:45:15][INFO] visual_prompt:  204: 	Training 100/553. train loss: 12.6454,	0.4956 s / batch. (data: 2.80e-04). ETA=7:03:57, max mem: 11.4 GB 
[10/26 18:46:45][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.4565,	0.5212 s / batch. (data: 2.86e-04). ETA=7:24:59, max mem: 11.4 GB 
[10/26 18:48:14][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.9225,	0.4960 s / batch. (data: 2.94e-04). ETA=7:02:38, max mem: 11.4 GB 
[10/26 18:49:44][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.5552,	1.5274 s / batch. (data: 1.03e+00). ETA=21:39:00, max mem: 11.4 GB 
[10/26 18:51:13][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.3872,	1.9611 s / batch. (data: 1.48e+00). ETA=1 day, 3:44:38, max mem: 11.4 GB 
[10/26 18:51:57][INFO] visual_prompt:  217: Epoch 8 / 100: avg data time: 3.96e-01, avg batch time: 0.8901, average train loss: 5.9818
[10/26 18:52:49][INFO] visual_prompt:  316: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1898, average loss: 2.3856
[10/26 18:52:49][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.00	
[10/26 18:52:49][INFO] visual_prompt:  165: Training 9 / 100 epoch, with learning rate 2.0
[10/26 18:54:21][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.0000,	0.4960 s / batch. (data: 7.96e-03). ETA=6:59:43, max mem: 11.4 GB 
[10/26 18:55:50][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.5036,	0.5000 s / batch. (data: 2.62e-04). ETA=7:02:18, max mem: 11.4 GB 
[10/26 18:57:18][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.6310,	1.5927 s / batch. (data: 1.10e+00). ETA=22:22:31, max mem: 11.4 GB 
[10/26 18:58:48][INFO] visual_prompt:  204: 	Training 400/553. train loss: 5.4151,	0.4886 s / batch. (data: 7.96e-03). ETA=6:51:02, max mem: 11.4 GB 
[10/26 19:00:17][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.9241,	1.4040 s / batch. (data: 8.96e-01). ETA=19:38:48, max mem: 11.4 GB 
[10/26 19:01:01][INFO] visual_prompt:  217: Epoch 9 / 100: avg data time: 3.95e-01, avg batch time: 0.8883, average train loss: 5.2725
[10/26 19:01:53][INFO] visual_prompt:  316: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1901, average loss: 4.3186
[10/26 19:01:53][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.65	
[10/26 19:01:53][INFO] visual_prompt:  165: Training 10 / 100 epoch, with learning rate 2.25
[10/26 19:03:27][INFO] visual_prompt:  204: 	Training 100/553. train loss: 16.7518,	0.5880 s / batch. (data: 9.87e-02). ETA=8:12:10, max mem: 11.4 GB 
[10/26 19:04:56][INFO] visual_prompt:  204: 	Training 200/553. train loss: 2.6327,	0.5078 s / batch. (data: 5.46e-03). ETA=7:04:14, max mem: 11.4 GB 
[10/26 19:06:25][INFO] visual_prompt:  204: 	Training 300/553. train loss: 3.6062,	2.4309 s / batch. (data: 1.95e+00). ETA=1 day, 9:46:38, max mem: 11.4 GB 
[10/26 19:07:52][INFO] visual_prompt:  204: 	Training 400/553. train loss: 8.7843,	1.3077 s / batch. (data: 8.11e-01). ETA=18:08:02, max mem: 11.4 GB 
[10/26 19:09:23][INFO] visual_prompt:  204: 	Training 500/553. train loss: 2.5871,	1.5953 s / batch. (data: 1.10e+00). ETA=22:04:41, max mem: 11.4 GB 
[10/26 19:10:09][INFO] visual_prompt:  217: Epoch 10 / 100: avg data time: 4.01e-01, avg batch time: 0.8955, average train loss: 6.4923
[10/26 19:11:01][INFO] visual_prompt:  316: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1916, average loss: 2.4429
[10/26 19:11:01][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.76	
[10/26 19:11:01][INFO] visual_prompt:  165: Training 11 / 100 epoch, with learning rate 2.5
[10/26 19:12:35][INFO] visual_prompt:  204: 	Training 100/553. train loss: 20.4373,	0.4988 s / batch. (data: 2.45e-04). ETA=6:52:55, max mem: 11.4 GB 
[10/26 19:14:06][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.6473,	0.5475 s / batch. (data: 1.15e-02). ETA=7:32:18, max mem: 11.4 GB 
[10/26 19:15:34][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.0000,	1.2569 s / batch. (data: 7.45e-01). ETA=17:16:20, max mem: 11.4 GB 
[10/26 19:17:01][INFO] visual_prompt:  204: 	Training 400/553. train loss: 6.8035,	0.5240 s / batch. (data: 7.54e-04). ETA=7:11:10, max mem: 11.4 GB 
[10/26 19:18:29][INFO] visual_prompt:  204: 	Training 500/553. train loss: 4.9223,	0.5032 s / batch. (data: 1.15e-02). ETA=6:53:12, max mem: 11.4 GB 
[10/26 19:19:14][INFO] visual_prompt:  217: Epoch 11 / 100: avg data time: 3.96e-01, avg batch time: 0.8904, average train loss: 7.1219
[10/26 19:20:06][INFO] visual_prompt:  316: Inference (val):avg data time: 1.51e-04, avg batch time: 0.1916, average loss: 1.7640
[10/26 19:20:06][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.85	
[10/26 19:20:06][INFO] visual_prompt:  165: Training 12 / 100 epoch, with learning rate 2.4992385337738696
[10/26 19:21:39][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.7026,	0.5484 s / batch. (data: 4.50e-02). ETA=7:28:56, max mem: 11.4 GB 
[10/26 19:23:09][INFO] visual_prompt:  204: 	Training 200/553. train loss: 2.0880,	0.4784 s / batch. (data: 2.87e-04). ETA=6:30:47, max mem: 11.4 GB 
[10/26 19:24:36][INFO] visual_prompt:  204: 	Training 300/553. train loss: 2.2043,	0.4810 s / batch. (data: 2.61e-04). ETA=6:32:10, max mem: 11.4 GB 
[10/26 19:26:05][INFO] visual_prompt:  204: 	Training 400/553. train loss: 17.1961,	0.5120 s / batch. (data: 2.63e-04). ETA=6:56:33, max mem: 11.4 GB 
[10/26 19:27:34][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.4388,	0.5079 s / batch. (data: 2.68e-04). ETA=6:52:23, max mem: 11.4 GB 
[10/26 19:28:18][INFO] visual_prompt:  217: Epoch 12 / 100: avg data time: 3.95e-01, avg batch time: 0.8888, average train loss: 6.4858
[10/26 19:29:10][INFO] visual_prompt:  316: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1909, average loss: 10.2925
[10/26 19:29:10][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.57	
[10/26 19:29:10][INFO] visual_prompt:  165: Training 13 / 100 epoch, with learning rate 2.4969550628247803
[10/26 19:30:42][INFO] visual_prompt:  204: 	Training 100/553. train loss: 3.0640,	0.4880 s / batch. (data: 5.60e-03). ETA=6:35:00, max mem: 11.4 GB 
[10/26 19:32:08][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.5026,	0.4841 s / batch. (data: 2.69e-04). ETA=6:31:02, max mem: 11.4 GB 
[10/26 19:33:38][INFO] visual_prompt:  204: 	Training 300/553. train loss: 5.1663,	2.1332 s / batch. (data: 1.66e+00). ETA=1 day, 4:39:27, max mem: 11.4 GB 
[10/26 19:35:05][INFO] visual_prompt:  204: 	Training 400/553. train loss: 24.0377,	0.4883 s / batch. (data: 5.41e-03). ETA=6:32:48, max mem: 11.4 GB 
[10/26 19:36:35][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.9895,	0.5295 s / batch. (data: 2.56e-02). ETA=7:05:04, max mem: 11.4 GB 
[10/26 19:37:20][INFO] visual_prompt:  217: Epoch 13 / 100: avg data time: 3.93e-01, avg batch time: 0.8869, average train loss: 5.7301
[10/26 19:38:13][INFO] visual_prompt:  316: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1915, average loss: 1.7001
[10/26 19:38:13][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.17	
[10/26 19:38:13][INFO] visual_prompt:   36: Best epoch 13: best metric: -1.700
[10/26 19:38:13][INFO] visual_prompt:  165: Training 14 / 100 epoch, with learning rate 2.4931523692103417
[10/26 19:39:45][INFO] visual_prompt:  204: 	Training 100/553. train loss: 8.3392,	0.4913 s / batch. (data: 2.53e-04). ETA=6:33:08, max mem: 11.4 GB 
[10/26 19:41:14][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.0000,	1.8956 s / batch. (data: 1.41e+00). ETA=1 day, 1:13:42, max mem: 11.4 GB 
[10/26 19:42:42][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.3481,	1.1600 s / batch. (data: 6.68e-01). ETA=15:24:19, max mem: 11.4 GB 
[10/26 19:44:10][INFO] visual_prompt:  204: 	Training 400/553. train loss: 3.3540,	0.7400 s / batch. (data: 2.36e-01). ETA=9:48:25, max mem: 11.4 GB 
[10/26 19:45:39][INFO] visual_prompt:  204: 	Training 500/553. train loss: 6.3310,	0.5097 s / batch. (data: 2.56e-04). ETA=6:44:26, max mem: 11.4 GB 
[10/26 19:46:23][INFO] visual_prompt:  217: Epoch 14 / 100: avg data time: 3.91e-01, avg batch time: 0.8858, average train loss: 5.7099
[10/26 19:47:15][INFO] visual_prompt:  316: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1904, average loss: 9.5580
[10/26 19:47:15][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.35	
[10/26 19:47:15][INFO] visual_prompt:  165: Training 15 / 100 epoch, with learning rate 2.487835085926963
[10/26 19:48:47][INFO] visual_prompt:  204: 	Training 100/553. train loss: 4.9510,	0.5051 s / batch. (data: 1.55e-02). ETA=6:39:28, max mem: 11.4 GB 
[10/26 19:50:14][INFO] visual_prompt:  204: 	Training 200/553. train loss: 7.3209,	0.4840 s / batch. (data: 2.56e-04). ETA=6:22:02, max mem: 11.4 GB 
[10/26 19:51:44][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.4087,	0.5012 s / batch. (data: 2.97e-04). ETA=6:34:45, max mem: 11.4 GB 
[10/26 19:53:11][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.4336,	0.4883 s / batch. (data: 6.06e-03). ETA=6:23:46, max mem: 11.4 GB 
[10/26 19:54:40][INFO] visual_prompt:  204: 	Training 500/553. train loss: 3.8035,	0.5004 s / batch. (data: 2.60e-04). ETA=6:32:29, max mem: 11.4 GB 
[10/26 19:55:27][INFO] visual_prompt:  217: Epoch 15 / 100: avg data time: 3.94e-01, avg batch time: 0.8888, average train loss: 6.6277
[10/26 19:56:19][INFO] visual_prompt:  316: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1911, average loss: 3.6001
[10/26 19:56:19][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.04	
[10/26 19:56:19][INFO] visual_prompt:  165: Training 16 / 100 epoch, with learning rate 2.4810096912652604
[10/26 19:57:50][INFO] visual_prompt:  204: 	Training 100/553. train loss: 7.6596,	0.4999 s / batch. (data: 2.51e-04). ETA=6:30:46, max mem: 11.4 GB 
[10/26 19:59:19][INFO] visual_prompt:  204: 	Training 200/553. train loss: 2.2163,	0.4960 s / batch. (data: 2.58e-04). ETA=6:26:55, max mem: 11.4 GB 
[10/26 20:00:48][INFO] visual_prompt:  204: 	Training 300/553. train loss: 6.7848,	0.5040 s / batch. (data: 2.49e-04). ETA=6:32:20, max mem: 11.4 GB 
[10/26 20:02:16][INFO] visual_prompt:  204: 	Training 400/553. train loss: 11.9919,	0.5120 s / batch. (data: 6.47e-04). ETA=6:37:43, max mem: 11.4 GB 
[10/26 20:03:44][INFO] visual_prompt:  204: 	Training 500/553. train loss: 3.5479,	2.0837 s / batch. (data: 1.60e+00). ETA=1 day, 2:55:02, max mem: 11.4 GB 
[10/26 20:04:30][INFO] visual_prompt:  217: Epoch 16 / 100: avg data time: 3.92e-01, avg batch time: 0.8869, average train loss: 5.9262
[10/26 20:05:22][INFO] visual_prompt:  316: Inference (val):avg data time: 1.49e-04, avg batch time: 0.1914, average loss: 1.0772
[10/26 20:05:22][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 47.49	
[10/26 20:05:22][INFO] visual_prompt:   36: Best epoch 16: best metric: -1.077
[10/26 20:05:22][INFO] visual_prompt:  165: Training 17 / 100 epoch, with learning rate 2.472684500917257
[10/26 20:06:52][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.0424,	0.5122 s / batch. (data: 1.63e-02). ETA=6:35:39, max mem: 11.4 GB 
[10/26 20:08:23][INFO] visual_prompt:  204: 	Training 200/553. train loss: 11.0443,	0.4927 s / batch. (data: 5.39e-03). ETA=6:19:48, max mem: 11.4 GB 
[10/26 20:09:51][INFO] visual_prompt:  204: 	Training 300/553. train loss: 24.4907,	0.5042 s / batch. (data: 1.04e-02). ETA=6:27:48, max mem: 11.4 GB 
[10/26 20:11:18][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.1508,	0.4880 s / batch. (data: 7.96e-03). ETA=6:14:32, max mem: 11.4 GB 
[10/26 20:12:47][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.4800,	2.1843 s / batch. (data: 1.70e+00). ETA=1 day, 3:52:53, max mem: 11.4 GB 
[10/26 20:13:33][INFO] visual_prompt:  217: Epoch 17 / 100: avg data time: 3.93e-01, avg batch time: 0.8879, average train loss: 5.2000
[10/26 20:14:25][INFO] visual_prompt:  316: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1928, average loss: 2.2280
[10/26 20:14:25][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.08	
[10/26 20:14:25][INFO] visual_prompt:  165: Training 18 / 100 epoch, with learning rate 2.4628696578449953
[10/26 20:15:58][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.9857,	0.5204 s / batch. (data: 9.30e-03). ETA=6:37:14, max mem: 11.4 GB 
[10/26 20:17:28][INFO] visual_prompt:  204: 	Training 200/553. train loss: 9.3762,	0.4913 s / batch. (data: 2.57e-04). ETA=6:14:12, max mem: 11.4 GB 
[10/26 20:18:57][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.5380,	0.5000 s / batch. (data: 2.67e-04). ETA=6:19:59, max mem: 11.4 GB 
[10/26 20:20:25][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.8163,	0.5123 s / batch. (data: 1.04e-02). ETA=6:28:31, max mem: 11.4 GB 
[10/26 20:21:53][INFO] visual_prompt:  204: 	Training 500/553. train loss: 4.5498,	0.5160 s / batch. (data: 2.72e-04). ETA=6:30:24, max mem: 11.4 GB 
[10/26 20:22:37][INFO] visual_prompt:  217: Epoch 18 / 100: avg data time: 3.95e-01, avg batch time: 0.8884, average train loss: 5.1056
[10/26 20:23:29][INFO] visual_prompt:  316: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1909, average loss: 1.0217
[10/26 20:23:29][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.49	
[10/26 20:23:29][INFO] visual_prompt:   36: Best epoch 18: best metric: -1.022
[10/26 20:23:29][INFO] visual_prompt:  165: Training 19 / 100 epoch, with learning rate 2.4515771199228986
[10/26 20:25:01][INFO] visual_prompt:  204: 	Training 100/553. train loss: 6.6666,	0.5000 s / batch. (data: 2.83e-04). ETA=6:17:02, max mem: 11.4 GB 
[10/26 20:26:29][INFO] visual_prompt:  204: 	Training 200/553. train loss: 5.4647,	0.4840 s / batch. (data: 2.67e-04). ETA=6:04:10, max mem: 11.4 GB 
[10/26 20:27:58][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.0010,	1.1552 s / batch. (data: 6.63e-01). ETA=14:27:16, max mem: 11.4 GB 
[10/26 20:29:28][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.9409,	0.4997 s / batch. (data: 2.67e-04). ETA=6:14:17, max mem: 11.4 GB 
[10/26 20:30:52][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.7718,	0.5160 s / batch. (data: 7.97e-03). ETA=6:25:40, max mem: 11.4 GB 
[10/26 20:31:39][INFO] visual_prompt:  217: Epoch 19 / 100: avg data time: 3.91e-01, avg batch time: 0.8852, average train loss: 5.1869
[10/26 20:32:31][INFO] visual_prompt:  316: Inference (val):avg data time: 1.47e-04, avg batch time: 0.1905, average loss: 18.8231
[10/26 20:32:31][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.79	
[10/26 20:32:31][INFO] visual_prompt:  165: Training 20 / 100 epoch, with learning rate 2.438820645368942
[10/26 20:34:02][INFO] visual_prompt:  204: 	Training 100/553. train loss: 1.1407,	0.4885 s / batch. (data: 2.55e-04). ETA=6:03:50, max mem: 11.4 GB 
[10/26 20:35:32][INFO] visual_prompt:  204: 	Training 200/553. train loss: 2.1479,	0.5080 s / batch. (data: 7.95e-03). ETA=6:17:32, max mem: 11.4 GB 
[10/26 20:37:01][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.0877,	0.5120 s / batch. (data: 6.88e-04). ETA=6:19:40, max mem: 11.4 GB 
[10/26 20:38:30][INFO] visual_prompt:  204: 	Training 400/553. train loss: 12.4037,	0.4786 s / batch. (data: 3.04e-04). ETA=5:54:06, max mem: 11.4 GB 
[10/26 20:39:58][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.7724,	0.4927 s / batch. (data: 1.04e-02). ETA=6:03:44, max mem: 11.4 GB 
[10/26 20:40:45][INFO] visual_prompt:  217: Epoch 20 / 100: avg data time: 4.00e-01, avg batch time: 0.8936, average train loss: 6.2631
[10/26 20:41:38][INFO] visual_prompt:  316: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1913, average loss: 0.7667
[10/26 20:41:38][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.73	
[10/26 20:41:38][INFO] visual_prompt:   36: Best epoch 20: best metric: -0.767
[10/26 20:41:38][INFO] visual_prompt:  165: Training 21 / 100 epoch, with learning rate 2.4246157759823856
[10/26 20:43:12][INFO] visual_prompt:  204: 	Training 100/553. train loss: 3.1162,	0.5080 s / batch. (data: 2.59e-04). ETA=6:13:44, max mem: 11.4 GB 
[10/26 20:44:41][INFO] visual_prompt:  204: 	Training 200/553. train loss: 15.3350,	0.5000 s / batch. (data: 2.13e-04). ETA=6:07:00, max mem: 11.4 GB 
[10/26 20:46:10][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.8649,	1.6503 s / batch. (data: 1.16e+00). ETA=20:08:32, max mem: 11.4 GB 
[10/26 20:47:37][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.2048,	0.4999 s / batch. (data: 4.47e-04). ETA=6:05:17, max mem: 11.4 GB 
[10/26 20:49:08][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.1634,	0.5040 s / batch. (data: 1.02e-02). ETA=6:07:26, max mem: 11.4 GB 
[10/26 20:49:52][INFO] visual_prompt:  217: Epoch 21 / 100: avg data time: 3.99e-01, avg batch time: 0.8937, average train loss: 5.6657
[10/26 20:50:45][INFO] visual_prompt:  316: Inference (val):avg data time: 1.47e-04, avg batch time: 0.1903, average loss: 1.5301
[10/26 20:50:45][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.43	
[10/26 20:50:45][INFO] visual_prompt:  165: Training 22 / 100 epoch, with learning rate 2.4089798182084845
[10/26 20:52:16][INFO] visual_prompt:  204: 	Training 100/553. train loss: 2.2411,	0.4960 s / batch. (data: 2.69e-04). ETA=6:00:17, max mem: 11.4 GB 
[10/26 20:53:46][INFO] visual_prompt:  204: 	Training 200/553. train loss: 3.9229,	0.5034 s / batch. (data: 2.49e-04). ETA=6:04:51, max mem: 11.4 GB 
[10/26 20:55:12][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.0010,	0.4879 s / batch. (data: 3.62e-04). ETA=5:52:49, max mem: 11.4 GB 
[10/26 20:56:42][INFO] visual_prompt:  204: 	Training 400/553. train loss: 5.8634,	0.4908 s / batch. (data: 2.59e-04). ETA=5:54:06, max mem: 11.4 GB 
[10/26 20:58:11][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.9266,	0.5004 s / batch. (data: 7.99e-03). ETA=6:00:10, max mem: 11.4 GB 
[10/26 20:58:58][INFO] visual_prompt:  217: Epoch 22 / 100: avg data time: 3.98e-01, avg batch time: 0.8920, average train loss: 4.4686
[10/26 20:59:51][INFO] visual_prompt:  316: Inference (val):avg data time: 2.14e-04, avg batch time: 0.1918, average loss: 3.2706
[10/26 20:59:51][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.10	
[10/26 20:59:51][INFO] visual_prompt:  165: Training 23 / 100 epoch, with learning rate 2.391931822053251
[10/26 21:01:24][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.6164,	0.5476 s / batch. (data: 4.35e-02). ETA=6:32:43, max mem: 11.4 GB 
[10/26 21:02:55][INFO] visual_prompt:  204: 	Training 200/553. train loss: 17.1310,	1.5605 s / batch. (data: 1.07e+00). ETA=18:36:37, max mem: 11.4 GB 
[10/26 21:04:25][INFO] visual_prompt:  204: 	Training 300/553. train loss: 3.6070,	0.5241 s / batch. (data: 1.68e-02). ETA=6:14:09, max mem: 11.4 GB 
[10/26 21:05:52][INFO] visual_prompt:  204: 	Training 400/553. train loss: 3.7909,	0.5001 s / batch. (data: 7.46e-04). ETA=5:56:10, max mem: 11.4 GB 
[10/26 21:07:19][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.5381,	0.5280 s / batch. (data: 2.65e-04). ETA=6:15:11, max mem: 11.4 GB 
[10/26 21:08:05][INFO] visual_prompt:  217: Epoch 23 / 100: avg data time: 3.99e-01, avg batch time: 0.8927, average train loss: 4.7311
[10/26 21:08:57][INFO] visual_prompt:  316: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1920, average loss: 4.4873
[10/26 21:08:57][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.31	
[10/26 21:08:57][INFO] visual_prompt:  165: Training 24 / 100 epoch, with learning rate 2.3734925578739587
[10/26 21:10:27][INFO] visual_prompt:  204: 	Training 100/553. train loss: 14.3210,	0.4800 s / batch. (data: 2.72e-04). ETA=5:39:51, max mem: 11.4 GB 
[10/26 21:11:56][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.2510,	0.5039 s / batch. (data: 1.04e-03). ETA=5:55:57, max mem: 11.4 GB 
[10/26 21:13:26][INFO] visual_prompt:  204: 	Training 300/553. train loss: 2.9182,	1.4779 s / batch. (data: 9.90e-01). ETA=17:21:26, max mem: 11.4 GB 
[10/26 21:14:54][INFO] visual_prompt:  204: 	Training 400/553. train loss: 2.5248,	0.4881 s / batch. (data: 2.78e-04). ETA=5:43:10, max mem: 11.4 GB 
[10/26 21:16:25][INFO] visual_prompt:  204: 	Training 500/553. train loss: 6.5960,	0.7037 s / batch. (data: 2.06e-01). ETA=8:13:32, max mem: 11.4 GB 
[10/26 21:17:11][INFO] visual_prompt:  217: Epoch 24 / 100: avg data time: 3.98e-01, avg batch time: 0.8931, average train loss: 5.3354
[10/26 21:18:04][INFO] visual_prompt:  316: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1906, average loss: 1.0632
[10/26 21:18:04][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 53.20	
[10/26 21:18:04][INFO] visual_prompt:  165: Training 25 / 100 epoch, with learning rate 2.3536844910736585
[10/26 21:19:39][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.0266,	0.4915 s / batch. (data: 2.57e-04). ETA=5:43:29, max mem: 11.4 GB 
[10/26 21:21:05][INFO] visual_prompt:  204: 	Training 200/553. train loss: 3.6534,	0.4960 s / batch. (data: 2.88e-04). ETA=5:45:45, max mem: 11.4 GB 
[10/26 21:22:33][INFO] visual_prompt:  204: 	Training 300/553. train loss: 5.9171,	0.4990 s / batch. (data: 2.54e-04). ETA=5:47:00, max mem: 11.4 GB 
[10/26 21:24:04][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.5744,	1.7711 s / batch. (data: 1.28e+00). ETA=20:28:49, max mem: 11.4 GB 
[10/26 21:25:33][INFO] visual_prompt:  204: 	Training 500/553. train loss: 4.6170,	2.0074 s / batch. (data: 1.52e+00). ETA=23:09:24, max mem: 11.4 GB 
[10/26 21:26:18][INFO] visual_prompt:  217: Epoch 25 / 100: avg data time: 3.98e-01, avg batch time: 0.8927, average train loss: 4.2081
[10/26 21:27:10][INFO] visual_prompt:  316: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1889, average loss: 9.0301
[10/26 21:27:10][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.37	
[10/26 21:27:10][INFO] visual_prompt:  165: Training 26 / 100 epoch, with learning rate 2.3325317547305486
[10/26 21:28:42][INFO] visual_prompt:  204: 	Training 100/553. train loss: 3.9070,	0.5404 s / batch. (data: 2.43e-02). ETA=6:12:37, max mem: 11.4 GB 
[10/26 21:30:13][INFO] visual_prompt:  204: 	Training 200/553. train loss: 20.9396,	2.2040 s / batch. (data: 1.71e+00). ETA=1 day, 1:16:10, max mem: 11.4 GB 
[10/26 21:31:42][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.0004,	0.5069 s / batch. (data: 7.31e-04). ETA=5:47:52, max mem: 11.4 GB 
[10/26 21:33:09][INFO] visual_prompt:  204: 	Training 400/553. train loss: 1.7500,	0.4928 s / batch. (data: 2.73e-04). ETA=5:37:23, max mem: 11.4 GB 
[10/26 21:34:36][INFO] visual_prompt:  204: 	Training 500/553. train loss: 6.7239,	0.4787 s / batch. (data: 2.53e-04). ETA=5:26:53, max mem: 11.4 GB 
[10/26 21:35:22][INFO] visual_prompt:  217: Epoch 26 / 100: avg data time: 3.93e-01, avg batch time: 0.8880, average train loss: 5.1324
[10/26 21:36:14][INFO] visual_prompt:  316: Inference (val):avg data time: 2.52e-04, avg batch time: 0.1902, average loss: 2.7773
[10/26 21:36:14][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.50	
[10/26 21:36:14][INFO] visual_prompt:  165: Training 27 / 100 epoch, with learning rate 2.310060120195532
[10/26 21:37:46][INFO] visual_prompt:  204: 	Training 100/553. train loss: 2.2443,	0.5093 s / batch. (data: 1.67e-02). ETA=5:46:31, max mem: 11.4 GB 
[10/26 21:39:15][INFO] visual_prompt:  204: 	Training 200/553. train loss: 10.8247,	2.2545 s / batch. (data: 1.78e+00). ETA=1 day, 1:30:07, max mem: 11.4 GB 
[10/26 21:40:43][INFO] visual_prompt:  204: 	Training 300/553. train loss: 4.8428,	1.1483 s / batch. (data: 6.53e-01). ETA=12:57:24, max mem: 11.4 GB 
[10/26 21:42:12][INFO] visual_prompt:  204: 	Training 400/553. train loss: 6.2609,	0.5160 s / batch. (data: 7.07e-04). ETA=5:48:30, max mem: 11.4 GB 
[10/26 21:43:42][INFO] visual_prompt:  204: 	Training 500/553. train loss: 2.3257,	0.5004 s / batch. (data: 7.46e-04). ETA=5:37:07, max mem: 11.4 GB 
[10/26 21:44:26][INFO] visual_prompt:  217: Epoch 27 / 100: avg data time: 3.95e-01, avg batch time: 0.8897, average train loss: 4.9289
[10/26 21:45:19][INFO] visual_prompt:  316: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1904, average loss: 3.4641
[10/26 21:45:19][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.38	
[10/26 21:45:19][INFO] visual_prompt:   42: Stopping early.

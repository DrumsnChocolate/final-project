[11/23 13:13:32][INFO] visual_prompt:   95: Rank of current process: 0. World size: 1
[11/23 13:13:32][INFO] visual_prompt:   97: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/23 13:13:32][INFO] visual_prompt:   99: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '4', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '800', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '14', 'SOLVER.CRITERION', 'loss'])
[11/23 13:13:32][INFO] visual_prompt:  101: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/23 13:13:32][INFO] visual_prompt:  108: Training with config:
[11/23 13:13:32][INFO] visual_prompt:  109: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size800/val/seed0/lr25.0_wd0.001/patience14/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 14, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 25.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 800, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 4, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/23 13:13:32][INFO] visual_prompt:   55: Loading training data...
[11/23 13:13:32][INFO] visual_prompt:   28: Constructing mammo-cbis dataset train...
[11/23 13:13:32][INFO] visual_prompt:   57: Loading validation data...
[11/23 13:13:32][INFO] visual_prompt:   28: Constructing mammo-cbis dataset val...
[11/23 13:13:32][INFO] visual_prompt:   38: Constructing models...
[11/23 13:13:35][INFO] visual_prompt:   52: Total Parameters: 88030466	 Gradient Parameters: 462338
[11/23 13:13:35][INFO] visual_prompt:   54: tuned percent:0.525
[11/23 13:13:35][INFO] visual_prompt:   40: Device used for model: 0
[11/23 13:13:35][INFO] visual_prompt:   40: Setting up Evaluator...
[11/23 13:13:35][INFO] visual_prompt:   42: Setting up Trainer...
[11/23 13:13:35][INFO] visual_prompt:   45: 	Setting up the optimizer...
[11/23 13:13:35][INFO] visual_prompt:  165: Training 1 / 100 epoch, with learning rate 0.0
[11/23 13:15:18][INFO] visual_prompt:  204: 	Training 100/553. train loss: 2.1087,	0.8320 s / batch. (data: 1.05e-02). ETA=12:45:25, max mem: 20.9 GB 
[11/23 13:16:57][INFO] visual_prompt:  204: 	Training 200/553. train loss: 1.3578,	0.8080 s / batch. (data: 2.97e-04). ETA=12:22:01, max mem: 20.9 GB 
[11/23 13:18:39][INFO] visual_prompt:  204: 	Training 300/553. train loss: 1.3905,	1.6480 s / batch. (data: 8.24e-01). ETA=1 day, 1:10:39, max mem: 20.9 GB 
[11/23 13:20:17][INFO] visual_prompt:  204: 	Training 400/553. train loss: 0.0383,	0.8219 s / batch. (data: 5.42e-03). ETA=12:32:01, max mem: 20.9 GB 
[11/23 13:22:00][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.9538,	0.8170 s / batch. (data: 7.62e-04). ETA=12:26:12, max mem: 20.9 GB 
[11/23 13:22:53][INFO] visual_prompt:  217: Epoch 1 / 100: avg data time: 1.87e-01, avg batch time: 1.0088, average train loss: 1.5403
[11/23 13:23:50][INFO] visual_prompt:  316: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3070, average loss: 1.5201
[11/23 13:23:50][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.08	
[11/23 13:23:50][INFO] visual_prompt:  165: Training 2 / 100 epoch, with learning rate 2.5
[11/23 13:25:34][INFO] visual_prompt:  204: 	Training 100/553. train loss: 11.4707,	0.8160 s / batch. (data: 3.23e-04). ETA=12:23:12, max mem: 20.9 GB 
[11/23 13:27:14][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.0000,	0.8217 s / batch. (data: 3.16e-04). ETA=12:26:58, max mem: 20.9 GB 
[11/23 13:28:56][INFO] visual_prompt:  204: 	Training 300/553. train loss: 5.9929,	1.0410 s / batch. (data: 2.28e-01). ETA=15:44:38, max mem: 20.9 GB 
[11/23 13:30:35][INFO] visual_prompt:  204: 	Training 400/553. train loss: 7.5676,	0.8280 s / batch. (data: 2.99e-04). ETA=12:29:59, max mem: 20.9 GB 
[11/23 13:32:17][INFO] visual_prompt:  204: 	Training 500/553. train loss: 1.9307,	0.8280 s / batch. (data: 9.96e-04). ETA=12:28:37, max mem: 20.9 GB 
[11/23 13:33:08][INFO] visual_prompt:  217: Epoch 2 / 100: avg data time: 1.86e-01, avg batch time: 1.0084, average train loss: 9.8064
[11/23 13:34:05][INFO] visual_prompt:  316: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3075, average loss: 21.4495
[11/23 13:34:05][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 51.40	
[11/23 13:34:05][INFO] visual_prompt:  165: Training 3 / 100 epoch, with learning rate 5.0
[11/23 13:35:48][INFO] visual_prompt:  204: 	Training 100/553. train loss: 15.6780,	0.8306 s / batch. (data: 3.16e-04). ETA=12:28:52, max mem: 20.9 GB 
[11/23 13:37:30][INFO] visual_prompt:  204: 	Training 200/553. train loss: 6.2633,	0.8253 s / batch. (data: 2.86e-04). ETA=12:22:43, max mem: 20.9 GB 
[11/23 13:39:09][INFO] visual_prompt:  204: 	Training 300/553. train loss: 8.7254,	0.8440 s / batch. (data: 2.84e-04). ETA=12:38:06, max mem: 20.9 GB 
[11/23 13:40:50][INFO] visual_prompt:  204: 	Training 400/553. train loss: 15.1111,	0.8514 s / batch. (data: 1.05e-02). ETA=12:43:22, max mem: 20.9 GB 
[11/23 13:42:31][INFO] visual_prompt:  204: 	Training 500/553. train loss: 71.0609,	1.2386 s / batch. (data: 4.25e-01). ETA=18:28:26, max mem: 20.9 GB 
[11/23 13:43:22][INFO] visual_prompt:  217: Epoch 3 / 100: avg data time: 1.84e-01, avg batch time: 1.0074, average train loss: 14.2301
[11/23 13:44:20][INFO] visual_prompt:  316: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3075, average loss: 21.8537
[11/23 13:44:20][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.46	
[11/23 13:44:20][INFO] visual_prompt:  165: Training 4 / 100 epoch, with learning rate 7.5
[11/23 13:46:06][INFO] visual_prompt:  204: 	Training 100/553. train loss: 9.9626,	0.8400 s / batch. (data: 2.84e-04). ETA=12:29:34, max mem: 20.9 GB 
[11/23 13:47:47][INFO] visual_prompt:  204: 	Training 200/553. train loss: 59.3786,	0.8160 s / batch. (data: 7.96e-03). ETA=12:06:47, max mem: 20.9 GB 
[11/23 13:49:28][INFO] visual_prompt:  204: 	Training 300/553. train loss: 4.1254,	1.5068 s / batch. (data: 6.98e-01). ETA=22:19:32, max mem: 20.9 GB 
[11/23 13:51:04][INFO] visual_prompt:  204: 	Training 400/553. train loss: 6.7726,	1.4480 s / batch. (data: 6.33e-01). ETA=21:24:52, max mem: 20.9 GB 
[11/23 13:52:46][INFO] visual_prompt:  204: 	Training 500/553. train loss: 49.7587,	3.1600 s / batch. (data: 2.33e+00). ETA=1 day, 22:38:43, max mem: 20.9 GB 
[11/23 13:53:40][INFO] visual_prompt:  217: Epoch 4 / 100: avg data time: 1.90e-01, avg batch time: 1.0133, average train loss: 26.8838
[11/23 13:54:37][INFO] visual_prompt:  316: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3079, average loss: 25.4921
[11/23 13:54:37][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 60.11	
[11/23 13:54:37][INFO] visual_prompt:  165: Training 5 / 100 epoch, with learning rate 10.0
[11/23 13:56:20][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.0000,	0.8080 s / batch. (data: 2.87e-04). ETA=11:53:35, max mem: 20.9 GB 
[11/23 13:58:01][INFO] visual_prompt:  204: 	Training 200/553. train loss: 14.0414,	1.2567 s / batch. (data: 4.13e-01). ETA=18:27:46, max mem: 20.9 GB 
[11/23 13:59:42][INFO] visual_prompt:  204: 	Training 300/553. train loss: 54.3054,	0.8200 s / batch. (data: 2.97e-04). ETA=12:01:25, max mem: 20.9 GB 
[11/23 14:01:21][INFO] visual_prompt:  204: 	Training 400/553. train loss: 31.3902,	0.8400 s / batch. (data: 3.00e-04). ETA=12:17:38, max mem: 20.9 GB 
[11/23 14:03:02][INFO] visual_prompt:  204: 	Training 500/553. train loss: 138.4451,	0.8053 s / batch. (data: 7.95e-03). ETA=11:45:51, max mem: 20.9 GB 
[11/23 14:03:55][INFO] visual_prompt:  217: Epoch 5 / 100: avg data time: 1.87e-01, avg batch time: 1.0084, average train loss: 29.4678
[11/23 14:04:53][INFO] visual_prompt:  316: Inference (val):avg data time: 3.65e-05, avg batch time: 0.3063, average loss: 44.2434
[11/23 14:04:53][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.93	
[11/23 14:04:53][INFO] visual_prompt:  165: Training 6 / 100 epoch, with learning rate 12.5
[11/23 14:06:39][INFO] visual_prompt:  204: 	Training 100/553. train loss: 152.0571,	0.8240 s / batch. (data: 8.22e-04). ETA=12:00:04, max mem: 20.9 GB 
[11/23 14:08:18][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.0000,	0.8320 s / batch. (data: 5.44e-03). ETA=12:05:41, max mem: 20.9 GB 
[11/23 14:09:57][INFO] visual_prompt:  204: 	Training 300/553. train loss: 257.1109,	0.8311 s / batch. (data: 5.47e-03). ETA=12:03:35, max mem: 20.9 GB 
[11/23 14:11:41][INFO] visual_prompt:  204: 	Training 400/553. train loss: 63.9711,	0.7996 s / batch. (data: 4.63e-04). ETA=11:34:48, max mem: 20.9 GB 
[11/23 14:13:19][INFO] visual_prompt:  204: 	Training 500/553. train loss: 22.8794,	0.8240 s / batch. (data: 3.26e-04). ETA=11:54:36, max mem: 20.9 GB 
[11/23 14:14:11][INFO] visual_prompt:  217: Epoch 6 / 100: avg data time: 1.90e-01, avg batch time: 1.0102, average train loss: 43.9060
[11/23 14:15:09][INFO] visual_prompt:  316: Inference (val):avg data time: 3.54e-05, avg batch time: 0.3068, average loss: 43.3593
[11/23 14:15:09][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 53.47	
[11/23 14:15:09][INFO] visual_prompt:  165: Training 7 / 100 epoch, with learning rate 15.0
[11/23 14:16:52][INFO] visual_prompt:  204: 	Training 100/553. train loss: 202.3694,	0.8520 s / batch. (data: 1.60e-02). ETA=12:16:43, max mem: 20.9 GB 
[11/23 14:18:31][INFO] visual_prompt:  204: 	Training 200/553. train loss: 34.1242,	0.8320 s / batch. (data: 1.20e-02). ETA=11:58:03, max mem: 20.9 GB 
[11/23 14:20:16][INFO] visual_prompt:  204: 	Training 300/553. train loss: 39.2158,	2.0171 s / batch. (data: 1.19e+00). ETA=1 day, 4:57:25, max mem: 20.9 GB 
[11/23 14:21:56][INFO] visual_prompt:  204: 	Training 400/553. train loss: 74.3282,	1.9291 s / batch. (data: 1.12e+00). ETA=1 day, 3:38:26, max mem: 20.9 GB 
[11/23 14:23:34][INFO] visual_prompt:  204: 	Training 500/553. train loss: 79.8908,	0.8369 s / batch. (data: 2.41e-03). ETA=11:58:04, max mem: 20.9 GB 
[11/23 14:24:25][INFO] visual_prompt:  217: Epoch 7 / 100: avg data time: 1.86e-01, avg batch time: 1.0061, average train loss: 55.4997
[11/23 14:25:23][INFO] visual_prompt:  316: Inference (val):avg data time: 3.63e-05, avg batch time: 0.3064, average loss: 23.6999
[11/23 14:25:23][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.52	
[11/23 14:25:23][INFO] visual_prompt:  165: Training 8 / 100 epoch, with learning rate 17.5
[11/23 14:27:05][INFO] visual_prompt:  204: 	Training 100/553. train loss: 2.0181,	0.8261 s / batch. (data: 3.23e-04). ETA=11:46:40, max mem: 20.9 GB 
[11/23 14:28:47][INFO] visual_prompt:  204: 	Training 200/553. train loss: 61.1637,	0.8248 s / batch. (data: 2.82e-03). ETA=11:44:13, max mem: 20.9 GB 
[11/23 14:30:27][INFO] visual_prompt:  204: 	Training 300/553. train loss: 145.5445,	0.8115 s / batch. (data: 3.00e-04). ETA=11:31:33, max mem: 20.9 GB 
[11/23 14:32:07][INFO] visual_prompt:  204: 	Training 400/553. train loss: 41.8939,	0.8092 s / batch. (data: 3.02e-04). ETA=11:28:13, max mem: 20.9 GB 
[11/23 14:33:48][INFO] visual_prompt:  204: 	Training 500/553. train loss: 21.5642,	1.5600 s / batch. (data: 7.34e-01). ETA=22:04:08, max mem: 20.9 GB 
[11/23 14:34:40][INFO] visual_prompt:  217: Epoch 8 / 100: avg data time: 1.88e-01, avg batch time: 1.0084, average train loss: 56.5643
[11/23 14:35:38][INFO] visual_prompt:  316: Inference (val):avg data time: 2.18e-04, avg batch time: 0.3079, average loss: 57.6910
[11/23 14:35:38][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.04	
[11/23 14:35:38][INFO] visual_prompt:  165: Training 9 / 100 epoch, with learning rate 20.0
[11/23 14:37:22][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.0000,	0.8360 s / batch. (data: 3.21e-04). ETA=11:47:28, max mem: 20.9 GB 
[11/23 14:39:01][INFO] visual_prompt:  204: 	Training 200/553. train loss: 785.9756,	0.8200 s / batch. (data: 2.98e-04). ETA=11:32:35, max mem: 20.9 GB 
[11/23 14:40:42][INFO] visual_prompt:  204: 	Training 300/553. train loss: 56.0445,	1.8611 s / batch. (data: 1.03e+00). ETA=1 day, 2:08:46, max mem: 20.9 GB 
[11/23 14:42:23][INFO] visual_prompt:  204: 	Training 400/553. train loss: 135.3336,	0.8062 s / batch. (data: 2.89e-04). ETA=11:18:15, max mem: 20.9 GB 
[11/23 14:44:04][INFO] visual_prompt:  204: 	Training 500/553. train loss: 26.5560,	0.9437 s / batch. (data: 1.38e-01). ETA=13:12:20, max mem: 20.9 GB 
[11/23 14:44:54][INFO] visual_prompt:  217: Epoch 9 / 100: avg data time: 1.87e-01, avg batch time: 1.0066, average train loss: 68.2584
[11/23 14:45:52][INFO] visual_prompt:  316: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3065, average loss: 19.6656
[11/23 14:45:52][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 48.07	
[11/23 14:45:52][INFO] visual_prompt:  165: Training 10 / 100 epoch, with learning rate 22.5
[11/23 14:47:39][INFO] visual_prompt:  204: 	Training 100/553. train loss: 115.1190,	0.8060 s / batch. (data: 2.99e-04). ETA=11:14:38, max mem: 20.9 GB 
[11/23 14:49:18][INFO] visual_prompt:  204: 	Training 200/553. train loss: 64.3213,	0.8426 s / batch. (data: 3.00e-04). ETA=11:43:52, max mem: 20.9 GB 
[11/23 14:50:58][INFO] visual_prompt:  204: 	Training 300/553. train loss: 6.0597,	1.2173 s / batch. (data: 3.87e-01). ETA=16:54:53, max mem: 20.9 GB 
[11/23 14:52:35][INFO] visual_prompt:  204: 	Training 400/553. train loss: 90.0477,	0.8366 s / batch. (data: 1.05e-02). ETA=11:36:05, max mem: 20.9 GB 
[11/23 14:54:17][INFO] visual_prompt:  204: 	Training 500/553. train loss: 8.8133,	0.8560 s / batch. (data: 3.52e-04). ETA=11:50:48, max mem: 20.9 GB 
[11/23 14:55:10][INFO] visual_prompt:  217: Epoch 10 / 100: avg data time: 1.90e-01, avg batch time: 1.0090, average train loss: 76.3979
[11/23 14:56:07][INFO] visual_prompt:  316: Inference (val):avg data time: 3.71e-05, avg batch time: 0.3067, average loss: 44.8438
[11/23 14:56:07][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.15	
[11/23 14:56:07][INFO] visual_prompt:  165: Training 11 / 100 epoch, with learning rate 25.0
[11/23 14:57:53][INFO] visual_prompt:  204: 	Training 100/553. train loss: 196.5261,	0.8160 s / batch. (data: 2.86e-04). ETA=11:15:30, max mem: 20.9 GB 
[11/23 14:59:35][INFO] visual_prompt:  204: 	Training 200/553. train loss: 278.2780,	0.8520 s / batch. (data: 7.97e-03). ETA=11:43:52, max mem: 20.9 GB 
[11/23 15:01:14][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.0000,	2.1160 s / batch. (data: 1.29e+00). ETA=1 day, 5:04:38, max mem: 20.9 GB 
[11/23 15:02:53][INFO] visual_prompt:  204: 	Training 400/553. train loss: 36.4821,	0.8199 s / batch. (data: 7.52e-04). ETA=11:14:37, max mem: 20.9 GB 
[11/23 15:04:32][INFO] visual_prompt:  204: 	Training 500/553. train loss: 162.8521,	0.8145 s / batch. (data: 3.46e-04). ETA=11:08:49, max mem: 20.9 GB 
[11/23 15:05:23][INFO] visual_prompt:  217: Epoch 11 / 100: avg data time: 1.86e-01, avg batch time: 1.0052, average train loss: 83.0005
[11/23 15:06:20][INFO] visual_prompt:  316: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3074, average loss: 18.7106
[11/23 15:06:21][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 43.66	
[11/23 15:06:21][INFO] visual_prompt:  165: Training 12 / 100 epoch, with learning rate 24.9923853377387
[11/23 15:08:06][INFO] visual_prompt:  204: 	Training 100/553. train loss: 126.5586,	0.8560 s / batch. (data: 3.50e-02). ETA=11:40:43, max mem: 20.9 GB 
[11/23 15:09:47][INFO] visual_prompt:  204: 	Training 200/553. train loss: 152.5565,	0.8033 s / batch. (data: 3.18e-04). ETA=10:56:16, max mem: 20.9 GB 
[11/23 15:11:26][INFO] visual_prompt:  204: 	Training 300/553. train loss: 21.7700,	0.8385 s / batch. (data: 6.50e-03). ETA=11:23:37, max mem: 20.9 GB 
[11/23 15:13:06][INFO] visual_prompt:  204: 	Training 400/553. train loss: 93.6778,	0.8025 s / batch. (data: 3.10e-04). ETA=10:52:55, max mem: 20.9 GB 
[11/23 15:14:47][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.0000,	0.7996 s / batch. (data: 4.99e-04). ETA=10:49:16, max mem: 20.9 GB 
[11/23 15:15:38][INFO] visual_prompt:  217: Epoch 12 / 100: avg data time: 1.89e-01, avg batch time: 1.0077, average train loss: 86.6268
[11/23 15:16:35][INFO] visual_prompt:  316: Inference (val):avg data time: 3.52e-05, avg batch time: 0.3065, average loss: 228.3232
[11/23 15:16:35][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.58	
[11/23 15:16:35][INFO] visual_prompt:  165: Training 13 / 100 epoch, with learning rate 24.969550628247802
[11/23 15:18:21][INFO] visual_prompt:  204: 	Training 100/553. train loss: 23.4021,	0.8080 s / batch. (data: 3.05e-04). ETA=10:53:58, max mem: 20.9 GB 
[11/23 15:19:57][INFO] visual_prompt:  204: 	Training 200/553. train loss: 55.1706,	0.8280 s / batch. (data: 3.00e-04). ETA=11:08:48, max mem: 20.9 GB 
[11/23 15:21:38][INFO] visual_prompt:  204: 	Training 300/553. train loss: 16.4816,	1.7708 s / batch. (data: 9.48e-01). ETA=23:47:22, max mem: 20.9 GB 
[11/23 15:23:17][INFO] visual_prompt:  204: 	Training 400/553. train loss: 7.3962,	0.8377 s / batch. (data: 1.33e-02). ETA=11:13:51, max mem: 20.9 GB 
[11/23 15:24:59][INFO] visual_prompt:  204: 	Training 500/553. train loss: 32.3706,	0.8160 s / batch. (data: 4.48e-03). ETA=10:55:02, max mem: 20.9 GB 
[11/23 15:25:50][INFO] visual_prompt:  217: Epoch 13 / 100: avg data time: 1.85e-01, avg batch time: 1.0040, average train loss: 89.5001
[11/23 15:26:48][INFO] visual_prompt:  316: Inference (val):avg data time: 1.54e-04, avg batch time: 0.3056, average loss: 41.0821
[11/23 15:26:48][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 61.43	
[11/23 15:26:48][INFO] visual_prompt:  165: Training 14 / 100 epoch, with learning rate 24.931523692103415
[11/23 15:28:33][INFO] visual_prompt:  204: 	Training 100/553. train loss: 79.9684,	0.8313 s / batch. (data: 7.26e-03). ETA=11:05:11, max mem: 20.9 GB 
[11/23 15:30:13][INFO] visual_prompt:  204: 	Training 200/553. train loss: 0.0000,	1.2360 s / batch. (data: 4.09e-01). ETA=16:26:58, max mem: 20.9 GB 
[11/23 15:31:53][INFO] visual_prompt:  204: 	Training 300/553. train loss: 81.4320,	0.8040 s / batch. (data: 3.04e-04). ETA=10:40:41, max mem: 20.9 GB 
[11/23 15:33:33][INFO] visual_prompt:  204: 	Training 400/553. train loss: 23.2426,	0.8320 s / batch. (data: 1.19e-02). ETA=11:01:35, max mem: 20.9 GB 
[11/23 15:35:13][INFO] visual_prompt:  204: 	Training 500/553. train loss: 128.8445,	0.8049 s / batch. (data: 2.75e-04). ETA=10:38:42, max mem: 20.9 GB 
[11/23 15:36:04][INFO] visual_prompt:  217: Epoch 14 / 100: avg data time: 1.88e-01, avg batch time: 1.0057, average train loss: 94.1733
[11/23 15:37:01][INFO] visual_prompt:  316: Inference (val):avg data time: 3.69e-05, avg batch time: 0.3062, average loss: 72.9262
[11/23 15:37:01][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.32	
[11/23 15:37:01][INFO] visual_prompt:  165: Training 15 / 100 epoch, with learning rate 24.87835085926963
[11/23 15:38:45][INFO] visual_prompt:  204: 	Training 100/553. train loss: 107.7245,	1.0118 s / batch. (data: 2.00e-01). ETA=13:20:16, max mem: 20.9 GB 
[11/23 15:40:24][INFO] visual_prompt:  204: 	Training 200/553. train loss: 338.1583,	0.8301 s / batch. (data: 1.59e-02). ETA=10:55:11, max mem: 20.9 GB 
[11/23 15:42:06][INFO] visual_prompt:  204: 	Training 300/553. train loss: 10.7235,	0.8069 s / batch. (data: 2.94e-04). ETA=10:35:30, max mem: 20.9 GB 
[11/23 15:43:43][INFO] visual_prompt:  204: 	Training 400/553. train loss: 204.6113,	0.9286 s / batch. (data: 1.16e-01). ETA=12:09:48, max mem: 20.9 GB 
[11/23 15:45:24][INFO] visual_prompt:  204: 	Training 500/553. train loss: 120.6582,	0.8240 s / batch. (data: 5.40e-03). ETA=10:46:16, max mem: 20.9 GB 
[11/23 15:46:18][INFO] visual_prompt:  217: Epoch 15 / 100: avg data time: 1.86e-01, avg batch time: 1.0057, average train loss: 93.8931
[11/23 15:47:15][INFO] visual_prompt:  316: Inference (val):avg data time: 3.79e-05, avg batch time: 0.3060, average loss: 5.1922
[11/23 15:47:15][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 51.96	
[11/23 15:47:15][INFO] visual_prompt:   36: Best epoch 15: best metric: -5.192
[11/23 15:47:15][INFO] visual_prompt:  165: Training 16 / 100 epoch, with learning rate 24.8100969126526
[11/23 15:48:58][INFO] visual_prompt:  204: 	Training 100/553. train loss: 40.4202,	0.8072 s / batch. (data: 2.59e-04). ETA=10:31:02, max mem: 20.9 GB 
[11/23 15:50:38][INFO] visual_prompt:  204: 	Training 200/553. train loss: 140.7945,	0.8050 s / batch. (data: 2.92e-04). ETA=10:27:59, max mem: 20.9 GB 
[11/23 15:52:19][INFO] visual_prompt:  204: 	Training 300/553. train loss: 13.6437,	0.8221 s / batch. (data: 2.99e-04). ETA=10:39:56, max mem: 20.9 GB 
[11/23 15:53:59][INFO] visual_prompt:  204: 	Training 400/553. train loss: 64.7440,	0.8286 s / batch. (data: 2.02e-02). ETA=10:43:36, max mem: 20.9 GB 
[11/23 15:55:39][INFO] visual_prompt:  204: 	Training 500/553. train loss: 203.5595,	0.8062 s / batch. (data: 3.24e-04). ETA=10:24:53, max mem: 20.9 GB 
[11/23 15:56:32][INFO] visual_prompt:  217: Epoch 16 / 100: avg data time: 1.88e-01, avg batch time: 1.0071, average train loss: 78.6311
[11/23 15:57:29][INFO] visual_prompt:  316: Inference (val):avg data time: 2.17e-04, avg batch time: 0.3076, average loss: 4.1551
[11/23 15:57:29][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.29	
[11/23 15:57:29][INFO] visual_prompt:   36: Best epoch 16: best metric: -4.155
[11/23 15:57:29][INFO] visual_prompt:  165: Training 17 / 100 epoch, with learning rate 24.72684500917257
[11/23 15:59:12][INFO] visual_prompt:  204: 	Training 100/553. train loss: 3.5588,	0.8346 s / batch. (data: 2.27e-02). ETA=10:44:46, max mem: 20.9 GB 
[11/23 16:00:54][INFO] visual_prompt:  204: 	Training 200/553. train loss: 182.7799,	0.8080 s / batch. (data: 3.62e-04). ETA=10:22:52, max mem: 20.9 GB 
[11/23 16:02:33][INFO] visual_prompt:  204: 	Training 300/553. train loss: 111.7145,	0.8260 s / batch. (data: 4.32e-04). ETA=10:35:21, max mem: 20.9 GB 
[11/23 16:04:13][INFO] visual_prompt:  204: 	Training 400/553. train loss: 320.7078,	1.1556 s / batch. (data: 3.41e-01). ETA=14:46:58, max mem: 20.9 GB 
[11/23 16:05:53][INFO] visual_prompt:  204: 	Training 500/553. train loss: 124.6979,	1.6720 s / batch. (data: 8.13e-01). ETA=21:20:31, max mem: 20.9 GB 
[11/23 16:06:46][INFO] visual_prompt:  217: Epoch 17 / 100: avg data time: 1.87e-01, avg batch time: 1.0060, average train loss: 80.3591
[11/23 16:07:43][INFO] visual_prompt:  316: Inference (val):avg data time: 3.75e-05, avg batch time: 0.3056, average loss: 219.9071
[11/23 16:07:43][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 49.28	
[11/23 16:07:43][INFO] visual_prompt:  165: Training 18 / 100 epoch, with learning rate 24.628696578449954
[11/23 16:09:28][INFO] visual_prompt:  204: 	Training 100/553. train loss: 65.2004,	0.8241 s / batch. (data: 3.02e-04). ETA=10:29:01, max mem: 20.9 GB 
[11/23 16:11:10][INFO] visual_prompt:  204: 	Training 200/553. train loss: 30.3554,	0.8368 s / batch. (data: 3.07e-03). ETA=10:37:23, max mem: 20.9 GB 
[11/23 16:12:50][INFO] visual_prompt:  204: 	Training 300/553. train loss: 118.4125,	0.8440 s / batch. (data: 1.59e-02). ETA=10:41:25, max mem: 20.9 GB 
[11/23 16:14:30][INFO] visual_prompt:  204: 	Training 400/553. train loss: 23.7052,	0.8280 s / batch. (data: 3.02e-04). ETA=10:27:53, max mem: 20.9 GB 
[11/23 16:16:09][INFO] visual_prompt:  204: 	Training 500/553. train loss: 84.4155,	0.8260 s / batch. (data: 2.94e-04). ETA=10:24:58, max mem: 20.9 GB 
[11/23 16:17:01][INFO] visual_prompt:  217: Epoch 18 / 100: avg data time: 1.88e-01, avg batch time: 1.0077, average train loss: 81.1541
[11/23 16:17:58][INFO] visual_prompt:  316: Inference (val):avg data time: 3.60e-05, avg batch time: 0.3060, average loss: 55.5910
[11/23 16:17:58][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.31	
[11/23 16:17:58][INFO] visual_prompt:  165: Training 19 / 100 epoch, with learning rate 24.515771199228986
[11/23 16:19:42][INFO] visual_prompt:  204: 	Training 100/553. train loss: 33.5354,	1.1343 s / batch. (data: 3.37e-01). ETA=14:15:23, max mem: 20.9 GB 
[11/23 16:21:22][INFO] visual_prompt:  204: 	Training 200/553. train loss: 48.6493,	0.8440 s / batch. (data: 7.96e-03). ETA=10:35:03, max mem: 20.9 GB 
[11/23 16:23:03][INFO] visual_prompt:  204: 	Training 300/553. train loss: 153.9044,	0.8340 s / batch. (data: 2.21e-02). ETA=10:26:07, max mem: 20.9 GB 
[11/23 16:24:44][INFO] visual_prompt:  204: 	Training 400/553. train loss: 50.0509,	0.8387 s / batch. (data: 7.95e-04). ETA=10:28:15, max mem: 20.9 GB 
[11/23 16:26:21][INFO] visual_prompt:  204: 	Training 500/553. train loss: 2.3170,	0.8176 s / batch. (data: 2.90e-04). ETA=10:11:06, max mem: 20.9 GB 
[11/23 16:27:13][INFO] visual_prompt:  217: Epoch 19 / 100: avg data time: 1.83e-01, avg batch time: 1.0039, average train loss: 72.0063
[11/23 16:28:10][INFO] visual_prompt:  316: Inference (val):avg data time: 3.42e-05, avg batch time: 0.3069, average loss: 17.9899
[11/23 16:28:10][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.09	
[11/23 16:28:10][INFO] visual_prompt:  165: Training 20 / 100 epoch, with learning rate 24.38820645368942
[11/23 16:29:53][INFO] visual_prompt:  204: 	Training 100/553. train loss: 5.2882,	0.8200 s / batch. (data: 5.43e-03). ETA=10:10:46, max mem: 20.9 GB 
[11/23 16:31:35][INFO] visual_prompt:  204: 	Training 200/553. train loss: 18.6954,	0.8290 s / batch. (data: 3.14e-04). ETA=10:16:06, max mem: 20.9 GB 
[11/23 16:33:15][INFO] visual_prompt:  204: 	Training 300/553. train loss: 22.6528,	0.8172 s / batch. (data: 5.41e-03). ETA=10:06:00, max mem: 20.9 GB 
[11/23 16:34:54][INFO] visual_prompt:  204: 	Training 400/553. train loss: 22.2894,	0.8099 s / batch. (data: 3.12e-04). ETA=9:59:14, max mem: 20.9 GB 
[11/23 16:36:34][INFO] visual_prompt:  204: 	Training 500/553. train loss: 41.9790,	0.8160 s / batch. (data: 2.68e-04). ETA=10:02:25, max mem: 20.9 GB 
[11/23 16:37:28][INFO] visual_prompt:  217: Epoch 20 / 100: avg data time: 1.89e-01, avg batch time: 1.0076, average train loss: 79.3548
[11/23 16:38:25][INFO] visual_prompt:  316: Inference (val):avg data time: 3.66e-05, avg batch time: 0.3056, average loss: 51.9980
[11/23 16:38:25][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 50.62	
[11/23 16:38:25][INFO] visual_prompt:  165: Training 21 / 100 epoch, with learning rate 24.246157759823856
[11/23 16:40:11][INFO] visual_prompt:  204: 	Training 100/553. train loss: 77.1954,	1.1092 s / batch. (data: 2.91e-01). ETA=13:36:00, max mem: 20.9 GB 
[11/23 16:41:50][INFO] visual_prompt:  204: 	Training 200/553. train loss: 90.5409,	0.8158 s / batch. (data: 3.01e-04). ETA=9:58:48, max mem: 20.9 GB 
[11/23 16:43:30][INFO] visual_prompt:  204: 	Training 300/553. train loss: 120.1923,	1.1307 s / batch. (data: 3.15e-01). ETA=13:48:04, max mem: 20.9 GB 
[11/23 16:45:09][INFO] visual_prompt:  204: 	Training 400/553. train loss: 23.0740,	0.8335 s / batch. (data: 3.25e-04). ETA=10:08:59, max mem: 20.9 GB 
[11/23 16:46:50][INFO] visual_prompt:  204: 	Training 500/553. train loss: 62.0444,	0.8051 s / batch. (data: 3.01e-04). ETA=9:46:56, max mem: 20.9 GB 
[11/23 16:47:41][INFO] visual_prompt:  217: Epoch 21 / 100: avg data time: 1.87e-01, avg batch time: 1.0061, average train loss: 80.6375
[11/23 16:48:39][INFO] visual_prompt:  316: Inference (val):avg data time: 3.70e-05, avg batch time: 0.3059, average loss: 5.7173
[11/23 16:48:39][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 45.95	
[11/23 16:48:39][INFO] visual_prompt:  165: Training 22 / 100 epoch, with learning rate 24.08979818208484
[11/23 16:50:22][INFO] visual_prompt:  204: 	Training 100/553. train loss: 45.5380,	0.8168 s / batch. (data: 1.19e-02). ETA=9:53:21, max mem: 20.9 GB 
[11/23 16:52:02][INFO] visual_prompt:  204: 	Training 200/553. train loss: 14.3998,	0.8440 s / batch. (data: 2.75e-04). ETA=10:11:41, max mem: 20.9 GB 
[11/23 16:53:41][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.0000,	0.8240 s / batch. (data: 3.21e-04). ETA=9:55:51, max mem: 20.9 GB 
[11/23 16:55:21][INFO] visual_prompt:  204: 	Training 400/553. train loss: 41.4337,	0.8136 s / batch. (data: 6.70e-03). ETA=9:46:59, max mem: 20.9 GB 
[11/23 16:57:01][INFO] visual_prompt:  204: 	Training 500/553. train loss: 41.4893,	0.8452 s / batch. (data: 9.14e-03). ETA=10:08:22, max mem: 20.9 GB 
[11/23 16:57:56][INFO] visual_prompt:  217: Epoch 22 / 100: avg data time: 1.88e-01, avg batch time: 1.0066, average train loss: 82.7818
[11/23 16:58:53][INFO] visual_prompt:  316: Inference (val):avg data time: 2.44e-04, avg batch time: 0.3077, average loss: 30.7563
[11/23 16:58:53][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[11/23 16:58:53][INFO] visual_prompt:  165: Training 23 / 100 epoch, with learning rate 23.91931822053251
[11/23 17:00:41][INFO] visual_prompt:  204: 	Training 100/553. train loss: 123.4828,	0.8440 s / batch. (data: 1.19e-02). ETA=10:05:20, max mem: 20.9 GB 
[11/23 17:02:23][INFO] visual_prompt:  204: 	Training 200/553. train loss: 15.2011,	0.8400 s / batch. (data: 2.61e-04). ETA=10:01:05, max mem: 20.9 GB 
[11/23 17:04:06][INFO] visual_prompt:  204: 	Training 300/553. train loss: 234.7354,	0.8280 s / batch. (data: 4.21e-04). ETA=9:51:04, max mem: 20.9 GB 
[11/23 17:05:45][INFO] visual_prompt:  204: 	Training 400/553. train loss: 102.2313,	0.8361 s / batch. (data: 5.43e-03). ETA=9:55:28, max mem: 20.9 GB 
[11/23 17:07:23][INFO] visual_prompt:  204: 	Training 500/553. train loss: 0.0000,	0.8176 s / batch. (data: 2.99e-04). ETA=9:40:59, max mem: 20.9 GB 
[11/23 17:08:16][INFO] visual_prompt:  217: Epoch 23 / 100: avg data time: 1.98e-01, avg batch time: 1.0173, average train loss: 75.8306
[11/23 17:09:13][INFO] visual_prompt:  316: Inference (val):avg data time: 3.57e-05, avg batch time: 0.3059, average loss: 146.7328
[11/23 17:09:13][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 46.00	
[11/23 17:09:13][INFO] visual_prompt:  165: Training 24 / 100 epoch, with learning rate 23.73492557873959
[11/23 17:10:55][INFO] visual_prompt:  204: 	Training 100/553. train loss: 80.0707,	0.8124 s / batch. (data: 4.00e-04). ETA=9:35:11, max mem: 20.9 GB 
[11/23 17:12:34][INFO] visual_prompt:  204: 	Training 200/553. train loss: 151.4542,	0.7980 s / batch. (data: 2.80e-04). ETA=9:23:41, max mem: 20.9 GB 
[11/23 17:14:17][INFO] visual_prompt:  204: 	Training 300/553. train loss: 10.0722,	1.0280 s / batch. (data: 2.21e-01). ETA=12:04:24, max mem: 20.9 GB 
[11/23 17:15:58][INFO] visual_prompt:  204: 	Training 400/553. train loss: 18.1664,	0.8275 s / batch. (data: 3.50e-04). ETA=9:41:44, max mem: 20.9 GB 
[11/23 17:17:40][INFO] visual_prompt:  204: 	Training 500/553. train loss: 34.1217,	0.8303 s / batch. (data: 3.25e-04). ETA=9:42:17, max mem: 20.9 GB 
[11/23 17:18:33][INFO] visual_prompt:  217: Epoch 24 / 100: avg data time: 1.93e-01, avg batch time: 1.0117, average train loss: 79.6634
[11/23 17:19:31][INFO] visual_prompt:  316: Inference (val):avg data time: 3.73e-05, avg batch time: 0.3066, average loss: 23.4213
[11/23 17:19:31][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.63	
[11/23 17:19:31][INFO] visual_prompt:  165: Training 25 / 100 epoch, with learning rate 23.536844910736587
[11/23 17:21:19][INFO] visual_prompt:  204: 	Training 100/553. train loss: 141.2382,	0.8182 s / batch. (data: 5.45e-03). ETA=9:31:43, max mem: 20.9 GB 
[11/23 17:22:56][INFO] visual_prompt:  204: 	Training 200/553. train loss: 37.6976,	1.2120 s / batch. (data: 3.85e-01). ETA=14:04:55, max mem: 20.9 GB 
[11/23 17:24:36][INFO] visual_prompt:  204: 	Training 300/553. train loss: 216.4294,	0.9320 s / batch. (data: 1.10e-01). ETA=10:48:09, max mem: 20.9 GB 
[11/23 17:26:17][INFO] visual_prompt:  204: 	Training 400/553. train loss: 105.7682,	1.3561 s / batch. (data: 5.24e-01). ETA=15:40:50, max mem: 20.9 GB 
[11/23 17:27:59][INFO] visual_prompt:  204: 	Training 500/553. train loss: 40.7476,	1.5525 s / batch. (data: 7.35e-01). ETA=17:54:31, max mem: 20.9 GB 
[11/23 17:28:51][INFO] visual_prompt:  217: Epoch 25 / 100: avg data time: 1.96e-01, avg batch time: 1.0137, average train loss: 73.2479
[11/23 17:29:49][INFO] visual_prompt:  316: Inference (val):avg data time: 3.76e-05, avg batch time: 0.3075, average loss: 121.4401
[11/23 17:29:49][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.64	
[11/23 17:29:49][INFO] visual_prompt:  165: Training 26 / 100 epoch, with learning rate 23.325317547305485
[11/23 17:31:34][INFO] visual_prompt:  204: 	Training 100/553. train loss: 232.7358,	0.8240 s / batch. (data: 2.95e-04). ETA=9:28:13, max mem: 20.9 GB 
[11/23 17:33:16][INFO] visual_prompt:  204: 	Training 200/553. train loss: 22.3119,	1.9473 s / batch. (data: 1.11e+00). ETA=22:19:33, max mem: 20.9 GB 
[11/23 17:34:58][INFO] visual_prompt:  204: 	Training 300/553. train loss: 224.7032,	0.8323 s / batch. (data: 7.44e-04). ETA=9:31:08, max mem: 20.9 GB 
[11/23 17:36:38][INFO] visual_prompt:  204: 	Training 400/553. train loss: 11.7838,	0.8216 s / batch. (data: 2.76e-04). ETA=9:22:26, max mem: 20.9 GB 
[11/23 17:38:18][INFO] visual_prompt:  204: 	Training 500/553. train loss: 139.0402,	0.8120 s / batch. (data: 2.97e-04). ETA=9:14:30, max mem: 20.9 GB 
[11/23 17:39:10][INFO] visual_prompt:  217: Epoch 26 / 100: avg data time: 1.95e-01, avg batch time: 1.0141, average train loss: 90.0629
[11/23 17:40:08][INFO] visual_prompt:  316: Inference (val):avg data time: 3.94e-05, avg batch time: 0.3062, average loss: 45.1047
[11/23 17:40:08][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 44.26	
[11/23 17:40:08][INFO] visual_prompt:  165: Training 27 / 100 epoch, with learning rate 23.100601201955325
[11/23 17:41:53][INFO] visual_prompt:  204: 	Training 100/553. train loss: 22.0259,	0.8320 s / batch. (data: 3.06e-04). ETA=9:26:04, max mem: 20.9 GB 
[11/23 17:43:36][INFO] visual_prompt:  204: 	Training 200/553. train loss: 166.5758,	1.4085 s / batch. (data: 6.06e-01). ETA=15:55:55, max mem: 20.9 GB 
[11/23 17:45:17][INFO] visual_prompt:  204: 	Training 300/553. train loss: 47.4541,	0.8021 s / batch. (data: 3.24e-04). ETA=9:03:02, max mem: 20.9 GB 
[11/23 17:46:59][INFO] visual_prompt:  204: 	Training 400/553. train loss: 40.6689,	0.8456 s / batch. (data: 7.50e-04). ETA=9:31:03, max mem: 20.9 GB 
[11/23 17:48:40][INFO] visual_prompt:  204: 	Training 500/553. train loss: 119.7567,	0.8400 s / batch. (data: 3.12e-04). ETA=9:25:53, max mem: 20.9 GB 
[11/23 17:49:31][INFO] visual_prompt:  217: Epoch 27 / 100: avg data time: 1.99e-01, avg batch time: 1.0178, average train loss: 77.0611
[11/23 17:50:28][INFO] visual_prompt:  316: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3054, average loss: 40.9811
[11/23 17:50:28][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 43.89	
[11/23 17:50:28][INFO] visual_prompt:  165: Training 28 / 100 epoch, with learning rate 22.86296965693802
[11/23 17:52:12][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.0000,	0.8416 s / batch. (data: 2.84e-02). ETA=9:24:49, max mem: 20.9 GB 
[11/23 17:53:53][INFO] visual_prompt:  204: 	Training 200/553. train loss: 54.0814,	0.8240 s / batch. (data: 3.01e-04). ETA=9:11:40, max mem: 20.9 GB 
[11/23 17:55:36][INFO] visual_prompt:  204: 	Training 300/553. train loss: 81.7058,	1.5342 s / batch. (data: 7.26e-01). ETA=17:04:34, max mem: 20.9 GB 
[11/23 17:57:15][INFO] visual_prompt:  204: 	Training 400/553. train loss: 65.5140,	0.8455 s / batch. (data: 9.44e-03). ETA=9:23:12, max mem: 20.9 GB 
[11/23 17:58:54][INFO] visual_prompt:  204: 	Training 500/553. train loss: 182.4296,	0.8179 s / batch. (data: 3.94e-04). ETA=9:03:29, max mem: 20.9 GB 
[11/23 17:59:49][INFO] visual_prompt:  217: Epoch 28 / 100: avg data time: 1.94e-01, avg batch time: 1.0128, average train loss: 82.3342
[11/23 18:00:46][INFO] visual_prompt:  316: Inference (val):avg data time: 3.81e-05, avg batch time: 0.3072, average loss: 18.1313
[11/23 18:00:46][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.82	
[11/23 18:00:46][INFO] visual_prompt:  165: Training 29 / 100 epoch, with learning rate 22.612712429686844
[11/23 18:02:37][INFO] visual_prompt:  204: 	Training 100/553. train loss: 0.0000,	0.8335 s / batch. (data: 2.15e-02). ETA=9:11:44, max mem: 20.9 GB 
[11/23 18:04:17][INFO] visual_prompt:  204: 	Training 200/553. train loss: 130.6215,	1.7870 s / batch. (data: 9.77e-01). ETA=19:39:53, max mem: 20.9 GB 
[11/23 18:05:55][INFO] visual_prompt:  204: 	Training 300/553. train loss: 62.4456,	0.8269 s / batch. (data: 1.10e-02). ETA=9:04:37, max mem: 20.9 GB 
[11/23 18:07:32][INFO] visual_prompt:  204: 	Training 400/553. train loss: 62.6526,	1.2942 s / batch. (data: 4.85e-01). ETA=14:10:11, max mem: 20.9 GB 
[11/23 18:09:13][INFO] visual_prompt:  204: 	Training 500/553. train loss: 33.2094,	0.8094 s / batch. (data: 3.18e-04). ETA=8:50:21, max mem: 20.9 GB 
[11/23 18:10:05][INFO] visual_prompt:  217: Epoch 29 / 100: avg data time: 1.91e-01, avg batch time: 1.0100, average train loss: 68.3521
[11/23 18:11:02][INFO] visual_prompt:  316: Inference (val):avg data time: 3.86e-05, avg batch time: 0.3060, average loss: 34.2258
[11/23 18:11:02][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 41.92	
[11/23 18:11:02][INFO] visual_prompt:  165: Training 30 / 100 epoch, with learning rate 22.35013442008402
[11/23 18:12:45][INFO] visual_prompt:  204: 	Training 100/553. train loss: 201.8579,	0.8018 s / batch. (data: 2.90e-04). ETA=8:43:19, max mem: 20.9 GB 
[11/23 18:14:27][INFO] visual_prompt:  204: 	Training 200/553. train loss: 57.9761,	0.8460 s / batch. (data: 2.19e-02). ETA=9:10:47, max mem: 20.9 GB 
[11/23 18:16:06][INFO] visual_prompt:  204: 	Training 300/553. train loss: 0.0000,	1.3760 s / batch. (data: 5.48e-01). ETA=14:53:32, max mem: 20.9 GB 
[11/23 18:17:48][INFO] visual_prompt:  204: 	Training 400/553. train loss: 193.9628,	1.1080 s / batch. (data: 2.85e-01). ETA=11:57:40, max mem: 20.9 GB 
[11/23 18:19:28][INFO] visual_prompt:  204: 	Training 500/553. train loss: 112.9856,	1.5504 s / batch. (data: 7.42e-01). ETA=16:41:38, max mem: 20.9 GB 
[11/23 18:20:22][INFO] visual_prompt:  217: Epoch 30 / 100: avg data time: 1.92e-01, avg batch time: 1.0122, average train loss: 75.3280
[11/23 18:21:20][INFO] visual_prompt:  316: Inference (val):avg data time: 5.08e-05, avg batch time: 0.3080, average loss: 84.2754
[11/23 18:21:20][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 46.72	
[11/23 18:21:20][INFO] visual_prompt:   42: Stopping early.

[11/22 11:21:58][INFO] visual_prompt:   95: Rank of current process: 0. World size: 1
[11/22 11:21:59][INFO] visual_prompt:   97: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.1.0
PyTorch Debug Build  False
CUDA available       True
CUDA ID              0
GPU 0                NVIDIA GeForce RTX 4050 Laptop GPU
Pillow               9.4.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/22 11:21:59][INFO] visual_prompt:   99: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', train_type='prompt', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '1', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '123', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '1', 'SOLVER.CRITERION', 'loss'])
[11/22 11:21:59][INFO] visual_prompt:  101: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[11/22 11:21:59][INFO] visual_prompt:  108: Training with config:
[11/22 11:21:59][INFO] visual_prompt:  109: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/prompt50/size123/val/seed0/lr50.0_wd0.01/patience1/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 1, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 50.0, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 123, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 1, 'NUM_WORKERS': 4, 'PIN_MEMORY': True})})
[11/22 11:21:59][INFO] visual_prompt:   57: Loading training data...
[11/22 11:21:59][INFO] visual_prompt:   28: Constructing mammo-cbis dataset train...
[11/22 11:21:59][INFO] visual_prompt:   59: Loading validation data...
[11/22 11:21:59][INFO] visual_prompt:   28: Constructing mammo-cbis dataset val...
[11/22 11:21:59][INFO] visual_prompt:   39: Constructing models...
[11/22 11:22:01][INFO] visual_prompt:   52: Total Parameters: 86148098	 Gradient Parameters: 462338
[11/22 11:22:01][INFO] visual_prompt:   54: tuned percent:0.537
[11/22 11:22:01][INFO] visual_prompt:   40: Device used for model: 0
[11/22 11:22:01][INFO] visual_prompt:   41: Setting up Evaluator...
[11/22 11:22:01][INFO] visual_prompt:   43: Setting up Trainer...
[11/22 11:22:01][INFO] visual_prompt:   45: 	Setting up the optimizer...
[11/22 11:22:01][INFO] visual_prompt:  165: Training 1 / 100 epoch, with learning rate 0.0
[11/22 11:22:06][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 1.5160,	0.0199 s / batch. (data: 6.65e-05). ETA=1:13:15, max mem: 0.4 GB 
[11/22 11:22:10][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 2.9942,	0.0209 s / batch. (data: 7.96e-05). ETA=1:17:09, max mem: 0.4 GB 
[11/22 11:22:14][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.0986,	0.0188 s / batch. (data: 7.82e-05). ETA=1:09:10, max mem: 0.4 GB 
[11/22 11:22:19][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.0699,	0.0208 s / batch. (data: 8.23e-05). ETA=1:16:24, max mem: 0.4 GB 
[11/22 11:22:24][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 4.0292,	0.0219 s / batch. (data: 8.32e-05). ETA=1:20:41, max mem: 0.4 GB 
[11/22 11:22:28][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.0278,	0.0126 s / batch. (data: 4.67e-05). ETA=0:46:14, max mem: 0.4 GB 
[11/22 11:22:33][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 1.7255,	0.0524 s / batch. (data: 3.91e-02). ETA=3:12:32, max mem: 0.4 GB 
[11/22 11:22:37][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 2.6151,	0.0582 s / batch. (data: 2.99e-02). ETA=3:33:41, max mem: 0.4 GB 
[11/22 11:22:41][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 3.9732,	0.0204 s / batch. (data: 1.71e-04). ETA=1:14:52, max mem: 0.4 GB 
[11/22 11:22:46][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 4.1492,	0.0227 s / batch. (data: 7.84e-05). ETA=1:23:13, max mem: 0.4 GB 
[11/22 11:22:51][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.0232,	0.0210 s / batch. (data: 8.03e-05). ETA=1:17:07, max mem: 0.4 GB 
[11/22 11:22:55][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.0188,	0.0206 s / batch. (data: 1.06e-04). ETA=1:15:42, max mem: 0.4 GB 
[11/22 11:22:59][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 3.3262,	0.0211 s / batch. (data: 6.72e-05). ETA=1:17:17, max mem: 0.4 GB 
[11/22 11:23:04][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 4.3187,	0.2702 s / batch. (data: 2.49e-01). ETA=16:29:53, max mem: 0.4 GB 
[11/22 11:23:09][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 2.6273,	0.1635 s / batch. (data: 1.49e-01). ETA=9:58:39, max mem: 0.4 GB 
[11/22 11:23:13][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.1198,	0.0459 s / batch. (data: 2.35e-02). ETA=2:48:05, max mem: 0.4 GB 
[11/22 11:23:18][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 3.6023,	0.0306 s / batch. (data: 2.11e-04). ETA=1:51:52, max mem: 0.4 GB 
[11/22 11:23:22][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.0230,	0.0565 s / batch. (data: 2.74e-02). ETA=3:26:45, max mem: 0.4 GB 
[11/22 11:23:27][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 3.0862,	0.0606 s / batch. (data: 3.90e-02). ETA=3:41:31, max mem: 0.4 GB 
[11/22 11:23:32][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 2.5929,	0.1095 s / batch. (data: 8.75e-02). ETA=6:39:52, max mem: 0.4 GB 
[11/22 11:23:36][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.0811,	0.0129 s / batch. (data: 7.87e-05). ETA=0:46:59, max mem: 0.4 GB 
[11/22 11:23:40][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 2.6646,	0.0973 s / batch. (data: 7.45e-02). ETA=5:54:58, max mem: 0.4 GB 
[11/22 11:23:41][INFO] visual_prompt:  217: Epoch 1 / 100: avg data time: 2.36e-02, avg batch time: 0.0451, average train loss: 1.4045
[11/22 11:23:45][INFO] visual_prompt:  303: 	Test 100/246. loss: 2.666, 0.0064 s / batch. (data: 2.36e-05)max mem: 0.39113 GB 
[11/22 11:23:49][INFO] visual_prompt:  303: 	Test 200/246. loss: 3.271, 0.0119 s / batch. (data: 1.96e-05)max mem: 0.39113 GB 
[11/22 11:23:51][INFO] visual_prompt:  316: Inference (val):avg data time: 2.62e-05, avg batch time: 0.0081, average loss: 1.3280
[11/22 11:23:51][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 47.90	
[11/22 11:23:51][INFO] visual_prompt:  165: Training 2 / 100 epoch, with learning rate 5.0
[11/22 11:23:56][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 45.0891,	0.0301 s / batch. (data: 1.69e-04). ETA=1:49:37, max mem: 0.4 GB 
[11/22 11:24:00][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 50.2426,	0.0285 s / batch. (data: 1.59e-04). ETA=1:44:01, max mem: 0.4 GB 
[11/22 11:24:05][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 39.5609,	0.0202 s / batch. (data: 4.91e-05). ETA=1:13:44, max mem: 0.4 GB 
[11/22 11:24:09][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 85.1134,	0.0280 s / batch. (data: 1.51e-04). ETA=1:42:05, max mem: 0.4 GB 
[11/22 11:24:12][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 36.0831,	0.0283 s / batch. (data: 7.75e-05). ETA=1:42:52, max mem: 0.4 GB 
[11/22 11:24:16][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 14.5521,	0.0193 s / batch. (data: 5.84e-05). ETA=1:10:24, max mem: 0.4 GB 
[11/22 11:24:19][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 53.3318,	0.0207 s / batch. (data: 5.36e-05). ETA=1:15:22, max mem: 0.4 GB 
[11/22 11:24:23][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.0000,	0.0213 s / batch. (data: 2.14e-04). ETA=1:17:21, max mem: 0.4 GB 
[11/22 11:24:26][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.0000,	0.0275 s / batch. (data: 1.45e-04). ETA=1:39:50, max mem: 0.4 GB 
[11/22 11:24:30][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 56.4746,	0.0213 s / batch. (data: 7.94e-05). ETA=1:17:24, max mem: 0.4 GB 
[11/22 11:24:34][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.0000,	0.0196 s / batch. (data: 1.01e-04). ETA=1:11:06, max mem: 0.4 GB 
[11/22 11:24:37][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 40.1296,	0.0198 s / batch. (data: 7.63e-05). ETA=1:11:47, max mem: 0.4 GB 
[11/22 11:24:41][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 57.2009,	0.0264 s / batch. (data: 1.02e-04). ETA=1:35:50, max mem: 0.4 GB 
[11/22 11:24:44][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.0000,	0.0271 s / batch. (data: 7.77e-05). ETA=1:38:20, max mem: 0.4 GB 
[11/22 11:24:48][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 61.6249,	0.0338 s / batch. (data: 6.49e-03). ETA=2:02:40, max mem: 0.4 GB 
[11/22 11:24:51][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 57.1523,	0.0277 s / batch. (data: 8.89e-05). ETA=1:40:14, max mem: 0.4 GB 
[11/22 11:24:55][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 37.3495,	0.0195 s / batch. (data: 6.87e-05). ETA=1:10:41, max mem: 0.4 GB 
[11/22 11:24:58][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 153.8590,	0.0206 s / batch. (data: 7.99e-05). ETA=1:14:35, max mem: 0.4 GB 
[11/22 11:25:02][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 44.0060,	0.0277 s / batch. (data: 1.17e-04). ETA=1:40:23, max mem: 0.4 GB 
[11/22 11:25:06][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.0000,	0.0277 s / batch. (data: 8.01e-05). ETA=1:40:12, max mem: 0.4 GB 
[11/22 11:25:10][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 17.4905,	0.0866 s / batch. (data: 5.75e-02). ETA=5:12:55, max mem: 0.4 GB 
[11/22 11:25:13][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.0000,	0.0280 s / batch. (data: 7.77e-05). ETA=1:41:03, max mem: 0.4 GB 
[11/22 11:25:13][INFO] visual_prompt:  217: Epoch 2 / 100: avg data time: 1.24e-02, avg batch time: 0.0369, average train loss: 38.2699
[11/22 11:25:18][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.000, 0.0112 s / batch. (data: 1.48e-05)max mem: 0.39113 GB 
[11/22 11:25:23][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.000, 0.0110 s / batch. (data: 1.31e-05)max mem: 0.39113 GB 
[11/22 11:25:25][INFO] visual_prompt:  316: Inference (val):avg data time: 2.77e-05, avg batch time: 0.0115, average loss: 45.4368
[11/22 11:25:25][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.75	
[11/22 11:25:25][INFO] visual_prompt:  165: Training 3 / 100 epoch, with learning rate 10.0
[11/22 11:25:30][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 271.4766,	0.0250 s / batch. (data: 5.87e-05). ETA=1:30:26, max mem: 0.4 GB 
[11/22 11:25:35][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.0000,	0.1060 s / batch. (data: 7.61e-02). ETA=6:22:33, max mem: 0.4 GB 
[11/22 11:25:39][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 58.1117,	0.0319 s / batch. (data: 1.06e-04). ETA=1:54:59, max mem: 0.4 GB 
[11/22 11:25:44][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.0000,	0.0237 s / batch. (data: 2.08e-03). ETA=1:25:22, max mem: 0.4 GB 
[11/22 11:25:49][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.0000,	0.0564 s / batch. (data: 3.44e-02). ETA=3:23:23, max mem: 0.4 GB 
[11/22 11:25:53][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.0000,	0.0201 s / batch. (data: 6.58e-05). ETA=1:12:15, max mem: 0.4 GB 
[11/22 11:25:58][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 126.5710,	0.0208 s / batch. (data: 5.32e-05). ETA=1:14:46, max mem: 0.4 GB 
[11/22 11:26:02][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.0000,	0.2131 s / batch. (data: 2.00e-01). ETA=12:46:59, max mem: 0.4 GB 
[11/22 11:26:08][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 243.2954,	0.0220 s / batch. (data: 8.99e-05). ETA=1:19:11, max mem: 0.4 GB 
[11/22 11:26:12][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 231.2562,	0.0292 s / batch. (data: 9.44e-05). ETA=1:45:06, max mem: 0.4 GB 
[11/22 11:26:17][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 137.5447,	0.1583 s / batch. (data: 1.45e-01). ETA=9:29:10, max mem: 0.4 GB 
[11/22 11:26:22][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.0000,	0.2393 s / batch. (data: 2.18e-01). ETA=14:19:42, max mem: 0.4 GB 
[11/22 11:26:27][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.0000,	0.0948 s / batch. (data: 7.39e-02). ETA=5:40:22, max mem: 0.4 GB 
[11/22 11:26:31][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.0000,	0.1890 s / batch. (data: 1.75e-01). ETA=11:18:28, max mem: 0.4 GB 
[11/22 11:26:36][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 81.7398,	0.0144 s / batch. (data: 5.60e-05). ETA=0:51:39, max mem: 0.4 GB 
[11/22 11:26:41][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.0000,	0.0472 s / batch. (data: 3.33e-02). ETA=2:49:22, max mem: 0.4 GB 
[11/22 11:26:45][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 71.1833,	0.0845 s / batch. (data: 7.09e-02). ETA=5:03:03, max mem: 0.4 GB 
[11/22 11:26:50][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.0000,	0.0364 s / batch. (data: 1.41e-02). ETA=2:10:19, max mem: 0.4 GB 
[11/22 11:26:55][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 161.4980,	0.0976 s / batch. (data: 7.21e-02). ETA=5:49:29, max mem: 0.4 GB 
[11/22 11:27:00][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.0000,	0.0383 s / batch. (data: 2.50e-02). ETA=2:17:16, max mem: 0.4 GB 
[11/22 11:27:05][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 117.3318,	0.0214 s / batch. (data: 1.45e-04). ETA=1:16:25, max mem: 0.4 GB 
[11/22 11:27:09][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.0000,	0.0298 s / batch. (data: 9.80e-05). ETA=1:46:42, max mem: 0.4 GB 
[11/22 11:27:10][INFO] visual_prompt:  217: Epoch 3 / 100: avg data time: 2.71e-02, avg batch time: 0.0475, average train loss: 72.1529
[11/22 11:27:15][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.000, 0.0118 s / batch. (data: 1.50e-05)max mem: 0.39113 GB 
[11/22 11:27:19][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.000, 0.0113 s / batch. (data: 1.48e-05)max mem: 0.39113 GB 
[11/22 11:27:22][INFO] visual_prompt:  316: Inference (val):avg data time: 2.78e-05, avg batch time: 0.0112, average loss: 245.6420
[11/22 11:27:22][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.09	
[11/22 11:27:22][INFO] visual_prompt:  165: Training 4 / 100 epoch, with learning rate 15.0
[11/22 11:27:26][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.0001,	0.0255 s / batch. (data: 6.15e-05). ETA=1:31:03, max mem: 0.4 GB 
[11/22 11:27:31][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.0000,	0.0152 s / batch. (data: 6.99e-05). ETA=0:54:18, max mem: 0.4 GB 
[11/22 11:27:35][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 79.8324,	0.0239 s / batch. (data: 9.97e-04). ETA=1:25:25, max mem: 0.4 GB 
[11/22 11:27:39][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.0000,	0.0309 s / batch. (data: 1.92e-04). ETA=1:50:11, max mem: 0.4 GB 
[11/22 11:27:42][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 15.1579,	0.0310 s / batch. (data: 1.98e-04). ETA=1:50:40, max mem: 0.4 GB 
[11/22 11:27:46][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 220.3753,	0.0309 s / batch. (data: 9.82e-05). ETA=1:50:10, max mem: 0.4 GB 
[11/22 11:27:49][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 14.7178,	0.0237 s / batch. (data: 1.31e-04). ETA=1:24:18, max mem: 0.4 GB 
[11/22 11:27:53][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 14.5476,	0.0271 s / batch. (data: 9.39e-05). ETA=1:36:31, max mem: 0.4 GB 
[11/22 11:27:58][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 171.9654,	0.0292 s / batch. (data: 8.34e-05). ETA=1:44:05, max mem: 0.4 GB 
[11/22 11:28:01][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.0000,	0.0592 s / batch. (data: 2.97e-02). ETA=3:30:44, max mem: 0.4 GB 
[11/22 11:28:06][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.0000,	0.0359 s / batch. (data: 1.53e-02). ETA=2:07:42, max mem: 0.4 GB 
[11/22 11:28:11][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.0000,	0.0288 s / batch. (data: 9.23e-05). ETA=1:42:20, max mem: 0.4 GB 
[11/22 11:28:15][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 314.6863,	0.0210 s / batch. (data: 9.01e-05). ETA=1:14:29, max mem: 0.4 GB 
[11/22 11:28:19][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.0000,	0.0271 s / batch. (data: 9.63e-05). ETA=1:36:20, max mem: 0.4 GB 
[11/22 11:28:23][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.0000,	0.0215 s / batch. (data: 9.25e-05). ETA=1:16:25, max mem: 0.4 GB 
[11/22 11:28:27][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.0000,	0.0203 s / batch. (data: 8.73e-05). ETA=1:11:53, max mem: 0.4 GB 
[11/22 11:28:32][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 348.9399,	0.1803 s / batch. (data: 1.67e-01). ETA=10:39:39, max mem: 0.4 GB 
[11/22 11:28:37][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.0000,	0.0154 s / batch. (data: 9.04e-05). ETA=0:54:35, max mem: 0.4 GB 
[11/22 11:28:41][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 136.9298,	0.0132 s / batch. (data: 6.37e-05). ETA=0:46:42, max mem: 0.4 GB 
[11/22 11:28:46][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.0000,	0.0357 s / batch. (data: 1.91e-04). ETA=2:06:25, max mem: 0.4 GB 
[11/22 11:28:51][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.0000,	0.0147 s / batch. (data: 6.06e-05). ETA=0:52:06, max mem: 0.4 GB 
[11/22 11:28:55][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.0000,	0.0199 s / batch. (data: 5.72e-05). ETA=1:10:24, max mem: 0.4 GB 
[11/22 11:28:56][INFO] visual_prompt:  217: Epoch 4 / 100: avg data time: 1.93e-02, avg batch time: 0.0424, average train loss: 97.4022
[11/22 11:29:01][INFO] visual_prompt:  303: 	Test 100/246. loss: 76.997, 0.0117 s / batch. (data: 1.36e-05)max mem: 0.39113 GB 
[11/22 11:29:06][INFO] visual_prompt:  303: 	Test 200/246. loss: 76.771, 0.0116 s / batch. (data: 1.69e-05)max mem: 0.39113 GB 
[11/22 11:29:08][INFO] visual_prompt:  316: Inference (val):avg data time: 3.53e-05, avg batch time: 0.0119, average loss: 34.7601
[11/22 11:29:08][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 52.47	
[11/22 11:29:08][INFO] visual_prompt:  165: Training 5 / 100 epoch, with learning rate 20.0
[11/22 11:29:14][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.0000,	0.0350 s / batch. (data: 1.67e-04). ETA=2:03:56, max mem: 0.4 GB 
[11/22 11:29:18][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 662.3439,	0.0368 s / batch. (data: 1.37e-04). ETA=2:10:16, max mem: 0.4 GB 
[11/22 11:29:23][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 10.3114,	0.0315 s / batch. (data: 1.72e-04). ETA=1:51:25, max mem: 0.4 GB 
[11/22 11:29:27][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.0000,	0.0250 s / batch. (data: 1.95e-04). ETA=1:28:28, max mem: 0.4 GB 
[11/22 11:29:32][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 391.0456,	0.0165 s / batch. (data: 1.57e-03). ETA=0:58:24, max mem: 0.4 GB 
[11/22 11:29:37][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 105.3620,	0.0228 s / batch. (data: 5.34e-05). ETA=1:20:17, max mem: 0.4 GB 
[11/22 11:29:42][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 59.4096,	0.0598 s / batch. (data: 1.29e-04). ETA=3:30:46, max mem: 0.4 GB 
[11/22 11:29:46][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 716.0469,	0.0224 s / batch. (data: 7.32e-05). ETA=1:19:01, max mem: 0.4 GB 
[11/22 11:29:50][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.0000,	0.0205 s / batch. (data: 1.15e-04). ETA=1:12:22, max mem: 0.4 GB 
[11/22 11:29:55][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.0000,	0.0203 s / batch. (data: 8.54e-05). ETA=1:11:34, max mem: 0.4 GB 
[11/22 11:30:00][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.0000,	0.0468 s / batch. (data: 1.91e-04). ETA=2:44:52, max mem: 0.4 GB 
[11/22 11:30:04][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.0000,	0.0269 s / batch. (data: 7.87e-05). ETA=1:34:37, max mem: 0.4 GB 
[11/22 11:30:08][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 769.5526,	0.0270 s / batch. (data: 7.94e-05). ETA=1:34:57, max mem: 0.4 GB 
[11/22 11:30:12][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 672.1343,	0.0231 s / batch. (data: 5.67e-05). ETA=1:21:20, max mem: 0.4 GB 
[11/22 11:30:17][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.0000,	0.0133 s / batch. (data: 1.11e-04). ETA=0:46:45, max mem: 0.4 GB 
[11/22 11:30:21][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 34.1972,	0.0273 s / batch. (data: 7.75e-05). ETA=1:35:56, max mem: 0.4 GB 
[11/22 11:30:25][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 802.0184,	0.0217 s / batch. (data: 2.02e-04). ETA=1:16:16, max mem: 0.4 GB 
[11/22 11:30:29][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 693.9696,	0.0273 s / batch. (data: 7.15e-05). ETA=1:35:53, max mem: 0.4 GB 
[11/22 11:30:32][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 399.3422,	0.0196 s / batch. (data: 9.16e-05). ETA=1:08:37, max mem: 0.4 GB 
[11/22 11:30:36][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.0000,	0.1162 s / batch. (data: 9.61e-02). ETA=6:47:28, max mem: 0.4 GB 
[11/22 11:30:40][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 190.0997,	0.0219 s / batch. (data: 1.85e-04). ETA=1:16:43, max mem: 0.4 GB 
[11/22 11:30:45][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 3.1262,	0.0315 s / batch. (data: 8.99e-05). ETA=1:50:12, max mem: 0.4 GB 
[11/22 11:30:46][INFO] visual_prompt:  217: Epoch 5 / 100: avg data time: 1.88e-02, avg batch time: 0.0439, average train loss: 139.2349
[11/22 11:30:51][INFO] visual_prompt:  303: 	Test 100/246. loss: 138.428, 0.0132 s / batch. (data: 3.24e-05)max mem: 0.39113 GB 
[11/22 11:30:55][INFO] visual_prompt:  303: 	Test 200/246. loss: 136.433, 0.0130 s / batch. (data: 3.55e-05)max mem: 0.39113 GB 
[11/22 11:30:58][INFO] visual_prompt:  316: Inference (val):avg data time: 4.72e-05, avg batch time: 0.0123, average loss: 62.4834
[11/22 11:30:58][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.44	
[11/22 11:30:58][INFO] visual_prompt:  165: Training 6 / 100 epoch, with learning rate 25.0
[11/22 11:31:03][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 208.9840,	0.1084 s / batch. (data: 9.08e-02). ETA=6:19:28, max mem: 0.4 GB 
[11/22 11:31:07][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 61.9815,	0.0338 s / batch. (data: 9.20e-05). ETA=1:58:16, max mem: 0.4 GB 
[11/22 11:31:11][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.0000,	0.0308 s / batch. (data: 1.39e-04). ETA=1:47:45, max mem: 0.4 GB 
[11/22 11:31:15][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.0000,	0.0239 s / batch. (data: 8.92e-05). ETA=1:23:36, max mem: 0.4 GB 
[11/22 11:31:19][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.0000,	0.1085 s / batch. (data: 8.41e-02). ETA=6:19:00, max mem: 0.4 GB 
[11/22 11:31:23][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.0000,	0.0225 s / batch. (data: 5.41e-05). ETA=1:18:28, max mem: 0.4 GB 
[11/22 11:31:27][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.0000,	0.0220 s / batch. (data: 5.03e-05). ETA=1:16:37, max mem: 0.4 GB 
[11/22 11:31:30][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.0000,	0.0209 s / batch. (data: 4.98e-05). ETA=1:12:50, max mem: 0.4 GB 
[11/22 11:31:34][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 340.4715,	0.0237 s / batch. (data: 7.58e-05). ETA=1:22:49, max mem: 0.4 GB 
[11/22 11:31:37][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 425.3497,	0.0565 s / batch. (data: 4.27e-02). ETA=3:17:05, max mem: 0.4 GB 
[11/22 11:31:41][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 318.5586,	0.0216 s / batch. (data: 8.13e-05). ETA=1:15:18, max mem: 0.4 GB 
[11/22 11:31:45][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.0000,	0.0329 s / batch. (data: 1.86e-04). ETA=1:54:26, max mem: 0.4 GB 
[11/22 11:31:49][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.0000,	0.0290 s / batch. (data: 1.02e-04). ETA=1:40:48, max mem: 0.4 GB 
[11/22 11:31:54][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 951.2058,	0.0139 s / batch. (data: 5.22e-05). ETA=0:48:21, max mem: 0.4 GB 
[11/22 11:31:59][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 45.1124,	0.0201 s / batch. (data: 5.82e-05). ETA=1:09:48, max mem: 0.4 GB 
[11/22 11:32:04][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 315.4565,	0.0261 s / batch. (data: 1.54e-04). ETA=1:30:39, max mem: 0.4 GB 
[11/22 11:32:09][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 55.9680,	0.0156 s / batch. (data: 1.32e-04). ETA=0:54:01, max mem: 0.4 GB 
[11/22 11:32:14][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.0000,	0.0212 s / batch. (data: 7.51e-05). ETA=1:13:41, max mem: 0.4 GB 
[11/22 11:32:19][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.0000,	0.0333 s / batch. (data: 9.80e-05). ETA=1:55:25, max mem: 0.4 GB 
[11/22 11:32:23][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.0000,	0.0226 s / batch. (data: 5.87e-05). ETA=1:18:21, max mem: 0.4 GB 
[11/22 11:32:28][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.0000,	0.0784 s / batch. (data: 6.37e-02). ETA=4:31:53, max mem: 0.4 GB 
[11/22 11:32:33][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.0000,	0.0374 s / batch. (data: 1.35e-04). ETA=2:09:43, max mem: 0.4 GB 
[11/22 11:32:34][INFO] visual_prompt:  217: Epoch 6 / 100: avg data time: 1.84e-02, avg batch time: 0.0434, average train loss: 166.0422
[11/22 11:32:39][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.000, 0.0060 s / batch. (data: 6.20e-06)max mem: 0.39113 GB 
[11/22 11:32:44][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.000, 0.0064 s / batch. (data: 1.16e-04)max mem: 0.39113 GB 
[11/22 11:32:46][INFO] visual_prompt:  316: Inference (val):avg data time: 3.42e-05, avg batch time: 0.0094, average loss: 73.1813
[11/22 11:32:46][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.66	
[11/22 11:32:46][INFO] visual_prompt:  165: Training 7 / 100 epoch, with learning rate 30.0
[11/22 11:32:52][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 192.5403,	0.0277 s / batch. (data: 1.05e-04). ETA=1:35:52, max mem: 0.4 GB 
[11/22 11:32:56][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 318.2131,	0.0291 s / batch. (data: 9.94e-05). ETA=1:40:41, max mem: 0.4 GB 
[11/22 11:33:00][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.0000,	0.0275 s / batch. (data: 9.16e-05). ETA=1:35:14, max mem: 0.4 GB 
[11/22 11:33:05][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.0000,	0.0224 s / batch. (data: 8.68e-05). ETA=1:17:34, max mem: 0.4 GB 
[11/22 11:33:10][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 722.9432,	0.0223 s / batch. (data: 8.92e-05). ETA=1:16:59, max mem: 0.4 GB 
[11/22 11:33:14][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.0000,	0.0219 s / batch. (data: 8.94e-05). ETA=1:15:44, max mem: 0.4 GB 
[11/22 11:33:18][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.0000,	0.0230 s / batch. (data: 1.01e-04). ETA=1:19:22, max mem: 0.4 GB 
[11/22 11:33:23][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.0000,	0.0242 s / batch. (data: 9.61e-05). ETA=1:23:40, max mem: 0.4 GB 
[11/22 11:33:27][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.0000,	0.0151 s / batch. (data: 5.60e-05). ETA=0:52:03, max mem: 0.4 GB 
[11/22 11:33:32][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.0000,	0.0153 s / batch. (data: 1.08e-04). ETA=0:52:36, max mem: 0.4 GB 
[11/22 11:33:37][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.0000,	0.0224 s / batch. (data: 8.73e-05). ETA=1:17:18, max mem: 0.4 GB 
[11/22 11:33:42][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.5264,	0.0130 s / batch. (data: 5.27e-05). ETA=0:44:51, max mem: 0.4 GB 
[11/22 11:33:46][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 226.8843,	0.2197 s / batch. (data: 1.98e-01). ETA=12:36:45, max mem: 0.4 GB 
[11/22 11:33:50][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.0000,	0.0269 s / batch. (data: 1.52e-04). ETA=1:32:40, max mem: 0.4 GB 
[11/22 11:33:55][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.0000,	0.0277 s / batch. (data: 1.79e-04). ETA=1:35:16, max mem: 0.4 GB 
[11/22 11:33:58][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.0000,	0.0458 s / batch. (data: 1.74e-04). ETA=2:37:30, max mem: 0.4 GB 
[11/22 11:34:03][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.0962,	0.0149 s / batch. (data: 1.30e-04). ETA=0:51:16, max mem: 0.4 GB 
[11/22 11:34:08][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 81.9510,	0.1045 s / batch. (data: 8.21e-02). ETA=5:58:56, max mem: 0.4 GB 
[11/22 11:34:12][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.0000,	0.0138 s / batch. (data: 1.00e-04). ETA=0:47:22, max mem: 0.4 GB 
[11/22 11:34:17][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.0000,	0.1585 s / batch. (data: 1.45e-01). ETA=9:03:56, max mem: 0.4 GB 
[11/22 11:34:21][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.0000,	0.0370 s / batch. (data: 2.33e-02). ETA=2:07:03, max mem: 0.4 GB 
[11/22 11:34:26][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.0000,	0.1910 s / batch. (data: 1.78e-01). ETA=10:55:00, max mem: 0.4 GB 
[11/22 11:34:27][INFO] visual_prompt:  217: Epoch 7 / 100: avg data time: 2.30e-02, avg batch time: 0.0453, average train loss: 180.3731
[11/22 11:34:32][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.000, 0.0073 s / batch. (data: 2.62e-05)max mem: 0.39113 GB 
[11/22 11:34:36][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.000, 0.0066 s / batch. (data: 2.26e-05)max mem: 0.39113 GB 
[11/22 11:34:39][INFO] visual_prompt:  316: Inference (val):avg data time: 1.90e-05, avg batch time: 0.0066, average loss: 399.0679
[11/22 11:34:39][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 50.42	
[11/22 11:34:39][INFO] visual_prompt:  165: Training 8 / 100 epoch, with learning rate 35.0
[11/22 11:34:43][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.0000,	0.0350 s / batch. (data: 8.37e-05). ETA=2:00:03, max mem: 0.4 GB 
[11/22 11:34:47][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.0000,	0.0269 s / batch. (data: 1.50e-04). ETA=1:32:05, max mem: 0.4 GB 
[11/22 11:34:50][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.0000,	0.0278 s / batch. (data: 9.23e-05). ETA=1:35:11, max mem: 0.4 GB 
[11/22 11:34:54][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 0.0000,	0.0246 s / batch. (data: 8.42e-05). ETA=1:24:09, max mem: 0.4 GB 
[11/22 11:34:57][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 303.8009,	0.0218 s / batch. (data: 7.89e-05). ETA=1:14:27, max mem: 0.4 GB 
[11/22 11:35:02][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.0000,	0.0305 s / batch. (data: 8.23e-05). ETA=1:44:24, max mem: 0.4 GB 
[11/22 11:35:05][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.0000,	0.0302 s / batch. (data: 1.45e-04). ETA=1:43:05, max mem: 0.4 GB 
[11/22 11:35:09][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 216.6557,	0.0213 s / batch. (data: 9.04e-05). ETA=1:12:50, max mem: 0.4 GB 
[11/22 11:35:13][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.0000,	0.0229 s / batch. (data: 1.66e-04). ETA=1:18:17, max mem: 0.4 GB 
[11/22 11:35:18][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.0000,	0.0228 s / batch. (data: 8.20e-05). ETA=1:17:56, max mem: 0.4 GB 
[11/22 11:35:22][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.0000,	0.0216 s / batch. (data: 9.42e-05). ETA=1:13:35, max mem: 0.4 GB 
[11/22 11:35:26][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 105.0151,	0.0244 s / batch. (data: 9.42e-05). ETA=1:23:12, max mem: 0.4 GB 
[11/22 11:35:30][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 1383.8340,	0.0232 s / batch. (data: 8.49e-05). ETA=1:18:54, max mem: 0.4 GB 
[11/22 11:35:34][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.0000,	0.0216 s / batch. (data: 1.00e-04). ETA=1:13:25, max mem: 0.4 GB 
[11/22 11:35:39][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.0001,	0.0204 s / batch. (data: 9.16e-05). ETA=1:09:20, max mem: 0.4 GB 
[11/22 11:35:43][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.0000,	0.0210 s / batch. (data: 1.55e-04). ETA=1:11:26, max mem: 0.4 GB 
[11/22 11:35:48][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 397.7932,	0.0262 s / batch. (data: 7.68e-05). ETA=1:29:08, max mem: 0.4 GB 
[11/22 11:35:51][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 53.0936,	0.0206 s / batch. (data: 9.18e-05). ETA=1:09:59, max mem: 0.4 GB 
[11/22 11:35:56][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.0000,	0.0203 s / batch. (data: 8.27e-05). ETA=1:08:49, max mem: 0.4 GB 
[11/22 11:35:59][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 360.6141,	0.0201 s / batch. (data: 7.68e-05). ETA=1:08:21, max mem: 0.4 GB 
[11/22 11:36:04][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 398.3463,	0.0278 s / batch. (data: 2.33e-04). ETA=1:34:25, max mem: 0.4 GB 
[11/22 11:36:08][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 754.5866,	0.0219 s / batch. (data: 8.01e-05). ETA=1:14:26, max mem: 0.4 GB 
[11/22 11:36:10][INFO] visual_prompt:  217: Epoch 8 / 100: avg data time: 1.64e-02, avg batch time: 0.0407, average train loss: 232.4086
[11/22 11:36:15][INFO] visual_prompt:  303: 	Test 100/246. loss: 643.094, 0.0064 s / batch. (data: 2.62e-05)max mem: 0.39113 GB 
[11/22 11:36:20][INFO] visual_prompt:  303: 	Test 200/246. loss: 640.706, 0.0111 s / batch. (data: 8.85e-05)max mem: 0.39113 GB 
[11/22 11:36:22][INFO] visual_prompt:  316: Inference (val):avg data time: 3.14e-05, avg batch time: 0.0079, average loss: 289.1686
[11/22 11:36:22][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.53	
[11/22 11:36:22][INFO] visual_prompt:  165: Training 9 / 100 epoch, with learning rate 40.0
[11/22 11:36:27][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 0.0000,	0.0294 s / batch. (data: 2.04e-04). ETA=1:39:46, max mem: 0.4 GB 
[11/22 11:36:32][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 163.9158,	0.0705 s / batch. (data: 5.29e-02). ETA=3:58:46, max mem: 0.4 GB 
[11/22 11:36:36][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.0000,	0.0250 s / batch. (data: 7.34e-05). ETA=1:24:49, max mem: 0.4 GB 
[11/22 11:36:41][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 255.5691,	0.0252 s / batch. (data: 6.27e-05). ETA=1:25:08, max mem: 0.4 GB 
[11/22 11:36:45][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 681.9912,	0.1205 s / batch. (data: 9.81e-02). ETA=6:47:42, max mem: 0.4 GB 
[11/22 11:36:50][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 782.4445,	0.0212 s / batch. (data: 5.94e-05). ETA=1:11:45, max mem: 0.4 GB 
[11/22 11:36:54][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.0000,	0.1594 s / batch. (data: 1.37e-01). ETA=8:58:38, max mem: 0.4 GB 
[11/22 11:36:59][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.0000,	0.0305 s / batch. (data: 9.32e-05). ETA=1:43:07, max mem: 0.4 GB 
[11/22 11:37:04][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.0000,	0.0225 s / batch. (data: 1.70e-04). ETA=1:15:49, max mem: 0.4 GB 
[11/22 11:37:08][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.0000,	0.0213 s / batch. (data: 6.46e-05). ETA=1:12:02, max mem: 0.4 GB 
[11/22 11:37:13][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 1011.5637,	0.0228 s / batch. (data: 1.52e-04). ETA=1:16:56, max mem: 0.4 GB 
[11/22 11:37:17][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.0000,	0.0237 s / batch. (data: 4.79e-05). ETA=1:19:48, max mem: 0.4 GB 
[11/22 11:37:22][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 263.7813,	0.0341 s / batch. (data: 2.38e-04). ETA=1:54:47, max mem: 0.4 GB 
[11/22 11:37:27][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.0000,	0.0304 s / batch. (data: 2.02e-04). ETA=1:42:24, max mem: 0.4 GB 
[11/22 11:37:31][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.0000,	0.0209 s / batch. (data: 8.75e-05). ETA=1:10:18, max mem: 0.4 GB 
[11/22 11:37:37][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 308.5577,	0.1748 s / batch. (data: 1.52e-01). ETA=9:48:16, max mem: 0.4 GB 
[11/22 11:37:42][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 1077.2246,	0.0218 s / batch. (data: 9.27e-05). ETA=1:13:14, max mem: 0.4 GB 
[11/22 11:37:47][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.0000,	0.0294 s / batch. (data: 9.23e-05). ETA=1:38:56, max mem: 0.4 GB 
[11/22 11:37:52][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 0.0000,	0.0297 s / batch. (data: 2.21e-04). ETA=1:39:43, max mem: 0.4 GB 
[11/22 11:37:56][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.0000,	0.0304 s / batch. (data: 9.94e-05). ETA=1:42:07, max mem: 0.4 GB 
[11/22 11:38:00][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 182.4563,	0.0209 s / batch. (data: 5.72e-05). ETA=1:10:13, max mem: 0.4 GB 
[11/22 11:38:05][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 636.5981,	0.0267 s / batch. (data: 7.89e-05). ETA=1:29:25, max mem: 0.4 GB 
[11/22 11:38:06][INFO] visual_prompt:  217: Epoch 9 / 100: avg data time: 2.17e-02, avg batch time: 0.0465, average train loss: 230.9275
[11/22 11:38:12][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.000, 0.0125 s / batch. (data: 5.56e-05)max mem: 0.39113 GB 
[11/22 11:38:16][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.000, 0.0111 s / batch. (data: 6.60e-05)max mem: 0.39113 GB 
[11/22 11:38:19][INFO] visual_prompt:  316: Inference (val):avg data time: 4.05e-05, avg batch time: 0.0119, average loss: 25.4405
[11/22 11:38:19][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.32	
[11/22 11:38:19][INFO] visual_prompt:  165: Training 10 / 100 epoch, with learning rate 45.0
[11/22 11:38:25][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 1208.8164,	0.0861 s / batch. (data: 6.19e-02). ETA=4:48:38, max mem: 0.4 GB 
[11/22 11:38:30][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 123.5191,	0.0271 s / batch. (data: 6.48e-05). ETA=1:30:42, max mem: 0.4 GB 
[11/22 11:38:34][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 1161.6492,	0.0353 s / batch. (data: 9.99e-05). ETA=1:58:07, max mem: 0.4 GB 
[11/22 11:38:39][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 2157.6155,	0.0349 s / batch. (data: 1.13e-04). ETA=1:56:45, max mem: 0.4 GB 
[11/22 11:38:43][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.0000,	0.0294 s / batch. (data: 9.89e-05). ETA=1:38:24, max mem: 0.4 GB 
[11/22 11:38:48][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.0123,	0.0249 s / batch. (data: 9.99e-05). ETA=1:23:13, max mem: 0.4 GB 
[11/22 11:38:52][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 0.0000,	0.0258 s / batch. (data: 1.71e-04). ETA=1:26:19, max mem: 0.4 GB 
[11/22 11:38:57][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.0000,	0.0224 s / batch. (data: 7.03e-05). ETA=1:14:58, max mem: 0.4 GB 
[11/22 11:39:01][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 1513.9414,	0.0235 s / batch. (data: 1.82e-04). ETA=1:18:35, max mem: 0.4 GB 
[11/22 11:39:06][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.0159,	0.0304 s / batch. (data: 9.30e-05). ETA=1:41:29, max mem: 0.4 GB 
[11/22 11:39:10][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.0000,	0.0218 s / batch. (data: 6.20e-05). ETA=1:12:41, max mem: 0.4 GB 
[11/22 11:39:15][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 398.5022,	0.0224 s / batch. (data: 5.91e-05). ETA=1:14:41, max mem: 0.4 GB 
[11/22 11:39:19][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 1350.1412,	0.0229 s / batch. (data: 7.30e-05). ETA=1:16:16, max mem: 0.4 GB 
[11/22 11:39:24][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 620.0048,	0.0295 s / batch. (data: 1.67e-04). ETA=1:38:24, max mem: 0.4 GB 
[11/22 11:39:28][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 496.8769,	0.0309 s / batch. (data: 8.08e-05). ETA=1:43:01, max mem: 0.4 GB 
[11/22 11:39:33][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.0000,	0.0244 s / batch. (data: 1.48e-04). ETA=1:21:20, max mem: 0.4 GB 
[11/22 11:39:37][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 0.0000,	0.0300 s / batch. (data: 1.42e-04). ETA=1:39:43, max mem: 0.4 GB 
[11/22 11:39:42][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 525.2484,	0.0285 s / batch. (data: 1.47e-04). ETA=1:34:48, max mem: 0.4 GB 
[11/22 11:39:47][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 382.3144,	0.0214 s / batch. (data: 1.83e-04). ETA=1:10:57, max mem: 0.4 GB 
[11/22 11:39:52][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 0.0000,	0.0139 s / batch. (data: 5.65e-05). ETA=0:46:11, max mem: 0.4 GB 
[11/22 11:39:57][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.0000,	0.0212 s / batch. (data: 9.37e-05). ETA=1:10:22, max mem: 0.4 GB 
[11/22 11:40:02][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 838.6051,	0.0611 s / batch. (data: 3.91e-02). ETA=3:22:47, max mem: 0.4 GB 
[11/22 11:40:03][INFO] visual_prompt:  217: Epoch 10 / 100: avg data time: 1.96e-02, avg batch time: 0.0466, average train loss: 242.0603
[11/22 11:40:09][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.000, 0.0120 s / batch. (data: 1.62e-05)max mem: 0.39113 GB 
[11/22 11:40:14][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.000, 0.0067 s / batch. (data: 2.43e-05)max mem: 0.39113 GB 
[11/22 11:40:16][INFO] visual_prompt:  316: Inference (val):avg data time: 3.04e-05, avg batch time: 0.0100, average loss: 500.1685
[11/22 11:40:16][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 52.85	
[11/22 11:40:16][INFO] visual_prompt:  165: Training 11 / 100 epoch, with learning rate 50.0
[11/22 11:40:22][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 932.8094,	0.0364 s / batch. (data: 8.15e-03). ETA=2:00:42, max mem: 0.4 GB 
[11/22 11:40:26][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.0000,	0.0904 s / batch. (data: 6.17e-02). ETA=4:59:39, max mem: 0.4 GB 
[11/22 11:40:30][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 392.5385,	0.1198 s / batch. (data: 9.21e-02). ETA=6:36:57, max mem: 0.4 GB 
[11/22 11:40:35][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 565.5043,	0.1617 s / batch. (data: 1.44e-01). ETA=8:55:22, max mem: 0.4 GB 
[11/22 11:40:39][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 0.0000,	0.0270 s / batch. (data: 1.56e-04). ETA=1:29:19, max mem: 0.4 GB 
[11/22 11:40:44][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.0000,	0.0353 s / batch. (data: 1.59e-02). ETA=1:56:45, max mem: 0.4 GB 
[11/22 11:40:49][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 188.8881,	0.1691 s / batch. (data: 1.30e-01). ETA=9:19:01, max mem: 0.4 GB 
[11/22 11:40:53][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 228.1256,	0.0228 s / batch. (data: 8.68e-05). ETA=1:15:13, max mem: 0.4 GB 
[11/22 11:40:58][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.0000,	0.0381 s / batch. (data: 1.26e-04). ETA=2:05:49, max mem: 0.4 GB 
[11/22 11:41:02][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 1162.8359,	0.0222 s / batch. (data: 5.60e-05). ETA=1:13:09, max mem: 0.4 GB 
[11/22 11:41:06][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 1359.0649,	0.0701 s / batch. (data: 4.63e-02). ETA=3:51:11, max mem: 0.4 GB 
[11/22 11:41:11][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.0000,	0.0246 s / batch. (data: 1.85e-04). ETA=1:20:58, max mem: 0.4 GB 
[11/22 11:41:15][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 0.0000,	0.0240 s / batch. (data: 1.58e-04). ETA=1:19:02, max mem: 0.4 GB 
[11/22 11:41:20][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 0.0000,	0.0267 s / batch. (data: 1.65e-04). ETA=1:27:54, max mem: 0.4 GB 
[11/22 11:41:25][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.0000,	0.1005 s / batch. (data: 8.53e-02). ETA=5:30:55, max mem: 0.4 GB 
[11/22 11:41:30][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.0000,	0.0259 s / batch. (data: 1.77e-04). ETA=1:25:08, max mem: 0.4 GB 
[11/22 11:41:34][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 751.5884,	0.1092 s / batch. (data: 7.51e-02). ETA=5:59:10, max mem: 0.4 GB 
[11/22 11:41:39][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 184.9771,	0.0149 s / batch. (data: 1.22e-04). ETA=0:48:59, max mem: 0.4 GB 
[11/22 11:41:43][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 414.8223,	0.0237 s / batch. (data: 2.06e-04). ETA=1:17:47, max mem: 0.4 GB 
[11/22 11:41:48][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 898.8256,	0.0142 s / batch. (data: 6.29e-05). ETA=0:46:32, max mem: 0.4 GB 
[11/22 11:41:53][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 83.8722,	0.1415 s / batch. (data: 1.18e-01). ETA=7:44:36, max mem: 0.4 GB 
[11/22 11:41:58][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 357.9888,	0.0996 s / batch. (data: 7.30e-02). ETA=5:26:40, max mem: 0.4 GB 
[11/22 11:42:00][INFO] visual_prompt:  217: Epoch 11 / 100: avg data time: 2.28e-02, avg batch time: 0.0464, average train loss: 306.9387
[11/22 11:42:06][INFO] visual_prompt:  303: 	Test 100/246. loss: 0.200, 0.0136 s / batch. (data: 6.79e-05)max mem: 0.39113 GB 
[11/22 11:42:10][INFO] visual_prompt:  303: 	Test 200/246. loss: 0.431, 0.0075 s / batch. (data: 7.30e-05)max mem: 0.39113 GB 
[11/22 11:42:14][INFO] visual_prompt:  316: Inference (val):avg data time: 4.63e-05, avg batch time: 0.0117, average loss: 0.8609
[11/22 11:42:14][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 46.34	rocauc: 46.23	
[11/22 11:42:14][INFO] visual_prompt:  165: Training 12 / 100 epoch, with learning rate 49.9847706754774
[11/22 11:42:20][INFO] visual_prompt:  204: 	Training 100/2212. train loss: 887.5117,	0.0184 s / batch. (data: 8.08e-05). ETA=1:00:24, max mem: 0.4 GB 
[11/22 11:42:25][INFO] visual_prompt:  204: 	Training 200/2212. train loss: 0.0000,	0.0173 s / batch. (data: 6.60e-05). ETA=0:56:41, max mem: 0.4 GB 
[11/22 11:42:29][INFO] visual_prompt:  204: 	Training 300/2212. train loss: 0.0000,	0.0422 s / batch. (data: 2.03e-04). ETA=2:18:07, max mem: 0.4 GB 
[11/22 11:42:34][INFO] visual_prompt:  204: 	Training 400/2212. train loss: 1152.8958,	0.0287 s / batch. (data: 1.59e-04). ETA=1:34:02, max mem: 0.4 GB 
[11/22 11:42:39][INFO] visual_prompt:  204: 	Training 500/2212. train loss: 1044.3108,	0.0476 s / batch. (data: 9.61e-05). ETA=2:35:46, max mem: 0.4 GB 
[11/22 11:42:44][INFO] visual_prompt:  204: 	Training 600/2212. train loss: 0.0000,	0.0443 s / batch. (data: 2.63e-04). ETA=2:24:58, max mem: 0.4 GB 
[11/22 11:42:49][INFO] visual_prompt:  204: 	Training 700/2212. train loss: 1055.2762,	0.0291 s / batch. (data: 2.22e-04). ETA=1:35:17, max mem: 0.4 GB 
[11/22 11:42:54][INFO] visual_prompt:  204: 	Training 800/2212. train loss: 0.0000,	0.0373 s / batch. (data: 2.04e-04). ETA=2:01:46, max mem: 0.4 GB 
[11/22 11:42:59][INFO] visual_prompt:  204: 	Training 900/2212. train loss: 0.0000,	0.0421 s / batch. (data: 9.01e-05). ETA=2:17:25, max mem: 0.4 GB 
[11/22 11:43:04][INFO] visual_prompt:  204: 	Training 1000/2212. train loss: 0.0000,	0.0252 s / batch. (data: 7.01e-05). ETA=1:22:15, max mem: 0.4 GB 
[11/22 11:43:09][INFO] visual_prompt:  204: 	Training 1100/2212. train loss: 0.0000,	0.0954 s / batch. (data: 7.87e-02). ETA=5:11:25, max mem: 0.4 GB 
[11/22 11:43:13][INFO] visual_prompt:  204: 	Training 1200/2212. train loss: 0.0000,	0.0303 s / batch. (data: 8.92e-05). ETA=1:38:48, max mem: 0.4 GB 
[11/22 11:43:18][INFO] visual_prompt:  204: 	Training 1300/2212. train loss: 564.1704,	0.1285 s / batch. (data: 9.79e-02). ETA=6:58:54, max mem: 0.4 GB 
[11/22 11:43:23][INFO] visual_prompt:  204: 	Training 1400/2212. train loss: 883.7784,	0.0671 s / batch. (data: 4.18e-02). ETA=3:38:33, max mem: 0.4 GB 
[11/22 11:43:27][INFO] visual_prompt:  204: 	Training 1500/2212. train loss: 0.0000,	0.0256 s / batch. (data: 1.13e-04). ETA=1:23:22, max mem: 0.4 GB 
[11/22 11:43:31][INFO] visual_prompt:  204: 	Training 1600/2212. train loss: 0.0000,	0.0322 s / batch. (data: 1.53e-04). ETA=1:44:52, max mem: 0.4 GB 
[11/22 11:43:35][INFO] visual_prompt:  204: 	Training 1700/2212. train loss: 875.3388,	0.0256 s / batch. (data: 6.15e-05). ETA=1:23:10, max mem: 0.4 GB 
[11/22 11:43:39][INFO] visual_prompt:  204: 	Training 1800/2212. train loss: 0.0000,	0.0312 s / batch. (data: 9.70e-05). ETA=1:41:23, max mem: 0.4 GB 
[11/22 11:43:44][INFO] visual_prompt:  204: 	Training 1900/2212. train loss: 1298.8545,	0.0243 s / batch. (data: 9.20e-05). ETA=1:19:00, max mem: 0.4 GB 
[11/22 11:43:49][INFO] visual_prompt:  204: 	Training 2000/2212. train loss: 1319.7349,	0.1519 s / batch. (data: 1.21e-01). ETA=8:13:18, max mem: 0.4 GB 
[11/22 11:43:53][INFO] visual_prompt:  204: 	Training 2100/2212. train loss: 0.0021,	0.0408 s / batch. (data: 9.44e-03). ETA=2:12:21, max mem: 0.4 GB 
[11/22 11:43:58][INFO] visual_prompt:  204: 	Training 2200/2212. train loss: 0.0000,	0.1296 s / batch. (data: 1.08e-01). ETA=7:00:38, max mem: 0.4 GB 
[11/22 11:44:00][INFO] visual_prompt:  217: Epoch 12 / 100: avg data time: 1.77e-02, avg batch time: 0.0472, average train loss: 305.5328
[11/22 11:44:06][INFO] visual_prompt:  303: 	Test 100/246. loss: 531.127, 0.0115 s / batch. (data: 2.72e-05)max mem: 0.39113 GB 
[11/22 11:44:10][INFO] visual_prompt:  303: 	Test 200/246. loss: 529.107, 0.0106 s / batch. (data: 1.26e-05)max mem: 0.39113 GB 
[11/22 11:44:13][INFO] visual_prompt:  316: Inference (val):avg data time: 3.43e-05, avg batch time: 0.0113, average loss: 238.1308
[11/22 11:44:13][INFO] visual_prompt:  113: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 53.22	
[11/22 11:44:13][INFO] visual_prompt:   42: Stopping early.

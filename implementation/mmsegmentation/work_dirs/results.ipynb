{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from mmengine.config import Config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T22:50:37.764610966Z",
     "start_time": "2023-11-29T22:50:37.055294032Z"
    }
   },
   "id": "cf0b841cfe1fadb8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-29T22:50:39.536772700Z",
     "start_time": "2023-11-29T22:50:39.514773404Z"
    }
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "def listdir(*rel_paths):\n",
    "    return os.listdir(os.path.join(current_dir, *rel_paths))\n",
    "\n",
    "def isdir(*rel_paths):\n",
    "    return os.path.isdir(os.path.join(current_dir, *rel_paths))\n",
    "\n",
    "def join(*rel_paths):\n",
    "    return os.path.join(current_dir, *rel_paths)\n",
    "\n",
    "def isfile(*rel_paths):\n",
    "    return os.path.isfile(join(*rel_paths))\n",
    "\n",
    "\n",
    "def get_log_runs():\n",
    "    work_dirs = [d for d in os.listdir() if isdir(d)]\n",
    "    print(f'work dirs: {len(work_dirs)}')\n",
    "    log_runs = [(work_dir, d) for work_dir in work_dirs for d in listdir(work_dir) if isdir(work_dir,d)]\n",
    "    print(f'log runs: {len(log_runs)}')\n",
    "    return log_runs\n",
    "\n",
    "\n",
    "def started_train_run(work_dir, d):\n",
    "    return isfile(work_dir, d, 'vis_data', f'{d}.json')\n",
    "\n",
    "def started_test_run(work_dir, d):\n",
    "    return isfile(work_dir, d, f'{d}.json')\n",
    "\n",
    "def get_logs_df():\n",
    "    log_runs = get_log_runs()\n",
    "    return pd.DataFrame(log_runs, columns=['work_dir', 'log_run'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "I want to get the following information, if possible:\n",
    "- filter for train runs, not test\n",
    "- config contents\n",
    "    - model\n",
    "        - hyperparameters (batch size, learning rate, weight decay, momentum, prompt depth)\n",
    "    - dataset\n",
    "\n",
    "- total time spent during training\n",
    "- percentage of trainable parameters\n",
    "    - trainable parameters\n",
    "    - total parameters\n",
    "- for training runs:\n",
    "    - training curve\n",
    "    - validation curve\n",
    "    - memory profile\n",
    "    - whether there is a corresponding test run?\n",
    "- for test runs:\n",
    "    - test results\n",
    "    - test mode (slide or whole)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3a416c915cdfb0d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work dirs: 4\n",
      "log runs: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                           work_dir          log_run  train  \\\n0     setr_vit-l_pup-noaux_8xb2-160k_ade20k-512x512  20231119_220357   True   \n1  setrvpt_vit-l_pup-noaux_8xb2-160k_ade20k-512x512  20231125_124611   True   \n2        setrvpt_vit-l_pup_8xb2-160k_ade20k-512x512  20231120_011426   True   \n3        setrvpt_vit-l_pup_8xb2-160k_ade20k-512x512  20231025_174532   True   \n4        setrvpt_vit-l_pup_8xb2-160k_ade20k-512x512  20231018_192725   True   \n5           setr_vit-l_pup_8xb2-160k_ade20k-512x512  20231023_140016   True   \n6           setr_vit-l_pup_8xb2-160k_ade20k-512x512  20231119_220403   True   \n7           setr_vit-l_pup_8xb2-160k_ade20k-512x512  20231017_224030  False   \n8           setr_vit-l_pup_8xb2-160k_ade20k-512x512  20231014_232627  False   \n9           setr_vit-l_pup_8xb2-160k_ade20k-512x512  20231023_123801  False   \n\n    test                   backbone  pretrained                  checkpoint  \\\n0  False          VisionTransformer        True  pretrain/vit_large_p16.pth   \n1  False  PromptedVisionTransformer        True  pretrain/vit_large_p16.pth   \n2  False  PromptedVisionTransformer        True  pretrain/vit_large_p16.pth   \n3  False  PromptedVisionTransformer        True  pretrain/vit_large_p16.pth   \n4  False  PromptedVisionTransformer        True  pretrain/vit_large_p16.pth   \n5  False          VisionTransformer        True  pretrain/vit_large_p16.pth   \n6  False          VisionTransformer        True  pretrain/vit_large_p16.pth   \n7   True          VisionTransformer        True  pretrain/vit_large_p16.pth   \n8   True          VisionTransformer        True  pretrain/vit_large_p16.pth   \n9   True          VisionTransformer        True  pretrain/vit_large_p16.pth   \n\n                                           load_from  auxhead  train_dataset  \\\n0                                               None    False  ADE20KDataset   \n1                                               None    False  ADE20KDataset   \n2                                               None     True  ADE20KDataset   \n3                                               None     True  ADE20KDataset   \n4                                               None     True  ADE20KDataset   \n5                                               None     True  ADE20KDataset   \n6                                               None     True  ADE20KDataset   \n7  checkpoints/setr_pup_512x512_160k_b16_ade20k_2...     True  ADE20KDataset   \n8  checkpoints/setr_pup_512x512_160k_b16_ade20k_2...     True  ADE20KDataset   \n9  checkpoints/setr_pup_512x512_160k_b16_ade20k_2...     True  ADE20KDataset   \n\n   ...  momentum prompt_depth prompt_length  prompt_dropout  memory_file  \\\n0  ...       0.9          NaN           NaN             NaN         None   \n1  ...       0.9         24.0          50.0             0.1         None   \n2  ...       0.9         24.0          50.0             0.1         None   \n3  ...       0.9         24.0          50.0             0.1         None   \n4  ...       0.9         24.0          50.0             0.1         None   \n5  ...       0.9          NaN           NaN             NaN         None   \n6  ...       0.9          NaN           NaN             NaN         None   \n7  ...       0.9          NaN           NaN             NaN         None   \n8  ...       0.9          NaN           NaN             NaN         None   \n9  ...       0.9          NaN           NaN             NaN         None   \n\n   total_parameters  trainable_parameters  trainable_parameters_percentage  \\\n0               NaN                   NaN                              NaN   \n1       309546646.0             5400214.0                         0.017446   \n2               NaN                   NaN                              NaN   \n3               NaN                   NaN                              NaN   \n4               NaN                   NaN                              NaN   \n5               NaN                   NaN                              NaN   \n6               NaN                   NaN                              NaN   \n7               NaN                   NaN                              NaN   \n8               NaN                   NaN                              NaN   \n9               NaN                   NaN                              NaN   \n\n         duration                                           run_info  \n0 3 days 02:53:09  [{'base_lr': 0.000999751932150666, 'lr': 0.000...  \n1 3 days 06:57:31  [{'base_lr': 0.0004998897476225205, 'lr': 0.00...  \n2 2 days 05:50:09  [{'base_lr': 0.0004998897476225205, 'lr': 0.00...  \n3 2 days 05:17:51  [{'base_lr': 0.0004998897476225205, 'lr': 0.00...  \n4 1 days 21:29:44  [{'base_lr': 0.0004998897476225205, 'lr': 0.00...  \n5 2 days 02:54:34  [{'base_lr': 0.000999751932150666, 'lr': 0.000...  \n6 2 days 04:13:26  [{'base_lr': 0.000999751932150666, 'lr': 0.000...  \n7 0 days 00:11:39  [{'aAcc': 80.57, 'mIoU': 44.28, 'mAcc': 55.66,...  \n8 0 days 00:11:35  [{'aAcc': 80.57, 'mIoU': 44.28, 'mAcc': 55.66,...  \n9 0 days 00:11:40  [{'aAcc': 80.57, 'mIoU': 44.28, 'mAcc': 55.66,...  \n\n[10 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>work_dir</th>\n      <th>log_run</th>\n      <th>train</th>\n      <th>test</th>\n      <th>backbone</th>\n      <th>pretrained</th>\n      <th>checkpoint</th>\n      <th>load_from</th>\n      <th>auxhead</th>\n      <th>train_dataset</th>\n      <th>...</th>\n      <th>momentum</th>\n      <th>prompt_depth</th>\n      <th>prompt_length</th>\n      <th>prompt_dropout</th>\n      <th>memory_file</th>\n      <th>total_parameters</th>\n      <th>trainable_parameters</th>\n      <th>trainable_parameters_percentage</th>\n      <th>duration</th>\n      <th>run_info</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>setr_vit-l_pup-noaux_8xb2-160k_ade20k-512x512</td>\n      <td>20231119_220357</td>\n      <td>True</td>\n      <td>False</td>\n      <td>VisionTransformer</td>\n      <td>True</td>\n      <td>pretrain/vit_large_p16.pth</td>\n      <td>None</td>\n      <td>False</td>\n      <td>ADE20KDataset</td>\n      <td>...</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3 days 02:53:09</td>\n      <td>[{'base_lr': 0.000999751932150666, 'lr': 0.000...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>setrvpt_vit-l_pup-noaux_8xb2-160k_ade20k-512x512</td>\n      <td>20231125_124611</td>\n      <td>True</td>\n      <td>False</td>\n      <td>PromptedVisionTransformer</td>\n      <td>True</td>\n      <td>pretrain/vit_large_p16.pth</td>\n      <td>None</td>\n      <td>False</td>\n      <td>ADE20KDataset</td>\n      <td>...</td>\n      <td>0.9</td>\n      <td>24.0</td>\n      <td>50.0</td>\n      <td>0.1</td>\n      <td>None</td>\n      <td>309546646.0</td>\n      <td>5400214.0</td>\n      <td>0.017446</td>\n      <td>3 days 06:57:31</td>\n      <td>[{'base_lr': 0.0004998897476225205, 'lr': 0.00...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>setrvpt_vit-l_pup_8xb2-160k_ade20k-512x512</td>\n      <td>20231120_011426</td>\n      <td>True</td>\n      <td>False</td>\n      <td>PromptedVisionTransformer</td>\n      <td>True</td>\n      <td>pretrain/vit_large_p16.pth</td>\n      <td>None</td>\n      <td>True</td>\n      <td>ADE20KDataset</td>\n      <td>...</td>\n      <td>0.9</td>\n      <td>24.0</td>\n      <td>50.0</td>\n      <td>0.1</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2 days 05:50:09</td>\n      <td>[{'base_lr': 0.0004998897476225205, 'lr': 0.00...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>setrvpt_vit-l_pup_8xb2-160k_ade20k-512x512</td>\n      <td>20231025_174532</td>\n      <td>True</td>\n      <td>False</td>\n      <td>PromptedVisionTransformer</td>\n      <td>True</td>\n      <td>pretrain/vit_large_p16.pth</td>\n      <td>None</td>\n      <td>True</td>\n      <td>ADE20KDataset</td>\n      <td>...</td>\n      <td>0.9</td>\n      <td>24.0</td>\n      <td>50.0</td>\n      <td>0.1</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2 days 05:17:51</td>\n      <td>[{'base_lr': 0.0004998897476225205, 'lr': 0.00...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>setrvpt_vit-l_pup_8xb2-160k_ade20k-512x512</td>\n      <td>20231018_192725</td>\n      <td>True</td>\n      <td>False</td>\n      <td>PromptedVisionTransformer</td>\n      <td>True</td>\n      <td>pretrain/vit_large_p16.pth</td>\n      <td>None</td>\n      <td>True</td>\n      <td>ADE20KDataset</td>\n      <td>...</td>\n      <td>0.9</td>\n      <td>24.0</td>\n      <td>50.0</td>\n      <td>0.1</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1 days 21:29:44</td>\n      <td>[{'base_lr': 0.0004998897476225205, 'lr': 0.00...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>setr_vit-l_pup_8xb2-160k_ade20k-512x512</td>\n      <td>20231023_140016</td>\n      <td>True</td>\n      <td>False</td>\n      <td>VisionTransformer</td>\n      <td>True</td>\n      <td>pretrain/vit_large_p16.pth</td>\n      <td>None</td>\n      <td>True</td>\n      <td>ADE20KDataset</td>\n      <td>...</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2 days 02:54:34</td>\n      <td>[{'base_lr': 0.000999751932150666, 'lr': 0.000...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>setr_vit-l_pup_8xb2-160k_ade20k-512x512</td>\n      <td>20231119_220403</td>\n      <td>True</td>\n      <td>False</td>\n      <td>VisionTransformer</td>\n      <td>True</td>\n      <td>pretrain/vit_large_p16.pth</td>\n      <td>None</td>\n      <td>True</td>\n      <td>ADE20KDataset</td>\n      <td>...</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2 days 04:13:26</td>\n      <td>[{'base_lr': 0.000999751932150666, 'lr': 0.000...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>setr_vit-l_pup_8xb2-160k_ade20k-512x512</td>\n      <td>20231017_224030</td>\n      <td>False</td>\n      <td>True</td>\n      <td>VisionTransformer</td>\n      <td>True</td>\n      <td>pretrain/vit_large_p16.pth</td>\n      <td>checkpoints/setr_pup_512x512_160k_b16_ade20k_2...</td>\n      <td>True</td>\n      <td>ADE20KDataset</td>\n      <td>...</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0 days 00:11:39</td>\n      <td>[{'aAcc': 80.57, 'mIoU': 44.28, 'mAcc': 55.66,...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>setr_vit-l_pup_8xb2-160k_ade20k-512x512</td>\n      <td>20231014_232627</td>\n      <td>False</td>\n      <td>True</td>\n      <td>VisionTransformer</td>\n      <td>True</td>\n      <td>pretrain/vit_large_p16.pth</td>\n      <td>checkpoints/setr_pup_512x512_160k_b16_ade20k_2...</td>\n      <td>True</td>\n      <td>ADE20KDataset</td>\n      <td>...</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0 days 00:11:35</td>\n      <td>[{'aAcc': 80.57, 'mIoU': 44.28, 'mAcc': 55.66,...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>setr_vit-l_pup_8xb2-160k_ade20k-512x512</td>\n      <td>20231023_123801</td>\n      <td>False</td>\n      <td>True</td>\n      <td>VisionTransformer</td>\n      <td>True</td>\n      <td>pretrain/vit_large_p16.pth</td>\n      <td>checkpoints/setr_pup_512x512_160k_b16_ade20k_2...</td>\n      <td>True</td>\n      <td>ADE20KDataset</td>\n      <td>...</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0 days 00:11:40</td>\n      <td>[{'aAcc': 80.57, 'mIoU': 44.28, 'mAcc': 55.66,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 26 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_config(logs_row):\n",
    "    return Config.fromfile(join(logs_row['work_dir'], logs_row['log_run'], 'vis_data', 'config.py'))\n",
    "\n",
    "def extract_pretrain_config_info(logs):\n",
    "    def get_checkpoint(row):\n",
    "        if row['config'].model.backbone.init_cfg.type == 'Pretrained':\n",
    "            return row['config'].model.backbone.init_cfg.checkpoint\n",
    "        return None\n",
    "    def get_load_from(row):\n",
    "        return row['config'].load_from\n",
    "    logs['checkpoint'] = logs.apply(get_checkpoint, axis=1)\n",
    "    logs['load_from'] = logs.apply(get_load_from, axis=1)\n",
    "    return logs\n",
    "\n",
    "def extract_prompt_config_info(logs):\n",
    "    logs['prompt_depth'] = logs.apply(lambda row: row['config'].model.backbone.prompt_cfg.depth if row['backbone'] == 'PromptedVisionTransformer' else None, axis=1)\n",
    "    logs['prompt_length'] = logs.apply(lambda row: row['config'].model.backbone.prompt_cfg.length if row['backbone'] == 'PromptedVisionTransformer' else None , axis=1)\n",
    "    logs['prompt_dropout'] = logs.apply(lambda row: row['config'].model.backbone.prompt_cfg.dropout if row['backbone'] == 'PromptedVisionTransformer' else None, axis=1)\n",
    "    return logs\n",
    "\n",
    "def extract_memory_file(logs):\n",
    "    memory_files = logs.apply(lambda row: join(row['work_dir'], row['log_run'], 'memory_snapshot.pickle'), axis=1)\n",
    "    logs['memory_file'] = memory_files.apply(lambda path: path if isfile(path) else None)\n",
    "    return logs\n",
    "    \n",
    "def extract_config_info(logs):\n",
    "    configs = logs_df.apply(load_config, axis=1)\n",
    "    logs_df['config'] = configs\n",
    "    logs['backbone'] = configs.apply(lambda config: config.model.backbone.type)\n",
    "    logs['pretrained'] = configs.apply(lambda config: config.model.backbone.init_cfg.type == 'Pretrained')\n",
    "    logs = extract_pretrain_config_info(logs)\n",
    "    logs['auxhead'] = configs.apply(lambda config: len(config.model.auxiliary_head) > 0)\n",
    "    logs['train_dataset'] = configs.apply(lambda config: config.train_dataloader.dataset.type)\n",
    "    logs['train_iters'] = configs.apply(lambda config: config.train_cfg.max_iters)\n",
    "    logs['test_dataset'] = configs.apply(lambda config: config.test_dataloader.dataset.type)\n",
    "    logs['test_mode'] = configs.apply(lambda config: config.model.test_cfg.mode)\n",
    "    logs['batch_size'] = configs.apply(lambda config: config.train_dataloader.batch_size)\n",
    "    logs['learning_rate'] = configs.apply(lambda config: config.optimizer.lr)\n",
    "    logs['weight_decay'] = configs.apply(lambda config: config.optimizer.weight_decay)\n",
    "    logs['momentum'] = configs.apply(lambda config: config.optimizer.momentum)\n",
    "    logs = extract_prompt_config_info(logs)\n",
    "    logs = extract_memory_file(logs)\n",
    "    logs = logs.drop(columns=['config'])\n",
    "    return logs\n",
    "\n",
    "\n",
    "def readlogs(row):\n",
    "    with open(join(row['work_dir'], row['log_run'], f'{row[\"log_run\"]}.log'), 'r') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def extract_total_parameters(log_contents):\n",
    "    for line in log_contents:\n",
    "        if 'Total parameters' in line:\n",
    "            return int(line.split('Total parameters: ')[1].strip())\n",
    "    return None\n",
    "\n",
    "def extract_trainable_parameters(log_contents):\n",
    "    for line in log_contents:\n",
    "        if 'Trainable parameters' in line:\n",
    "            return int(line.split('Trainable parameters: ')[1].strip())\n",
    "    return None\n",
    "\n",
    "def extract_time_at_index(log_contents, index):\n",
    "    line = log_contents[index]\n",
    "    return datetime.strptime(line.split('- mmengine -')[0].strip(), '%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "def extract_start_time(log_contents):\n",
    "    return extract_time_at_index(log_contents, 0)\n",
    "\n",
    "def extract_end_time(log_contents):\n",
    "    return extract_time_at_index(log_contents, -1)\n",
    "\n",
    "def extract_log_info(logs):\n",
    "    log_contents = logs.apply(readlogs, axis=1)\n",
    "    logs['total_parameters'] = log_contents.apply(extract_total_parameters)\n",
    "    logs['trainable_parameters'] = log_contents.apply(extract_trainable_parameters)\n",
    "    logs['trainable_parameters_percentage'] = logs['trainable_parameters'] / logs['total_parameters']\n",
    "    logs['duration'] = log_contents.apply(extract_end_time) - log_contents.apply(extract_start_time)\n",
    "    return logs\n",
    "\n",
    "def get_train_run_info(row):\n",
    "    with open(join(row['work_dir'], row['log_run'], 'vis_data', f'{row[\"log_run\"]}.json'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        return [json.loads(line) for line in lines]\n",
    "\n",
    "def get_test_run_info(row):\n",
    "    with open(join(row['work_dir'], row['log_run'], f'{row[\"log_run\"]}.json'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        return [json.loads(line) for line in lines]\n",
    "\n",
    "def get_train_time(row):\n",
    "    if not row['train']:\n",
    "        return None\n",
    "    run_info = row['run_info']\n",
    "    train_outputs = [line for line in run_info if line.get('loss') is not None]\n",
    "    return sum([line['time'] for line in train_outputs])\n",
    "\n",
    "def extract_run_info(logs):\n",
    "    run_info = logs.apply(lambda row: get_train_run_info(row) if row['train'] else get_test_run_info(row), axis=1)\n",
    "    logs['run_info'] = run_info\n",
    "    return logs\n",
    "    \n",
    "\n",
    "logs_df = get_logs_df()\n",
    "logs_df['train'] = logs_df.apply(lambda row: started_train_run(row['work_dir'], row['log_run']), axis=1)\n",
    "logs_df['test'] = logs_df.apply(lambda row: started_test_run(row['work_dir'], row['log_run']), axis=1)\n",
    "logs_df = extract_config_info(logs_df)\n",
    "logs_df = extract_log_info(logs_df)\n",
    "logs_df = extract_run_info(logs_df)\n",
    "display(logs_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T22:50:43.261497171Z",
     "start_time": "2023-11-29T22:50:42.137589804Z"
    }
   },
   "id": "6fa0dfff2c4c4611"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Displaying training curves, test results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f019b023a3b2c801"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "179898bb67be13b8"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def parse_train_line(line):\n",
    "    parsed_line = {}\n",
    "    t_str = line.split('- mmengine -')[0].strip()\n",
    "    t = datetime.strptime(t_str, '%Y/%m/%d %H:%M:%S')\n",
    "    iteration_str = line.split('Iter(train) [')[1].split(']')[0].strip().split('/')[0]\n",
    "    iteration = int(iteration_str)\n",
    "    \n",
    "    parsed_line['at_iteration'] = iteration\n",
    "    parsed_line['time'] = t\n",
    "    parsed_line['type'] = 'train'\n",
    "    return parsed_line\n",
    "    \n",
    "\n",
    "def parse_val_line(line):\n",
    "    parsed_line = {}\n",
    "    t_str = line.split('- mmengine -')[0].strip()\n",
    "    t = datetime.strptime(t_str, '%Y/%m/%d %H:%M:%S')\n",
    "    iteration_str = line.split('Iter(val) [')[1].split(']')[0].strip().split('/')[0]\n",
    "    iteration = int(iteration_str)\n",
    "    \n",
    "    parsed_line['at_iteration'] = iteration\n",
    "    parsed_line['time'] = t\n",
    "    parsed_line['type'] = 'val'\n",
    "    return parsed_line\n",
    "\n",
    "def calculate_time_after(parsed_lines):\n",
    "    for i, line in enumerate(parsed_lines):\n",
    "        line['duration_after'] = timedelta(0)\n",
    "        if i < len(parsed_lines) - 1:\n",
    "            line['duration_after'] = parsed_lines[i+1]['time'] - line['time']\n",
    "\n",
    "def calculate_time_before(parsed_lines):\n",
    "    for i, line in enumerate(parsed_lines):\n",
    "        line['duration_before'] = timedelta(0)\n",
    "        if i > 0:\n",
    "            line['duration_before'] = line['time'] - parsed_lines[i-1]['time']\n",
    "        \n",
    "def calculate_inbetween_times(parsed_lines):\n",
    "    calculate_time_after(parsed_lines)\n",
    "    calculate_time_before(parsed_lines)\n",
    "\n",
    "def calculate_iterations_per_line(parsed_lines):\n",
    "    for i, line in enumerate(parsed_lines):\n",
    "        if i > 0 and line['type'] == parsed_lines[i-1]['type']:\n",
    "            line['iterations'] = line['at_iteration'] - parsed_lines[i-1]['at_iteration']\n",
    "            continue\n",
    "        line['iterations'] = line['at_iteration']\n",
    "\n",
    "def calculate_duration_per_iteration(parsed_lines):\n",
    "    for line in parsed_lines:\n",
    "        line['duration_per_iteration'] = line['duration_before'] / line['iterations']        \n",
    "    \n",
    "\n",
    "def parse_logfile(rel_log_path):\n",
    "    parsed_lines = []\n",
    "    train_pattern = \"Iter(train)\"\n",
    "    val_pattern = \"Iter(val)\"\n",
    "    with open(os.path.join(current_dir, rel_log_path), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parsed_line = None\n",
    "            if train_pattern in line:\n",
    "                parsed_line = parse_train_line(line)\n",
    "            if val_pattern in line:\n",
    "                parsed_line = parse_val_line(line)\n",
    "            if parsed_line is None:\n",
    "                continue\n",
    "            parsed_lines.append(parsed_line)\n",
    "    # for now, let's filter out lines that report the same iteration as a previous line\n",
    "    parsed_lines = [parsed_line for i, parsed_line in enumerate(parsed_lines) if i == 0 or parsed_line['at_iteration'] != parsed_lines[i-1]['at_iteration']]\n",
    "    calculate_inbetween_times(parsed_lines)\n",
    "    calculate_iterations_per_line(parsed_lines)\n",
    "    calculate_duration_per_iteration(parsed_lines)\n",
    "    return parsed_lines\n",
    "            \n",
    "parsed_logfiles = {}\n",
    "for logfile in logfiles:\n",
    "    parsed_logfiles[logfile] = parse_logfile(logfile)\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:00:30.643407113Z",
     "start_time": "2023-11-19T16:00:30.625232895Z"
    }
   },
   "id": "b43e87da490e3de2"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "      at_iteration                time   type  duration_after duration_before  \\\n0               50 2023-10-23 14:02:39  train 0 days 00:00:57 0 days 00:00:00   \n1              100 2023-10-23 14:03:36  train 0 days 00:00:58 0 days 00:00:57   \n2              150 2023-10-23 14:04:34  train 0 days 00:00:57 0 days 00:00:58   \n3              200 2023-10-23 14:05:31  train 0 days 00:00:56 0 days 00:00:57   \n4              250 2023-10-23 14:06:27  train 0 days 00:00:57 0 days 00:00:56   \n...            ...                 ...    ...             ...             ...   \n3295           300 2023-10-25 16:54:35    val 0 days 00:00:06 0 days 00:00:08   \n3296           350 2023-10-25 16:54:41    val 0 days 00:00:08 0 days 00:00:06   \n3297           400 2023-10-25 16:54:49    val 0 days 00:00:07 0 days 00:00:08   \n3298           450 2023-10-25 16:54:56    val 0 days 00:00:07 0 days 00:00:07   \n3299           500 2023-10-25 16:55:03    val 0 days 00:00:00 0 days 00:00:07   \n\n      iterations duration_per_iteration  \n0             50        0 days 00:00:00  \n1             50 0 days 00:00:01.140000  \n2             50 0 days 00:00:01.160000  \n3             50 0 days 00:00:01.140000  \n4             50 0 days 00:00:01.120000  \n...          ...                    ...  \n3295          50 0 days 00:00:00.160000  \n3296          50 0 days 00:00:00.120000  \n3297          50 0 days 00:00:00.160000  \n3298          50 0 days 00:00:00.140000  \n3299          50 0 days 00:00:00.140000  \n\n[3300 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>at_iteration</th>\n      <th>time</th>\n      <th>type</th>\n      <th>duration_after</th>\n      <th>duration_before</th>\n      <th>iterations</th>\n      <th>duration_per_iteration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50</td>\n      <td>2023-10-23 14:02:39</td>\n      <td>train</td>\n      <td>0 days 00:00:57</td>\n      <td>0 days 00:00:00</td>\n      <td>50</td>\n      <td>0 days 00:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100</td>\n      <td>2023-10-23 14:03:36</td>\n      <td>train</td>\n      <td>0 days 00:00:58</td>\n      <td>0 days 00:00:57</td>\n      <td>50</td>\n      <td>0 days 00:00:01.140000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>150</td>\n      <td>2023-10-23 14:04:34</td>\n      <td>train</td>\n      <td>0 days 00:00:57</td>\n      <td>0 days 00:00:58</td>\n      <td>50</td>\n      <td>0 days 00:00:01.160000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>200</td>\n      <td>2023-10-23 14:05:31</td>\n      <td>train</td>\n      <td>0 days 00:00:56</td>\n      <td>0 days 00:00:57</td>\n      <td>50</td>\n      <td>0 days 00:00:01.140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>250</td>\n      <td>2023-10-23 14:06:27</td>\n      <td>train</td>\n      <td>0 days 00:00:57</td>\n      <td>0 days 00:00:56</td>\n      <td>50</td>\n      <td>0 days 00:00:01.120000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3295</th>\n      <td>300</td>\n      <td>2023-10-25 16:54:35</td>\n      <td>val</td>\n      <td>0 days 00:00:06</td>\n      <td>0 days 00:00:08</td>\n      <td>50</td>\n      <td>0 days 00:00:00.160000</td>\n    </tr>\n    <tr>\n      <th>3296</th>\n      <td>350</td>\n      <td>2023-10-25 16:54:41</td>\n      <td>val</td>\n      <td>0 days 00:00:08</td>\n      <td>0 days 00:00:06</td>\n      <td>50</td>\n      <td>0 days 00:00:00.120000</td>\n    </tr>\n    <tr>\n      <th>3297</th>\n      <td>400</td>\n      <td>2023-10-25 16:54:49</td>\n      <td>val</td>\n      <td>0 days 00:00:07</td>\n      <td>0 days 00:00:08</td>\n      <td>50</td>\n      <td>0 days 00:00:00.160000</td>\n    </tr>\n    <tr>\n      <th>3298</th>\n      <td>450</td>\n      <td>2023-10-25 16:54:56</td>\n      <td>val</td>\n      <td>0 days 00:00:07</td>\n      <td>0 days 00:00:07</td>\n      <td>50</td>\n      <td>0 days 00:00:00.140000</td>\n    </tr>\n    <tr>\n      <th>3299</th>\n      <td>500</td>\n      <td>2023-10-25 16:55:03</td>\n      <td>val</td>\n      <td>0 days 00:00:00</td>\n      <td>0 days 00:00:07</td>\n      <td>50</td>\n      <td>0 days 00:00:00.140000</td>\n    </tr>\n  </tbody>\n</table>\n<p>3300 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "       duration_after                                                      \\\n                  sum                      mean                       std   \ntype                                                                        \ntrain 2 days 02:32:47 0 days 00:00:56.864687500 0 days 00:00:02.158056626   \nval   0 days 00:19:37    0 days 00:00:11.770000 0 days 00:00:14.779767087   \n\n      duration_before                                                   \\\n                  sum                   mean                       std   \ntype                                                                     \ntrain 2 days 02:38:12 0 days 00:00:56.966250 0 days 00:00:01.182390725   \nval   0 days 00:14:12 0 days 00:00:08.520000 0 days 00:00:04.083670363   \n\n      iterations                        duration_per_iteration  \\\n             sum   mean          std                      mean   \ntype                                                             \ntrain     880000  275.0  4770.375918 0 days 00:00:01.136046960   \nval         5000   50.0     0.000000    0 days 00:00:00.170400   \n\n                                 \n                            std  \ntype                             \ntrain 0 days 00:00:00.064732105  \nval   0 days 00:00:00.081673407  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"3\" halign=\"left\">duration_after</th>\n      <th colspan=\"3\" halign=\"left\">duration_before</th>\n      <th colspan=\"3\" halign=\"left\">iterations</th>\n      <th colspan=\"2\" halign=\"left\">duration_per_iteration</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>sum</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>sum</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>sum</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n    <tr>\n      <th>type</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>2 days 02:32:47</td>\n      <td>0 days 00:00:56.864687500</td>\n      <td>0 days 00:00:02.158056626</td>\n      <td>2 days 02:38:12</td>\n      <td>0 days 00:00:56.966250</td>\n      <td>0 days 00:00:01.182390725</td>\n      <td>880000</td>\n      <td>275.0</td>\n      <td>4770.375918</td>\n      <td>0 days 00:00:01.136046960</td>\n      <td>0 days 00:00:00.064732105</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>0 days 00:19:37</td>\n      <td>0 days 00:00:11.770000</td>\n      <td>0 days 00:00:14.779767087</td>\n      <td>0 days 00:14:12</td>\n      <td>0 days 00:00:08.520000</td>\n      <td>0 days 00:00:04.083670363</td>\n      <td>5000</td>\n      <td>50.0</td>\n      <td>0.000000</td>\n      <td>0 days 00:00:00.170400</td>\n      <td>0 days 00:00:00.081673407</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of time spent on training vs validation:\n",
      "type\n",
      "train    0.995348\n",
      "val      0.004652\n",
      "Name: sum, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for logfile in logfiles:\n",
    "    parsed_logfile = parsed_logfiles[logfile]\n",
    "    if len(parsed_logfile) == 0:\n",
    "        continue\n",
    "    df = pd.DataFrame(parsed_logfiles[logfile])\n",
    "    display(df)\n",
    "    aggregate_df = df.groupby('type').agg({'duration_after': ['sum', 'mean', 'std'], 'duration_before': ['sum', 'mean', 'std'], 'iterations': ['sum', 'mean', 'std'], 'duration_per_iteration': ['mean', 'std']})\n",
    "    display(aggregate_df)\n",
    "\n",
    "    # percentage of time spent on training vs validation:\n",
    "    print('percentage of time spent on training vs validation:')\n",
    "    print(aggregate_df['duration_before']['sum'] / aggregate_df['duration_before']['sum'].sum())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:00:30.671791310Z",
     "start_time": "2023-11-19T16:00:30.658403158Z"
    }
   },
   "id": "66fd6e5082bc208e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

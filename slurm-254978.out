/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
/home/s1952889/miniconda3/envs/segment_anything/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/s1952889/miniconda3/envs/segment_anything/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
{'model': {'optimizer': {'name': 'sgd', 'lr': 0.01, 'wd': 0.0005, 'momentum': 0.9}, 'finetuning': {'name': 'full'}, 'name': 'sam', 'checkpoint': 'checkpoints/sam_vit_l_0b3195.pth', 'backbone': 'vit_l', 'loss': {'reduction': 'mean', 'parts': [{'name': 'Dice', 'weight': 1}, {'name': 'Focal', 'weight': 20}]}, 'metrics': [{'name': 'Dice'}, {'name': 'Focal'}, {'name': 'IoU'}]}, 'device': 'cuda', 'data': {'name': 'ade20k', 'root': 'data/ade/ADEChallengeData2016', 'image_extension': '.jpg', 'annotation_extension': '.png', 'preprocess': [{'name': 'resize', 'dimensions': [512, 512], 'mode': 'bilinear'}], 'train': {'image_dir': 'images/training', 'annotation_dir': 'annotations/training', 'batch_size': 8}, 'val': {'image_dir': 'images/validation', 'annotation_dir': 'annotations/validation', 'batch_size': 8}, 'test': {'image_dir': 'images/validation', 'annotation_dir': 'annotations/validation', 'batch_size': 1}}, '_bases_': ['finetune/configs/_base_/datasets/ade20k.yaml', 'finetune/configs/_base_/models/sam.yaml', 'finetune/configs/_base_/finetuning/full.yaml'], 'out': 'finetune/outputs', 'schedule': {'iterations': 160000, 'val_interval': 16000, 'log_interval': 1000}, 'out_dir': 'outputs'}
torch.Size([1, 3, 512, 512]) tensor(-13.0408, device='cuda:0', grad_fn=<MinBackward1>) tensor(13.7151, device='cuda:0', grad_fn=<MaxBackward1>)
torch.Size([1, 3, 512, 512]) tensor(0., device='cuda:0') tensor(1., device='cuda:0')
torch.Size([1, 3, 512, 512]) tensor(-13.0408, device='cuda:0', grad_fn=<MinBackward1>) tensor(13.7151, device='cuda:0', grad_fn=<MaxBackward1>)
torch.Size([1, 3, 512, 512]) tensor(0, device='cuda:0') tensor(1, device='cuda:0')
tensor(3.1559, device='cuda:0', grad_fn=<MeanBackward1>)
torch.Size([1, 3, 512, 512]) tensor(-12.5315, device='cuda:0', grad_fn=<MinBackward1>) tensor(12.4115, device='cuda:0', grad_fn=<MaxBackward1>)
torch.Size([1, 3, 512, 512]) tensor(0., device='cuda:0') tensor(1., device='cuda:0')
torch.Size([1, 3, 512, 512]) tensor(-12.5315, device='cuda:0', grad_fn=<MinBackward1>) tensor(12.4115, device='cuda:0', grad_fn=<MaxBackward1>)
torch.Size([1, 3, 512, 512]) tensor(0, device='cuda:0') tensor(1, device='cuda:0')
tensor(2.4060, device='cuda:0', grad_fn=<MeanBackward1>)
torch.Size([1, 3, 512, 512]) tensor(-68.0899, device='cuda:0', grad_fn=<MinBackward1>) tensor(30.0222, device='cuda:0', grad_fn=<MaxBackward1>)
torch.Size([1, 3, 512, 512]) tensor(0., device='cuda:0') tensor(1., device='cuda:0')
torch.Size([1, 3, 512, 512]) tensor(-68.0899, device='cuda:0', grad_fn=<MinBackward1>) tensor(30.0222, device='cuda:0', grad_fn=<MaxBackward1>)
torch.Size([1, 3, 512, 512]) tensor(0, device='cuda:0') tensor(1, device='cuda:0')
tensor(0.9427, device='cuda:0', grad_fn=<MeanBackward1>)
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/segment_anything/finetune/test.py", line 24, in <module>
    main()
  File "/home/s1952889/final-project/implementation/segment_anything/finetune/test.py", line 21, in main
    test(cfg)
  File "/home/s1952889/final-project/implementation/segment_anything/finetune/test.py", line 15, in test
    test_epoch(cfg, model, loss_function, metric_functions, dataloaders, logger)
  File "/home/s1952889/final-project/implementation/segment_anything/finetune/train.py", line 188, in test_epoch
    outputs = model(samples, foreground_points)
  File "/home/s1952889/final-project/implementation/segment_anything/finetune/models.py", line 47, in __call__
    image_embeddings = self.model.image_encoder(preprocessed_samples)
  File "/home/s1952889/miniconda3/envs/segment_anything/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s1952889/final-project/implementation/segment_anything/segment_anything/modeling/image_encoder.py", line 112, in forward
    x = blk(x)
  File "/home/s1952889/miniconda3/envs/segment_anything/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s1952889/final-project/implementation/segment_anything/segment_anything/modeling/image_encoder.py", line 174, in forward
    x = self.attn(x)
  File "/home/s1952889/miniconda3/envs/segment_anything/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s1952889/final-project/implementation/segment_anything/segment_anything/modeling/image_encoder.py", line 234, in forward
    attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))
  File "/home/s1952889/final-project/implementation/segment_anything/segment_anything/modeling/image_encoder.py", line 358, in add_decomposed_rel_pos
    attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 44.35 GiB total capacity; 43.03 GiB already allocated; 718.75 MiB free; 43.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)

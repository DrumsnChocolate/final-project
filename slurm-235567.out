[09/16 23:22:27 visual_prompt]: Rank of current process: 0. World size: 1
[09/16 23:22:27 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/16 23:22:27 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed42'], train_type='')
[09/16 23:22:27 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/16 23:22:27 visual_prompt]: Training with config:
[09/16 23:22:27 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")',
          'NO_TEST': False,
          'NUMBER_CLASSES': 9,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed42/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/16 23:22:27 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-16 23:22:27.477853: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-16 23:22:27.653488: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-16 23:22:28.652343: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 23:22:28.652425: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 23:22:28.652434: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-16 23:22:31.001048: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 23:22:31.001168: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-16 23:22:31.001186: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/16 23:22:31 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
2023-09-16 23:22:31.096506: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/16 23:22:33 visual_prompt]: Number of images: 1000
[09/16 23:22:33 visual_prompt]: Number of classes: 9 / 9
[09/16 23:22:33 visual_prompt]: Loading validation data...
[09/16 23:22:33 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/16 23:22:33 visual_prompt]: Number of images: 200
[09/16 23:22:33 visual_prompt]: Number of classes: 9 / 9
[09/16 23:22:33 visual_prompt]: Loading test data...
[09/16 23:22:33 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/16 23:22:50 visual_prompt]: Number of images: 12150
[09/16 23:22:50 visual_prompt]: Number of classes: 9 / 9
[09/16 23:22:50 visual_prompt]: Constructing models...
[09/16 23:22:53 visual_prompt]: Total Parameters: 86727177	 Gradient Parameters: 928521
[09/16 23:22:53 visual_prompt]: tuned percent:1.071
[09/16 23:22:55 visual_prompt]: Device used for model: 0
[09/16 23:22:55 visual_prompt]: Setting up Evalutator...
[09/16 23:22:55 visual_prompt]: Setting up Trainer...
[09/16 23:22:55 visual_prompt]: 	Setting up the optimizer...
[09/16 23:22:55 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/16 23:23:05 visual_prompt]: Epoch 1 / 100: avg data time: 9.37e-02, avg batch time: 0.5738, average train loss: 2.5199
[09/16 23:23:08 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1417, average loss: 2.5409
[09/16 23:23:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.50	top5: 52.00	
[09/16 23:23:29 visual_prompt]: 	Test 100/190. loss: 2.500, 0.1906 s / batch. (data: 9.75e-03)max mem: 17.22448 GB 
[09/16 23:23:47 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1937, average loss: 2.4808
[09/16 23:23:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.08	top5: 55.60	
[09/16 23:23:47 visual_prompt]: Best epoch 1: best metric: 0.075
[09/16 23:23:47 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/16 23:23:56 visual_prompt]: Epoch 2 / 100: avg data time: 7.97e-02, avg batch time: 0.4812, average train loss: 2.8882
[09/16 23:23:58 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1423, average loss: 2.4094
[09/16 23:23:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.00	
[09/16 23:24:19 visual_prompt]: 	Test 100/190. loss: 2.323, 0.2073 s / batch. (data: 1.13e-02)max mem: 17.22448 GB 
[09/16 23:24:37 visual_prompt]: Inference (test):avg data time: 5.95e-03, avg batch time: 0.1919, average loss: 2.3469
[09/16 23:24:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 55.67	
[09/16 23:24:37 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/16 23:24:46 visual_prompt]: Epoch 3 / 100: avg data time: 9.03e-02, avg batch time: 0.4889, average train loss: 2.2999
[09/16 23:24:48 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1426, average loss: 2.3523
[09/16 23:24:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/16 23:25:09 visual_prompt]: 	Test 100/190. loss: 2.228, 0.1847 s / batch. (data: 1.79e-04)max mem: 17.22448 GB 
[09/16 23:25:27 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1917, average loss: 2.3322
[09/16 23:25:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 55.69	
[09/16 23:25:27 visual_prompt]: Best epoch 3: best metric: 0.130
[09/16 23:25:27 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/16 23:25:36 visual_prompt]: Epoch 4 / 100: avg data time: 9.17e-02, avg batch time: 0.4928, average train loss: 2.4994
[09/16 23:25:38 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1426, average loss: 2.8257
[09/16 23:25:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 61.50	
[09/16 23:25:59 visual_prompt]: 	Test 100/190. loss: 2.939, 0.1821 s / batch. (data: 1.31e-04)max mem: 17.22448 GB 
[09/16 23:26:18 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1947, average loss: 3.0056
[09/16 23:26:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.19	
[09/16 23:26:18 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/16 23:26:26 visual_prompt]: Epoch 5 / 100: avg data time: 9.97e-02, avg batch time: 0.5015, average train loss: 2.7719
[09/16 23:26:29 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1424, average loss: 2.8788
[09/16 23:26:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 57.00	
[09/16 23:26:50 visual_prompt]: 	Test 100/190. loss: 2.905, 0.1924 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/16 23:27:08 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1918, average loss: 2.9535
[09/16 23:27:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.60	top5: 55.62	
[09/16 23:27:08 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/16 23:27:16 visual_prompt]: Epoch 6 / 100: avg data time: 9.16e-02, avg batch time: 0.4928, average train loss: 2.8920
[09/16 23:27:19 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1426, average loss: 3.2502
[09/16 23:27:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.50	
[09/16 23:27:40 visual_prompt]: 	Test 100/190. loss: 3.072, 0.1966 s / batch. (data: 1.46e-02)max mem: 17.22448 GB 
[09/16 23:27:58 visual_prompt]: Inference (test):avg data time: 7.16e-03, avg batch time: 0.1925, average loss: 3.1722
[09/16 23:27:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 55.25	
[09/16 23:27:58 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/16 23:28:06 visual_prompt]: Epoch 7 / 100: avg data time: 8.29e-02, avg batch time: 0.4860, average train loss: 3.4713
[09/16 23:28:09 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1427, average loss: 3.6622
[09/16 23:28:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.50	
[09/16 23:28:30 visual_prompt]: 	Test 100/190. loss: 4.469, 0.1827 s / batch. (data: 1.35e-04)max mem: 17.22448 GB 
[09/16 23:28:48 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1921, average loss: 3.8478
[09/16 23:28:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.88	
[09/16 23:28:48 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/16 23:28:57 visual_prompt]: Epoch 8 / 100: avg data time: 9.02e-02, avg batch time: 0.4957, average train loss: 3.9531
[09/16 23:29:00 visual_prompt]: Inference (val):avg data time: 3.18e-04, avg batch time: 0.2215, average loss: 4.3298
[09/16 23:29:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 48.50	
[09/16 23:29:21 visual_prompt]: 	Test 100/190. loss: 4.007, 0.1826 s / batch. (data: 1.33e-04)max mem: 17.22448 GB 
[09/16 23:29:39 visual_prompt]: Inference (test):avg data time: 8.59e-03, avg batch time: 0.1931, average loss: 4.1311
[09/16 23:29:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.44	
[09/16 23:29:39 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/16 23:29:47 visual_prompt]: Epoch 9 / 100: avg data time: 9.36e-02, avg batch time: 0.4946, average train loss: 7.8789
[09/16 23:29:50 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1428, average loss: 6.8353
[09/16 23:29:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/16 23:30:11 visual_prompt]: 	Test 100/190. loss: 8.313, 0.2105 s / batch. (data: 2.71e-02)max mem: 17.22448 GB 
[09/16 23:30:29 visual_prompt]: Inference (test):avg data time: 7.14e-03, avg batch time: 0.1922, average loss: 7.1759
[09/16 23:30:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.79	top5: 55.45	
[09/16 23:30:29 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/16 23:30:38 visual_prompt]: Epoch 10 / 100: avg data time: 8.66e-02, avg batch time: 0.4898, average train loss: 8.8914
[09/16 23:30:40 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1437, average loss: 28.9382
[09/16 23:30:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 54.00	
[09/16 23:31:01 visual_prompt]: 	Test 100/190. loss: 28.261, 0.1958 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/16 23:31:19 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1926, average loss: 28.5779
[09/16 23:31:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 55.59	
[09/16 23:31:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/16 23:31:28 visual_prompt]: Epoch 11 / 100: avg data time: 8.74e-02, avg batch time: 0.4894, average train loss: 28.7074
[09/16 23:31:31 visual_prompt]: Inference (val):avg data time: 4.60e-05, avg batch time: 0.1436, average loss: 27.2009
[09/16 23:31:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/16 23:31:51 visual_prompt]: 	Test 100/190. loss: 27.725, 0.2017 s / batch. (data: 1.77e-02)max mem: 17.22448 GB 
[09/16 23:32:10 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1923, average loss: 27.3810
[09/16 23:32:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 55.80	
[09/16 23:32:10 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/16 23:32:18 visual_prompt]: Epoch 12 / 100: avg data time: 9.04e-02, avg batch time: 0.4903, average train loss: 29.0650
[09/16 23:32:21 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1429, average loss: 26.2215
[09/16 23:32:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/16 23:32:42 visual_prompt]: 	Test 100/190. loss: 25.137, 0.1823 s / batch. (data: 1.09e-04)max mem: 17.22448 GB 
[09/16 23:33:00 visual_prompt]: Inference (test):avg data time: 6.57e-03, avg batch time: 0.1921, average loss: 26.8195
[09/16 23:33:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 56.27	
[09/16 23:33:00 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/16 23:33:09 visual_prompt]: Epoch 13 / 100: avg data time: 9.00e-02, avg batch time: 0.4981, average train loss: 18.8575
[09/16 23:33:11 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1427, average loss: 9.9216
[09/16 23:33:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 57.50	
[09/16 23:33:32 visual_prompt]: 	Test 100/190. loss: 10.628, 0.1827 s / batch. (data: 1.66e-04)max mem: 17.22448 GB 
[09/16 23:33:51 visual_prompt]: Inference (test):avg data time: 8.38e-03, avg batch time: 0.1948, average loss: 10.9017
[09/16 23:33:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.36	
[09/16 23:33:51 visual_prompt]: Best epoch 13: best metric: 0.145
[09/16 23:33:51 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/16 23:33:59 visual_prompt]: Epoch 14 / 100: avg data time: 9.41e-02, avg batch time: 0.4979, average train loss: 13.7171
[09/16 23:34:02 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1427, average loss: 10.1929
[09/16 23:34:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/16 23:34:23 visual_prompt]: 	Test 100/190. loss: 12.571, 0.2095 s / batch. (data: 1.42e-04)max mem: 17.22448 GB 
[09/16 23:34:42 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1954, average loss: 11.3375
[09/16 23:34:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.19	
[09/16 23:34:42 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/16 23:34:51 visual_prompt]: Epoch 15 / 100: avg data time: 1.07e-01, avg batch time: 0.5107, average train loss: 10.3866
[09/16 23:34:53 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1429, average loss: 11.6712
[09/16 23:34:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.00	
[09/16 23:35:14 visual_prompt]: 	Test 100/190. loss: 10.889, 0.1888 s / batch. (data: 1.24e-04)max mem: 17.22448 GB 
[09/16 23:35:33 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1946, average loss: 11.4867
[09/16 23:35:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.12	
[09/16 23:35:33 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/16 23:35:41 visual_prompt]: Epoch 16 / 100: avg data time: 9.50e-02, avg batch time: 0.4994, average train loss: 6.4901
[09/16 23:35:44 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1442, average loss: 6.6806
[09/16 23:35:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 53.00	
[09/16 23:36:04 visual_prompt]: 	Test 100/190. loss: 7.192, 0.1972 s / batch. (data: 1.53e-02)max mem: 17.22448 GB 
[09/16 23:36:23 visual_prompt]: Inference (test):avg data time: 6.00e-03, avg batch time: 0.1916, average loss: 6.9207
[09/16 23:36:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.81	
[09/16 23:36:23 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/16 23:36:32 visual_prompt]: Epoch 17 / 100: avg data time: 9.97e-02, avg batch time: 0.4993, average train loss: 5.0039
[09/16 23:36:34 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1429, average loss: 3.8991
[09/16 23:36:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/16 23:36:55 visual_prompt]: 	Test 100/190. loss: 4.595, 0.1831 s / batch. (data: 9.39e-05)max mem: 17.22448 GB 
[09/16 23:37:13 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1935, average loss: 4.1286
[09/16 23:37:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.57	
[09/16 23:37:13 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/16 23:37:22 visual_prompt]: Epoch 18 / 100: avg data time: 8.36e-02, avg batch time: 0.4864, average train loss: 3.8183
[09/16 23:37:24 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1429, average loss: 4.6282
[09/16 23:37:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 51.50	
[09/16 23:37:45 visual_prompt]: 	Test 100/190. loss: 4.674, 0.1830 s / batch. (data: 1.18e-04)max mem: 17.22448 GB 
[09/16 23:38:04 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1937, average loss: 4.3935
[09/16 23:38:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 55.79	
[09/16 23:38:04 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/16 23:38:12 visual_prompt]: Epoch 19 / 100: avg data time: 7.99e-02, avg batch time: 0.4866, average train loss: 4.2350
[09/16 23:38:15 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1429, average loss: 3.8924
[09/16 23:38:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 52.00	
[09/16 23:38:36 visual_prompt]: 	Test 100/190. loss: 4.013, 0.1837 s / batch. (data: 1.05e-04)max mem: 17.22448 GB 
[09/16 23:38:54 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1924, average loss: 3.8474
[09/16 23:38:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.29	
[09/16 23:38:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/16 23:39:03 visual_prompt]: Epoch 20 / 100: avg data time: 1.06e-01, avg batch time: 0.5095, average train loss: 3.9602
[09/16 23:39:06 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1428, average loss: 3.7594
[09/16 23:39:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/16 23:39:26 visual_prompt]: 	Test 100/190. loss: 3.987, 0.1832 s / batch. (data: 1.26e-04)max mem: 17.22448 GB 
[09/16 23:39:44 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1920, average loss: 3.9331
[09/16 23:39:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 55.20	
[09/16 23:39:44 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/16 23:39:53 visual_prompt]: Epoch 21 / 100: avg data time: 9.40e-02, avg batch time: 0.4997, average train loss: 3.0158
[09/16 23:39:56 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1428, average loss: 2.9840
[09/16 23:39:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/16 23:40:16 visual_prompt]: 	Test 100/190. loss: 3.154, 0.1820 s / batch. (data: 1.20e-04)max mem: 17.22448 GB 
[09/16 23:40:35 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1917, average loss: 3.1028
[09/16 23:40:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.34	
[09/16 23:40:35 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/16 23:40:43 visual_prompt]: Epoch 22 / 100: avg data time: 8.67e-02, avg batch time: 0.4899, average train loss: 3.0551
[09/16 23:40:46 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1428, average loss: 3.6460
[09/16 23:40:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/16 23:41:07 visual_prompt]: 	Test 100/190. loss: 4.327, 0.1926 s / batch. (data: 1.03e-02)max mem: 17.22448 GB 
[09/16 23:41:25 visual_prompt]: Inference (test):avg data time: 6.62e-03, avg batch time: 0.1920, average loss: 3.9930
[09/16 23:41:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.20	
[09/16 23:41:25 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/16 23:41:33 visual_prompt]: Epoch 23 / 100: avg data time: 9.91e-02, avg batch time: 0.4994, average train loss: 3.6392
[09/16 23:41:36 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1428, average loss: 3.5122
[09/16 23:41:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 48.50	
[09/16 23:41:57 visual_prompt]: 	Test 100/190. loss: 3.292, 0.1832 s / batch. (data: 1.35e-04)max mem: 17.22448 GB 
[09/16 23:42:15 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1929, average loss: 3.3141
[09/16 23:42:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.24	top5: 55.44	
[09/16 23:42:15 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/16 23:42:24 visual_prompt]: Epoch 24 / 100: avg data time: 9.46e-02, avg batch time: 0.4951, average train loss: 3.0960
[09/16 23:42:26 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1428, average loss: 2.9972
[09/16 23:42:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/16 23:42:47 visual_prompt]: 	Test 100/190. loss: 3.063, 0.1957 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/16 23:43:06 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1927, average loss: 2.8930
[09/16 23:43:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.85	
[09/16 23:43:06 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/16 23:43:14 visual_prompt]: Epoch 25 / 100: avg data time: 8.69e-02, avg batch time: 0.4887, average train loss: 2.9406
[09/16 23:43:17 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1428, average loss: 2.7556
[09/16 23:43:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.50	top5: 55.00	
[09/16 23:43:38 visual_prompt]: 	Test 100/190. loss: 2.588, 0.1845 s / batch. (data: 9.58e-05)max mem: 17.22448 GB 
[09/16 23:43:56 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1924, average loss: 2.7048
[09/16 23:43:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.01	top5: 55.88	
[09/16 23:43:56 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/16 23:44:04 visual_prompt]: Epoch 26 / 100: avg data time: 9.63e-02, avg batch time: 0.4980, average train loss: 3.0763
[09/16 23:44:07 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1426, average loss: 3.2737
[09/16 23:44:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 48.50	
[09/16 23:44:28 visual_prompt]: 	Test 100/190. loss: 3.102, 0.1833 s / batch. (data: 1.35e-04)max mem: 17.22448 GB 
[09/16 23:44:46 visual_prompt]: Inference (test):avg data time: 7.11e-03, avg batch time: 0.1937, average loss: 3.1635
[09/16 23:44:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.42	
[09/16 23:44:46 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/16 23:44:55 visual_prompt]: Epoch 27 / 100: avg data time: 9.62e-02, avg batch time: 0.4983, average train loss: 2.7384
[09/16 23:44:58 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1428, average loss: 2.7704
[09/16 23:44:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 62.00	
[09/16 23:45:18 visual_prompt]: 	Test 100/190. loss: 2.680, 0.1828 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/16 23:45:36 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1919, average loss: 2.7597
[09/16 23:45:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 56.60	
[09/16 23:45:37 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/16 23:45:45 visual_prompt]: Epoch 28 / 100: avg data time: 9.06e-02, avg batch time: 0.4923, average train loss: 2.4934
[09/16 23:45:48 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1427, average loss: 2.6430
[09/16 23:45:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/16 23:46:09 visual_prompt]: 	Test 100/190. loss: 2.904, 0.1831 s / batch. (data: 1.29e-04)max mem: 17.22448 GB 
[09/16 23:46:27 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1924, average loss: 2.7914
[09/16 23:46:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.33	
[09/16 23:46:27 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/16 23:46:35 visual_prompt]: Epoch 29 / 100: avg data time: 9.23e-02, avg batch time: 0.4925, average train loss: 2.5107
[09/16 23:46:38 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1447, average loss: 2.3809
[09/16 23:46:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 63.00	
[09/16 23:46:59 visual_prompt]: 	Test 100/190. loss: 2.619, 0.1826 s / batch. (data: 1.39e-04)max mem: 17.22448 GB 
[09/16 23:47:17 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1937, average loss: 2.4773
[09/16 23:47:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 56.18	
[09/16 23:47:17 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/16 23:47:26 visual_prompt]: Epoch 30 / 100: avg data time: 1.00e-01, avg batch time: 0.5023, average train loss: 2.5307
[09/16 23:47:29 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1430, average loss: 3.2175
[09/16 23:47:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/16 23:47:49 visual_prompt]: 	Test 100/190. loss: 3.374, 0.1831 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/16 23:48:08 visual_prompt]: Inference (test):avg data time: 8.50e-03, avg batch time: 0.1927, average loss: 3.1179
[09/16 23:48:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.10	
[09/16 23:48:08 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/16 23:48:16 visual_prompt]: Epoch 31 / 100: avg data time: 8.76e-02, avg batch time: 0.4907, average train loss: 2.6663
[09/16 23:48:19 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1427, average loss: 2.3913
[09/16 23:48:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 54.50	
[09/16 23:48:40 visual_prompt]: 	Test 100/190. loss: 2.205, 0.1824 s / batch. (data: 1.13e-04)max mem: 17.22448 GB 
[09/16 23:48:58 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1933, average loss: 2.3812
[09/16 23:48:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.71	top5: 55.33	
[09/16 23:48:58 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/16 23:49:07 visual_prompt]: Epoch 32 / 100: avg data time: 8.24e-02, avg batch time: 0.4843, average train loss: 2.4206
[09/16 23:49:09 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1428, average loss: 2.9456
[09/16 23:49:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/16 23:49:30 visual_prompt]: 	Test 100/190. loss: 3.176, 0.1828 s / batch. (data: 9.27e-05)max mem: 17.22448 GB 
[09/16 23:49:48 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1920, average loss: 2.9783
[09/16 23:49:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.22	
[09/16 23:49:48 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/16 23:49:57 visual_prompt]: Epoch 33 / 100: avg data time: 9.58e-02, avg batch time: 0.5013, average train loss: 2.6880
[09/16 23:50:00 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1425, average loss: 2.5601
[09/16 23:50:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 57.00	
[09/16 23:50:21 visual_prompt]: 	Test 100/190. loss: 2.629, 0.1827 s / batch. (data: 1.44e-04)max mem: 17.22448 GB 
[09/16 23:50:39 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1928, average loss: 2.6326
[09/16 23:50:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 54.16	
[09/16 23:50:39 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/16 23:50:47 visual_prompt]: Epoch 34 / 100: avg data time: 8.66e-02, avg batch time: 0.4867, average train loss: 2.5253
[09/16 23:50:51 visual_prompt]: Inference (val):avg data time: 3.28e-04, avg batch time: 0.2718, average loss: 2.9167
[09/16 23:50:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 51.50	
[09/16 23:51:11 visual_prompt]: 	Test 100/190. loss: 3.149, 0.1877 s / batch. (data: 1.21e-04)max mem: 17.22448 GB 
[09/16 23:51:29 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1919, average loss: 2.8516
[09/16 23:51:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 55.79	
[09/16 23:51:29 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/16 23:51:38 visual_prompt]: Epoch 35 / 100: avg data time: 9.17e-02, avg batch time: 0.4937, average train loss: 2.9025
[09/16 23:51:41 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1427, average loss: 2.2911
[09/16 23:51:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/16 23:52:02 visual_prompt]: 	Test 100/190. loss: 2.323, 0.1939 s / batch. (data: 1.23e-02)max mem: 17.22448 GB 
[09/16 23:52:20 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1939, average loss: 2.3654
[09/16 23:52:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 58.19	
[09/16 23:52:20 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/16 23:52:29 visual_prompt]: Epoch 36 / 100: avg data time: 8.46e-02, avg batch time: 0.4886, average train loss: 2.6036
[09/16 23:52:31 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1427, average loss: 2.6685
[09/16 23:52:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 54.50	
[09/16 23:52:52 visual_prompt]: 	Test 100/190. loss: 2.665, 0.1963 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/16 23:53:10 visual_prompt]: Inference (test):avg data time: 7.10e-03, avg batch time: 0.1919, average loss: 2.6290
[09/16 23:53:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.51	top5: 55.08	
[09/16 23:53:10 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/16 23:53:19 visual_prompt]: Epoch 37 / 100: avg data time: 9.19e-02, avg batch time: 0.4932, average train loss: 2.6377
[09/16 23:53:21 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1426, average loss: 2.4830
[09/16 23:53:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/16 23:53:42 visual_prompt]: 	Test 100/190. loss: 2.529, 0.1963 s / batch. (data: 1.42e-02)max mem: 17.22448 GB 
[09/16 23:54:00 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1928, average loss: 2.4541
[09/16 23:54:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.19	top5: 57.23	
[09/16 23:54:00 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/16 23:54:09 visual_prompt]: Epoch 38 / 100: avg data time: 9.02e-02, avg batch time: 0.4921, average train loss: 2.6626
[09/16 23:54:12 visual_prompt]: Inference (val):avg data time: 3.82e-04, avg batch time: 0.2282, average loss: 2.3590
[09/16 23:54:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 62.50	
[09/16 23:54:33 visual_prompt]: 	Test 100/190. loss: 2.541, 0.2078 s / batch. (data: 2.57e-02)max mem: 17.22448 GB 
[09/16 23:54:51 visual_prompt]: Inference (test):avg data time: 7.66e-03, avg batch time: 0.1919, average loss: 2.4575
[09/16 23:54:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.71	
[09/16 23:54:51 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/16 23:55:00 visual_prompt]: Epoch 39 / 100: avg data time: 1.01e-01, avg batch time: 0.5017, average train loss: 2.4429
[09/16 23:55:02 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1427, average loss: 2.5667
[09/16 23:55:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 67.00	
[09/16 23:55:23 visual_prompt]: 	Test 100/190. loss: 2.682, 0.1980 s / batch. (data: 1.44e-02)max mem: 17.22448 GB 
[09/16 23:55:41 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1932, average loss: 2.6679
[09/16 23:55:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 59.48	
[09/16 23:55:41 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/16 23:55:50 visual_prompt]: Epoch 40 / 100: avg data time: 9.17e-02, avg batch time: 0.4923, average train loss: 2.4933
[09/16 23:55:52 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1439, average loss: 2.5419
[09/16 23:55:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 56.50	
[09/16 23:56:13 visual_prompt]: 	Test 100/190. loss: 2.385, 0.1959 s / batch. (data: 1.37e-02)max mem: 17.22448 GB 
[09/16 23:56:32 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1933, average loss: 2.5165
[09/16 23:56:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 58.12	
[09/16 23:56:32 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/16 23:56:40 visual_prompt]: Epoch 41 / 100: avg data time: 9.36e-02, avg batch time: 0.4952, average train loss: 2.6471
[09/16 23:56:43 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1429, average loss: 2.5742
[09/16 23:56:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 60.50	
[09/16 23:57:04 visual_prompt]: 	Test 100/190. loss: 2.849, 0.2167 s / batch. (data: 3.51e-02)max mem: 17.22448 GB 
[09/16 23:57:22 visual_prompt]: Inference (test):avg data time: 6.41e-03, avg batch time: 0.1918, average loss: 2.6356
[09/16 23:57:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.70	
[09/16 23:57:22 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/16 23:57:30 visual_prompt]: Epoch 42 / 100: avg data time: 9.41e-02, avg batch time: 0.4973, average train loss: 2.4870
[09/16 23:57:33 visual_prompt]: Inference (val):avg data time: 2.15e-05, avg batch time: 0.1426, average loss: 2.4565
[09/16 23:57:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/16 23:57:54 visual_prompt]: 	Test 100/190. loss: 2.685, 0.2090 s / batch. (data: 2.73e-02)max mem: 17.22448 GB 
[09/16 23:58:12 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1930, average loss: 2.5569
[09/16 23:58:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 55.71	
[09/16 23:58:12 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/16 23:58:21 visual_prompt]: Epoch 43 / 100: avg data time: 8.28e-02, avg batch time: 0.4835, average train loss: 2.4553
[09/16 23:58:23 visual_prompt]: Inference (val):avg data time: 4.01e-05, avg batch time: 0.1440, average loss: 2.3760
[09/16 23:58:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/16 23:58:44 visual_prompt]: 	Test 100/190. loss: 2.282, 0.2010 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/16 23:59:02 visual_prompt]: Inference (test):avg data time: 6.52e-03, avg batch time: 0.1919, average loss: 2.3644
[09/16 23:59:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 55.38	
[09/16 23:59:02 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/16 23:59:11 visual_prompt]: Epoch 44 / 100: avg data time: 9.30e-02, avg batch time: 0.4929, average train loss: 2.4291
[09/16 23:59:14 visual_prompt]: Inference (val):avg data time: 3.42e-05, avg batch time: 0.1427, average loss: 2.2745
[09/16 23:59:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 53.00	
[09/16 23:59:34 visual_prompt]: 	Test 100/190. loss: 2.185, 0.2029 s / batch. (data: 2.17e-02)max mem: 17.22448 GB 
[09/16 23:59:53 visual_prompt]: Inference (test):avg data time: 7.55e-03, avg batch time: 0.1921, average loss: 2.2433
[09/16 23:59:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.46	top5: 55.87	
[09/16 23:59:53 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/17 00:00:01 visual_prompt]: Epoch 45 / 100: avg data time: 1.01e-01, avg batch time: 0.5002, average train loss: 2.4145
[09/17 00:00:04 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1426, average loss: 2.3424
[09/17 00:00:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/17 00:00:25 visual_prompt]: 	Test 100/190. loss: 2.290, 0.2087 s / batch. (data: 2.69e-02)max mem: 17.22448 GB 
[09/17 00:00:43 visual_prompt]: Inference (test):avg data time: 8.43e-03, avg batch time: 0.1929, average loss: 2.3830
[09/17 00:00:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 56.62	
[09/17 00:00:43 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/17 00:00:52 visual_prompt]: Epoch 46 / 100: avg data time: 8.01e-02, avg batch time: 0.4829, average train loss: 2.2989
[09/17 00:00:54 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1428, average loss: 2.2584
[09/17 00:00:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 65.50	
[09/17 00:01:15 visual_prompt]: 	Test 100/190. loss: 2.347, 0.1832 s / batch. (data: 1.28e-04)max mem: 17.22448 GB 
[09/17 00:01:33 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1923, average loss: 2.3676
[09/17 00:01:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.84	top5: 61.72	
[09/17 00:01:33 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/17 00:01:42 visual_prompt]: Epoch 47 / 100: avg data time: 9.04e-02, avg batch time: 0.4933, average train loss: 2.3105
[09/17 00:01:45 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1452, average loss: 2.4807
[09/17 00:01:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 59.50	
[09/17 00:02:05 visual_prompt]: 	Test 100/190. loss: 2.559, 0.1826 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/17 00:02:23 visual_prompt]: Inference (test):avg data time: 8.81e-03, avg batch time: 0.1929, average loss: 2.4957
[09/17 00:02:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.40	top5: 58.91	
[09/17 00:02:24 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/17 00:02:32 visual_prompt]: Epoch 48 / 100: avg data time: 9.67e-02, avg batch time: 0.4990, average train loss: 2.4358
[09/17 00:02:35 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1429, average loss: 2.5175
[09/17 00:02:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/17 00:02:56 visual_prompt]: 	Test 100/190. loss: 2.674, 0.2102 s / batch. (data: 2.55e-02)max mem: 17.22448 GB 
[09/17 00:03:14 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1922, average loss: 2.5690
[09/17 00:03:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 56.20	
[09/17 00:03:14 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/17 00:03:22 visual_prompt]: Epoch 49 / 100: avg data time: 8.03e-02, avg batch time: 0.4839, average train loss: 2.3939
[09/17 00:03:25 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1425, average loss: 2.7979
[09/17 00:03:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 49.50	
[09/17 00:03:46 visual_prompt]: 	Test 100/190. loss: 2.594, 0.1963 s / batch. (data: 1.39e-02)max mem: 17.22448 GB 
[09/17 00:04:04 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1927, average loss: 2.6805
[09/17 00:04:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 57.34	
[09/17 00:04:04 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/17 00:04:13 visual_prompt]: Epoch 50 / 100: avg data time: 8.73e-02, avg batch time: 0.4913, average train loss: 2.3239
[09/17 00:04:15 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1536, average loss: 2.5610
[09/17 00:04:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 49.50	
[09/17 00:04:36 visual_prompt]: 	Test 100/190. loss: 2.313, 0.1858 s / batch. (data: 1.03e-04)max mem: 17.22448 GB 
[09/17 00:04:54 visual_prompt]: Inference (test):avg data time: 6.73e-03, avg batch time: 0.1915, average loss: 2.4584
[09/17 00:04:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.02	top5: 57.72	
[09/17 00:04:54 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/17 00:05:03 visual_prompt]: Epoch 51 / 100: avg data time: 9.18e-02, avg batch time: 0.4965, average train loss: 2.3612
[09/17 00:05:05 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1427, average loss: 2.2324
[09/17 00:05:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 70.50	
[09/17 00:05:26 visual_prompt]: 	Test 100/190. loss: 2.193, 0.2087 s / batch. (data: 2.71e-02)max mem: 17.22448 GB 
[09/17 00:05:44 visual_prompt]: Inference (test):avg data time: 6.85e-03, avg batch time: 0.1913, average loss: 2.2851
[09/17 00:05:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.66	top5: 63.51	
[09/17 00:05:44 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/17 00:05:52 visual_prompt]: Epoch 52 / 100: avg data time: 7.64e-02, avg batch time: 0.4796, average train loss: 2.2282
[09/17 00:05:55 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1426, average loss: 2.1962
[09/17 00:05:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 72.50	
[09/17 00:06:16 visual_prompt]: 	Test 100/190. loss: 2.109, 0.1829 s / batch. (data: 1.54e-04)max mem: 17.22448 GB 
[09/17 00:06:34 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1921, average loss: 2.1654
[09/17 00:06:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.73	top5: 71.51	
[09/17 00:06:34 visual_prompt]: Best epoch 52: best metric: 0.165
[09/17 00:06:34 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/17 00:06:43 visual_prompt]: Epoch 53 / 100: avg data time: 8.50e-02, avg batch time: 0.4886, average train loss: 2.2513
[09/17 00:06:45 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1425, average loss: 2.5728
[09/17 00:06:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/17 00:07:07 visual_prompt]: 	Test 100/190. loss: 2.462, 0.2012 s / batch. (data: 4.60e-05)max mem: 17.22448 GB 
[09/17 00:07:25 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1950, average loss: 2.5377
[09/17 00:07:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 61.79	
[09/17 00:07:25 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/17 00:07:34 visual_prompt]: Epoch 54 / 100: avg data time: 9.52e-02, avg batch time: 0.4961, average train loss: 2.3369
[09/17 00:07:37 visual_prompt]: Inference (val):avg data time: 3.54e-05, avg batch time: 0.1427, average loss: 2.2064
[09/17 00:07:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 70.00	
[09/17 00:07:57 visual_prompt]: 	Test 100/190. loss: 2.335, 0.1831 s / batch. (data: 1.18e-04)max mem: 17.22448 GB 
[09/17 00:08:16 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1930, average loss: 2.2807
[09/17 00:08:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.28	top5: 67.88	
[09/17 00:08:16 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/17 00:08:24 visual_prompt]: Epoch 55 / 100: avg data time: 8.54e-02, avg batch time: 0.4877, average train loss: 2.2046
[09/17 00:08:27 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1428, average loss: 2.3714
[09/17 00:08:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 64.50	
[09/17 00:08:48 visual_prompt]: 	Test 100/190. loss: 2.336, 0.1974 s / batch. (data: 1.54e-02)max mem: 17.22448 GB 
[09/17 00:09:06 visual_prompt]: Inference (test):avg data time: 7.16e-03, avg batch time: 0.1942, average loss: 2.4375
[09/17 00:09:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.81	top5: 60.12	
[09/17 00:09:06 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/17 00:09:15 visual_prompt]: Epoch 56 / 100: avg data time: 9.97e-02, avg batch time: 0.5016, average train loss: 2.1121
[09/17 00:09:17 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1426, average loss: 2.0790
[09/17 00:09:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 75.00	
[09/17 00:09:38 visual_prompt]: 	Test 100/190. loss: 2.060, 0.1831 s / batch. (data: 1.08e-04)max mem: 17.22448 GB 
[09/17 00:09:57 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1955, average loss: 2.0498
[09/17 00:09:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.23	top5: 77.63	
[09/17 00:09:57 visual_prompt]: Best epoch 56: best metric: 0.195
[09/17 00:09:57 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/17 00:10:06 visual_prompt]: Epoch 57 / 100: avg data time: 9.87e-02, avg batch time: 0.4992, average train loss: 2.0751
[09/17 00:10:08 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1426, average loss: 2.5272
[09/17 00:10:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 62.50	
[09/17 00:10:29 visual_prompt]: 	Test 100/190. loss: 2.317, 0.1830 s / batch. (data: 9.54e-05)max mem: 17.22448 GB 
[09/17 00:10:48 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1922, average loss: 2.5190
[09/17 00:10:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.89	top5: 63.76	
[09/17 00:10:48 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/17 00:10:56 visual_prompt]: Epoch 58 / 100: avg data time: 8.68e-02, avg batch time: 0.4920, average train loss: 2.2493
[09/17 00:10:59 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1429, average loss: 2.1579
[09/17 00:10:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 69.50	
[09/17 00:11:19 visual_prompt]: 	Test 100/190. loss: 2.263, 0.1918 s / batch. (data: 1.03e-04)max mem: 17.22448 GB 
[09/17 00:11:38 visual_prompt]: Inference (test):avg data time: 6.82e-03, avg batch time: 0.1913, average loss: 2.1783
[09/17 00:11:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.14	top5: 66.78	
[09/17 00:11:38 visual_prompt]: Best epoch 58: best metric: 0.200
[09/17 00:11:38 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/17 00:11:46 visual_prompt]: Epoch 59 / 100: avg data time: 8.93e-02, avg batch time: 0.4927, average train loss: 2.0532
[09/17 00:11:49 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1429, average loss: 2.2864
[09/17 00:11:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 70.00	
[09/17 00:12:10 visual_prompt]: 	Test 100/190. loss: 2.479, 0.2080 s / batch. (data: 2.60e-02)max mem: 17.22448 GB 
[09/17 00:12:28 visual_prompt]: Inference (test):avg data time: 7.87e-03, avg batch time: 0.1928, average loss: 2.3971
[09/17 00:12:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.33	top5: 70.12	
[09/17 00:12:28 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/17 00:12:37 visual_prompt]: Epoch 60 / 100: avg data time: 9.68e-02, avg batch time: 0.4983, average train loss: 2.0042
[09/17 00:12:39 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1428, average loss: 1.9605
[09/17 00:12:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 80.00	
[09/17 00:13:01 visual_prompt]: 	Test 100/190. loss: 1.987, 0.1944 s / batch. (data: 1.14e-04)max mem: 17.22448 GB 
[09/17 00:13:19 visual_prompt]: Inference (test):avg data time: 6.05e-03, avg batch time: 0.1913, average loss: 1.9509
[09/17 00:13:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.91	top5: 83.95	
[09/17 00:13:19 visual_prompt]: Best epoch 60: best metric: 0.220
[09/17 00:13:19 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/17 00:13:27 visual_prompt]: Epoch 61 / 100: avg data time: 9.31e-02, avg batch time: 0.4941, average train loss: 2.0017
[09/17 00:13:30 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1436, average loss: 2.0180
[09/17 00:13:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 80.50	
[09/17 00:13:50 visual_prompt]: 	Test 100/190. loss: 1.856, 0.1832 s / batch. (data: 1.36e-04)max mem: 17.22448 GB 
[09/17 00:14:09 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1925, average loss: 1.9920
[09/17 00:14:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.12	top5: 80.36	
[09/17 00:14:09 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/17 00:14:17 visual_prompt]: Epoch 62 / 100: avg data time: 8.42e-02, avg batch time: 0.4852, average train loss: 2.0106
[09/17 00:14:20 visual_prompt]: Inference (val):avg data time: 4.33e-05, avg batch time: 0.1427, average loss: 1.9952
[09/17 00:14:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 77.50	
[09/17 00:14:41 visual_prompt]: 	Test 100/190. loss: 2.049, 0.1971 s / batch. (data: 1.52e-02)max mem: 17.22448 GB 
[09/17 00:14:59 visual_prompt]: Inference (test):avg data time: 6.81e-03, avg batch time: 0.1921, average loss: 2.0838
[09/17 00:14:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.28	top5: 76.06	
[09/17 00:14:59 visual_prompt]: Best epoch 62: best metric: 0.240
[09/17 00:14:59 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/17 00:15:08 visual_prompt]: Epoch 63 / 100: avg data time: 9.84e-02, avg batch time: 0.4992, average train loss: 1.9078
[09/17 00:15:10 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1428, average loss: 2.0113
[09/17 00:15:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 79.00	
[09/17 00:15:31 visual_prompt]: 	Test 100/190. loss: 2.128, 0.1980 s / batch. (data: 1.61e-02)max mem: 17.22448 GB 
[09/17 00:15:49 visual_prompt]: Inference (test):avg data time: 6.86e-03, avg batch time: 0.1920, average loss: 2.1306
[09/17 00:15:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.84	top5: 78.77	
[09/17 00:15:49 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/17 00:15:58 visual_prompt]: Epoch 64 / 100: avg data time: 9.37e-02, avg batch time: 0.4967, average train loss: 2.0727
[09/17 00:16:00 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1428, average loss: 1.9831
[09/17 00:16:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 80.00	
[09/17 00:16:21 visual_prompt]: 	Test 100/190. loss: 1.901, 0.2039 s / batch. (data: 2.20e-02)max mem: 17.22448 GB 
[09/17 00:16:39 visual_prompt]: Inference (test):avg data time: 6.87e-03, avg batch time: 0.1922, average loss: 1.9775
[09/17 00:16:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.75	top5: 83.69	
[09/17 00:16:39 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/17 00:16:48 visual_prompt]: Epoch 65 / 100: avg data time: 8.66e-02, avg batch time: 0.4874, average train loss: 1.9874
[09/17 00:16:50 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.1429, average loss: 2.0560
[09/17 00:16:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 80.50	
[09/17 00:17:11 visual_prompt]: 	Test 100/190. loss: 2.186, 0.2104 s / batch. (data: 1.59e-02)max mem: 17.22448 GB 
[09/17 00:17:29 visual_prompt]: Inference (test):avg data time: 6.89e-03, avg batch time: 0.1923, average loss: 2.1180
[09/17 00:17:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.65	top5: 78.94	
[09/17 00:17:29 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/17 00:17:38 visual_prompt]: Epoch 66 / 100: avg data time: 8.43e-02, avg batch time: 0.4858, average train loss: 2.1558
[09/17 00:17:40 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1428, average loss: 2.2800
[09/17 00:17:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/17 00:18:01 visual_prompt]: 	Test 100/190. loss: 2.333, 0.1959 s / batch. (data: 1.39e-02)max mem: 17.22448 GB 
[09/17 00:18:19 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1921, average loss: 2.2961
[09/17 00:18:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 63.32	
[09/17 00:18:19 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/17 00:18:28 visual_prompt]: Epoch 67 / 100: avg data time: 8.72e-02, avg batch time: 0.4909, average train loss: 2.2654
[09/17 00:18:31 visual_prompt]: Inference (val):avg data time: 3.55e-05, avg batch time: 0.1431, average loss: 2.2733
[09/17 00:18:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 61.50	
[09/17 00:18:51 visual_prompt]: 	Test 100/190. loss: 2.229, 0.1973 s / batch. (data: 1.55e-04)max mem: 17.22448 GB 
[09/17 00:19:10 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1932, average loss: 2.2779
[09/17 00:19:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.26	top5: 64.25	
[09/17 00:19:10 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/17 00:19:18 visual_prompt]: Epoch 68 / 100: avg data time: 7.94e-02, avg batch time: 0.4834, average train loss: 2.1484
[09/17 00:19:21 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1427, average loss: 2.0074
[09/17 00:19:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 76.00	
[09/17 00:19:42 visual_prompt]: 	Test 100/190. loss: 1.965, 0.1902 s / batch. (data: 7.64e-03)max mem: 17.22448 GB 
[09/17 00:20:00 visual_prompt]: Inference (test):avg data time: 6.90e-03, avg batch time: 0.1936, average loss: 1.9990
[09/17 00:20:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.72	top5: 79.70	
[09/17 00:20:00 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/17 00:20:09 visual_prompt]: Epoch 69 / 100: avg data time: 9.98e-02, avg batch time: 0.5001, average train loss: 1.9854
[09/17 00:20:11 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1427, average loss: 1.9403
[09/17 00:20:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 82.00	
[09/17 00:20:33 visual_prompt]: 	Test 100/190. loss: 1.994, 0.1825 s / batch. (data: 1.47e-04)max mem: 17.22448 GB 
[09/17 00:20:51 visual_prompt]: Inference (test):avg data time: 6.57e-03, avg batch time: 0.1953, average loss: 1.9928
[09/17 00:20:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.49	top5: 82.84	
[09/17 00:20:51 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/17 00:21:00 visual_prompt]: Epoch 70 / 100: avg data time: 8.51e-02, avg batch time: 0.4901, average train loss: 1.9219
[09/17 00:21:02 visual_prompt]: Inference (val):avg data time: 3.74e-05, avg batch time: 0.1428, average loss: 1.8745
[09/17 00:21:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 86.00	
[09/17 00:21:23 visual_prompt]: 	Test 100/190. loss: 1.871, 0.1831 s / batch. (data: 9.39e-05)max mem: 17.22448 GB 
[09/17 00:21:42 visual_prompt]: Inference (test):avg data time: 9.11e-03, avg batch time: 0.1934, average loss: 1.9391
[09/17 00:21:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.21	top5: 81.90	
[09/17 00:21:42 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/17 00:21:50 visual_prompt]: Epoch 71 / 100: avg data time: 7.64e-02, avg batch time: 0.4799, average train loss: 1.9844
[09/17 00:21:53 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1428, average loss: 1.8873
[09/17 00:21:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 85.50	
[09/17 00:22:13 visual_prompt]: 	Test 100/190. loss: 1.920, 0.1829 s / batch. (data: 1.14e-04)max mem: 17.22448 GB 
[09/17 00:22:32 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1920, average loss: 1.9463
[09/17 00:22:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.43	top5: 85.89	
[09/17 00:22:32 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/17 00:22:40 visual_prompt]: Epoch 72 / 100: avg data time: 9.18e-02, avg batch time: 0.4918, average train loss: 1.8931
[09/17 00:22:43 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1427, average loss: 1.8587
[09/17 00:22:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 83.50	
[09/17 00:23:04 visual_prompt]: 	Test 100/190. loss: 1.809, 0.1821 s / batch. (data: 4.08e-05)max mem: 17.22448 GB 
[09/17 00:23:22 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1923, average loss: 1.9414
[09/17 00:23:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.93	top5: 82.08	
[09/17 00:23:22 visual_prompt]: Best epoch 72: best metric: 0.250
[09/17 00:23:22 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/17 00:23:30 visual_prompt]: Epoch 73 / 100: avg data time: 9.00e-02, avg batch time: 0.4918, average train loss: 1.7828
[09/17 00:23:33 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1428, average loss: 1.7404
[09/17 00:23:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 88.00	
[09/17 00:23:54 visual_prompt]: 	Test 100/190. loss: 1.734, 0.1834 s / batch. (data: 1.66e-04)max mem: 17.22448 GB 
[09/17 00:24:12 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1937, average loss: 1.8532
[09/17 00:24:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.78	top5: 86.23	
[09/17 00:24:12 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/17 00:24:21 visual_prompt]: Epoch 74 / 100: avg data time: 9.76e-02, avg batch time: 0.4992, average train loss: 1.8332
[09/17 00:24:23 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1428, average loss: 2.0083
[09/17 00:24:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 76.50	
[09/17 00:24:44 visual_prompt]: 	Test 100/190. loss: 1.911, 0.1963 s / batch. (data: 1.42e-02)max mem: 17.22448 GB 
[09/17 00:25:03 visual_prompt]: Inference (test):avg data time: 8.10e-03, avg batch time: 0.1931, average loss: 2.0739
[09/17 00:25:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.21	top5: 79.04	
[09/17 00:25:03 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/17 00:25:11 visual_prompt]: Epoch 75 / 100: avg data time: 8.38e-02, avg batch time: 0.4881, average train loss: 1.8702
[09/17 00:25:14 visual_prompt]: Inference (val):avg data time: 2.02e-05, avg batch time: 0.1426, average loss: 1.7350
[09/17 00:25:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 91.00	
[09/17 00:25:35 visual_prompt]: 	Test 100/190. loss: 1.758, 0.1987 s / batch. (data: 1.54e-02)max mem: 17.22448 GB 
[09/17 00:25:53 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1924, average loss: 1.8607
[09/17 00:25:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.23	top5: 84.95	
[09/17 00:25:53 visual_prompt]: Best epoch 75: best metric: 0.275
[09/17 00:25:53 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/17 00:26:01 visual_prompt]: Epoch 76 / 100: avg data time: 9.78e-02, avg batch time: 0.4978, average train loss: 1.7101
[09/17 00:26:04 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1427, average loss: 1.6835
[09/17 00:26:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 95.00	
[09/17 00:26:24 visual_prompt]: 	Test 100/190. loss: 1.704, 0.2003 s / batch. (data: 1.80e-02)max mem: 17.22448 GB 
[09/17 00:26:43 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1918, average loss: 1.8005
[09/17 00:26:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.33	top5: 89.95	
[09/17 00:26:43 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/17 00:26:51 visual_prompt]: Epoch 77 / 100: avg data time: 8.89e-02, avg batch time: 0.4904, average train loss: 1.6573
[09/17 00:26:54 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1426, average loss: 1.6640
[09/17 00:26:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 94.00	
[09/17 00:27:15 visual_prompt]: 	Test 100/190. loss: 1.682, 0.1962 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 00:27:33 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1931, average loss: 1.7755
[09/17 00:27:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.08	top5: 90.88	
[09/17 00:27:33 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/17 00:27:42 visual_prompt]: Epoch 78 / 100: avg data time: 9.65e-02, avg batch time: 0.4973, average train loss: 1.6930
[09/17 00:27:44 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1427, average loss: 1.7784
[09/17 00:27:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 89.50	
[09/17 00:28:05 visual_prompt]: 	Test 100/190. loss: 1.646, 0.1959 s / batch. (data: 1.33e-02)max mem: 17.22448 GB 
[09/17 00:28:23 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1925, average loss: 1.8735
[09/17 00:28:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.95	top5: 88.43	
[09/17 00:28:24 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/17 00:28:32 visual_prompt]: Epoch 79 / 100: avg data time: 8.63e-02, avg batch time: 0.4923, average train loss: 1.6800
[09/17 00:28:35 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1428, average loss: 1.6327
[09/17 00:28:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 96.00	
[09/17 00:28:55 visual_prompt]: 	Test 100/190. loss: 1.663, 0.1963 s / batch. (data: 1.36e-02)max mem: 17.22448 GB 
[09/17 00:29:14 visual_prompt]: Inference (test):avg data time: 6.64e-03, avg batch time: 0.1922, average loss: 1.7658
[09/17 00:29:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.67	top5: 91.06	
[09/17 00:29:14 visual_prompt]: Best epoch 79: best metric: 0.290
[09/17 00:29:14 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/17 00:29:22 visual_prompt]: Epoch 80 / 100: avg data time: 9.84e-02, avg batch time: 0.5017, average train loss: 1.6323
[09/17 00:29:25 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1427, average loss: 1.6056
[09/17 00:29:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 94.50	
[09/17 00:29:46 visual_prompt]: 	Test 100/190. loss: 1.718, 0.2117 s / batch. (data: 2.99e-02)max mem: 17.22448 GB 
[09/17 00:30:04 visual_prompt]: Inference (test):avg data time: 7.26e-03, avg batch time: 0.1924, average loss: 1.7913
[09/17 00:30:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.09	top5: 90.32	
[09/17 00:30:04 visual_prompt]: Best epoch 80: best metric: 0.355
[09/17 00:30:04 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/17 00:30:12 visual_prompt]: Epoch 81 / 100: avg data time: 8.68e-02, avg batch time: 0.4905, average train loss: 1.6844
[09/17 00:30:15 visual_prompt]: Inference (val):avg data time: 3.47e-05, avg batch time: 0.1430, average loss: 1.5957
[09/17 00:30:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 96.00	
[09/17 00:30:36 visual_prompt]: 	Test 100/190. loss: 1.690, 0.1824 s / batch. (data: 9.39e-05)max mem: 17.22448 GB 
[09/17 00:30:54 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1918, average loss: 1.7758
[09/17 00:30:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.09	top5: 91.09	
[09/17 00:30:54 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/17 00:31:02 visual_prompt]: Epoch 82 / 100: avg data time: 7.11e-02, avg batch time: 0.4740, average train loss: 1.5766
[09/17 00:31:05 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1429, average loss: 1.6952
[09/17 00:31:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 92.00	
[09/17 00:31:25 visual_prompt]: 	Test 100/190. loss: 1.786, 0.1930 s / batch. (data: 1.03e-02)max mem: 17.22448 GB 
[09/17 00:31:43 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1918, average loss: 1.9496
[09/17 00:31:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.09	top5: 88.14	
[09/17 00:31:44 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/17 00:31:52 visual_prompt]: Epoch 83 / 100: avg data time: 7.84e-02, avg batch time: 0.4810, average train loss: 1.6676
[09/17 00:31:54 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1428, average loss: 1.5454
[09/17 00:31:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 95.50	
[09/17 00:32:15 visual_prompt]: 	Test 100/190. loss: 1.655, 0.1828 s / batch. (data: 1.24e-04)max mem: 17.22448 GB 
[09/17 00:32:33 visual_prompt]: Inference (test):avg data time: 7.02e-03, avg batch time: 0.1919, average loss: 1.7707
[09/17 00:32:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.09	top5: 90.77	
[09/17 00:32:33 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/17 00:32:42 visual_prompt]: Epoch 84 / 100: avg data time: 7.96e-02, avg batch time: 0.4834, average train loss: 1.5287
[09/17 00:32:45 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1430, average loss: 1.5201
[09/17 00:32:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 96.50	
[09/17 00:33:05 visual_prompt]: 	Test 100/190. loss: 1.670, 0.1828 s / batch. (data: 1.27e-04)max mem: 17.22448 GB 
[09/17 00:33:24 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1918, average loss: 1.7705
[09/17 00:33:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.30	top5: 91.23	
[09/17 00:33:24 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/17 00:33:32 visual_prompt]: Epoch 85 / 100: avg data time: 9.33e-02, avg batch time: 0.4955, average train loss: 1.4908
[09/17 00:33:35 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.1427, average loss: 1.4412
[09/17 00:33:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 98.00	
[09/17 00:33:56 visual_prompt]: 	Test 100/190. loss: 1.672, 0.1827 s / batch. (data: 1.19e-04)max mem: 17.22448 GB 
[09/17 00:34:14 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1922, average loss: 1.7616
[09/17 00:34:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.05	top5: 92.05	
[09/17 00:34:14 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/17 00:34:22 visual_prompt]: Epoch 86 / 100: avg data time: 9.53e-02, avg batch time: 0.4962, average train loss: 1.4443
[09/17 00:34:25 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1429, average loss: 1.4441
[09/17 00:34:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 98.00	
[09/17 00:34:46 visual_prompt]: 	Test 100/190. loss: 1.645, 0.1970 s / batch. (data: 1.53e-02)max mem: 17.22448 GB 
[09/17 00:35:04 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.1921, average loss: 1.7619
[09/17 00:35:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.84	top5: 92.21	
[09/17 00:35:04 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/17 00:35:12 visual_prompt]: Epoch 87 / 100: avg data time: 7.86e-02, avg batch time: 0.4863, average train loss: 1.4073
[09/17 00:35:15 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1429, average loss: 1.3596
[09/17 00:35:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 99.00	
[09/17 00:35:36 visual_prompt]: 	Test 100/190. loss: 1.688, 0.1824 s / batch. (data: 1.35e-04)max mem: 17.22448 GB 
[09/17 00:35:54 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1924, average loss: 1.7985
[09/17 00:35:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.48	top5: 92.34	
[09/17 00:35:54 visual_prompt]: Best epoch 87: best metric: 0.380
[09/17 00:35:54 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/17 00:36:03 visual_prompt]: Epoch 88 / 100: avg data time: 9.28e-02, avg batch time: 0.4961, average train loss: 1.3693
[09/17 00:36:05 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1426, average loss: 1.4714
[09/17 00:36:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.50	top5: 97.00	
[09/17 00:36:26 visual_prompt]: 	Test 100/190. loss: 1.757, 0.1828 s / batch. (data: 1.08e-04)max mem: 17.22448 GB 
[09/17 00:36:45 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1925, average loss: 2.0113
[09/17 00:36:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.19	top5: 89.37	
[09/17 00:36:45 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/17 00:36:53 visual_prompt]: Epoch 89 / 100: avg data time: 8.30e-02, avg batch time: 0.4861, average train loss: 1.3477
[09/17 00:36:56 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1428, average loss: 1.3959
[09/17 00:36:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.50	top5: 99.00	
[09/17 00:37:17 visual_prompt]: 	Test 100/190. loss: 1.776, 0.2088 s / batch. (data: 1.61e-02)max mem: 17.22448 GB 
[09/17 00:37:35 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1930, average loss: 1.8536
[09/17 00:37:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.05	top5: 91.55	
[09/17 00:37:35 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/17 00:37:44 visual_prompt]: Epoch 90 / 100: avg data time: 9.02e-02, avg batch time: 0.4926, average train loss: 1.2986
[09/17 00:37:46 visual_prompt]: Inference (val):avg data time: 6.09e-05, avg batch time: 0.1537, average loss: 1.3197
[09/17 00:37:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 41.00	top5: 99.00	
[09/17 00:38:08 visual_prompt]: 	Test 100/190. loss: 1.772, 0.2088 s / batch. (data: 2.65e-02)max mem: 17.22448 GB 
[09/17 00:38:26 visual_prompt]: Inference (test):avg data time: 8.83e-03, avg batch time: 0.1939, average loss: 1.9217
[09/17 00:38:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 91.44	
[09/17 00:38:26 visual_prompt]: Best epoch 90: best metric: 0.410
[09/17 00:38:26 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/17 00:38:35 visual_prompt]: Epoch 91 / 100: avg data time: 9.01e-02, avg batch time: 0.4931, average train loss: 1.2817
[09/17 00:38:37 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1426, average loss: 1.2870
[09/17 00:38:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 41.00	top5: 99.00	
[09/17 00:38:58 visual_prompt]: 	Test 100/190. loss: 1.813, 0.1824 s / batch. (data: 1.11e-04)max mem: 17.22448 GB 
[09/17 00:39:17 visual_prompt]: Inference (test):avg data time: 8.06e-03, avg batch time: 0.1935, average loss: 1.9307
[09/17 00:39:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.68	top5: 91.22	
[09/17 00:39:17 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/17 00:39:25 visual_prompt]: Epoch 92 / 100: avg data time: 8.71e-02, avg batch time: 0.4888, average train loss: 1.2303
[09/17 00:39:28 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1428, average loss: 1.2402
[09/17 00:39:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 48.00	top5: 99.50	
[09/17 00:39:49 visual_prompt]: 	Test 100/190. loss: 1.800, 0.1972 s / batch. (data: 1.54e-02)max mem: 17.22448 GB 
[09/17 00:40:07 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1923, average loss: 1.9033
[09/17 00:40:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.07	top5: 91.95	
[09/17 00:40:07 visual_prompt]: Best epoch 92: best metric: 0.480
[09/17 00:40:07 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/17 00:40:16 visual_prompt]: Epoch 93 / 100: avg data time: 1.02e-01, avg batch time: 0.5036, average train loss: 1.2309
[09/17 00:40:18 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1427, average loss: 1.4690
[09/17 00:40:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 98.50	
[09/17 00:40:39 visual_prompt]: 	Test 100/190. loss: 2.014, 0.1829 s / batch. (data: 1.33e-04)max mem: 17.22448 GB 
[09/17 00:40:57 visual_prompt]: Inference (test):avg data time: 6.10e-03, avg batch time: 0.1913, average loss: 2.2433
[09/17 00:40:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 88.57	
[09/17 00:40:57 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/17 00:41:06 visual_prompt]: Epoch 94 / 100: avg data time: 9.67e-02, avg batch time: 0.4978, average train loss: 1.2797
[09/17 00:41:09 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1429, average loss: 1.1659
[09/17 00:41:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 47.50	top5: 99.50	
[09/17 00:41:29 visual_prompt]: 	Test 100/190. loss: 1.738, 0.1827 s / batch. (data: 2.36e-04)max mem: 17.22448 GB 
[09/17 00:41:48 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1927, average loss: 1.8840
[09/17 00:41:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.95	top5: 92.46	
[09/17 00:41:48 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/17 00:41:57 visual_prompt]: Epoch 95 / 100: avg data time: 9.80e-02, avg batch time: 0.5054, average train loss: 1.1828
[09/17 00:41:59 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1440, average loss: 1.2525
[09/17 00:41:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 44.00	top5: 99.50	
[09/17 00:42:20 visual_prompt]: 	Test 100/190. loss: 1.911, 0.1827 s / batch. (data: 1.31e-04)max mem: 17.22448 GB 
[09/17 00:42:38 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1923, average loss: 2.0725
[09/17 00:42:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.02	top5: 90.76	
[09/17 00:42:38 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/17 00:42:47 visual_prompt]: Epoch 96 / 100: avg data time: 8.84e-02, avg batch time: 0.4906, average train loss: 1.1629
[09/17 00:42:50 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1445, average loss: 1.1494
[09/17 00:42:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 51.00	top5: 99.50	
[09/17 00:43:11 visual_prompt]: 	Test 100/190. loss: 1.800, 0.1830 s / batch. (data: 1.59e-04)max mem: 17.22448 GB 
[09/17 00:43:29 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1932, average loss: 1.9436
[09/17 00:43:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.70	top5: 92.12	
[09/17 00:43:29 visual_prompt]: Best epoch 96: best metric: 0.510
[09/17 00:43:29 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/17 00:43:38 visual_prompt]: Epoch 97 / 100: avg data time: 1.03e-01, avg batch time: 0.5036, average train loss: 1.1181
[09/17 00:43:41 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1429, average loss: 1.1472
[09/17 00:43:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 48.00	top5: 99.50	
[09/17 00:44:01 visual_prompt]: 	Test 100/190. loss: 1.867, 0.1831 s / batch. (data: 1.56e-04)max mem: 17.22448 GB 
[09/17 00:44:20 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1931, average loss: 1.9616
[09/17 00:44:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.61	top5: 91.83	
[09/17 00:44:20 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/17 00:44:28 visual_prompt]: Epoch 98 / 100: avg data time: 9.81e-02, avg batch time: 0.4984, average train loss: 1.1039
[09/17 00:44:31 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1429, average loss: 1.1465
[09/17 00:44:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 50.50	top5: 99.50	
[09/17 00:44:52 visual_prompt]: 	Test 100/190. loss: 1.840, 0.1825 s / batch. (data: 9.58e-05)max mem: 17.22448 GB 
[09/17 00:45:10 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1923, average loss: 1.9748
[09/17 00:45:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.17	top5: 92.06	
[09/17 00:45:10 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/17 00:45:19 visual_prompt]: Epoch 99 / 100: avg data time: 9.34e-02, avg batch time: 0.4949, average train loss: 1.1009
[09/17 00:45:22 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1427, average loss: 1.1454
[09/17 00:45:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 49.50	top5: 99.50	
[09/17 00:45:43 visual_prompt]: 	Test 100/190. loss: 1.846, 0.1824 s / batch. (data: 1.19e-04)max mem: 17.22448 GB 
[09/17 00:46:01 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1928, average loss: 1.9863
[09/17 00:46:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.01	top5: 91.94	
[09/17 00:46:01 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/17 00:46:09 visual_prompt]: Epoch 100 / 100: avg data time: 8.43e-02, avg batch time: 0.4867, average train loss: 1.1003
[09/17 00:46:12 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1429, average loss: 1.1401
[09/17 00:46:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 48.00	top5: 99.50	
[09/17 00:46:33 visual_prompt]: 	Test 100/190. loss: 1.842, 0.2060 s / batch. (data: 3.24e-05)max mem: 17.22448 GB 
[09/17 00:46:51 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1930, average loss: 1.9748
[09/17 00:46:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.56	top5: 92.06	
[09/17 00:47:42 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 00:47:42 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 00:47:42 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed44'], train_type='')
[09/17 00:47:42 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 00:47:42 visual_prompt]: Training with config:
[09/17 00:47:42 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")',
          'NO_TEST': False,
          'NUMBER_CLASSES': 9,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed44/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 00:47:42 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 00:47:42.527994: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 00:47:42.722223: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 00:47:43.784990: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 00:47:43.785158: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 00:47:43.785170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 00:47:46.123769: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 00:47:46.123982: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 00:47:46.124000: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 00:47:46 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
2023-09-17 00:47:46.142524: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 00:47:48 visual_prompt]: Number of images: 1000
[09/17 00:47:48 visual_prompt]: Number of classes: 9 / 9
[09/17 00:47:48 visual_prompt]: Loading validation data...
[09/17 00:47:48 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 00:47:48 visual_prompt]: Number of images: 200
[09/17 00:47:48 visual_prompt]: Number of classes: 9 / 9
[09/17 00:47:48 visual_prompt]: Loading test data...
[09/17 00:47:48 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 00:48:05 visual_prompt]: Number of images: 12150
[09/17 00:48:05 visual_prompt]: Number of classes: 9 / 9
[09/17 00:48:05 visual_prompt]: Constructing models...
[09/17 00:48:10 visual_prompt]: Total Parameters: 86727177	 Gradient Parameters: 928521
[09/17 00:48:10 visual_prompt]: tuned percent:1.071
[09/17 00:48:13 visual_prompt]: Device used for model: 0
[09/17 00:48:13 visual_prompt]: Setting up Evalutator...
[09/17 00:48:13 visual_prompt]: Setting up Trainer...
[09/17 00:48:13 visual_prompt]: 	Setting up the optimizer...
[09/17 00:48:13 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 00:48:23 visual_prompt]: Epoch 1 / 100: avg data time: 1.18e-01, avg batch time: 0.5904, average train loss: 2.3652
[09/17 00:48:26 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1416, average loss: 2.3318
[09/17 00:48:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 57.00	
[09/17 00:48:47 visual_prompt]: 	Test 100/190. loss: 2.451, 0.1962 s / batch. (data: 1.51e-02)max mem: 17.22448 GB 
[09/17 00:49:06 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1948, average loss: 2.3753
[09/17 00:49:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.06	top5: 55.00	
[09/17 00:49:06 visual_prompt]: Best epoch 1: best metric: 0.145
[09/17 00:49:06 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/17 00:49:15 visual_prompt]: Epoch 2 / 100: avg data time: 1.05e-01, avg batch time: 0.5082, average train loss: 3.3396
[09/17 00:49:18 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1423, average loss: 2.5172
[09/17 00:49:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 49.50	
[09/17 00:49:39 visual_prompt]: 	Test 100/190. loss: 2.450, 0.1825 s / batch. (data: 1.50e-04)max mem: 17.22448 GB 
[09/17 00:49:57 visual_prompt]: Inference (test):avg data time: 6.75e-03, avg batch time: 0.1922, average loss: 2.4128
[09/17 00:49:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 55.94	
[09/17 00:49:58 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/17 00:50:07 visual_prompt]: Epoch 3 / 100: avg data time: 1.10e-01, avg batch time: 0.5108, average train loss: 2.4589
[09/17 00:50:10 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1425, average loss: 2.4701
[09/17 00:50:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.50	
[09/17 00:50:31 visual_prompt]: 	Test 100/190. loss: 2.609, 0.1961 s / batch. (data: 1.31e-04)max mem: 17.22448 GB 
[09/17 00:50:49 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1935, average loss: 2.5132
[09/17 00:50:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.88	top5: 55.28	
[09/17 00:50:49 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/17 00:50:58 visual_prompt]: Epoch 4 / 100: avg data time: 1.09e-01, avg batch time: 0.5107, average train loss: 2.4245
[09/17 00:51:02 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1434, average loss: 2.3389
[09/17 00:51:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 59.00	
[09/17 00:51:23 visual_prompt]: 	Test 100/190. loss: 2.406, 0.1827 s / batch. (data: 9.99e-05)max mem: 17.22448 GB 
[09/17 00:51:41 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1945, average loss: 2.4559
[09/17 00:51:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.70	
[09/17 00:51:41 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/17 00:51:50 visual_prompt]: Epoch 5 / 100: avg data time: 9.47e-02, avg batch time: 0.4982, average train loss: 2.5785
[09/17 00:51:53 visual_prompt]: Inference (val):avg data time: 3.38e-05, avg batch time: 0.1426, average loss: 2.6239
[09/17 00:51:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 54.50	
[09/17 00:52:15 visual_prompt]: 	Test 100/190. loss: 2.426, 0.2119 s / batch. (data: 3.01e-02)max mem: 17.22448 GB 
[09/17 00:52:33 visual_prompt]: Inference (test):avg data time: 8.49e-03, avg batch time: 0.1939, average loss: 2.6458
[09/17 00:52:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.16	top5: 55.33	
[09/17 00:52:33 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/17 00:52:42 visual_prompt]: Epoch 6 / 100: avg data time: 1.01e-01, avg batch time: 0.5052, average train loss: 2.8166
[09/17 00:52:45 visual_prompt]: Inference (val):avg data time: 3.52e-05, avg batch time: 0.1430, average loss: 2.9188
[09/17 00:52:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.50	
[09/17 00:53:07 visual_prompt]: 	Test 100/190. loss: 2.978, 0.1834 s / batch. (data: 1.42e-04)max mem: 17.22448 GB 
[09/17 00:53:25 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1928, average loss: 2.9061
[09/17 00:53:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.59	
[09/17 00:53:25 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/17 00:53:34 visual_prompt]: Epoch 7 / 100: avg data time: 8.99e-02, avg batch time: 0.4963, average train loss: 3.4713
[09/17 00:53:37 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1427, average loss: 3.8475
[09/17 00:53:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 48.50	
[09/17 00:53:58 visual_prompt]: 	Test 100/190. loss: 3.663, 0.1826 s / batch. (data: 1.22e-04)max mem: 17.22448 GB 
[09/17 00:54:17 visual_prompt]: Inference (test):avg data time: 8.20e-03, avg batch time: 0.1940, average loss: 3.6900
[09/17 00:54:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.44	
[09/17 00:54:17 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/17 00:54:26 visual_prompt]: Epoch 8 / 100: avg data time: 1.13e-01, avg batch time: 0.5131, average train loss: 5.5296
[09/17 00:54:29 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1427, average loss: 5.8719
[09/17 00:54:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 54.00	
[09/17 00:54:50 visual_prompt]: 	Test 100/190. loss: 5.672, 0.1977 s / batch. (data: 1.57e-02)max mem: 17.22448 GB 
[09/17 00:55:09 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1921, average loss: 5.6917
[09/17 00:55:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 55.21	
[09/17 00:55:09 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/17 00:55:18 visual_prompt]: Epoch 9 / 100: avg data time: 1.15e-01, avg batch time: 0.5180, average train loss: 7.6824
[09/17 00:55:21 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.1432, average loss: 8.7894
[09/17 00:55:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/17 00:55:42 visual_prompt]: 	Test 100/190. loss: 8.910, 0.1972 s / batch. (data: 1.10e-04)max mem: 17.22448 GB 
[09/17 00:56:01 visual_prompt]: Inference (test):avg data time: 8.16e-03, avg batch time: 0.1932, average loss: 8.1612
[09/17 00:56:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.52	
[09/17 00:56:01 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/17 00:56:10 visual_prompt]: Epoch 10 / 100: avg data time: 1.02e-01, avg batch time: 0.5038, average train loss: 11.1440
[09/17 00:56:13 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1425, average loss: 15.8163
[09/17 00:56:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/17 00:56:34 visual_prompt]: 	Test 100/190. loss: 15.147, 0.1863 s / batch. (data: 1.19e-04)max mem: 17.22448 GB 
[09/17 00:56:52 visual_prompt]: Inference (test):avg data time: 6.84e-03, avg batch time: 0.1931, average loss: 16.0614
[09/17 00:56:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.54	
[09/17 00:56:52 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/17 00:57:01 visual_prompt]: Epoch 11 / 100: avg data time: 1.10e-01, avg batch time: 0.5121, average train loss: 12.3355
[09/17 00:57:05 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1428, average loss: 15.2998
[09/17 00:57:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.00	
[09/17 00:57:26 visual_prompt]: 	Test 100/190. loss: 15.575, 0.1968 s / batch. (data: 1.52e-02)max mem: 17.22448 GB 
[09/17 00:57:45 visual_prompt]: Inference (test):avg data time: 8.41e-03, avg batch time: 0.1942, average loss: 15.9257
[09/17 00:57:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.21	top5: 55.33	
[09/17 00:57:45 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/17 00:57:54 visual_prompt]: Epoch 12 / 100: avg data time: 1.14e-01, avg batch time: 0.5148, average train loss: 10.0953
[09/17 00:57:57 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1425, average loss: 6.0115
[09/17 00:57:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.50	
[09/17 00:58:18 visual_prompt]: 	Test 100/190. loss: 6.597, 0.1987 s / batch. (data: 1.60e-02)max mem: 17.22448 GB 
[09/17 00:58:36 visual_prompt]: Inference (test):avg data time: 6.72e-03, avg batch time: 0.1925, average loss: 6.8341
[09/17 00:58:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.09	
[09/17 00:58:36 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/17 00:58:46 visual_prompt]: Epoch 13 / 100: avg data time: 1.08e-01, avg batch time: 0.5103, average train loss: 6.3047
[09/17 00:58:48 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1453, average loss: 4.3840
[09/17 00:58:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 59.00	
[09/17 00:59:10 visual_prompt]: 	Test 100/190. loss: 5.059, 0.1956 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 00:59:28 visual_prompt]: Inference (test):avg data time: 7.44e-03, avg batch time: 0.1933, average loss: 4.5889
[09/17 00:59:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.70	
[09/17 00:59:28 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/17 00:59:37 visual_prompt]: Epoch 14 / 100: avg data time: 1.08e-01, avg batch time: 0.5091, average train loss: 3.5888
[09/17 00:59:40 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.1426, average loss: 3.1116
[09/17 00:59:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/17 01:00:02 visual_prompt]: 	Test 100/190. loss: 3.375, 0.1839 s / batch. (data: 1.28e-04)max mem: 17.22448 GB 
[09/17 01:00:20 visual_prompt]: Inference (test):avg data time: 7.04e-03, avg batch time: 0.1941, average loss: 3.3238
[09/17 01:00:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 55.63	
[09/17 01:00:20 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/17 01:00:30 visual_prompt]: Epoch 15 / 100: avg data time: 1.23e-01, avg batch time: 0.5234, average train loss: 3.2789
[09/17 01:00:33 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1428, average loss: 3.0874
[09/17 01:00:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/17 01:00:54 visual_prompt]: 	Test 100/190. loss: 3.209, 0.1962 s / batch. (data: 1.05e-04)max mem: 17.22448 GB 
[09/17 01:01:12 visual_prompt]: Inference (test):avg data time: 7.85e-03, avg batch time: 0.1933, average loss: 3.1107
[09/17 01:01:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.47	
[09/17 01:01:12 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/17 01:01:21 visual_prompt]: Epoch 16 / 100: avg data time: 9.78e-02, avg batch time: 0.5027, average train loss: 3.0548
[09/17 01:01:24 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1424, average loss: 3.1575
[09/17 01:01:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 60.00	
[09/17 01:01:45 visual_prompt]: 	Test 100/190. loss: 3.019, 0.1983 s / batch. (data: 1.62e-02)max mem: 17.22448 GB 
[09/17 01:02:04 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1931, average loss: 3.2672
[09/17 01:02:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.03	
[09/17 01:02:04 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/17 01:02:13 visual_prompt]: Epoch 17 / 100: avg data time: 1.13e-01, avg batch time: 0.5141, average train loss: 3.1196
[09/17 01:02:16 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1428, average loss: 3.0879
[09/17 01:02:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 64.00	
[09/17 01:02:38 visual_prompt]: 	Test 100/190. loss: 3.277, 0.1829 s / batch. (data: 1.01e-04)max mem: 17.22448 GB 
[09/17 01:02:56 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1927, average loss: 3.3474
[09/17 01:02:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 56.29	
[09/17 01:02:56 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/17 01:03:05 visual_prompt]: Epoch 18 / 100: avg data time: 1.07e-01, avg batch time: 0.5076, average train loss: 2.9338
[09/17 01:03:08 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1427, average loss: 2.4963
[09/17 01:03:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 59.50	
[09/17 01:03:29 visual_prompt]: 	Test 100/190. loss: 2.599, 0.1931 s / batch. (data: 1.11e-02)max mem: 17.22448 GB 
[09/17 01:03:48 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1935, average loss: 2.5604
[09/17 01:03:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.81	top5: 55.20	
[09/17 01:03:48 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/17 01:03:57 visual_prompt]: Epoch 19 / 100: avg data time: 1.08e-01, avg batch time: 0.5091, average train loss: 2.8171
[09/17 01:04:00 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1426, average loss: 2.6089
[09/17 01:04:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.00	
[09/17 01:04:21 visual_prompt]: 	Test 100/190. loss: 2.739, 0.1962 s / batch. (data: 1.42e-02)max mem: 17.22448 GB 
[09/17 01:04:40 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1943, average loss: 2.6454
[09/17 01:04:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 56.71	
[09/17 01:04:40 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/17 01:04:49 visual_prompt]: Epoch 20 / 100: avg data time: 1.09e-01, avg batch time: 0.5120, average train loss: 2.7201
[09/17 01:04:52 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1427, average loss: 2.4721
[09/17 01:04:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/17 01:05:13 visual_prompt]: 	Test 100/190. loss: 2.531, 0.1910 s / batch. (data: 9.35e-03)max mem: 17.22448 GB 
[09/17 01:05:32 visual_prompt]: Inference (test):avg data time: 8.04e-03, avg batch time: 0.1930, average loss: 2.4611
[09/17 01:05:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.25	top5: 56.02	
[09/17 01:05:32 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/17 01:05:41 visual_prompt]: Epoch 21 / 100: avg data time: 9.06e-02, avg batch time: 0.4979, average train loss: 2.5530
[09/17 01:05:44 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1426, average loss: 2.7218
[09/17 01:05:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 54.00	
[09/17 01:06:05 visual_prompt]: 	Test 100/190. loss: 2.876, 0.1980 s / batch. (data: 1.06e-04)max mem: 17.22448 GB 
[09/17 01:06:24 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1931, average loss: 2.7822
[09/17 01:06:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.67	top5: 55.51	
[09/17 01:06:24 visual_prompt]: Best epoch 21: best metric: 0.165
[09/17 01:06:24 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/17 01:06:32 visual_prompt]: Epoch 22 / 100: avg data time: 9.00e-02, avg batch time: 0.4966, average train loss: 2.4657
[09/17 01:06:35 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1425, average loss: 2.5296
[09/17 01:06:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 54.50	
[09/17 01:06:57 visual_prompt]: 	Test 100/190. loss: 2.527, 0.2093 s / batch. (data: 1.51e-02)max mem: 17.22448 GB 
[09/17 01:07:15 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1934, average loss: 2.4994
[09/17 01:07:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.17	top5: 56.26	
[09/17 01:07:15 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/17 01:07:24 visual_prompt]: Epoch 23 / 100: avg data time: 1.18e-01, avg batch time: 0.5190, average train loss: 2.4755
[09/17 01:07:28 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1425, average loss: 2.9284
[09/17 01:07:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/17 01:07:49 visual_prompt]: 	Test 100/190. loss: 3.111, 0.2015 s / batch. (data: 1.25e-02)max mem: 17.22448 GB 
[09/17 01:08:07 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1918, average loss: 2.9457
[09/17 01:08:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 57.69	
[09/17 01:08:07 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/17 01:08:16 visual_prompt]: Epoch 24 / 100: avg data time: 1.07e-01, avg batch time: 0.5093, average train loss: 2.7827
[09/17 01:08:19 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1427, average loss: 3.0132
[09/17 01:08:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/17 01:08:40 visual_prompt]: 	Test 100/190. loss: 3.061, 0.2081 s / batch. (data: 1.82e-02)max mem: 17.22448 GB 
[09/17 01:09:00 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1992, average loss: 2.9474
[09/17 01:09:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.45	top5: 57.04	
[09/17 01:09:00 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/17 01:09:09 visual_prompt]: Epoch 25 / 100: avg data time: 1.04e-01, avg batch time: 0.5064, average train loss: 2.6034
[09/17 01:09:12 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1429, average loss: 2.4531
[09/17 01:09:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 64.50	
[09/17 01:09:33 visual_prompt]: 	Test 100/190. loss: 2.163, 0.1954 s / batch. (data: 1.32e-02)max mem: 17.22448 GB 
[09/17 01:09:51 visual_prompt]: Inference (test):avg data time: 6.73e-03, avg batch time: 0.1925, average loss: 2.4343
[09/17 01:09:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.37	top5: 65.70	
[09/17 01:09:51 visual_prompt]: Best epoch 25: best metric: 0.170
[09/17 01:09:51 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/17 01:10:01 visual_prompt]: Epoch 26 / 100: avg data time: 1.06e-01, avg batch time: 0.5072, average train loss: 2.4663
[09/17 01:10:04 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1427, average loss: 2.5190
[09/17 01:10:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 63.00	
[09/17 01:10:25 visual_prompt]: 	Test 100/190. loss: 2.337, 0.1828 s / batch. (data: 8.58e-05)max mem: 17.22448 GB 
[09/17 01:10:43 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1928, average loss: 2.5211
[09/17 01:10:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.27	top5: 61.13	
[09/17 01:10:43 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/17 01:10:53 visual_prompt]: Epoch 27 / 100: avg data time: 1.11e-01, avg batch time: 0.5139, average train loss: 2.4157
[09/17 01:10:56 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1427, average loss: 2.3794
[09/17 01:10:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 64.00	
[09/17 01:11:17 visual_prompt]: 	Test 100/190. loss: 2.475, 0.1950 s / batch. (data: 1.24e-02)max mem: 17.22448 GB 
[09/17 01:11:35 visual_prompt]: Inference (test):avg data time: 6.52e-03, avg batch time: 0.1922, average loss: 2.4019
[09/17 01:11:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.46	top5: 67.19	
[09/17 01:11:35 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/17 01:11:44 visual_prompt]: Epoch 28 / 100: avg data time: 1.15e-01, avg batch time: 0.5160, average train loss: 2.2904
[09/17 01:11:48 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1429, average loss: 2.2045
[09/17 01:11:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 68.50	
[09/17 01:12:08 visual_prompt]: 	Test 100/190. loss: 2.126, 0.1848 s / batch. (data: 1.34e-04)max mem: 17.22448 GB 
[09/17 01:12:27 visual_prompt]: Inference (test):avg data time: 8.52e-03, avg batch time: 0.1940, average loss: 2.2592
[09/17 01:12:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.94	top5: 66.43	
[09/17 01:12:27 visual_prompt]: Best epoch 28: best metric: 0.245
[09/17 01:12:27 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/17 01:12:37 visual_prompt]: Epoch 29 / 100: avg data time: 1.13e-01, avg batch time: 0.5162, average train loss: 2.0636
[09/17 01:12:40 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1427, average loss: 1.8236
[09/17 01:12:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 88.00	
[09/17 01:13:01 visual_prompt]: 	Test 100/190. loss: 1.914, 0.1830 s / batch. (data: 9.82e-05)max mem: 17.22448 GB 
[09/17 01:13:20 visual_prompt]: Inference (test):avg data time: 5.80e-03, avg batch time: 0.1955, average loss: 1.8612
[09/17 01:13:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.31	top5: 86.30	
[09/17 01:13:20 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/17 01:13:29 visual_prompt]: Epoch 30 / 100: avg data time: 1.05e-01, avg batch time: 0.5069, average train loss: 2.2957
[09/17 01:13:32 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1426, average loss: 2.3063
[09/17 01:13:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 70.50	
[09/17 01:13:53 visual_prompt]: 	Test 100/190. loss: 2.313, 0.1821 s / batch. (data: 9.58e-05)max mem: 17.22448 GB 
[09/17 01:14:12 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1933, average loss: 2.3362
[09/17 01:14:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.99	top5: 70.51	
[09/17 01:14:12 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/17 01:14:21 visual_prompt]: Epoch 31 / 100: avg data time: 1.14e-01, avg batch time: 0.5144, average train loss: 2.3025
[09/17 01:14:24 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1426, average loss: 2.1899
[09/17 01:14:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 79.50	
[09/17 01:14:46 visual_prompt]: 	Test 100/190. loss: 2.266, 0.2001 s / batch. (data: 1.75e-02)max mem: 17.22448 GB 
[09/17 01:15:05 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1975, average loss: 2.3546
[09/17 01:15:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.44	top5: 72.44	
[09/17 01:15:05 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/17 01:15:14 visual_prompt]: Epoch 32 / 100: avg data time: 1.03e-01, avg batch time: 0.5070, average train loss: 2.2627
[09/17 01:15:17 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1427, average loss: 2.4435
[09/17 01:15:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 75.50	
[09/17 01:15:38 visual_prompt]: 	Test 100/190. loss: 2.211, 0.1968 s / batch. (data: 1.48e-02)max mem: 17.22448 GB 
[09/17 01:15:56 visual_prompt]: Inference (test):avg data time: 6.44e-03, avg batch time: 0.1923, average loss: 2.4113
[09/17 01:15:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.21	top5: 77.24	
[09/17 01:15:56 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/17 01:16:05 visual_prompt]: Epoch 33 / 100: avg data time: 1.01e-01, avg batch time: 0.5034, average train loss: 2.1080
[09/17 01:16:08 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1427, average loss: 2.1105
[09/17 01:16:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 87.00	
[09/17 01:16:30 visual_prompt]: 	Test 100/190. loss: 1.986, 0.2109 s / batch. (data: 2.91e-02)max mem: 17.22448 GB 
[09/17 01:16:48 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1935, average loss: 2.1714
[09/17 01:16:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.21	top5: 83.50	
[09/17 01:16:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/17 01:16:57 visual_prompt]: Epoch 34 / 100: avg data time: 1.01e-01, avg batch time: 0.5039, average train loss: 1.9754
[09/17 01:17:00 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1425, average loss: 1.8335
[09/17 01:17:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 93.00	
[09/17 01:17:21 visual_prompt]: 	Test 100/190. loss: 1.990, 0.2024 s / batch. (data: 1.37e-02)max mem: 17.22448 GB 
[09/17 01:17:40 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1928, average loss: 1.9589
[09/17 01:17:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.90	top5: 88.12	
[09/17 01:17:40 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/17 01:17:49 visual_prompt]: Epoch 35 / 100: avg data time: 9.95e-02, avg batch time: 0.5064, average train loss: 2.2455
[09/17 01:17:52 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1427, average loss: 2.0589
[09/17 01:17:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 86.50	
[09/17 01:18:13 visual_prompt]: 	Test 100/190. loss: 2.198, 0.1969 s / batch. (data: 1.19e-04)max mem: 17.22448 GB 
[09/17 01:18:31 visual_prompt]: Inference (test):avg data time: 6.73e-03, avg batch time: 0.1924, average loss: 2.1159
[09/17 01:18:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.15	top5: 82.15	
[09/17 01:18:31 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/17 01:18:40 visual_prompt]: Epoch 36 / 100: avg data time: 8.99e-02, avg batch time: 0.4962, average train loss: 2.1039
[09/17 01:18:43 visual_prompt]: Inference (val):avg data time: 2.34e-05, avg batch time: 0.1427, average loss: 2.0365
[09/17 01:18:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 85.50	
[09/17 01:19:05 visual_prompt]: 	Test 100/190. loss: 2.185, 0.2057 s / batch. (data: 1.39e-02)max mem: 17.22448 GB 
[09/17 01:19:23 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1942, average loss: 2.1225
[09/17 01:19:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.87	top5: 82.42	
[09/17 01:19:23 visual_prompt]: Best epoch 36: best metric: 0.250
[09/17 01:19:23 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/17 01:19:32 visual_prompt]: Epoch 37 / 100: avg data time: 1.07e-01, avg batch time: 0.5082, average train loss: 1.9023
[09/17 01:19:35 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1427, average loss: 2.3248
[09/17 01:19:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 82.00	
[09/17 01:19:57 visual_prompt]: 	Test 100/190. loss: 2.491, 0.2066 s / batch. (data: 2.46e-02)max mem: 17.22448 GB 
[09/17 01:20:15 visual_prompt]: Inference (test):avg data time: 8.74e-03, avg batch time: 0.1939, average loss: 2.4824
[09/17 01:20:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.31	top5: 75.78	
[09/17 01:20:15 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/17 01:20:24 visual_prompt]: Epoch 38 / 100: avg data time: 1.04e-01, avg batch time: 0.5081, average train loss: 2.2057
[09/17 01:20:27 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1457, average loss: 2.0350
[09/17 01:20:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 82.50	
[09/17 01:20:48 visual_prompt]: 	Test 100/190. loss: 2.144, 0.1865 s / batch. (data: 1.33e-04)max mem: 17.22448 GB 
[09/17 01:21:07 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1932, average loss: 2.0950
[09/17 01:21:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.00	top5: 81.94	
[09/17 01:21:07 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/17 01:21:16 visual_prompt]: Epoch 39 / 100: avg data time: 1.08e-01, avg batch time: 0.5140, average train loss: 2.0735
[09/17 01:21:19 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1429, average loss: 1.9603
[09/17 01:21:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 92.50	
[09/17 01:21:40 visual_prompt]: 	Test 100/190. loss: 1.956, 0.2181 s / batch. (data: 3.64e-02)max mem: 17.22448 GB 
[09/17 01:21:59 visual_prompt]: Inference (test):avg data time: 6.81e-03, avg batch time: 0.1944, average loss: 2.0743
[09/17 01:21:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.06	top5: 87.38	
[09/17 01:21:59 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/17 01:22:08 visual_prompt]: Epoch 40 / 100: avg data time: 1.09e-01, avg batch time: 0.5101, average train loss: 1.9993
[09/17 01:22:11 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1426, average loss: 1.8702
[09/17 01:22:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 95.00	
[09/17 01:22:32 visual_prompt]: 	Test 100/190. loss: 2.121, 0.1824 s / batch. (data: 1.31e-04)max mem: 17.22448 GB 
[09/17 01:22:51 visual_prompt]: Inference (test):avg data time: 6.65e-03, avg batch time: 0.1928, average loss: 2.0996
[09/17 01:22:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.38	top5: 89.12	
[09/17 01:22:51 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/17 01:23:00 visual_prompt]: Epoch 41 / 100: avg data time: 9.82e-02, avg batch time: 0.5027, average train loss: 1.9819
[09/17 01:23:03 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1428, average loss: 2.9682
[09/17 01:23:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 61.00	
[09/17 01:23:24 visual_prompt]: 	Test 100/190. loss: 2.590, 0.2082 s / batch. (data: 2.64e-02)max mem: 17.22448 GB 
[09/17 01:23:43 visual_prompt]: Inference (test):avg data time: 9.10e-03, avg batch time: 0.1960, average loss: 2.9973
[09/17 01:23:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.58	top5: 66.06	
[09/17 01:23:43 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/17 01:23:52 visual_prompt]: Epoch 42 / 100: avg data time: 1.02e-01, avg batch time: 0.5032, average train loss: 1.9628
[09/17 01:23:55 visual_prompt]: Inference (val):avg data time: 1.95e-05, avg batch time: 0.1427, average loss: 1.6601
[09/17 01:23:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 94.50	
[09/17 01:24:16 visual_prompt]: 	Test 100/190. loss: 1.798, 0.1823 s / batch. (data: 2.65e-05)max mem: 17.22448 GB 
[09/17 01:24:34 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1924, average loss: 1.8362
[09/17 01:24:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.95	top5: 89.51	
[09/17 01:24:34 visual_prompt]: Best epoch 42: best metric: 0.320
[09/17 01:24:34 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/17 01:24:44 visual_prompt]: Epoch 43 / 100: avg data time: 1.14e-01, avg batch time: 0.5186, average train loss: 1.7151
[09/17 01:24:47 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1426, average loss: 1.6863
[09/17 01:24:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 94.00	
[09/17 01:25:08 visual_prompt]: 	Test 100/190. loss: 2.001, 0.1823 s / batch. (data: 1.13e-04)max mem: 17.22448 GB 
[09/17 01:25:26 visual_prompt]: Inference (test):avg data time: 8.41e-03, avg batch time: 0.1934, average loss: 1.9413
[09/17 01:25:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.91	top5: 88.79	
[09/17 01:25:26 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/17 01:25:36 visual_prompt]: Epoch 44 / 100: avg data time: 1.09e-01, avg batch time: 0.5099, average train loss: 1.8674
[09/17 01:25:39 visual_prompt]: Inference (val):avg data time: 3.56e-05, avg batch time: 0.1427, average loss: 2.2401
[09/17 01:25:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 82.50	
[09/17 01:26:00 visual_prompt]: 	Test 100/190. loss: 2.503, 0.1956 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 01:26:19 visual_prompt]: Inference (test):avg data time: 6.82e-03, avg batch time: 0.1950, average loss: 2.4848
[09/17 01:26:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.12	top5: 80.03	
[09/17 01:26:19 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/17 01:26:28 visual_prompt]: Epoch 45 / 100: avg data time: 1.14e-01, avg batch time: 0.5412, average train loss: 2.1031
[09/17 01:26:31 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1428, average loss: 1.8450
[09/17 01:26:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 91.00	
[09/17 01:26:53 visual_prompt]: 	Test 100/190. loss: 1.956, 0.1959 s / batch. (data: 1.42e-04)max mem: 17.22448 GB 
[09/17 01:27:12 visual_prompt]: Inference (test):avg data time: 6.99e-03, avg batch time: 0.1970, average loss: 2.0056
[09/17 01:27:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.02	top5: 86.28	
[09/17 01:27:12 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/17 01:27:21 visual_prompt]: Epoch 46 / 100: avg data time: 1.07e-01, avg batch time: 0.5116, average train loss: 1.9132
[09/17 01:27:24 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1426, average loss: 1.8871
[09/17 01:27:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.50	top5: 92.00	
[09/17 01:27:45 visual_prompt]: 	Test 100/190. loss: 2.134, 0.1974 s / batch. (data: 1.40e-02)max mem: 17.22448 GB 
[09/17 01:28:03 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1929, average loss: 2.1159
[09/17 01:28:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.21	top5: 86.25	
[09/17 01:28:04 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/17 01:28:13 visual_prompt]: Epoch 47 / 100: avg data time: 1.07e-01, avg batch time: 0.5092, average train loss: 1.7968
[09/17 01:28:16 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1427, average loss: 1.8043
[09/17 01:28:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 93.00	
[09/17 01:28:37 visual_prompt]: 	Test 100/190. loss: 2.202, 0.1958 s / batch. (data: 1.28e-04)max mem: 17.22448 GB 
[09/17 01:28:56 visual_prompt]: Inference (test):avg data time: 7.46e-03, avg batch time: 0.1941, average loss: 2.1017
[09/17 01:28:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.60	top5: 89.71	
[09/17 01:28:56 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/17 01:29:05 visual_prompt]: Epoch 48 / 100: avg data time: 1.04e-01, avg batch time: 0.5058, average train loss: 1.8208
[09/17 01:29:08 visual_prompt]: Inference (val):avg data time: 3.99e-05, avg batch time: 0.1456, average loss: 1.8623
[09/17 01:29:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 91.50	
[09/17 01:29:29 visual_prompt]: 	Test 100/190. loss: 2.533, 0.2202 s / batch. (data: 3.87e-02)max mem: 17.22448 GB 
[09/17 01:29:47 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1924, average loss: 2.2798
[09/17 01:29:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.69	top5: 85.55	
[09/17 01:29:47 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/17 01:29:56 visual_prompt]: Epoch 49 / 100: avg data time: 1.08e-01, avg batch time: 0.5094, average train loss: 1.9961
[09/17 01:29:59 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1427, average loss: 2.1156
[09/17 01:29:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 91.50	
[09/17 01:30:21 visual_prompt]: 	Test 100/190. loss: 2.035, 0.1968 s / batch. (data: 1.47e-02)max mem: 17.22448 GB 
[09/17 01:30:39 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1940, average loss: 2.4683
[09/17 01:30:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.97	top5: 87.14	
[09/17 01:30:39 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/17 01:30:48 visual_prompt]: Epoch 50 / 100: avg data time: 1.07e-01, avg batch time: 0.5092, average train loss: 1.8077
[09/17 01:30:51 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1426, average loss: 1.6584
[09/17 01:30:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 97.00	
[09/17 01:31:12 visual_prompt]: 	Test 100/190. loss: 2.034, 0.1961 s / batch. (data: 1.44e-02)max mem: 17.22448 GB 
[09/17 01:31:31 visual_prompt]: Inference (test):avg data time: 6.70e-03, avg batch time: 0.1916, average loss: 1.8910
[09/17 01:31:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.72	top5: 92.01	
[09/17 01:31:31 visual_prompt]: Best epoch 50: best metric: 0.345
[09/17 01:31:31 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/17 01:31:40 visual_prompt]: Epoch 51 / 100: avg data time: 1.07e-01, avg batch time: 0.5088, average train loss: 1.7168
[09/17 01:31:43 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.1428, average loss: 1.4994
[09/17 01:31:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.50	top5: 98.00	
[09/17 01:32:04 visual_prompt]: 	Test 100/190. loss: 2.040, 0.1828 s / batch. (data: 1.12e-04)max mem: 17.22448 GB 
[09/17 01:32:22 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1940, average loss: 1.8817
[09/17 01:32:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.83	top5: 91.94	
[09/17 01:32:22 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/17 01:32:32 visual_prompt]: Epoch 52 / 100: avg data time: 1.10e-01, avg batch time: 0.5108, average train loss: 1.5061
[09/17 01:32:35 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1427, average loss: 1.4110
[09/17 01:32:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.50	top5: 99.00	
[09/17 01:32:56 visual_prompt]: 	Test 100/190. loss: 1.835, 0.1960 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/17 01:33:14 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1921, average loss: 1.7995
[09/17 01:33:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.67	top5: 92.91	
[09/17 01:33:14 visual_prompt]: Best epoch 52: best metric: 0.385
[09/17 01:33:14 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/17 01:33:23 visual_prompt]: Epoch 53 / 100: avg data time: 1.13e-01, avg batch time: 0.5161, average train loss: 1.5338
[09/17 01:33:26 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1427, average loss: 1.6290
[09/17 01:33:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 94.50	
[09/17 01:33:48 visual_prompt]: 	Test 100/190. loss: 2.111, 0.1993 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/17 01:34:06 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1954, average loss: 2.0753
[09/17 01:34:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.41	top5: 87.06	
[09/17 01:34:06 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/17 01:34:16 visual_prompt]: Epoch 54 / 100: avg data time: 1.11e-01, avg batch time: 0.5152, average train loss: 1.4748
[09/17 01:34:19 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1425, average loss: 1.4061
[09/17 01:34:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 99.00	
[09/17 01:34:40 visual_prompt]: 	Test 100/190. loss: 1.913, 0.1824 s / batch. (data: 1.54e-04)max mem: 17.22448 GB 
[09/17 01:34:58 visual_prompt]: Inference (test):avg data time: 6.86e-03, avg batch time: 0.1922, average loss: 1.7959
[09/17 01:34:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.94	top5: 94.04	
[09/17 01:34:58 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/17 01:35:07 visual_prompt]: Epoch 55 / 100: avg data time: 1.01e-01, avg batch time: 0.5020, average train loss: 1.3815
[09/17 01:35:10 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1429, average loss: 1.2822
[09/17 01:35:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 46.00	top5: 99.50	
[09/17 01:35:32 visual_prompt]: 	Test 100/190. loss: 2.062, 0.1973 s / batch. (data: 1.53e-02)max mem: 17.22448 GB 
[09/17 01:35:50 visual_prompt]: Inference (test):avg data time: 7.90e-03, avg batch time: 0.1929, average loss: 1.8528
[09/17 01:35:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.02	top5: 93.63	
[09/17 01:35:50 visual_prompt]: Best epoch 55: best metric: 0.460
[09/17 01:35:50 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/17 01:35:59 visual_prompt]: Epoch 56 / 100: avg data time: 1.09e-01, avg batch time: 0.5139, average train loss: 1.3278
[09/17 01:36:03 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1427, average loss: 1.2961
[09/17 01:36:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 99.50	
[09/17 01:36:24 visual_prompt]: 	Test 100/190. loss: 2.064, 0.2077 s / batch. (data: 1.51e-02)max mem: 17.22448 GB 
[09/17 01:36:42 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1923, average loss: 1.7910
[09/17 01:36:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.31	top5: 93.02	
[09/17 01:36:42 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/17 01:36:51 visual_prompt]: Epoch 57 / 100: avg data time: 1.10e-01, avg batch time: 0.5111, average train loss: 1.3391
[09/17 01:36:54 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1425, average loss: 1.4636
[09/17 01:36:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.00	top5: 100.00	
[09/17 01:37:15 visual_prompt]: 	Test 100/190. loss: 2.289, 0.1828 s / batch. (data: 1.11e-04)max mem: 17.22448 GB 
[09/17 01:37:34 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1942, average loss: 2.0553
[09/17 01:37:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.29	top5: 92.60	
[09/17 01:37:34 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/17 01:37:43 visual_prompt]: Epoch 58 / 100: avg data time: 1.04e-01, avg batch time: 0.5065, average train loss: 1.4835
[09/17 01:37:46 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1427, average loss: 1.1314
[09/17 01:37:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 46.00	top5: 100.00	
[09/17 01:38:07 visual_prompt]: 	Test 100/190. loss: 1.766, 0.1841 s / batch. (data: 1.26e-04)max mem: 17.22448 GB 
[09/17 01:38:26 visual_prompt]: Inference (test):avg data time: 8.64e-03, avg batch time: 0.1941, average loss: 1.7390
[09/17 01:38:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.58	top5: 94.94	
[09/17 01:38:26 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/17 01:38:35 visual_prompt]: Epoch 59 / 100: avg data time: 1.10e-01, avg batch time: 0.5120, average train loss: 1.3554
[09/17 01:38:38 visual_prompt]: Inference (val):avg data time: 3.84e-05, avg batch time: 0.1427, average loss: 1.5376
[09/17 01:38:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.00	top5: 97.50	
[09/17 01:39:00 visual_prompt]: 	Test 100/190. loss: 2.330, 0.2078 s / batch. (data: 2.58e-02)max mem: 17.22448 GB 
[09/17 01:39:18 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1958, average loss: 2.2283
[09/17 01:39:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.33	top5: 93.34	
[09/17 01:39:18 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/17 01:39:27 visual_prompt]: Epoch 60 / 100: avg data time: 1.02e-01, avg batch time: 0.5031, average train loss: 1.4574
[09/17 01:39:30 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1427, average loss: 1.4334
[09/17 01:39:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 43.50	top5: 97.00	
[09/17 01:39:51 visual_prompt]: 	Test 100/190. loss: 1.984, 0.1826 s / batch. (data: 1.26e-04)max mem: 17.22448 GB 
[09/17 01:40:10 visual_prompt]: Inference (test):avg data time: 6.20e-03, avg batch time: 0.1918, average loss: 1.9717
[09/17 01:40:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.83	top5: 93.26	
[09/17 01:40:10 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/17 01:40:19 visual_prompt]: Epoch 61 / 100: avg data time: 1.17e-01, avg batch time: 0.5193, average train loss: 1.3365
[09/17 01:40:22 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1427, average loss: 1.2756
[09/17 01:40:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 44.50	top5: 99.50	
[09/17 01:40:43 visual_prompt]: 	Test 100/190. loss: 2.164, 0.1993 s / batch. (data: 1.37e-02)max mem: 17.22448 GB 
[09/17 01:41:02 visual_prompt]: Inference (test):avg data time: 7.02e-03, avg batch time: 0.1916, average loss: 1.9607
[09/17 01:41:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.36	top5: 93.68	
[09/17 01:41:02 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/17 01:41:11 visual_prompt]: Epoch 62 / 100: avg data time: 1.01e-01, avg batch time: 0.5035, average train loss: 1.2168
[09/17 01:41:14 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1427, average loss: 1.3768
[09/17 01:41:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 42.50	top5: 99.50	
[09/17 01:41:35 visual_prompt]: 	Test 100/190. loss: 2.536, 0.2041 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 01:41:54 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1940, average loss: 2.2762
[09/17 01:41:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.81	top5: 93.46	
[09/17 01:41:54 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/17 01:42:03 visual_prompt]: Epoch 63 / 100: avg data time: 1.09e-01, avg batch time: 0.5097, average train loss: 1.4013
[09/17 01:42:06 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1425, average loss: 1.4430
[09/17 01:42:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.00	top5: 99.00	
[09/17 01:42:27 visual_prompt]: 	Test 100/190. loss: 2.006, 0.1957 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 01:42:45 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1935, average loss: 1.9844
[09/17 01:42:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.40	top5: 92.44	
[09/17 01:42:46 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/17 01:42:55 visual_prompt]: Epoch 64 / 100: avg data time: 1.08e-01, avg batch time: 0.5088, average train loss: 1.4668
[09/17 01:42:58 visual_prompt]: Inference (val):avg data time: 2.94e-04, avg batch time: 0.2262, average loss: 1.3316
[09/17 01:42:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.50	top5: 97.00	
[09/17 01:43:19 visual_prompt]: 	Test 100/190. loss: 1.800, 0.1961 s / batch. (data: 1.40e-02)max mem: 17.22448 GB 
[09/17 01:43:38 visual_prompt]: Inference (test):avg data time: 6.85e-03, avg batch time: 0.1942, average loss: 1.8732
[09/17 01:43:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.19	top5: 91.74	
[09/17 01:43:38 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/17 01:43:47 visual_prompt]: Epoch 65 / 100: avg data time: 1.13e-01, avg batch time: 0.5148, average train loss: 1.3027
[09/17 01:43:50 visual_prompt]: Inference (val):avg data time: 1.14e-04, avg batch time: 0.2174, average loss: 1.1014
[09/17 01:43:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 49.50	top5: 100.00	
[09/17 01:44:11 visual_prompt]: 	Test 100/190. loss: 2.104, 0.1828 s / batch. (data: 1.18e-04)max mem: 17.22448 GB 
[09/17 01:44:29 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1925, average loss: 1.8537
[09/17 01:44:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.46	top5: 94.38	
[09/17 01:44:29 visual_prompt]: Best epoch 65: best metric: 0.495
[09/17 01:44:29 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/17 01:44:39 visual_prompt]: Epoch 66 / 100: avg data time: 1.09e-01, avg batch time: 0.5104, average train loss: 1.0758
[09/17 01:44:42 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1428, average loss: 0.9457
[09/17 01:44:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 57.00	top5: 100.00	
[09/17 01:45:03 visual_prompt]: 	Test 100/190. loss: 1.848, 0.1866 s / batch. (data: 1.20e-04)max mem: 17.22448 GB 
[09/17 01:45:21 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1926, average loss: 1.8346
[09/17 01:45:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.87	top5: 95.29	
[09/17 01:45:21 visual_prompt]: Best epoch 66: best metric: 0.570
[09/17 01:45:21 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/17 01:45:31 visual_prompt]: Epoch 67 / 100: avg data time: 1.15e-01, avg batch time: 0.5163, average train loss: 0.9580
[09/17 01:45:34 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1428, average loss: 1.2280
[09/17 01:45:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 44.50	top5: 99.50	
[09/17 01:45:55 visual_prompt]: 	Test 100/190. loss: 2.149, 0.1947 s / batch. (data: 1.09e-04)max mem: 17.22448 GB 
[09/17 01:46:14 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1944, average loss: 2.1293
[09/17 01:46:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.39	top5: 94.19	
[09/17 01:46:14 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/17 01:46:23 visual_prompt]: Epoch 68 / 100: avg data time: 1.14e-01, avg batch time: 0.5147, average train loss: 1.1210
[09/17 01:46:26 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1427, average loss: 1.1092
[09/17 01:46:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 51.00	top5: 99.50	
[09/17 01:46:47 visual_prompt]: 	Test 100/190. loss: 1.560, 0.2105 s / batch. (data: 1.31e-02)max mem: 17.22448 GB 
[09/17 01:47:06 visual_prompt]: Inference (test):avg data time: 7.35e-03, avg batch time: 0.1942, average loss: 1.8510
[09/17 01:47:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.72	top5: 92.91	
[09/17 01:47:06 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/17 01:47:15 visual_prompt]: Epoch 69 / 100: avg data time: 1.07e-01, avg batch time: 0.5077, average train loss: 0.9717
[09/17 01:47:18 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1427, average loss: 0.8823
[09/17 01:47:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 61.50	top5: 100.00	
[09/17 01:47:39 visual_prompt]: 	Test 100/190. loss: 2.215, 0.1833 s / batch. (data: 1.26e-04)max mem: 17.22448 GB 
[09/17 01:47:58 visual_prompt]: Inference (test):avg data time: 8.66e-03, avg batch time: 0.1938, average loss: 2.0840
[09/17 01:47:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.14	top5: 95.29	
[09/17 01:47:58 visual_prompt]: Best epoch 69: best metric: 0.615
[09/17 01:47:58 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/17 01:48:07 visual_prompt]: Epoch 70 / 100: avg data time: 1.07e-01, avg batch time: 0.5077, average train loss: 0.8873
[09/17 01:48:10 visual_prompt]: Inference (val):avg data time: 2.18e-05, avg batch time: 0.1426, average loss: 0.8395
[09/17 01:48:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 60.50	top5: 100.00	
[09/17 01:48:31 visual_prompt]: 	Test 100/190. loss: 1.971, 0.1932 s / batch. (data: 1.13e-02)max mem: 17.22448 GB 
[09/17 01:48:50 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1930, average loss: 1.9848
[09/17 01:48:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.98	top5: 95.66	
[09/17 01:48:50 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/17 01:48:59 visual_prompt]: Epoch 71 / 100: avg data time: 1.06e-01, avg batch time: 0.5089, average train loss: 0.9141
[09/17 01:49:02 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1425, average loss: 0.9185
[09/17 01:49:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 55.00	top5: 100.00	
[09/17 01:49:23 visual_prompt]: 	Test 100/190. loss: 1.978, 0.1920 s / batch. (data: 9.73e-05)max mem: 17.22448 GB 
[09/17 01:49:42 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1940, average loss: 2.1005
[09/17 01:49:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.06	top5: 95.61	
[09/17 01:49:42 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/17 01:49:51 visual_prompt]: Epoch 72 / 100: avg data time: 1.06e-01, avg batch time: 0.5076, average train loss: 0.7879
[09/17 01:49:54 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.1426, average loss: 0.9220
[09/17 01:49:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 58.00	top5: 100.00	
[09/17 01:50:15 visual_prompt]: 	Test 100/190. loss: 2.674, 0.1830 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/17 01:50:34 visual_prompt]: Inference (test):avg data time: 8.90e-03, avg batch time: 0.1941, average loss: 2.5623
[09/17 01:50:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.97	top5: 95.49	
[09/17 01:50:34 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/17 01:50:43 visual_prompt]: Epoch 73 / 100: avg data time: 1.02e-01, avg batch time: 0.5089, average train loss: 0.9600
[09/17 01:50:46 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1431, average loss: 0.9758
[09/17 01:50:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 58.00	top5: 100.00	
[09/17 01:51:07 visual_prompt]: 	Test 100/190. loss: 1.868, 0.1956 s / batch. (data: 1.36e-02)max mem: 17.22448 GB 
[09/17 01:51:26 visual_prompt]: Inference (test):avg data time: 7.84e-03, avg batch time: 0.1933, average loss: 2.0826
[09/17 01:51:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.99	top5: 93.74	
[09/17 01:51:26 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/17 01:51:35 visual_prompt]: Epoch 74 / 100: avg data time: 1.05e-01, avg batch time: 0.5044, average train loss: 0.8817
[09/17 01:51:38 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1427, average loss: 0.8010
[09/17 01:51:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 63.00	top5: 100.00	
[09/17 01:51:59 visual_prompt]: 	Test 100/190. loss: 2.226, 0.1823 s / batch. (data: 1.44e-04)max mem: 17.22448 GB 
[09/17 01:52:17 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1928, average loss: 2.2721
[09/17 01:52:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.54	top5: 94.64	
[09/17 01:52:17 visual_prompt]: Best epoch 74: best metric: 0.630
[09/17 01:52:17 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/17 01:52:26 visual_prompt]: Epoch 75 / 100: avg data time: 1.03e-01, avg batch time: 0.5060, average train loss: 0.6881
[09/17 01:52:29 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1429, average loss: 0.6426
[09/17 01:52:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 72.50	top5: 100.00	
[09/17 01:52:51 visual_prompt]: 	Test 100/190. loss: 2.540, 0.1992 s / batch. (data: 1.47e-02)max mem: 17.22448 GB 
[09/17 01:53:09 visual_prompt]: Inference (test):avg data time: 8.47e-03, avg batch time: 0.1956, average loss: 2.3041
[09/17 01:53:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.82	top5: 96.11	
[09/17 01:53:10 visual_prompt]: Best epoch 75: best metric: 0.725
[09/17 01:53:10 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/17 01:53:18 visual_prompt]: Epoch 76 / 100: avg data time: 9.26e-02, avg batch time: 0.4977, average train loss: 0.7825
[09/17 01:53:22 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1427, average loss: 0.7746
[09/17 01:53:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 65.00	top5: 100.00	
[09/17 01:53:43 visual_prompt]: 	Test 100/190. loss: 2.637, 0.1824 s / batch. (data: 1.39e-04)max mem: 17.22448 GB 
[09/17 01:54:02 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1958, average loss: 2.2566
[09/17 01:54:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.60	top5: 95.86	
[09/17 01:54:02 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/17 01:54:11 visual_prompt]: Epoch 77 / 100: avg data time: 1.06e-01, avg batch time: 0.5074, average train loss: 0.7413
[09/17 01:54:14 visual_prompt]: Inference (val):avg data time: 2.29e-05, avg batch time: 0.1426, average loss: 0.7285
[09/17 01:54:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 67.50	top5: 100.00	
[09/17 01:54:35 visual_prompt]: 	Test 100/190. loss: 2.519, 0.1973 s / batch. (data: 1.52e-02)max mem: 17.22448 GB 
[09/17 01:54:53 visual_prompt]: Inference (test):avg data time: 6.24e-03, avg batch time: 0.1920, average loss: 2.2943
[09/17 01:54:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.30	top5: 95.52	
[09/17 01:54:53 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/17 01:55:02 visual_prompt]: Epoch 78 / 100: avg data time: 1.03e-01, avg batch time: 0.5041, average train loss: 0.5849
[09/17 01:55:05 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1428, average loss: 0.4930
[09/17 01:55:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 81.00	top5: 100.00	
[09/17 01:55:26 visual_prompt]: 	Test 100/190. loss: 2.988, 0.1825 s / batch. (data: 1.78e-04)max mem: 17.22448 GB 
[09/17 01:55:45 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1935, average loss: 2.7359
[09/17 01:55:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.80	top5: 95.83	
[09/17 01:55:45 visual_prompt]: Best epoch 78: best metric: 0.810
[09/17 01:55:45 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/17 01:55:54 visual_prompt]: Epoch 79 / 100: avg data time: 9.14e-02, avg batch time: 0.4956, average train loss: 0.5606
[09/17 01:55:57 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1426, average loss: 0.3917
[09/17 01:55:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 85.00	top5: 100.00	
[09/17 01:56:18 visual_prompt]: 	Test 100/190. loss: 3.467, 0.1854 s / batch. (data: 1.57e-04)max mem: 17.22448 GB 
[09/17 01:56:36 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1930, average loss: 3.2588
[09/17 01:56:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.99	top5: 95.51	
[09/17 01:56:36 visual_prompt]: Best epoch 79: best metric: 0.850
[09/17 01:56:36 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/17 01:56:45 visual_prompt]: Epoch 80 / 100: avg data time: 1.01e-01, avg batch time: 0.5042, average train loss: 0.5222
[09/17 01:56:49 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1427, average loss: 0.7232
[09/17 01:56:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 71.00	top5: 100.00	
[09/17 01:57:10 visual_prompt]: 	Test 100/190. loss: 3.592, 0.1970 s / batch. (data: 1.50e-02)max mem: 17.22448 GB 
[09/17 01:57:28 visual_prompt]: Inference (test):avg data time: 6.85e-03, avg batch time: 0.1924, average loss: 3.2145
[09/17 01:57:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.79	top5: 94.65	
[09/17 01:57:29 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/17 01:57:38 visual_prompt]: Epoch 81 / 100: avg data time: 1.02e-01, avg batch time: 0.5063, average train loss: 0.6533
[09/17 01:57:41 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1427, average loss: 0.5406
[09/17 01:57:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 79.50	top5: 100.00	
[09/17 01:58:02 visual_prompt]: 	Test 100/190. loss: 2.124, 0.1947 s / batch. (data: 1.23e-02)max mem: 17.22448 GB 
[09/17 01:58:21 visual_prompt]: Inference (test):avg data time: 7.26e-03, avg batch time: 0.1944, average loss: 2.1439
[09/17 01:58:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.90	top5: 95.49	
[09/17 01:58:21 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/17 01:58:30 visual_prompt]: Epoch 82 / 100: avg data time: 1.04e-01, avg batch time: 0.5103, average train loss: 0.4520
[09/17 01:58:33 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1427, average loss: 0.4290
[09/17 01:58:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 83.50	top5: 100.00	
[09/17 01:58:54 visual_prompt]: 	Test 100/190. loss: 3.205, 0.1827 s / batch. (data: 1.51e-04)max mem: 17.22448 GB 
[09/17 01:59:12 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1923, average loss: 3.1717
[09/17 01:59:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 94.92	
[09/17 01:59:12 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/17 01:59:21 visual_prompt]: Epoch 83 / 100: avg data time: 1.12e-01, avg batch time: 0.5138, average train loss: 0.3873
[09/17 01:59:24 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1429, average loss: 0.4733
[09/17 01:59:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 82.00	top5: 100.00	
[09/17 01:59:45 visual_prompt]: 	Test 100/190. loss: 3.574, 0.1827 s / batch. (data: 1.31e-04)max mem: 17.22448 GB 
[09/17 02:00:04 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.1925, average loss: 3.5377
[09/17 02:00:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.35	top5: 95.40	
[09/17 02:00:04 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/17 02:00:13 visual_prompt]: Epoch 84 / 100: avg data time: 1.05e-01, avg batch time: 0.5093, average train loss: 0.3422
[09/17 02:00:16 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1427, average loss: 0.3069
[09/17 02:00:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 88.00	top5: 100.00	
[09/17 02:00:37 visual_prompt]: 	Test 100/190. loss: 3.933, 0.1959 s / batch. (data: 1.36e-02)max mem: 17.22448 GB 
[09/17 02:00:55 visual_prompt]: Inference (test):avg data time: 7.25e-03, avg batch time: 0.1930, average loss: 3.6115
[09/17 02:00:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.31	top5: 95.06	
[09/17 02:00:55 visual_prompt]: Best epoch 84: best metric: 0.880
[09/17 02:00:55 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/17 02:01:04 visual_prompt]: Epoch 85 / 100: avg data time: 1.10e-01, avg batch time: 0.5149, average train loss: 0.3348
[09/17 02:01:08 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1432, average loss: 0.2555
[09/17 02:01:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 93.50	top5: 100.00	
[09/17 02:01:29 visual_prompt]: 	Test 100/190. loss: 4.007, 0.1958 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 02:01:47 visual_prompt]: Inference (test):avg data time: 6.32e-03, avg batch time: 0.1923, average loss: 4.0042
[09/17 02:01:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.76	top5: 95.51	
[09/17 02:01:47 visual_prompt]: Best epoch 85: best metric: 0.935
[09/17 02:01:47 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/17 02:01:56 visual_prompt]: Epoch 86 / 100: avg data time: 1.05e-01, avg batch time: 0.5101, average train loss: 0.2875
[09/17 02:01:59 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1427, average loss: 0.1324
[09/17 02:01:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 96.00	top5: 100.00	
[09/17 02:02:20 visual_prompt]: 	Test 100/190. loss: 3.801, 0.1818 s / batch. (data: 9.56e-05)max mem: 17.22448 GB 
[09/17 02:02:39 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1921, average loss: 3.9178
[09/17 02:02:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.43	top5: 96.11	
[09/17 02:02:39 visual_prompt]: Best epoch 86: best metric: 0.960
[09/17 02:02:39 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/17 02:02:48 visual_prompt]: Epoch 87 / 100: avg data time: 1.04e-01, avg batch time: 0.5094, average train loss: 0.2968
[09/17 02:02:51 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1428, average loss: 0.2968
[09/17 02:02:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 88.00	top5: 100.00	
[09/17 02:03:12 visual_prompt]: 	Test 100/190. loss: 4.689, 0.1846 s / batch. (data: 1.29e-04)max mem: 17.22448 GB 
[09/17 02:03:30 visual_prompt]: Inference (test):avg data time: 7.49e-03, avg batch time: 0.1930, average loss: 4.3303
[09/17 02:03:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.78	top5: 95.44	
[09/17 02:03:30 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/17 02:03:40 visual_prompt]: Epoch 88 / 100: avg data time: 1.09e-01, avg batch time: 0.5096, average train loss: 0.2612
[09/17 02:03:43 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1448, average loss: 0.3974
[09/17 02:03:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 86.00	top5: 100.00	
[09/17 02:04:04 visual_prompt]: 	Test 100/190. loss: 3.915, 0.2232 s / batch. (data: 4.15e-02)max mem: 17.22448 GB 
[09/17 02:04:22 visual_prompt]: Inference (test):avg data time: 6.74e-03, avg batch time: 0.1919, average loss: 4.3124
[09/17 02:04:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.99	top5: 95.42	
[09/17 02:04:22 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/17 02:04:31 visual_prompt]: Epoch 89 / 100: avg data time: 1.09e-01, avg batch time: 0.5295, average train loss: 0.1755
[09/17 02:04:34 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1428, average loss: 0.1656
[09/17 02:04:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 94.00	top5: 100.00	
[09/17 02:04:56 visual_prompt]: 	Test 100/190. loss: 4.630, 0.1825 s / batch. (data: 1.21e-04)max mem: 17.22448 GB 
[09/17 02:05:14 visual_prompt]: Inference (test):avg data time: 7.18e-03, avg batch time: 0.1942, average loss: 4.6425
[09/17 02:05:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.86	top5: 95.22	
[09/17 02:05:14 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/17 02:05:24 visual_prompt]: Epoch 90 / 100: avg data time: 1.13e-01, avg batch time: 0.5137, average train loss: 0.1328
[09/17 02:05:27 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1480, average loss: 0.1497
[09/17 02:05:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 94.50	top5: 100.00	
[09/17 02:05:48 visual_prompt]: 	Test 100/190. loss: 5.181, 0.2002 s / batch. (data: 1.80e-02)max mem: 17.22448 GB 
[09/17 02:06:06 visual_prompt]: Inference (test):avg data time: 8.43e-03, avg batch time: 0.1931, average loss: 4.9372
[09/17 02:06:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.10	top5: 95.13	
[09/17 02:06:06 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/17 02:06:15 visual_prompt]: Epoch 91 / 100: avg data time: 9.16e-02, avg batch time: 0.4998, average train loss: 0.0948
[09/17 02:06:19 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1427, average loss: 0.0907
[09/17 02:06:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 96.50	top5: 100.00	
[09/17 02:06:40 visual_prompt]: 	Test 100/190. loss: 4.751, 0.1981 s / batch. (data: 1.59e-02)max mem: 17.22448 GB 
[09/17 02:06:58 visual_prompt]: Inference (test):avg data time: 7.62e-03, avg batch time: 0.1923, average loss: 4.8085
[09/17 02:06:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.25	top5: 95.57	
[09/17 02:06:58 visual_prompt]: Best epoch 91: best metric: 0.965
[09/17 02:06:58 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/17 02:07:07 visual_prompt]: Epoch 92 / 100: avg data time: 1.02e-01, avg batch time: 0.5064, average train loss: 0.0790
[09/17 02:07:10 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.1425, average loss: 0.1102
[09/17 02:07:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 96.00	top5: 100.00	
[09/17 02:07:31 visual_prompt]: 	Test 100/190. loss: 4.901, 0.1823 s / batch. (data: 1.32e-04)max mem: 17.22448 GB 
[09/17 02:07:50 visual_prompt]: Inference (test):avg data time: 6.21e-03, avg batch time: 0.1919, average loss: 5.1345
[09/17 02:07:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.98	top5: 95.47	
[09/17 02:07:50 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/17 02:07:59 visual_prompt]: Epoch 93 / 100: avg data time: 1.10e-01, avg batch time: 0.5116, average train loss: 0.0531
[09/17 02:08:02 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1427, average loss: 0.0865
[09/17 02:08:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 96.00	top5: 100.00	
[09/17 02:08:23 visual_prompt]: 	Test 100/190. loss: 5.154, 0.1826 s / batch. (data: 1.01e-04)max mem: 17.22448 GB 
[09/17 02:08:41 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1919, average loss: 5.2669
[09/17 02:08:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.35	top5: 95.56	
[09/17 02:08:41 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/17 02:08:51 visual_prompt]: Epoch 94 / 100: avg data time: 1.11e-01, avg batch time: 0.5119, average train loss: 0.0485
[09/17 02:08:54 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1426, average loss: 0.0794
[09/17 02:08:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 97.00	top5: 100.00	
[09/17 02:09:15 visual_prompt]: 	Test 100/190. loss: 5.302, 0.1836 s / batch. (data: 1.22e-04)max mem: 17.22448 GB 
[09/17 02:09:34 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1964, average loss: 5.3447
[09/17 02:09:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.33	top5: 95.56	
[09/17 02:09:34 visual_prompt]: Best epoch 94: best metric: 0.970
[09/17 02:09:34 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/17 02:09:43 visual_prompt]: Epoch 95 / 100: avg data time: 9.94e-02, avg batch time: 0.5027, average train loss: 0.0462
[09/17 02:09:46 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1427, average loss: 0.0824
[09/17 02:09:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 96.50	top5: 100.00	
[09/17 02:10:07 visual_prompt]: 	Test 100/190. loss: 5.274, 0.1910 s / batch. (data: 1.09e-04)max mem: 17.22448 GB 
[09/17 02:10:25 visual_prompt]: Inference (test):avg data time: 6.87e-03, avg batch time: 0.1925, average loss: 5.3467
[09/17 02:10:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.65	top5: 95.55	
[09/17 02:10:25 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/17 02:10:34 visual_prompt]: Epoch 96 / 100: avg data time: 1.14e-01, avg batch time: 0.5147, average train loss: 0.0423
[09/17 02:10:37 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1427, average loss: 0.0550
[09/17 02:10:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 98.50	top5: 100.00	
[09/17 02:10:59 visual_prompt]: 	Test 100/190. loss: 5.499, 0.2175 s / batch. (data: 2.60e-02)max mem: 17.22448 GB 
[09/17 02:11:17 visual_prompt]: Inference (test):avg data time: 6.34e-03, avg batch time: 0.1920, average loss: 5.4944
[09/17 02:11:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.07	top5: 95.48	
[09/17 02:11:17 visual_prompt]: Best epoch 96: best metric: 0.985
[09/17 02:11:17 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/17 02:11:26 visual_prompt]: Epoch 97 / 100: avg data time: 1.09e-01, avg batch time: 0.5101, average train loss: 0.0432
[09/17 02:11:29 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1424, average loss: 0.0582
[09/17 02:11:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 98.50	top5: 100.00	
[09/17 02:11:50 visual_prompt]: 	Test 100/190. loss: 5.438, 0.1958 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/17 02:12:09 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1935, average loss: 5.4683
[09/17 02:12:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.50	top5: 95.55	
[09/17 02:12:09 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/17 02:12:18 visual_prompt]: Epoch 98 / 100: avg data time: 1.06e-01, avg batch time: 0.5079, average train loss: 0.0370
[09/17 02:12:21 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1427, average loss: 0.0536
[09/17 02:12:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 98.50	top5: 100.00	
[09/17 02:12:43 visual_prompt]: 	Test 100/190. loss: 5.449, 0.1843 s / batch. (data: 1.36e-04)max mem: 17.22448 GB 
[09/17 02:13:02 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1959, average loss: 5.4564
[09/17 02:13:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.51	top5: 95.56	
[09/17 02:13:02 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/17 02:13:11 visual_prompt]: Epoch 99 / 100: avg data time: 1.11e-01, avg batch time: 0.5121, average train loss: 0.0331
[09/17 02:13:14 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1430, average loss: 0.0539
[09/17 02:13:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 98.50	top5: 100.00	
[09/17 02:13:35 visual_prompt]: 	Test 100/190. loss: 5.445, 0.1877 s / batch. (data: 1.52e-04)max mem: 17.22448 GB 
[09/17 02:13:53 visual_prompt]: Inference (test):avg data time: 6.96e-03, avg batch time: 0.1927, average loss: 5.4545
[09/17 02:13:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.48	top5: 95.58	
[09/17 02:13:53 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/17 02:14:03 visual_prompt]: Epoch 100 / 100: avg data time: 1.01e-01, avg batch time: 0.5105, average train loss: 0.0314
[09/17 02:14:06 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1427, average loss: 0.0545
[09/17 02:14:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 98.50	top5: 100.00	
[09/17 02:14:27 visual_prompt]: 	Test 100/190. loss: 5.449, 0.1821 s / batch. (data: 1.43e-04)max mem: 17.22448 GB 
[09/17 02:14:45 visual_prompt]: Inference (test):avg data time: 6.33e-03, avg batch time: 0.1922, average loss: 5.4594
[09/17 02:14:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.49	top5: 95.58	
[09/17 02:15:07 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 02:15:07 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 02:15:07 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed82'], train_type='')
[09/17 02:15:07 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 02:15:07 visual_prompt]: Training with config:
[09/17 02:15:07 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")',
          'NO_TEST': False,
          'NUMBER_CLASSES': 9,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed82/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 02:15:07 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 02:15:07.820592: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 02:15:07.988884: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 02:15:08.913955: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 02:15:08.914034: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 02:15:08.914043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 02:15:11.025432: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 02:15:11.025546: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 02:15:11.025563: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 02:15:11 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
2023-09-17 02:15:11.056896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 02:15:12 visual_prompt]: Number of images: 1000
[09/17 02:15:12 visual_prompt]: Number of classes: 9 / 9
[09/17 02:15:12 visual_prompt]: Loading validation data...
[09/17 02:15:12 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 02:15:13 visual_prompt]: Number of images: 200
[09/17 02:15:13 visual_prompt]: Number of classes: 9 / 9
[09/17 02:15:13 visual_prompt]: Loading test data...
[09/17 02:15:13 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 02:15:30 visual_prompt]: Number of images: 12150
[09/17 02:15:30 visual_prompt]: Number of classes: 9 / 9
[09/17 02:15:30 visual_prompt]: Constructing models...
[09/17 02:15:33 visual_prompt]: Total Parameters: 86727177	 Gradient Parameters: 928521
[09/17 02:15:33 visual_prompt]: tuned percent:1.071
[09/17 02:15:35 visual_prompt]: Device used for model: 0
[09/17 02:15:35 visual_prompt]: Setting up Evalutator...
[09/17 02:15:35 visual_prompt]: Setting up Trainer...
[09/17 02:15:35 visual_prompt]: 	Setting up the optimizer...
[09/17 02:15:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 02:15:46 visual_prompt]: Epoch 1 / 100: avg data time: 1.27e-01, avg batch time: 0.6000, average train loss: 2.7858
[09/17 02:15:49 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1418, average loss: 2.7724
[09/17 02:15:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.00	top5: 54.50	
[09/17 02:16:10 visual_prompt]: 	Test 100/190. loss: 2.636, 0.1816 s / batch. (data: 1.18e-04)max mem: 17.22448 GB 
[09/17 02:16:29 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1916, average loss: 2.6964
[09/17 02:16:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 57.05	
[09/17 02:16:29 visual_prompt]: Best epoch 1: best metric: 0.080
[09/17 02:16:29 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/17 02:16:38 visual_prompt]: Epoch 2 / 100: avg data time: 1.11e-01, avg batch time: 0.5148, average train loss: 4.5295
[09/17 02:16:41 visual_prompt]: Inference (val):avg data time: 2.00e-05, avg batch time: 0.1422, average loss: 3.4546
[09/17 02:16:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.00	
[09/17 02:17:02 visual_prompt]: 	Test 100/190. loss: 3.771, 0.1825 s / batch. (data: 3.41e-05)max mem: 17.22448 GB 
[09/17 02:17:21 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1934, average loss: 3.8787
[09/17 02:17:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.06	
[09/17 02:17:21 visual_prompt]: Best epoch 2: best metric: 0.095
[09/17 02:17:21 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/17 02:17:30 visual_prompt]: Epoch 3 / 100: avg data time: 1.08e-01, avg batch time: 0.5110, average train loss: 2.6608
[09/17 02:17:33 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1426, average loss: 2.3056
[09/17 02:17:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/17 02:17:54 visual_prompt]: 	Test 100/190. loss: 2.397, 0.1850 s / batch. (data: 1.29e-04)max mem: 17.22448 GB 
[09/17 02:18:13 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1933, average loss: 2.3858
[09/17 02:18:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.28	
[09/17 02:18:13 visual_prompt]: Best epoch 3: best metric: 0.130
[09/17 02:18:13 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/17 02:18:22 visual_prompt]: Epoch 4 / 100: avg data time: 1.02e-01, avg batch time: 0.5030, average train loss: 2.3682
[09/17 02:18:25 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1428, average loss: 2.4468
[09/17 02:18:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.00	
[09/17 02:18:47 visual_prompt]: 	Test 100/190. loss: 2.512, 0.1931 s / batch. (data: 1.05e-02)max mem: 17.22448 GB 
[09/17 02:19:06 visual_prompt]: Inference (test):avg data time: 6.63e-03, avg batch time: 0.1978, average loss: 2.5239
[09/17 02:19:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.04	
[09/17 02:19:06 visual_prompt]: Best epoch 4: best metric: 0.145
[09/17 02:19:06 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/17 02:19:15 visual_prompt]: Epoch 5 / 100: avg data time: 1.14e-01, avg batch time: 0.5334, average train loss: 2.4060
[09/17 02:19:19 visual_prompt]: Inference (val):avg data time: 3.66e-05, avg batch time: 0.2370, average loss: 2.4192
[09/17 02:19:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/17 02:19:40 visual_prompt]: 	Test 100/190. loss: 2.458, 0.1902 s / batch. (data: 1.39e-04)max mem: 17.22448 GB 
[09/17 02:19:58 visual_prompt]: Inference (test):avg data time: 8.03e-03, avg batch time: 0.1938, average loss: 2.4782
[09/17 02:19:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.77	top5: 55.34	
[09/17 02:19:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/17 02:20:08 visual_prompt]: Epoch 6 / 100: avg data time: 1.19e-01, avg batch time: 0.5185, average train loss: 2.5297
[09/17 02:20:11 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1424, average loss: 2.5876
[09/17 02:20:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.50	
[09/17 02:20:32 visual_prompt]: 	Test 100/190. loss: 2.434, 0.1826 s / batch. (data: 1.03e-04)max mem: 17.22448 GB 
[09/17 02:20:51 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1931, average loss: 2.5989
[09/17 02:20:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 60.21	
[09/17 02:20:51 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/17 02:21:00 visual_prompt]: Epoch 7 / 100: avg data time: 1.17e-01, avg batch time: 0.5178, average train loss: 2.6000
[09/17 02:21:03 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1425, average loss: 2.5855
[09/17 02:21:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 54.00	
[09/17 02:21:24 visual_prompt]: 	Test 100/190. loss: 2.553, 0.1960 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 02:21:43 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.1927, average loss: 2.5551
[09/17 02:21:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.55	top5: 58.63	
[09/17 02:21:43 visual_prompt]: Best epoch 7: best metric: 0.200
[09/17 02:21:43 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/17 02:21:52 visual_prompt]: Epoch 8 / 100: avg data time: 1.11e-01, avg batch time: 0.5119, average train loss: 3.4991
[09/17 02:21:55 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1425, average loss: 5.1213
[09/17 02:21:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.00	
[09/17 02:22:16 visual_prompt]: 	Test 100/190. loss: 4.849, 0.1831 s / batch. (data: 1.54e-04)max mem: 17.22448 GB 
[09/17 02:22:34 visual_prompt]: Inference (test):avg data time: 7.71e-03, avg batch time: 0.1924, average loss: 5.2091
[09/17 02:22:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.21	top5: 55.88	
[09/17 02:22:34 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/17 02:22:44 visual_prompt]: Epoch 9 / 100: avg data time: 1.08e-01, avg batch time: 0.5101, average train loss: 11.7999
[09/17 02:22:47 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1427, average loss: 16.1357
[09/17 02:22:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.50	
[09/17 02:23:08 visual_prompt]: 	Test 100/190. loss: 16.481, 0.1824 s / batch. (data: 1.12e-04)max mem: 17.22448 GB 
[09/17 02:23:27 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1937, average loss: 15.8407
[09/17 02:23:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 55.35	
[09/17 02:23:27 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/17 02:23:36 visual_prompt]: Epoch 10 / 100: avg data time: 1.11e-01, avg batch time: 0.5118, average train loss: 10.2443
[09/17 02:23:39 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1426, average loss: 8.3810
[09/17 02:23:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 58.50	
[09/17 02:24:00 visual_prompt]: 	Test 100/190. loss: 8.841, 0.2101 s / batch. (data: 1.35e-02)max mem: 17.22448 GB 
[09/17 02:24:19 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1929, average loss: 8.4705
[09/17 02:24:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 55.96	
[09/17 02:24:19 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/17 02:24:28 visual_prompt]: Epoch 11 / 100: avg data time: 1.15e-01, avg batch time: 0.5171, average train loss: 8.1706
[09/17 02:24:31 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1428, average loss: 8.3384
[09/17 02:24:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/17 02:24:52 visual_prompt]: 	Test 100/190. loss: 8.350, 0.1825 s / batch. (data: 1.03e-04)max mem: 17.22448 GB 
[09/17 02:25:11 visual_prompt]: Inference (test):avg data time: 6.76e-03, avg batch time: 0.1924, average loss: 8.1324
[09/17 02:25:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.52	
[09/17 02:25:11 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/17 02:25:20 visual_prompt]: Epoch 12 / 100: avg data time: 9.56e-02, avg batch time: 0.4996, average train loss: 6.2669
[09/17 02:25:23 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.1427, average loss: 5.5687
[09/17 02:25:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 55.00	
[09/17 02:25:44 visual_prompt]: 	Test 100/190. loss: 6.416, 0.1817 s / batch. (data: 1.22e-04)max mem: 17.22448 GB 
[09/17 02:26:03 visual_prompt]: Inference (test):avg data time: 7.82e-03, avg batch time: 0.1932, average loss: 5.4124
[09/17 02:26:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 56.02	
[09/17 02:26:03 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/17 02:26:12 visual_prompt]: Epoch 13 / 100: avg data time: 1.08e-01, avg batch time: 0.5098, average train loss: 5.4905
[09/17 02:26:15 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1429, average loss: 4.4260
[09/17 02:26:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/17 02:26:36 visual_prompt]: 	Test 100/190. loss: 4.773, 0.2128 s / batch. (data: 1.21e-04)max mem: 17.22448 GB 
[09/17 02:26:55 visual_prompt]: Inference (test):avg data time: 7.06e-03, avg batch time: 0.1918, average loss: 4.4925
[09/17 02:26:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.45	
[09/17 02:26:55 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/17 02:27:04 visual_prompt]: Epoch 14 / 100: avg data time: 1.16e-01, avg batch time: 0.5189, average train loss: 5.8249
[09/17 02:27:07 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 6.1886
[09/17 02:27:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/17 02:27:29 visual_prompt]: 	Test 100/190. loss: 6.285, 0.1936 s / batch. (data: 1.10e-04)max mem: 17.22448 GB 
[09/17 02:27:47 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1962, average loss: 6.2701
[09/17 02:27:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 56.42	
[09/17 02:27:47 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/17 02:27:57 visual_prompt]: Epoch 15 / 100: avg data time: 1.17e-01, avg batch time: 0.5225, average train loss: 5.6383
[09/17 02:28:00 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1426, average loss: 3.5873
[09/17 02:28:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/17 02:28:21 visual_prompt]: 	Test 100/190. loss: 3.587, 0.1828 s / batch. (data: 1.55e-04)max mem: 17.22448 GB 
[09/17 02:28:40 visual_prompt]: Inference (test):avg data time: 6.80e-03, avg batch time: 0.1936, average loss: 3.6655
[09/17 02:28:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.11	
[09/17 02:28:40 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/17 02:28:49 visual_prompt]: Epoch 16 / 100: avg data time: 1.21e-01, avg batch time: 0.5234, average train loss: 3.6558
[09/17 02:28:52 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1425, average loss: 3.1166
[09/17 02:28:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.50	
[09/17 02:29:13 visual_prompt]: 	Test 100/190. loss: 2.841, 0.1998 s / batch. (data: 1.37e-02)max mem: 17.22448 GB 
[09/17 02:29:32 visual_prompt]: Inference (test):avg data time: 6.95e-03, avg batch time: 0.1920, average loss: 3.1227
[09/17 02:29:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.24	
[09/17 02:29:32 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/17 02:29:41 visual_prompt]: Epoch 17 / 100: avg data time: 1.12e-01, avg batch time: 0.5147, average train loss: 2.8680
[09/17 02:29:44 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1426, average loss: 2.6599
[09/17 02:29:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/17 02:30:05 visual_prompt]: 	Test 100/190. loss: 2.753, 0.1965 s / batch. (data: 1.44e-02)max mem: 17.22448 GB 
[09/17 02:30:24 visual_prompt]: Inference (test):avg data time: 7.12e-03, avg batch time: 0.1931, average loss: 2.5958
[09/17 02:30:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.79	
[09/17 02:30:24 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/17 02:30:33 visual_prompt]: Epoch 18 / 100: avg data time: 1.18e-01, avg batch time: 0.5186, average train loss: 2.7387
[09/17 02:30:36 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.1427, average loss: 2.6892
[09/17 02:30:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 56.50	
[09/17 02:30:58 visual_prompt]: 	Test 100/190. loss: 2.756, 0.1977 s / batch. (data: 1.54e-02)max mem: 17.22448 GB 
[09/17 02:31:16 visual_prompt]: Inference (test):avg data time: 8.13e-03, avg batch time: 0.1929, average loss: 2.6916
[09/17 02:31:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.09	
[09/17 02:31:16 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/17 02:31:26 visual_prompt]: Epoch 19 / 100: avg data time: 1.19e-01, avg batch time: 0.5294, average train loss: 2.6033
[09/17 02:31:29 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1426, average loss: 2.7454
[09/17 02:31:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 54.50	
[09/17 02:31:50 visual_prompt]: 	Test 100/190. loss: 2.660, 0.1843 s / batch. (data: 1.61e-04)max mem: 17.22448 GB 
[09/17 02:32:08 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1929, average loss: 2.6245
[09/17 02:32:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.17	top5: 56.02	
[09/17 02:32:08 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/17 02:32:17 visual_prompt]: Epoch 20 / 100: avg data time: 1.01e-01, avg batch time: 0.5007, average train loss: 2.5399
[09/17 02:32:21 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1427, average loss: 2.3373
[09/17 02:32:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 60.50	
[09/17 02:32:42 visual_prompt]: 	Test 100/190. loss: 2.405, 0.1966 s / batch. (data: 1.43e-02)max mem: 17.22448 GB 
[09/17 02:33:00 visual_prompt]: Inference (test):avg data time: 7.20e-03, avg batch time: 0.1925, average loss: 2.3927
[09/17 02:33:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 56.30	
[09/17 02:33:00 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/17 02:33:10 visual_prompt]: Epoch 21 / 100: avg data time: 1.15e-01, avg batch time: 0.5159, average train loss: 2.5197
[09/17 02:33:13 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1428, average loss: 2.6578
[09/17 02:33:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 48.50	
[09/17 02:33:34 visual_prompt]: 	Test 100/190. loss: 2.515, 0.1963 s / batch. (data: 1.40e-02)max mem: 17.22448 GB 
[09/17 02:33:52 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1928, average loss: 2.5888
[09/17 02:33:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.51	
[09/17 02:33:52 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/17 02:34:01 visual_prompt]: Epoch 22 / 100: avg data time: 9.96e-02, avg batch time: 0.5024, average train loss: 2.5907
[09/17 02:34:04 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1426, average loss: 2.5831
[09/17 02:34:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 50.50	
[09/17 02:34:26 visual_prompt]: 	Test 100/190. loss: 2.381, 0.1967 s / batch. (data: 1.51e-04)max mem: 17.22448 GB 
[09/17 02:34:45 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1964, average loss: 2.5175
[09/17 02:34:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.43	
[09/17 02:34:45 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/17 02:34:54 visual_prompt]: Epoch 23 / 100: avg data time: 1.06e-01, avg batch time: 0.5098, average train loss: 2.7717
[09/17 02:34:57 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1453, average loss: 2.6983
[09/17 02:34:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 53.50	
[09/17 02:35:19 visual_prompt]: 	Test 100/190. loss: 2.714, 0.1825 s / batch. (data: 1.26e-04)max mem: 17.22448 GB 
[09/17 02:35:37 visual_prompt]: Inference (test):avg data time: 6.14e-03, avg batch time: 0.1946, average loss: 2.6964
[09/17 02:35:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.85	
[09/17 02:35:37 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/17 02:35:47 visual_prompt]: Epoch 24 / 100: avg data time: 1.15e-01, avg batch time: 0.5149, average train loss: 2.6204
[09/17 02:35:50 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1428, average loss: 3.1234
[09/17 02:35:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/17 02:36:11 visual_prompt]: 	Test 100/190. loss: 3.095, 0.1830 s / batch. (data: 1.49e-04)max mem: 17.22448 GB 
[09/17 02:36:30 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1944, average loss: 3.0647
[09/17 02:36:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 56.11	
[09/17 02:36:30 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/17 02:36:39 visual_prompt]: Epoch 25 / 100: avg data time: 9.94e-02, avg batch time: 0.5027, average train loss: 2.6253
[09/17 02:36:42 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1454, average loss: 2.5756
[09/17 02:36:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.00	
[09/17 02:37:03 visual_prompt]: 	Test 100/190. loss: 2.632, 0.1833 s / batch. (data: 1.44e-04)max mem: 17.22448 GB 
[09/17 02:37:22 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1931, average loss: 2.5612
[09/17 02:37:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 56.03	
[09/17 02:37:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/17 02:37:31 visual_prompt]: Epoch 26 / 100: avg data time: 1.13e-01, avg batch time: 0.5137, average train loss: 2.4428
[09/17 02:37:34 visual_prompt]: Inference (val):avg data time: 2.69e-05, avg batch time: 0.1425, average loss: 2.5612
[09/17 02:37:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 56.00	
[09/17 02:37:55 visual_prompt]: 	Test 100/190. loss: 2.571, 0.2063 s / batch. (data: 1.03e-02)max mem: 17.22448 GB 
[09/17 02:38:14 visual_prompt]: Inference (test):avg data time: 7.97e-03, avg batch time: 0.1933, average loss: 2.5246
[09/17 02:38:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.23	top5: 55.33	
[09/17 02:38:14 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/17 02:38:23 visual_prompt]: Epoch 27 / 100: avg data time: 9.61e-02, avg batch time: 0.4999, average train loss: 2.4065
[09/17 02:38:26 visual_prompt]: Inference (val):avg data time: 3.19e-05, avg batch time: 0.1432, average loss: 2.6657
[09/17 02:38:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 54.00	
[09/17 02:38:47 visual_prompt]: 	Test 100/190. loss: 2.488, 0.1827 s / batch. (data: 1.41e-04)max mem: 17.22448 GB 
[09/17 02:39:06 visual_prompt]: Inference (test):avg data time: 6.90e-03, avg batch time: 0.1930, average loss: 2.6126
[09/17 02:39:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.59	
[09/17 02:39:06 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/17 02:39:15 visual_prompt]: Epoch 28 / 100: avg data time: 1.10e-01, avg batch time: 0.5135, average train loss: 2.4714
[09/17 02:39:18 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1426, average loss: 2.5893
[09/17 02:39:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 56.00	
[09/17 02:39:40 visual_prompt]: 	Test 100/190. loss: 2.422, 0.1979 s / batch. (data: 1.62e-02)max mem: 17.22448 GB 
[09/17 02:39:59 visual_prompt]: Inference (test):avg data time: 8.85e-03, avg batch time: 0.1949, average loss: 2.5380
[09/17 02:39:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.71	top5: 59.23	
[09/17 02:39:59 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/17 02:40:08 visual_prompt]: Epoch 29 / 100: avg data time: 1.09e-01, avg batch time: 0.5097, average train loss: 2.6231
[09/17 02:40:11 visual_prompt]: Inference (val):avg data time: 2.20e-05, avg batch time: 0.1424, average loss: 2.5346
[09/17 02:40:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/17 02:40:32 visual_prompt]: 	Test 100/190. loss: 2.448, 0.1983 s / batch. (data: 1.51e-02)max mem: 17.22448 GB 
[09/17 02:40:51 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1965, average loss: 2.5475
[09/17 02:40:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.24	
[09/17 02:40:51 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/17 02:41:01 visual_prompt]: Epoch 30 / 100: avg data time: 1.15e-01, avg batch time: 0.5162, average train loss: 2.4019
[09/17 02:41:04 visual_prompt]: Inference (val):avg data time: 1.11e-04, avg batch time: 0.2264, average loss: 2.4008
[09/17 02:41:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.50	
[09/17 02:41:25 visual_prompt]: 	Test 100/190. loss: 2.341, 0.1826 s / batch. (data: 1.24e-04)max mem: 17.22448 GB 
[09/17 02:41:44 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1928, average loss: 2.4039
[09/17 02:41:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.21	top5: 55.62	
[09/17 02:41:44 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/17 02:41:53 visual_prompt]: Epoch 31 / 100: avg data time: 1.16e-01, avg batch time: 0.5166, average train loss: 2.4054
[09/17 02:41:56 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1426, average loss: 2.3971
[09/17 02:41:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/17 02:42:17 visual_prompt]: 	Test 100/190. loss: 2.482, 0.1974 s / batch. (data: 1.53e-02)max mem: 17.22448 GB 
[09/17 02:42:36 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1929, average loss: 2.4771
[09/17 02:42:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 59.80	
[09/17 02:42:36 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/17 02:42:45 visual_prompt]: Epoch 32 / 100: avg data time: 1.08e-01, avg batch time: 0.5081, average train loss: 2.5771
[09/17 02:42:48 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1427, average loss: 2.4634
[09/17 02:42:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 52.00	
[09/17 02:43:09 visual_prompt]: 	Test 100/190. loss: 2.324, 0.1897 s / batch. (data: 1.21e-04)max mem: 17.22448 GB 
[09/17 02:43:28 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1924, average loss: 2.4111
[09/17 02:43:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 55.69	
[09/17 02:43:28 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/17 02:43:37 visual_prompt]: Epoch 33 / 100: avg data time: 1.03e-01, avg batch time: 0.5056, average train loss: 2.4513
[09/17 02:43:40 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1426, average loss: 2.6486
[09/17 02:43:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 59.50	
[09/17 02:44:01 visual_prompt]: 	Test 100/190. loss: 2.517, 0.1826 s / batch. (data: 1.84e-04)max mem: 17.22448 GB 
[09/17 02:44:20 visual_prompt]: Inference (test):avg data time: 6.79e-03, avg batch time: 0.1924, average loss: 2.6318
[09/17 02:44:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 58.64	
[09/17 02:44:20 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/17 02:44:29 visual_prompt]: Epoch 34 / 100: avg data time: 9.79e-02, avg batch time: 0.5020, average train loss: 2.4061
[09/17 02:44:32 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1426, average loss: 2.2059
[09/17 02:44:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/17 02:44:53 visual_prompt]: 	Test 100/190. loss: 2.245, 0.1823 s / batch. (data: 1.31e-04)max mem: 17.22448 GB 
[09/17 02:45:12 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1936, average loss: 2.2347
[09/17 02:45:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 55.28	
[09/17 02:45:12 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/17 02:45:21 visual_prompt]: Epoch 35 / 100: avg data time: 9.55e-02, avg batch time: 0.4979, average train loss: 2.3706
[09/17 02:45:24 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1428, average loss: 2.5109
[09/17 02:45:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 63.00	
[09/17 02:45:46 visual_prompt]: 	Test 100/190. loss: 2.407, 0.2002 s / batch. (data: 1.41e-04)max mem: 17.22448 GB 
[09/17 02:46:04 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1947, average loss: 2.5500
[09/17 02:46:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 60.26	
[09/17 02:46:04 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/17 02:46:13 visual_prompt]: Epoch 36 / 100: avg data time: 9.45e-02, avg batch time: 0.5005, average train loss: 2.5228
[09/17 02:46:17 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1428, average loss: 2.3401
[09/17 02:46:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/17 02:46:38 visual_prompt]: 	Test 100/190. loss: 2.501, 0.1823 s / batch. (data: 1.51e-04)max mem: 17.22448 GB 
[09/17 02:46:57 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1955, average loss: 2.4460
[09/17 02:46:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.19	
[09/17 02:46:57 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/17 02:47:06 visual_prompt]: Epoch 37 / 100: avg data time: 1.14e-01, avg batch time: 0.5133, average train loss: 2.9158
[09/17 02:47:09 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1426, average loss: 2.5746
[09/17 02:47:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.00	
[09/17 02:47:30 visual_prompt]: 	Test 100/190. loss: 2.847, 0.2360 s / batch. (data: 5.45e-02)max mem: 17.22448 GB 
[09/17 02:47:48 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1932, average loss: 2.7013
[09/17 02:47:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.54	
[09/17 02:47:48 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/17 02:47:58 visual_prompt]: Epoch 38 / 100: avg data time: 1.07e-01, avg batch time: 0.5095, average train loss: 2.5812
[09/17 02:48:01 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1426, average loss: 2.3459
[09/17 02:48:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 56.00	
[09/17 02:48:22 visual_prompt]: 	Test 100/190. loss: 2.362, 0.1840 s / batch. (data: 1.14e-04)max mem: 17.22448 GB 
[09/17 02:48:40 visual_prompt]: Inference (test):avg data time: 6.28e-03, avg batch time: 0.1908, average loss: 2.3570
[09/17 02:48:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.33	
[09/17 02:48:40 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/17 02:48:50 visual_prompt]: Epoch 39 / 100: avg data time: 1.13e-01, avg batch time: 0.5128, average train loss: 2.7185
[09/17 02:48:53 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1427, average loss: 2.2940
[09/17 02:48:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 61.50	
[09/17 02:49:14 visual_prompt]: 	Test 100/190. loss: 2.265, 0.2041 s / batch. (data: 2.22e-02)max mem: 17.22448 GB 
[09/17 02:49:32 visual_prompt]: Inference (test):avg data time: 8.12e-03, avg batch time: 0.1930, average loss: 2.3466
[09/17 02:49:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 55.28	
[09/17 02:49:32 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/17 02:49:42 visual_prompt]: Epoch 40 / 100: avg data time: 1.18e-01, avg batch time: 0.5188, average train loss: 2.3597
[09/17 02:49:45 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1426, average loss: 2.3100
[09/17 02:49:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 57.50	
[09/17 02:50:06 visual_prompt]: 	Test 100/190. loss: 2.365, 0.1821 s / batch. (data: 1.19e-04)max mem: 17.22448 GB 
[09/17 02:50:25 visual_prompt]: Inference (test):avg data time: 7.04e-03, avg batch time: 0.1957, average loss: 2.3457
[09/17 02:50:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.68	
[09/17 02:50:25 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/17 02:50:34 visual_prompt]: Epoch 41 / 100: avg data time: 1.10e-01, avg batch time: 0.5113, average train loss: 2.4442
[09/17 02:50:37 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1426, average loss: 2.2477
[09/17 02:50:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.50	
[09/17 02:50:58 visual_prompt]: 	Test 100/190. loss: 2.290, 0.1980 s / batch. (data: 1.51e-02)max mem: 17.22448 GB 
[09/17 02:51:17 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1928, average loss: 2.2746
[09/17 02:51:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.98	
[09/17 02:51:17 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/17 02:51:26 visual_prompt]: Epoch 42 / 100: avg data time: 1.11e-01, avg batch time: 0.5119, average train loss: 2.4253
[09/17 02:51:29 visual_prompt]: Inference (val):avg data time: 3.32e-05, avg batch time: 0.1428, average loss: 2.5185
[09/17 02:51:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 57.00	
[09/17 02:51:50 visual_prompt]: 	Test 100/190. loss: 2.440, 0.1962 s / batch. (data: 1.40e-02)max mem: 17.22448 GB 
[09/17 02:52:09 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1925, average loss: 2.4813
[09/17 02:52:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 55.62	
[09/17 02:52:09 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/17 02:52:18 visual_prompt]: Epoch 43 / 100: avg data time: 1.07e-01, avg batch time: 0.5081, average train loss: 2.5607
[09/17 02:52:21 visual_prompt]: Inference (val):avg data time: 3.64e-04, avg batch time: 0.2037, average loss: 2.3623
[09/17 02:52:21 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 59.00	
[09/17 02:52:42 visual_prompt]: 	Test 100/190. loss: 2.191, 0.1869 s / batch. (data: 1.40e-04)max mem: 17.22448 GB 
[09/17 02:53:01 visual_prompt]: Inference (test):avg data time: 8.33e-03, avg batch time: 0.1935, average loss: 2.3445
[09/17 02:53:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.70	
[09/17 02:53:01 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/17 02:53:10 visual_prompt]: Epoch 44 / 100: avg data time: 1.14e-01, avg batch time: 0.5138, average train loss: 2.3781
[09/17 02:53:13 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1428, average loss: 2.2623
[09/17 02:53:13 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 53.50	
[09/17 02:53:34 visual_prompt]: 	Test 100/190. loss: 2.242, 0.1955 s / batch. (data: 1.37e-02)max mem: 17.22448 GB 
[09/17 02:53:53 visual_prompt]: Inference (test):avg data time: 6.19e-03, avg batch time: 0.1913, average loss: 2.2772
[09/17 02:53:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.62	
[09/17 02:53:53 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/17 02:54:02 visual_prompt]: Epoch 45 / 100: avg data time: 1.14e-01, avg batch time: 0.5177, average train loss: 2.3868
[09/17 02:54:05 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.1427, average loss: 2.3444
[09/17 02:54:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 58.00	
[09/17 02:54:26 visual_prompt]: 	Test 100/190. loss: 2.272, 0.1858 s / batch. (data: 1.57e-04)max mem: 17.22448 GB 
[09/17 02:54:44 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1925, average loss: 2.3859
[09/17 02:54:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.19	
[09/17 02:54:45 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/17 02:54:54 visual_prompt]: Epoch 46 / 100: avg data time: 1.14e-01, avg batch time: 0.5152, average train loss: 2.3459
[09/17 02:54:57 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1424, average loss: 2.4907
[09/17 02:54:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/17 02:55:18 visual_prompt]: 	Test 100/190. loss: 2.435, 0.2156 s / batch. (data: 2.31e-02)max mem: 17.22448 GB 
[09/17 02:55:37 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.1934, average loss: 2.4125
[09/17 02:55:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.86	
[09/17 02:55:37 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/17 02:55:46 visual_prompt]: Epoch 47 / 100: avg data time: 1.13e-01, avg batch time: 0.5149, average train loss: 2.4150
[09/17 02:55:49 visual_prompt]: Inference (val):avg data time: 3.53e-05, avg batch time: 0.1424, average loss: 2.4948
[09/17 02:55:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.00	
[09/17 02:56:10 visual_prompt]: 	Test 100/190. loss: 2.443, 0.1821 s / batch. (data: 1.28e-04)max mem: 17.22448 GB 
[09/17 02:56:29 visual_prompt]: Inference (test):avg data time: 8.85e-03, avg batch time: 0.1942, average loss: 2.4128
[09/17 02:56:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 55.52	
[09/17 02:56:29 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/17 02:56:38 visual_prompt]: Epoch 48 / 100: avg data time: 1.04e-01, avg batch time: 0.5053, average train loss: 2.3187
[09/17 02:56:41 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1449, average loss: 2.3442
[09/17 02:56:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.00	
[09/17 02:57:02 visual_prompt]: 	Test 100/190. loss: 2.571, 0.2016 s / batch. (data: 1.31e-02)max mem: 17.22448 GB 
[09/17 02:57:21 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1955, average loss: 2.5035
[09/17 02:57:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.46	
[09/17 02:57:21 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/17 02:57:31 visual_prompt]: Epoch 49 / 100: avg data time: 1.14e-01, avg batch time: 0.5178, average train loss: 2.3829
[09/17 02:57:34 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1426, average loss: 2.3585
[09/17 02:57:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/17 02:57:55 visual_prompt]: 	Test 100/190. loss: 2.251, 0.1956 s / batch. (data: 1.37e-02)max mem: 17.22448 GB 
[09/17 02:58:14 visual_prompt]: Inference (test):avg data time: 6.98e-03, avg batch time: 0.1921, average loss: 2.3144
[09/17 02:58:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 61.51	
[09/17 02:58:14 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/17 02:58:23 visual_prompt]: Epoch 50 / 100: avg data time: 9.91e-02, avg batch time: 0.5003, average train loss: 2.4685
[09/17 02:58:26 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1426, average loss: 2.5447
[09/17 02:58:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.50	
[09/17 02:58:47 visual_prompt]: 	Test 100/190. loss: 2.555, 0.2162 s / batch. (data: 1.37e-04)max mem: 17.22448 GB 
[09/17 02:59:06 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.1955, average loss: 2.6222
[09/17 02:59:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 56.09	
[09/17 02:59:06 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/17 02:59:15 visual_prompt]: Epoch 51 / 100: avg data time: 1.09e-01, avg batch time: 0.5122, average train loss: 2.3020
[09/17 02:59:18 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1424, average loss: 2.0885
[09/17 02:59:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 73.50	
[09/17 02:59:39 visual_prompt]: 	Test 100/190. loss: 2.034, 0.1959 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/17 02:59:58 visual_prompt]: Inference (test):avg data time: 8.15e-03, avg batch time: 0.1925, average loss: 2.1054
[09/17 02:59:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.49	top5: 70.64	
[09/17 02:59:58 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/17 03:00:07 visual_prompt]: Epoch 52 / 100: avg data time: 9.25e-02, avg batch time: 0.4949, average train loss: 2.2861
[09/17 03:00:10 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1427, average loss: 2.3925
[09/17 03:00:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.50	
[09/17 03:00:31 visual_prompt]: 	Test 100/190. loss: 2.256, 0.1973 s / batch. (data: 1.51e-02)max mem: 17.22448 GB 
[09/17 03:00:50 visual_prompt]: Inference (test):avg data time: 7.56e-03, avg batch time: 0.1925, average loss: 2.3967
[09/17 03:00:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 62.39	
[09/17 03:00:50 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/17 03:00:59 visual_prompt]: Epoch 53 / 100: avg data time: 1.11e-01, avg batch time: 0.5192, average train loss: 2.3382
[09/17 03:01:02 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.1427, average loss: 2.6204
[09/17 03:01:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 48.50	
[09/17 03:01:23 visual_prompt]: 	Test 100/190. loss: 2.484, 0.1830 s / batch. (data: 1.43e-04)max mem: 17.22448 GB 
[09/17 03:01:42 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1930, average loss: 2.5079
[09/17 03:01:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.44	
[09/17 03:01:42 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/17 03:01:51 visual_prompt]: Epoch 54 / 100: avg data time: 1.01e-01, avg batch time: 0.5062, average train loss: 2.3472
[09/17 03:01:54 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 2.2750
[09/17 03:01:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 61.50	
[09/17 03:02:15 visual_prompt]: 	Test 100/190. loss: 2.248, 0.1855 s / batch. (data: 1.27e-04)max mem: 17.22448 GB 
[09/17 03:02:34 visual_prompt]: Inference (test):avg data time: 7.02e-03, avg batch time: 0.1930, average loss: 2.2636
[09/17 03:02:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.32	top5: 63.20	
[09/17 03:02:34 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/17 03:02:43 visual_prompt]: Epoch 55 / 100: avg data time: 1.07e-01, avg batch time: 0.5085, average train loss: 2.2431
[09/17 03:02:46 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.1467, average loss: 2.1017
[09/17 03:02:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 76.50	
[09/17 03:03:08 visual_prompt]: 	Test 100/190. loss: 2.029, 0.1973 s / batch. (data: 1.51e-02)max mem: 17.22448 GB 
[09/17 03:03:26 visual_prompt]: Inference (test):avg data time: 7.13e-03, avg batch time: 0.1946, average loss: 2.1083
[09/17 03:03:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.91	top5: 74.50	
[09/17 03:03:26 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/17 03:03:35 visual_prompt]: Epoch 56 / 100: avg data time: 1.01e-01, avg batch time: 0.5032, average train loss: 2.1058
[09/17 03:03:38 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1427, average loss: 2.1335
[09/17 03:03:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 79.50	
[09/17 03:03:59 visual_prompt]: 	Test 100/190. loss: 2.173, 0.2246 s / batch. (data: 4.31e-02)max mem: 17.22448 GB 
[09/17 03:04:18 visual_prompt]: Inference (test):avg data time: 8.62e-03, avg batch time: 0.1945, average loss: 2.1645
[09/17 03:04:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.37	top5: 78.12	
[09/17 03:04:18 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/17 03:04:27 visual_prompt]: Epoch 57 / 100: avg data time: 9.84e-02, avg batch time: 0.5017, average train loss: 2.2760
[09/17 03:04:30 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1427, average loss: 2.0510
[09/17 03:04:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 82.50	
[09/17 03:04:51 visual_prompt]: 	Test 100/190. loss: 2.187, 0.1942 s / batch. (data: 1.24e-02)max mem: 17.22448 GB 
[09/17 03:05:10 visual_prompt]: Inference (test):avg data time: 6.64e-03, avg batch time: 0.1926, average loss: 2.1396
[09/17 03:05:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.35	top5: 77.56	
[09/17 03:05:10 visual_prompt]: Best epoch 57: best metric: 0.220
[09/17 03:05:10 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/17 03:05:19 visual_prompt]: Epoch 58 / 100: avg data time: 1.04e-01, avg batch time: 0.5116, average train loss: 2.1575
[09/17 03:05:22 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1426, average loss: 2.1562
[09/17 03:05:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 66.00	
[09/17 03:05:43 visual_prompt]: 	Test 100/190. loss: 2.338, 0.2019 s / batch. (data: 1.99e-02)max mem: 17.22448 GB 
[09/17 03:06:02 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1925, average loss: 2.2711
[09/17 03:06:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.56	top5: 62.30	
[09/17 03:06:02 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/17 03:06:11 visual_prompt]: Epoch 59 / 100: avg data time: 1.12e-01, avg batch time: 0.5300, average train loss: 2.0876
[09/17 03:06:14 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1426, average loss: 1.9132
[09/17 03:06:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 82.50	
[09/17 03:06:35 visual_prompt]: 	Test 100/190. loss: 1.833, 0.1859 s / batch. (data: 1.01e-04)max mem: 17.22448 GB 
[09/17 03:06:54 visual_prompt]: Inference (test):avg data time: 8.80e-03, avg batch time: 0.1946, average loss: 1.9454
[09/17 03:06:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.62	top5: 82.46	
[09/17 03:06:54 visual_prompt]: Best epoch 59: best metric: 0.240
[09/17 03:06:54 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/17 03:07:03 visual_prompt]: Epoch 60 / 100: avg data time: 1.10e-01, avg batch time: 0.5112, average train loss: 2.0137
[09/17 03:07:06 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1428, average loss: 1.8951
[09/17 03:07:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 87.50	
[09/17 03:07:28 visual_prompt]: 	Test 100/190. loss: 2.060, 0.1951 s / batch. (data: 1.34e-02)max mem: 17.22448 GB 
[09/17 03:07:46 visual_prompt]: Inference (test):avg data time: 7.16e-03, avg batch time: 0.1920, average loss: 2.0343
[09/17 03:07:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.02	top5: 81.44	
[09/17 03:07:46 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/17 03:07:55 visual_prompt]: Epoch 61 / 100: avg data time: 1.14e-01, avg batch time: 0.5140, average train loss: 2.0259
[09/17 03:07:58 visual_prompt]: Inference (val):avg data time: 2.07e-05, avg batch time: 0.1427, average loss: 1.8579
[09/17 03:07:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 84.50	
[09/17 03:08:19 visual_prompt]: 	Test 100/190. loss: 1.916, 0.1977 s / batch. (data: 1.56e-02)max mem: 17.22448 GB 
[09/17 03:08:38 visual_prompt]: Inference (test):avg data time: 8.45e-03, avg batch time: 0.1933, average loss: 2.0215
[09/17 03:08:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.57	top5: 78.13	
[09/17 03:08:38 visual_prompt]: Best epoch 61: best metric: 0.280
[09/17 03:08:38 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/17 03:08:47 visual_prompt]: Epoch 62 / 100: avg data time: 1.15e-01, avg batch time: 0.5162, average train loss: 2.0235
[09/17 03:08:50 visual_prompt]: Inference (val):avg data time: 2.51e-05, avg batch time: 0.1427, average loss: 2.2805
[09/17 03:08:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 76.50	
[09/17 03:09:11 visual_prompt]: 	Test 100/190. loss: 2.482, 0.1827 s / batch. (data: 1.20e-04)max mem: 17.22448 GB 
[09/17 03:09:30 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1952, average loss: 2.3748
[09/17 03:09:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.44	top5: 76.30	
[09/17 03:09:30 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/17 03:09:39 visual_prompt]: Epoch 63 / 100: avg data time: 1.01e-01, avg batch time: 0.5028, average train loss: 2.0744
[09/17 03:09:42 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1428, average loss: 1.9115
[09/17 03:09:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 85.50	
[09/17 03:10:04 visual_prompt]: 	Test 100/190. loss: 1.867, 0.2050 s / batch. (data: 1.24e-02)max mem: 17.22448 GB 
[09/17 03:10:22 visual_prompt]: Inference (test):avg data time: 7.39e-03, avg batch time: 0.1929, average loss: 2.0147
[09/17 03:10:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.37	top5: 84.21	
[09/17 03:10:22 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/17 03:10:31 visual_prompt]: Epoch 64 / 100: avg data time: 1.05e-01, avg batch time: 0.5078, average train loss: 1.9008
[09/17 03:10:34 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1427, average loss: 1.8690
[09/17 03:10:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 91.50	
[09/17 03:10:55 visual_prompt]: 	Test 100/190. loss: 2.047, 0.1952 s / batch. (data: 1.10e-04)max mem: 17.22448 GB 
[09/17 03:11:14 visual_prompt]: Inference (test):avg data time: 6.43e-03, avg batch time: 0.1920, average loss: 2.0226
[09/17 03:11:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.41	top5: 84.72	
[09/17 03:11:14 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/17 03:11:23 visual_prompt]: Epoch 65 / 100: avg data time: 1.11e-01, avg batch time: 0.5128, average train loss: 2.1043
[09/17 03:11:27 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1427, average loss: 2.0713
[09/17 03:11:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 76.50	
[09/17 03:11:48 visual_prompt]: 	Test 100/190. loss: 2.163, 0.1982 s / batch. (data: 1.49e-04)max mem: 17.22448 GB 
[09/17 03:12:07 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1954, average loss: 2.1895
[09/17 03:12:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.80	top5: 71.76	
[09/17 03:12:07 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/17 03:12:16 visual_prompt]: Epoch 66 / 100: avg data time: 1.04e-01, avg batch time: 0.5062, average train loss: 2.0201
[09/17 03:12:19 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1426, average loss: 1.9207
[09/17 03:12:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 87.50	
[09/17 03:12:41 visual_prompt]: 	Test 100/190. loss: 2.009, 0.1977 s / batch. (data: 1.37e-02)max mem: 17.22448 GB 
[09/17 03:12:59 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1937, average loss: 2.0498
[09/17 03:12:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.29	top5: 84.27	
[09/17 03:12:59 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/17 03:13:09 visual_prompt]: Epoch 67 / 100: avg data time: 9.66e-02, avg batch time: 0.4987, average train loss: 1.8240
[09/17 03:13:12 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1427, average loss: 2.0603
[09/17 03:13:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 81.50	
[09/17 03:13:33 visual_prompt]: 	Test 100/190. loss: 2.135, 0.1962 s / batch. (data: 1.40e-02)max mem: 17.22448 GB 
[09/17 03:13:52 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1938, average loss: 2.1882
[09/17 03:13:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.15	top5: 79.37	
[09/17 03:13:52 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/17 03:14:01 visual_prompt]: Epoch 68 / 100: avg data time: 1.14e-01, avg batch time: 0.5154, average train loss: 1.8298
[09/17 03:14:04 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1425, average loss: 1.6697
[09/17 03:14:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 93.00	
[09/17 03:14:25 visual_prompt]: 	Test 100/190. loss: 1.809, 0.1890 s / batch. (data: 1.12e-04)max mem: 17.22448 GB 
[09/17 03:14:44 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1927, average loss: 1.8666
[09/17 03:14:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.07	top5: 86.70	
[09/17 03:14:44 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/17 03:14:53 visual_prompt]: Epoch 69 / 100: avg data time: 1.17e-01, avg batch time: 0.5185, average train loss: 1.6988
[09/17 03:14:56 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1425, average loss: 1.6105
[09/17 03:14:56 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 97.50	
[09/17 03:15:17 visual_prompt]: 	Test 100/190. loss: 1.773, 0.1829 s / batch. (data: 1.07e-04)max mem: 17.22448 GB 
[09/17 03:15:36 visual_prompt]: Inference (test):avg data time: 7.70e-03, avg batch time: 0.1925, average loss: 1.8838
[09/17 03:15:36 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.65	top5: 89.53	
[09/17 03:15:36 visual_prompt]: Best epoch 69: best metric: 0.300
[09/17 03:15:36 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/17 03:15:45 visual_prompt]: Epoch 70 / 100: avg data time: 1.01e-01, avg batch time: 0.5081, average train loss: 1.7126
[09/17 03:15:48 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.1427, average loss: 1.6849
[09/17 03:15:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 94.00	
[09/17 03:16:09 visual_prompt]: 	Test 100/190. loss: 1.817, 0.1832 s / batch. (data: 9.13e-05)max mem: 17.22448 GB 
[09/17 03:16:28 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1924, average loss: 1.8932
[09/17 03:16:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.74	top5: 87.10	
[09/17 03:16:28 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/17 03:16:37 visual_prompt]: Epoch 71 / 100: avg data time: 1.08e-01, avg batch time: 0.5080, average train loss: 1.7234
[09/17 03:16:40 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1428, average loss: 1.6238
[09/17 03:16:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 94.00	
[09/17 03:17:01 visual_prompt]: 	Test 100/190. loss: 1.757, 0.1831 s / batch. (data: 1.27e-04)max mem: 17.22448 GB 
[09/17 03:17:20 visual_prompt]: Inference (test):avg data time: 8.53e-03, avg batch time: 0.1931, average loss: 1.8470
[09/17 03:17:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.93	top5: 88.95	
[09/17 03:17:20 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/17 03:17:29 visual_prompt]: Epoch 72 / 100: avg data time: 1.04e-01, avg batch time: 0.5038, average train loss: 1.7089
[09/17 03:17:32 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1428, average loss: 1.5985
[09/17 03:17:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 95.50	
[09/17 03:17:53 visual_prompt]: 	Test 100/190. loss: 1.799, 0.2063 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/17 03:18:12 visual_prompt]: Inference (test):avg data time: 8.59e-03, avg batch time: 0.1931, average loss: 1.8296
[09/17 03:18:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.24	top5: 89.05	
[09/17 03:18:12 visual_prompt]: Best epoch 72: best metric: 0.345
[09/17 03:18:12 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/17 03:18:21 visual_prompt]: Epoch 73 / 100: avg data time: 1.11e-01, avg batch time: 0.5133, average train loss: 1.6090
[09/17 03:18:25 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1426, average loss: 1.8488
[09/17 03:18:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 92.00	
[09/17 03:18:46 visual_prompt]: 	Test 100/190. loss: 1.977, 0.1967 s / batch. (data: 1.47e-02)max mem: 17.22448 GB 
[09/17 03:19:04 visual_prompt]: Inference (test):avg data time: 7.66e-03, avg batch time: 0.1933, average loss: 2.2034
[09/17 03:19:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.42	top5: 84.63	
[09/17 03:19:04 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/17 03:19:14 visual_prompt]: Epoch 74 / 100: avg data time: 1.13e-01, avg batch time: 0.5154, average train loss: 1.7082
[09/17 03:19:17 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.1425, average loss: 1.6693
[09/17 03:19:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 94.00	
[09/17 03:19:38 visual_prompt]: 	Test 100/190. loss: 1.835, 0.1962 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/17 03:19:57 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1930, average loss: 2.0280
[09/17 03:19:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.01	top5: 86.70	
[09/17 03:19:57 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/17 03:20:06 visual_prompt]: Epoch 75 / 100: avg data time: 1.16e-01, avg batch time: 0.5161, average train loss: 1.6468
[09/17 03:20:09 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.1426, average loss: 1.6091
[09/17 03:20:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.00	top5: 92.00	
[09/17 03:20:30 visual_prompt]: 	Test 100/190. loss: 1.848, 0.1825 s / batch. (data: 1.03e-04)max mem: 17.22448 GB 
[09/17 03:20:49 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1930, average loss: 2.0102
[09/17 03:20:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.52	top5: 86.49	
[09/17 03:20:49 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/17 03:20:58 visual_prompt]: Epoch 76 / 100: avg data time: 1.09e-01, avg batch time: 0.5114, average train loss: 1.5637
[09/17 03:21:01 visual_prompt]: Inference (val):avg data time: 2.13e-05, avg batch time: 0.1426, average loss: 1.4616
[09/17 03:21:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.00	top5: 98.00	
[09/17 03:21:22 visual_prompt]: 	Test 100/190. loss: 1.755, 0.1963 s / batch. (data: 1.39e-02)max mem: 17.22448 GB 
[09/17 03:21:41 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1921, average loss: 1.8837
[09/17 03:21:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.37	top5: 89.96	
[09/17 03:21:41 visual_prompt]: Best epoch 76: best metric: 0.400
[09/17 03:21:41 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/17 03:21:50 visual_prompt]: Epoch 77 / 100: avg data time: 1.14e-01, avg batch time: 0.5158, average train loss: 1.4946
[09/17 03:21:53 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1427, average loss: 1.3302
[09/17 03:21:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.00	top5: 99.00	
[09/17 03:22:14 visual_prompt]: 	Test 100/190. loss: 1.842, 0.1877 s / batch. (data: 5.35e-03)max mem: 17.22448 GB 
[09/17 03:22:33 visual_prompt]: Inference (test):avg data time: 8.66e-03, avg batch time: 0.1936, average loss: 1.9078
[09/17 03:22:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.52	top5: 90.71	
[09/17 03:22:33 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/17 03:22:42 visual_prompt]: Epoch 78 / 100: avg data time: 1.12e-01, avg batch time: 0.5137, average train loss: 1.5277
[09/17 03:22:46 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1429, average loss: 1.3529
[09/17 03:22:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 41.50	top5: 99.00	
[09/17 03:23:07 visual_prompt]: 	Test 100/190. loss: 1.570, 0.1913 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/17 03:23:25 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1929, average loss: 1.8249
[09/17 03:23:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.98	top5: 90.16	
[09/17 03:23:25 visual_prompt]: Best epoch 78: best metric: 0.415
[09/17 03:23:25 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/17 03:23:34 visual_prompt]: Epoch 79 / 100: avg data time: 1.15e-01, avg batch time: 0.5156, average train loss: 1.4563
[09/17 03:23:38 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1426, average loss: 1.4414
[09/17 03:23:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 98.50	
[09/17 03:23:59 visual_prompt]: 	Test 100/190. loss: 1.781, 0.1851 s / batch. (data: 1.55e-04)max mem: 17.22448 GB 
[09/17 03:24:17 visual_prompt]: Inference (test):avg data time: 7.21e-03, avg batch time: 0.1931, average loss: 1.8740
[09/17 03:24:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.08	top5: 90.12	
[09/17 03:24:17 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/17 03:24:27 visual_prompt]: Epoch 80 / 100: avg data time: 1.11e-01, avg batch time: 0.5157, average train loss: 1.3932
[09/17 03:24:30 visual_prompt]: Inference (val):avg data time: 4.47e-05, avg batch time: 0.1445, average loss: 1.2642
[09/17 03:24:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 45.00	top5: 99.50	
[09/17 03:24:51 visual_prompt]: 	Test 100/190. loss: 1.722, 0.1944 s / batch. (data: 1.23e-02)max mem: 17.22448 GB 
[09/17 03:25:09 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1931, average loss: 1.8604
[09/17 03:25:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.84	top5: 91.12	
[09/17 03:25:09 visual_prompt]: Best epoch 80: best metric: 0.450
[09/17 03:25:09 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/17 03:25:19 visual_prompt]: Epoch 81 / 100: avg data time: 1.04e-01, avg batch time: 0.5049, average train loss: 1.3514
[09/17 03:25:22 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.1427, average loss: 1.3733
[09/17 03:25:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 99.50	
[09/17 03:25:43 visual_prompt]: 	Test 100/190. loss: 1.794, 0.1826 s / batch. (data: 1.17e-04)max mem: 17.22448 GB 
[09/17 03:26:01 visual_prompt]: Inference (test):avg data time: 7.23e-03, avg batch time: 0.1932, average loss: 2.1137
[09/17 03:26:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.21	top5: 88.58	
[09/17 03:26:01 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/17 03:26:11 visual_prompt]: Epoch 82 / 100: avg data time: 1.08e-01, avg batch time: 0.5101, average train loss: 1.2901
[09/17 03:26:14 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1429, average loss: 1.2426
[09/17 03:26:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 41.50	top5: 99.00	
[09/17 03:26:35 visual_prompt]: 	Test 100/190. loss: 1.868, 0.1965 s / batch. (data: 6.37e-03)max mem: 17.22448 GB 
[09/17 03:26:54 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1931, average loss: 1.9380
[09/17 03:26:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.65	top5: 91.14	
[09/17 03:26:54 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/17 03:27:03 visual_prompt]: Epoch 83 / 100: avg data time: 1.13e-01, avg batch time: 0.5146, average train loss: 1.2676
[09/17 03:27:06 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.1426, average loss: 1.1521
[09/17 03:27:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 51.50	top5: 99.50	
[09/17 03:27:27 visual_prompt]: 	Test 100/190. loss: 1.818, 0.2099 s / batch. (data: 2.80e-02)max mem: 17.22448 GB 
[09/17 03:27:46 visual_prompt]: Inference (test):avg data time: 7.60e-03, avg batch time: 0.1926, average loss: 2.0379
[09/17 03:27:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.47	top5: 90.77	
[09/17 03:27:46 visual_prompt]: Best epoch 83: best metric: 0.515
[09/17 03:27:46 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/17 03:27:55 visual_prompt]: Epoch 84 / 100: avg data time: 1.03e-01, avg batch time: 0.5089, average train loss: 1.2539
[09/17 03:27:58 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1427, average loss: 1.2841
[09/17 03:27:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.50	top5: 99.00	
[09/17 03:28:19 visual_prompt]: 	Test 100/190. loss: 1.839, 0.1829 s / batch. (data: 1.11e-04)max mem: 17.22448 GB 
[09/17 03:28:38 visual_prompt]: Inference (test):avg data time: 8.62e-03, avg batch time: 0.1943, average loss: 2.0963
[09/17 03:28:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.95	top5: 89.26	
[09/17 03:28:38 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/17 03:28:47 visual_prompt]: Epoch 85 / 100: avg data time: 1.01e-01, avg batch time: 0.5058, average train loss: 1.3686
[09/17 03:28:50 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1425, average loss: 1.3454
[09/17 03:28:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.50	top5: 99.50	
[09/17 03:29:11 visual_prompt]: 	Test 100/190. loss: 1.862, 0.1942 s / batch. (data: 1.25e-02)max mem: 17.22448 GB 
[09/17 03:29:30 visual_prompt]: Inference (test):avg data time: 6.06e-03, avg batch time: 0.1920, average loss: 2.0839
[09/17 03:29:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.64	top5: 89.49	
[09/17 03:29:30 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/17 03:29:39 visual_prompt]: Epoch 86 / 100: avg data time: 1.20e-01, avg batch time: 0.5199, average train loss: 1.3347
[09/17 03:29:42 visual_prompt]: Inference (val):avg data time: 4.61e-05, avg batch time: 0.1428, average loss: 1.1451
[09/17 03:29:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 49.00	top5: 100.00	
[09/17 03:30:04 visual_prompt]: 	Test 100/190. loss: 1.602, 0.1878 s / batch. (data: 1.64e-04)max mem: 17.22448 GB 
[09/17 03:30:22 visual_prompt]: Inference (test):avg data time: 8.37e-03, avg batch time: 0.1935, average loss: 1.8871
[09/17 03:30:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.47	top5: 90.53	
[09/17 03:30:22 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/17 03:30:31 visual_prompt]: Epoch 87 / 100: avg data time: 9.85e-02, avg batch time: 0.5038, average train loss: 1.1786
[09/17 03:30:35 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.1427, average loss: 1.1111
[09/17 03:30:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 50.50	top5: 100.00	
[09/17 03:30:56 visual_prompt]: 	Test 100/190. loss: 1.763, 0.1903 s / batch. (data: 1.52e-04)max mem: 17.22448 GB 
[09/17 03:31:14 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1939, average loss: 1.9669
[09/17 03:31:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.53	top5: 91.28	
[09/17 03:31:14 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/17 03:31:24 visual_prompt]: Epoch 88 / 100: avg data time: 1.06e-01, avg batch time: 0.5125, average train loss: 1.1022
[09/17 03:31:27 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1428, average loss: 1.0594
[09/17 03:31:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 51.00	top5: 100.00	
[09/17 03:31:48 visual_prompt]: 	Test 100/190. loss: 1.863, 0.1947 s / batch. (data: 1.28e-04)max mem: 17.22448 GB 
[09/17 03:32:06 visual_prompt]: Inference (test):avg data time: 7.15e-03, avg batch time: 0.1928, average loss: 2.2067
[09/17 03:32:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.76	top5: 90.49	
[09/17 03:32:06 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/17 03:32:15 visual_prompt]: Epoch 89 / 100: avg data time: 9.68e-02, avg batch time: 0.5044, average train loss: 1.1273
[09/17 03:32:18 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.1427, average loss: 1.0881
[09/17 03:32:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 50.00	top5: 100.00	
[09/17 03:32:40 visual_prompt]: 	Test 100/190. loss: 2.090, 0.1823 s / batch. (data: 4.27e-05)max mem: 17.22448 GB 
[09/17 03:32:58 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1933, average loss: 2.1682
[09/17 03:32:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.09	top5: 90.43	
[09/17 03:32:58 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/17 03:33:08 visual_prompt]: Epoch 90 / 100: avg data time: 1.24e-01, avg batch time: 0.5260, average train loss: 1.0922
[09/17 03:33:11 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1427, average loss: 0.9526
[09/17 03:33:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 59.50	top5: 100.00	
[09/17 03:33:32 visual_prompt]: 	Test 100/190. loss: 1.916, 0.1860 s / batch. (data: 9.97e-05)max mem: 17.22448 GB 
[09/17 03:33:50 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1924, average loss: 2.2325
[09/17 03:33:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.79	top5: 90.56	
[09/17 03:33:50 visual_prompt]: Best epoch 90: best metric: 0.595
[09/17 03:33:50 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/17 03:34:00 visual_prompt]: Epoch 91 / 100: avg data time: 1.09e-01, avg batch time: 0.5112, average train loss: 0.9731
[09/17 03:34:03 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1428, average loss: 0.9562
[09/17 03:34:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 56.50	top5: 100.00	
[09/17 03:34:24 visual_prompt]: 	Test 100/190. loss: 1.987, 0.1952 s / batch. (data: 1.25e-02)max mem: 17.22448 GB 
[09/17 03:34:43 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1928, average loss: 2.3432
[09/17 03:34:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 90.82	
[09/17 03:34:43 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/17 03:34:52 visual_prompt]: Epoch 92 / 100: avg data time: 1.06e-01, avg batch time: 0.5089, average train loss: 0.9677
[09/17 03:34:55 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.1429, average loss: 0.8485
[09/17 03:34:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 65.00	top5: 100.00	
[09/17 03:35:16 visual_prompt]: 	Test 100/190. loss: 2.023, 0.1822 s / batch. (data: 9.23e-05)max mem: 17.22448 GB 
[09/17 03:35:34 visual_prompt]: Inference (test):avg data time: 7.79e-03, avg batch time: 0.1923, average loss: 2.3623
[09/17 03:35:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.75	top5: 91.28	
[09/17 03:35:34 visual_prompt]: Best epoch 92: best metric: 0.650
[09/17 03:35:34 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/17 03:35:43 visual_prompt]: Epoch 93 / 100: avg data time: 1.07e-01, avg batch time: 0.5091, average train loss: 0.8801
[09/17 03:35:47 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1441, average loss: 0.8663
[09/17 03:35:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 63.00	top5: 100.00	
[09/17 03:36:08 visual_prompt]: 	Test 100/190. loss: 2.053, 0.1945 s / batch. (data: 1.25e-02)max mem: 17.22448 GB 
[09/17 03:36:27 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1937, average loss: 2.5047
[09/17 03:36:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.98	top5: 90.84	
[09/17 03:36:27 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/17 03:36:36 visual_prompt]: Epoch 94 / 100: avg data time: 1.13e-01, avg batch time: 0.5147, average train loss: 0.8622
[09/17 03:36:39 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1426, average loss: 0.8003
[09/17 03:36:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 65.50	top5: 100.00	
[09/17 03:37:00 visual_prompt]: 	Test 100/190. loss: 2.152, 0.1981 s / batch. (data: 1.25e-04)max mem: 17.22448 GB 
[09/17 03:37:19 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1936, average loss: 2.6164
[09/17 03:37:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.01	top5: 90.80	
[09/17 03:37:19 visual_prompt]: Best epoch 94: best metric: 0.655
[09/17 03:37:19 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/17 03:37:28 visual_prompt]: Epoch 95 / 100: avg data time: 9.61e-02, avg batch time: 0.5036, average train loss: 0.8857
[09/17 03:37:31 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1428, average loss: 0.7738
[09/17 03:37:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 73.00	top5: 100.00	
[09/17 03:37:52 visual_prompt]: 	Test 100/190. loss: 2.235, 0.1881 s / batch. (data: 5.97e-03)max mem: 17.22448 GB 
[09/17 03:38:11 visual_prompt]: Inference (test):avg data time: 6.30e-03, avg batch time: 0.1927, average loss: 2.5446
[09/17 03:38:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.30	top5: 91.19	
[09/17 03:38:11 visual_prompt]: Best epoch 95: best metric: 0.730
[09/17 03:38:11 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/17 03:38:20 visual_prompt]: Epoch 96 / 100: avg data time: 1.08e-01, avg batch time: 0.5099, average train loss: 0.7961
[09/17 03:38:23 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.1426, average loss: 0.7894
[09/17 03:38:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 68.00	top5: 100.00	
[09/17 03:38:44 visual_prompt]: 	Test 100/190. loss: 2.263, 0.1956 s / batch. (data: 1.37e-02)max mem: 17.22448 GB 
[09/17 03:39:02 visual_prompt]: Inference (test):avg data time: 8.59e-03, avg batch time: 0.1933, average loss: 2.6455
[09/17 03:39:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.86	top5: 91.00	
[09/17 03:39:02 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/17 03:39:11 visual_prompt]: Epoch 97 / 100: avg data time: 9.57e-02, avg batch time: 0.4992, average train loss: 0.7710
[09/17 03:39:15 visual_prompt]: Inference (val):avg data time: 3.79e-05, avg batch time: 0.1428, average loss: 0.7852
[09/17 03:39:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 70.00	top5: 100.00	
[09/17 03:39:36 visual_prompt]: 	Test 100/190. loss: 2.324, 0.1953 s / batch. (data: 1.34e-02)max mem: 17.22448 GB 
[09/17 03:39:54 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1922, average loss: 2.7003
[09/17 03:39:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.79	top5: 90.86	
[09/17 03:39:54 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/17 03:40:03 visual_prompt]: Epoch 98 / 100: avg data time: 1.05e-01, avg batch time: 0.5301, average train loss: 0.7643
[09/17 03:40:07 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1431, average loss: 0.7837
[09/17 03:40:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 69.50	top5: 100.00	
[09/17 03:40:28 visual_prompt]: 	Test 100/190. loss: 2.348, 0.1829 s / batch. (data: 1.68e-04)max mem: 17.22448 GB 
[09/17 03:40:46 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1932, average loss: 2.7326
[09/17 03:40:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.03	top5: 90.87	
[09/17 03:40:46 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/17 03:40:56 visual_prompt]: Epoch 99 / 100: avg data time: 1.15e-01, avg batch time: 0.5173, average train loss: 0.7786
[09/17 03:40:59 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1425, average loss: 0.7801
[09/17 03:40:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 72.00	top5: 100.00	
[09/17 03:41:20 visual_prompt]: 	Test 100/190. loss: 2.360, 0.1829 s / batch. (data: 1.04e-04)max mem: 17.22448 GB 
[09/17 03:41:39 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1940, average loss: 2.7268
[09/17 03:41:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.05	top5: 90.90	
[09/17 03:41:39 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/17 03:41:48 visual_prompt]: Epoch 100 / 100: avg data time: 9.51e-02, avg batch time: 0.4981, average train loss: 0.7595
[09/17 03:41:51 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1430, average loss: 0.7781
[09/17 03:41:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 69.50	top5: 100.00	
[09/17 03:42:12 visual_prompt]: 	Test 100/190. loss: 2.348, 0.1830 s / batch. (data: 1.18e-04)max mem: 17.22448 GB 
[09/17 03:42:31 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1928, average loss: 2.7478
[09/17 03:42:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 90.78	
[09/17 03:42:59 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 03:42:59 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 03:42:59 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed100'], train_type='')
[09/17 03:42:59 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 03:42:59 visual_prompt]: Training with config:
[09/17 03:42:59 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")',
          'NO_TEST': False,
          'NUMBER_CLASSES': 9,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed100/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 03:42:59 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 03:42:59.569597: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 03:42:59.742309: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 03:43:00.643400: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 03:43:00.643480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 03:43:00.643489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 03:43:02.667212: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 03:43:02.667322: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 03:43:02.667335: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 03:43:02 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
2023-09-17 03:43:02.702969: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 03:43:04 visual_prompt]: Number of images: 1000
[09/17 03:43:04 visual_prompt]: Number of classes: 9 / 9
[09/17 03:43:04 visual_prompt]: Loading validation data...
[09/17 03:43:04 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 03:43:04 visual_prompt]: Number of images: 200
[09/17 03:43:04 visual_prompt]: Number of classes: 9 / 9
[09/17 03:43:04 visual_prompt]: Loading test data...
[09/17 03:43:04 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 03:43:21 visual_prompt]: Number of images: 12150
[09/17 03:43:21 visual_prompt]: Number of classes: 9 / 9
[09/17 03:43:21 visual_prompt]: Constructing models...
[09/17 03:43:24 visual_prompt]: Total Parameters: 86727177	 Gradient Parameters: 928521
[09/17 03:43:24 visual_prompt]: tuned percent:1.071
[09/17 03:43:27 visual_prompt]: Device used for model: 0
[09/17 03:43:27 visual_prompt]: Setting up Evalutator...
[09/17 03:43:27 visual_prompt]: Setting up Trainer...
[09/17 03:43:27 visual_prompt]: 	Setting up the optimizer...
[09/17 03:43:27 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 03:43:37 visual_prompt]: Epoch 1 / 100: avg data time: 1.25e-01, avg batch time: 0.5956, average train loss: 2.3935
[09/17 03:43:40 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.1444, average loss: 2.3894
[09/17 03:43:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 52.50	
[09/17 03:44:01 visual_prompt]: 	Test 100/190. loss: 2.436, 0.1975 s / batch. (data: 1.28e-02)max mem: 17.22448 GB 
[09/17 03:44:20 visual_prompt]: Inference (test):avg data time: 7.71e-03, avg batch time: 0.1916, average loss: 2.3690
[09/17 03:44:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.69	top5: 55.29	
[09/17 03:44:20 visual_prompt]: Best epoch 1: best metric: 0.140
[09/17 03:44:20 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/17 03:44:29 visual_prompt]: Epoch 2 / 100: avg data time: 1.22e-01, avg batch time: 0.5218, average train loss: 2.8733
[09/17 03:44:32 visual_prompt]: Inference (val):avg data time: 4.74e-05, avg batch time: 0.1427, average loss: 2.3275
[09/17 03:44:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 52.00	
[09/17 03:44:53 visual_prompt]: 	Test 100/190. loss: 2.278, 0.1827 s / batch. (data: 2.23e-04)max mem: 17.22448 GB 
[09/17 03:45:12 visual_prompt]: Inference (test):avg data time: 6.38e-03, avg batch time: 0.1920, average loss: 2.2743
[09/17 03:45:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.49	top5: 55.51	
[09/17 03:45:12 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/17 03:45:21 visual_prompt]: Epoch 3 / 100: avg data time: 1.07e-01, avg batch time: 0.5058, average train loss: 2.3878
[09/17 03:45:24 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1424, average loss: 2.3888
[09/17 03:45:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/17 03:45:45 visual_prompt]: 	Test 100/190. loss: 2.455, 0.1826 s / batch. (data: 1.05e-04)max mem: 17.22448 GB 
[09/17 03:46:03 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1933, average loss: 2.5086
[09/17 03:46:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.37	
[09/17 03:46:03 visual_prompt]: Best epoch 3: best metric: 0.145
[09/17 03:46:03 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/17 03:46:12 visual_prompt]: Epoch 4 / 100: avg data time: 1.01e-01, avg batch time: 0.5053, average train loss: 2.5045
[09/17 03:46:15 visual_prompt]: Inference (val):avg data time: 2.33e-05, avg batch time: 0.1426, average loss: 2.5995
[09/17 03:46:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/17 03:46:37 visual_prompt]: 	Test 100/190. loss: 2.554, 0.2220 s / batch. (data: 3.70e-02)max mem: 17.22448 GB 
[09/17 03:46:55 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1941, average loss: 2.6097
[09/17 03:46:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.39	
[09/17 03:46:55 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/17 03:47:04 visual_prompt]: Epoch 5 / 100: avg data time: 1.08e-01, avg batch time: 0.5083, average train loss: 2.5662
[09/17 03:47:07 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.1424, average loss: 2.3795
[09/17 03:47:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 59.50	
[09/17 03:47:28 visual_prompt]: 	Test 100/190. loss: 2.550, 0.1955 s / batch. (data: 1.37e-02)max mem: 17.22448 GB 
[09/17 03:47:47 visual_prompt]: Inference (test):avg data time: 6.43e-03, avg batch time: 0.1919, average loss: 2.4923
[09/17 03:47:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.23	
[09/17 03:47:47 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/17 03:47:56 visual_prompt]: Epoch 6 / 100: avg data time: 1.04e-01, avg batch time: 0.5067, average train loss: 3.2273
[09/17 03:47:59 visual_prompt]: Inference (val):avg data time: 2.41e-05, avg batch time: 0.1425, average loss: 3.1268
[09/17 03:47:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 52.00	
[09/17 03:48:20 visual_prompt]: 	Test 100/190. loss: 3.006, 0.1984 s / batch. (data: 1.66e-02)max mem: 17.22448 GB 
[09/17 03:48:39 visual_prompt]: Inference (test):avg data time: 6.79e-03, avg batch time: 0.1931, average loss: 2.9288
[09/17 03:48:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 55.52	
[09/17 03:48:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/17 03:48:48 visual_prompt]: Epoch 7 / 100: avg data time: 1.17e-01, avg batch time: 0.5194, average train loss: 4.3478
[09/17 03:48:51 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1426, average loss: 4.8675
[09/17 03:48:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 58.00	
[09/17 03:49:12 visual_prompt]: 	Test 100/190. loss: 4.609, 0.1879 s / batch. (data: 1.32e-04)max mem: 17.22448 GB 
[09/17 03:49:31 visual_prompt]: Inference (test):avg data time: 7.86e-03, avg batch time: 0.1931, average loss: 4.9045
[09/17 03:49:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.11	
[09/17 03:49:31 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/17 03:49:40 visual_prompt]: Epoch 8 / 100: avg data time: 1.10e-01, avg batch time: 0.5128, average train loss: 4.7757
[09/17 03:49:43 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1427, average loss: 4.7461
[09/17 03:49:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 56.50	
[09/17 03:50:04 visual_prompt]: 	Test 100/190. loss: 5.175, 0.2072 s / batch. (data: 1.47e-02)max mem: 17.22448 GB 
[09/17 03:50:23 visual_prompt]: Inference (test):avg data time: 7.61e-03, avg batch time: 0.1930, average loss: 4.8661
[09/17 03:50:23 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 56.04	
[09/17 03:50:23 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/17 03:50:32 visual_prompt]: Epoch 9 / 100: avg data time: 1.06e-01, avg batch time: 0.5066, average train loss: 3.8280
[09/17 03:50:35 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1428, average loss: 4.2692
[09/17 03:50:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.00	
[09/17 03:50:56 visual_prompt]: 	Test 100/190. loss: 3.846, 0.2074 s / batch. (data: 2.57e-02)max mem: 17.22448 GB 
[09/17 03:51:14 visual_prompt]: Inference (test):avg data time: 7.16e-03, avg batch time: 0.1924, average loss: 4.1916
[09/17 03:51:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.98	
[09/17 03:51:14 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/17 03:51:23 visual_prompt]: Epoch 10 / 100: avg data time: 1.06e-01, avg batch time: 0.5102, average train loss: 4.7734
[09/17 03:51:27 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1425, average loss: 4.1988
[09/17 03:51:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.00	
[09/17 03:51:48 visual_prompt]: 	Test 100/190. loss: 4.686, 0.1864 s / batch. (data: 1.58e-04)max mem: 17.22448 GB 
[09/17 03:52:06 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1923, average loss: 4.3795
[09/17 03:52:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.33	
[09/17 03:52:06 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/17 03:52:15 visual_prompt]: Epoch 11 / 100: avg data time: 1.13e-01, avg batch time: 0.5171, average train loss: 4.4744
[09/17 03:52:18 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1425, average loss: 3.2565
[09/17 03:52:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 55.50	
[09/17 03:52:40 visual_prompt]: 	Test 100/190. loss: 3.059, 0.1830 s / batch. (data: 3.79e-05)max mem: 17.22448 GB 
[09/17 03:52:58 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1959, average loss: 3.1899
[09/17 03:52:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.54	
[09/17 03:52:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/17 03:53:08 visual_prompt]: Epoch 12 / 100: avg data time: 1.12e-01, avg batch time: 0.5127, average train loss: 8.5880
[09/17 03:53:11 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.1429, average loss: 4.4606
[09/17 03:53:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 59.00	
[09/17 03:53:32 visual_prompt]: 	Test 100/190. loss: 4.892, 0.1968 s / batch. (data: 1.48e-02)max mem: 17.22448 GB 
[09/17 03:53:50 visual_prompt]: Inference (test):avg data time: 7.34e-03, avg batch time: 0.1934, average loss: 4.7660
[09/17 03:53:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.21	top5: 55.84	
[09/17 03:53:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/17 03:53:59 visual_prompt]: Epoch 13 / 100: avg data time: 9.93e-02, avg batch time: 0.5015, average train loss: 13.0768
[09/17 03:54:03 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.1428, average loss: 13.3356
[09/17 03:54:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/17 03:54:24 visual_prompt]: 	Test 100/190. loss: 11.305, 0.1833 s / batch. (data: 1.42e-04)max mem: 17.22448 GB 
[09/17 03:54:42 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1929, average loss: 13.0153
[09/17 03:54:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 55.16	
[09/17 03:54:42 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/17 03:54:51 visual_prompt]: Epoch 14 / 100: avg data time: 9.90e-02, avg batch time: 0.5072, average train loss: 14.7568
[09/17 03:54:54 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1427, average loss: 9.5901
[09/17 03:54:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 62.50	
[09/17 03:55:15 visual_prompt]: 	Test 100/190. loss: 10.830, 0.1959 s / batch. (data: 1.42e-02)max mem: 17.22448 GB 
[09/17 03:55:34 visual_prompt]: Inference (test):avg data time: 8.92e-03, avg batch time: 0.1938, average loss: 10.6710
[09/17 03:55:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 55.71	
[09/17 03:55:34 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/17 03:55:43 visual_prompt]: Epoch 15 / 100: avg data time: 9.02e-02, avg batch time: 0.4942, average train loss: 11.1546
[09/17 03:55:46 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1438, average loss: 7.6007
[09/17 03:55:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 54.50	
[09/17 03:56:07 visual_prompt]: 	Test 100/190. loss: 7.640, 0.1827 s / batch. (data: 1.26e-04)max mem: 17.22448 GB 
[09/17 03:56:25 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1931, average loss: 7.6042
[09/17 03:56:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 56.05	
[09/17 03:56:26 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/17 03:56:35 visual_prompt]: Epoch 16 / 100: avg data time: 1.06e-01, avg batch time: 0.5108, average train loss: 5.8699
[09/17 03:56:38 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1427, average loss: 3.1490
[09/17 03:56:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/17 03:56:59 visual_prompt]: 	Test 100/190. loss: 3.316, 0.1826 s / batch. (data: 1.14e-04)max mem: 17.22448 GB 
[09/17 03:57:17 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1929, average loss: 3.4229
[09/17 03:57:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.20	
[09/17 03:57:17 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/17 03:57:26 visual_prompt]: Epoch 17 / 100: avg data time: 1.08e-01, avg batch time: 0.5111, average train loss: 4.1747
[09/17 03:57:29 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.1426, average loss: 5.0139
[09/17 03:57:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 52.00	
[09/17 03:57:51 visual_prompt]: 	Test 100/190. loss: 4.945, 0.2109 s / batch. (data: 1.52e-02)max mem: 17.22448 GB 
[09/17 03:58:10 visual_prompt]: Inference (test):avg data time: 6.81e-03, avg batch time: 0.1957, average loss: 4.8265
[09/17 03:58:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 56.65	
[09/17 03:58:10 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/17 03:58:19 visual_prompt]: Epoch 18 / 100: avg data time: 1.01e-01, avg batch time: 0.5053, average train loss: 4.4574
[09/17 03:58:22 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1427, average loss: 3.4579
[09/17 03:58:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/17 03:58:43 visual_prompt]: 	Test 100/190. loss: 2.991, 0.1849 s / batch. (data: 1.26e-04)max mem: 17.22448 GB 
[09/17 03:59:02 visual_prompt]: Inference (test):avg data time: 6.54e-03, avg batch time: 0.1945, average loss: 3.2360
[09/17 03:59:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.26	
[09/17 03:59:02 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/17 03:59:11 visual_prompt]: Epoch 19 / 100: avg data time: 1.06e-01, avg batch time: 0.5079, average train loss: 3.7464
[09/17 03:59:14 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.1432, average loss: 3.9265
[09/17 03:59:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.00	
[09/17 03:59:36 visual_prompt]: 	Test 100/190. loss: 3.486, 0.1876 s / batch. (data: 1.37e-04)max mem: 17.22448 GB 
[09/17 03:59:54 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1961, average loss: 3.6817
[09/17 03:59:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.21	top5: 55.59	
[09/17 03:59:54 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/17 04:00:03 visual_prompt]: Epoch 20 / 100: avg data time: 9.74e-02, avg batch time: 0.5013, average train loss: 2.9787
[09/17 04:00:07 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1426, average loss: 3.1847
[09/17 04:00:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 53.00	
[09/17 04:00:28 visual_prompt]: 	Test 100/190. loss: 3.149, 0.1952 s / batch. (data: 1.35e-02)max mem: 17.22448 GB 
[09/17 04:00:46 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1932, average loss: 3.1709
[09/17 04:00:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 56.19	
[09/17 04:00:46 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/17 04:00:56 visual_prompt]: Epoch 21 / 100: avg data time: 1.17e-01, avg batch time: 0.5182, average train loss: 3.0210
[09/17 04:00:59 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1426, average loss: 2.4629
[09/17 04:00:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.50	top5: 56.50	
[09/17 04:01:20 visual_prompt]: 	Test 100/190. loss: 2.452, 0.1842 s / batch. (data: 1.37e-04)max mem: 17.22448 GB 
[09/17 04:01:38 visual_prompt]: Inference (test):avg data time: 8.30e-03, avg batch time: 0.1930, average loss: 2.4520
[09/17 04:01:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.14	top5: 56.13	
[09/17 04:01:38 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/17 04:01:47 visual_prompt]: Epoch 22 / 100: avg data time: 1.08e-01, avg batch time: 0.5135, average train loss: 2.5052
[09/17 04:01:50 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1426, average loss: 2.6557
[09/17 04:01:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 8.50	top5: 57.50	
[09/17 04:02:11 visual_prompt]: 	Test 100/190. loss: 2.680, 0.2210 s / batch. (data: 1.25e-02)max mem: 17.22448 GB 
[09/17 04:02:30 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1930, average loss: 2.6650
[09/17 04:02:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.22	top5: 55.46	
[09/17 04:02:30 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/17 04:02:39 visual_prompt]: Epoch 23 / 100: avg data time: 1.02e-01, avg batch time: 0.5053, average train loss: 2.4546
[09/17 04:02:42 visual_prompt]: Inference (val):avg data time: 2.24e-05, avg batch time: 0.1424, average loss: 2.5209
[09/17 04:02:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 57.50	
[09/17 04:03:03 visual_prompt]: 	Test 100/190. loss: 2.522, 0.1828 s / batch. (data: 1.65e-04)max mem: 17.22448 GB 
[09/17 04:03:22 visual_prompt]: Inference (test):avg data time: 6.59e-03, avg batch time: 0.1929, average loss: 2.4696
[09/17 04:03:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 60.02	
[09/17 04:03:22 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/17 04:03:31 visual_prompt]: Epoch 24 / 100: avg data time: 1.01e-01, avg batch time: 0.5018, average train loss: 2.7565
[09/17 04:03:34 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1428, average loss: 2.4653
[09/17 04:03:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 57.50	
[09/17 04:03:55 visual_prompt]: 	Test 100/190. loss: 2.328, 0.1959 s / batch. (data: 1.39e-02)max mem: 17.22448 GB 
[09/17 04:04:14 visual_prompt]: Inference (test):avg data time: 8.37e-03, avg batch time: 0.1937, average loss: 2.3770
[09/17 04:04:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.39	top5: 58.80	
[09/17 04:04:14 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/17 04:04:23 visual_prompt]: Epoch 25 / 100: avg data time: 1.11e-01, avg batch time: 0.5183, average train loss: 2.6598
[09/17 04:04:26 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1425, average loss: 2.7412
[09/17 04:04:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 54.50	
[09/17 04:04:47 visual_prompt]: 	Test 100/190. loss: 2.347, 0.2155 s / batch. (data: 1.27e-04)max mem: 17.22448 GB 
[09/17 04:05:06 visual_prompt]: Inference (test):avg data time: 7.78e-03, avg batch time: 0.1931, average loss: 2.6074
[09/17 04:05:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.79	top5: 55.33	
[09/17 04:05:06 visual_prompt]: Best epoch 25: best metric: 0.200
[09/17 04:05:06 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/17 04:05:15 visual_prompt]: Epoch 26 / 100: avg data time: 1.03e-01, avg batch time: 0.5069, average train loss: 2.3840
[09/17 04:05:18 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1426, average loss: 2.4776
[09/17 04:05:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 59.50	
[09/17 04:05:39 visual_prompt]: 	Test 100/190. loss: 2.353, 0.1944 s / batch. (data: 1.25e-02)max mem: 17.22448 GB 
[09/17 04:05:57 visual_prompt]: Inference (test):avg data time: 8.11e-03, avg batch time: 0.1931, average loss: 2.4434
[09/17 04:05:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.49	top5: 61.19	
[09/17 04:05:57 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/17 04:06:06 visual_prompt]: Epoch 27 / 100: avg data time: 1.09e-01, avg batch time: 0.5112, average train loss: 2.3602
[09/17 04:06:09 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1427, average loss: 2.3371
[09/17 04:06:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.00	top5: 57.00	
[09/17 04:06:31 visual_prompt]: 	Test 100/190. loss: 2.147, 0.1848 s / batch. (data: 1.20e-04)max mem: 17.22448 GB 
[09/17 04:06:50 visual_prompt]: Inference (test):avg data time: 8.00e-03, avg batch time: 0.1963, average loss: 2.3063
[09/17 04:06:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.94	top5: 60.68	
[09/17 04:06:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/17 04:06:59 visual_prompt]: Epoch 28 / 100: avg data time: 1.04e-01, avg batch time: 0.5070, average train loss: 2.4740
[09/17 04:07:02 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1425, average loss: 2.6768
[09/17 04:07:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.50	top5: 55.50	
[09/17 04:07:23 visual_prompt]: 	Test 100/190. loss: 2.452, 0.1965 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/17 04:07:42 visual_prompt]: Inference (test):avg data time: 8.32e-03, avg batch time: 0.1938, average loss: 2.5934
[09/17 04:07:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.24	top5: 57.79	
[09/17 04:07:42 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/17 04:07:51 visual_prompt]: Epoch 29 / 100: avg data time: 1.09e-01, avg batch time: 0.5118, average train loss: 2.5223
[09/17 04:07:54 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.1427, average loss: 2.4785
[09/17 04:07:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.50	top5: 62.50	
[09/17 04:08:15 visual_prompt]: 	Test 100/190. loss: 2.382, 0.1822 s / batch. (data: 9.01e-05)max mem: 17.22448 GB 
[09/17 04:08:34 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1932, average loss: 2.4922
[09/17 04:08:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.82	top5: 66.13	
[09/17 04:08:34 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/17 04:08:43 visual_prompt]: Epoch 30 / 100: avg data time: 1.09e-01, avg batch time: 0.5110, average train loss: 2.3492
[09/17 04:08:46 visual_prompt]: Inference (val):avg data time: 2.36e-05, avg batch time: 0.1428, average loss: 2.5578
[09/17 04:08:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 62.00	
[09/17 04:09:07 visual_prompt]: 	Test 100/190. loss: 2.253, 0.1978 s / batch. (data: 1.53e-02)max mem: 17.22448 GB 
[09/17 04:09:26 visual_prompt]: Inference (test):avg data time: 8.17e-03, avg batch time: 0.1938, average loss: 2.5567
[09/17 04:09:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.39	top5: 65.31	
[09/17 04:09:26 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/17 04:09:35 visual_prompt]: Epoch 31 / 100: avg data time: 1.16e-01, avg batch time: 0.5159, average train loss: 2.5717
[09/17 04:09:38 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1428, average loss: 2.7468
[09/17 04:09:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 62.00	
[09/17 04:10:00 visual_prompt]: 	Test 100/190. loss: 3.086, 0.2105 s / batch. (data: 1.39e-02)max mem: 17.22448 GB 
[09/17 04:10:18 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1929, average loss: 2.9249
[09/17 04:10:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.18	top5: 56.21	
[09/17 04:10:18 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/17 04:10:27 visual_prompt]: Epoch 32 / 100: avg data time: 1.06e-01, avg batch time: 0.5099, average train loss: 2.4846
[09/17 04:10:30 visual_prompt]: Inference (val):avg data time: 2.43e-05, avg batch time: 0.1425, average loss: 2.0627
[09/17 04:10:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 79.50	
[09/17 04:10:51 visual_prompt]: 	Test 100/190. loss: 2.213, 0.2170 s / batch. (data: 1.58e-02)max mem: 17.22448 GB 
[09/17 04:11:10 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1957, average loss: 2.2620
[09/17 04:11:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.63	top5: 76.66	
[09/17 04:11:10 visual_prompt]: Best epoch 32: best metric: 0.255
[09/17 04:11:10 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/17 04:11:19 visual_prompt]: Epoch 33 / 100: avg data time: 1.06e-01, avg batch time: 0.5080, average train loss: 2.2454
[09/17 04:11:23 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1427, average loss: 2.1804
[09/17 04:11:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 75.50	
[09/17 04:11:44 visual_prompt]: 	Test 100/190. loss: 2.496, 0.1972 s / batch. (data: 1.51e-02)max mem: 17.22448 GB 
[09/17 04:12:02 visual_prompt]: Inference (test):avg data time: 7.37e-03, avg batch time: 0.1932, average loss: 2.3416
[09/17 04:12:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.47	top5: 71.14	
[09/17 04:12:02 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/17 04:12:11 visual_prompt]: Epoch 34 / 100: avg data time: 1.09e-01, avg batch time: 0.5110, average train loss: 2.3741
[09/17 04:12:14 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1426, average loss: 2.5225
[09/17 04:12:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 70.00	
[09/17 04:12:35 visual_prompt]: 	Test 100/190. loss: 2.730, 0.1866 s / batch. (data: 1.34e-04)max mem: 17.22448 GB 
[09/17 04:12:54 visual_prompt]: Inference (test):avg data time: 8.22e-03, avg batch time: 0.1937, average loss: 2.6093
[09/17 04:12:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.68	top5: 67.33	
[09/17 04:12:54 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/17 04:13:03 visual_prompt]: Epoch 35 / 100: avg data time: 1.03e-01, avg batch time: 0.5035, average train loss: 2.3455
[09/17 04:13:06 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1428, average loss: 2.0930
[09/17 04:13:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 77.50	
[09/17 04:13:27 visual_prompt]: 	Test 100/190. loss: 2.144, 0.1932 s / batch. (data: 1.10e-02)max mem: 17.22448 GB 
[09/17 04:13:46 visual_prompt]: Inference (test):avg data time: 6.92e-03, avg batch time: 0.1929, average loss: 2.1130
[09/17 04:13:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.28	top5: 77.46	
[09/17 04:13:46 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/17 04:13:55 visual_prompt]: Epoch 36 / 100: avg data time: 1.09e-01, avg batch time: 0.5115, average train loss: 2.1645
[09/17 04:13:58 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1427, average loss: 2.3106
[09/17 04:13:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 68.50	
[09/17 04:14:19 visual_prompt]: 	Test 100/190. loss: 2.527, 0.1827 s / batch. (data: 1.26e-04)max mem: 17.22448 GB 
[09/17 04:14:38 visual_prompt]: Inference (test):avg data time: 8.91e-03, avg batch time: 0.1942, average loss: 2.4990
[09/17 04:14:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.34	top5: 62.93	
[09/17 04:14:38 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/17 04:14:47 visual_prompt]: Epoch 37 / 100: avg data time: 9.81e-02, avg batch time: 0.5017, average train loss: 2.3050
[09/17 04:14:50 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1428, average loss: 2.0513
[09/17 04:14:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 74.00	
[09/17 04:15:11 visual_prompt]: 	Test 100/190. loss: 2.083, 0.1962 s / batch. (data: 1.40e-02)max mem: 17.22448 GB 
[09/17 04:15:30 visual_prompt]: Inference (test):avg data time: 8.67e-03, avg batch time: 0.1934, average loss: 2.1000
[09/17 04:15:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.70	top5: 75.87	
[09/17 04:15:30 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/17 04:15:39 visual_prompt]: Epoch 38 / 100: avg data time: 1.08e-01, avg batch time: 0.5087, average train loss: 2.1462
[09/17 04:15:42 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1426, average loss: 1.8881
[09/17 04:15:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 83.00	
[09/17 04:16:03 visual_prompt]: 	Test 100/190. loss: 1.892, 0.1826 s / batch. (data: 1.38e-04)max mem: 17.22448 GB 
[09/17 04:16:22 visual_prompt]: Inference (test):avg data time: 8.21e-03, avg batch time: 0.1946, average loss: 2.0280
[09/17 04:16:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.97	top5: 76.47	
[09/17 04:16:22 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/17 04:16:31 visual_prompt]: Epoch 39 / 100: avg data time: 1.05e-01, avg batch time: 0.5067, average train loss: 2.0703
[09/17 04:16:34 visual_prompt]: Inference (val):avg data time: 1.76e-05, avg batch time: 0.1426, average loss: 2.0049
[09/17 04:16:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 82.00	
[09/17 04:16:56 visual_prompt]: 	Test 100/190. loss: 1.992, 0.1973 s / batch. (data: 1.53e-02)max mem: 17.22448 GB 
[09/17 04:17:14 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1959, average loss: 2.0673
[09/17 04:17:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.86	top5: 77.01	
[09/17 04:17:14 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/17 04:17:24 visual_prompt]: Epoch 40 / 100: avg data time: 1.13e-01, avg batch time: 0.5508, average train loss: 2.0562
[09/17 04:17:27 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1461, average loss: 2.0973
[09/17 04:17:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 85.50	
[09/17 04:17:48 visual_prompt]: 	Test 100/190. loss: 1.988, 0.1962 s / batch. (data: 1.40e-02)max mem: 17.22448 GB 
[09/17 04:18:07 visual_prompt]: Inference (test):avg data time: 7.66e-03, avg batch time: 0.1935, average loss: 2.1253
[09/17 04:18:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.87	top5: 81.73	
[09/17 04:18:07 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/17 04:18:16 visual_prompt]: Epoch 41 / 100: avg data time: 1.12e-01, avg batch time: 0.5136, average train loss: 2.0940
[09/17 04:18:19 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 2.1084
[09/17 04:18:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.50	top5: 87.50	
[09/17 04:18:40 visual_prompt]: 	Test 100/190. loss: 2.090, 0.1828 s / batch. (data: 1.27e-04)max mem: 17.22448 GB 
[09/17 04:18:59 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1926, average loss: 2.2253
[09/17 04:18:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.29	top5: 79.42	
[09/17 04:18:59 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/17 04:19:08 visual_prompt]: Epoch 42 / 100: avg data time: 9.92e-02, avg batch time: 0.5021, average train loss: 2.0044
[09/17 04:19:11 visual_prompt]: Inference (val):avg data time: 2.23e-05, avg batch time: 0.1426, average loss: 2.0748
[09/17 04:19:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 84.50	
[09/17 04:19:32 visual_prompt]: 	Test 100/190. loss: 2.187, 0.1825 s / batch. (data: 1.45e-04)max mem: 17.22448 GB 
[09/17 04:19:51 visual_prompt]: Inference (test):avg data time: 7.48e-03, avg batch time: 0.1926, average loss: 2.2869
[09/17 04:19:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.73	top5: 78.01	
[09/17 04:19:51 visual_prompt]: Best epoch 42: best metric: 0.260
[09/17 04:19:51 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/17 04:20:00 visual_prompt]: Epoch 43 / 100: avg data time: 1.02e-01, avg batch time: 0.5065, average train loss: 2.2730
[09/17 04:20:03 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1423, average loss: 1.9905
[09/17 04:20:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.00	top5: 86.00	
[09/17 04:20:24 visual_prompt]: 	Test 100/190. loss: 2.191, 0.1830 s / batch. (data: 1.28e-04)max mem: 17.22448 GB 
[09/17 04:20:43 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1928, average loss: 2.1624
[09/17 04:20:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.25	top5: 81.93	
[09/17 04:20:43 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/17 04:20:52 visual_prompt]: Epoch 44 / 100: avg data time: 1.06e-01, avg batch time: 0.5074, average train loss: 2.0404
[09/17 04:20:55 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1428, average loss: 1.9918
[09/17 04:20:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 86.50	
[09/17 04:21:17 visual_prompt]: 	Test 100/190. loss: 1.931, 0.1919 s / batch. (data: 9.41e-03)max mem: 17.22448 GB 
[09/17 04:21:35 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1979, average loss: 2.0732
[09/17 04:21:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.69	top5: 79.73	
[09/17 04:21:35 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/17 04:21:45 visual_prompt]: Epoch 45 / 100: avg data time: 1.12e-01, avg batch time: 0.5132, average train loss: 1.9760
[09/17 04:21:48 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1426, average loss: 2.1728
[09/17 04:21:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.50	top5: 77.50	
[09/17 04:22:09 visual_prompt]: 	Test 100/190. loss: 2.010, 0.1823 s / batch. (data: 1.20e-04)max mem: 17.22448 GB 
[09/17 04:22:27 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1936, average loss: 2.2819
[09/17 04:22:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.61	top5: 74.55	
[09/17 04:22:27 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/17 04:22:36 visual_prompt]: Epoch 46 / 100: avg data time: 1.02e-01, avg batch time: 0.5062, average train loss: 2.0637
[09/17 04:22:39 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1428, average loss: 1.9441
[09/17 04:22:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.50	top5: 90.00	
[09/17 04:23:01 visual_prompt]: 	Test 100/190. loss: 2.158, 0.1963 s / batch. (data: 1.40e-02)max mem: 17.22448 GB 
[09/17 04:23:19 visual_prompt]: Inference (test):avg data time: 6.49e-03, avg batch time: 0.1923, average loss: 2.1887
[09/17 04:23:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.77	top5: 83.20	
[09/17 04:23:19 visual_prompt]: Best epoch 46: best metric: 0.295
[09/17 04:23:19 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/17 04:23:28 visual_prompt]: Epoch 47 / 100: avg data time: 1.03e-01, avg batch time: 0.5060, average train loss: 2.0393
[09/17 04:23:31 visual_prompt]: Inference (val):avg data time: 2.17e-05, avg batch time: 0.1427, average loss: 1.9398
[09/17 04:23:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.50	top5: 86.50	
[09/17 04:23:52 visual_prompt]: 	Test 100/190. loss: 2.128, 0.1830 s / batch. (data: 9.23e-05)max mem: 17.22448 GB 
[09/17 04:24:11 visual_prompt]: Inference (test):avg data time: 6.58e-03, avg batch time: 0.1929, average loss: 2.1331
[09/17 04:24:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.78	top5: 81.89	
[09/17 04:24:11 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/17 04:24:20 visual_prompt]: Epoch 48 / 100: avg data time: 1.15e-01, avg batch time: 0.5179, average train loss: 1.8982
[09/17 04:24:23 visual_prompt]: Inference (val):avg data time: 2.08e-05, avg batch time: 0.1425, average loss: 1.7501
[09/17 04:24:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 90.50	
[09/17 04:24:45 visual_prompt]: 	Test 100/190. loss: 1.801, 0.1940 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/17 04:25:03 visual_prompt]: Inference (test):avg data time: 6.61e-03, avg batch time: 0.1934, average loss: 1.9587
[09/17 04:25:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.76	top5: 81.88	
[09/17 04:25:03 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/17 04:25:12 visual_prompt]: Epoch 49 / 100: avg data time: 1.06e-01, avg batch time: 0.5078, average train loss: 1.8611
[09/17 04:25:15 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1427, average loss: 1.8875
[09/17 04:25:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 87.50	
[09/17 04:25:37 visual_prompt]: 	Test 100/190. loss: 2.080, 0.2083 s / batch. (data: 2.64e-02)max mem: 17.22448 GB 
[09/17 04:25:55 visual_prompt]: Inference (test):avg data time: 8.05e-03, avg batch time: 0.1929, average loss: 2.0598
[09/17 04:25:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.93	top5: 83.53	
[09/17 04:25:55 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/17 04:26:04 visual_prompt]: Epoch 50 / 100: avg data time: 1.08e-01, avg batch time: 0.5106, average train loss: 1.8632
[09/17 04:26:07 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1427, average loss: 1.7805
[09/17 04:26:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 91.00	
[09/17 04:26:28 visual_prompt]: 	Test 100/190. loss: 2.029, 0.1827 s / batch. (data: 1.11e-04)max mem: 17.22448 GB 
[09/17 04:26:47 visual_prompt]: Inference (test):avg data time: 7.32e-03, avg batch time: 0.1923, average loss: 2.0295
[09/17 04:26:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.09	top5: 80.77	
[09/17 04:26:47 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/17 04:26:56 visual_prompt]: Epoch 51 / 100: avg data time: 1.11e-01, avg batch time: 0.5113, average train loss: 1.8365
[09/17 04:26:59 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1427, average loss: 1.6593
[09/17 04:26:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 95.00	
[09/17 04:27:20 visual_prompt]: 	Test 100/190. loss: 1.679, 0.1831 s / batch. (data: 1.36e-04)max mem: 17.22448 GB 
[09/17 04:27:39 visual_prompt]: Inference (test):avg data time: 7.52e-03, avg batch time: 0.1933, average loss: 1.8602
[09/17 04:27:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 87.46	
[09/17 04:27:39 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/17 04:27:48 visual_prompt]: Epoch 52 / 100: avg data time: 1.05e-01, avg batch time: 0.5075, average train loss: 1.8141
[09/17 04:27:51 visual_prompt]: Inference (val):avg data time: 2.28e-05, avg batch time: 0.1426, average loss: 1.6757
[09/17 04:27:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.00	top5: 96.00	
[09/17 04:28:12 visual_prompt]: 	Test 100/190. loss: 1.836, 0.1825 s / batch. (data: 1.17e-04)max mem: 17.22448 GB 
[09/17 04:28:31 visual_prompt]: Inference (test):avg data time: 6.22e-03, avg batch time: 0.1968, average loss: 1.9113
[09/17 04:28:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.20	top5: 86.65	
[09/17 04:28:31 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/17 04:28:41 visual_prompt]: Epoch 53 / 100: avg data time: 1.07e-01, avg batch time: 0.5097, average train loss: 1.8850
[09/17 04:28:44 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1426, average loss: 2.0829
[09/17 04:28:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 87.00	
[09/17 04:29:05 visual_prompt]: 	Test 100/190. loss: 2.228, 0.1890 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/17 04:29:24 visual_prompt]: Inference (test):avg data time: 6.71e-03, avg batch time: 0.1924, average loss: 2.3603
[09/17 04:29:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.42	top5: 80.27	
[09/17 04:29:24 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/17 04:29:32 visual_prompt]: Epoch 54 / 100: avg data time: 9.58e-02, avg batch time: 0.4999, average train loss: 1.9621
[09/17 04:29:36 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1428, average loss: 2.2576
[09/17 04:29:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.00	top5: 76.50	
[09/17 04:29:57 visual_prompt]: 	Test 100/190. loss: 2.183, 0.1944 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/17 04:30:15 visual_prompt]: Inference (test):avg data time: 6.54e-03, avg batch time: 0.1937, average loss: 2.4842
[09/17 04:30:15 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.59	top5: 72.78	
[09/17 04:30:15 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/17 04:30:24 visual_prompt]: Epoch 55 / 100: avg data time: 1.08e-01, avg batch time: 0.5128, average train loss: 1.7671
[09/17 04:30:28 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 1.7554
[09/17 04:30:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 94.50	
[09/17 04:30:49 visual_prompt]: 	Test 100/190. loss: 2.063, 0.1957 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/17 04:31:08 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1940, average loss: 2.0822
[09/17 04:31:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.52	top5: 86.43	
[09/17 04:31:08 visual_prompt]: Best epoch 55: best metric: 0.310
[09/17 04:31:08 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/17 04:31:17 visual_prompt]: Epoch 56 / 100: avg data time: 1.14e-01, avg batch time: 0.5152, average train loss: 1.7067
[09/17 04:31:20 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.1427, average loss: 1.9112
[09/17 04:31:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 87.00	
[09/17 04:31:41 visual_prompt]: 	Test 100/190. loss: 2.069, 0.1963 s / batch. (data: 1.42e-04)max mem: 17.22448 GB 
[09/17 04:31:59 visual_prompt]: Inference (test):avg data time: 6.77e-03, avg batch time: 0.1923, average loss: 2.2905
[09/17 04:31:59 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.47	top5: 79.14	
[09/17 04:31:59 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/17 04:32:08 visual_prompt]: Epoch 57 / 100: avg data time: 1.08e-01, avg batch time: 0.5101, average train loss: 1.6997
[09/17 04:32:11 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1427, average loss: 1.5688
[09/17 04:32:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.00	
[09/17 04:32:32 visual_prompt]: 	Test 100/190. loss: 1.811, 0.2286 s / batch. (data: 4.68e-02)max mem: 17.22448 GB 
[09/17 04:32:51 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1923, average loss: 1.8990
[09/17 04:32:51 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.91	top5: 87.62	
[09/17 04:32:51 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/17 04:33:00 visual_prompt]: Epoch 58 / 100: avg data time: 1.16e-01, avg batch time: 0.5182, average train loss: 1.6107
[09/17 04:33:03 visual_prompt]: Inference (val):avg data time: 3.45e-05, avg batch time: 0.1426, average loss: 1.5144
[09/17 04:33:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.00	top5: 97.50	
[09/17 04:33:24 visual_prompt]: 	Test 100/190. loss: 2.021, 0.2061 s / batch. (data: 9.47e-03)max mem: 17.22448 GB 
[09/17 04:33:43 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1931, average loss: 2.0670
[09/17 04:33:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.41	top5: 87.97	
[09/17 04:33:43 visual_prompt]: Best epoch 58: best metric: 0.400
[09/17 04:33:43 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/17 04:33:52 visual_prompt]: Epoch 59 / 100: avg data time: 1.04e-01, avg batch time: 0.5058, average train loss: 1.6559
[09/17 04:33:55 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1430, average loss: 1.4899
[09/17 04:33:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 44.00	top5: 97.00	
[09/17 04:34:16 visual_prompt]: 	Test 100/190. loss: 1.889, 0.1830 s / batch. (data: 1.18e-04)max mem: 17.22448 GB 
[09/17 04:34:35 visual_prompt]: Inference (test):avg data time: 6.43e-03, avg batch time: 0.1930, average loss: 2.0058
[09/17 04:34:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.77	top5: 88.78	
[09/17 04:34:35 visual_prompt]: Best epoch 59: best metric: 0.440
[09/17 04:34:35 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/17 04:34:45 visual_prompt]: Epoch 60 / 100: avg data time: 1.07e-01, avg batch time: 0.5137, average train loss: 1.5790
[09/17 04:34:48 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1426, average loss: 1.4358
[09/17 04:34:48 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.50	top5: 96.50	
[09/17 04:35:09 visual_prompt]: 	Test 100/190. loss: 1.790, 0.1901 s / batch. (data: 1.22e-04)max mem: 17.22448 GB 
[09/17 04:35:27 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1927, average loss: 1.9490
[09/17 04:35:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.68	top5: 86.68	
[09/17 04:35:27 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/17 04:35:37 visual_prompt]: Epoch 61 / 100: avg data time: 1.15e-01, avg batch time: 0.5170, average train loss: 1.9178
[09/17 04:35:40 visual_prompt]: Inference (val):avg data time: 2.45e-05, avg batch time: 0.1428, average loss: 1.6637
[09/17 04:35:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 92.50	
[09/17 04:36:02 visual_prompt]: 	Test 100/190. loss: 1.874, 0.1913 s / batch. (data: 9.94e-03)max mem: 17.22448 GB 
[09/17 04:36:20 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1977, average loss: 1.9633
[09/17 04:36:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.85	top5: 83.14	
[09/17 04:36:20 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/17 04:36:30 visual_prompt]: Epoch 62 / 100: avg data time: 1.18e-01, avg batch time: 0.5202, average train loss: 1.6427
[09/17 04:36:33 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1426, average loss: 1.4750
[09/17 04:36:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 36.00	top5: 95.50	
[09/17 04:36:54 visual_prompt]: 	Test 100/190. loss: 1.984, 0.1823 s / batch. (data: 1.07e-04)max mem: 17.22448 GB 
[09/17 04:37:12 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1932, average loss: 1.9758
[09/17 04:37:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.41	top5: 87.09	
[09/17 04:37:12 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/17 04:37:21 visual_prompt]: Epoch 63 / 100: avg data time: 1.11e-01, avg batch time: 0.5122, average train loss: 1.5519
[09/17 04:37:24 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1426, average loss: 2.1021
[09/17 04:37:24 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.50	top5: 88.50	
[09/17 04:37:46 visual_prompt]: 	Test 100/190. loss: 2.301, 0.1831 s / batch. (data: 1.01e-04)max mem: 17.22448 GB 
[09/17 04:38:04 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1938, average loss: 2.7057
[09/17 04:38:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.06	top5: 80.09	
[09/17 04:38:04 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/17 04:38:13 visual_prompt]: Epoch 64 / 100: avg data time: 1.08e-01, avg batch time: 0.5139, average train loss: 1.7259
[09/17 04:38:16 visual_prompt]: Inference (val):avg data time: 3.51e-05, avg batch time: 0.1430, average loss: 1.5887
[09/17 04:38:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 33.00	top5: 93.50	
[09/17 04:38:37 visual_prompt]: 	Test 100/190. loss: 1.858, 0.1962 s / batch. (data: 1.37e-02)max mem: 17.22448 GB 
[09/17 04:38:56 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1933, average loss: 1.9370
[09/17 04:38:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.35	top5: 84.35	
[09/17 04:38:56 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/17 04:39:05 visual_prompt]: Epoch 65 / 100: avg data time: 9.88e-02, avg batch time: 0.5017, average train loss: 1.5732
[09/17 04:39:08 visual_prompt]: Inference (val):avg data time: 4.31e-05, avg batch time: 0.1438, average loss: 1.5759
[09/17 04:39:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 37.50	top5: 95.50	
[09/17 04:39:29 visual_prompt]: 	Test 100/190. loss: 1.900, 0.1903 s / batch. (data: 3.31e-05)max mem: 17.22448 GB 
[09/17 04:39:48 visual_prompt]: Inference (test):avg data time: 6.70e-03, avg batch time: 0.1946, average loss: 2.0653
[09/17 04:39:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.47	top5: 87.12	
[09/17 04:39:48 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/17 04:39:57 visual_prompt]: Epoch 66 / 100: avg data time: 1.01e-01, avg batch time: 0.5036, average train loss: 1.4642
[09/17 04:40:00 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.1424, average loss: 1.4638
[09/17 04:40:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 98.50	
[09/17 04:40:21 visual_prompt]: 	Test 100/190. loss: 2.107, 0.1822 s / batch. (data: 1.29e-04)max mem: 17.22448 GB 
[09/17 04:40:40 visual_prompt]: Inference (test):avg data time: 7.46e-03, avg batch time: 0.1929, average loss: 2.2294
[09/17 04:40:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.57	top5: 88.53	
[09/17 04:40:40 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/17 04:40:49 visual_prompt]: Epoch 67 / 100: avg data time: 1.08e-01, avg batch time: 0.5100, average train loss: 1.4052
[09/17 04:40:52 visual_prompt]: Inference (val):avg data time: 4.08e-05, avg batch time: 0.1469, average loss: 1.1750
[09/17 04:40:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 42.50	top5: 100.00	
[09/17 04:41:13 visual_prompt]: 	Test 100/190. loss: 1.973, 0.1900 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/17 04:41:32 visual_prompt]: Inference (test):avg data time: 7.59e-03, avg batch time: 0.1929, average loss: 2.2615
[09/17 04:41:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.30	top5: 88.25	
[09/17 04:41:32 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/17 04:41:41 visual_prompt]: Epoch 68 / 100: avg data time: 1.09e-01, avg batch time: 0.5103, average train loss: 1.3988
[09/17 04:41:44 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1426, average loss: 1.5264
[09/17 04:41:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 96.00	
[09/17 04:42:05 visual_prompt]: 	Test 100/190. loss: 1.992, 0.1962 s / batch. (data: 1.39e-02)max mem: 17.22448 GB 
[09/17 04:42:24 visual_prompt]: Inference (test):avg data time: 9.21e-03, avg batch time: 0.1942, average loss: 2.2371
[09/17 04:42:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.54	top5: 84.35	
[09/17 04:42:24 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/17 04:42:33 visual_prompt]: Epoch 69 / 100: avg data time: 1.10e-01, avg batch time: 0.5119, average train loss: 1.3588
[09/17 04:42:36 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1425, average loss: 1.1119
[09/17 04:42:36 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 48.50	top5: 99.50	
[09/17 04:42:57 visual_prompt]: 	Test 100/190. loss: 1.987, 0.1826 s / batch. (data: 1.21e-04)max mem: 17.22448 GB 
[09/17 04:43:16 visual_prompt]: Inference (test):avg data time: 7.17e-03, avg batch time: 0.1928, average loss: 2.2574
[09/17 04:43:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.87	top5: 88.97	
[09/17 04:43:16 visual_prompt]: Best epoch 69: best metric: 0.485
[09/17 04:43:16 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/17 04:43:25 visual_prompt]: Epoch 70 / 100: avg data time: 1.06e-01, avg batch time: 0.5076, average train loss: 1.2175
[09/17 04:43:28 visual_prompt]: Inference (val):avg data time: 2.22e-05, avg batch time: 0.1424, average loss: 1.1590
[09/17 04:43:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 45.50	top5: 99.00	
[09/17 04:43:49 visual_prompt]: 	Test 100/190. loss: 1.900, 0.1819 s / batch. (data: 1.36e-04)max mem: 17.22448 GB 
[09/17 04:44:08 visual_prompt]: Inference (test):avg data time: 6.80e-03, avg batch time: 0.1926, average loss: 2.2516
[09/17 04:44:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.61	top5: 88.34	
[09/17 04:44:08 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/17 04:44:17 visual_prompt]: Epoch 71 / 100: avg data time: 9.85e-02, avg batch time: 0.5033, average train loss: 1.2807
[09/17 04:44:20 visual_prompt]: Inference (val):avg data time: 2.21e-05, avg batch time: 0.1430, average loss: 1.2109
[09/17 04:44:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 47.50	top5: 98.50	
[09/17 04:44:41 visual_prompt]: 	Test 100/190. loss: 2.264, 0.1962 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 04:45:00 visual_prompt]: Inference (test):avg data time: 7.58e-03, avg batch time: 0.1929, average loss: 2.1842
[09/17 04:45:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 88.98	
[09/17 04:45:00 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/17 04:45:09 visual_prompt]: Epoch 72 / 100: avg data time: 1.15e-01, avg batch time: 0.5169, average train loss: 1.1864
[09/17 04:45:12 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1426, average loss: 1.5494
[09/17 04:45:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.00	top5: 97.00	
[09/17 04:45:33 visual_prompt]: 	Test 100/190. loss: 2.534, 0.1941 s / batch. (data: 1.24e-04)max mem: 17.22448 GB 
[09/17 04:45:52 visual_prompt]: Inference (test):avg data time: 7.46e-03, avg batch time: 0.1926, average loss: 2.7173
[09/17 04:45:52 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.67	top5: 85.06	
[09/17 04:45:52 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/17 04:46:01 visual_prompt]: Epoch 73 / 100: avg data time: 1.04e-01, avg batch time: 0.5077, average train loss: 1.3756
[09/17 04:46:04 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1427, average loss: 1.5295
[09/17 04:46:04 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.50	top5: 92.00	
[09/17 04:46:25 visual_prompt]: 	Test 100/190. loss: 2.147, 0.1824 s / batch. (data: 1.32e-04)max mem: 17.22448 GB 
[09/17 04:46:43 visual_prompt]: Inference (test):avg data time: 6.44e-03, avg batch time: 0.1922, average loss: 2.4419
[09/17 04:46:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.44	top5: 80.81	
[09/17 04:46:43 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/17 04:46:52 visual_prompt]: Epoch 74 / 100: avg data time: 9.52e-02, avg batch time: 0.5026, average train loss: 1.3505
[09/17 04:46:55 visual_prompt]: Inference (val):avg data time: 4.32e-05, avg batch time: 0.1426, average loss: 1.1458
[09/17 04:46:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 45.50	top5: 99.50	
[09/17 04:47:16 visual_prompt]: 	Test 100/190. loss: 2.112, 0.1974 s / batch. (data: 1.54e-02)max mem: 17.22448 GB 
[09/17 04:47:35 visual_prompt]: Inference (test):avg data time: 8.03e-03, avg batch time: 0.1938, average loss: 2.3099
[09/17 04:47:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.83	top5: 87.99	
[09/17 04:47:35 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/17 04:47:44 visual_prompt]: Epoch 75 / 100: avg data time: 1.13e-01, avg batch time: 0.5152, average train loss: 1.0749
[09/17 04:47:47 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1424, average loss: 0.9073
[09/17 04:47:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 60.50	top5: 99.50	
[09/17 04:48:09 visual_prompt]: 	Test 100/190. loss: 2.105, 0.2103 s / batch. (data: 7.99e-05)max mem: 17.22448 GB 
[09/17 04:48:27 visual_prompt]: Inference (test):avg data time: 8.72e-03, avg batch time: 0.1937, average loss: 2.4051
[09/17 04:48:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.49	top5: 89.60	
[09/17 04:48:27 visual_prompt]: Best epoch 75: best metric: 0.605
[09/17 04:48:27 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/17 04:48:36 visual_prompt]: Epoch 76 / 100: avg data time: 1.01e-01, avg batch time: 0.5134, average train loss: 0.9548
[09/17 04:48:40 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1438, average loss: 0.8539
[09/17 04:48:40 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 63.50	top5: 100.00	
[09/17 04:49:01 visual_prompt]: 	Test 100/190. loss: 2.201, 0.1861 s / batch. (data: 1.54e-04)max mem: 17.22448 GB 
[09/17 04:49:19 visual_prompt]: Inference (test):avg data time: 8.18e-03, avg batch time: 0.1933, average loss: 2.7983
[09/17 04:49:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.79	top5: 89.67	
[09/17 04:49:19 visual_prompt]: Best epoch 76: best metric: 0.635
[09/17 04:49:19 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/17 04:49:29 visual_prompt]: Epoch 77 / 100: avg data time: 1.15e-01, avg batch time: 0.5399, average train loss: 0.8811
[09/17 04:49:32 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1426, average loss: 0.8479
[09/17 04:49:32 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 67.00	top5: 100.00	
[09/17 04:49:54 visual_prompt]: 	Test 100/190. loss: 2.518, 0.1825 s / batch. (data: 3.65e-05)max mem: 17.22448 GB 
[09/17 04:50:12 visual_prompt]: Inference (test):avg data time: 9.57e-03, avg batch time: 0.1960, average loss: 2.7012
[09/17 04:50:12 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.44	top5: 89.98	
[09/17 04:50:12 visual_prompt]: Best epoch 77: best metric: 0.670
[09/17 04:50:12 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/17 04:50:21 visual_prompt]: Epoch 78 / 100: avg data time: 1.09e-01, avg batch time: 0.5115, average train loss: 0.9168
[09/17 04:50:25 visual_prompt]: Inference (val):avg data time: 1.96e-05, avg batch time: 0.1428, average loss: 1.4474
[09/17 04:50:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 45.50	top5: 100.00	
[09/17 04:50:46 visual_prompt]: 	Test 100/190. loss: 3.334, 0.1964 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 04:51:04 visual_prompt]: Inference (test):avg data time: 8.38e-03, avg batch time: 0.1930, average loss: 3.0661
[09/17 04:51:04 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.37	top5: 88.67	
[09/17 04:51:04 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/17 04:51:14 visual_prompt]: Epoch 79 / 100: avg data time: 1.17e-01, avg batch time: 0.5244, average train loss: 1.0397
[09/17 04:51:16 visual_prompt]: Inference (val):avg data time: 2.35e-05, avg batch time: 0.1427, average loss: 0.8567
[09/17 04:51:16 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 65.00	top5: 100.00	
[09/17 04:51:37 visual_prompt]: 	Test 100/190. loss: 2.104, 0.1931 s / batch. (data: 1.12e-02)max mem: 17.22448 GB 
[09/17 04:51:56 visual_prompt]: Inference (test):avg data time: 8.35e-03, avg batch time: 0.1932, average loss: 2.4096
[09/17 04:51:56 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.54	top5: 88.85	
[09/17 04:51:56 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/17 04:52:05 visual_prompt]: Epoch 80 / 100: avg data time: 1.07e-01, avg batch time: 0.5084, average train loss: 0.8072
[09/17 04:52:08 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.1455, average loss: 0.9628
[09/17 04:52:08 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 55.50	top5: 100.00	
[09/17 04:52:30 visual_prompt]: 	Test 100/190. loss: 1.905, 0.2075 s / batch. (data: 2.56e-02)max mem: 17.22448 GB 
[09/17 04:52:48 visual_prompt]: Inference (test):avg data time: 8.24e-03, avg batch time: 0.1938, average loss: 2.2303
[09/17 04:52:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.67	top5: 87.91	
[09/17 04:52:48 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/17 04:52:57 visual_prompt]: Epoch 81 / 100: avg data time: 1.12e-01, avg batch time: 0.5138, average train loss: 0.7282
[09/17 04:53:00 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1433, average loss: 0.6130
[09/17 04:53:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 76.50	top5: 100.00	
[09/17 04:53:21 visual_prompt]: 	Test 100/190. loss: 3.295, 0.1826 s / batch. (data: 1.43e-04)max mem: 17.22448 GB 
[09/17 04:53:40 visual_prompt]: Inference (test):avg data time: 7.57e-03, avg batch time: 0.1932, average loss: 3.4077
[09/17 04:53:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.80	top5: 89.23	
[09/17 04:53:40 visual_prompt]: Best epoch 81: best metric: 0.765
[09/17 04:53:40 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/17 04:53:49 visual_prompt]: Epoch 82 / 100: avg data time: 1.11e-01, avg batch time: 0.5133, average train loss: 0.6733
[09/17 04:53:52 visual_prompt]: Inference (val):avg data time: 2.31e-05, avg batch time: 0.1428, average loss: 0.6938
[09/17 04:53:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 72.00	top5: 100.00	
[09/17 04:54:13 visual_prompt]: 	Test 100/190. loss: 3.033, 0.2143 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 04:54:32 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1942, average loss: 3.6432
[09/17 04:54:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.76	top5: 88.87	
[09/17 04:54:32 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/17 04:54:41 visual_prompt]: Epoch 83 / 100: avg data time: 1.09e-01, avg batch time: 0.5179, average train loss: 0.7219
[09/17 04:54:44 visual_prompt]: Inference (val):avg data time: 2.27e-05, avg batch time: 0.1426, average loss: 0.8352
[09/17 04:54:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 65.50	top5: 99.50	
[09/17 04:55:06 visual_prompt]: 	Test 100/190. loss: 3.032, 0.1950 s / batch. (data: 1.27e-02)max mem: 17.22448 GB 
[09/17 04:55:24 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1938, average loss: 3.2843
[09/17 04:55:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.19	top5: 89.92	
[09/17 04:55:24 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/17 04:55:34 visual_prompt]: Epoch 84 / 100: avg data time: 1.14e-01, avg batch time: 0.5362, average train loss: 0.7327
[09/17 04:55:37 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1428, average loss: 0.5878
[09/17 04:55:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 78.00	top5: 99.50	
[09/17 04:55:58 visual_prompt]: 	Test 100/190. loss: 2.891, 0.2075 s / batch. (data: 2.54e-02)max mem: 17.22448 GB 
[09/17 04:56:16 visual_prompt]: Inference (test):avg data time: 7.24e-03, avg batch time: 0.1927, average loss: 3.2484
[09/17 04:56:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.47	top5: 88.28	
[09/17 04:56:16 visual_prompt]: Best epoch 84: best metric: 0.780
[09/17 04:56:16 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/17 04:56:25 visual_prompt]: Epoch 85 / 100: avg data time: 1.13e-01, avg batch time: 0.5134, average train loss: 0.6063
[09/17 04:56:28 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1429, average loss: 0.7867
[09/17 04:56:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 69.00	top5: 100.00	
[09/17 04:56:50 visual_prompt]: 	Test 100/190. loss: 3.657, 0.1840 s / batch. (data: 1.04e-04)max mem: 17.22448 GB 
[09/17 04:57:08 visual_prompt]: Inference (test):avg data time: 6.86e-03, avg batch time: 0.1927, average loss: 4.3160
[09/17 04:57:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.84	top5: 88.16	
[09/17 04:57:08 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/17 04:57:17 visual_prompt]: Epoch 86 / 100: avg data time: 1.07e-01, avg batch time: 0.5082, average train loss: 0.5804
[09/17 04:57:20 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.1427, average loss: 0.4277
[09/17 04:57:20 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 86.50	top5: 100.00	
[09/17 04:57:42 visual_prompt]: 	Test 100/190. loss: 3.043, 0.1829 s / batch. (data: 1.07e-04)max mem: 17.22448 GB 
[09/17 04:58:00 visual_prompt]: Inference (test):avg data time: 8.63e-03, avg batch time: 0.1940, average loss: 3.5167
[09/17 04:58:00 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.17	top5: 89.43	
[09/17 04:58:00 visual_prompt]: Best epoch 86: best metric: 0.865
[09/17 04:58:00 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/17 04:58:09 visual_prompt]: Epoch 87 / 100: avg data time: 1.09e-01, avg batch time: 0.5109, average train loss: 0.4293
[09/17 04:58:12 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1428, average loss: 0.3418
[09/17 04:58:12 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 87.00	top5: 100.00	
[09/17 04:58:34 visual_prompt]: 	Test 100/190. loss: 3.715, 0.2090 s / batch. (data: 2.72e-02)max mem: 17.22448 GB 
[09/17 04:58:53 visual_prompt]: Inference (test):avg data time: 8.41e-03, avg batch time: 0.1960, average loss: 4.4927
[09/17 04:58:53 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.79	top5: 89.21	
[09/17 04:58:53 visual_prompt]: Best epoch 87: best metric: 0.870
[09/17 04:58:53 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/17 04:59:02 visual_prompt]: Epoch 88 / 100: avg data time: 1.07e-01, avg batch time: 0.5135, average train loss: 0.4069
[09/17 04:59:05 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.1427, average loss: 0.5248
[09/17 04:59:05 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 80.00	top5: 100.00	
[09/17 04:59:26 visual_prompt]: 	Test 100/190. loss: 4.371, 0.1960 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/17 04:59:44 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1925, average loss: 4.9718
[09/17 04:59:44 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.40	top5: 88.95	
[09/17 04:59:44 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/17 04:59:54 visual_prompt]: Epoch 89 / 100: avg data time: 1.02e-01, avg batch time: 0.5050, average train loss: 0.4291
[09/17 04:59:57 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 0.6623
[09/17 04:59:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 76.00	top5: 100.00	
[09/17 05:00:18 visual_prompt]: 	Test 100/190. loss: 4.475, 0.1972 s / batch. (data: 1.53e-02)max mem: 17.22448 GB 
[09/17 05:00:37 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1940, average loss: 5.1915
[09/17 05:00:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.25	top5: 87.91	
[09/17 05:00:37 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/17 05:00:46 visual_prompt]: Epoch 90 / 100: avg data time: 1.12e-01, avg batch time: 0.5141, average train loss: 0.3203
[09/17 05:00:49 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1428, average loss: 0.3305
[09/17 05:00:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 89.00	top5: 100.00	
[09/17 05:01:10 visual_prompt]: 	Test 100/190. loss: 4.302, 0.1833 s / batch. (data: 3.32e-04)max mem: 17.22448 GB 
[09/17 05:01:28 visual_prompt]: Inference (test):avg data time: 6.11e-03, avg batch time: 0.1921, average loss: 4.9962
[09/17 05:01:28 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.55	top5: 88.84	
[09/17 05:01:28 visual_prompt]: Best epoch 90: best metric: 0.890
[09/17 05:01:28 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/17 05:01:38 visual_prompt]: Epoch 91 / 100: avg data time: 1.16e-01, avg batch time: 0.5188, average train loss: 0.2900
[09/17 05:01:41 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.1427, average loss: 0.1802
[09/17 05:01:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 94.00	top5: 100.00	
[09/17 05:02:02 visual_prompt]: 	Test 100/190. loss: 4.563, 0.1905 s / batch. (data: 1.29e-04)max mem: 17.22448 GB 
[09/17 05:02:21 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1938, average loss: 5.1526
[09/17 05:02:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.09	top5: 89.50	
[09/17 05:02:21 visual_prompt]: Best epoch 91: best metric: 0.940
[09/17 05:02:21 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/17 05:02:30 visual_prompt]: Epoch 92 / 100: avg data time: 1.12e-01, avg batch time: 0.5375, average train loss: 0.2542
[09/17 05:02:33 visual_prompt]: Inference (val):avg data time: 2.37e-05, avg batch time: 0.1425, average loss: 0.1804
[09/17 05:02:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 95.00	top5: 100.00	
[09/17 05:02:55 visual_prompt]: 	Test 100/190. loss: 4.662, 0.1830 s / batch. (data: 1.31e-04)max mem: 17.22448 GB 
[09/17 05:03:13 visual_prompt]: Inference (test):avg data time: 7.51e-03, avg batch time: 0.1929, average loss: 5.4766
[09/17 05:03:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.35	top5: 88.97	
[09/17 05:03:13 visual_prompt]: Best epoch 92: best metric: 0.950
[09/17 05:03:13 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/17 05:03:22 visual_prompt]: Epoch 93 / 100: avg data time: 1.08e-01, avg batch time: 0.5109, average train loss: 0.1895
[09/17 05:03:25 visual_prompt]: Inference (val):avg data time: 2.30e-05, avg batch time: 0.1426, average loss: 0.1060
[09/17 05:03:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 97.00	top5: 100.00	
[09/17 05:03:47 visual_prompt]: 	Test 100/190. loss: 4.858, 0.1860 s / batch. (data: 1.53e-04)max mem: 17.22448 GB 
[09/17 05:04:05 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1923, average loss: 5.6905
[09/17 05:04:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.23	top5: 89.00	
[09/17 05:04:05 visual_prompt]: Best epoch 93: best metric: 0.970
[09/17 05:04:05 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/17 05:04:14 visual_prompt]: Epoch 94 / 100: avg data time: 1.10e-01, avg batch time: 0.5101, average train loss: 0.1941
[09/17 05:04:18 visual_prompt]: Inference (val):avg data time: 1.48e-04, avg batch time: 0.2317, average loss: 0.2625
[09/17 05:04:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 91.00	top5: 100.00	
[09/17 05:04:39 visual_prompt]: 	Test 100/190. loss: 5.358, 0.1823 s / batch. (data: 1.26e-04)max mem: 17.22448 GB 
[09/17 05:04:57 visual_prompt]: Inference (test):avg data time: 7.12e-03, avg batch time: 0.1931, average loss: 6.0369
[09/17 05:04:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.54	top5: 88.78	
[09/17 05:04:57 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/17 05:05:07 visual_prompt]: Epoch 95 / 100: avg data time: 1.14e-01, avg batch time: 0.5191, average train loss: 0.1650
[09/17 05:05:10 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1427, average loss: 0.1348
[09/17 05:05:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 95.50	top5: 100.00	
[09/17 05:05:31 visual_prompt]: 	Test 100/190. loss: 5.124, 0.1973 s / batch. (data: 1.52e-02)max mem: 17.22448 GB 
[09/17 05:05:49 visual_prompt]: Inference (test):avg data time: 7.47e-03, avg batch time: 0.1936, average loss: 5.9199
[09/17 05:05:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.19	top5: 89.11	
[09/17 05:05:49 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/17 05:05:58 visual_prompt]: Epoch 96 / 100: avg data time: 9.41e-02, avg batch time: 0.4975, average train loss: 0.1275
[09/17 05:06:01 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1428, average loss: 0.1416
[09/17 05:06:01 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 95.50	top5: 100.00	
[09/17 05:06:22 visual_prompt]: 	Test 100/190. loss: 5.080, 0.1831 s / batch. (data: 1.18e-04)max mem: 17.22448 GB 
[09/17 05:06:41 visual_prompt]: Inference (test):avg data time: 7.98e-03, avg batch time: 0.1937, average loss: 5.9248
[09/17 05:06:41 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.72	top5: 89.09	
[09/17 05:06:41 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/17 05:06:50 visual_prompt]: Epoch 97 / 100: avg data time: 1.07e-01, avg batch time: 0.5090, average train loss: 0.1028
[09/17 05:06:53 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1427, average loss: 0.1121
[09/17 05:06:53 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 96.50	top5: 100.00	
[09/17 05:07:14 visual_prompt]: 	Test 100/190. loss: 5.261, 0.1955 s / batch. (data: 1.36e-02)max mem: 17.22448 GB 
[09/17 05:07:33 visual_prompt]: Inference (test):avg data time: 6.10e-03, avg batch time: 0.1919, average loss: 6.0117
[09/17 05:07:33 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.77	top5: 89.26	
[09/17 05:07:33 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/17 05:07:42 visual_prompt]: Epoch 98 / 100: avg data time: 9.51e-02, avg batch time: 0.4956, average train loss: 0.0982
[09/17 05:07:45 visual_prompt]: Inference (val):avg data time: 2.32e-05, avg batch time: 0.1428, average loss: 0.1215
[09/17 05:07:45 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 96.00	top5: 100.00	
[09/17 05:08:06 visual_prompt]: 	Test 100/190. loss: 5.336, 0.1827 s / batch. (data: 1.10e-04)max mem: 17.22448 GB 
[09/17 05:08:25 visual_prompt]: Inference (test):avg data time: 8.09e-03, avg batch time: 0.1936, average loss: 6.0879
[09/17 05:08:25 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.83	top5: 89.05	
[09/17 05:08:25 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/17 05:08:34 visual_prompt]: Epoch 99 / 100: avg data time: 1.10e-01, avg batch time: 0.5111, average train loss: 0.0880
[09/17 05:08:37 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1427, average loss: 0.1187
[09/17 05:08:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 95.50	top5: 100.00	
[09/17 05:08:58 visual_prompt]: 	Test 100/190. loss: 5.394, 0.1956 s / batch. (data: 1.37e-02)max mem: 17.22448 GB 
[09/17 05:09:16 visual_prompt]: Inference (test):avg data time: 6.81e-03, avg batch time: 0.1930, average loss: 6.1158
[09/17 05:09:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.84	top5: 89.05	
[09/17 05:09:16 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/17 05:09:25 visual_prompt]: Epoch 100 / 100: avg data time: 1.11e-01, avg batch time: 0.5137, average train loss: 0.1026
[09/17 05:09:28 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1425, average loss: 0.1195
[09/17 05:09:28 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 95.00	top5: 100.00	
[09/17 05:09:50 visual_prompt]: 	Test 100/190. loss: 5.404, 0.1987 s / batch. (data: 1.29e-04)max mem: 17.22448 GB 
[09/17 05:10:08 visual_prompt]: Inference (test):avg data time: 9.18e-03, avg batch time: 0.1949, average loss: 6.1205
[09/17 05:10:08 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 29.93	top5: 89.05	
[09/17 05:10:45 visual_prompt]: Rank of current process: 0. World size: 1
[09/17 05:10:45 visual_prompt]: Environment info:
-------------------  ----------------------------------------------------
Python               3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              1.7.1
PyTorch Debug Build  False
CUDA available       True
CUDA ID              3
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  ----------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[09/17 05:10:45 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/prompt/cub.yaml', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '64', 'MODEL.PROMPT.NUM_TOKENS', '100', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.NAME', 'vtab-smallnorb(predicted_attribute="label_elevation")', 'DATA.NUMBER_CLASSES', '9', 'SOLVER.BASE_LR', '5.0', 'SOLVER.WEIGHT_DECAY', '0.0001', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir/seed800'], train_type='')
[09/17 05:10:45 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/prompt/cub.yaml:
_BASE_: "../base-prompt.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.1
  WEIGHT_DECAY: 0.01
[09/17 05:10:45 visual_prompt]: Training with config:
[09/17 05:10:45 visual_prompt]: {'CUDNN_BENCHMARK': False,
 'DATA': {'BATCH_SIZE': 64,
          'CLASS_WEIGHTS_TYPE': 'none',
          'CROPSIZE': 224,
          'DATAPATH': 'visual_prompt_tuning/data_path',
          'FEATURE': 'sup_vitb16_imagenet21k',
          'MULTILABEL': False,
          'NAME': 'vtab-smallnorb(predicted_attribute="label_elevation")',
          'NO_TEST': False,
          'NUMBER_CLASSES': 9,
          'NUM_WORKERS': 4,
          'PERCENTAGE': 1.0,
          'PIN_MEMORY': True},
 'DBG': False,
 'MODEL': {'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'}),
           'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}),
           'MLP_NUM': 0,
           'MODEL_ROOT': 'visual_prompt_tuning/model_root',
           'PROMPT': {'CLSEMB_FOLDER': '',
                      'CLSEMB_PATH': '',
                      'DEEP': True,
                      'DEEP_SHARED': False,
                      'DROPOUT': 0.1,
                      'FORWARD_DEEP_NOEXPAND': False,
                      'INITIATION': 'random',
                      'LOCATION': 'prepend',
                      'NUM_DEEP_LAYERS': None,
                      'NUM_TOKENS': 100,
                      'PROJECT': -1,
                      'REVERSE_DEEP': False,
                      'SAVE_FOR_EACH_EPOCH': False,
                      'VIT_POOL_TYPE': 'original'},
           'SAVE_CKPT': False,
           'TRANSFER_TYPE': 'prompt',
           'TYPE': 'vit',
           'WEIGHT_PATH': ''},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/seed800/vtab-smallnorb(predicted_attribute="label_elevation")/sup_vitb16_imagenet21k/lr5.0_wd0.0001/run1',
 'RUN_N_TIMES': 1,
 'SEED': None,
 'SOLVER': {'BASE_LR': 5.0,
            'BIAS_MULTIPLIER': 1.0,
            'DBG_TRAINABLE': False,
            'LOG_EVERY_N': 100,
            'LOSS': 'softmax',
            'LOSS_ALPHA': 0.01,
            'MOMENTUM': 0.9,
            'OPTIMIZER': 'sgd',
            'PATIENCE': 300,
            'SCHEDULER': 'cosine',
            'TOTAL_EPOCH': 100,
            'WARMUP_EPOCH': 10,
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_BIAS': 0}}
[09/17 05:10:45 visual_prompt]: Loading training data (final training data for vtab)...
2023-09-17 05:10:46.092933: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 05:10:46.269408: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-17 05:10:50.018941: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 05:10:50.019021: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 05:10:50.019029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-17 05:11:02.197983: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 05:11:02.198163: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/s1952889/miniconda3/envs/prompt/lib/python3.7/site-packages/cv2/../../lib64:
2023-09-17 05:11:02.198187: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[09/17 05:11:02 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset trainval...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
2023-09-17 05:11:02.218771: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split train[:800]+test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 05:11:04 visual_prompt]: Number of images: 1000
[09/17 05:11:04 visual_prompt]: Number of classes: 9 / 9
[09/17 05:11:04 visual_prompt]: Loading validation data...
[09/17 05:11:04 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset val...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[:200], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 05:11:04 visual_prompt]: Number of images: 200
[09/17 05:11:04 visual_prompt]: Number of classes: 9 / 9
[09/17 05:11:04 visual_prompt]: Loading test data...
[09/17 05:11:04 visual_prompt]: Constructing vtab-smallnorb(predicted_attribute="label_elevation") dataset test...
[INFO: dataset_info.py:  566]: Load dataset info from visual_prompt_tuning/data_path/smallnorb/2.0.0
[INFO: dataset_builder.py:  510]: Reusing dataset smallnorb (visual_prompt_tuning/data_path/smallnorb/2.0.0)
[INFO: logging_logger.py:   53]: Constructing tf.data.Dataset smallnorb for split test[50%:], from visual_prompt_tuning/data_path/smallnorb/2.0.0
[09/17 05:11:21 visual_prompt]: Number of images: 12150
[09/17 05:11:21 visual_prompt]: Number of classes: 9 / 9
[09/17 05:11:21 visual_prompt]: Constructing models...
[09/17 05:11:24 visual_prompt]: Total Parameters: 86727177	 Gradient Parameters: 928521
[09/17 05:11:24 visual_prompt]: tuned percent:1.071
[09/17 05:11:26 visual_prompt]: Device used for model: 0
[09/17 05:11:26 visual_prompt]: Setting up Evalutator...
[09/17 05:11:26 visual_prompt]: Setting up Trainer...
[09/17 05:11:26 visual_prompt]: 	Setting up the optimizer...
[09/17 05:11:26 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[09/17 05:11:37 visual_prompt]: Epoch 1 / 100: avg data time: 1.17e-01, avg batch time: 0.5945, average train loss: 2.8164
[09/17 05:11:39 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1417, average loss: 2.7292
[09/17 05:11:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 62.00	
[09/17 05:12:00 visual_prompt]: 	Test 100/190. loss: 2.780, 0.1973 s / batch. (data: 1.33e-02)max mem: 17.22448 GB 
[09/17 05:12:19 visual_prompt]: Inference (test):avg data time: 8.39e-03, avg batch time: 0.1921, average loss: 2.8016
[09/17 05:12:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.00	
[09/17 05:12:19 visual_prompt]: Best epoch 1: best metric: 0.095
[09/17 05:12:19 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.5
[09/17 05:12:28 visual_prompt]: Epoch 2 / 100: avg data time: 1.16e-01, avg batch time: 0.5156, average train loss: 3.9921
[09/17 05:12:31 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1420, average loss: 4.3862
[09/17 05:12:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 58.00	
[09/17 05:12:52 visual_prompt]: 	Test 100/190. loss: 4.864, 0.1827 s / batch. (data: 1.13e-04)max mem: 17.22448 GB 
[09/17 05:13:10 visual_prompt]: Inference (test):avg data time: 6.41e-03, avg batch time: 0.1922, average loss: 4.6224
[09/17 05:13:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.34	
[09/17 05:13:10 visual_prompt]: Best epoch 2: best metric: 0.100
[09/17 05:13:10 visual_prompt]: Training 3 / 100 epoch, with learning rate 1.0
[09/17 05:13:20 visual_prompt]: Epoch 3 / 100: avg data time: 1.06e-01, avg batch time: 0.5074, average train loss: 4.8402
[09/17 05:13:23 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.1426, average loss: 4.4157
[09/17 05:13:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/17 05:13:44 visual_prompt]: 	Test 100/190. loss: 4.996, 0.2021 s / batch. (data: 1.52e-02)max mem: 17.22448 GB 
[09/17 05:14:02 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1927, average loss: 4.7578
[09/17 05:14:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 55.34	
[09/17 05:14:02 visual_prompt]: Best epoch 3: best metric: 0.130
[09/17 05:14:02 visual_prompt]: Training 4 / 100 epoch, with learning rate 1.5
[09/17 05:14:11 visual_prompt]: Epoch 4 / 100: avg data time: 1.17e-01, avg batch time: 0.5176, average train loss: 3.4890
[09/17 05:14:14 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1426, average loss: 2.7551
[09/17 05:14:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 55.50	
[09/17 05:14:36 visual_prompt]: 	Test 100/190. loss: 2.828, 0.2102 s / batch. (data: 2.82e-02)max mem: 17.22448 GB 
[09/17 05:14:54 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1954, average loss: 2.8173
[09/17 05:14:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.63	
[09/17 05:14:54 visual_prompt]: Best epoch 4: best metric: 0.145
[09/17 05:14:54 visual_prompt]: Training 5 / 100 epoch, with learning rate 2.0
[09/17 05:15:03 visual_prompt]: Epoch 5 / 100: avg data time: 1.04e-01, avg batch time: 0.5041, average train loss: 3.0382
[09/17 05:15:06 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1468, average loss: 3.1542
[09/17 05:15:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 58.00	
[09/17 05:15:28 visual_prompt]: 	Test 100/190. loss: 3.121, 0.2120 s / batch. (data: 3.03e-02)max mem: 17.22448 GB 
[09/17 05:15:46 visual_prompt]: Inference (test):avg data time: 8.54e-03, avg batch time: 0.1942, average loss: 3.2290
[09/17 05:15:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.62	top5: 55.04	
[09/17 05:15:46 visual_prompt]: Training 6 / 100 epoch, with learning rate 2.5
[09/17 05:15:56 visual_prompt]: Epoch 6 / 100: avg data time: 1.05e-01, avg batch time: 0.5254, average train loss: 2.7987
[09/17 05:15:59 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1426, average loss: 2.6291
[09/17 05:15:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 52.00	
[09/17 05:16:20 visual_prompt]: 	Test 100/190. loss: 2.688, 0.1983 s / batch. (data: 1.26e-02)max mem: 17.22448 GB 
[09/17 05:16:39 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1935, average loss: 2.6272
[09/17 05:16:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.08	top5: 55.29	
[09/17 05:16:39 visual_prompt]: Training 7 / 100 epoch, with learning rate 3.0
[09/17 05:16:48 visual_prompt]: Epoch 7 / 100: avg data time: 1.07e-01, avg batch time: 0.5078, average train loss: 2.8984
[09/17 05:16:51 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1427, average loss: 3.2856
[09/17 05:16:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 55.50	
[09/17 05:17:12 visual_prompt]: 	Test 100/190. loss: 3.507, 0.1963 s / batch. (data: 1.34e-02)max mem: 17.22448 GB 
[09/17 05:17:30 visual_prompt]: Inference (test):avg data time: 6.69e-03, avg batch time: 0.1924, average loss: 3.3987
[09/17 05:17:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.94	top5: 55.37	
[09/17 05:17:30 visual_prompt]: Best epoch 7: best metric: 0.165
[09/17 05:17:30 visual_prompt]: Training 8 / 100 epoch, with learning rate 3.5
[09/17 05:17:39 visual_prompt]: Epoch 8 / 100: avg data time: 1.09e-01, avg batch time: 0.5116, average train loss: 3.5199
[09/17 05:17:43 visual_prompt]: Inference (val):avg data time: 3.40e-05, avg batch time: 0.1451, average loss: 2.6966
[09/17 05:17:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/17 05:18:04 visual_prompt]: 	Test 100/190. loss: 2.870, 0.1857 s / batch. (data: 2.55e-04)max mem: 17.22448 GB 
[09/17 05:18:22 visual_prompt]: Inference (test):avg data time: 6.70e-03, avg batch time: 0.1942, average loss: 2.7428
[09/17 05:18:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 57.30	
[09/17 05:18:22 visual_prompt]: Training 9 / 100 epoch, with learning rate 4.0
[09/17 05:18:32 visual_prompt]: Epoch 9 / 100: avg data time: 1.12e-01, avg batch time: 0.5132, average train loss: 3.0541
[09/17 05:18:35 visual_prompt]: Inference (val):avg data time: 3.76e-05, avg batch time: 0.1433, average loss: 4.1054
[09/17 05:18:35 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.50	
[09/17 05:18:56 visual_prompt]: 	Test 100/190. loss: 3.891, 0.1959 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 05:19:14 visual_prompt]: Inference (test):avg data time: 7.89e-03, avg batch time: 0.1933, average loss: 3.9702
[09/17 05:19:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 57.39	
[09/17 05:19:14 visual_prompt]: Training 10 / 100 epoch, with learning rate 4.5
[09/17 05:19:23 visual_prompt]: Epoch 10 / 100: avg data time: 1.06e-01, avg batch time: 0.5066, average train loss: 4.3009
[09/17 05:19:27 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1427, average loss: 3.6770
[09/17 05:19:27 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 54.00	
[09/17 05:19:48 visual_prompt]: 	Test 100/190. loss: 3.776, 0.1823 s / batch. (data: 9.35e-05)max mem: 17.22448 GB 
[09/17 05:20:06 visual_prompt]: Inference (test):avg data time: 8.02e-03, avg batch time: 0.1937, average loss: 3.5018
[09/17 05:20:07 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 55.79	
[09/17 05:20:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 5.0
[09/17 05:20:16 visual_prompt]: Epoch 11 / 100: avg data time: 1.12e-01, avg batch time: 0.5132, average train loss: 3.6501
[09/17 05:20:19 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 3.5432
[09/17 05:20:19 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/17 05:20:40 visual_prompt]: 	Test 100/190. loss: 3.379, 0.1990 s / batch. (data: 1.72e-02)max mem: 17.22448 GB 
[09/17 05:20:58 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1927, average loss: 3.3497
[09/17 05:20:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 58.83	
[09/17 05:20:58 visual_prompt]: Training 12 / 100 epoch, with learning rate 4.998477067547739
[09/17 05:21:08 visual_prompt]: Epoch 12 / 100: avg data time: 1.11e-01, avg batch time: 0.5108, average train loss: 3.3496
[09/17 05:21:11 visual_prompt]: Inference (val):avg data time: 2.48e-05, avg batch time: 0.1425, average loss: 2.5547
[09/17 05:21:11 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 61.50	
[09/17 05:21:32 visual_prompt]: 	Test 100/190. loss: 2.527, 0.1923 s / batch. (data: 1.03e-02)max mem: 17.22448 GB 
[09/17 05:21:50 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1937, average loss: 2.6120
[09/17 05:21:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 56.94	
[09/17 05:21:50 visual_prompt]: Training 13 / 100 epoch, with learning rate 4.993910125649561
[09/17 05:21:59 visual_prompt]: Epoch 13 / 100: avg data time: 1.08e-01, avg batch time: 0.5105, average train loss: 3.1978
[09/17 05:22:03 visual_prompt]: Inference (val):avg data time: 3.78e-05, avg batch time: 0.1429, average loss: 4.2671
[09/17 05:22:03 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 63.00	
[09/17 05:22:24 visual_prompt]: 	Test 100/190. loss: 4.321, 0.1835 s / batch. (data: 3.12e-04)max mem: 17.22448 GB 
[09/17 05:22:42 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1925, average loss: 4.3473
[09/17 05:22:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 56.35	
[09/17 05:22:42 visual_prompt]: Training 14 / 100 epoch, with learning rate 4.986304738420683
[09/17 05:22:51 visual_prompt]: Epoch 14 / 100: avg data time: 9.54e-02, avg batch time: 0.5007, average train loss: 3.2346
[09/17 05:22:54 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1427, average loss: 2.7508
[09/17 05:22:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 59.00	
[09/17 05:23:15 visual_prompt]: 	Test 100/190. loss: 2.731, 0.1974 s / batch. (data: 1.59e-02)max mem: 17.22448 GB 
[09/17 05:23:34 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1941, average loss: 2.6902
[09/17 05:23:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 60.84	
[09/17 05:23:34 visual_prompt]: Training 15 / 100 epoch, with learning rate 4.975670171853926
[09/17 05:23:43 visual_prompt]: Epoch 15 / 100: avg data time: 1.08e-01, avg batch time: 0.5103, average train loss: 2.8358
[09/17 05:23:46 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.1428, average loss: 2.8307
[09/17 05:23:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 59.00	
[09/17 05:24:07 visual_prompt]: 	Test 100/190. loss: 2.425, 0.2267 s / batch. (data: 3.49e-02)max mem: 17.22448 GB 
[09/17 05:24:27 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1971, average loss: 2.8057
[09/17 05:24:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.35	top5: 56.31	
[09/17 05:24:27 visual_prompt]: Training 16 / 100 epoch, with learning rate 4.962019382530521
[09/17 05:24:36 visual_prompt]: Epoch 16 / 100: avg data time: 1.07e-01, avg batch time: 0.5083, average train loss: 2.6337
[09/17 05:24:39 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1426, average loss: 2.1174
[09/17 05:24:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 77.50	
[09/17 05:25:00 visual_prompt]: 	Test 100/190. loss: 2.144, 0.1822 s / batch. (data: 1.24e-04)max mem: 17.22448 GB 
[09/17 05:25:19 visual_prompt]: Inference (test):avg data time: 6.25e-03, avg batch time: 0.1936, average loss: 2.1730
[09/17 05:25:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.57	top5: 73.96	
[09/17 05:25:19 visual_prompt]: Best epoch 16: best metric: 0.200
[09/17 05:25:19 visual_prompt]: Training 17 / 100 epoch, with learning rate 4.945369001834514
[09/17 05:25:28 visual_prompt]: Epoch 17 / 100: avg data time: 9.79e-02, avg batch time: 0.5042, average train loss: 2.7626
[09/17 05:25:31 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1427, average loss: 2.9067
[09/17 05:25:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/17 05:25:52 visual_prompt]: 	Test 100/190. loss: 3.077, 0.1822 s / batch. (data: 1.70e-04)max mem: 17.22448 GB 
[09/17 05:26:11 visual_prompt]: Inference (test):avg data time: 6.97e-03, avg batch time: 0.1968, average loss: 2.9253
[09/17 05:26:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.09	
[09/17 05:26:11 visual_prompt]: Training 18 / 100 epoch, with learning rate 4.925739315689991
[09/17 05:26:20 visual_prompt]: Epoch 18 / 100: avg data time: 9.88e-02, avg batch time: 0.5086, average train loss: 2.8156
[09/17 05:26:23 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.1430, average loss: 2.8868
[09/17 05:26:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 58.00	
[09/17 05:26:44 visual_prompt]: 	Test 100/190. loss: 3.052, 0.2056 s / batch. (data: 1.30e-02)max mem: 17.22448 GB 
[09/17 05:27:03 visual_prompt]: Inference (test):avg data time: 7.64e-03, avg batch time: 0.1937, average loss: 2.9369
[09/17 05:27:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.04	
[09/17 05:27:03 visual_prompt]: Training 19 / 100 epoch, with learning rate 4.903154239845797
[09/17 05:27:12 visual_prompt]: Epoch 19 / 100: avg data time: 1.11e-01, avg batch time: 0.5115, average train loss: 2.7085
[09/17 05:27:15 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1429, average loss: 2.3670
[09/17 05:27:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 63.00	
[09/17 05:27:36 visual_prompt]: 	Test 100/190. loss: 2.424, 0.1835 s / batch. (data: 1.57e-04)max mem: 17.22448 GB 
[09/17 05:27:55 visual_prompt]: Inference (test):avg data time: 7.45e-03, avg batch time: 0.1933, average loss: 2.3634
[09/17 05:27:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 60.02	
[09/17 05:27:55 visual_prompt]: Training 20 / 100 epoch, with learning rate 4.877641290737884
[09/17 05:28:04 visual_prompt]: Epoch 20 / 100: avg data time: 1.10e-01, avg batch time: 0.5106, average train loss: 2.4299
[09/17 05:28:07 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1447, average loss: 2.9526
[09/17 05:28:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 64.50	
[09/17 05:28:28 visual_prompt]: 	Test 100/190. loss: 2.801, 0.1883 s / batch. (data: 5.90e-03)max mem: 17.22448 GB 
[09/17 05:28:47 visual_prompt]: Inference (test):avg data time: 8.44e-03, avg batch time: 0.1936, average loss: 2.9671
[09/17 05:28:47 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 68.00	
[09/17 05:28:47 visual_prompt]: Training 21 / 100 epoch, with learning rate 4.849231551964771
[09/17 05:28:56 visual_prompt]: Epoch 21 / 100: avg data time: 9.97e-02, avg batch time: 0.5004, average train loss: 2.4881
[09/17 05:28:59 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1457, average loss: 2.5228
[09/17 05:28:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.00	top5: 67.00	
[09/17 05:29:20 visual_prompt]: 	Test 100/190. loss: 2.747, 0.2063 s / batch. (data: 2.16e-02)max mem: 17.22448 GB 
[09/17 05:29:38 visual_prompt]: Inference (test):avg data time: 8.42e-03, avg batch time: 0.1933, average loss: 2.7144
[09/17 05:29:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.08	top5: 63.19	
[09/17 05:29:38 visual_prompt]: Training 22 / 100 epoch, with learning rate 4.817959636416969
[09/17 05:29:48 visual_prompt]: Epoch 22 / 100: avg data time: 1.06e-01, avg batch time: 0.5309, average train loss: 2.6453
[09/17 05:29:51 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1427, average loss: 2.5305
[09/17 05:29:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 75.00	
[09/17 05:30:12 visual_prompt]: 	Test 100/190. loss: 2.537, 0.1825 s / batch. (data: 1.18e-04)max mem: 17.22448 GB 
[09/17 05:30:30 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1931, average loss: 2.6662
[09/17 05:30:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.36	top5: 70.57	
[09/17 05:30:30 visual_prompt]: Training 23 / 100 epoch, with learning rate 4.783863644106502
[09/17 05:30:39 visual_prompt]: Epoch 23 / 100: avg data time: 1.00e-01, avg batch time: 0.5043, average train loss: 2.3542
[09/17 05:30:42 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1427, average loss: 1.9955
[09/17 05:30:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 84.00	
[09/17 05:31:04 visual_prompt]: 	Test 100/190. loss: 2.047, 0.1978 s / batch. (data: 1.53e-02)max mem: 17.22448 GB 
[09/17 05:31:22 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1933, average loss: 2.0851
[09/17 05:31:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.39	top5: 79.19	
[09/17 05:31:22 visual_prompt]: Best epoch 23: best metric: 0.250
[09/17 05:31:22 visual_prompt]: Training 24 / 100 epoch, with learning rate 4.7469851157479175
[09/17 05:31:31 visual_prompt]: Epoch 24 / 100: avg data time: 8.73e-02, avg batch time: 0.4914, average train loss: 2.1536
[09/17 05:31:34 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.1428, average loss: 2.1096
[09/17 05:31:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.50	top5: 79.00	
[09/17 05:31:55 visual_prompt]: 	Test 100/190. loss: 2.119, 0.1939 s / batch. (data: 1.13e-02)max mem: 17.22448 GB 
[09/17 05:32:14 visual_prompt]: Inference (test):avg data time: 8.81e-03, avg batch time: 0.1942, average loss: 2.1402
[09/17 05:32:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.12	top5: 76.50	
[09/17 05:32:14 visual_prompt]: Training 25 / 100 epoch, with learning rate 4.707368982147317
[09/17 05:32:23 visual_prompt]: Epoch 25 / 100: avg data time: 1.12e-01, avg batch time: 0.5128, average train loss: 2.1253
[09/17 05:32:26 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.1425, average loss: 2.8277
[09/17 05:32:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 61.50	
[09/17 05:32:47 visual_prompt]: 	Test 100/190. loss: 2.509, 0.1972 s / batch. (data: 1.52e-02)max mem: 17.22448 GB 
[09/17 05:33:06 visual_prompt]: Inference (test):avg data time: 7.16e-03, avg batch time: 0.1928, average loss: 2.7611
[09/17 05:33:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.21	top5: 64.80	
[09/17 05:33:06 visual_prompt]: Training 26 / 100 epoch, with learning rate 4.665063509461097
[09/17 05:33:15 visual_prompt]: Epoch 26 / 100: avg data time: 1.07e-01, avg batch time: 0.5077, average train loss: 2.4439
[09/17 05:33:18 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1428, average loss: 2.5726
[09/17 05:33:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.50	top5: 70.00	
[09/17 05:33:39 visual_prompt]: 	Test 100/190. loss: 2.598, 0.1934 s / batch. (data: 1.13e-02)max mem: 17.22448 GB 
[09/17 05:33:58 visual_prompt]: Inference (test):avg data time: 7.99e-03, avg batch time: 0.1932, average loss: 2.6640
[09/17 05:33:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.86	top5: 70.21	
[09/17 05:33:58 visual_prompt]: Training 27 / 100 epoch, with learning rate 4.620120240391064
[09/17 05:34:07 visual_prompt]: Epoch 27 / 100: avg data time: 1.07e-01, avg batch time: 0.5086, average train loss: 2.1957
[09/17 05:34:10 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1427, average loss: 2.0978
[09/17 05:34:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.50	top5: 83.50	
[09/17 05:34:32 visual_prompt]: 	Test 100/190. loss: 2.083, 0.1824 s / batch. (data: 1.29e-04)max mem: 17.22448 GB 
[09/17 05:34:50 visual_prompt]: Inference (test):avg data time: 7.41e-03, avg batch time: 0.1934, average loss: 2.0987
[09/17 05:34:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.94	top5: 81.05	
[09/17 05:34:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 4.572593931387604
[09/17 05:34:59 visual_prompt]: Epoch 28 / 100: avg data time: 1.00e-01, avg batch time: 0.5045, average train loss: 2.1701
[09/17 05:35:02 visual_prompt]: Inference (val):avg data time: 4.73e-05, avg batch time: 0.1427, average loss: 2.4168
[09/17 05:35:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 70.50	
[09/17 05:35:23 visual_prompt]: 	Test 100/190. loss: 2.622, 0.1835 s / batch. (data: 1.19e-04)max mem: 17.22448 GB 
[09/17 05:35:42 visual_prompt]: Inference (test):avg data time: 8.28e-03, avg batch time: 0.1938, average loss: 2.5560
[09/17 05:35:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.33	top5: 68.40	
[09/17 05:35:42 visual_prompt]: Training 29 / 100 epoch, with learning rate 4.522542485937368
[09/17 05:35:51 visual_prompt]: Epoch 29 / 100: avg data time: 1.02e-01, avg batch time: 0.5060, average train loss: 2.1801
[09/17 05:35:54 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.1427, average loss: 2.1422
[09/17 05:35:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 79.50	
[09/17 05:36:15 visual_prompt]: 	Test 100/190. loss: 2.255, 0.2244 s / batch. (data: 4.25e-02)max mem: 17.22448 GB 
[09/17 05:36:34 visual_prompt]: Inference (test):avg data time: 7.73e-03, avg batch time: 0.1932, average loss: 2.2880
[09/17 05:36:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.22	top5: 77.18	
[09/17 05:36:34 visual_prompt]: Training 30 / 100 epoch, with learning rate 4.4700268840168045
[09/17 05:36:43 visual_prompt]: Epoch 30 / 100: avg data time: 1.06e-01, avg batch time: 0.5069, average train loss: 2.2644
[09/17 05:36:46 visual_prompt]: Inference (val):avg data time: 2.47e-05, avg batch time: 0.1428, average loss: 2.0430
[09/17 05:36:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 86.00	
[09/17 05:37:07 visual_prompt]: 	Test 100/190. loss: 2.109, 0.1876 s / batch. (data: 3.24e-05)max mem: 17.22448 GB 
[09/17 05:37:26 visual_prompt]: Inference (test):avg data time: 7.54e-03, avg batch time: 0.1931, average loss: 2.1955
[09/17 05:37:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.34	top5: 80.02	
[09/17 05:37:26 visual_prompt]: Training 31 / 100 epoch, with learning rate 4.415111107797445
[09/17 05:37:35 visual_prompt]: Epoch 31 / 100: avg data time: 9.27e-02, avg batch time: 0.4960, average train loss: 2.2056
[09/17 05:37:38 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1428, average loss: 2.2707
[09/17 05:37:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 83.50	
[09/17 05:37:59 visual_prompt]: 	Test 100/190. loss: 2.400, 0.1959 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 05:38:18 visual_prompt]: Inference (test):avg data time: 7.22e-03, avg batch time: 0.1950, average loss: 2.3517
[09/17 05:38:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.68	top5: 78.97	
[09/17 05:38:18 visual_prompt]: Training 32 / 100 epoch, with learning rate 4.357862063693486
[09/17 05:38:27 visual_prompt]: Epoch 32 / 100: avg data time: 9.48e-02, avg batch time: 0.4971, average train loss: 2.5260
[09/17 05:38:30 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.1427, average loss: 2.3194
[09/17 05:38:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 62.50	
[09/17 05:38:51 visual_prompt]: 	Test 100/190. loss: 2.187, 0.1941 s / batch. (data: 8.92e-03)max mem: 17.22448 GB 
[09/17 05:39:09 visual_prompt]: Inference (test):avg data time: 6.49e-03, avg batch time: 0.1931, average loss: 2.2972
[09/17 05:39:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.42	top5: 64.88	
[09/17 05:39:09 visual_prompt]: Training 33 / 100 epoch, with learning rate 4.298349500846628
[09/17 05:39:18 visual_prompt]: Epoch 33 / 100: avg data time: 1.07e-01, avg batch time: 0.5069, average train loss: 4.0503
[09/17 05:39:22 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1424, average loss: 9.1683
[09/17 05:39:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 55.50	
[09/17 05:39:43 visual_prompt]: 	Test 100/190. loss: 11.193, 0.2042 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/17 05:40:02 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1948, average loss: 11.5347
[09/17 05:40:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.76	
[09/17 05:40:02 visual_prompt]: Training 34 / 100 epoch, with learning rate 4.236645926147493
[09/17 05:40:11 visual_prompt]: Epoch 34 / 100: avg data time: 1.04e-01, avg batch time: 0.5046, average train loss: 17.0183
[09/17 05:40:14 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1425, average loss: 26.5361
[09/17 05:40:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 63.50	
[09/17 05:40:35 visual_prompt]: 	Test 100/190. loss: 28.455, 0.1828 s / batch. (data: 1.33e-04)max mem: 17.22448 GB 
[09/17 05:40:54 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1958, average loss: 29.7520
[09/17 05:40:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 56.12	
[09/17 05:40:54 visual_prompt]: Training 35 / 100 epoch, with learning rate 4.172826515897146
[09/17 05:41:03 visual_prompt]: Epoch 35 / 100: avg data time: 1.12e-01, avg batch time: 0.5127, average train loss: 26.4690
[09/17 05:41:06 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.1438, average loss: 13.4134
[09/17 05:41:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/17 05:41:27 visual_prompt]: 	Test 100/190. loss: 14.479, 0.1836 s / batch. (data: 1.54e-04)max mem: 17.22448 GB 
[09/17 05:41:45 visual_prompt]: Inference (test):avg data time: 7.42e-03, avg batch time: 0.1925, average loss: 14.6573
[09/17 05:41:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.21	top5: 55.79	
[09/17 05:41:45 visual_prompt]: Training 36 / 100 epoch, with learning rate 4.106969024216348
[09/17 05:41:55 visual_prompt]: Epoch 36 / 100: avg data time: 1.11e-01, avg batch time: 0.5141, average train loss: 17.8282
[09/17 05:41:58 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.1427, average loss: 21.3226
[09/17 05:41:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 55.50	
[09/17 05:42:19 visual_prompt]: 	Test 100/190. loss: 20.790, 0.2009 s / batch. (data: 1.02e-02)max mem: 17.22448 GB 
[09/17 05:42:37 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1932, average loss: 20.7972
[09/17 05:42:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 55.60	
[09/17 05:42:37 visual_prompt]: Training 37 / 100 epoch, with learning rate 4.039153688314146
[09/17 05:42:47 visual_prompt]: Epoch 37 / 100: avg data time: 1.09e-01, avg batch time: 0.5120, average train loss: 14.3313
[09/17 05:42:50 visual_prompt]: Inference (val):avg data time: 3.18e-05, avg batch time: 0.1425, average loss: 7.1466
[09/17 05:42:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 54.00	
[09/17 05:43:11 visual_prompt]: 	Test 100/190. loss: 6.700, 0.2056 s / batch. (data: 2.09e-02)max mem: 17.22448 GB 
[09/17 05:43:29 visual_prompt]: Inference (test):avg data time: 6.22e-03, avg batch time: 0.1918, average loss: 7.0505
[09/17 05:43:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 55.37	
[09/17 05:43:29 visual_prompt]: Training 38 / 100 epoch, with learning rate 3.969463130731183
[09/17 05:43:38 visual_prompt]: Epoch 38 / 100: avg data time: 9.96e-02, avg batch time: 0.5074, average train loss: 9.8001
[09/17 05:43:41 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1425, average loss: 15.1085
[09/17 05:43:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 51.50	
[09/17 05:44:03 visual_prompt]: 	Test 100/190. loss: 15.429, 0.2387 s / batch. (data: 1.08e-02)max mem: 17.22448 GB 
[09/17 05:44:21 visual_prompt]: Inference (test):avg data time: 8.97e-03, avg batch time: 0.1946, average loss: 14.1488
[09/17 05:44:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 55.77	
[09/17 05:44:21 visual_prompt]: Training 39 / 100 epoch, with learning rate 3.897982258676867
[09/17 05:44:30 visual_prompt]: Epoch 39 / 100: avg data time: 1.09e-01, avg batch time: 0.5111, average train loss: 10.5693
[09/17 05:44:33 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.1426, average loss: 11.0104
[09/17 05:44:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 54.50	
[09/17 05:44:55 visual_prompt]: 	Test 100/190. loss: 12.245, 0.1855 s / batch. (data: 1.61e-04)max mem: 17.22448 GB 
[09/17 05:45:13 visual_prompt]: Inference (test):avg data time: 7.81e-03, avg batch time: 0.1929, average loss: 10.9320
[09/17 05:45:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.14	top5: 55.08	
[09/17 05:45:13 visual_prompt]: Training 40 / 100 epoch, with learning rate 3.824798160583012
[09/17 05:45:22 visual_prompt]: Epoch 40 / 100: avg data time: 1.12e-01, avg batch time: 0.5134, average train loss: 10.1369
[09/17 05:45:25 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1427, average loss: 10.2022
[09/17 05:45:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/17 05:45:46 visual_prompt]: 	Test 100/190. loss: 10.339, 0.1826 s / batch. (data: 9.68e-05)max mem: 17.22448 GB 
[09/17 05:46:06 visual_prompt]: Inference (test):avg data time: 7.40e-03, avg batch time: 0.1964, average loss: 10.2561
[09/17 05:46:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.04	top5: 55.97	
[09/17 05:46:06 visual_prompt]: Training 41 / 100 epoch, with learning rate 3.75
[09/17 05:46:15 visual_prompt]: Epoch 41 / 100: avg data time: 1.09e-01, avg batch time: 0.5090, average train loss: 9.6873
[09/17 05:46:18 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.1426, average loss: 5.1963
[09/17 05:46:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 62.50	
[09/17 05:46:39 visual_prompt]: 	Test 100/190. loss: 5.965, 0.2082 s / batch. (data: 2.65e-02)max mem: 17.22448 GB 
[09/17 05:46:57 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1924, average loss: 5.4271
[09/17 05:46:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 55.71	
[09/17 05:46:58 visual_prompt]: Training 42 / 100 epoch, with learning rate 3.673678906964727
[09/17 05:47:07 visual_prompt]: Epoch 42 / 100: avg data time: 1.13e-01, avg batch time: 0.5151, average train loss: 8.2185
[09/17 05:47:10 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.1443, average loss: 10.1101
[09/17 05:47:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 54.00	
[09/17 05:47:31 visual_prompt]: 	Test 100/190. loss: 9.772, 0.1820 s / batch. (data: 1.18e-04)max mem: 17.22448 GB 
[09/17 05:47:50 visual_prompt]: Inference (test):avg data time: 8.19e-03, avg batch time: 0.1966, average loss: 10.0816
[09/17 05:47:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 55.51	
[09/17 05:47:50 visual_prompt]: Training 43 / 100 epoch, with learning rate 3.5959278669726933
[09/17 05:47:59 visual_prompt]: Epoch 43 / 100: avg data time: 9.35e-02, avg batch time: 0.5122, average train loss: 6.2302
[09/17 05:48:02 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1427, average loss: 4.2214
[09/17 05:48:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 9.50	top5: 58.00	
[09/17 05:48:24 visual_prompt]: 	Test 100/190. loss: 4.242, 0.1966 s / batch. (data: 1.50e-02)max mem: 17.22448 GB 
[09/17 05:48:42 visual_prompt]: Inference (test):avg data time: 7.91e-03, avg batch time: 0.1935, average loss: 4.4822
[09/17 05:48:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.97	top5: 54.39	
[09/17 05:48:42 visual_prompt]: Training 44 / 100 epoch, with learning rate 3.516841607689501
[09/17 05:48:51 visual_prompt]: Epoch 44 / 100: avg data time: 1.03e-01, avg batch time: 0.5064, average train loss: 4.3196
[09/17 05:48:54 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1427, average loss: 3.4148
[09/17 05:48:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.50	
[09/17 05:49:16 visual_prompt]: 	Test 100/190. loss: 3.319, 0.1821 s / batch. (data: 3.65e-05)max mem: 17.22448 GB 
[09/17 05:49:35 visual_prompt]: Inference (test):avg data time: 7.63e-03, avg batch time: 0.1957, average loss: 3.3150
[09/17 05:49:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.35	
[09/17 05:49:35 visual_prompt]: Training 45 / 100 epoch, with learning rate 3.4365164835397803
[09/17 05:49:44 visual_prompt]: Epoch 45 / 100: avg data time: 1.12e-01, avg batch time: 0.5156, average train loss: 4.0026
[09/17 05:49:47 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1427, average loss: 5.3554
[09/17 05:49:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 7.00	top5: 53.50	
[09/17 05:50:08 visual_prompt]: 	Test 100/190. loss: 4.764, 0.1973 s / batch. (data: 1.50e-02)max mem: 17.22448 GB 
[09/17 05:50:27 visual_prompt]: Inference (test):avg data time: 7.88e-03, avg batch time: 0.1932, average loss: 4.8534
[09/17 05:50:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.40	top5: 55.87	
[09/17 05:50:27 visual_prompt]: Training 46 / 100 epoch, with learning rate 3.3550503583141724
[09/17 05:50:36 visual_prompt]: Epoch 46 / 100: avg data time: 1.12e-01, avg batch time: 0.5165, average train loss: 3.6940
[09/17 05:50:39 visual_prompt]: Inference (val):avg data time: 2.56e-05, avg batch time: 0.1428, average loss: 2.5847
[09/17 05:50:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 52.00	
[09/17 05:51:00 visual_prompt]: 	Test 100/190. loss: 2.510, 0.2145 s / batch. (data: 3.26e-02)max mem: 17.22448 GB 
[09/17 05:51:19 visual_prompt]: Inference (test):avg data time: 8.74e-03, avg batch time: 0.1939, average loss: 2.5630
[09/17 05:51:19 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.38	
[09/17 05:51:19 visual_prompt]: Training 47 / 100 epoch, with learning rate 3.2725424859373686
[09/17 05:51:28 visual_prompt]: Epoch 47 / 100: avg data time: 1.11e-01, avg batch time: 0.5140, average train loss: 2.7870
[09/17 05:51:31 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.1438, average loss: 2.5341
[09/17 05:51:31 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.00	top5: 52.00	
[09/17 05:51:52 visual_prompt]: 	Test 100/190. loss: 2.532, 0.1848 s / batch. (data: 1.24e-04)max mem: 17.22448 GB 
[09/17 05:52:11 visual_prompt]: Inference (test):avg data time: 8.25e-03, avg batch time: 0.1937, average loss: 2.4956
[09/17 05:52:11 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.23	top5: 55.52	
[09/17 05:52:11 visual_prompt]: Training 48 / 100 epoch, with learning rate 3.1890933895424975
[09/17 05:52:20 visual_prompt]: Epoch 48 / 100: avg data time: 1.11e-01, avg batch time: 0.5113, average train loss: 2.7053
[09/17 05:52:23 visual_prompt]: Inference (val):avg data time: 2.50e-05, avg batch time: 0.1428, average loss: 2.6177
[09/17 05:52:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 59.00	
[09/17 05:52:44 visual_prompt]: 	Test 100/190. loss: 2.824, 0.2049 s / batch. (data: 2.25e-02)max mem: 17.22448 GB 
[09/17 05:53:03 visual_prompt]: Inference (test):avg data time: 7.69e-03, avg batch time: 0.1934, average loss: 2.8413
[09/17 05:53:03 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.75	top5: 55.84	
[09/17 05:53:03 visual_prompt]: Training 49 / 100 epoch, with learning rate 3.104804738999169
[09/17 05:53:12 visual_prompt]: Epoch 49 / 100: avg data time: 1.02e-01, avg batch time: 0.5083, average train loss: 2.6447
[09/17 05:53:15 visual_prompt]: Inference (val):avg data time: 4.46e-05, avg batch time: 0.1429, average loss: 2.9306
[09/17 05:53:15 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 56.50	
[09/17 05:53:36 visual_prompt]: 	Test 100/190. loss: 3.027, 0.1921 s / batch. (data: 9.27e-05)max mem: 17.22448 GB 
[09/17 05:53:55 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1941, average loss: 2.8889
[09/17 05:53:55 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.05	top5: 57.07	
[09/17 05:53:55 visual_prompt]: Training 50 / 100 epoch, with learning rate 3.019779227044398
[09/17 05:54:04 visual_prompt]: Epoch 50 / 100: avg data time: 1.04e-01, avg batch time: 0.5060, average train loss: 2.5620
[09/17 05:54:07 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.1427, average loss: 2.7974
[09/17 05:54:07 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.00	top5: 53.00	
[09/17 05:54:28 visual_prompt]: 	Test 100/190. loss: 2.632, 0.1973 s / batch. (data: 1.53e-02)max mem: 17.22448 GB 
[09/17 05:54:46 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1933, average loss: 2.6709
[09/17 05:54:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 10.71	top5: 55.77	
[09/17 05:54:46 visual_prompt]: Training 51 / 100 epoch, with learning rate 2.934120444167326
[09/17 05:54:55 visual_prompt]: Epoch 51 / 100: avg data time: 1.08e-01, avg batch time: 0.5104, average train loss: 2.4258
[09/17 05:54:59 visual_prompt]: Inference (val):avg data time: 4.30e-05, avg batch time: 0.1429, average loss: 2.3852
[09/17 05:54:59 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.00	top5: 59.00	
[09/17 05:55:20 visual_prompt]: 	Test 100/190. loss: 2.441, 0.1964 s / batch. (data: 1.57e-04)max mem: 17.22448 GB 
[09/17 05:55:39 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1948, average loss: 2.4161
[09/17 05:55:39 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.58	top5: 57.14	
[09/17 05:55:39 visual_prompt]: Training 52 / 100 epoch, with learning rate 2.8479327524001636
[09/17 05:55:48 visual_prompt]: Epoch 52 / 100: avg data time: 1.15e-01, avg batch time: 0.5387, average train loss: 2.4482
[09/17 05:55:51 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1427, average loss: 2.4739
[09/17 05:55:51 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.00	top5: 55.50	
[09/17 05:56:12 visual_prompt]: 	Test 100/190. loss: 2.392, 0.2101 s / batch. (data: 2.85e-02)max mem: 17.22448 GB 
[09/17 05:56:31 visual_prompt]: Inference (test):avg data time: 7.30e-03, avg batch time: 0.1928, average loss: 2.4780
[09/17 05:56:31 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 11.22	top5: 61.11	
[09/17 05:56:31 visual_prompt]: Training 53 / 100 epoch, with learning rate 2.761321158169134
[09/17 05:56:40 visual_prompt]: Epoch 53 / 100: avg data time: 9.34e-02, avg batch time: 0.4975, average train loss: 2.3449
[09/17 05:56:43 visual_prompt]: Inference (val):avg data time: 3.15e-05, avg batch time: 0.1427, average loss: 2.4373
[09/17 05:56:43 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.00	top5: 54.00	
[09/17 05:57:04 visual_prompt]: 	Test 100/190. loss: 2.428, 0.1826 s / batch. (data: 1.23e-04)max mem: 17.22448 GB 
[09/17 05:57:22 visual_prompt]: Inference (test):avg data time: 7.05e-03, avg batch time: 0.1922, average loss: 2.4584
[09/17 05:57:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.68	top5: 55.59	
[09/17 05:57:22 visual_prompt]: Training 54 / 100 epoch, with learning rate 2.6743911843603128
[09/17 05:57:31 visual_prompt]: Epoch 54 / 100: avg data time: 1.01e-01, avg batch time: 0.5028, average train loss: 2.5366
[09/17 05:57:34 visual_prompt]: Inference (val):avg data time: 3.87e-05, avg batch time: 0.1452, average loss: 2.3498
[09/17 05:57:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.50	top5: 61.00	
[09/17 05:57:56 visual_prompt]: 	Test 100/190. loss: 2.407, 0.1961 s / batch. (data: 1.44e-02)max mem: 17.22448 GB 
[09/17 05:58:14 visual_prompt]: Inference (test):avg data time: 7.96e-03, avg batch time: 0.1955, average loss: 2.3479
[09/17 05:58:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 16.54	top5: 63.74	
[09/17 05:58:14 visual_prompt]: Training 55 / 100 epoch, with learning rate 2.5872487417562526
[09/17 05:58:23 visual_prompt]: Epoch 55 / 100: avg data time: 1.06e-01, avg batch time: 0.5065, average train loss: 2.2954
[09/17 05:58:26 visual_prompt]: Inference (val):avg data time: 2.53e-05, avg batch time: 0.1427, average loss: 2.5117
[09/17 05:58:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.50	top5: 54.00	
[09/17 05:58:48 visual_prompt]: 	Test 100/190. loss: 2.312, 0.1815 s / batch. (data: 3.89e-05)max mem: 17.22448 GB 
[09/17 05:59:06 visual_prompt]: Inference (test):avg data time: 8.01e-03, avg batch time: 0.1929, average loss: 2.4344
[09/17 05:59:06 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 14.63	top5: 61.83	
[09/17 05:59:06 visual_prompt]: Training 56 / 100 epoch, with learning rate 2.5
[09/17 05:59:15 visual_prompt]: Epoch 56 / 100: avg data time: 1.05e-01, avg batch time: 0.5065, average train loss: 2.2247
[09/17 05:59:18 visual_prompt]: Inference (val):avg data time: 2.38e-05, avg batch time: 0.1427, average loss: 2.2737
[09/17 05:59:18 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.00	top5: 71.50	
[09/17 05:59:40 visual_prompt]: 	Test 100/190. loss: 2.244, 0.1945 s / batch. (data: 1.26e-02)max mem: 17.22448 GB 
[09/17 05:59:58 visual_prompt]: Inference (test):avg data time: 7.53e-03, avg batch time: 0.1929, average loss: 2.2563
[09/17 05:59:58 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 17.78	top5: 75.01	
[09/17 05:59:58 visual_prompt]: Training 57 / 100 epoch, with learning rate 2.4127512582437483
[09/17 06:00:07 visual_prompt]: Epoch 57 / 100: avg data time: 1.10e-01, avg batch time: 0.5118, average train loss: 2.1257
[09/17 06:00:10 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.1427, average loss: 2.0065
[09/17 06:00:10 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.50	top5: 76.50	
[09/17 06:00:31 visual_prompt]: 	Test 100/190. loss: 1.888, 0.2066 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/17 06:00:50 visual_prompt]: Inference (test):avg data time: 7.29e-03, avg batch time: 0.1943, average loss: 2.0519
[09/17 06:00:50 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.92	top5: 75.04	
[09/17 06:00:50 visual_prompt]: Training 58 / 100 epoch, with learning rate 2.325608815639687
[09/17 06:00:59 visual_prompt]: Epoch 58 / 100: avg data time: 1.13e-01, avg batch time: 0.5165, average train loss: 2.1206
[09/17 06:01:02 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.1426, average loss: 2.2595
[09/17 06:01:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.00	top5: 66.50	
[09/17 06:01:24 visual_prompt]: 	Test 100/190. loss: 2.011, 0.2213 s / batch. (data: 2.86e-02)max mem: 17.22448 GB 
[09/17 06:01:43 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1976, average loss: 2.2412
[09/17 06:01:43 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 15.18	top5: 69.05	
[09/17 06:01:43 visual_prompt]: Training 59 / 100 epoch, with learning rate 2.2386788418308665
[09/17 06:01:52 visual_prompt]: Epoch 59 / 100: avg data time: 1.11e-01, avg batch time: 0.5145, average train loss: 2.0979
[09/17 06:01:55 visual_prompt]: Inference (val):avg data time: 2.42e-05, avg batch time: 0.1426, average loss: 2.5103
[09/17 06:01:55 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 12.00	top5: 61.50	
[09/17 06:02:16 visual_prompt]: 	Test 100/190. loss: 2.238, 0.1825 s / batch. (data: 1.38e-04)max mem: 17.22448 GB 
[09/17 06:02:35 visual_prompt]: Inference (test):avg data time: 8.29e-03, avg batch time: 0.1953, average loss: 2.4885
[09/17 06:02:35 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 13.21	top5: 65.71	
[09/17 06:02:35 visual_prompt]: Training 60 / 100 epoch, with learning rate 2.1520672475998373
[09/17 06:02:44 visual_prompt]: Epoch 60 / 100: avg data time: 9.45e-02, avg batch time: 0.4994, average train loss: 2.0245
[09/17 06:02:47 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1427, average loss: 2.1896
[09/17 06:02:47 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 85.00	
[09/17 06:03:08 visual_prompt]: 	Test 100/190. loss: 2.060, 0.1826 s / batch. (data: 1.31e-04)max mem: 17.22448 GB 
[09/17 06:03:27 visual_prompt]: Inference (test):avg data time: 7.43e-03, avg batch time: 0.1927, average loss: 2.3185
[09/17 06:03:27 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 18.33	top5: 76.84	
[09/17 06:03:27 visual_prompt]: Training 61 / 100 epoch, with learning rate 2.0658795558326744
[09/17 06:03:36 visual_prompt]: Epoch 61 / 100: avg data time: 9.52e-02, avg batch time: 0.4983, average train loss: 1.9728
[09/17 06:03:39 visual_prompt]: Inference (val):avg data time: 2.39e-05, avg batch time: 0.1427, average loss: 1.8985
[09/17 06:03:39 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.00	top5: 83.50	
[09/17 06:04:00 visual_prompt]: 	Test 100/190. loss: 1.843, 0.1967 s / batch. (data: 1.46e-02)max mem: 17.22448 GB 
[09/17 06:04:18 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1931, average loss: 1.9620
[09/17 06:04:18 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.10	top5: 82.74	
[09/17 06:04:18 visual_prompt]: Training 62 / 100 epoch, with learning rate 1.980220772955602
[09/17 06:04:27 visual_prompt]: Epoch 62 / 100: avg data time: 8.48e-02, avg batch time: 0.4890, average train loss: 1.9140
[09/17 06:04:30 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.1427, average loss: 1.9177
[09/17 06:04:30 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 85.50	
[09/17 06:04:51 visual_prompt]: 	Test 100/190. loss: 1.979, 0.1829 s / batch. (data: 1.22e-04)max mem: 17.22448 GB 
[09/17 06:05:10 visual_prompt]: Inference (test):avg data time: 8.61e-03, avg batch time: 0.1941, average loss: 2.0549
[09/17 06:05:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.20	top5: 78.26	
[09/17 06:05:10 visual_prompt]: Training 63 / 100 epoch, with learning rate 1.895195261000831
[09/17 06:05:20 visual_prompt]: Epoch 63 / 100: avg data time: 1.11e-01, avg batch time: 0.5298, average train loss: 1.9855
[09/17 06:05:23 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1427, average loss: 1.8684
[09/17 06:05:23 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 83.00	
[09/17 06:05:44 visual_prompt]: 	Test 100/190. loss: 2.004, 0.2040 s / batch. (data: 2.17e-02)max mem: 17.22448 GB 
[09/17 06:06:02 visual_prompt]: Inference (test):avg data time: 6.65e-03, avg batch time: 0.1932, average loss: 1.9625
[09/17 06:06:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.80	top5: 82.86	
[09/17 06:06:02 visual_prompt]: Training 64 / 100 epoch, with learning rate 1.8109066104575022
[09/17 06:06:12 visual_prompt]: Epoch 64 / 100: avg data time: 1.11e-01, avg batch time: 0.5119, average train loss: 2.0180
[09/17 06:06:14 visual_prompt]: Inference (val):avg data time: 2.44e-05, avg batch time: 0.1431, average loss: 1.9040
[09/17 06:06:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.00	top5: 84.50	
[09/17 06:06:36 visual_prompt]: 	Test 100/190. loss: 1.858, 0.1957 s / batch. (data: 1.39e-02)max mem: 17.22448 GB 
[09/17 06:06:54 visual_prompt]: Inference (test):avg data time: 7.74e-03, avg batch time: 0.1923, average loss: 1.9569
[09/17 06:06:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.35	top5: 84.40	
[09/17 06:06:54 visual_prompt]: Training 65 / 100 epoch, with learning rate 1.7274575140626316
[09/17 06:07:03 visual_prompt]: Epoch 65 / 100: avg data time: 1.04e-01, avg batch time: 0.5040, average train loss: 1.9442
[09/17 06:07:06 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.1427, average loss: 1.9475
[09/17 06:07:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 81.50	
[09/17 06:07:27 visual_prompt]: 	Test 100/190. loss: 1.966, 0.2373 s / batch. (data: 1.33e-02)max mem: 17.22448 GB 
[09/17 06:07:46 visual_prompt]: Inference (test):avg data time: 7.33e-03, avg batch time: 0.1930, average loss: 2.1114
[09/17 06:07:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.60	top5: 78.87	
[09/17 06:07:46 visual_prompt]: Training 66 / 100 epoch, with learning rate 1.6449496416858285
[09/17 06:07:55 visual_prompt]: Epoch 66 / 100: avg data time: 1.09e-01, avg batch time: 0.5177, average train loss: 1.8430
[09/17 06:07:58 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.1426, average loss: 1.7106
[09/17 06:07:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 28.50	top5: 90.00	
[09/17 06:08:19 visual_prompt]: 	Test 100/190. loss: 1.778, 0.1961 s / batch. (data: 1.42e-02)max mem: 17.22448 GB 
[09/17 06:08:38 visual_prompt]: Inference (test):avg data time: 7.92e-03, avg batch time: 0.1934, average loss: 1.8532
[09/17 06:08:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.76	top5: 86.96	
[09/17 06:08:38 visual_prompt]: Best epoch 66: best metric: 0.285
[09/17 06:08:38 visual_prompt]: Training 67 / 100 epoch, with learning rate 1.5634835164602199
[09/17 06:08:47 visual_prompt]: Epoch 67 / 100: avg data time: 1.02e-01, avg batch time: 0.5064, average train loss: 1.8509
[09/17 06:08:50 visual_prompt]: Inference (val):avg data time: 3.34e-05, avg batch time: 0.1427, average loss: 1.8120
[09/17 06:08:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.50	top5: 86.00	
[09/17 06:09:11 visual_prompt]: 	Test 100/190. loss: 1.841, 0.1935 s / batch. (data: 1.17e-02)max mem: 17.22448 GB 
[09/17 06:09:30 visual_prompt]: Inference (test):avg data time: 7.68e-03, avg batch time: 0.1968, average loss: 1.9322
[09/17 06:09:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 19.77	top5: 85.11	
[09/17 06:09:30 visual_prompt]: Training 68 / 100 epoch, with learning rate 1.4831583923104998
[09/17 06:09:39 visual_prompt]: Epoch 68 / 100: avg data time: 1.02e-01, avg batch time: 0.5115, average train loss: 1.8177
[09/17 06:09:42 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.1427, average loss: 1.7131
[09/17 06:09:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.50	top5: 92.00	
[09/17 06:10:03 visual_prompt]: 	Test 100/190. loss: 1.843, 0.1978 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 06:10:21 visual_prompt]: Inference (test):avg data time: 6.14e-03, avg batch time: 0.1910, average loss: 1.9085
[09/17 06:10:21 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.95	top5: 86.60	
[09/17 06:10:21 visual_prompt]: Training 69 / 100 epoch, with learning rate 1.4040721330273063
[09/17 06:10:30 visual_prompt]: Epoch 69 / 100: avg data time: 1.10e-01, avg batch time: 0.5114, average train loss: 1.8151
[09/17 06:10:33 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.1425, average loss: 2.0151
[09/17 06:10:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.00	top5: 85.00	
[09/17 06:10:54 visual_prompt]: 	Test 100/190. loss: 1.848, 0.1821 s / batch. (data: 1.24e-04)max mem: 17.22448 GB 
[09/17 06:11:13 visual_prompt]: Inference (test):avg data time: 7.08e-03, avg batch time: 0.1925, average loss: 2.1897
[09/17 06:11:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.75	top5: 81.09	
[09/17 06:11:13 visual_prompt]: Training 70 / 100 epoch, with learning rate 1.3263210930352738
[09/17 06:11:22 visual_prompt]: Epoch 70 / 100: avg data time: 1.06e-01, avg batch time: 0.5096, average train loss: 1.8282
[09/17 06:11:25 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.1428, average loss: 2.0417
[09/17 06:11:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.50	top5: 77.00	
[09/17 06:11:46 visual_prompt]: 	Test 100/190. loss: 1.951, 0.1821 s / batch. (data: 1.35e-04)max mem: 17.22448 GB 
[09/17 06:12:05 visual_prompt]: Inference (test):avg data time: 7.04e-03, avg batch time: 0.1955, average loss: 2.2009
[09/17 06:12:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.88	top5: 77.69	
[09/17 06:12:05 visual_prompt]: Training 71 / 100 epoch, with learning rate 1.2500000000000004
[09/17 06:12:14 visual_prompt]: Epoch 71 / 100: avg data time: 1.02e-01, avg batch time: 0.5041, average train loss: 1.7824
[09/17 06:12:17 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1427, average loss: 1.6184
[09/17 06:12:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 31.00	top5: 95.00	
[09/17 06:12:38 visual_prompt]: 	Test 100/190. loss: 1.825, 0.1976 s / batch. (data: 1.38e-02)max mem: 17.22448 GB 
[09/17 06:12:57 visual_prompt]: Inference (test):avg data time: 8.34e-03, avg batch time: 0.1940, average loss: 1.8483
[09/17 06:12:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.12	top5: 88.71	
[09/17 06:12:57 visual_prompt]: Best epoch 71: best metric: 0.310
[09/17 06:12:57 visual_prompt]: Training 72 / 100 epoch, with learning rate 1.175201839416988
[09/17 06:13:06 visual_prompt]: Epoch 72 / 100: avg data time: 9.96e-02, avg batch time: 0.5018, average train loss: 1.7080
[09/17 06:13:09 visual_prompt]: Inference (val):avg data time: 3.42e-04, avg batch time: 0.2099, average loss: 1.5924
[09/17 06:13:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.00	top5: 92.50	
[09/17 06:13:30 visual_prompt]: 	Test 100/190. loss: 1.862, 0.1825 s / batch. (data: 1.32e-04)max mem: 17.22448 GB 
[09/17 06:13:49 visual_prompt]: Inference (test):avg data time: 6.59e-03, avg batch time: 0.1949, average loss: 1.8944
[09/17 06:13:49 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.62	top5: 87.86	
[09/17 06:13:49 visual_prompt]: Training 73 / 100 epoch, with learning rate 1.1020177413231333
[09/17 06:13:58 visual_prompt]: Epoch 73 / 100: avg data time: 1.07e-01, avg batch time: 0.5080, average train loss: 1.7157
[09/17 06:14:02 visual_prompt]: Inference (val):avg data time: 3.30e-04, avg batch time: 0.2613, average loss: 1.5737
[09/17 06:14:02 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.50	top5: 95.00	
[09/17 06:14:23 visual_prompt]: 	Test 100/190. loss: 1.773, 0.1941 s / batch. (data: 1.22e-04)max mem: 17.22448 GB 
[09/17 06:14:42 visual_prompt]: Inference (test):avg data time: 8.14e-03, avg batch time: 0.1935, average loss: 1.8204
[09/17 06:14:42 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.13	top5: 88.58	
[09/17 06:14:42 visual_prompt]: Training 74 / 100 epoch, with learning rate 1.0305368692688175
[09/17 06:14:51 visual_prompt]: Epoch 74 / 100: avg data time: 9.67e-02, avg batch time: 0.4992, average train loss: 1.6914
[09/17 06:14:54 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.1427, average loss: 1.7240
[09/17 06:14:54 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.00	top5: 93.50	
[09/17 06:15:15 visual_prompt]: 	Test 100/190. loss: 1.812, 0.2043 s / batch. (data: 1.56e-02)max mem: 17.22448 GB 
[09/17 06:15:34 visual_prompt]: Inference (test):avg data time: 7.28e-03, avg batch time: 0.1938, average loss: 1.9823
[09/17 06:15:34 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.25	top5: 86.87	
[09/17 06:15:34 visual_prompt]: Training 75 / 100 epoch, with learning rate 0.9608463116858543
[09/17 06:15:43 visual_prompt]: Epoch 75 / 100: avg data time: 1.05e-01, avg batch time: 0.5074, average train loss: 1.7338
[09/17 06:15:46 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.1425, average loss: 1.5922
[09/17 06:15:46 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 32.00	top5: 93.50	
[09/17 06:16:07 visual_prompt]: 	Test 100/190. loss: 1.883, 0.1972 s / batch. (data: 1.55e-02)max mem: 17.22448 GB 
[09/17 06:16:26 visual_prompt]: Inference (test):avg data time: 7.95e-03, avg batch time: 0.1934, average loss: 1.9440
[09/17 06:16:26 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.41	top5: 87.35	
[09/17 06:16:26 visual_prompt]: Best epoch 75: best metric: 0.320
[09/17 06:16:26 visual_prompt]: Training 76 / 100 epoch, with learning rate 0.8930309757836516
[09/17 06:16:35 visual_prompt]: Epoch 76 / 100: avg data time: 9.46e-02, avg batch time: 0.4993, average train loss: 1.6649
[09/17 06:16:38 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1428, average loss: 1.5397
[09/17 06:16:38 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 34.50	top5: 94.50	
[09/17 06:16:59 visual_prompt]: 	Test 100/190. loss: 1.736, 0.1829 s / batch. (data: 1.13e-04)max mem: 17.22448 GB 
[09/17 06:17:17 visual_prompt]: Inference (test):avg data time: 7.04e-03, avg batch time: 0.1931, average loss: 1.9295
[09/17 06:17:17 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.91	top5: 87.02	
[09/17 06:17:17 visual_prompt]: Best epoch 76: best metric: 0.345
[09/17 06:17:17 visual_prompt]: Training 77 / 100 epoch, with learning rate 0.8271734841028553
[09/17 06:17:26 visual_prompt]: Epoch 77 / 100: avg data time: 1.00e-01, avg batch time: 0.5016, average train loss: 1.6640
[09/17 06:17:29 visual_prompt]: Inference (val):avg data time: 2.52e-05, avg batch time: 0.1426, average loss: 1.5465
[09/17 06:17:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 96.50	
[09/17 06:17:50 visual_prompt]: 	Test 100/190. loss: 2.035, 0.1996 s / batch. (data: 3.01e-03)max mem: 17.22448 GB 
[09/17 06:18:09 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1932, average loss: 1.9585
[09/17 06:18:09 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 21.81	top5: 87.32	
[09/17 06:18:09 visual_prompt]: Best epoch 77: best metric: 0.355
[09/17 06:18:09 visual_prompt]: Training 78 / 100 epoch, with learning rate 0.7633540738525066
[09/17 06:18:18 visual_prompt]: Epoch 78 / 100: avg data time: 1.05e-01, avg batch time: 0.5047, average train loss: 1.6965
[09/17 06:18:22 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.1429, average loss: 1.5360
[09/17 06:18:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 30.00	top5: 97.00	
[09/17 06:18:43 visual_prompt]: 	Test 100/190. loss: 1.896, 0.1826 s / batch. (data: 1.33e-04)max mem: 17.22448 GB 
[09/17 06:19:01 visual_prompt]: Inference (test):avg data time: 7.12e-03, avg batch time: 0.1934, average loss: 1.9307
[09/17 06:19:01 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 22.57	top5: 87.17	
[09/17 06:19:01 visual_prompt]: Training 79 / 100 epoch, with learning rate 0.7016504991533725
[09/17 06:19:10 visual_prompt]: Epoch 79 / 100: avg data time: 8.89e-02, avg batch time: 0.4918, average train loss: 1.6142
[09/17 06:19:14 visual_prompt]: Inference (val):avg data time: 3.98e-04, avg batch time: 0.2753, average loss: 1.6362
[09/17 06:19:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.50	top5: 93.50	
[09/17 06:19:35 visual_prompt]: 	Test 100/190. loss: 1.924, 0.1827 s / batch. (data: 1.40e-04)max mem: 17.22448 GB 
[09/17 06:19:54 visual_prompt]: Inference (test):avg data time: 7.00e-03, avg batch time: 0.1929, average loss: 2.1003
[09/17 06:19:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 20.93	top5: 84.92	
[09/17 06:19:54 visual_prompt]: Training 80 / 100 epoch, with learning rate 0.6421379363065141
[09/17 06:20:03 visual_prompt]: Epoch 80 / 100: avg data time: 9.77e-02, avg batch time: 0.5024, average train loss: 1.5240
[09/17 06:20:06 visual_prompt]: Inference (val):avg data time: 3.36e-05, avg batch time: 0.1426, average loss: 1.3666
[09/17 06:20:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 41.00	top5: 97.00	
[09/17 06:20:27 visual_prompt]: 	Test 100/190. loss: 1.927, 0.1963 s / batch. (data: 1.42e-02)max mem: 17.22448 GB 
[09/17 06:20:45 visual_prompt]: Inference (test):avg data time: 6.65e-03, avg batch time: 0.1921, average loss: 1.9814
[09/17 06:20:45 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.44	top5: 87.32	
[09/17 06:20:45 visual_prompt]: Best epoch 80: best metric: 0.410
[09/17 06:20:45 visual_prompt]: Training 81 / 100 epoch, with learning rate 0.5848888922025552
[09/17 06:20:54 visual_prompt]: Epoch 81 / 100: avg data time: 1.07e-01, avg batch time: 0.5101, average train loss: 1.4647
[09/17 06:20:57 visual_prompt]: Inference (val):avg data time: 2.64e-05, avg batch time: 0.1427, average loss: 1.4141
[09/17 06:20:57 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 38.50	top5: 97.00	
[09/17 06:21:18 visual_prompt]: 	Test 100/190. loss: 1.941, 0.1841 s / batch. (data: 1.12e-04)max mem: 17.22448 GB 
[09/17 06:21:37 visual_prompt]: Inference (test):avg data time: 6.49e-03, avg batch time: 0.1927, average loss: 2.0721
[09/17 06:21:37 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.93	top5: 86.60	
[09/17 06:21:37 visual_prompt]: Training 82 / 100 epoch, with learning rate 0.5299731159831953
[09/17 06:21:46 visual_prompt]: Epoch 82 / 100: avg data time: 1.04e-01, avg batch time: 0.5069, average train loss: 1.4059
[09/17 06:21:49 visual_prompt]: Inference (val):avg data time: 2.55e-05, avg batch time: 0.1426, average loss: 1.3637
[09/17 06:21:49 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 42.00	top5: 97.00	
[09/17 06:22:10 visual_prompt]: 	Test 100/190. loss: 1.871, 0.1947 s / batch. (data: 1.28e-02)max mem: 17.22448 GB 
[09/17 06:22:29 visual_prompt]: Inference (test):avg data time: 7.76e-03, avg batch time: 0.1934, average loss: 2.0421
[09/17 06:22:29 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.06	top5: 87.72	
[09/17 06:22:29 visual_prompt]: Best epoch 82: best metric: 0.420
[09/17 06:22:29 visual_prompt]: Training 83 / 100 epoch, with learning rate 0.47745751406263165
[09/17 06:22:38 visual_prompt]: Epoch 83 / 100: avg data time: 1.06e-01, avg batch time: 0.5112, average train loss: 1.5104
[09/17 06:22:41 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.1429, average loss: 1.4350
[09/17 06:22:41 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.50	top5: 98.00	
[09/17 06:23:02 visual_prompt]: 	Test 100/190. loss: 2.015, 0.1833 s / batch. (data: 1.62e-04)max mem: 17.22448 GB 
[09/17 06:23:20 visual_prompt]: Inference (test):avg data time: 5.98e-03, avg batch time: 0.1907, average loss: 1.9424
[09/17 06:23:20 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.05	top5: 88.07	
[09/17 06:23:20 visual_prompt]: Training 84 / 100 epoch, with learning rate 0.42740606861239594
[09/17 06:23:30 visual_prompt]: Epoch 84 / 100: avg data time: 1.12e-01, avg batch time: 0.5121, average train loss: 1.4807
[09/17 06:23:33 visual_prompt]: Inference (val):avg data time: 4.51e-04, avg batch time: 0.2477, average loss: 1.3909
[09/17 06:23:33 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 39.50	top5: 96.50	
[09/17 06:23:54 visual_prompt]: 	Test 100/190. loss: 1.876, 0.1834 s / batch. (data: 1.46e-04)max mem: 17.22448 GB 
[09/17 06:24:13 visual_prompt]: Inference (test):avg data time: 7.31e-03, avg batch time: 0.1930, average loss: 1.9658
[09/17 06:24:13 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.34	top5: 87.17	
[09/17 06:24:13 visual_prompt]: Training 85 / 100 epoch, with learning rate 0.3798797596089351
[09/17 06:24:22 visual_prompt]: Epoch 85 / 100: avg data time: 9.43e-02, avg batch time: 0.4977, average train loss: 1.3618
[09/17 06:24:25 visual_prompt]: Inference (val):avg data time: 1.11e-04, avg batch time: 0.2158, average loss: 1.4800
[09/17 06:24:25 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 35.00	top5: 97.00	
[09/17 06:24:46 visual_prompt]: 	Test 100/190. loss: 1.939, 0.1936 s / batch. (data: 1.58e-04)max mem: 17.22448 GB 
[09/17 06:25:05 visual_prompt]: Inference (test):avg data time: 7.27e-03, avg batch time: 0.1941, average loss: 2.1860
[09/17 06:25:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 24.03	top5: 85.98	
[09/17 06:25:05 visual_prompt]: Training 86 / 100 epoch, with learning rate 0.33493649053890323
[09/17 06:25:14 visual_prompt]: Epoch 86 / 100: avg data time: 1.14e-01, avg batch time: 0.5142, average train loss: 1.3506
[09/17 06:25:17 visual_prompt]: Inference (val):avg data time: 2.40e-05, avg batch time: 0.1427, average loss: 1.2578
[09/17 06:25:17 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 41.00	top5: 99.00	
[09/17 06:25:38 visual_prompt]: 	Test 100/190. loss: 1.999, 0.1826 s / batch. (data: 1.34e-04)max mem: 17.22448 GB 
[09/17 06:25:57 visual_prompt]: Inference (test):avg data time: 7.36e-03, avg batch time: 0.1938, average loss: 2.0860
[09/17 06:25:57 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.84	top5: 88.70	
[09/17 06:25:57 visual_prompt]: Training 87 / 100 epoch, with learning rate 0.29263101785268253
[09/17 06:26:06 visual_prompt]: Epoch 87 / 100: avg data time: 9.55e-02, avg batch time: 0.4984, average train loss: 1.2919
[09/17 06:26:09 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.1427, average loss: 1.2005
[09/17 06:26:09 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 47.00	top5: 99.50	
[09/17 06:26:30 visual_prompt]: 	Test 100/190. loss: 2.147, 0.1830 s / batch. (data: 1.60e-04)max mem: 17.22448 GB 
[09/17 06:26:48 visual_prompt]: Inference (test):avg data time: 6.22e-03, avg batch time: 0.1921, average loss: 2.1453
[09/17 06:26:48 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.28	top5: 87.82	
[09/17 06:26:48 visual_prompt]: Best epoch 87: best metric: 0.470
[09/17 06:26:48 visual_prompt]: Training 88 / 100 epoch, with learning rate 0.25301488425208296
[09/17 06:26:57 visual_prompt]: Epoch 88 / 100: avg data time: 1.11e-01, avg batch time: 0.5113, average train loss: 1.2656
[09/17 06:27:00 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.1426, average loss: 1.1805
[09/17 06:27:00 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 48.00	top5: 99.50	
[09/17 06:27:21 visual_prompt]: 	Test 100/190. loss: 2.129, 0.1956 s / batch. (data: 1.34e-02)max mem: 17.22448 GB 
[09/17 06:27:40 visual_prompt]: Inference (test):avg data time: 6.93e-03, avg batch time: 0.1931, average loss: 2.2275
[09/17 06:27:40 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.85	top5: 87.77	
[09/17 06:27:40 visual_prompt]: Best epoch 88: best metric: 0.480
[09/17 06:27:40 visual_prompt]: Training 89 / 100 epoch, with learning rate 0.21613635589349756
[09/17 06:27:49 visual_prompt]: Epoch 89 / 100: avg data time: 1.09e-01, avg batch time: 0.5090, average train loss: 1.2157
[09/17 06:27:52 visual_prompt]: Inference (val):avg data time: 2.49e-05, avg batch time: 0.1427, average loss: 1.1150
[09/17 06:27:52 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 47.50	top5: 99.50	
[09/17 06:28:13 visual_prompt]: 	Test 100/190. loss: 2.280, 0.1908 s / batch. (data: 1.13e-04)max mem: 17.22448 GB 
[09/17 06:28:32 visual_prompt]: Inference (test):avg data time: 8.08e-03, avg batch time: 0.1947, average loss: 2.2244
[09/17 06:28:32 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.66	top5: 87.97	
[09/17 06:28:32 visual_prompt]: Training 90 / 100 epoch, with learning rate 0.18204036358303172
[09/17 06:28:41 visual_prompt]: Epoch 90 / 100: avg data time: 1.04e-01, avg batch time: 0.5082, average train loss: 1.2185
[09/17 06:28:44 visual_prompt]: Inference (val):avg data time: 3.58e-05, avg batch time: 0.1437, average loss: 1.1170
[09/17 06:28:44 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 49.50	top5: 99.50	
[09/17 06:29:06 visual_prompt]: 	Test 100/190. loss: 2.133, 0.1961 s / batch. (data: 1.41e-02)max mem: 17.22448 GB 
[09/17 06:29:24 visual_prompt]: Inference (test):avg data time: 7.93e-03, avg batch time: 0.1950, average loss: 2.1809
[09/17 06:29:24 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.60	top5: 89.05	
[09/17 06:29:24 visual_prompt]: Best epoch 90: best metric: 0.495
[09/17 06:29:24 visual_prompt]: Training 91 / 100 epoch, with learning rate 0.1507684480352292
[09/17 06:29:34 visual_prompt]: Epoch 91 / 100: avg data time: 1.08e-01, avg batch time: 0.5111, average train loss: 1.1866
[09/17 06:29:37 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.1427, average loss: 1.2751
[09/17 06:29:37 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 40.50	top5: 99.00	
[09/17 06:29:58 visual_prompt]: 	Test 100/190. loss: 2.186, 0.1973 s / batch. (data: 1.51e-02)max mem: 17.22448 GB 
[09/17 06:30:16 visual_prompt]: Inference (test):avg data time: 7.65e-03, avg batch time: 0.1929, average loss: 2.3615
[09/17 06:30:16 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 23.89	top5: 87.08	
[09/17 06:30:16 visual_prompt]: Training 92 / 100 epoch, with learning rate 0.12235870926211617
[09/17 06:30:26 visual_prompt]: Epoch 92 / 100: avg data time: 1.17e-01, avg batch time: 0.5611, average train loss: 1.1315
[09/17 06:30:29 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.1425, average loss: 1.0411
[09/17 06:30:29 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 58.00	top5: 99.50	
[09/17 06:30:51 visual_prompt]: 	Test 100/190. loss: 2.327, 0.1974 s / batch. (data: 1.22e-04)max mem: 17.22448 GB 
[09/17 06:31:10 visual_prompt]: Inference (test):avg data time: 6.79e-03, avg batch time: 0.1966, average loss: 2.3124
[09/17 06:31:10 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.43	top5: 88.53	
[09/17 06:31:10 visual_prompt]: Best epoch 92: best metric: 0.580
[09/17 06:31:10 visual_prompt]: Training 93 / 100 epoch, with learning rate 0.09684576015420276
[09/17 06:31:19 visual_prompt]: Epoch 93 / 100: avg data time: 9.55e-02, avg batch time: 0.4989, average train loss: 1.1189
[09/17 06:31:22 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.1429, average loss: 1.0876
[09/17 06:31:22 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 51.50	top5: 99.50	
[09/17 06:31:43 visual_prompt]: 	Test 100/190. loss: 2.345, 0.2012 s / batch. (data: 1.78e-02)max mem: 17.22448 GB 
[09/17 06:32:01 visual_prompt]: Inference (test):avg data time: 6.75e-03, avg batch time: 0.1929, average loss: 2.4493
[09/17 06:32:02 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.43	top5: 87.98	
[09/17 06:32:02 visual_prompt]: Training 94 / 100 epoch, with learning rate 0.07426068431000882
[09/17 06:32:11 visual_prompt]: Epoch 94 / 100: avg data time: 1.14e-01, avg batch time: 0.5141, average train loss: 1.0778
[09/17 06:32:14 visual_prompt]: Inference (val):avg data time: 3.11e-05, avg batch time: 0.1427, average loss: 1.1073
[09/17 06:32:14 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 52.50	top5: 99.00	
[09/17 06:32:35 visual_prompt]: 	Test 100/190. loss: 2.375, 0.1828 s / batch. (data: 1.30e-04)max mem: 17.22448 GB 
[09/17 06:32:54 visual_prompt]: Inference (test):avg data time: 8.06e-03, avg batch time: 0.1966, average loss: 2.4921
[09/17 06:32:54 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 25.57	top5: 87.98	
[09/17 06:32:54 visual_prompt]: Training 95 / 100 epoch, with learning rate 0.05463099816548578
[09/17 06:33:03 visual_prompt]: Epoch 95 / 100: avg data time: 1.05e-01, avg batch time: 0.5078, average train loss: 1.0803
[09/17 06:33:06 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.1425, average loss: 1.0214
[09/17 06:33:06 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 54.00	top5: 99.50	
[09/17 06:33:28 visual_prompt]: 	Test 100/190. loss: 2.540, 0.2086 s / batch. (data: 2.69e-02)max mem: 17.22448 GB 
[09/17 06:33:46 visual_prompt]: Inference (test):avg data time: 7.77e-03, avg batch time: 0.1936, average loss: 2.5209
[09/17 06:33:46 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 27.01	top5: 88.35	
[09/17 06:33:46 visual_prompt]: Training 96 / 100 epoch, with learning rate 0.03798061746947995
[09/17 06:33:55 visual_prompt]: Epoch 96 / 100: avg data time: 1.08e-01, avg batch time: 0.5098, average train loss: 1.0446
[09/17 06:33:58 visual_prompt]: Inference (val):avg data time: 3.08e-05, avg batch time: 0.1426, average loss: 1.0254
[09/17 06:33:58 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 54.00	top5: 100.00	
[09/17 06:34:19 visual_prompt]: 	Test 100/190. loss: 2.560, 0.1837 s / batch. (data: 1.31e-04)max mem: 17.22448 GB 
[09/17 06:34:38 visual_prompt]: Inference (test):avg data time: 7.04e-03, avg batch time: 0.1927, average loss: 2.5806
[09/17 06:34:38 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.67	top5: 88.15	
[09/17 06:34:38 visual_prompt]: Training 97 / 100 epoch, with learning rate 0.024329828146074095
[09/17 06:34:47 visual_prompt]: Epoch 97 / 100: avg data time: 1.03e-01, avg batch time: 0.5030, average train loss: 1.0315
[09/17 06:34:50 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.1428, average loss: 1.0046
[09/17 06:34:50 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 56.00	top5: 100.00	
[09/17 06:35:11 visual_prompt]: 	Test 100/190. loss: 2.596, 0.1923 s / batch. (data: 9.57e-03)max mem: 17.22448 GB 
[09/17 06:35:30 visual_prompt]: Inference (test):avg data time: 7.20e-03, avg batch time: 0.1942, average loss: 2.6171
[09/17 06:35:30 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.77	top5: 88.29	
[09/17 06:35:30 visual_prompt]: Training 98 / 100 epoch, with learning rate 0.013695261579316775
[09/17 06:35:39 visual_prompt]: Epoch 98 / 100: avg data time: 1.04e-01, avg batch time: 0.5048, average train loss: 1.0128
[09/17 06:35:42 visual_prompt]: Inference (val):avg data time: 2.25e-05, avg batch time: 0.1426, average loss: 0.9990
[09/17 06:35:42 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 56.00	top5: 100.00	
[09/17 06:36:03 visual_prompt]: 	Test 100/190. loss: 2.611, 0.1829 s / batch. (data: 1.31e-04)max mem: 17.22448 GB 
[09/17 06:36:22 visual_prompt]: Inference (test):avg data time: 7.80e-03, avg batch time: 0.1954, average loss: 2.6224
[09/17 06:36:22 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.84	top5: 88.29	
[09/17 06:36:22 visual_prompt]: Training 99 / 100 epoch, with learning rate 0.006089874350439506
[09/17 06:36:31 visual_prompt]: Epoch 99 / 100: avg data time: 1.04e-01, avg batch time: 0.5042, average train loss: 1.0042
[09/17 06:36:34 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.1428, average loss: 1.0139
[09/17 06:36:34 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 55.00	top5: 100.00	
[09/17 06:36:55 visual_prompt]: 	Test 100/190. loss: 2.614, 0.1943 s / batch. (data: 1.25e-02)max mem: 17.22448 GB 
[09/17 06:37:14 visual_prompt]: Inference (test):avg data time: 7.75e-03, avg batch time: 0.1941, average loss: 2.6411
[09/17 06:37:14 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.66	top5: 88.21	
[09/17 06:37:14 visual_prompt]: Training 100 / 100 epoch, with learning rate 0.0015229324522605947
[09/17 06:37:23 visual_prompt]: Epoch 100 / 100: avg data time: 1.07e-01, avg batch time: 0.5095, average train loss: 1.0278
[09/17 06:37:26 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.1425, average loss: 1.0187
[09/17 06:37:26 visual_prompt]: Classification results with val_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 55.50	top5: 100.00	
[09/17 06:37:47 visual_prompt]: 	Test 100/190. loss: 2.612, 0.1829 s / batch. (data: 1.17e-04)max mem: 17.22448 GB 
[09/17 06:38:05 visual_prompt]: Inference (test):avg data time: 7.72e-03, avg batch time: 0.1921, average loss: 2.6436
[09/17 06:38:05 visual_prompt]: Classification results with test_vtab-smallnorb(predicted_attribute="label_elevation"): top1: 26.56	top5: 88.10	

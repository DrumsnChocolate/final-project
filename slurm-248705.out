/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.
Traceback (most recent call last):
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/file_io.py", line 946, in __log_tmetry_keys
    handler.log_event()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/iopath/common/event_logger.py", line 97, in log_event
    del self._evt
        ^^^^^^^^^
AttributeError: 'NativePathHandler' object has no attribute '_evt'
Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/20 01:15:32 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 01:15:33 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 01:15:33 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/20 01:15:33 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 01:15:33 visual_prompt]: Training with config:
[11/20 01:15:33 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size500/val/seed0/lr0.005_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 500, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 32, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 01:15:33 visual_prompt]: Loading training data...
[11/20 01:15:33 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 01:15:33 visual_prompt]: Loading validation data...
[11/20 01:15:33 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 01:15:33 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 962, 768])
load_pretrained: grid-size from 14 to 31
[11/20 01:15:35 visual_prompt]: Enable all parameters update during training
[11/20 01:15:35 visual_prompt]: Total Parameters: 86387714	 Gradient Parameters: 86387714
[11/20 01:15:35 visual_prompt]: tuned percent:100.000
[11/20 01:15:35 visual_prompt]: Device used for model: 0
[11/20 01:15:35 visual_prompt]: Setting up Evaluator...
[11/20 01:15:35 visual_prompt]: Setting up Trainer...
[11/20 01:15:35 visual_prompt]: 	Setting up the optimizer...
[11/20 01:15:35 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 01:23:16 visual_prompt]: Epoch 1 / 100: avg data time: 4.91e+00, avg batch time: 6.5866, average train loss: 7.2380
[11/20 01:24:10 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.5438, average loss: 6.4181
[11/20 01:24:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 52.79	
[11/20 01:24:10 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 01:31:43 visual_prompt]: Epoch 2 / 100: avg data time: 4.84e+00, avg batch time: 6.4804, average train loss: 2.2189
[11/20 01:32:36 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.5414, average loss: 0.7748
[11/20 01:32:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.03	
[11/20 01:32:36 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 01:40:12 visual_prompt]: Epoch 3 / 100: avg data time: 4.87e+00, avg batch time: 6.5094, average train loss: 1.2867
[11/20 01:41:05 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.5394, average loss: 0.7010
[11/20 01:41:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.74	
[11/20 01:41:05 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 01:48:39 visual_prompt]: Epoch 4 / 100: avg data time: 4.85e+00, avg batch time: 6.4920, average train loss: 0.8251
[11/20 01:49:32 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.5422, average loss: 0.7832
[11/20 01:49:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.52	
[11/20 01:49:32 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 01:57:06 visual_prompt]: Epoch 5 / 100: avg data time: 4.85e+00, avg batch time: 6.4862, average train loss: 0.8104
[11/20 01:57:59 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.5398, average loss: 0.6978
[11/20 01:57:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.50	
[11/20 01:57:59 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 02:05:36 visual_prompt]: Epoch 6 / 100: avg data time: 4.88e+00, avg batch time: 6.5206, average train loss: 0.8001
[11/20 02:06:29 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.5390, average loss: 0.9378
[11/20 02:06:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.07	
[11/20 02:06:29 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 02:14:01 visual_prompt]: Epoch 7 / 100: avg data time: 4.82e+00, avg batch time: 6.4540, average train loss: 0.8097
[11/20 02:14:54 visual_prompt]: Inference (val):avg data time: 2.70e-05, avg batch time: 0.5400, average loss: 0.8017
[11/20 02:14:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.27	
[11/20 02:14:54 visual_prompt]: Best epoch 7: best metric: -0.802
[11/20 02:14:54 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 02:22:25 visual_prompt]: Epoch 8 / 100: avg data time: 4.81e+00, avg batch time: 6.4452, average train loss: 0.7642
[11/20 02:23:18 visual_prompt]: Inference (val):avg data time: 3.13e-05, avg batch time: 0.5429, average loss: 1.0447
[11/20 02:23:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.61	
[11/20 02:23:18 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 02:30:51 visual_prompt]: Epoch 9 / 100: avg data time: 4.82e+00, avg batch time: 6.4626, average train loss: 0.7982
[11/20 02:31:44 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.5397, average loss: 0.9022
[11/20 02:31:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.00	
[11/20 02:31:44 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 02:39:17 visual_prompt]: Epoch 10 / 100: avg data time: 4.83e+00, avg batch time: 6.4653, average train loss: 0.9450
[11/20 02:40:10 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.5399, average loss: 0.7112
[11/20 02:40:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.42	
[11/20 02:40:10 visual_prompt]: Best epoch 10: best metric: -0.711
[11/20 02:40:10 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 02:47:42 visual_prompt]: Epoch 11 / 100: avg data time: 4.82e+00, avg batch time: 6.4623, average train loss: 0.7206
[11/20 02:48:36 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5402, average loss: 0.8538
[11/20 02:48:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.65	
[11/20 02:48:36 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 02:56:09 visual_prompt]: Epoch 12 / 100: avg data time: 4.83e+00, avg batch time: 6.4702, average train loss: 0.7562
[11/20 02:57:02 visual_prompt]: Inference (val):avg data time: 2.91e-05, avg batch time: 0.5387, average loss: 1.1632
[11/20 02:57:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.12	
[11/20 02:57:02 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 03:04:37 visual_prompt]: Epoch 13 / 100: avg data time: 4.85e+00, avg batch time: 6.4916, average train loss: 0.7914
[11/20 03:05:30 visual_prompt]: Inference (val):avg data time: 2.61e-05, avg batch time: 0.5406, average loss: 0.7081
[11/20 03:05:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.62	
[11/20 03:05:30 visual_prompt]: Best epoch 13: best metric: -0.708
[11/20 03:05:30 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 03:13:10 visual_prompt]: Epoch 14 / 100: avg data time: 4.93e+00, avg batch time: 6.5726, average train loss: 0.7228
[11/20 03:14:04 visual_prompt]: Inference (val):avg data time: 2.92e-05, avg batch time: 0.5388, average loss: 0.6905
[11/20 03:14:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.53	
[11/20 03:14:04 visual_prompt]: Best epoch 14: best metric: -0.690
[11/20 03:14:04 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 03:21:37 visual_prompt]: Epoch 15 / 100: avg data time: 4.84e+00, avg batch time: 6.4744, average train loss: 0.7479
[11/20 03:22:31 visual_prompt]: Inference (val):avg data time: 2.84e-05, avg batch time: 0.5397, average loss: 0.7276
[11/20 03:22:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.99	
[11/20 03:22:31 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 03:30:07 visual_prompt]: Epoch 16 / 100: avg data time: 4.87e+00, avg batch time: 6.5112, average train loss: 0.7722
[11/20 03:31:00 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.5389, average loss: 0.6868
[11/20 03:31:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.83	
[11/20 03:31:00 visual_prompt]: Best epoch 16: best metric: -0.687
[11/20 03:31:00 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 03:38:36 visual_prompt]: Epoch 17 / 100: avg data time: 4.86e+00, avg batch time: 6.5029, average train loss: 0.7654
[11/20 03:39:30 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.5405, average loss: 0.8098
[11/20 03:39:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.36	
[11/20 03:39:30 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 03:47:05 visual_prompt]: Epoch 18 / 100: avg data time: 4.87e+00, avg batch time: 6.5050, average train loss: 0.7406
[11/20 03:47:59 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.5391, average loss: 0.8781
[11/20 03:47:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.29	
[11/20 03:47:59 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 03:55:37 visual_prompt]: Epoch 19 / 100: avg data time: 4.91e+00, avg batch time: 6.5488, average train loss: 0.7397
[11/20 03:56:31 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.5419, average loss: 0.7693
[11/20 03:56:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.97	
[11/20 03:56:31 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 04:04:07 visual_prompt]: Epoch 20 / 100: avg data time: 4.88e+00, avg batch time: 6.5175, average train loss: 0.7627
[11/20 04:05:01 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.5385, average loss: 0.6971
[11/20 04:05:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.43	
[11/20 04:05:01 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/20 04:12:38 visual_prompt]: Epoch 21 / 100: avg data time: 4.89e+00, avg batch time: 6.5268, average train loss: 0.6935
[11/20 04:13:32 visual_prompt]: Inference (val):avg data time: 3.04e-05, avg batch time: 0.5391, average loss: 0.6921
[11/20 04:13:32 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 56.24	
[11/20 04:13:32 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/20 04:21:07 visual_prompt]: Epoch 22 / 100: avg data time: 4.87e+00, avg batch time: 6.5072, average train loss: 0.7223
[11/20 04:22:01 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.5383, average loss: 0.7201
[11/20 04:22:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.67	
[11/20 04:22:01 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/20 04:29:36 visual_prompt]: Epoch 23 / 100: avg data time: 4.86e+00, avg batch time: 6.4925, average train loss: 0.6989
[11/20 04:30:30 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5386, average loss: 0.9138
[11/20 04:30:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.02	
[11/20 04:30:30 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/20 04:38:07 visual_prompt]: Epoch 24 / 100: avg data time: 4.89e+00, avg batch time: 6.5292, average train loss: 0.7105
[11/20 04:39:01 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.5396, average loss: 0.6819
[11/20 04:39:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 57.12	
[11/20 04:39:01 visual_prompt]: Best epoch 24: best metric: -0.682
[11/20 04:39:01 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/20 04:46:37 visual_prompt]: Epoch 25 / 100: avg data time: 4.88e+00, avg batch time: 6.5194, average train loss: 0.7455
[11/20 04:47:31 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5401, average loss: 0.8827
[11/20 04:47:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.52	
[11/20 04:47:31 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/20 04:55:11 visual_prompt]: Epoch 26 / 100: avg data time: 4.94e+00, avg batch time: 6.5751, average train loss: 0.7143
[11/20 04:56:05 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.5384, average loss: 0.6830
[11/20 04:56:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.28	
[11/20 04:56:05 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/20 05:03:41 visual_prompt]: Epoch 27 / 100: avg data time: 4.87e+00, avg batch time: 6.5093, average train loss: 0.7713
[11/20 05:04:34 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.5406, average loss: 0.6953
[11/20 05:04:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.92	
[11/20 05:04:34 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/20 05:12:11 visual_prompt]: Epoch 28 / 100: avg data time: 4.89e+00, avg batch time: 6.5323, average train loss: 0.7010
[11/20 05:13:05 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.5388, average loss: 0.6949
[11/20 05:13:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 55.80	
[11/20 05:13:05 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/20 05:20:45 visual_prompt]: Epoch 29 / 100: avg data time: 4.93e+00, avg batch time: 6.5686, average train loss: 0.7183
[11/20 05:21:39 visual_prompt]: Inference (val):avg data time: 2.81e-05, avg batch time: 0.5392, average loss: 0.8619
[11/20 05:21:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.21	
[11/20 05:21:39 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/20 05:29:21 visual_prompt]: Epoch 30 / 100: avg data time: 4.95e+00, avg batch time: 6.5858, average train loss: 0.7107
[11/20 05:30:14 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.5407, average loss: 0.8241
[11/20 05:30:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.37	
[11/20 05:30:14 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/20 05:37:51 visual_prompt]: Epoch 31 / 100: avg data time: 4.89e+00, avg batch time: 6.5306, average train loss: 0.7761
[11/20 05:38:45 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.5396, average loss: 0.8198
[11/20 05:38:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.75	
[11/20 05:38:45 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/20 05:46:21 visual_prompt]: Epoch 32 / 100: avg data time: 4.88e+00, avg batch time: 6.5183, average train loss: 0.6966
[11/20 05:47:15 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.5416, average loss: 0.6830
[11/20 05:47:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 58.19	
[11/20 05:47:15 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/20 05:54:54 visual_prompt]: Epoch 33 / 100: avg data time: 4.91e+00, avg batch time: 6.5540, average train loss: 0.7266
[11/20 05:55:48 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.5399, average loss: 0.8466
[11/20 05:55:48 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 55.44	
[11/20 05:55:48 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/20 06:03:30 visual_prompt]: Epoch 34 / 100: avg data time: 4.95e+00, avg batch time: 6.5908, average train loss: 0.7398
[11/20 06:04:23 visual_prompt]: Inference (val):avg data time: 3.06e-05, avg batch time: 0.5409, average loss: 0.7145
[11/20 06:04:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.77	
[11/20 06:04:23 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0039358216567073594
[11/20 06:12:00 visual_prompt]: Epoch 35 / 100: avg data time: 4.88e+00, avg batch time: 6.5220, average train loss: 0.7354
[11/20 06:12:53 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.5404, average loss: 0.6883
[11/20 06:12:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 52.03	rocauc: 57.95	
[11/20 06:12:53 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.003867370395306068
[11/20 06:20:32 visual_prompt]: Epoch 36 / 100: avg data time: 4.91e+00, avg batch time: 6.5460, average train loss: 0.6873
[11/20 06:21:25 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.5404, average loss: 0.7399
[11/20 06:21:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 48.78	rocauc: 56.45	
[11/20 06:21:25 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.0037974239344530382
[11/20 06:29:05 visual_prompt]: Epoch 37 / 100: avg data time: 4.92e+00, avg batch time: 6.5669, average train loss: 0.6988
[11/20 06:29:59 visual_prompt]: Inference (val):avg data time: 2.93e-05, avg batch time: 0.5385, average loss: 0.6822
[11/20 06:29:59 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.53	
[11/20 06:29:59 visual_prompt]: Training 38 / 100 epoch, with learning rate 0.0037260587595762708
[11/20 06:37:37 visual_prompt]: Epoch 38 / 100: avg data time: 4.91e+00, avg batch time: 6.5438, average train loss: 0.6986
[11/20 06:38:30 visual_prompt]: Inference (val):avg data time: 2.89e-05, avg batch time: 0.5406, average loss: 0.9092
[11/20 06:38:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.34	
[11/20 06:38:30 visual_prompt]: Training 39 / 100 epoch, with learning rate 0.0036533529074467197
[11/20 06:46:08 visual_prompt]: Epoch 39 / 100: avg data time: 4.89e+00, avg batch time: 6.5349, average train loss: 0.7061
[11/20 06:47:02 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.5389, average loss: 0.6972
[11/20 06:47:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.31	
[11/20 06:47:02 visual_prompt]: Training 40 / 100 epoch, with learning rate 0.003579385880846232
[11/20 06:54:46 visual_prompt]: Epoch 40 / 100: avg data time: 4.98e+00, avg batch time: 6.6244, average train loss: 0.6779
[11/20 06:55:39 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.5407, average loss: 0.6855
[11/20 06:55:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.79	
[11/20 06:55:39 visual_prompt]: Training 41 / 100 epoch, with learning rate 0.003504238561632424
[11/20 07:03:19 visual_prompt]: Epoch 41 / 100: avg data time: 4.93e+00, avg batch time: 6.5700, average train loss: 0.6802
[11/20 07:04:13 visual_prompt]: Inference (val):avg data time: 3.29e-05, avg batch time: 0.5387, average loss: 0.7024
[11/20 07:04:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 59.76	rocauc: 60.33	
[11/20 07:04:13 visual_prompt]: Training 42 / 100 epoch, with learning rate 0.003427993122295552
[11/20 07:11:50 visual_prompt]: Epoch 42 / 100: avg data time: 4.88e+00, avg batch time: 6.5242, average train loss: 0.6893
[11/20 07:12:44 visual_prompt]: Inference (val):avg data time: 2.86e-05, avg batch time: 0.5384, average loss: 0.7106
[11/20 07:12:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.55	
[11/20 07:12:44 visual_prompt]: Training 43 / 100 epoch, with learning rate 0.003350732936104108
[11/20 07:20:23 visual_prompt]: Epoch 43 / 100: avg data time: 4.92e+00, avg batch time: 6.5573, average train loss: 0.6958
[11/20 07:21:17 visual_prompt]: Inference (val):avg data time: 3.05e-05, avg batch time: 0.5414, average loss: 0.7675
[11/20 07:21:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 58.35	
[11/20 07:21:17 visual_prompt]: Training 44 / 100 epoch, with learning rate 0.0032725424859373687
[11/20 07:28:53 visual_prompt]: Epoch 44 / 100: avg data time: 4.88e+00, avg batch time: 6.5231, average train loss: 0.7197
[11/20 07:29:47 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.5384, average loss: 0.6950
[11/20 07:29:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 57.03	
[11/20 07:29:47 visual_prompt]: Training 45 / 100 epoch, with learning rate 0.0031935072719046116
[11/20 07:37:22 visual_prompt]: Epoch 45 / 100: avg data time: 4.86e+00, avg batch time: 6.5008, average train loss: 0.6895
[11/20 07:38:16 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.5393, average loss: 0.7476
[11/20 07:38:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.45	
[11/20 07:38:16 visual_prompt]: Stopping early.
[11/20 07:38:16 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 07:38:16 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 07:38:16 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/20 07:38:16 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 07:38:16 visual_prompt]: Training with config:
[11/20 07:38:16 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size500/val/seed0/lr0.005_wd0.001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 500, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 32, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 07:38:16 visual_prompt]: Loading training data...
[11/20 07:38:16 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 07:38:16 visual_prompt]: Loading validation data...
[11/20 07:38:16 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 07:38:16 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 962, 768])
load_pretrained: grid-size from 14 to 31
[11/20 07:38:18 visual_prompt]: Enable all parameters update during training
[11/20 07:38:18 visual_prompt]: Total Parameters: 86387714	 Gradient Parameters: 86387714
[11/20 07:38:18 visual_prompt]: tuned percent:100.000
[11/20 07:38:18 visual_prompt]: Device used for model: 0
[11/20 07:38:18 visual_prompt]: Setting up Evaluator...
[11/20 07:38:18 visual_prompt]: Setting up Trainer...
[11/20 07:38:18 visual_prompt]: 	Setting up the optimizer...
[11/20 07:38:18 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 07:45:48 visual_prompt]: Epoch 1 / 100: avg data time: 4.78e+00, avg batch time: 6.4269, average train loss: 7.2380
[11/20 07:46:42 visual_prompt]: Inference (val):avg data time: 2.54e-05, avg batch time: 0.5418, average loss: 6.4181
[11/20 07:46:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 52.79	
[11/20 07:46:42 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 07:54:10 visual_prompt]: Epoch 2 / 100: avg data time: 4.77e+00, avg batch time: 6.4086, average train loss: 2.2186
[11/20 07:55:03 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5432, average loss: 0.7720
[11/20 07:55:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.65	
[11/20 07:55:03 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 08:02:34 visual_prompt]: Epoch 3 / 100: avg data time: 4.79e+00, avg batch time: 6.4350, average train loss: 1.2856
[11/20 08:03:27 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.5410, average loss: 0.7008
[11/20 08:03:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.63	
[11/20 08:03:27 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 08:10:57 visual_prompt]: Epoch 4 / 100: avg data time: 4.78e+00, avg batch time: 6.4206, average train loss: 0.8300
[11/20 08:11:50 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.5400, average loss: 0.7953
[11/20 08:11:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.57	
[11/20 08:11:50 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 08:19:20 visual_prompt]: Epoch 5 / 100: avg data time: 4.79e+00, avg batch time: 6.4248, average train loss: 0.8053
[11/20 08:20:12 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.5399, average loss: 0.6882
[11/20 08:20:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 56.66	
[11/20 08:20:12 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 08:27:43 visual_prompt]: Epoch 6 / 100: avg data time: 4.80e+00, avg batch time: 6.4392, average train loss: 0.7974
[11/20 08:28:36 visual_prompt]: Inference (val):avg data time: 2.57e-05, avg batch time: 0.5393, average loss: 0.9325
[11/20 08:28:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.78	
[11/20 08:28:36 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 08:36:05 visual_prompt]: Epoch 7 / 100: avg data time: 4.77e+00, avg batch time: 6.4077, average train loss: 0.8169
[11/20 08:36:58 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.5397, average loss: 0.7987
[11/20 08:36:58 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.42	
[11/20 08:36:58 visual_prompt]: Best epoch 7: best metric: -0.799
[11/20 08:36:58 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 08:44:28 visual_prompt]: Epoch 8 / 100: avg data time: 4.79e+00, avg batch time: 6.4319, average train loss: 0.7508
[11/20 08:45:21 visual_prompt]: Inference (val):avg data time: 2.66e-05, avg batch time: 0.5392, average loss: 1.1207
[11/20 08:45:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.39	
[11/20 08:45:21 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 08:52:51 visual_prompt]: Epoch 9 / 100: avg data time: 4.79e+00, avg batch time: 6.4226, average train loss: 0.8069
[11/20 08:53:44 visual_prompt]: Inference (val):avg data time: 2.98e-05, avg batch time: 0.5400, average loss: 0.9896
[11/20 08:53:44 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.44	
[11/20 08:53:44 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 09:01:13 visual_prompt]: Epoch 10 / 100: avg data time: 4.77e+00, avg batch time: 6.4162, average train loss: 0.9424
[11/20 09:02:07 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.5417, average loss: 0.7205
[11/20 09:02:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.33	
[11/20 09:02:07 visual_prompt]: Best epoch 10: best metric: -0.720
[11/20 09:02:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 09:09:36 visual_prompt]: Epoch 11 / 100: avg data time: 4.77e+00, avg batch time: 6.4121, average train loss: 0.7241
[11/20 09:10:29 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.5399, average loss: 0.7788
[11/20 09:10:29 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.65	
[11/20 09:10:29 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 09:17:58 visual_prompt]: Epoch 12 / 100: avg data time: 4.77e+00, avg batch time: 6.4157, average train loss: 0.7574
[11/20 09:18:51 visual_prompt]: Inference (val):avg data time: 2.68e-05, avg batch time: 0.5442, average loss: 1.1318
[11/20 09:18:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.50	
[11/20 09:18:51 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 09:26:20 visual_prompt]: Epoch 13 / 100: avg data time: 4.77e+00, avg batch time: 6.4078, average train loss: 0.7926
[11/20 09:27:13 visual_prompt]: Inference (val):avg data time: 2.82e-05, avg batch time: 0.5436, average loss: 0.7343
[11/20 09:27:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.18	
[11/20 09:27:13 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 09:34:47 visual_prompt]: Epoch 14 / 100: avg data time: 4.85e+00, avg batch time: 6.4877, average train loss: 0.7225
[11/20 09:35:41 visual_prompt]: Inference (val):avg data time: 2.83e-05, avg batch time: 0.5388, average loss: 0.6846
[11/20 09:35:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.65	
[11/20 09:35:41 visual_prompt]: Best epoch 14: best metric: -0.685
[11/20 09:35:41 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 09:43:11 visual_prompt]: Epoch 15 / 100: avg data time: 4.79e+00, avg batch time: 6.4288, average train loss: 0.7834
[11/20 09:44:04 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.5399, average loss: 0.9353
[11/20 09:44:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 59.84	
[11/20 09:44:04 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 09:51:32 visual_prompt]: Epoch 16 / 100: avg data time: 4.76e+00, avg batch time: 6.4023, average train loss: 0.7642
[11/20 09:52:25 visual_prompt]: Inference (val):avg data time: 4.21e-05, avg batch time: 0.5385, average loss: 0.6871
[11/20 09:52:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.36	
[11/20 09:52:25 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 09:59:57 visual_prompt]: Epoch 17 / 100: avg data time: 4.81e+00, avg batch time: 6.4454, average train loss: 0.7671
[11/20 10:00:50 visual_prompt]: Inference (val):avg data time: 3.28e-05, avg batch time: 0.5386, average loss: 0.8144
[11/20 10:00:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.50	
[11/20 10:00:50 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 10:08:20 visual_prompt]: Epoch 18 / 100: avg data time: 4.79e+00, avg batch time: 6.4236, average train loss: 0.7628
[11/20 10:09:13 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5420, average loss: 0.8183
[11/20 10:09:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.73	
[11/20 10:09:13 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 10:16:46 visual_prompt]: Epoch 19 / 100: avg data time: 4.83e+00, avg batch time: 6.4737, average train loss: 0.7427
[11/20 10:17:39 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5385, average loss: 0.7605
[11/20 10:17:39 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.68	
[11/20 10:17:39 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 10:25:12 visual_prompt]: Epoch 20 / 100: avg data time: 4.83e+00, avg batch time: 6.4703, average train loss: 0.7705
[11/20 10:26:05 visual_prompt]: Inference (val):avg data time: 3.68e-05, avg batch time: 0.5382, average loss: 0.7154
[11/20 10:26:05 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.76	
[11/20 10:26:05 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/20 10:33:34 visual_prompt]: Epoch 21 / 100: avg data time: 4.77e+00, avg batch time: 6.4138, average train loss: 0.6986
[11/20 10:34:28 visual_prompt]: Inference (val):avg data time: 2.65e-05, avg batch time: 0.5403, average loss: 0.6848
[11/20 10:34:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.47	rocauc: 55.76	
[11/20 10:34:28 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/20 10:41:58 visual_prompt]: Epoch 22 / 100: avg data time: 4.79e+00, avg batch time: 6.4279, average train loss: 0.7226
[11/20 10:42:51 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5395, average loss: 0.7598
[11/20 10:42:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.51	
[11/20 10:42:51 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/20 10:50:20 visual_prompt]: Epoch 23 / 100: avg data time: 4.78e+00, avg batch time: 6.4150, average train loss: 0.7018
[11/20 10:51:13 visual_prompt]: Inference (val):avg data time: 2.74e-05, avg batch time: 0.5393, average loss: 0.8967
[11/20 10:51:13 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.08	
[11/20 10:51:13 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/20 10:58:45 visual_prompt]: Epoch 24 / 100: avg data time: 4.81e+00, avg batch time: 6.4472, average train loss: 0.7597
[11/20 10:59:38 visual_prompt]: Inference (val):avg data time: 4.00e-05, avg batch time: 0.5435, average loss: 0.7039
[11/20 10:59:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 48.83	
[11/20 10:59:38 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/20 11:07:08 visual_prompt]: Epoch 25 / 100: avg data time: 4.79e+00, avg batch time: 6.4342, average train loss: 0.7924
[11/20 11:08:01 visual_prompt]: Inference (val):avg data time: 3.27e-05, avg batch time: 0.5427, average loss: 0.8659
[11/20 11:08:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.29	
[11/20 11:08:01 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/20 11:15:32 visual_prompt]: Epoch 26 / 100: avg data time: 4.79e+00, avg batch time: 6.4313, average train loss: 0.7305
[11/20 11:16:25 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.5409, average loss: 0.6865
[11/20 11:16:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.01	
[11/20 11:16:25 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/20 11:23:57 visual_prompt]: Epoch 27 / 100: avg data time: 4.82e+00, avg batch time: 6.4571, average train loss: 0.7736
[11/20 11:24:50 visual_prompt]: Inference (val):avg data time: 2.75e-05, avg batch time: 0.5400, average loss: 0.6928
[11/20 11:24:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 53.25	rocauc: 56.66	
[11/20 11:24:50 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/20 11:32:18 visual_prompt]: Epoch 28 / 100: avg data time: 4.76e+00, avg batch time: 6.4020, average train loss: 0.7046
[11/20 11:33:12 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.5410, average loss: 0.6884
[11/20 11:33:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.19	
[11/20 11:33:12 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/20 11:40:45 visual_prompt]: Epoch 29 / 100: avg data time: 4.83e+00, avg batch time: 6.4706, average train loss: 0.7262
[11/20 11:41:38 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.5395, average loss: 0.8165
[11/20 11:41:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.76	
[11/20 11:41:38 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/20 11:49:10 visual_prompt]: Epoch 30 / 100: avg data time: 4.81e+00, avg batch time: 6.4484, average train loss: 0.7176
[11/20 11:50:03 visual_prompt]: Inference (val):avg data time: 2.72e-05, avg batch time: 0.5442, average loss: 0.7783
[11/20 11:50:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.82	
[11/20 11:50:03 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/20 11:57:32 visual_prompt]: Epoch 31 / 100: avg data time: 4.77e+00, avg batch time: 6.4091, average train loss: 0.7742
[11/20 11:58:25 visual_prompt]: Inference (val):avg data time: 3.31e-05, avg batch time: 0.5418, average loss: 0.8235
[11/20 11:58:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.33	
[11/20 11:58:25 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/20 12:06:06 visual_prompt]: Epoch 32 / 100: avg data time: 4.96e+00, avg batch time: 6.5946, average train loss: 0.7059
[11/20 12:07:00 visual_prompt]: Inference (val):avg data time: 2.46e-05, avg batch time: 0.5408, average loss: 0.6915
[11/20 12:07:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.07	rocauc: 56.44	
[11/20 12:07:00 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/20 12:14:32 visual_prompt]: Epoch 33 / 100: avg data time: 4.81e+00, avg batch time: 6.4554, average train loss: 0.7424
[11/20 12:15:25 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.5432, average loss: 0.8384
[11/20 12:15:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.99	
[11/20 12:15:25 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/20 12:22:58 visual_prompt]: Epoch 34 / 100: avg data time: 4.82e+00, avg batch time: 6.4597, average train loss: 0.7434
[11/20 12:23:51 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.5434, average loss: 0.7018
[11/20 12:23:51 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.57	
[11/20 12:23:51 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0039358216567073594
[11/20 12:31:21 visual_prompt]: Epoch 35 / 100: avg data time: 4.79e+00, avg batch time: 6.4327, average train loss: 0.7291
[11/20 12:32:15 visual_prompt]: Inference (val):avg data time: 3.62e-05, avg batch time: 0.5408, average loss: 0.6879
[11/20 12:32:15 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.32	
[11/20 12:32:15 visual_prompt]: Stopping early.
[11/20 12:32:15 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 12:32:15 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 12:32:15 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/20 12:32:15 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 12:32:15 visual_prompt]: Training with config:
[11/20 12:32:15 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size500/val/seed0/lr0.005_wd0.0001/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 500, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 32, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 12:32:15 visual_prompt]: Loading training data...
[11/20 12:32:15 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 12:32:15 visual_prompt]: Loading validation data...
[11/20 12:32:15 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 12:32:15 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 962, 768])
load_pretrained: grid-size from 14 to 31
[11/20 12:32:17 visual_prompt]: Enable all parameters update during training
[11/20 12:32:17 visual_prompt]: Total Parameters: 86387714	 Gradient Parameters: 86387714
[11/20 12:32:17 visual_prompt]: tuned percent:100.000
[11/20 12:32:17 visual_prompt]: Device used for model: 0
[11/20 12:32:17 visual_prompt]: Setting up Evaluator...
[11/20 12:32:17 visual_prompt]: Setting up Trainer...
[11/20 12:32:17 visual_prompt]: 	Setting up the optimizer...
[11/20 12:32:17 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 12:40:00 visual_prompt]: Epoch 1 / 100: avg data time: 4.98e+00, avg batch time: 6.6197, average train loss: 7.2380
[11/20 12:40:54 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5428, average loss: 6.4181
[11/20 12:40:54 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 52.79	
[11/20 12:40:54 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 12:48:22 visual_prompt]: Epoch 2 / 100: avg data time: 4.76e+00, avg batch time: 6.3980, average train loss: 2.2194
[11/20 12:49:14 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.5394, average loss: 0.7794
[11/20 12:49:14 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.80	
[11/20 12:49:14 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 12:56:46 visual_prompt]: Epoch 3 / 100: avg data time: 4.81e+00, avg batch time: 6.4435, average train loss: 1.2856
[11/20 12:57:38 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.5403, average loss: 0.6982
[11/20 12:57:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.84	
[11/20 12:57:38 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 13:05:10 visual_prompt]: Epoch 4 / 100: avg data time: 4.80e+00, avg batch time: 6.4455, average train loss: 0.8261
[11/20 13:06:03 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5414, average loss: 0.7768
[11/20 13:06:03 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.67	
[11/20 13:06:03 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 13:13:31 visual_prompt]: Epoch 5 / 100: avg data time: 4.75e+00, avg batch time: 6.3993, average train loss: 0.8085
[11/20 13:14:24 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.5399, average loss: 0.6948
[11/20 13:14:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.02	
[11/20 13:14:24 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 13:21:53 visual_prompt]: Epoch 6 / 100: avg data time: 4.77e+00, avg batch time: 6.4084, average train loss: 0.8006
[11/20 13:22:45 visual_prompt]: Inference (val):avg data time: 3.14e-05, avg batch time: 0.5385, average loss: 0.9182
[11/20 13:22:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.29	
[11/20 13:22:45 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 13:30:12 visual_prompt]: Epoch 7 / 100: avg data time: 4.74e+00, avg batch time: 6.3751, average train loss: 0.8154
[11/20 13:31:04 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.5416, average loss: 0.8020
[11/20 13:31:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.06	
[11/20 13:31:04 visual_prompt]: Best epoch 7: best metric: -0.802
[11/20 13:31:04 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 13:38:32 visual_prompt]: Epoch 8 / 100: avg data time: 4.75e+00, avg batch time: 6.3884, average train loss: 0.7536
[11/20 13:39:25 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5425, average loss: 1.1075
[11/20 13:39:25 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.26	
[11/20 13:39:25 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 13:46:53 visual_prompt]: Epoch 9 / 100: avg data time: 4.76e+00, avg batch time: 6.4048, average train loss: 0.8032
[11/20 13:47:46 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.5426, average loss: 0.9008
[11/20 13:47:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.78	
[11/20 13:47:46 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 13:55:14 visual_prompt]: Epoch 10 / 100: avg data time: 4.76e+00, avg batch time: 6.3984, average train loss: 0.9518
[11/20 13:56:07 visual_prompt]: Inference (val):avg data time: 2.60e-05, avg batch time: 0.5389, average loss: 0.8031
[11/20 13:56:07 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.82	
[11/20 13:56:07 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 14:03:36 visual_prompt]: Epoch 11 / 100: avg data time: 4.76e+00, avg batch time: 6.4034, average train loss: 0.7239
[11/20 14:04:28 visual_prompt]: Inference (val):avg data time: 3.03e-05, avg batch time: 0.5420, average loss: 0.7376
[11/20 14:04:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.52	
[11/20 14:04:28 visual_prompt]: Best epoch 11: best metric: -0.738
[11/20 14:04:28 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 14:11:56 visual_prompt]: Epoch 12 / 100: avg data time: 4.75e+00, avg batch time: 6.3888, average train loss: 0.7496
[11/20 14:12:49 visual_prompt]: Inference (val):avg data time: 2.80e-05, avg batch time: 0.5386, average loss: 1.2135
[11/20 14:12:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 58.19	
[11/20 14:12:49 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 14:20:16 visual_prompt]: Epoch 13 / 100: avg data time: 4.74e+00, avg batch time: 6.3820, average train loss: 0.7889
[11/20 14:21:08 visual_prompt]: Inference (val):avg data time: 2.62e-05, avg batch time: 0.5405, average loss: 0.7206
[11/20 14:21:08 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.10	rocauc: 57.06	
[11/20 14:21:08 visual_prompt]: Best epoch 13: best metric: -0.721
[11/20 14:21:08 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 14:28:37 visual_prompt]: Epoch 14 / 100: avg data time: 4.76e+00, avg batch time: 6.4034, average train loss: 0.7191
[11/20 14:29:30 visual_prompt]: Inference (val):avg data time: 2.76e-05, avg batch time: 0.5408, average loss: 0.6819
[11/20 14:29:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 58.64	
[11/20 14:29:30 visual_prompt]: Best epoch 14: best metric: -0.682
[11/20 14:29:30 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 14:36:56 visual_prompt]: Epoch 15 / 100: avg data time: 4.73e+00, avg batch time: 6.3716, average train loss: 0.7402
[11/20 14:37:49 visual_prompt]: Inference (val):avg data time: 3.23e-05, avg batch time: 0.5404, average loss: 0.7124
[11/20 14:37:49 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.34	
[11/20 14:37:49 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 14:45:14 visual_prompt]: Epoch 16 / 100: avg data time: 4.72e+00, avg batch time: 6.3563, average train loss: 0.7642
[11/20 14:46:06 visual_prompt]: Inference (val):avg data time: 2.58e-05, avg batch time: 0.5418, average loss: 0.6802
[11/20 14:46:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 57.38	
[11/20 14:46:06 visual_prompt]: Best epoch 16: best metric: -0.680
[11/20 14:46:06 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 14:53:35 visual_prompt]: Epoch 17 / 100: avg data time: 4.76e+00, avg batch time: 6.4002, average train loss: 0.7837
[11/20 14:54:27 visual_prompt]: Inference (val):avg data time: 2.79e-05, avg batch time: 0.5392, average loss: 0.7066
[11/20 14:54:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.53	
[11/20 14:54:27 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 15:01:52 visual_prompt]: Epoch 18 / 100: avg data time: 4.71e+00, avg batch time: 6.3495, average train loss: 0.7432
[11/20 15:02:45 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.5405, average loss: 0.8949
[11/20 15:02:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.66	
[11/20 15:02:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 15:10:14 visual_prompt]: Epoch 19 / 100: avg data time: 4.77e+00, avg batch time: 6.4072, average train loss: 0.7465
[11/20 15:11:06 visual_prompt]: Inference (val):avg data time: 2.78e-05, avg batch time: 0.5407, average loss: 0.7807
[11/20 15:11:06 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.48	
[11/20 15:11:06 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 15:18:37 visual_prompt]: Epoch 20 / 100: avg data time: 4.80e+00, avg batch time: 6.4404, average train loss: 0.7626
[11/20 15:19:30 visual_prompt]: Inference (val):avg data time: 2.73e-05, avg batch time: 0.5394, average loss: 0.6917
[11/20 15:19:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 57.95	
[11/20 15:19:30 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/20 15:26:59 visual_prompt]: Epoch 21 / 100: avg data time: 4.77e+00, avg batch time: 6.4114, average train loss: 0.6932
[11/20 15:27:53 visual_prompt]: Inference (val):avg data time: 3.43e-05, avg batch time: 0.5388, average loss: 0.6882
[11/20 15:27:53 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 56.11	
[11/20 15:27:53 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/20 15:35:23 visual_prompt]: Epoch 22 / 100: avg data time: 4.79e+00, avg batch time: 6.4294, average train loss: 0.7202
[11/20 15:36:16 visual_prompt]: Inference (val):avg data time: 2.71e-05, avg batch time: 0.5393, average loss: 0.7064
[11/20 15:36:16 visual_prompt]: Classification results with val_mammo-cbis: top1: 46.75	rocauc: 56.59	
[11/20 15:36:16 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/20 15:43:44 visual_prompt]: Epoch 23 / 100: avg data time: 4.76e+00, avg batch time: 6.3993, average train loss: 0.6976
[11/20 15:44:37 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.5396, average loss: 0.8752
[11/20 15:44:37 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 57.36	
[11/20 15:44:37 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/20 15:52:07 visual_prompt]: Epoch 24 / 100: avg data time: 4.79e+00, avg batch time: 6.4290, average train loss: 0.7123
[11/20 15:53:00 visual_prompt]: Inference (val):avg data time: 2.67e-05, avg batch time: 0.5379, average loss: 0.6841
[11/20 15:53:00 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 56.22	
[11/20 15:53:00 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/20 16:00:29 visual_prompt]: Epoch 25 / 100: avg data time: 4.77e+00, avg batch time: 6.4037, average train loss: 0.7482
[11/20 16:01:22 visual_prompt]: Inference (val):avg data time: 3.49e-05, avg batch time: 0.5420, average loss: 0.8789
[11/20 16:01:22 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.23	
[11/20 16:01:22 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/20 16:08:54 visual_prompt]: Epoch 26 / 100: avg data time: 4.82e+00, avg batch time: 6.4551, average train loss: 0.7166
[11/20 16:09:47 visual_prompt]: Inference (val):avg data time: 3.02e-05, avg batch time: 0.5404, average loss: 0.6804
[11/20 16:09:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 57.51	
[11/20 16:09:47 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/20 16:17:18 visual_prompt]: Epoch 27 / 100: avg data time: 4.80e+00, avg batch time: 6.4430, average train loss: 0.7704
[11/20 16:18:11 visual_prompt]: Inference (val):avg data time: 2.96e-05, avg batch time: 0.5437, average loss: 0.6872
[11/20 16:18:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.72	rocauc: 57.12	
[11/20 16:18:11 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/20 16:25:40 visual_prompt]: Epoch 28 / 100: avg data time: 4.77e+00, avg batch time: 6.4077, average train loss: 0.6905
[11/20 16:26:33 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.5401, average loss: 0.6932
[11/20 16:26:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 56.94	
[11/20 16:26:33 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/20 16:34:02 visual_prompt]: Epoch 29 / 100: avg data time: 4.77e+00, avg batch time: 6.4108, average train loss: 0.7233
[11/20 16:34:55 visual_prompt]: Inference (val):avg data time: 2.85e-05, avg batch time: 0.5391, average loss: 0.8404
[11/20 16:34:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 58.60	
[11/20 16:34:55 visual_prompt]: Training 30 / 100 epoch, with learning rate 0.004253091960681222
[11/20 16:42:24 visual_prompt]: Epoch 30 / 100: avg data time: 4.77e+00, avg batch time: 6.4118, average train loss: 0.7040
[11/20 16:43:17 visual_prompt]: Inference (val):avg data time: 2.94e-05, avg batch time: 0.5404, average loss: 0.8241
[11/20 16:43:17 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.09	
[11/20 16:43:17 visual_prompt]: Training 31 / 100 epoch, with learning rate 0.004193203929064353
[11/20 16:50:43 visual_prompt]: Epoch 31 / 100: avg data time: 4.73e+00, avg batch time: 6.3665, average train loss: 0.7792
[11/20 16:51:36 visual_prompt]: Inference (val):avg data time: 2.63e-05, avg batch time: 0.5393, average loss: 0.8144
[11/20 16:51:36 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 57.25	
[11/20 16:51:36 visual_prompt]: Training 32 / 100 epoch, with learning rate 0.004131464403810421
[11/20 16:59:04 visual_prompt]: Epoch 32 / 100: avg data time: 4.77e+00, avg batch time: 6.4039, average train loss: 0.6937
[11/20 16:59:57 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.5390, average loss: 0.6843
[11/20 16:59:57 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 56.94	
[11/20 16:59:57 visual_prompt]: Training 33 / 100 epoch, with learning rate 0.004067940896183842
[11/20 17:07:25 visual_prompt]: Epoch 33 / 100: avg data time: 4.75e+00, avg batch time: 6.3894, average train loss: 0.7261
[11/20 17:08:18 visual_prompt]: Inference (val):avg data time: 3.37e-05, avg batch time: 0.5398, average loss: 0.8042
[11/20 17:08:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.66	
[11/20 17:08:18 visual_prompt]: Training 34 / 100 epoch, with learning rate 0.004002702868207563
[11/20 17:15:47 visual_prompt]: Epoch 34 / 100: avg data time: 4.77e+00, avg batch time: 6.4089, average train loss: 0.7300
[11/20 17:16:40 visual_prompt]: Inference (val):avg data time: 3.01e-05, avg batch time: 0.5385, average loss: 0.6984
[11/20 17:16:40 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 59.16	
[11/20 17:16:40 visual_prompt]: Training 35 / 100 epoch, with learning rate 0.0039358216567073594
[11/20 17:24:11 visual_prompt]: Epoch 35 / 100: avg data time: 4.79e+00, avg batch time: 6.4311, average train loss: 0.7206
[11/20 17:25:04 visual_prompt]: Inference (val):avg data time: 4.04e-05, avg batch time: 0.5409, average loss: 0.6852
[11/20 17:25:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 58.30	
[11/20 17:25:04 visual_prompt]: Training 36 / 100 epoch, with learning rate 0.003867370395306068
[11/20 17:32:35 visual_prompt]: Epoch 36 / 100: avg data time: 4.80e+00, avg batch time: 6.4365, average train loss: 0.6844
[11/20 17:33:27 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.5418, average loss: 0.7680
[11/20 17:33:27 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 57.19	
[11/20 17:33:27 visual_prompt]: Training 37 / 100 epoch, with learning rate 0.0037974239344530382
[11/20 17:40:57 visual_prompt]: Epoch 37 / 100: avg data time: 4.78e+00, avg batch time: 6.4183, average train loss: 0.6996
[11/20 17:41:50 visual_prompt]: Inference (val):avg data time: 2.59e-05, avg batch time: 0.5395, average loss: 0.6911
[11/20 17:41:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.75	
[11/20 17:41:50 visual_prompt]: Stopping early.
[11/20 17:41:50 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 17:41:50 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 17:41:50 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/20 17:41:50 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 17:41:50 visual_prompt]: Training with config:
[11/20 17:41:50 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size500/val/seed0/lr0.005_wd0.0/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.005, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 500, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 32, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 17:41:50 visual_prompt]: Loading training data...
[11/20 17:41:50 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 17:41:50 visual_prompt]: Loading validation data...
[11/20 17:41:50 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 17:41:50 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 962, 768])
load_pretrained: grid-size from 14 to 31
[11/20 17:41:52 visual_prompt]: Enable all parameters update during training
[11/20 17:41:52 visual_prompt]: Total Parameters: 86387714	 Gradient Parameters: 86387714
[11/20 17:41:52 visual_prompt]: tuned percent:100.000
[11/20 17:41:52 visual_prompt]: Device used for model: 0
[11/20 17:41:52 visual_prompt]: Setting up Evaluator...
[11/20 17:41:52 visual_prompt]: Setting up Trainer...
[11/20 17:41:52 visual_prompt]: 	Setting up the optimizer...
[11/20 17:41:52 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
[11/20 17:49:25 visual_prompt]: Epoch 1 / 100: avg data time: 4.84e+00, avg batch time: 6.4675, average train loss: 7.2380
[11/20 17:50:18 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.5426, average loss: 6.4181
[11/20 17:50:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.53	rocauc: 52.79	
[11/20 17:50:18 visual_prompt]: Training 2 / 100 epoch, with learning rate 0.001
[11/20 17:57:45 visual_prompt]: Epoch 2 / 100: avg data time: 4.75e+00, avg batch time: 6.3780, average train loss: 5.7524
[11/20 17:58:38 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.5409, average loss: 0.8323
[11/20 17:58:38 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.48	
[11/20 17:58:38 visual_prompt]: Training 3 / 100 epoch, with learning rate 0.002
[11/20 18:06:08 visual_prompt]: Epoch 3 / 100: avg data time: 4.79e+00, avg batch time: 6.4220, average train loss: 1.0994
[11/20 18:07:01 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.5400, average loss: 1.0425
[11/20 18:07:01 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.95	
[11/20 18:07:01 visual_prompt]: Training 4 / 100 epoch, with learning rate 0.003
[11/20 18:14:31 visual_prompt]: Epoch 4 / 100: avg data time: 4.80e+00, avg batch time: 6.4217, average train loss: 0.8426
[11/20 18:15:24 visual_prompt]: Inference (val):avg data time: 3.17e-05, avg batch time: 0.5385, average loss: 1.2255
[11/20 18:15:24 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.27	
[11/20 18:15:24 visual_prompt]: Training 5 / 100 epoch, with learning rate 0.004
[11/20 18:22:53 visual_prompt]: Epoch 5 / 100: avg data time: 4.78e+00, avg batch time: 6.4134, average train loss: 0.9853
[11/20 18:23:46 visual_prompt]: Inference (val):avg data time: 3.50e-05, avg batch time: 0.5382, average loss: 0.9219
[11/20 18:23:46 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.98	
[11/20 18:23:46 visual_prompt]: Training 6 / 100 epoch, with learning rate 0.005
[11/20 18:31:16 visual_prompt]: Epoch 6 / 100: avg data time: 4.81e+00, avg batch time: 6.4332, average train loss: 1.1140
[11/20 18:32:10 visual_prompt]: Inference (val):avg data time: 3.09e-05, avg batch time: 0.5418, average loss: 1.8105
[11/20 18:32:10 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.53	
[11/20 18:32:10 visual_prompt]: Training 7 / 100 epoch, with learning rate 0.004998633143352315
[11/20 18:39:40 visual_prompt]: Epoch 7 / 100: avg data time: 4.80e+00, avg batch time: 6.4310, average train loss: 1.0635
[11/20 18:40:33 visual_prompt]: Inference (val):avg data time: 4.51e-05, avg batch time: 0.5378, average loss: 1.5912
[11/20 18:40:33 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.44	
[11/20 18:40:33 visual_prompt]: Best epoch 7: best metric: -1.591
[11/20 18:40:33 visual_prompt]: Training 8 / 100 epoch, with learning rate 0.004994534068046936
[11/20 18:48:02 visual_prompt]: Epoch 8 / 100: avg data time: 4.79e+00, avg batch time: 6.4138, average train loss: 0.9110
[11/20 18:48:55 visual_prompt]: Inference (val):avg data time: 3.59e-05, avg batch time: 0.5411, average loss: 0.6876
[11/20 18:48:55 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.50	rocauc: 54.48	
[11/20 18:48:55 visual_prompt]: Best epoch 8: best metric: -0.688
[11/20 18:48:55 visual_prompt]: Training 9 / 100 epoch, with learning rate 0.0049877072563625285
[11/20 18:56:25 visual_prompt]: Epoch 9 / 100: avg data time: 4.80e+00, avg batch time: 6.4249, average train loss: 0.9765
[11/20 18:57:18 visual_prompt]: Inference (val):avg data time: 2.88e-05, avg batch time: 0.5403, average loss: 3.0261
[11/20 18:57:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.24	
[11/20 18:57:18 visual_prompt]: Training 10 / 100 epoch, with learning rate 0.004978160173317438
[11/20 19:04:47 visual_prompt]: Epoch 10 / 100: avg data time: 4.78e+00, avg batch time: 6.4090, average train loss: 1.5162
[11/20 19:05:41 visual_prompt]: Inference (val):avg data time: 3.25e-05, avg batch time: 0.5387, average loss: 0.6969
[11/20 19:05:41 visual_prompt]: Classification results with val_mammo-cbis: top1: 56.91	rocauc: 55.32	
[11/20 19:05:41 visual_prompt]: Training 11 / 100 epoch, with learning rate 0.004965903258506806
[11/20 19:13:11 visual_prompt]: Epoch 11 / 100: avg data time: 4.81e+00, avg batch time: 6.4320, average train loss: 0.8540
[11/20 19:14:04 visual_prompt]: Inference (val):avg data time: 3.35e-05, avg batch time: 0.5388, average loss: 2.0887
[11/20 19:14:04 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.27	
[11/20 19:14:04 visual_prompt]: Training 12 / 100 epoch, with learning rate 0.004950949914687024
[11/20 19:21:37 visual_prompt]: Epoch 12 / 100: avg data time: 4.83e+00, avg batch time: 6.4557, average train loss: 1.6768
[11/20 19:22:30 visual_prompt]: Inference (val):avg data time: 2.87e-05, avg batch time: 0.5411, average loss: 2.2721
[11/20 19:22:30 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.01	
[11/20 19:22:30 visual_prompt]: Training 13 / 100 epoch, with learning rate 0.0049333164931200145
[11/20 19:30:02 visual_prompt]: Epoch 13 / 100: avg data time: 4.83e+00, avg batch time: 6.4614, average train loss: 1.0277
[11/20 19:30:56 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.5387, average loss: 0.8710
[11/20 19:30:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.20	
[11/20 19:30:56 visual_prompt]: Training 14 / 100 epoch, with learning rate 0.004913022275693372
[11/20 19:38:28 visual_prompt]: Epoch 14 / 100: avg data time: 4.82e+00, avg batch time: 6.4531, average train loss: 0.7774
[11/20 19:39:21 visual_prompt]: Inference (val):avg data time: 3.33e-05, avg batch time: 0.5378, average loss: 1.0550
[11/20 19:39:21 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 55.10	
[11/20 19:39:21 visual_prompt]: Training 15 / 100 epoch, with learning rate 0.0048900894538358945
[11/20 19:46:49 visual_prompt]: Epoch 15 / 100: avg data time: 4.77e+00, avg batch time: 6.3981, average train loss: 0.8396
[11/20 19:47:42 visual_prompt]: Inference (val):avg data time: 3.22e-05, avg batch time: 0.5396, average loss: 0.6992
[11/20 19:47:42 visual_prompt]: Classification results with val_mammo-cbis: top1: 57.32	rocauc: 55.18	
[11/20 19:47:42 visual_prompt]: Training 16 / 100 epoch, with learning rate 0.004864543104251586
[11/20 19:55:09 visual_prompt]: Epoch 16 / 100: avg data time: 4.75e+00, avg batch time: 6.3834, average train loss: 0.7581
[11/20 19:56:02 visual_prompt]: Inference (val):avg data time: 3.07e-05, avg batch time: 0.5389, average loss: 0.6934
[11/20 19:56:02 visual_prompt]: Classification results with val_mammo-cbis: top1: 58.13	rocauc: 54.99	
[11/20 19:56:02 visual_prompt]: Training 17 / 100 epoch, with learning rate 0.004836411161498653
[11/20 20:03:30 visual_prompt]: Epoch 17 / 100: avg data time: 4.78e+00, avg batch time: 6.4052, average train loss: 0.9725
[11/20 20:04:23 visual_prompt]: Inference (val):avg data time: 3.48e-05, avg batch time: 0.5410, average loss: 0.6909
[11/20 20:04:23 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.69	rocauc: 55.27	
[11/20 20:04:23 visual_prompt]: Training 18 / 100 epoch, with learning rate 0.004805724387443462
[11/20 20:11:52 visual_prompt]: Epoch 18 / 100: avg data time: 4.78e+00, avg batch time: 6.4090, average train loss: 0.8761
[11/20 20:12:45 visual_prompt]: Inference (val):avg data time: 3.30e-05, avg batch time: 0.5423, average loss: 2.1707
[11/20 20:12:45 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 55.26	
[11/20 20:12:45 visual_prompt]: Training 19 / 100 epoch, with learning rate 0.004772516337622906
[11/20 20:20:16 visual_prompt]: Epoch 19 / 100: avg data time: 4.80e+00, avg batch time: 6.4304, average train loss: 1.0734
[11/20 20:21:09 visual_prompt]: Inference (val):avg data time: 2.99e-05, avg batch time: 0.5395, average loss: 0.8351
[11/20 20:21:09 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.63	
[11/20 20:21:09 visual_prompt]: Training 20 / 100 epoch, with learning rate 0.004736823324551909
[11/20 20:28:41 visual_prompt]: Epoch 20 / 100: avg data time: 4.82e+00, avg batch time: 6.4489, average train loss: 0.8906
[11/20 20:29:34 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.5382, average loss: 0.6880
[11/20 20:29:34 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 55.73	
[11/20 20:29:34 visual_prompt]: Training 21 / 100 epoch, with learning rate 0.004698684378016222
[11/20 20:37:03 visual_prompt]: Epoch 21 / 100: avg data time: 4.79e+00, avg batch time: 6.4124, average train loss: 0.8705
[11/20 20:37:56 visual_prompt]: Inference (val):avg data time: 3.12e-05, avg batch time: 0.5379, average loss: 1.2949
[11/20 20:37:56 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.20	
[11/20 20:37:56 visual_prompt]: Training 22 / 100 epoch, with learning rate 0.004658141202393935
[11/20 20:45:26 visual_prompt]: Epoch 22 / 100: avg data time: 4.80e+00, avg batch time: 6.4190, average train loss: 0.9775
[11/20 20:46:18 visual_prompt]: Inference (val):avg data time: 2.90e-05, avg batch time: 0.5395, average loss: 0.7306
[11/20 20:46:18 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.20	
[11/20 20:46:18 visual_prompt]: Training 23 / 100 epoch, with learning rate 0.004615238131052338
[11/20 20:53:57 visual_prompt]: Epoch 23 / 100: avg data time: 4.92e+00, avg batch time: 6.5446, average train loss: 1.0269
[11/20 20:54:50 visual_prompt]: Inference (val):avg data time: 3.21e-05, avg batch time: 0.5389, average loss: 1.5812
[11/20 20:54:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.49	
[11/20 20:54:50 visual_prompt]: Training 24 / 100 epoch, with learning rate 0.00457002207787005
[11/20 21:02:18 visual_prompt]: Epoch 24 / 100: avg data time: 4.78e+00, avg batch time: 6.4046, average train loss: 0.9656
[11/20 21:03:12 visual_prompt]: Inference (val):avg data time: 2.97e-05, avg batch time: 0.5404, average loss: 0.7525
[11/20 21:03:12 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.69	
[11/20 21:03:12 visual_prompt]: Training 25 / 100 epoch, with learning rate 0.0045225424859373685
[11/20 21:10:38 visual_prompt]: Epoch 25 / 100: avg data time: 4.75e+00, avg batch time: 6.3732, average train loss: 0.7469
[11/20 21:11:31 visual_prompt]: Inference (val):avg data time: 3.00e-05, avg batch time: 0.5412, average loss: 1.1798
[11/20 21:11:31 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.39	
[11/20 21:11:31 visual_prompt]: Training 26 / 100 epoch, with learning rate 0.004472851273490984
[11/20 21:18:58 visual_prompt]: Epoch 26 / 100: avg data time: 4.76e+00, avg batch time: 6.3806, average train loss: 1.1473
[11/20 21:19:50 visual_prompt]: Inference (val):avg data time: 2.95e-05, avg batch time: 0.5407, average loss: 2.1928
[11/20 21:19:50 visual_prompt]: Classification results with val_mammo-cbis: top1: 54.88	rocauc: 54.72	
[11/20 21:19:50 visual_prompt]: Training 27 / 100 epoch, with learning rate 0.004421002777142148
[11/20 21:27:18 visual_prompt]: Epoch 27 / 100: avg data time: 4.76e+00, avg batch time: 6.3863, average train loss: 1.0442
[11/20 21:28:11 visual_prompt]: Inference (val):avg data time: 2.77e-05, avg batch time: 0.5408, average loss: 0.7450
[11/20 21:28:11 visual_prompt]: Classification results with val_mammo-cbis: top1: 55.28	rocauc: 54.33	
[11/20 21:28:11 visual_prompt]: Training 28 / 100 epoch, with learning rate 0.004367053692460385
[11/20 21:35:35 visual_prompt]: Epoch 28 / 100: avg data time: 4.72e+00, avg batch time: 6.3478, average train loss: 0.8729
[11/20 21:36:28 visual_prompt]: Inference (val):avg data time: 3.24e-05, avg batch time: 0.5384, average loss: 0.9904
[11/20 21:36:28 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 54.83	
[11/20 21:36:28 visual_prompt]: Training 29 / 100 epoch, with learning rate 0.004311063011977723
[11/20 21:43:54 visual_prompt]: Epoch 29 / 100: avg data time: 4.74e+00, avg batch time: 6.3685, average train loss: 0.8216
[11/20 21:44:47 visual_prompt]: Inference (val):avg data time: 4.15e-05, avg batch time: 0.5390, average loss: 0.7487
[11/20 21:44:47 visual_prompt]: Classification results with val_mammo-cbis: top1: 45.12	rocauc: 56.15	
[11/20 21:44:47 visual_prompt]: Stopping early.
[11/20 21:44:47 visual_prompt]: Rank of current process: 0. World size: 1
[11/20 21:44:47 visual_prompt]: Environment info:
-------------------  -------------------------------------------------
Python               3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
ENV_MODULE           <not set>
PyTorch              2.0.1+cu118
PyTorch Debug Build  False
CUDA available       True
CUDA ID              2
GPU 0                NVIDIA A40
Pillow               9.3.0
cv2                  4.8.0
-------------------  -------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[11/20 21:44:47 visual_prompt]: Command line arguments: Namespace(config_file='visual_prompt_tuning/configs/finetune/cub.yaml', train_type='finetune', opts=['MODEL.TYPE', 'vit', 'DATA.BATCH_SIZE', '32', 'MODEL.PROMPT.NUM_TOKENS', '50', 'MODEL.PROMPT.DEEP', 'True', 'MODEL.PROMPT.DROPOUT', '0.1', 'DATA.FEATURE', 'sup_vitb16_imagenet21k', 'DATA.IMGSIZE', '500', 'DATA.NAME', 'mammo-cbis', 'DATA.NUMBER_CLASSES', '2', 'DATA.CROP', 'False', 'MODEL.MODEL_ROOT', 'visual_prompt_tuning/model_root', 'DATA.DATAPATH', 'visual_prompt_tuning/data_path', 'OUTPUT_DIR', 'visual_prompt_tuning/output_dir', 'SOLVER.PATIENCE', '21', 'SOLVER.CRITERION', 'loss'])
[11/20 21:44:47 visual_prompt]: Contents of args.config_file=visual_prompt_tuning/configs/finetune/cub.yaml:
_BASE_: "../base-finetune.yaml"
RUN_N_TIMES: 1
DATA:
  NAME: "CUB"
  DATAPATH: ""  #TODO: need to specify here
  NUMBER_CLASSES: 200
  MULTILABEL: False
  FEATURE: "imagenet_supervised"  # need to tune
MODEL:
  TYPE: "vit"
SOLVER:
  BASE_LR: 0.00375
  WEIGHT_DECAY: 0.01

[11/20 21:44:47 visual_prompt]: Training with config:
[11/20 21:44:47 visual_prompt]: CfgNode({'DBG': False, 'OUTPUT_DIR': 'visual_prompt_tuning/output_dir/mammo-cbis/sup_vitb16_imagenet21k/end2end/size500/val/seed0/lr0.001_wd0.01/patience21/run1', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SEED': 0, 'MODEL': CfgNode({'TRANSFER_TYPE': 'end2end', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': 'visual_prompt_tuning/model_root', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 50, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': True, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.1, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'adamw', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 21, 'CRITERION': 'loss', 'SCHEDULER': 'cosine', 'BASE_LR': 0.001, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 5, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'mammo-cbis', 'DATAPATH': 'visual_prompt_tuning/data_path', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 2, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'IMGSIZE': 500, 'CROP': False, 'NO_TEST': False, 'BATCH_SIZE': 32, 'NUM_WORKERS': 8, 'PIN_MEMORY': True})})
[11/20 21:44:47 visual_prompt]: Loading training data...
[11/20 21:44:47 visual_prompt]: Constructing mammo-cbis dataset train...
[11/20 21:44:47 visual_prompt]: Loading validation data...
[11/20 21:44:47 visual_prompt]: Constructing mammo-cbis dataset val...
[11/20 21:44:47 visual_prompt]: Constructing models...
[INFO: vit.py:  342]: load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 962, 768])
load_pretrained: grid-size from 14 to 31
[11/20 21:44:49 visual_prompt]: Enable all parameters update during training
[11/20 21:44:49 visual_prompt]: Total Parameters: 86387714	 Gradient Parameters: 86387714
[11/20 21:44:49 visual_prompt]: tuned percent:100.000
[11/20 21:44:49 visual_prompt]: Device used for model: 0
[11/20 21:44:49 visual_prompt]: Setting up Evaluator...
[11/20 21:44:49 visual_prompt]: Setting up Trainer...
[11/20 21:44:49 visual_prompt]: 	Setting up the optimizer...
[11/20 21:44:49 visual_prompt]: Training 1 / 100 epoch, with learning rate 0.0
Traceback (most recent call last):
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 99, in <module>
    main(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 83, in main
    explore_lrwd_range(args)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 79, in explore_lrwd_range
    train(cfg, args, test=False)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/tune_cbis.py", line 44, in train
    trainer.train_classifier(train_loader, val_loader, test_loader)
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/engine/trainer.py", line 187, in train_classifier
    train_loss, _ = self.forward_one_batch(X, targets, True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s1952889/final-project/implementation/visual_prompt_tuning/src/engine/trainer.py", line 120, in forward_one_batch
    loss.backward()
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/s1952889/miniconda3/envs/prompt/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.32 GiB (GPU 0; 44.35 GiB total capacity; 41.11 GiB already allocated; 898.75 MiB free; 43.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/bin/bash: /home/s1952889/miniconda3/envs/prompt/lib/libtinfo.so.6: no version information available (required by /bin/bash)
